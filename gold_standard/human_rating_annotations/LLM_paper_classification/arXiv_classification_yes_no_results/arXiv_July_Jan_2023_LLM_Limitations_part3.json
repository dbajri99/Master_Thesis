[
    {
        "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
        "authors": [
            "Suyu Ge",
            "Yunan Zhang",
            "Liyuan Liu",
            "Minjia Zhang",
            "Jiawei Han",
            "Jianfeng Gao"
        ],
        "published": "2023-10-03T05:17:08Z",
        "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2310.01801v3.pdf"
    },
    {
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
        "authors": [
            "Jie Huang",
            "Xinyun Chen",
            "Swaroop Mishra",
            "Huaixiu Steven Zheng",
            "Adams Wei Yu",
            "Xinying Song",
            "Denny Zhou"
        ],
        "published": "2023-10-03T04:56:12Z",
        "summary": "Large Language Models (LLMs) have emerged as a groundbreaking technology with\ntheir unparalleled text generation capabilities across various applications.\nNevertheless, concerns persist regarding the accuracy and appropriateness of\ntheir generated content. A contemporary methodology, self-correction, has been\nproposed as a remedy to these issues. Building upon this premise, this paper\ncritically examines the role and efficacy of self-correction within LLMs,\nshedding light on its true potential and limitations. Central to our\ninvestigation is the notion of intrinsic self-correction, whereby an LLM\nattempts to correct its initial responses based solely on its inherent\ncapabilities, without the crutch of external feedback. In the context of\nreasoning, our research indicates that LLMs struggle to self-correct their\nresponses without external feedback, and at times, their performance even\ndegrades after self-correction. Drawing from these insights, we offer\nsuggestions for future research and practical applications in this field.",
        "pdf_link": "https://arxiv.org/pdf/2310.01798v2.pdf"
    },
    {
        "title": "HallE-Control: Controlling Object Hallucination in Large Multimodal Models",
        "authors": [
            "Bohan Zhai",
            "Shijia Yang",
            "Chenfeng Xu",
            "Sheng Shen",
            "Kurt Keutzer",
            "Chunyuan Li",
            "Manling Li"
        ],
        "published": "2023-10-03T04:01:27Z",
        "summary": "Current Large Multimodal Models (LMMs) achieve remarkable progress, yet there\nremains significant uncertainty regarding their ability to accurately apprehend\nvisual details, that is, in performing detailed captioning. To address this, we\nintroduce $\\textit{CCEval}$, a GPT-4 assisted evaluation method for detailed\ncaptioning. Interestingly, while LMMs demonstrate minimal object existence\nhallucination in existing VQA benchmarks, our proposed evaluation reveals\ncontinued susceptibility to such hallucinations. In this paper, we make the\nfirst attempt to investigate such hallucination from different aspects,\nincluding image resolution, the language decoder size, and instruction data\namount, quality, granularity. Our findings underscore the unwarranted inference\nwhen the language description includes details at a finer object granularity\nthan what the vision module can ground or verify, thus inducing hallucination.\nTo control such hallucinations, we further attribute the reliability of\ncaptioning to contextual knowledge (involving only contextually grounded\nobjects) and parametric knowledge (containing inferred objects by the model).\nThus, we introduce $\\textit{HallE-Control}$, a controllable LMM in terms of\n$\\textbf{Hall}$ucination in object $\\textbf{E}$xistence. HallE-Control can\ncondition the captioning to shift between (i) exclusively depicting contextual\nknowledge for grounded objects and (ii) blending it with parametric knowledge\nto imagine inferred objects. Our method reduces hallucination by 44% compared\nto LLaVA$_{7B}$ and maintains the object coverage.",
        "pdf_link": "https://arxiv.org/pdf/2310.01779v3.pdf"
    },
    {
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
        "authors": [
            "Ming Jin",
            "Shiyu Wang",
            "Lintao Ma",
            "Zhixuan Chu",
            "James Y. Zhang",
            "Xiaoming Shi",
            "Pin-Yu Chen",
            "Yuxuan Liang",
            "Yuan-Fang Li",
            "Shirui Pan",
            "Qingsong Wen"
        ],
        "published": "2023-10-03T01:31:25Z",
        "summary": "Time series forecasting holds significant importance in many real-world\ndynamic systems and has been extensively studied. Unlike natural language\nprocess (NLP) and computer vision (CV), where a single large model can tackle\nmultiple tasks, models for time series forecasting are often specialized,\nnecessitating distinct designs for different tasks and applications. While\npre-trained foundation models have made impressive strides in NLP and CV, their\ndevelopment in time series domains has been constrained by data sparsity.\nRecent studies have revealed that large language models (LLMs) possess robust\npattern recognition and reasoning abilities over complex sequences of tokens.\nHowever, the challenge remains in effectively aligning the modalities of time\nseries data and natural language to leverage these capabilities. In this work,\nwe present Time-LLM, a reprogramming framework to repurpose LLMs for general\ntime series forecasting with the backbone language models kept intact. We begin\nby reprogramming the input time series with text prototypes before feeding it\ninto the frozen LLM to align the two modalities. To augment the LLM's ability\nto reason with time series data, we propose Prompt-as-Prefix (PaP), which\nenriches the input context and directs the transformation of reprogrammed input\npatches. The transformed time series patches from the LLM are finally projected\nto obtain the forecasts. Our comprehensive evaluations demonstrate that\nTime-LLM is a powerful time series learner that outperforms state-of-the-art,\nspecialized forecasting models. Moreover, Time-LLM excels in both few-shot and\nzero-shot learning scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2310.01728v2.pdf"
    },
    {
        "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
        "authors": [
            "Jenny T. Liang",
            "Carmen Badea",
            "Christian Bird",
            "Robert DeLine",
            "Denae Ford",
            "Nicole Forsgren",
            "Thomas Zimmermann"
        ],
        "published": "2023-10-03T01:27:23Z",
        "summary": "Empirical software engineering research on production systems has brought\nforth a better understanding of the software engineering process for\npractitioners and researchers alike. However, only a small subset of production\nsystems is studied, limiting the impact of this research. While software\nengineering practitioners benefit from replicating research on their own data,\nthis poses its own set of challenges, since performing replications requires a\ndeep understanding of research methodologies and subtle nuances in software\nengineering data. Given that large language models (LLMs), such as GPT-4, show\npromise in tackling both software engineering- and science-related tasks, these\nmodels could help democratize empirical software engineering research.\n  In this paper, we examine LLMs' abilities to perform replications of\nempirical software engineering research on new data. We specifically study\ntheir ability to surface assumptions made in empirical software engineering\nresearch methodologies, as well as their ability to plan and generate code for\nanalysis pipelines on seven empirical software engineering papers. We perform a\nuser study with 14 participants with software engineering research expertise,\nwho evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of\nmodule specifications) from the papers. We find that GPT-4 is able to surface\ncorrect assumptions, but struggle to generate ones that reflect common\nknowledge about software engineering data. In a manual analysis of the\ngenerated code, we find that the GPT-4-generated code contains the correct\nhigh-level logic, given a subset of the methodology. However, the code contains\nmany small implementation-level errors, reflecting a lack of software\nengineering knowledge. Our findings have implications for leveraging LLMs for\nsoftware engineering research as well as practitioner data scientists in\nsoftware teams.",
        "pdf_link": "https://arxiv.org/pdf/2310.01727v1.pdf"
    },
    {
        "title": "Large Language Models for Test-Free Fault Localization",
        "authors": [
            "Aidan Z. H. Yang",
            "Ruben Martins",
            "Claire Le Goues",
            "Vincent J. Hellendoorn"
        ],
        "published": "2023-10-03T01:26:39Z",
        "summary": "Fault Localization (FL) aims to automatically localize buggy lines of code, a\nkey first step in many manual and automatic debugging tasks. Previous FL\ntechniques assume the provision of input tests, and often require extensive\nprogram analysis, program instrumentation, or data preprocessing. Prior work on\ndeep learning for APR struggles to learn from small datasets and produces\nlimited results on real-world programs. Inspired by the ability of large\nlanguage models (LLMs) of code to adapt to new tasks based on very few\nexamples, we investigate the applicability of LLMs to line level fault\nlocalization. Specifically, we propose to overcome the left-to-right nature of\nLLMs by fine-tuning a small set of bidirectional adapter layers on top of the\nrepresentations learned by LLMs to produce LLMAO, the first language model\nbased fault localization approach that locates buggy lines of code without any\ntest coverage information. We fine-tune LLMs with 350 million, 6 billion, and\n16 billion parameters on small, manually curated corpora of buggy programs such\nas the Defects4J corpus. We observe that our technique achieves substantially\nmore confidence in fault localization when built on the larger models, with bug\nlocalization performance scaling consistently with the LLM size. Our empirical\nevaluation shows that LLMAO improves the Top-1 results over the\nstate-of-the-art machine learning fault localization (MLFL) baselines by\n2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL\ntechnique trained using a language model architecture that can detect security\nvulnerabilities down to the code line level.",
        "pdf_link": "https://arxiv.org/pdf/2310.01726v1.pdf"
    },
    {
        "title": "Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making",
        "authors": [
            "D. Umerenkov",
            "G. Zubkova",
            "A. Nesterov"
        ],
        "published": "2023-10-03T00:08:23Z",
        "summary": "Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and\npatient data to offer real-time recommendations, with Large Language Models\n(LLMs) emerging as a promising tool to generate plain-text explanations for\nmedical decisions. This study explores the effectiveness and reliability of\nLLMs in generating explanations for diagnoses based on patient complaints.\nThree experienced doctors evaluated LLM-generated explanations of the\nconnection between patient complaints and doctor and model-assigned diagnoses\nacross several stages. Experimental results demonstrated that LLM explanations\nsignificantly increased doctors' agreement rates with given diagnoses and\nhighlighted potential errors in LLM outputs, ranging from 5% to 30%. The study\nunderscores the potential and challenges of LLMs in healthcare and emphasizes\nthe need for careful integration and evaluation to ensure patient safety and\noptimal clinical utility.",
        "pdf_link": "https://arxiv.org/pdf/2310.01708v1.pdf"
    },
    {
        "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
        "authors": [
            "Praneeth Kacham",
            "Vahab Mirrokni",
            "Peilin Zhong"
        ],
        "published": "2023-10-02T21:39:04Z",
        "summary": "The quadratic time and memory complexity inherent to self-attention\nmechanisms, with respect to sequence length, presents a critical computational\nbottleneck in the training and deployment of large-scale Transformer-based\nlanguage models. Recent theoretical results indicate the intractability of\nsub-quadratic softmax attention approximation under reasonable complexity\nassumptions. This paper addresses this challenge by first demonstrating that\npolynomial attention with high degree can effectively replace softmax without\nsacrificing model quality. Next, we develop polynomial sketching techniques\nfrom numerical linear algebra to achieve linear-time polynomial attention with\napproximation guarantees. Crucially, our approach achieves this speedup without\nrequiring the sparsification of attention matrices. We also present a\nblock-based algorithm to apply causal masking efficiently. Combining these\ntechniques, we provide \\emph{PolySketchFormer}, a practical linear-time\nTransformer architecture for language modeling that offers provable guarantees.\n  We validate PolySketchFormer empirically by training language models capable\nof handling long contexts. These experiments utilize both synthetic and\nreal-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context\nlengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in\ntraining compared to FlashAttention, with no observed degradation in quality\nacross our experiments.",
        "pdf_link": "https://arxiv.org/pdf/2310.01655v3.pdf"
    },
    {
        "title": "VAL: Interactive Task Learning with GPT Dialog Parsing",
        "authors": [
            "Lane Lawley",
            "Christopher J. MacLellan"
        ],
        "published": "2023-10-02T20:45:41Z",
        "summary": "Reinforcement learning often requires millions of examples to produce static,\nblack-box models. In contrast, interactive task learning (ITL) emphasizes\nincremental knowledge acquisition from limited instruction provided by humans\nin modalities such as natural language. However, in practice, ITL systems often\nsuffers from brittle, error-prone language parsing. Large language models\n(LLMs) are resistant to brittleness but are not interpretable and cannot learn\nincrementally. We present VAL, an ITL system with a new philosophy for\nLLM/symbolic integration. By using LLMs only for specific tasks -- such as\npredicate and argument selection -- within an algorithmic framework, VAL reaps\nthe benefits of LLMs to support interactive learning of hierarchical task\nknowledge from natural language. Acquired knowledge is human interpretable and\ngeneralizes to support execution of novel tasks without additional training. We\nstudied users' interactions with VAL in a video game setting, finding that most\nusers could successfully teach VAL using language they felt was natural.",
        "pdf_link": "https://arxiv.org/pdf/2310.01627v1.pdf"
    },
    {
        "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?",
        "authors": [
            "Hangfan Zhang",
            "Zhimeng Guo",
            "Huaisheng Zhu",
            "Bochuan Cao",
            "Lu Lin",
            "Jinyuan Jia",
            "Jinghui Chen",
            "Dinghao Wu"
        ],
        "published": "2023-10-02T19:22:01Z",
        "summary": "Large Language Models (LLMs) have achieved unprecedented performance in\nNatural Language Generation (NLG) tasks. However, many existing studies have\nshown that they could be misused to generate undesired content. In response,\nbefore releasing LLMs for public access, model developers usually align those\nlanguage models through Supervised Fine-Tuning (SFT) or Reinforcement Learning\nwith Human Feedback (RLHF). Consequently, those aligned large language models\nrefuse to generate undesired content when facing potentially harmful/unethical\nrequests. A natural question is \"could alignment really prevent those\nopen-sourced large language models from being misused to generate undesired\ncontent?''. In this work, we provide a negative answer to this question. In\nparticular, we show those open-sourced, aligned large language models could be\neasily misguided to generate undesired content without heavy computations or\ncareful prompt designs. Our key idea is to directly manipulate the generation\nprocess of open-sourced LLMs to misguide it to generate undesired content\nincluding harmful or biased information and even private data. We evaluate our\nmethod on 4 open-sourced LLMs accessible publicly and our finding highlights\nthe need for more advanced mitigation strategies for open-sourced LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.01581v1.pdf"
    },
    {
        "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",
        "authors": [
            "Jia-Yu Yao",
            "Kun-Peng Ning",
            "Zhen-Hui Liu",
            "Mu-Nan Ning",
            "Li Yuan"
        ],
        "published": "2023-10-02T17:01:56Z",
        "summary": "Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be\nknowledgeable and able to adapt to many tasks. However, we still can not\ncompletely trust their answer, since LLMs suffer from\nhallucination--fabricating non-existent facts to cheat users without\nperception. And the reasons for their existence and pervasiveness remain\nunclear. In this paper, we demonstrate that non-sense prompts composed of\nrandom tokens can also elicit the LLMs to respond with hallucinations. This\nphenomenon forces us to revisit that hallucination may be another view of\nadversarial examples, and it shares similar features with conventional\nadversarial examples as the basic feature of LLMs. Therefore, we formalize an\nautomatic hallucination triggering method as the hallucination attack in an\nadversarial way. Finally, we explore basic feature of attacked adversarial\nprompts and propose a simple yet effective defense strategy. Our code is\nreleased on GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2310.01469v2.pdf"
    },
    {
        "title": "Towards reporting bias in visual-language datasets: bimodal augmentation by decoupling object-attribute association",
        "authors": [
            "Qiyu Wu",
            "Mengjie Zhao",
            "Yutong He",
            "Lang Huang",
            "Junya Ono",
            "Hiromi Wakaki",
            "Yuki Mitsufuji"
        ],
        "published": "2023-10-02T16:48:50Z",
        "summary": "Reporting bias arises when people assume that some knowledge is universally\nunderstood and hence, do not necessitate explicit elaboration. In this paper,\nwe focus on the wide existence of reporting bias in visual-language datasets,\nembodied as the object-attribute association, which can subsequentially degrade\nmodels trained on them. To mitigate this bias, we propose a bimodal\naugmentation (BiAug) approach through object-attribute decoupling to flexibly\nsynthesize visual-language examples with a rich array of object-attribute\npairing and construct cross-modal hard negatives. We employ large language\nmodels (LLMs) in conjunction with a grounding object detector to extract target\nobjects. Subsequently, the LLM generates a detailed attribute description for\neach object and produces a corresponding hard negative counterpart. An\ninpainting model is then used to create images based on these detailed object\ndescriptions. By doing so, the synthesized examples explicitly complement\nomitted objects and attributes to learn, and the hard negative pairs steer the\nmodel to distinguish object attributes. Our experiments demonstrated that BiAug\nis superior in object-attribute understanding. In addition, BiAug also improves\nthe performance on zero-shot retrieval tasks on general benchmarks like MSCOCO\nand Flickr30K. BiAug refines the way of collecting text-image datasets.\nMitigating the reporting bias helps models achieve a deeper understanding of\nvisual-language phenomena, expanding beyond mere frequent patterns to encompass\nthe richness and diversity of real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2310.01330v1.pdf"
    },
    {
        "title": "Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation",
        "authors": [
            "Shenzhi Wang",
            "Chang Liu",
            "Zilong Zheng",
            "Siyuan Qi",
            "Shuo Chen",
            "Qisen Yang",
            "Andrew Zhao",
            "Chaofei Wang",
            "Shiji Song",
            "Gao Huang"
        ],
        "published": "2023-10-02T16:27:36Z",
        "summary": "Recent breakthroughs in large language models (LLMs) have brought remarkable\nsuccess in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is\nthat the information processed by LLMs is consistently honest, neglecting the\npervasive deceptive or misleading information in human society and AI-generated\ncontent. This oversight makes LLMs susceptible to malicious manipulations,\npotentially resulting in detrimental outcomes. This study utilizes the\nintricate Avalon game as a testbed to explore LLMs' potential in deceptive\nenvironments. Avalon, full of misinformation and requiring sophisticated logic,\nmanifests as a \"Game-of-Thoughts\". Inspired by the efficacy of humans'\nrecursive thinking and perspective-taking in the Avalon game, we introduce a\nnovel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to\nidentify and counteract deceptive information. ReCon combines formulation and\nrefinement contemplation processes; formulation contemplation produces initial\nthoughts and speech, while refinement contemplation further polishes them.\nAdditionally, we incorporate first-order and second-order perspective\ntransitions into these processes respectively. Specifically, the first-order\nallows an LLM agent to infer others' mental states, and the second-order\ninvolves understanding how others perceive the agent's mental state. After\nintegrating ReCon with different LLMs, extensive experiment results from the\nAvalon game indicate its efficacy in aiding LLMs to discern and maneuver around\ndeceptive information without extra fine-tuning and data. Finally, we offer a\npossible explanation for the efficacy of ReCon and explore the current\nlimitations of LLMs in terms of safety, reasoning, speaking style, and format,\npotentially furnishing insights for subsequent research.",
        "pdf_link": "https://arxiv.org/pdf/2310.01320v3.pdf"
    },
    {
        "title": "Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models",
        "authors": [
            "Wenxuan Ding",
            "Shangbin Feng",
            "Yuhan Liu",
            "Zhaoxuan Tan",
            "Vidhisha Balachandran",
            "Tianxing He",
            "Yulia Tsvetkov"
        ],
        "published": "2023-10-02T15:43:53Z",
        "summary": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks\nand have achieved impressive performance thanks to their knowledge abilities.\nWhile LLMs have demonstrated outstanding performance on atomic or linear\n(multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with\ninterweaving constraints remains an underexplored problem. In this work, we\npropose geometric reasoning over structured knowledge, where pieces of\nknowledge are connected in a graph structure and models need to fill in the\nmissing information. Such geometric knowledge reasoning would require the\nability to handle structured knowledge, reason with uncertainty, verify facts,\nand backtrack when an error occurs. We propose Knowledge Crosswords, a\nmulti-blank QA dataset where each problem consists of a natural language\nquestion representing the geometric constraints of an incomplete entity\nnetwork, where LLMs are tasked with working out the missing entities while\nmeeting all factual constraints. Knowledge Crosswords contains 2,101 individual\nproblems, covering various knowledge domains and further divided into three\ndifficulty levels. We conduct extensive experiments to evaluate existing LLM\nprompting approaches on the Knowledge Crosswords benchmark. We additionally\npropose two new approaches, Staged Prompting and Verify-All, to augment LLMs'\nability to backtrack and verify structured constraints. Our results demonstrate\nthat while baseline approaches perform well on easier problems but struggle\nwith hard ones, our proposed Verify-All outperforms other methods by a large\nmargin and is more robust with hard problems. Further analysis reveals that\nLLMs' ability of geometric reasoning over structured knowledge is still far\nfrom robust or perfect, susceptible to confounders such as the order of\noptions, certain structural patterns, assumption of existence of correct\nanswer, and more.",
        "pdf_link": "https://arxiv.org/pdf/2310.01290v1.pdf"
    },
    {
        "title": "SPELL: Semantic Prompt Evolution based on a LLM",
        "authors": [
            "Yujian Betterest Li",
            "Kai Wu"
        ],
        "published": "2023-10-02T14:51:16Z",
        "summary": "Prompt engineering is a new paradigm for enhancing the performance of trained\nneural network models. For optimizing text-style prompts, existing methods\nusually individually operate small portions of a text step by step, which\neither breaks the fluency or could not globally adjust a prompt. Since large\nlanguage models (LLMs) have powerful ability of generating coherent texts token\nby token, can we utilize LLMs for improving prompts? Based on this motivation,\nin this paper, considering a trained LLM as a text generator, we attempt to\ndesign a black-box evolution algorithm for automatically optimizing texts,\nnamely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method is\nevaluated with different LLMs and evolution parameters in different text tasks.\nExperimental results show that SPELL could rapidly improve the prompts indeed.\nWe further explore the evolution process and discuss on the limitations,\npotential possibilities and future work.",
        "pdf_link": "https://arxiv.org/pdf/2310.01260v1.pdf"
    },
    {
        "title": "Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback",
        "authors": [
            "Jacob Whitehill",
            "Jennifer LoCasale-Crouch"
        ],
        "published": "2023-10-02T12:11:17Z",
        "summary": "With the aim to provide teachers with more specific, frequent, and actionable\nfeedback about their teaching, we explore how Large Language Models (LLMs) can\nbe used to estimate ``Instructional Support'' domain scores of the CLassroom\nAssessment Scoring System (CLASS), a widely used observation protocol. We\ndesign a machine learning architecture that uses either zero-shot prompting of\nMeta's Llama2, and/or a classic Bag of Words (BoW) model, to classify\nindividual utterances of teachers' speech (transcribed automatically using\nOpenAI's Whisper) for the presence of Instructional Support. Then, these\nutterance-level judgments are aggregated over a 15-min observation session to\nestimate a global CLASS score. Experiments on two CLASS-coded datasets of\ntoddler and pre-kindergarten classrooms indicate that (1) automatic CLASS\nInstructional Support estimation accuracy using the proposed method (Pearson\n$R$ up to $0.48$) approaches human inter-rater reliability (up to $R=0.55$);\n(2) LLMs generally yield slightly greater accuracy than BoW for this task,\nthough the best models often combined features extracted from both LLM and BoW;\nand (3) for classifying individual utterances, there is still room for\nimprovement of automated methods compared to human-level judgments. Finally,\n(4) we illustrate how the model's outputs can be visualized at the utterance\nlevel to provide teachers with explainable feedback on which utterances were\nmost positively or negatively correlated with specific CLASS dimensions.",
        "pdf_link": "https://arxiv.org/pdf/2310.01132v3.pdf"
    },
    {
        "title": "GraphText: Graph Reasoning in Text Space",
        "authors": [
            "Jianan Zhao",
            "Le Zhuo",
            "Yikang Shen",
            "Meng Qu",
            "Kai Liu",
            "Michael Bronstein",
            "Zhaocheng Zhu",
            "Jian Tang"
        ],
        "published": "2023-10-02T11:03:57Z",
        "summary": "Large Language Models (LLMs) have gained the ability to assimilate human\nknowledge and facilitate natural language interactions with both humans and\nother LLMs. However, despite their impressive achievements, LLMs have not made\nsignificant advancements in the realm of graph machine learning. This\nlimitation arises because graphs encapsulate distinct relational data, making\nit challenging to transform them into natural language that LLMs understand. In\nthis paper, we bridge this gap with a novel framework, GraphText, that\ntranslates graphs into natural language. GraphText derives a graph-syntax tree\nfor each graph that encapsulates both the node attributes and inter-node\nrelationships. Traversal of the tree yields a graph text sequence, which is\nthen processed by an LLM to treat graph tasks as text generation tasks.\nNotably, GraphText offers multiple advantages. It introduces training-free\ngraph reasoning: even without training on graph data, GraphText with ChatGPT\ncan achieve on par with, or even surpassing, the performance of\nsupervised-trained graph neural networks through in-context learning (ICL).\nFurthermore, GraphText paves the way for interactive graph reasoning, allowing\nboth humans and LLMs to communicate with the model seamlessly using natural\nlanguage. These capabilities underscore the vast, yet-to-be-explored potential\nof LLMs in the domain of graph machine learning.",
        "pdf_link": "https://arxiv.org/pdf/2310.01089v1.pdf"
    },
    {
        "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue",
        "authors": [
            "Kentaro Mitsui",
            "Yukiya Hono",
            "Kei Sawada"
        ],
        "published": "2023-10-02T11:03:20Z",
        "summary": "The advent of large language models (LLMs) has made it possible to generate\nnatural written dialogues between two agents. However, generating human-like\nspoken dialogues from these written dialogues remains challenging. Spoken\ndialogues have several unique characteristics: they frequently include\nbackchannels and laughter, and the smoothness of turn-taking significantly\ninfluences the fluidity of conversation. This study proposes CHATS - CHatty\nAgents Text-to-Speech - a discrete token-based system designed to generate\nspoken dialogues based on written dialogues. Our system can generate speech for\nboth the speaker side and the listener side simultaneously, using only the\ntranscription from the speaker side, which eliminates the need for\ntranscriptions of backchannels or laughter. Moreover, CHATS facilitates natural\nturn-taking; it determines the appropriate duration of silence after each\nutterance in the absence of overlap, and it initiates the generation of\noverlapping speech based on the phoneme sequence of the next utterance in case\nof overlap. Experimental evaluations indicate that CHATS outperforms the\ntext-to-speech baseline, producing spoken dialogues that are more interactive\nand fluid while retaining clarity and intelligibility.",
        "pdf_link": "https://arxiv.org/pdf/2310.01088v1.pdf"
    },
    {
        "title": "Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models",
        "authors": [
            "Chenhan Yuan",
            "Qianqian Xie",
            "Jimin Huang",
            "Sophia Ananiadou"
        ],
        "published": "2023-10-02T10:35:23Z",
        "summary": "Temporal reasoning is a crucial NLP task, providing a nuanced understanding\nof time-sensitive contexts within textual data. Although recent advancements in\nLLMs have demonstrated their potential in temporal reasoning, the predominant\nfocus has been on tasks such as temporal expression and temporal relation\nextraction. These tasks are primarily designed for the extraction of direct and\npast temporal cues and to engage in simple reasoning processes. A significant\ngap remains when considering complex reasoning tasks such as event forecasting,\nwhich requires multi-step temporal reasoning on events and prediction on the\nfuture timestamp. Another notable limitation of existing methods is their\nincapability to provide an illustration of their reasoning process, hindering\nexplainability. In this paper, we introduce the first task of explainable\ntemporal reasoning, to predict an event's occurrence at a future timestamp\nbased on context which requires multiple reasoning over multiple events, and\nsubsequently provide a clear explanation for their prediction. Our task offers\na comprehensive evaluation of both the LLMs' complex temporal reasoning\nability, the future event prediction ability, and explainability-a critical\nattribute for AI applications. To support this task, we present the first\nmulti-source instruction-tuning dataset of explainable temporal reasoning\n(ExpTime) with 26k derived from the temporal knowledge graph datasets and their\ntemporal reasoning paths, using a novel knowledge-graph-instructed-generation\nstrategy. Based on the dataset, we propose the first open-source LLM series\nTimeLlaMA based on the foundation LlaMA2, with the ability of instruction\nfollowing for explainable temporal reasoning. We compare the performance of our\nmethod and a variety of LLMs, where our method achieves the state-of-the-art\nperformance of temporal prediction and explanation.",
        "pdf_link": "https://arxiv.org/pdf/2310.01074v2.pdf"
    },
    {
        "title": "Resolving Knowledge Conflicts in Large Language Models",
        "authors": [
            "Yike Wang",
            "Shangbin Feng",
            "Heng Wang",
            "Weijia Shi",
            "Vidhisha Balachandran",
            "Tianxing He",
            "Yulia Tsvetkov"
        ],
        "published": "2023-10-02T06:57:45Z",
        "summary": "Large language models (LLMs) often encounter knowledge conflicts, scenarios\nwhere discrepancy arises between the internal parametric knowledge of LLMs and\nnon-parametric information provided in the prompt context. In this work we ask\nwhat are the desiderata for LLMs when a knowledge conflict arises and whether\nexisting LLMs fulfill them. We posit that LLMs should 1) identify knowledge\nconflicts, 2) pinpoint conflicting information segments, and 3) provide\ndistinct answers or viewpoints in conflicting scenarios. To this end, we\nintroduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual\nknowledge conflicts and quantitatively evaluating to what extent LLMs achieve\nthese goals. KNOWLEDGE CONFLICT includes diverse and complex situations of\nknowledge conflict, knowledge from diverse entities and domains, two synthetic\nconflict creation methods, and settings with progressively increasing\ndifficulty to reflect realistic knowledge conflicts. Extensive experiments with\nthe KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in\nidentifying the existence of knowledge conflicts, they struggle to determine\nthe specific conflicting knowledge and produce a response with distinct answers\namidst conflicting information. To address these challenges, we propose new\ninstruction-based approaches that augment LLMs to better achieve the three\ngoals. Further analysis shows that abilities to tackle knowledge conflicts are\ngreatly impacted by factors such as knowledge domain and prompt text, while\ngenerating robust responses to knowledge conflict scenarios remains an open\nresearch question.",
        "pdf_link": "https://arxiv.org/pdf/2310.00935v1.pdf"
    },
    {
        "title": "All Languages Matter: On the Multilingual Safety of Large Language Models",
        "authors": [
            "Wenxuan Wang",
            "Zhaopeng Tu",
            "Chang Chen",
            "Youliang Yuan",
            "Jen-tse Huang",
            "Wenxiang Jiao",
            "Michael R. Lyu"
        ],
        "published": "2023-10-02T05:23:34Z",
        "summary": "Safety lies at the core of developing and deploying large language models\n(LLMs). However, previous safety benchmarks only concern the safety in one\nlanguage, e.g. the majority language in the pretraining data such as English.\nIn this work, we build the first multilingual safety benchmark for LLMs,\nXSafety, in response to the global deployment of LLMs in practice. XSafety\ncovers 14 kinds of commonly used safety issues across 10 languages that span\nseveral language families. We utilize XSafety to empirically study the\nmultilingual safety for 4 widely-used LLMs, including both close-API and\nopen-source models. Experimental results show that all LLMs produce\nsignificantly more unsafe responses for non-English queries than English ones,\nindicating the necessity of developing safety alignment for non-English\nlanguages. In addition, we propose several simple and effective prompting\nmethods to improve the multilingual safety of ChatGPT by evoking safety\nknowledge and improving cross-lingual generalization of safety alignment. Our\nprompting method can significantly reduce the ratio of unsafe responses from\n19.1% to 9.7% for non-English queries. We release our data at\nhttps://github.com/Jarviswang94/Multilingual_safety_benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2310.00905v1.pdf"
    },
    {
        "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models",
        "authors": [
            "Yongchan Kwon",
            "Eric Wu",
            "Kevin Wu",
            "James Zou"
        ],
        "published": "2023-10-02T04:59:19Z",
        "summary": "Quantifying the impact of training data points is crucial for understanding\nthe outputs of machine learning models and for improving the transparency of\nthe AI pipeline. The influence function is a principled and popular data\nattribution method, but its computational cost often makes it challenging to\nuse. This issue becomes more pronounced in the setting of large language models\nand text-to-image models. In this work, we propose DataInf, an efficient\ninfluence approximation method that is practical for large-scale generative AI\nmodels. Leveraging an easy-to-compute closed-form expression, DataInf\noutperforms existing influence computation algorithms in terms of computational\nand memory efficiency. Our theoretical analysis shows that DataInf is\nparticularly well-suited for parameter-efficient fine-tuning techniques such as\nLoRA. Through systematic empirical evaluations, we show that DataInf accurately\napproximates influence scores and is orders of magnitude faster than existing\nmethods. In applications to RoBERTa-large, Llama-2-13B-chat, and\nstable-diffusion-v1.5 models, DataInf effectively identifies the most\ninfluential fine-tuning examples better than other approximate influence\nscores. Moreover, it can help to identify which data points are mislabeled.",
        "pdf_link": "https://arxiv.org/pdf/2310.00902v3.pdf"
    },
    {
        "title": "Enabling Language Models to Implicitly Learn Self-Improvement",
        "authors": [
            "Ziqi Wang",
            "Le Hou",
            "Tianjian Lu",
            "Yuexin Wu",
            "Yunxuan Li",
            "Hongkun Yu",
            "Heng Ji"
        ],
        "published": "2023-10-02T04:29:40Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nopen-ended text generation tasks. However, the inherent open-ended nature of\nthese tasks implies that there is always room for improvement in the quality of\nmodel responses. To address this challenge, various approaches have been\nproposed to enhance the performance of LLMs. There has been a growing focus on\nenabling LLMs to self-improve their response quality, thereby reducing the\nreliance on extensive human annotation efforts for collecting diverse and\nhigh-quality training data. Recently, prompting-based methods have been widely\nexplored among self-improvement methods owing to their effectiveness,\nefficiency, and convenience. However, those methods usually require explicitly\nand thoroughly written rubrics as inputs to LLMs. It is expensive and\nchallenging to manually derive and provide all necessary rubrics with a\nreal-world complex goal for improvement (e.g., being more helpful and less\nharmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework\nthat implicitly learns the improvement goal from human preference data. PIT\nonly requires preference data that are used to train reward models without\nextra human efforts. Specifically, we reformulate the training objective of\nreinforcement learning from human feedback (RLHF) -- instead of maximizing\nresponse quality for a given input, we maximize the quality gap of the response\nconditioned on a reference response. In this way, PIT is implicitly trained\nwith the improvement goal of better aligning with human preferences.\nExperiments on two real-world datasets and one synthetic dataset show that our\nmethod significantly outperforms prompting-based methods.",
        "pdf_link": "https://arxiv.org/pdf/2310.00898v3.pdf"
    },
    {
        "title": "Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications",
        "authors": [
            "Duc N. M Hoang",
            "Minsik Cho",
            "Thomas Merth",
            "Mohammad Rastegari",
            "Zhangyang Wang"
        ],
        "published": "2023-10-02T03:12:06Z",
        "summary": "Compressing Large Language Models (LLMs) often leads to reduced performance,\nespecially for knowledge-intensive tasks. In this work, we dive into how\ncompression damages LLMs' inherent knowledge and the possible remedies. We\nstart by proposing two conjectures on the nature of the damage: one is certain\nknowledge being forgotten (or erased) after LLM compression, hence\nnecessitating the compressed model to (re)learn from data with additional\nparameters; the other presumes that knowledge is internally displaced and hence\none requires merely \"inference re-direction\" with input-side augmentation such\nas prompting, to recover the knowledge-related performance. Extensive\nexperiments are then designed to (in)validate the two conjectures. We observe\nthe promise of prompting in comparison to model tuning; we further unlock\nprompting's potential by introducing a variant called Inference-time Dynamic\nPrompting (IDP), that can effectively increase prompt diversity without\nincurring any inference overhead. Our experiments consistently suggest that\ncompared to the classical re-training alternatives such as LoRA, prompting with\nIDP leads to better or comparable post-compression performance recovery, while\nsaving the extra parameter size by 21x and reducing inference latency by 60%.\nOur experiments hence strongly endorse the conjecture of \"knowledge displaced\"\nover \"knowledge forgotten\", and shed light on a new efficient mechanism to\nrestore compressed LLM performance. We additionally visualize and analyze the\ndifferent attention and activation patterns between prompted and re-trained\nmodels, demonstrating they achieve performance recovery in two different\nregimes.",
        "pdf_link": "https://arxiv.org/pdf/2310.00867v3.pdf"
    },
    {
        "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
        "authors": [
            "Man Luo",
            "Shrinidhi Kumbhar",
            "Ming shen",
            "Mihir Parmar",
            "Neeraj Varshney",
            "Pratyay Banerjee",
            "Somak Aditya",
            "Chitta Baral"
        ],
        "published": "2023-10-02T01:00:50Z",
        "summary": "Logical reasoning is fundamental for humans yet presents a substantial\nchallenge in the domain of Artificial Intelligence. Initially, researchers used\nKnowledge Representation and Reasoning (KR) systems that did not scale and\nrequired non-trivial manual effort. Recently, the emergence of large language\nmodels (LLMs) has demonstrated the ability to overcome various limitations of\nformal Knowledge Representation (KR) systems. Consequently, there's a growing\ninterest in using LLMs for logical reasoning via natural language. This work\nstrives to understand the proficiency of LLMs in logical reasoning by offering\na brief review of the latest progress in this area; with a focus on the logical\nreasoning datasets, tasks, and the methods adopted to utilize LLMs for\nreasoning. To offer a thorough analysis, we have compiled a benchmark titled\nLogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,\nand inductive reasoning. Utilizing LogiGLUE as a foundation, we have trained an\ninstruction fine-tuned language model, resulting in LogiT5. We study\nsingle-task training, multi-task training, and \"chain-of-thought\" knowledge\ndistillation fine-tuning technique to assess the performance of model across\nthe different logical reasoning categories. We also assess various LLMs using\nLogiGLUE, and the findings indicate that LLMs excel most in abductive\nreasoning, followed by deductive reasoning, while they are least effective at\ninductive reasoning. We aim to shed light on the capabilities and potential\npathways for enhancing logical reasoning proficiency in LLMs, paving the way\nfor more advanced and nuanced developments in this critical field.",
        "pdf_link": "https://arxiv.org/pdf/2310.00836v3.pdf"
    },
    {
        "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
        "authors": [
            "Yapei Chang",
            "Kyle Lo",
            "Tanya Goyal",
            "Mohit Iyyer"
        ],
        "published": "2023-10-01T20:46:44Z",
        "summary": "Summarizing book-length documents (>100K tokens) that exceed the context\nwindow size of large language models (LLMs) requires first breaking the input\ndocument into smaller chunks and then prompting an LLM to merge, update, and\ncompress chunk-level summaries. Despite the complexity and importance of this\ntask, it has yet to be meaningfully studied due to the challenges of\nevaluation: existing book-length summarization datasets (e.g., BookSum) are in\nthe pretraining data of most public LLMs, and existing evaluation methods\nstruggle to capture errors made by modern LLM summarizers. In this paper, we\npresent the first study of the coherence of LLM-based book-length summarizers\nimplemented via two prompting workflows: (1) hierarchically merging chunk-level\nsummaries, and (2) incrementally updating a running summary. We obtain 1193\nfine-grained human annotations on GPT-4 generated summaries of 100\nrecently-published books and identify eight common types of coherence errors\nmade by LLMs. Because human evaluation is expensive and time-consuming, we\ndevelop an automatic metric, BooookScore, that measures the proportion of\nsentences in a summary that do not contain any of the identified error types.\nBooookScore has high agreement with human annotations and allows us to\nsystematically evaluate the impact of many other critical parameters (e.g.,\nchunk size, base LLM) while saving $15K USD and 500 hours in human evaluation\ncosts. We find that closed-source LLMs such as GPT-4 and Claude 2 produce\nsummaries with higher BooookScore than those generated by open-source models.\nWhile LLaMA 2 falls behind other models, Mixtral achieves performance on par\nwith GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher\nlevel of detail than hierarchical merging, a trade-off sometimes preferred by\nannotators.",
        "pdf_link": "https://arxiv.org/pdf/2310.00785v3.pdf"
    },
    {
        "title": "FELM: Benchmarking Factuality Evaluation of Large Language Models",
        "authors": [
            "Shiqi Chen",
            "Yiran Zhao",
            "Jinghan Zhang",
            "I-Chun Chern",
            "Siyang Gao",
            "Pengfei Liu",
            "Junxian He"
        ],
        "published": "2023-10-01T17:37:31Z",
        "summary": "Assessing factuality of text generated by large language models (LLMs) is an\nemerging yet crucial research area, aimed at alerting users to potential errors\nand guiding the development of more reliable LLMs. Nonetheless, the evaluators\nassessing factuality necessitate suitable evaluation themselves to gauge\nprogress and foster advancements. This direction remains under-explored,\nresulting in substantial impediments to the progress of factuality evaluators.\nTo mitigate this issue, we introduce a benchmark for Factuality Evaluation of\nlarge Language Models, referred to as felm. In this benchmark, we collect\nresponses generated from LLMs and annotate factuality labels in a fine-grained\nmanner. Contrary to previous studies that primarily concentrate on the\nfactuality of world knowledge (e.g.~information from Wikipedia), felm focuses\non factuality across diverse domains, spanning from world knowledge to math and\nreasoning. Our annotation is based on text segments, which can help pinpoint\nspecific factual errors. The factuality annotations are further supplemented by\npredefined error types and reference links that either support or contradict\nthe statement. In our experiments, we investigate the performance of several\nLLM-based factuality evaluators on felm, including both vanilla LLMs and those\naugmented with retrieval mechanisms and chain-of-thought processes. Our\nfindings reveal that while retrieval aids factuality evaluation, current LLMs\nare far from satisfactory to faithfully detect factual errors.",
        "pdf_link": "https://arxiv.org/pdf/2310.00741v2.pdf"
    },
    {
        "title": "GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models",
        "authors": [
            "Emilio Ferrara"
        ],
        "published": "2023-10-01T17:25:56Z",
        "summary": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\nare marvels of technology; celebrated for their prowess in natural language\nprocessing and multimodal content generation, they promise a transformative\nfuture. But as with all powerful tools, they come with their shadows. Picture\nliving in a world where deepfakes are indistinguishable from reality, where\nsynthetic identities orchestrate malicious campaigns, and where targeted\nmisinformation or scams are crafted with unparalleled precision. Welcome to the\ndarker side of GenAI applications. This article is not just a journey through\nthe meanders of potential misuse of GenAI and LLMs, but also a call to\nrecognize the urgency of the challenges ahead. As we navigate the seas of\nmisinformation campaigns, malicious content generation, and the eerie creation\nof sophisticated malware, we'll uncover the societal implications that ripple\nthrough the GenAI revolution we are witnessing. From AI-powered botnets on\nsocial media platforms to the unnerving potential of AI to generate fabricated\nidentities, or alibis made of synthetic realities, the stakes have never been\nhigher. The lines between the virtual and the real worlds are blurring, and the\nconsequences of potential GenAI's nefarious applications impact us all. This\narticle serves both as a synthesis of rigorous research presented on the risks\nof GenAI and misuse of LLMs and as a thought-provoking vision of the different\ntypes of harmful GenAI applications we might encounter in the near future, and\nsome ways we can prepare for them.",
        "pdf_link": "https://arxiv.org/pdf/2310.00737v3.pdf"
    },
    {
        "title": "LEGO-Prover: Neural Theorem Proving with Growing Libraries",
        "authors": [
            "Haiming Wang",
            "Huajian Xin",
            "Chuanyang Zheng",
            "Lin Li",
            "Zhengying Liu",
            "Qingxing Cao",
            "Yinya Huang",
            "Jing Xiong",
            "Han Shi",
            "Enze Xie",
            "Jian Yin",
            "Zhenguo Li",
            "Heng Liao",
            "Xiaodan Liang"
        ],
        "published": "2023-10-01T12:47:59Z",
        "summary": "Despite the success of large language models (LLMs), the task of theorem\nproving still remains one of the hardest reasoning tasks that is far from being\nfully solved. Prior methods using language models have demonstrated promising\nresults, but they still struggle to prove even middle school level theorems.\nOne common limitation of these methods is that they assume a fixed theorem\nlibrary during the whole theorem proving process. However, as we all know,\ncreating new useful theorems or even new theories is not only helpful but\ncrucial and necessary for advancing mathematics and proving harder and deeper\nresults. In this work, we present LEGO-Prover, which employs a growing skill\nlibrary containing verified lemmas as skills to augment the capability of LLMs\nused in theorem proving. By constructing the proof modularly, LEGO-Prover\nenables LLMs to utilize existing skills retrieved from the library and to\ncreate new skills during the proving process. These skills are further evolved\n(by prompting an LLM) to enrich the library on another scale. Modular and\nreusable skills are constantly added to the library to enable tackling\nincreasingly intricate mathematical problems. Moreover, the learned library\nfurther bridges the gap between human proofs and formal proofs by making it\neasier to impute missing steps. LEGO-Prover advances the state-of-the-art pass\nrate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).\nDuring the proving process, LEGO-Prover also manages to generate over 20,000\nskills (theorems/lemmas) and adds them to the growing library. Our ablation\nstudy indicates that these newly added skills are indeed helpful for proving\ntheorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We\nalso release our code and all the generated skills.",
        "pdf_link": "https://arxiv.org/pdf/2310.00656v3.pdf"
    },
    {
        "title": "Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning",
        "authors": [
            "Mustafa Shukor",
            "Alexandre Rame",
            "Corentin Dancette",
            "Matthieu Cord"
        ],
        "published": "2023-10-01T12:02:59Z",
        "summary": "Following the success of Large Language Models (LLMs), Large Multimodal\nModels (LMMs), such as the Flamingo model and its subsequent competitors, have\nstarted to emerge as natural steps towards generalist agents. However,\ninteracting with recent LMMs reveals major limitations that are hardly captured\nby the current evaluation benchmarks. Indeed, task performances (e.g., VQA\naccuracy) alone do not provide enough clues to understand their real\ncapabilities, limitations, and to which extent such models are aligned to human\nexpectations. To refine our understanding of those flaws, we deviate from the\ncurrent evaluation paradigm, and (1) evaluate 10 recent open-source LMMs from\n3B up to 80B parameter scale, on 5 different axes; hallucinations, abstention,\ncompositionality, explainability and instruction following. Our evaluation on\nthese axes reveals major flaws in LMMs. While the current go-to solution to\nalign these models is based on training, such as instruction tuning or RLHF, we\nrather (2) explore the training-free in-context learning (ICL) as a solution,\nand study how it affects these limitations. Based on our ICL study, (3) we push\nICL further and propose new multimodal ICL variants such as; Multitask-ICL,\nChain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows.\n(1) Despite their success, LMMs have flaws that remain unsolved with scaling\nalone. (2) The effect of ICL on LMMs flaws is nuanced; despite its\neffectiveness for improved explainability, answer abstention, ICL only slightly\nimproves instruction following, does not improve compositional abilities, and\nactually even amplifies hallucinations. (3) The proposed ICL variants are\npromising as post-hoc approaches to efficiently tackle some of those flaws. The\ncode is available here: https://github.com/mshukor/EvALign-ICL.",
        "pdf_link": "https://arxiv.org/pdf/2310.00647v2.pdf"
    },
    {
        "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
        "authors": [
            "Yair Gat",
            "Nitay Calderon",
            "Amir Feder",
            "Alexander Chapanin",
            "Amit Sharma",
            "Roi Reichart"
        ],
        "published": "2023-10-01T07:31:04Z",
        "summary": "Causal explanations of the predictions of NLP systems are essential to ensure\nsafety and establish trust. Yet, existing methods often fall short of\nexplaining model predictions effectively or efficiently and are often\nmodel-specific. In this paper, we address model-agnostic explanations,\nproposing two approaches for counterfactual (CF) approximation. The first\napproach is CF generation, where a large language model (LLM) is prompted to\nchange a specific text concept while keeping confounding concepts unchanged.\nWhile this approach is demonstrated to be very effective, applying LLM at\ninference-time is costly. We hence present a second approach based on matching,\nand propose a method that is guided by an LLM at training-time and learns a\ndedicated embedding space. This space is faithful to a given causal graph and\neffectively serves to identify matches that approximate CFs. After showing\ntheoretically that approximating CFs is required in order to construct faithful\nexplanations, we benchmark our approaches and explain several models, including\nLLMs with billions of parameters. Our empirical results demonstrate the\nexcellent performance of CF generation models as model-agnostic explainers.\nMoreover, our matching approach, which requires far less test-time resources,\nalso provides effective explanations, surpassing many baselines. We also find\nthat Top-K techniques universally improve every tested method. Finally, we\nshowcase the potential of LLMs in constructing new benchmarks for model\nexplanation and subsequently validate our conclusions. Our work illuminates new\npathways for efficient and accurate approaches to interpreting NLP systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.00603v2.pdf"
    },
    {
        "title": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length",
        "authors": [
            "Hongye Jin",
            "Xiaotian Han",
            "Jingfeng Yang",
            "Zhimeng Jiang",
            "Chia-Yuan Chang",
            "Xia Hu"
        ],
        "published": "2023-10-01T05:25:24Z",
        "summary": "The evolving sophistication and intricacies of Large Language Models (LLMs)\nyield unprecedented advancements, yet they simultaneously demand considerable\ncomputational resources and incur significant costs. To alleviate these\nchallenges, this paper introduces a novel, simple, and effective method named\n``\\growlength'' to accelerate the pretraining process of LLMs. Our method\nprogressively increases the training length throughout the pretraining phase,\nthereby mitigating computational costs and enhancing efficiency. For instance,\nit begins with a sequence length of 128 and progressively extends to 4096. This\napproach enables models to process a larger number of tokens within limited\ntime frames, potentially boosting their performance. In other words, the\nefficiency gain is derived from training with shorter sequences optimizing the\nutilization of resources. Our extensive experiments with various\nstate-of-the-art LLMs have revealed that models trained using our method not\nonly converge more swiftly but also exhibit superior performance metrics\ncompared to those trained with existing methods. Furthermore, our method for\nLLMs pretraining acceleration does not require any additional engineering\nefforts, making it a practical solution in the realm of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.00576v1.pdf"
    },
    {
        "title": "Measuring Value Understanding in Language Models through Discriminator-Critique Gap",
        "authors": [
            "Zhaowei Zhang",
            "Fengshuo Bai",
            "Jun Gao",
            "Yaodong Yang"
        ],
        "published": "2023-09-30T13:47:55Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have heightened concerns\nabout their potential misalignment with human values. However, evaluating their\ngrasp of these values is complex due to their intricate and adaptable nature.\nWe argue that truly understanding values in LLMs requires considering both\n\"know what\" and \"know why\". To this end, we present the Value Understanding\nMeasurement (VUM) framework that quantitatively assesses both \"know what\" and\n\"know why\" by measuring the discriminator-critique gap related to human values.\nUsing the Schwartz Value Survey, we specify our evaluation values and develop a\nthousand-level dialogue dataset with GPT-4. Our assessment looks at both the\nvalue alignment of LLM's outputs compared to baseline answers and how LLM\nresponses align with reasons for value recognition versus GPT-4's annotations.\nWe evaluate five representative LLMs and provide strong evidence that the\nscaling law significantly impacts \"know what\" but not much on \"know why\", which\nhas consistently maintained a high level. This may further suggest that LLMs\nmight craft plausible explanations based on the provided context without truly\nunderstanding their inherent value, indicating potential risks.",
        "pdf_link": "https://arxiv.org/pdf/2310.00378v3.pdf"
    },
    {
        "title": "Understanding In-Context Learning from Repetitions",
        "authors": [
            "Jianhao Yan",
            "Jin Xu",
            "Chiyu Song",
            "Chenming Wu",
            "Yafu Li",
            "Yue Zhang"
        ],
        "published": "2023-09-30T08:13:49Z",
        "summary": "This paper explores the elusive mechanism underpinning in-context learning in\nLarge Language Models (LLMs). Our work provides a novel perspective by\nexamining in-context learning via the lens of surface repetitions. We\nquantitatively investigate the role of surface features in text generation, and\nempirically establish the existence of \\emph{token co-occurrence\nreinforcement}, a principle that strengthens the relationship between two\ntokens based on their contextual co-occurrences. By investigating the dual\nimpacts of these features, our research illuminates the internal workings of\nin-context learning and expounds on the reasons for its failures. This paper\nprovides an essential contribution to the understanding of in-context learning\nand its potential limitations, providing a fresh perspective on this exciting\ncapability.",
        "pdf_link": "https://arxiv.org/pdf/2310.00297v3.pdf"
    },
    {
        "title": "Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting",
        "authors": [
            "Baphumelele Masikisiki",
            "Vukosi Marivate",
            "Yvette Hlope"
        ],
        "published": "2023-09-30T06:25:27Z",
        "summary": "Large Language Models, such as Generative Pre-trained Transformer 3 (aka.\nGPT-3), have been developed to understand language through the analysis of\nextensive text data, allowing them to identify patterns and connections between\nwords. While LLMs have demonstrated impressive performance across various\ntext-related tasks, they encounter challenges in tasks associated with\nreasoning. To address this challenge, Chain of Thought(CoT) prompting method\nhas been proposed as a means to enhance LLMs' proficiency in complex reasoning\ntasks like solving math word problems and answering questions based on logical\nargumentative reasoning. The primary aim of this research is to assess how well\nfour language models can grade reflective essays of third-year medical\nstudents. The assessment will specifically target the evaluation of critical\nthinking skills using CoT prompting.\n  The research will provide the following contributions; to introduce and\neducate on the process of instructing models to evaluate reflective essays from\na dataset they have not been previously trained on; to illustrate the use of\nCoT prompting as an instructional approach for training large models to carry\nout particular tasks. Our results suggest that among all the models, Llama-7b\nperforms the least effectively, displaying the highest mean squared error.\nConversely, ChatGPT emerges as the superior model, boasting a higher Cohen\nkappa score value of 0.53. Lastly, it's important to note that the selected\nmodels do prioritise user privacy by allowing users to delete their own\nconducted conversations.",
        "pdf_link": "https://arxiv.org/pdf/2310.00272v1.pdf"
    },
    {
        "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
        "authors": [
            "Zouying Cao",
            "Yifei Yang",
            "Hai Zhao"
        ],
        "published": "2023-09-30T05:20:02Z",
        "summary": "While Large language models (LLMs) have garnered widespread applications\nacross various domains due to their powerful language understanding and\ngeneration capabilities, the detection of non-factual or hallucinatory content\ngenerated by LLMs remains scarce. Currently, one significant challenge in\nhallucination detection is the laborious task of time-consuming and expensive\nmanual annotation of the hallucinatory generation. To address this issue, this\npaper first introduces a method for automatically constructing model-specific\nhallucination datasets based on existing fact-checking datasets called\nAutoHall. Furthermore, we propose a zero-resource and black-box hallucination\ndetection method based on self-contradiction. We conduct experiments towards\nprevalent open-/closed-source LLMs, achieving superior hallucination detection\nperformance compared to extant baselines. Moreover, our experiments reveal\nvariations in hallucination proportions and types among different models.",
        "pdf_link": "https://arxiv.org/pdf/2310.00259v1.pdf"
    },
    {
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
        "authors": [
            "Tianhao Wu",
            "Banghua Zhu",
            "Ruoyu Zhang",
            "Zhaojin Wen",
            "Kannan Ramchandran",
            "Jiantao Jiao"
        ],
        "published": "2023-09-30T01:23:22Z",
        "summary": "Large Language Models (LLMs) can acquire extensive world knowledge through\npre-training on large corpora. However, due to exposure to low-quality data,\nLLMs may exhibit harmful behavior without aligning with human values. The\ndominant approach for steering LLMs towards beneficial behavior involves\nReinforcement Learning with Human Feedback (RLHF), with Proximal Policy\nOptimization (PPO) serving as the default RL optimizer. Despite its\neffectiveness, PPO has limitations when optimizing rewards trained from\ncomparison-based loss. Primarily, PPO is not invariant to equivalent reward\nfunctions containing identical preference information due to the need to\ncalibrate the reward scale. Additionally, PPO's necessity for token-wise\nupdates introduces complexity in both function approximation and algorithm\ndesign compared to trajectory-wise optimization. This paper proposes a new\nframework, reinforcement learning with relative feedback, and a novel\ntrajectory-wise policy gradient algorithm, Pairwise Proximal Policy\nOptimization (P3O) that operates directly on comparative rewards. We show\ntheoretically that P3O is invariant to equivalent rewards and avoids the\ncomplexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO\nin the KL-Reward trade-off and can align with human preferences as well as or\nbetter than prior methods. In summary, this work introduces a simpler yet\neffective approach for aligning LLMs to human preferences through relative\nfeedback.",
        "pdf_link": "https://arxiv.org/pdf/2310.00212v3.pdf"
    },
    {
        "title": "Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs \"Difficult\" Downstream Tasks in LLMs",
        "authors": [
            "Lu Yin",
            "Ajay Jaiswal",
            "Shiwei Liu",
            "Souvik Kundu",
            "Zhangyang Wang"
        ],
        "published": "2023-09-29T22:55:06Z",
        "summary": "We present Junk DNA Hypothesis by adopting a novel task-centric angle for the\npre-trained weights of large language models (LLMs). It has been believed that\nweights in LLMs contain significant redundancy, leading to the conception that\na considerable chunk of the parameters can be removed by pruning without\ncompromising performance. Contrary to this belief, this paper presents a\ncounter-argument: small-magnitude weights of pre-trained model weights encode\nvital knowledge essential for tackling difficult downstream tasks - manifested\nas the monotonic relationship between the performance drop of downstream tasks\nacross the difficulty spectrum, as we prune more pre-trained weights by\nmagnitude. Moreover, we reveal that these seemingly inconsequential weights can\nresult in irreparable loss of knowledge and performance degradation in\ndifficult tasks, even when downstream continual training is allowed.\nInterestingly, our evaluations show that the other popular compression, namely\nquantization, fails to exhibit similar monotonic effect and does not as\nconvincingly disentangle this task-difficulty information. To study formally,\nwe introduce several quantifiable metrics to gauge the downstream task\ndifficulty: (1) within the same task category, and (2) across different task\ncategories. Our extensive experiments substantiate the Junk DNA Hypothesis\nacross a diverse range of model sizes, tasks, datasets, and even pruning\nmethods. Codes are available at:\nhttps://github.com/VITA-Group/Junk_DNA_Hypothesis.git.",
        "pdf_link": "https://arxiv.org/pdf/2310.02277v2.pdf"
    },
    {
        "title": "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation",
        "authors": [
            "Hangfeng He",
            "Hongming Zhang",
            "Dan Roth"
        ],
        "published": "2023-09-29T18:25:46Z",
        "summary": "To comprehensively assess the capacity of current models for complex\nreasoning, it is crucial to assess their step-by-step reasoning in a scalable\nmanner. Established reference-based evaluation metrics rely on human-annotated\nreasoning chains to assess the model-derived chains. However, such\n``gold-standard'' human-written reasoning chains may not be unique and their\nacquisition is often labor-intensive. Existing reference-free reasoning metrics\neliminate the need for human-crafted reasoning chains as references, but they\ntypically require fine-tuning on datasets with human-derived reasoning chains,\nwhich complicates the process and raises concerns regarding generalizability\nacross diverse datasets. To address these challenges, we harness GPT-4 to\nautomatically evaluate reasoning chain quality, obviating the need for\nhuman-crafted references. Leveraging the Socratic method, we devise tailored\nprompts to enhance reference-free reasoning evaluation, which we term SocREval\n(Socratic method for Reasoning Evaluation). Empirical results from four human\nannotated datasets reveal that SocREval significantly improves GPT-4's\nperformance, surpassing existing reference-free and reference-based reasoning\nevaluation metrics. Beyond its demonstrated efficacy, our proposed framework,\nlarge language models (LLMs) with the Socratic method, proves to be both\ncost-efficient and robust to prompt writing and example selection, as\nsubstantiated by our in-depth analysis.",
        "pdf_link": "https://arxiv.org/pdf/2310.00074v1.pdf"
    },
    {
        "title": "Efficient Streaming Language Models with Attention Sinks",
        "authors": [
            "Guangxuan Xiao",
            "Yuandong Tian",
            "Beidi Chen",
            "Song Han",
            "Mike Lewis"
        ],
        "published": "2023-09-29T17:59:56Z",
        "summary": "Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na \"sink\" even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
        "pdf_link": "https://arxiv.org/pdf/2309.17453v4.pdf"
    },
    {
        "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
        "authors": [
            "Ansong Ni",
            "Pengcheng Yin",
            "Yilun Zhao",
            "Martin Riddell",
            "Troy Feng",
            "Rui Shen",
            "Stephen Yin",
            "Ye Liu",
            "Semih Yavuz",
            "Caiming Xiong",
            "Shafiq Joty",
            "Yingbo Zhou",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "published": "2023-09-29T17:57:00Z",
        "summary": "Recently, large language models (LLMs), especially those that are pretrained\non code, have demonstrated strong capabilities in generating programs from\nnatural language inputs in a few-shot or even zero-shot manner. Despite\npromising results, there is a notable lack of a comprehensive evaluation of\nthese models language-to-code generation capabilities. Existing studies often\nfocus on specific tasks, model architectures, or learning paradigms, leading to\na fragmented understanding of the overall landscape. In this work, we present\nL2CEval, a systematic evaluation of the language-to-code generation\ncapabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,\nmath reasoning and Python programming, analyzing the factors that potentially\naffect their performance, such as model size, pretraining data, instruction\ntuning, and different prompting methods. In addition to assessing model\nperformance, we measure confidence calibration for the models and conduct human\nevaluations of the output programs. This enables us to identify and analyze the\ntypical failure modes across various tasks and models. L2CEval offers a\ncomprehensive understanding of the capabilities and limitations of LLMs in\nlanguage-to-code generation. We also release the evaluation framework and all\nmodel outputs, hoping to lay the groundwork for further future research in this\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2309.17446v2.pdf"
    },
    {
        "title": "LLM-grounded Video Diffusion Models",
        "authors": [
            "Long Lian",
            "Baifeng Shi",
            "Adam Yala",
            "Trevor Darrell",
            "Boyi Li"
        ],
        "published": "2023-09-29T17:54:46Z",
        "summary": "Text-conditioned diffusion models have emerged as a promising tool for neural\nvideo generation. However, current models still struggle with intricate\nspatiotemporal prompts and often generate restricted or incorrect motion (e.g.,\neven lacking the ability to be prompted for objects moving from left to right).\nTo address these limitations, we introduce LLM-grounded Video Diffusion (LVD).\nInstead of directly generating videos from the text inputs, LVD first leverages\na large language model (LLM) to generate dynamic scene layouts based on the\ntext inputs and subsequently uses the generated layouts to guide a diffusion\nmodel for video generation. We show that LLMs are able to understand complex\nspatiotemporal dynamics from text alone and generate layouts that align closely\nwith both the prompts and the object motion patterns typically observed in the\nreal world. We then propose to guide video diffusion models with these layouts\nby adjusting the attention maps. Our approach is training-free and can be\nintegrated into any video diffusion model that admits classifier guidance. Our\nresults demonstrate that LVD significantly outperforms its base video diffusion\nmodel and several strong baseline methods in faithfully generating videos with\nthe desired attributes and motion patterns.",
        "pdf_link": "https://arxiv.org/pdf/2309.17444v2.pdf"
    },
    {
        "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
        "authors": [
            "Vaidehi Patil",
            "Peter Hase",
            "Mohit Bansal"
        ],
        "published": "2023-09-29T17:12:43Z",
        "summary": "Pretrained language models sometimes possess knowledge that we do not wish\nthem to, including memorized personal information and knowledge that could be\nused to harm people. They can also output toxic or harmful text. To mitigate\nthese safety and informational issues, we propose an attack-and-defense\nframework for studying the task of deleting sensitive information directly from\nmodel weights. We study direct edits to model weights because (1) this approach\nshould guarantee that particular deleted information is never extracted by\nfuture prompt attacks, and (2) it should protect against whitebox attacks,\nwhich is necessary for making claims about safety/privacy in a setting where\npublicly available model weights could be used to elicit sensitive information.\nOur threat model assumes that an attack succeeds if the answer to a sensitive\nquestion is located among a set of B generated candidates, based on scenarios\nwhere the information would be insecure if the answer is among B candidates.\nExperimentally, we show that even state-of-the-art model editing methods such\nas ROME struggle to truly delete factual information from models like GPT-J, as\nour whitebox and blackbox attacks can recover \"deleted\" information from an\nedited model 38% of the time. These attacks leverage two key observations: (1)\nthat traces of deleted information can be found in intermediate model hidden\nstates, and (2) that applying an editing method for one question may not delete\ninformation across rephrased versions of the question. Finally, we provide new\ndefense methods that protect against some extraction attacks, but we do not\nfind a single universally effective defense method. Our results suggest that\ntruly deleting sensitive information is a tractable but difficult problem,\nsince even relatively low attack success rates have potentially severe societal\nimplications for real-world deployment of language models.",
        "pdf_link": "https://arxiv.org/pdf/2309.17410v1.pdf"
    },
    {
        "title": "LoRA ensembles for large language model fine-tuning",
        "authors": [
            "Xi Wang",
            "Laurence Aitchison",
            "Maja Rudolph"
        ],
        "published": "2023-09-29T16:38:38Z",
        "summary": "Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as\noverconfidence, poor calibration, and unreliable prediction results on test\ndata or out-of-distribution samples. One approach commonly used in vision for\nalleviating this issue is a deep ensemble, which constructs an ensemble by\ntraining the same model multiple times using different random initializations.\nHowever, there is a huge challenge to ensembling LLMs: the most effective LLMs\nare very, very large. Keeping a single LLM in memory is already challenging\nenough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many\nsettings. To address these issues, we propose an ensemble approach using\nLow-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique.\nCritically, these low-rank adapters represent a very small number of\nparameters, orders of magnitude less than the underlying pre-trained model.\nThus, it is possible to construct large ensembles of LoRA adapters with almost\nthe same computational overhead as using the original model. We find that LoRA\nensembles, applied on its own or on top of pre-existing regularization\ntechniques, gives consistent improvements in predictive accuracy and\nuncertainty quantification.",
        "pdf_link": "https://arxiv.org/pdf/2310.00035v2.pdf"
    },
    {
        "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",
        "authors": [
            "Zhihan Liu",
            "Hao Hu",
            "Shenao Zhang",
            "Hongyi Guo",
            "Shuqi Ke",
            "Boyi Liu",
            "Zhaoran Wang"
        ],
        "published": "2023-09-29T16:36:39Z",
        "summary": "Large language models (LLMs) demonstrate impressive reasoning abilities, but\ntranslating reasoning into actions in the real world remains challenging. In\nparticular, it remains unclear how to complete a given task provably within a\nminimum number of interactions with the external environment, e.g., through an\ninternal mechanism of reasoning. To this end, we propose a principled framework\nwith provable regret guarantees to orchestrate reasoning and acting, which we\ncall \"reason for future, act for now\" (\\texttt{RAFA}). Specifically, we design\na prompt template for reasoning that learns from the memory buffer and plans a\nfuture trajectory over a long horizon (\"reason for future\"). At each step, the\nLLM agent takes the initial action of the planned trajectory (\"act for now\"),\nstores the collected feedback in the memory buffer, and reinvokes the reasoning\nroutine to replan the future trajectory from the new state.\n  The key idea is to cast reasoning in LLMs as learning and planning in\nBayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt\nLLMs to form an updated posterior of the unknown environment from the memory\nbuffer (learning) and generate an optimal trajectory for multiple future steps\nthat maximizes a value function (planning). The learning and planning\nsubroutines are performed in an \"in-context\" manner to emulate the actor-critic\nupdate for MDPs. Our theoretical analysis proves that the novel combination of\nlong-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. In\nparticular, the regret bound highlights an intriguing interplay between the\nprior knowledge obtained through pretraining and the uncertainty reduction\nachieved by reasoning and acting. Our empirical validation shows that it\noutperforms various existing frameworks and achieves nearly perfect scores on a\nfew benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2309.17382v2.pdf"
    },
    {
        "title": "Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings",
        "authors": [
            "Edouard Yvinec",
            "Arnaud Dapogny",
            "Kevin Bailly"
        ],
        "published": "2023-09-29T16:04:55Z",
        "summary": "The massive interest in deep neural networks (DNNs) for both computer vision\nand natural language processing has been sparked by the growth in computational\npower. However, this led to an increase in the memory footprint, to a point\nwhere it can be challenging to simply load a model on commodity devices such as\nmobile phones. To address this limitation, quantization is a favored solution\nas it maps high precision tensors to a low precision, memory efficient format.\nIn terms of memory footprint reduction, its most effective variants are based\non codebooks. These methods, however, suffer from two limitations. First, they\neither define a single codebook for each tensor, or use a memory-expensive\nmapping to multiple codebooks. Second, gradient descent optimization of the\nmapping favors jumps toward extreme values, hence not defining a proximal\nsearch. In this work, we propose to address these two limitations. First, we\ninitially group similarly distributed neurons and leverage the re-ordered\nstructure to either apply different scale factors to the different groups, or\nmap weights that fall in these groups to several codebooks, without any mapping\noverhead. Second, stemming from this initialization, we propose a joint\nlearning of the codebook and weight mappings that bears similarities with\nrecent gradient-based post-training quantization techniques. Third, drawing\nestimation from straight-through estimation techniques, we introduce a novel\ngradient update definition to enable a proximal search of the codebooks and\ntheir mappings. The proposed jointly learnable codebooks and mappings (JLCM)\nmethod allows a very efficient approximation of any DNN: as such, a Llama 7B\ncan be compressed down to 2Go and loaded on 5-year-old smartphones.",
        "pdf_link": "https://arxiv.org/pdf/2309.17361v1.pdf"
    },
    {
        "title": "Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis",
        "authors": [
            "Paul Glasserman",
            "Caden Lin"
        ],
        "published": "2023-09-29T15:30:32Z",
        "summary": "Large language models (LLMs), including ChatGPT, can extract profitable\ntrading signals from the sentiment in news text. However, backtesting such\nstrategies poses a challenge because LLMs are trained on many years of data,\nand backtesting produces biased results if the training and backtesting periods\noverlap. This bias can take two forms: a look-ahead bias, in which the LLM may\nhave specific knowledge of the stock returns that followed a news article, and\na distraction effect, in which general knowledge of the companies named\ninterferes with the measurement of a text's sentiment. We investigate these\nsources of bias through trading strategies driven by the sentiment of financial\nnews headlines. We compare trading performance based on the original headlines\nwith de-biased strategies in which we remove the relevant company's identifiers\nfrom the text. In-sample (within the LLM training window), we find,\nsurprisingly, that the anonymized headlines outperform, indicating that the\ndistraction effect has a greater impact than look-ahead bias. This tendency is\nparticularly strong for larger companies--companies about which we expect an\nLLM to have greater general knowledge. Out-of-sample, look-ahead bias is not a\nconcern but distraction remains possible. Our proposed anonymization procedure\nis therefore potentially useful in out-of-sample implementation, as well as for\nde-biased backtesting.",
        "pdf_link": "https://arxiv.org/pdf/2309.17322v1.pdf"
    },
    {
        "title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators",
        "authors": [
            "Zongjie Li",
            "Chaozheng Wang",
            "Pingchuan Ma",
            "Daoyuan Wu",
            "Shuai Wang",
            "Cuiyun Gao",
            "Yang Liu"
        ],
        "published": "2023-09-29T14:38:58Z",
        "summary": "Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.01432v2.pdf"
    },
    {
        "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
        "authors": [
            "Jiaxian Guo",
            "Bo Yang",
            "Paul Yoo",
            "Bill Yuchen Lin",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "published": "2023-09-29T14:30:03Z",
        "summary": "Unlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities of\ndecision-making under uncertain or incomplete information. GPT-4, the recent\nbreakthrough in large language models (LLMs) trained on massive passive data,\nis notable for its knowledge retrieval and reasoning abilities. This paper\ndelves into the applicability of GPT-4's learned knowledge for imperfect\ninformation games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an\ninnovative agent that leverages GPT-4's capabilities for performing in\nimperfect information games. With proper prompt engineering to achieve\ndifferent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Importantly,\nGPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it\ncan understand others and intentionally impact others' behavior. Leveraging\nthis, we design a planning strategy that enables GPT-4 to competently play\nagainst different opponents, adapting its gameplay style as needed, while\nrequiring only the game rules and descriptions of observations as input. In the\nexperiments, we qualitatively showcase the capabilities of Suspicion-Agent\nacross three different imperfect information games and then quantitatively\nevaluate it in Leduc Hold'em. The results show that Suspicion-Agent can\npotentially outperform traditional algorithms designed for imperfect\ninformation games, without any specialized training or examples. In order to\nencourage and foster deeper insights within the community, we make our\ngame-related data publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2309.17277v2.pdf"
    },
    {
        "title": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games",
        "authors": [
            "Sahar Abdelnabi",
            "Amr Gomaa",
            "Sarath Sivaprasad",
            "Lea Sch\u00f6nherr",
            "Mario Fritz"
        ],
        "published": "2023-09-29T13:33:06Z",
        "summary": "There is a growing interest in using Large Language Models (LLMs) as agents\nto tackle real-world tasks that may require assessing complex situations. Yet,\nwe have a limited understanding of LLMs' reasoning and decision-making\ncapabilities, partly stemming from a lack of dedicated evaluation benchmarks.\nAs negotiating and compromising are key aspects of our everyday communication\nand collaboration, we propose using scorable negotiation games as a new\nevaluation framework for LLMs. We create a testbed of diverse text-based,\nmulti-agent, multi-issue, semantically rich negotiation games, with easily\ntunable difficulty. To solve the challenge, agents need to have strong\narithmetic, inference, exploration, and planning capabilities, while seamlessly\nintegrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT),\nwe show that agents can negotiate and consistently reach successful deals. We\nquantify the performance with multiple metrics and observe a large gap between\nGPT-4 and earlier models. Importantly, we test the generalization to new games\nand setups. Finally, we show that these games can help evaluate other critical\naspects, such as the interaction dynamics between agents in the presence of\ngreedy and adversarial players.",
        "pdf_link": "https://arxiv.org/pdf/2309.17234v1.pdf"
    },
    {
        "title": "Training and inference of large language models using 8-bit floating point",
        "authors": [
            "Sergio P. Perez",
            "Yan Zhang",
            "James Briggs",
            "Charlie Blake",
            "Josh Levy-Kramer",
            "Paul Balanca",
            "Carlo Luschi",
            "Stephen Barlow",
            "Andrew William Fitzgibbon"
        ],
        "published": "2023-09-29T13:24:33Z",
        "summary": "FP8 formats are gaining popularity to boost the computational efficiency for\ntraining and inference of large deep learning models. Their main challenge is\nthat a careful choice of scaling is needed to prevent degradation due to the\nreduced dynamic range compared to higher-precision formats. Although there\nexists ample literature about selecting such scalings for INT formats, this\ncritical aspect has yet to be addressed for FP8. This paper presents a\nmethodology to select the scalings for FP8 linear layers, based on dynamically\nupdating per-tensor scales for the weights, gradients and activations. We apply\nthis methodology to train and validate large language models of the type of GPT\nand Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate\nthe understanding of the FP8 dynamics, our results are accompanied by plots of\nthe per-tensor scale distribution for weights, activations and gradients during\nboth training and inference.",
        "pdf_link": "https://arxiv.org/pdf/2309.17224v1.pdf"
    },
    {
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training",
        "authors": [
            "Xidong Feng",
            "Ziyu Wan",
            "Muning Wen",
            "Stephen Marcus McAleer",
            "Ying Wen",
            "Weinan Zhang",
            "Jun Wang"
        ],
        "published": "2023-09-29T12:20:19Z",
        "summary": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim\nto augment the reasoning capabilities of LLMs by using tree-search algorithms\nto guide multi-step reasoning. These methods rely on prompting a pre-trained\nmodel to serve as a value function and focus on problems with low search depth.\nAs a result, these methods will not work in domains where the pre-trained LLM\ndoes not have enough knowledge to serve as an effective value function or in\ndomains that require long-horizon planning. To address these limitations, we\npresent an AlphaZero-like tree-search learning framework for LLMs (termed\nTS-LLM), systematically illustrating how tree-search with a learned value\nfunction can guide LLM decoding. TS-LLM distinguishes itself in two key ways.\n(1) Leveraging a learned value function and AlphaZero-like algorithms, our\napproach can be generally adaptable to a wide range of tasks, language models\nof any size, and tasks of varying search depths. (2) Our approach can guide\nLLMs during both inference and training, iteratively improving the LLM.\nEmpirical results across reasoning, planning, alignment, and decision-making\ntasks show that TS-LLM outperforms existing approaches and can handle trees\nwith a depth of 64.",
        "pdf_link": "https://arxiv.org/pdf/2309.17179v2.pdf"
    },
    {
        "title": "LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud",
        "authors": [
            "Mengke Zhang",
            "Tianxing He",
            "Tianle Wang",
            "Lu Mi",
            "Fatemehsadat Mireshghallah",
            "Binyi Chen",
            "Hao Wang",
            "Yulia Tsvetkov"
        ],
        "published": "2023-09-29T11:46:07Z",
        "summary": "In the current user-server interaction paradigm of prompted generation with\nlarge language models (LLM) on cloud, the server fully controls the generation\nprocess, which leaves zero options for users who want to keep the generated\ntext to themselves. We propose LatticeGen, a cooperative framework in which the\nserver still handles most of the computation while the user controls the\nsampling operation. The key idea is that the true generated sequence is mixed\nwith noise tokens by the user and hidden in a noised lattice. Considering\npotential attacks from a hypothetically malicious server and how the user can\ndefend against it, we propose the repeated beam-search attack and the mixing\nnoise scheme. In our experiments we apply LatticeGen to protect both prompt and\ngeneration. It is shown that while the noised lattice degrades generation\nquality, LatticeGen successfully protects the true generation to a remarkable\ndegree under strong attacks (more than 50% of the semantic remains hidden as\nmeasured by BERTScore).",
        "pdf_link": "https://arxiv.org/pdf/2309.17157v5.pdf"
    },
    {
        "title": "Using Large Language Models for Qualitative Analysis can Introduce Serious Bias",
        "authors": [
            "Julian Ashwin",
            "Aditya Chhabra",
            "Vijayendra Rao"
        ],
        "published": "2023-09-29T11:19:15Z",
        "summary": "Large Language Models (LLMs) are quickly becoming ubiquitous, but the\nimplications for social science research are not yet well understood. This\npaper asks whether LLMs can help us analyse large-N qualitative data from\nopen-ended interviews, with an application to transcripts of interviews with\nRohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of\ncaution is needed in using LLMs to annotate text as there is a risk of\nintroducing biases that can lead to misleading inferences. We here mean bias in\nthe technical sense, that the errors that LLMs make in annotating interview\ntranscripts are not random with respect to the characteristics of the interview\nsubjects. Training simpler supervised models on high-quality human annotations\nwith flexible coding leads to less measurement error and bias than LLM\nannotations. Therefore, given that some high quality annotations are necessary\nin order to asses whether an LLM introduces bias, we argue that it is probably\npreferable to train a bespoke model on these annotations than it is to use an\nLLM for annotation.",
        "pdf_link": "https://arxiv.org/pdf/2309.17147v2.pdf"
    },
    {
        "title": "Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?",
        "authors": [
            "Johannes Frey",
            "Lars-Peter Meyer",
            "Natanael Arndt",
            "Felix Brei",
            "Kirill Bulert"
        ],
        "published": "2023-09-29T10:36:04Z",
        "summary": "Large Language Models (LLMs) are advancing at a rapid pace, with significant\nimprovements at natural language processing and coding tasks. Yet, their\nability to work with formal languages representing data, specifically within\nthe realm of knowledge graph engineering, remains under-investigated. To\nevaluate the proficiency of various LLMs, we created a set of five tasks that\nprobe their ability to parse, understand, analyze, and create knowledge graphs\nserialized in Turtle syntax. These tasks, each embodying distinct degrees of\ncomplexity and being able to scale with the size of the problem, have been\nintegrated into our automated evaluation system, the LLM-KG-Bench. The\nevaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,\nClaude 1.3, and Claude 2.0, as well as two freely accessible offline models,\nGPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth\nunderstanding of the strengths and shortcomings of LLMs in relation to their\napplication within RDF knowledge graph engineering workflows utilizing Turtle\nrepresentation. While our findings show that the latest commercial models\noutperform their forerunners in terms of proficiency with the Turtle language,\nthey also reveal an apparent weakness. These models fall short when it comes to\nadhering strictly to the output formatting constraints, a crucial requirement\nin this context.",
        "pdf_link": "https://arxiv.org/pdf/2309.17122v1.pdf"
    },
    {
        "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
        "authors": [
            "Ryan Koo",
            "Minhwa Lee",
            "Vipul Raheja",
            "Jong Inn Park",
            "Zae Myung Kim",
            "Dongyeop Kang"
        ],
        "published": "2023-09-29T06:53:10Z",
        "summary": "Large Language Models (LLMs) have recently been shown to be effective as\nautomatic evaluators with simple prompting and in-context learning. In this\nwork, we assemble 15 LLMs of four different size ranges and evaluate their\noutput responses by preference ranking from the other LLMs as evaluators, such\nas System Star is better than System Square. We then evaluate the quality of\nranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators\n(CoBBLEr), a benchmark to measure six different cognitive biases in LLM\nevaluation outputs, such as the Egocentric bias where a model prefers to rank\nits own outputs highly in evaluation. We find that LLMs are biased text quality\nevaluators, exhibiting strong indications on our bias benchmark (average of 40%\nof comparisons across all models) within each of their evaluations that\nquestion their robustness as evaluators. Furthermore, we examine the\ncorrelation between human and machine preferences and calculate the average\nRank-Biased Overlap (RBO) score to be 49.6%, indicating that machine\npreferences are misaligned with humans. According to our findings, LLMs may\nstill be unable to be utilized for automatic annotation aligned with human\npreferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
        "pdf_link": "https://arxiv.org/pdf/2309.17012v1.pdf"
    },
    {
        "title": "Medical Foundation Models are Susceptible to Targeted Misinformation Attacks",
        "authors": [
            "Tianyu Han",
            "Sven Nebelung",
            "Firas Khader",
            "Tianci Wang",
            "Gustav Mueller-Franzes",
            "Christiane Kuhl",
            "Sebastian F\u00f6rsch",
            "Jens Kleesiek",
            "Christoph Haarburger",
            "Keno K. Bressem",
            "Jakob Nikolas Kather",
            "Daniel Truhn"
        ],
        "published": "2023-09-29T06:44:36Z",
        "summary": "Large language models (LLMs) have broad medical knowledge and can reason\nabout medical information across many domains, holding promising potential for\ndiverse medical applications in the near future. In this study, we demonstrate\na concerning vulnerability of LLMs in medicine. Through targeted manipulation\nof just 1.1% of the model's weights, we can deliberately inject an incorrect\nbiomedical fact. The erroneous information is then propagated in the model's\noutput, whilst its performance on other biomedical tasks remains intact. We\nvalidate our findings in a set of 1,038 incorrect biomedical facts. This\npeculiar susceptibility raises serious security and trustworthiness concerns\nfor the application of LLMs in healthcare settings. It accentuates the need for\nrobust protective measures, thorough verification mechanisms, and stringent\nmanagement of access to these models, ensuring their reliable and safe use in\nmedical practice.",
        "pdf_link": "https://arxiv.org/pdf/2309.17007v1.pdf"
    },
    {
        "title": "I Wish to Have an Argument: Argumentative Reasoning in Large Language Models",
        "authors": [
            "Adrian de Wynter",
            "Tommy Yuan"
        ],
        "published": "2023-09-29T02:41:38Z",
        "summary": "We evaluate the ability of contemporary large language models (LLMs) to\nperform argumentative reasoning. We frame our experiments in terms of the\nargument mining (AM) and argument pair extraction (APE) tasks, and evaluate\ntheir ability to perform reasoning at increasing levels of abstraction in the\ninput and output representations (e.g., arbitrary label sets, semantic graphs).\nWe find that, although LLMs are able to match or surpass the state-of-the-art\nin AM and APE, their argumentative reasoning performance is very dependent on\nthe input and output representation. We also find an \"exemplar effect\", where\ntoo many exemplars increasingly become detrimental for task performance, and\nabout 4-5 being the optimal amount. Neither result extends to chain-of-thought\n(CoT) prompting: we find the exemplar effect to be nullified, and our results\nsuggest that CoT allows for better performance under ill-conditioned problems.\nWe hope that the work reported contributes to the improvement of argumentative\nreasoning in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16938v1.pdf"
    },
    {
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond",
        "authors": [
            "Shen Zheng",
            "Yuyu Zhang",
            "Yijie Zhu",
            "Chenguang Xi",
            "Pengyang Gao",
            "Xun Zhou",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023-09-28T16:43:35Z",
        "summary": "With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16583v6.pdf"
    },
    {
        "title": "Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving",
        "authors": [
            "Sumit Kumar Jha",
            "Susmit Jha",
            "Patrick Lincoln",
            "Nathaniel D. Bastian",
            "Alvaro Velasquez",
            "Rickard Ewetz",
            "Sandeep Neema"
        ],
        "published": "2023-09-28T13:40:50Z",
        "summary": "Generative large language models (LLMs) with instruct training such as GPT-4\ncan follow human-provided instruction prompts and generate human-like responses\nto these prompts. Apart from natural language responses, they have also been\nfound to be effective at generating formal artifacts such as code, plans, and\nlogical specifications from natural language prompts. Despite their remarkably\nimproved accuracy, these models are still known to produce factually incorrect\nor contextually inappropriate results despite their syntactic coherence - a\nphenomenon often referred to as hallucination. This limitation makes it\ndifficult to use these models to synthesize formal artifacts that are used in\nsafety-critical applications. Unlike tasks such as text summarization and\nquestion-answering, bugs in code, plan, and other formal artifacts produced by\nLLMs can be catastrophic. We posit that we can use the satisfiability modulo\ntheory (SMT) solvers as deductive reasoning engines to analyze the generated\nsolutions from the LLMs, produce counterexamples when the solutions are\nincorrect, and provide that feedback to the LLMs exploiting the dialog\ncapability of instruct-trained LLMs. This interaction between inductive LLMs\nand deductive SMT solvers can iteratively steer the LLM to generate the correct\nresponse. In our experiments, we use planning over the domain of blocks as our\nsynthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo,\nDavinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our\nmethod allows the user to communicate the planning problem in natural language;\neven the formulation of queries to SMT solvers is automatically generated from\nnatural language. Thus, the proposed technique can enable non-expert users to\ndescribe their problems in natural language, and the combination of LLMs and\nSMT solvers can produce provably correct solutions.",
        "pdf_link": "https://arxiv.org/pdf/2309.16436v1.pdf"
    },
    {
        "title": "Human Feedback is not Gold Standard",
        "authors": [
            "Tom Hosking",
            "Phil Blunsom",
            "Max Bartolo"
        ],
        "published": "2023-09-28T11:18:20Z",
        "summary": "Human feedback has become the de facto standard for evaluating the\nperformance of Large Language Models, and is increasingly being used as a\ntraining objective. However, it is not clear which properties of a generated\noutput this single `preference' score captures. We hypothesise that preference\nscores are subjective and open to undesirable biases. We critically analyse the\nuse of human feedback for both training and evaluation, to verify whether it\nfully captures a range of crucial error criteria. We find that while preference\nscores have fairly good coverage, they under-represent important aspects like\nfactuality. We further hypothesise that both preference scores and error\nannotation may be affected by confounders, and leverage instruction-tuned\nmodels to generate outputs that vary along two possible confounding dimensions:\nassertiveness and complexity. We find that the assertiveness of an output skews\nthe perceived rate of factuality errors, indicating that human annotations are\nnot a fully reliable evaluation metric or training objective. Finally, we offer\npreliminary evidence that using human feedback as a training objective\ndisproportionately increases the assertiveness of model outputs. We encourage\nfuture work to carefully consider whether preference scores are well aligned\nwith the desired objective.",
        "pdf_link": "https://arxiv.org/pdf/2309.16349v2.pdf"
    },
    {
        "title": "Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks",
        "authors": [
            "Eleftherios Triantafyllidis",
            "Filippos Christianos",
            "Zhibin Li"
        ],
        "published": "2023-09-28T11:14:52Z",
        "summary": "Current reinforcement learning algorithms struggle in sparse and complex\nenvironments, most notably in long-horizon manipulation tasks entailing a\nplethora of different sequences. In this work, we propose the Intrinsically\nGuided Exploration from Large Language Models (IGE-LLMs) framework. By\nleveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the\nexploratory process in reinforcement learning to address intricate long-horizon\nwith sparse rewards robotic manipulation tasks. We evaluate our framework and\nrelated intrinsic learning methods in an environment challenged with\nexploration, and a complex robotic manipulation task challenged by both\nexploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher\nperformance over related intrinsic methods and the direct use of LLMs in\ndecision-making, (ii) can be combined and complement existing learning methods\nhighlighting its modularity, (iii) are fairly insensitive to different\nintrinsic scaling parameters, and (iv) maintain robustness against increased\nlevels of uncertainty and horizons.",
        "pdf_link": "https://arxiv.org/pdf/2309.16347v2.pdf"
    },
    {
        "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
        "authors": [
            "Zhiwei Fei",
            "Xiaoyu Shen",
            "Dawei Zhu",
            "Fengzhe Zhou",
            "Zhuo Han",
            "Songyang Zhang",
            "Kai Chen",
            "Zongwen Shen",
            "Jidong Ge"
        ],
        "published": "2023-09-28T09:35:59Z",
        "summary": "Large language models (LLMs) have demonstrated strong capabilities in various\naspects. However, when applying them to the highly specialized, safe-critical\nlegal domain, it is unclear how much legal knowledge they possess and whether\nthey can reliably perform legal-related tasks. To address this gap, we propose\na comprehensive evaluation benchmark LawBench. LawBench has been meticulously\ncrafted to have precise assessment of the LLMs' legal capabilities from three\ncognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize\nneeded legal concepts, articles and facts; (2) Legal knowledge understanding:\nwhether LLMs can comprehend entities, events and relationships within legal\ntext; (3) Legal knowledge applying: whether LLMs can properly utilize their\nlegal knowledge and make necessary reasoning steps to solve realistic legal\ntasks. LawBench contains 20 diverse tasks covering 5 task types: single-label\nclassification (SLC), multi-label classification (MLC), regression, extraction\nand generation. We perform extensive evaluations of 51 LLMs on LawBench,\nincluding 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific\nLLMs. The results show that GPT-4 remains the best-performing LLM in the legal\ndomain, surpassing the others by a significant margin. While fine-tuning LLMs\non legal specific text brings certain improvements, we are still a long way\nfrom obtaining usable and reliable LLMs in legal tasks. All data, model\npredictions and evaluation code are released in\nhttps://github.com/open-compass/LawBench/. We hope this benchmark provides\nin-depth understanding of the LLMs' domain-specified capabilities and speed up\nthe development of LLMs in the legal domain.",
        "pdf_link": "https://arxiv.org/pdf/2309.16289v1.pdf"
    },
    {
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
        "authors": [
            "Chaoqi Wang",
            "Yibo Jiang",
            "Chenghao Yang",
            "Han Liu",
            "Yuxin Chen"
        ],
        "published": "2023-09-28T08:29:44Z",
        "summary": "The increasing capabilities of large language models (LLMs) raise\nopportunities for artificial general intelligence but concurrently amplify\nsafety concerns, such as potential misuse of AI systems, necessitating\neffective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has\nemerged as a promising pathway towards AI alignment but brings forth challenges\ndue to its complexity and dependence on a separate reward model. Direct\nPreference Optimization (DPO) has been proposed as an alternative, and it\nremains equivalent to RLHF under the reverse KL regularization constraint. This\npaper presents $f$-DPO, a generalized approach to DPO by incorporating diverse\ndivergence constraints. We show that under certain $f$-divergences, including\nJensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the\ncomplex relationship between the reward and optimal policy can also be\nsimplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the\nneed for estimating the normalizing constant in the Bradley-Terry model and\nenables a tractable mapping between the reward function and the optimal policy.\nOur approach optimizes LLMs to align with human preferences in a more efficient\nand supervised manner under a broad set of divergence constraints. Empirically,\nadopting these divergences ensures a balance between alignment performance and\ngeneration diversity. Importantly, $f$-DPO outperforms PPO-based methods in\ndivergence efficiency, and divergence constraints directly influence expected\ncalibration error (ECE).",
        "pdf_link": "https://arxiv.org/pdf/2309.16240v1.pdf"
    },
    {
        "title": "AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events",
        "authors": [
            "Yiming Li",
            "Jianfu Li",
            "Jianping He",
            "Cui Tao"
        ],
        "published": "2023-09-28T03:53:21Z",
        "summary": "Though Vaccines are instrumental in global health, mitigating infectious\ndiseases and pandemic outbreaks, they can occasionally lead to adverse events\n(AEs). Recently, Large Language Models (LLMs) have shown promise in effectively\nidentifying and cataloging AEs within clinical reports. Utilizing data from the\nVaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study\nparticularly focuses on AEs to evaluate LLMs' capability for AE extraction. A\nvariety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2,\nwere evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5\nmodel (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match\nand 0.816 for relaxed match. The encouraging performance of the AE-GPT\nunderscores LLMs' potential in processing medical data, indicating a\nsignificant stride towards advanced AE detection, thus presumably generalizable\nto other AE extraction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2309.16150v1.pdf"
    },
    {
        "title": "ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers",
        "authors": [
            "Junjie Yin",
            "Jiahao Dong",
            "Yingheng Wang",
            "Christopher De Sa",
            "Volodymyr Kuleshov"
        ],
        "published": "2023-09-28T02:55:01Z",
        "summary": "We propose a memory-efficient finetuning algorithm for large language models\n(LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision\non as little as one 24GB GPU. Our method, modular low-rank adaptation\n(ModuLoRA), integrates any user-specified weight quantizer with finetuning via\nlow-rank adapters (LoRAs). Our approach relies on a simple\nquantization-agnostic backward pass that adaptively materializes low-precision\nLLM weights from a custom black-box quantization module. This approach enables\nfinetuning 2-bit and 3-bit LLMs for the first time -- leveraging\nstate-of-the-art 2-bit QuIP\\# quantization and 3-bit OPTQ quantization --\noutperforming finetuning that relies on less sophisticated 4-bit and 8-bit\nmethods. In our experiments, \\lplora~attains competitive performance on text\nclassification, natural language inference, and instruction following tasks\nusing significantly less memory than existing approaches, and we also surpass\nthe state-of-the-art ROUGE score on a popular summarization task. We release\n\\lplora~together with a series of low-precision models as part of \\llmtune, a\nuser-friendly library for quantizing, running, and finetuning LLMs on consumer\nGPUs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16119v2.pdf"
    },
    {
        "title": "MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases",
        "authors": [
            "Yucheng Shi",
            "Shaochen Xu",
            "Zhengliang Liu",
            "Tianming Liu",
            "Xiang Li",
            "Ninghao Liu"
        ],
        "published": "2023-09-27T21:26:03Z",
        "summary": "Large Language Models (LLMs), although powerful in general domains, often\nperform poorly on domain-specific tasks like medical question answering (QA).\nMoreover, they tend to function as \"black-boxes,\" making it challenging to\nmodify their behavior. Addressing this, our study delves into model editing\nutilizing in-context learning, aiming to improve LLM responses without the need\nfor fine-tuning or retraining. Specifically, we propose a comprehensive\nretrieval strategy to extract medical facts from an external knowledge base,\nand then we incorporate them into the query prompt for the LLM. Focusing on\nmedical QA using the MedQA-SMILE dataset, we evaluate the impact of different\nretrieval models and the number of facts provided to the LLM. Notably, our\nedited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%.\nThis work underscores the potential of model editing to enhance LLM\nperformance, offering a practical approach to mitigate the challenges of\nblack-box LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16035v1.pdf"
    },
    {
        "title": "Lyra: Orchestrating Dual Correction in Automated Theorem Proving",
        "authors": [
            "Chuanyang Zheng",
            "Haiming Wang",
            "Enze Xie",
            "Zhengying Liu",
            "Jiankai Sun",
            "Huajian Xin",
            "Jianhao Shen",
            "Zhenguo Li",
            "Yu Li"
        ],
        "published": "2023-09-27T17:29:41Z",
        "summary": "Large Language Models (LLMs) present an intriguing avenue for exploration in\nthe field of formal theorem proving. Nevertheless, their full potential,\nparticularly concerning the mitigation of hallucinations and refinement through\nprover error messages, remains an area that has yet to be thoroughly\ninvestigated. To enhance the effectiveness of LLMs in the field, we introduce\nthe Lyra, a new framework that employs two distinct correction mechanisms: Tool\nCorrection (TC) and Conjecture Correction (CC). To implement Tool Correction in\nthe post-processing of formal proofs, we leverage prior knowledge to utilize\npredefined prover tools (e.g., Sledgehammer) for guiding the replacement of\nincorrect tools. Tool Correction significantly contributes to mitigating\nhallucinations, thereby improving the overall accuracy of the proof. In\naddition, we introduce Conjecture Correction, an error feedback mechanism\ndesigned to interact with prover to refine formal proof conjectures with prover\nerror messages. Compared to the previous refinement framework, the proposed\nConjecture Correction refines generation with instruction but does not collect\npaired (generation, error & refinement) prompts. Our method has achieved\nstate-of-the-art (SOTA) performance on both miniF2F validation (48.0% -> 55.3%)\nand test (45.5% -> 51.2%). We also present 3 IMO problems solved by Lyra. We\nbelieve Tool Correction (post-process for hallucination mitigation) and\nConjecture Correction (subgoal adjustment from interaction with environment)\ncould provide a promising avenue for future research in this field.",
        "pdf_link": "https://arxiv.org/pdf/2309.15806v3.pdf"
    },
    {
        "title": "Large Language Model Routing with Benchmark Datasets",
        "authors": [
            "Tal Shnitzer",
            "Anthony Ou",
            "M\u00edrian Silva",
            "Kate Soule",
            "Yuekai Sun",
            "Justin Solomon",
            "Neil Thompson",
            "Mikhail Yurochkin"
        ],
        "published": "2023-09-27T17:08:40Z",
        "summary": "There is a rapidly growing number of open-source Large Language Models (LLMs)\nand benchmark datasets to compare them. While some models dominate these\nbenchmarks, no single model typically achieves the best accuracy in all tasks\nand use cases. In this work, we address the challenge of selecting the best LLM\nout of a collection of models for new tasks. We propose a new formulation for\nthe problem, in which benchmark datasets are repurposed to learn a \"router\"\nmodel for this LLM selection, and we show that this problem can be reduced to a\ncollection of binary classification tasks. We demonstrate the utility and\nlimitations of learning model routers from various benchmark datasets, where we\nconsistently improve performance upon using any single model for all tasks.",
        "pdf_link": "https://arxiv.org/pdf/2309.15789v1.pdf"
    },
    {
        "title": "Deep Model Fusion: A Survey",
        "authors": [
            "Weishi Li",
            "Yong Peng",
            "Miao Zhang",
            "Liang Ding",
            "Han Hu",
            "Li Shen"
        ],
        "published": "2023-09-27T14:40:12Z",
        "summary": "Deep model fusion/merging is an emerging technique that merges the parameters\nor predictions of multiple deep learning models into a single one. It combines\nthe abilities of different models to make up for the biases and errors of a\nsingle model to achieve better performance. However, deep model fusion on\nlarge-scale deep learning models (e.g., LLMs and foundation models) faces\nseveral challenges, including high computational cost, high-dimensional\nparameter space, interference between different heterogeneous models, etc.\nAlthough model fusion has attracted widespread attention due to its potential\nto solve complex real-world tasks, there is still a lack of complete and\ndetailed survey research on this technique. Accordingly, in order to understand\nthe model fusion method better and promote its development, we present a\ncomprehensive survey to summarize the recent progress. Specifically, we\ncategorize existing deep model fusion methods as four-fold: (1) \"Mode\nconnectivity\", which connects the solutions in weight space via a path of\nnon-increasing loss, in order to obtain better initialization for model fusion;\n(2) \"Alignment\" matches units between neural networks to create better\nconditions for fusion; (3) \"Weight average\", a classical model fusion method,\naverages the weights of multiple models to obtain more accurate results closer\nto the optimal solution; (4) \"Ensemble learning\" combines the outputs of\ndiverse models, which is a foundational technique for improving the accuracy\nand robustness of the final model. In addition, we analyze the challenges faced\nby deep model fusion and propose possible research directions for model fusion\nin the future. Our review is helpful in deeply understanding the correlation\nbetween different model fusion methods and practical application methods, which\ncan enlighten the research in the field of deep model fusion.",
        "pdf_link": "https://arxiv.org/pdf/2309.15698v1.pdf"
    },
    {
        "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems",
        "authors": [
            "Linxin Song",
            "Jieyu Zhang",
            "Lechao Cheng",
            "Pengyuan Zhou",
            "Tianyi Zhou",
            "Irene Li"
        ],
        "published": "2023-09-27T13:02:06Z",
        "summary": "Recent developments in large language models (LLMs) have shown promise in\nenhancing the capabilities of natural language processing (NLP). Despite these\nsuccesses, there remains a dearth of research dedicated to the NLP\nproblem-solving abilities of LLMs. To fill the gap in this area, we present a\nunique benchmarking dataset, NLPBench, comprising 378 college-level NLP\nquestions spanning various NLP topics sourced from Yale University's prior\nfinal exams. NLPBench includes questions with context, in which multiple\nsub-questions share the same public information, and diverse question types,\nincluding multiple choice, short answer, and math. Our evaluation, centered on\nLLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting\nstrategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study\nreveals that the effectiveness of the advanced prompting strategies can be\ninconsistent, occasionally damaging LLM performance, especially in smaller\nmodels like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated\nspecific shortcomings in LLMs' scientific problem-solving skills, with\nweaknesses in logical decomposition and reasoning notably affecting results.",
        "pdf_link": "https://arxiv.org/pdf/2309.15630v4.pdf"
    },
    {
        "title": "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models",
        "authors": [
            "Jung Hwan Heo",
            "Jeonghoon Kim",
            "Beomseok Kwon",
            "Byeongwook Kim",
            "Se Jung Kwon",
            "Dongsoo Lee"
        ],
        "published": "2023-09-27T09:48:31Z",
        "summary": "Large Language Models (LLMs) have recently demonstrated remarkable success\nacross various tasks. However, efficiently serving LLMs has been a challenge\ndue to the large memory bottleneck, specifically in small batch inference\nsettings (e.g. mobile devices). Weight-only quantization can be a promising\napproach, but sub-4 bit quantization remains a challenge due to large-magnitude\nactivation outliers. To mitigate the undesirable outlier effect, we first\npropose per-IC quantization, a simple yet effective method that creates\nquantization groups within each input channel (IC) rather than the conventional\nper-output-channel (per-OC). Our method is motivated by the observation that\nactivation outliers affect the input dimension of the weight matrix, so\nsimilarly grouping the weights in the IC direction can isolate outliers within\na group. We also find that activation outliers do not dictate quantization\ndifficulty, and inherent weight sensitivities also exist. With per-IC\nquantization as a new outlier-friendly scheme, we propose Adaptive Dimensions\n(AdaDim), a versatile quantization framework that can adapt to various weight\nsensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting\nprior methods such as Round-To-Nearest and GPTQ, showing significant\nimprovements across various language modeling benchmarks for both base (up to\n+4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is\navailable at https://github.com/johnheo/adadim-llm",
        "pdf_link": "https://arxiv.org/pdf/2309.15531v2.pdf"
    },
    {
        "title": "Graph Neural Prompting with Large Language Models",
        "authors": [
            "Yijun Tian",
            "Huan Song",
            "Zichen Wang",
            "Haozhu Wang",
            "Ziqing Hu",
            "Fang Wang",
            "Nitesh V. Chawla",
            "Panpan Xu"
        ],
        "published": "2023-09-27T06:33:29Z",
        "summary": "Large language models (LLMs) have shown remarkable generalization capability\nwith exceptional performance in various language modeling tasks. However, they\nstill exhibit inherent limitations in precisely capturing and returning\ngrounded knowledge. While existing work has explored utilizing knowledge graphs\n(KGs) to enhance language modeling via joint training and customized model\narchitectures, applying this to LLMs is problematic owing to their large number\nof parameters and high computational cost. Therefore, how to enhance\npre-trained LLMs using grounded knowledge, e.g., retrieval-augmented\ngeneration, remains an open question. In this work, we propose Graph Neural\nPrompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in\nlearning beneficial knowledge from KGs. GNP encompasses various designs,\nincluding a standard graph neural network encoder, a cross-modality pooling\nmodule, a domain projector, and a self-supervised link prediction objective.\nExtensive experiments on multiple datasets demonstrate the superiority of GNP\non both commonsense and biomedical reasoning tasks across different LLM sizes\nand settings. Code is available at https://github.com/meettyj/GNP.",
        "pdf_link": "https://arxiv.org/pdf/2309.15427v2.pdf"
    },
    {
        "title": "Beyond the Chat: Executable and Verifiable Text-Editing with LLMs",
        "authors": [
            "Philippe Laban",
            "Jesse Vig",
            "Marti A. Hearst",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023-09-27T00:56:17Z",
        "summary": "Conversational interfaces powered by Large Language Models (LLMs) have\nrecently become a popular way to obtain feedback during document editing.\nHowever, standard chat-based conversational interfaces do not support\ntransparency and verifiability of the editing changes that they suggest. To\ngive the author more agency when editing with an LLM, we present InkSync, an\nediting interface that suggests executable edits directly within the document\nbeing edited. Because LLMs are known to introduce factual errors, Inksync also\nsupports a 3-stage approach to mitigate this risk: Warn authors when a\nsuggested edit introduces new information, help authors Verify the new\ninformation's accuracy through external search, and allow an auditor to perform\nan a-posteriori verification by Auditing the document via a trace of all\nauto-generated content. Two usability studies confirm the effectiveness of\nInkSync's components when compared to standard LLM-based chat interfaces,\nleading to more accurate, more efficient editing, and improved user experience.",
        "pdf_link": "https://arxiv.org/pdf/2309.15337v1.pdf"
    },
    {
        "title": "Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI",
        "authors": [
            "Muhammad Aurangzeb Ahmad",
            "Ilker Yaramis",
            "Taposh Dutta Roy"
        ],
        "published": "2023-09-26T20:52:46Z",
        "summary": "Large language models have proliferated across multiple domains in as short\nperiod of time. There is however hesitation in the medical and healthcare\ndomain towards their adoption because of issues like factuality, coherence, and\nhallucinations. Give the high stakes nature of healthcare, many researchers\nhave even cautioned against its usage until these issues are resolved. The key\nto the implementation and deployment of LLMs in healthcare is to make these\nmodels trustworthy, transparent (as much possible) and explainable. In this\npaper we describe the key elements in creating reliable, trustworthy, and\nunbiased models as a necessary condition for their adoption in healthcare.\nSpecifically we focus on the quantification, validation, and mitigation of\nhallucinations in the context in healthcare. Lastly, we discuss how the future\nof LLMs in healthcare may look like.",
        "pdf_link": "https://arxiv.org/pdf/2311.01463v1.pdf"
    },
    {
        "title": "RAGAS: Automated Evaluation of Retrieval Augmented Generation",
        "authors": [
            "Shahul Es",
            "Jithin James",
            "Luis Espinosa-Anke",
            "Steven Schockaert"
        ],
        "published": "2023-09-26T19:23:54Z",
        "summary": "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With RAGAs, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.15217v1.pdf"
    },
    {
        "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
        "authors": [
            "Mert Yuksekgonul",
            "Varun Chandrasekaran",
            "Erik Jones",
            "Suriya Gunasekar",
            "Ranjita Naik",
            "Hamid Palangi",
            "Ece Kamar",
            "Besmira Nushi"
        ],
        "published": "2023-09-26T17:48:55Z",
        "summary": "We investigate the internal behavior of Transformer-based Large Language\nModels (LLMs) when they generate factually incorrect text. We propose modeling\nfactual queries as Constraint Satisfaction Problems and use this framework to\ninvestigate how the model interacts internally with factual constraints.\nSpecifically, we discover a strong positive relation between the model's\nattention to constraint tokens and the factual accuracy of its responses. In\nour curated suite of 11 datasets with over 40,000 prompts, we study the task of\npredicting factual errors with the Llama-2 family across all scales (7B, 13B,\n70B). We propose SAT Probe, a method probing self-attention patterns, that can\npredict constraint satisfaction and factual errors, and allows early error\nidentification. The approach and findings demonstrate how using the mechanistic\nunderstanding of factuality in LLMs can enhance reliability.",
        "pdf_link": "https://arxiv.org/pdf/2309.15098v1.pdf"
    },
    {
        "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
        "authors": [
            "Lorenzo Pacchiardi",
            "Alex J. Chan",
            "S\u00f6ren Mindermann",
            "Ilan Moscovitz",
            "Alexa Y. Pan",
            "Yarin Gal",
            "Owain Evans",
            "Jan Brauner"
        ],
        "published": "2023-09-26T16:07:54Z",
        "summary": "Large language models (LLMs) can \"lie\", which we define as outputting false\nstatements despite \"knowing\" the truth in a demonstrable sense. LLMs might\n\"lie\", for example, when instructed to output misinformation. Here, we develop\na simple lie detector that requires neither access to the LLM's activations\n(black-box) nor ground-truth knowledge of the fact in question. The detector\nworks by asking a predefined set of unrelated follow-up questions after a\nsuspected lie, and feeding the LLM's yes/no answers into a logistic regression\nclassifier. Despite its simplicity, this lie detector is highly accurate and\nsurprisingly general. When trained on examples from a single setting --\nprompting GPT-3.5 to lie about factual questions -- the detector generalises\nout-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,\n(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as\nsales. These results indicate that LLMs have distinctive lie-related\nbehavioural patterns, consistent across architectures and contexts, which could\nenable general-purpose lie detection.",
        "pdf_link": "https://arxiv.org/pdf/2309.15840v1.pdf"
    },
    {
        "title": "Large Language Model Alignment: A Survey",
        "authors": [
            "Tianhao Shen",
            "Renren Jin",
            "Yufei Huang",
            "Chuang Liu",
            "Weilong Dong",
            "Zishan Guo",
            "Xinwei Wu",
            "Yan Liu",
            "Deyi Xiong"
        ],
        "published": "2023-09-26T15:49:23Z",
        "summary": "Recent years have witnessed remarkable progress made in large language models\n(LLMs). Such advancements, while garnering significant attention, have\nconcurrently elicited various concerns. The potential of these models is\nundeniably vast; however, they may yield texts that are imprecise, misleading,\nor even detrimental. Consequently, it becomes paramount to employ alignment\ntechniques to ensure these models to exhibit behaviors consistent with human\nvalues.\n  This survey endeavors to furnish an extensive exploration of alignment\nmethodologies designed for LLMs, in conjunction with the extant capability\nresearch in this domain. Adopting the lens of AI alignment, we categorize the\nprevailing methods and emergent proposals for the alignment of LLMs into outer\nand inner alignment. We also probe into salient issues including the models'\ninterpretability, and potential vulnerabilities to adversarial attacks. To\nassess LLM alignment, we present a wide variety of benchmarks and evaluation\nmethodologies. After discussing the state of alignment research for LLMs, we\nfinally cast a vision toward the future, contemplating the promising avenues of\nresearch that lie ahead.\n  Our aspiration for this survey extends beyond merely spurring research\ninterests in this realm. We also envision bridging the gap between the AI\nalignment research community and the researchers engrossed in the capability\nexploration of LLMs for both capable and safe LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.15025v1.pdf"
    },
    {
        "title": "ConPET: Continual Parameter-Efficient Tuning for Large Language Models",
        "authors": [
            "Chenyang Song",
            "Xu Han",
            "Zheni Zeng",
            "Kuai Li",
            "Chen Chen",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Tao Yang"
        ],
        "published": "2023-09-26T08:52:04Z",
        "summary": "Continual learning necessitates the continual adaptation of models to newly\nemerging tasks while minimizing the catastrophic forgetting of old ones. This\nis extremely challenging for large language models (LLMs) with vanilla\nfull-parameter tuning due to high computation costs, memory consumption, and\nforgetting issue. Inspired by the success of parameter-efficient tuning (PET),\nwe propose Continual Parameter-Efficient Tuning (ConPET), a generalizable\nparadigm for continual task adaptation of LLMs with task-number-independent\ntraining complexity. ConPET includes two versions with different application\nscenarios. First, Static ConPET can adapt former continual learning methods\noriginally designed for relatively smaller models to LLMs through PET and a\ndynamic replay strategy, which largely reduces the tuning costs and alleviates\nthe over-fitting and forgetting issue. Furthermore, to maintain scalability,\nDynamic ConPET adopts separate PET modules for different tasks and a PET module\nselector for dynamic optimal selection. In our extensive experiments, the\nadaptation of Static ConPET helps multiple former methods reduce the scale of\ntunable parameters by over 3,000 times and surpass the PET-only baseline by at\nleast 5 points on five smaller benchmarks, while Dynamic ConPET gains its\nadvantage on the largest dataset. The codes and datasets are available at\nhttps://github.com/Raincleared-Song/ConPET.",
        "pdf_link": "https://arxiv.org/pdf/2309.14763v1.pdf"
    },
    {
        "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
        "authors": [
            "Yuhui Xu",
            "Lingxi Xie",
            "Xiaotao Gu",
            "Xin Chen",
            "Heng Chang",
            "Hengheng Zhang",
            "Zhengsu Chen",
            "Xiaopeng Zhang",
            "Qi Tian"
        ],
        "published": "2023-09-26T07:22:23Z",
        "summary": "Recently years have witnessed a rapid development of large language models\n(LLMs). Despite the strong ability in many language-understanding tasks, the\nheavy computational burden largely restricts the application of LLMs especially\nwhen one needs to deploy them onto edge devices. In this paper, we propose a\nquantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies\nin the imbalanced degrees of freedom of quantization and adaptation, and the\nsolution is to use group-wise operators which increase the degree of freedom of\nquantization meanwhile decreasing that of adaptation. QA-LoRA is easily\nimplemented with a few lines of code, and it equips the original LoRA with\ntwo-fold abilities: (i) during fine-tuning, the LLM's weights are quantized\n(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the\nLLM and auxiliary weights are naturally integrated into a quantized model\nwithout loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model\nfamilies and validate its effectiveness in different fine-tuning datasets and\ndownstream scenarios. Code will be made available at\nhttps://github.com/yuhuixu1993/qa-lora.",
        "pdf_link": "https://arxiv.org/pdf/2309.14717v2.pdf"
    },
    {
        "title": "Are Human-generated Demonstrations Necessary for In-context Learning?",
        "authors": [
            "Rui Li",
            "Guoyin Wang",
            "Jiwei Li"
        ],
        "published": "2023-09-26T05:10:08Z",
        "summary": "Despite the promising few-shot ability of large language models (LLMs), the\nstandard paradigm of In-context Learning (ICL) suffers the disadvantages of\nsusceptibility to selected demonstrations and the intricacy to generate these\ndemonstrations. In this paper, we raise the fundamental question that whether\nhuman-generated demonstrations are necessary for ICL. To answer this question,\nwe propose self-contemplation prompting strategy (SEC), a paradigm free from\nhuman-crafted demonstrations. The key point of SEC is that, instead of using\nhand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create\ndemonstrations on their own, based on which the final output is generated. SEC\nis a flexible framework and can be adapted to both the vanilla ICL and the\nchain-of-thought (CoT), but with greater ease: as the manual-generation process\nof both examples and rationale can be saved. Extensive experiments in\narithmetic reasoning, commonsense reasoning, multi-task language understanding,\nand code generation benchmarks, show that SEC, which does not require\nhand-crafted demonstrations, significantly outperforms the zero-shot learning\nstrategy, and achieves comparable results to ICL with hand-crafted\ndemonstrations. This demonstrates that, for many tasks, contemporary LLMs\npossess a sufficient level of competence to exclusively depend on their own\ncapacity for decision making, removing the need for external training data.\nCode is available at https://github.com/ruili33/SEC.",
        "pdf_link": "https://arxiv.org/pdf/2309.14681v4.pdf"
    },
    {
        "title": "Disinformation Detection: An Evolving Challenge in the Age of LLMs",
        "authors": [
            "Bohan Jiang",
            "Zhen Tan",
            "Ayushi Nirmal",
            "Huan Liu"
        ],
        "published": "2023-09-25T22:12:50Z",
        "summary": "The advent of generative Large Language Models (LLMs) such as ChatGPT has\ncatalyzed transformative advancements across multiple domains. However,\nalongside these advancements, they have also introduced potential threats. One\ncritical concern is the misuse of LLMs by disinformation spreaders, leveraging\nthese models to generate highly persuasive yet misleading content that\nchallenges the disinformation detection system. This work aims to address this\nissue by answering three research questions: (1) To what extent can the current\ndisinformation detection technique reliably detect LLM-generated\ndisinformation? (2) If traditional techniques prove less effective, can LLMs\nthemself be exploited to serve as a robust defense against advanced\ndisinformation? and, (3) Should both these strategies falter, what novel\napproaches can be proposed to counter this burgeoning threat effectively? A\nholistic exploration for the formation and detection of disinformation is\nconducted to foster this line of research.",
        "pdf_link": "https://arxiv.org/pdf/2309.15847v1.pdf"
    },
    {
        "title": "Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator",
        "authors": [
            "Hanzhuo Huang",
            "Yufan Feng",
            "Cheng Shi",
            "Lan Xu",
            "Jingyi Yu",
            "Sibei Yang"
        ],
        "published": "2023-09-25T19:42:16Z",
        "summary": "Text-to-video is a rapidly growing research area that aims to generate a\nsemantic, identical, and temporal coherence sequence of frames that accurately\nalign with the input text prompt. This study focuses on zero-shot text-to-video\ngeneration considering the data- and cost-efficient. To generate a\nsemantic-coherent video, exhibiting a rich portrayal of temporal semantics such\nas the whole process of flower blooming rather than a set of \"moving images\",\nwe propose a novel Free-Bloom pipeline that harnesses large language models\n(LLMs) as the director to generate a semantic-coherence prompt sequence, while\npre-trained latent diffusion models (LDMs) as the animator to generate the high\nfidelity frames. Furthermore, to ensure temporal and identical coherence while\nmaintaining semantic coherence, we propose a series of annotative modifications\nto adapting LDMs in the reverse process, including joint noise sampling,\nstep-aware attention shift, and dual-path interpolation. Without any video data\nand training requirements, Free-Bloom generates vivid and high-quality videos,\nawe-inspiring in generating complex scenes with semantic meaningful frame\nsequences. In addition, Free-Bloom is naturally compatible with LDMs-based\nextensions.",
        "pdf_link": "https://arxiv.org/pdf/2309.14494v1.pdf"
    },
    {
        "title": "Lifelong Robot Learning with Human Assisted Language Planners",
        "authors": [
            "Meenal Parakh",
            "Alisha Fong",
            "Anthony Simeonov",
            "Tao Chen",
            "Abhishek Gupta",
            "Pulkit Agrawal"
        ],
        "published": "2023-09-25T17:45:55Z",
        "summary": "Large Language Models (LLMs) have been shown to act like planners that can\ndecompose high-level instructions into a sequence of executable instructions.\nHowever, current LLM-based planners are only able to operate with a fixed set\nof skills. We overcome this critical limitation and present a method for using\nLLM-based planners to query new skills and teach robots these skills in a data\nand time-efficient manner for rigid object manipulation. Our system can re-use\nnewly acquired skills for future tasks, demonstrating the potential of open\nworld and lifelong learning. We evaluate the proposed framework on multiple\ntasks in simulation and the real world. Videos are available at:\nhttps://sites.google.com/mit.edu/halp-robot-learning.",
        "pdf_link": "https://arxiv.org/pdf/2309.14321v2.pdf"
    },
    {
        "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ],
        "published": "2023-09-25T17:37:20Z",
        "summary": "Large language models (LLMs) can store a vast amount of world knowledge,\noften extractable via question-answering (e.g., \"What is Abraham Lincoln's\nbirthday?\"). However, do they answer such questions based on exposure to\nsimilar questions during training (i.e., cheating), or by genuinely learning to\nextract knowledge from sources like Wikipedia?\n  In this paper, we investigate this issue using a controlled biography\ndataset. We find a strong correlation between the model's ability to extract\nknowledge and various diversity measures of the training data.\n$\\textbf{Essentially}$, for knowledge to be reliably extracted, it must be\nsufficiently augmented (e.g., through paraphrasing, sentence shuffling)\n$\\textit{during pretraining}$. Without such augmentation, knowledge may be\nmemorized but not extractable, leading to 0% accuracy, regardless of subsequent\ninstruction fine-tuning.\n  To understand why this occurs, we employ (nearly) linear probing to\ndemonstrate a strong connection between the observed correlation and how the\nmodel internally encodes knowledge -- whether it is linearly encoded in the\nhidden embeddings of entity names or distributed across other token embeddings\nin the training text.\n  This paper provides $\\textbf{several key recommendations for LLM pretraining\nin the industry}$: (1) rewrite the pretraining data -- using small, auxiliary\nmodels -- to provide knowledge augmentation, and (2) incorporate more\ninstruction-finetuning data into the pretraining stage before it becomes too\nlate.",
        "pdf_link": "https://arxiv.org/pdf/2309.14316v2.pdf"
    },
    {
        "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
        "authors": [
            "Yangjun Ruan",
            "Honghua Dong",
            "Andrew Wang",
            "Silviu Pitis",
            "Yongchao Zhou",
            "Jimmy Ba",
            "Yann Dubois",
            "Chris J. Maddison",
            "Tatsunori Hashimoto"
        ],
        "published": "2023-09-25T17:08:02Z",
        "summary": "Recent advances in Language Model (LM) agents and tool use, exemplified by\napplications like ChatGPT Plugins, enable a rich set of capabilities but also\namplify potential risks - such as leaking private data or causing financial\nlosses. Identifying these risks is labor-intensive, necessitating implementing\nthe tools, manually setting up the environment for each test scenario, and\nfinding risky cases. As tools and agents become more complex, the high cost of\ntesting these agents will make it increasingly difficult to find high-stakes,\nlong-tailed risks. To address these challenges, we introduce ToolEmu: a\nframework that uses an LM to emulate tool execution and enables the testing of\nLM agents against a diverse range of tools and scenarios, without manual\ninstantiation. Alongside the emulator, we develop an LM-based automatic safety\nevaluator that examines agent failures and quantifies associated risks. We test\nboth the tool emulator and evaluator through human evaluation and find that\n68.8% of failures identified with ToolEmu would be valid real-world agent\nfailures. Using our curated initial benchmark consisting of 36 high-stakes\ntools and 144 test cases, we provide a quantitative risk analysis of current LM\nagents and identify numerous failures with potentially severe outcomes.\nNotably, even the safest LM agent exhibits such failures 23.9% of the time\naccording to our evaluator, underscoring the need to develop safer LM agents\nfor real-world deployment.",
        "pdf_link": "https://arxiv.org/pdf/2309.15817v1.pdf"
    },
    {
        "title": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models",
        "authors": [
            "Ahmad Faiz",
            "Sotaro Kaneda",
            "Ruhan Wang",
            "Rita Osi",
            "Prateek Sharma",
            "Fan Chen",
            "Lei Jiang"
        ],
        "published": "2023-09-25T14:50:04Z",
        "summary": "The carbon footprint associated with large language models (LLMs) is a\nsignificant concern, encompassing emissions from their training, inference,\nexperimentation, and storage processes, including operational and embodied\ncarbon emissions. An essential aspect is accurately estimating the carbon\nimpact of emerging LLMs even before their training, which heavily relies on GPU\nusage. Existing studies have reported the carbon footprint of LLM training, but\nonly one tool, mlco2, can predict the carbon footprint of new neural networks\nprior to physical training. However, mlco2 has several serious limitations. It\ncannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,\ndisregards critical architectural parameters, focuses solely on GPUs, and\ncannot model embodied carbon footprints. Addressing these gaps, we introduce\n\\textit{\\carb}, an end-to-end carbon footprint projection model designed for\nboth dense and MoE LLMs. Compared to mlco2, \\carb~significantly enhances the\naccuracy of carbon footprint estimations for various LLMs. The source code is\nreleased at \\url{https://github.com/SotaroKaneda/MLCarbon}.",
        "pdf_link": "https://arxiv.org/pdf/2309.14393v2.pdf"
    },
    {
        "title": "The Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks",
        "authors": [
            "Andreas Tsamados",
            "Luciano Floridi",
            "Mariarosaria Taddeo"
        ],
        "published": "2023-09-25T10:48:46Z",
        "summary": "The widespread integration of autoregressive-large language models (AR-LLMs),\nsuch as ChatGPT, across established applications, like search engines, has\nintroduced critical vulnerabilities with uniquely scalable characteristics. In\nthis commentary, we analyse these vulnerabilities, their dependence on natural\nlanguage as a vector of attack, and their challenges to cybersecurity best\npractices. We offer recommendations designed to mitigate these challenges.",
        "pdf_link": "https://arxiv.org/pdf/2311.09224v1.pdf"
    },
    {
        "title": "Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering",
        "authors": [
            "Nidhi Hegde",
            "Sujoy Paul",
            "Gagan Madan",
            "Gaurav Aggarwal"
        ],
        "published": "2023-09-25T07:01:16Z",
        "summary": "Recent document question answering models consist of two key components: the\nvision encoder, which captures layout and visual elements in images, and a\nLarge Language Model (LLM) that helps contextualize questions to the image and\nsupplements them with external world knowledge to generate accurate answers.\nHowever, the relative contributions of the vision encoder and the language\nmodel in these tasks remain unclear. This is especially interesting given the\neffectiveness of instruction-tuned LLMs, which exhibit remarkable adaptability\nto new tasks. To this end, we explore the following aspects in this work: (1)\nThe efficacy of an LLM-only approach on document question answering tasks (2)\nstrategies for serializing textual information within document images and\nfeeding it directly to an instruction-tuned LLM, thus bypassing the need for an\nexplicit vision encoder (3) thorough quantitative analysis on the feasibility\nof such an approach. Our comprehensive analysis encompasses six diverse\nbenchmark datasets, utilizing LLMs of varying scales. Our findings reveal that\na strategy exclusively reliant on the LLM yields results that are on par with\nor closely approach state-of-the-art performance across a range of datasets. We\nposit that this evaluation framework will serve as a guiding resource for\nselecting appropriate datasets for future research endeavors that emphasize the\nfundamental importance of layout and image content information.",
        "pdf_link": "https://arxiv.org/pdf/2309.14389v1.pdf"
    },
    {
        "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
        "authors": [
            "Ida Momennejad",
            "Hosein Hasanbeig",
            "Felipe Vieira",
            "Hiteshi Sharma",
            "Robert Osazuwa Ness",
            "Nebojsa Jojic",
            "Hamid Palangi",
            "Jonathan Larson"
        ],
        "published": "2023-09-25T01:20:13Z",
        "summary": "Recently an influx of studies claim emergent cognitive abilities in large\nlanguage models (LLMs). Yet, most rely on anecdotes, overlook contamination of\ntraining sets, or lack systematic Evaluation involving multiple tasks, control\nconditions, multiple iterations, and statistical robustness tests. Here we make\ntwo major contributions. First, we propose CogEval, a cognitive\nscience-inspired protocol for the systematic evaluation of cognitive capacities\nin Large Language Models. The CogEval protocol can be followed for the\nevaluation of various abilities. Second, here we follow CogEval to\nsystematically evaluate cognitive maps and planning ability across eight LLMs\n(OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard,\nCohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base\nour task prompts on human experiments, which offer both established construct\nvalidity for evaluating planning, and are absent from LLM training sets. We\nfind that, while LLMs show apparent competence in a few planning tasks with\nsimpler structures, systematic evaluation reveals striking failure modes in\nplanning tasks, including hallucinations of invalid trajectories and getting\ntrapped in loops. These findings do not support the idea of emergent\nout-of-the-box planning ability in LLMs. This could be because LLMs do not\nunderstand the latent relational structures underlying planning problems, known\nas cognitive maps, and fail at unrolling goal-directed trajectories based on\nthe underlying structure. Implications for application and future directions\nare discussed.",
        "pdf_link": "https://arxiv.org/pdf/2309.15129v1.pdf"
    },
    {
        "title": "Can LLM-Generated Misinformation Be Detected?",
        "authors": [
            "Canyu Chen",
            "Kai Shu"
        ],
        "published": "2023-09-25T00:45:07Z",
        "summary": "The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.",
        "pdf_link": "https://arxiv.org/pdf/2309.13788v3.pdf"
    },
    {
        "title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning",
        "authors": [
            "Hosein Hasanbeig",
            "Hiteshi Sharma",
            "Leo Betthauser",
            "Felipe Vieira Frujeri",
            "Ida Momennejad"
        ],
        "published": "2023-09-24T17:15:58Z",
        "summary": "From grading papers to summarizing medical documents, large language models\n(LLMs) are evermore used for evaluation of text generated by humans and AI\nalike. However, despite their extensive utility, LLMs exhibit distinct failure\nmodes, necessitating a thorough audit and improvement of their text evaluation\ncapabilities. Here we introduce ALLURE, a systematic approach to Auditing Large\nLanguage Models Understanding and Reasoning Errors. ALLURE involves comparing\nLLM-generated evaluations with annotated data, and iteratively incorporating\ninstances of significant deviation into the evaluator, which leverages\nin-context learning (ICL) to enhance and improve robust evaluation of text by\nLLMs. Through this iterative process, we refine the performance of the\nevaluator LLM, ultimately reducing reliance on human annotators in the\nevaluation process. We anticipate ALLURE to serve diverse applications of LLMs\nin various domains related to evaluation of textual data, such as medical\nsummarization, education, and and productivity.",
        "pdf_link": "https://arxiv.org/pdf/2309.13701v2.pdf"
    },
    {
        "title": "Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve",
        "authors": [
            "R. Thomas McCoy",
            "Shunyu Yao",
            "Dan Friedman",
            "Matthew Hardy",
            "Thomas L. Griffiths"
        ],
        "published": "2023-09-24T13:35:28Z",
        "summary": "The widespread adoption of large language models (LLMs) makes it important to\nrecognize their strengths and limitations. We argue that in order to develop a\nholistic understanding of these systems we need to consider the problem that\nthey were trained to solve: next-word prediction over Internet text. By\nrecognizing the pressures that this task exerts we can make predictions about\nthe strategies that LLMs will adopt, allowing us to reason about when they will\nsucceed or fail. This approach - which we call the teleological approach -\nleads us to identify three factors that we hypothesize will influence LLM\naccuracy: the probability of the task to be performed, the probability of the\ntarget output, and the probability of the provided input. We predict that LLMs\nwill achieve higher accuracy when these probabilities are high than when they\nare low - even in deterministic settings where probability should not matter.\nTo test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven\ntasks, and we find robust evidence that LLMs are influenced by probability in\nthe ways that we have hypothesized. In many cases, the experiments reveal\nsurprising failure modes. For instance, GPT-4's accuracy at decoding a simple\ncipher is 51% when the output is a high-probability word sequence but only 13%\nwhen it is low-probability. These results show that AI practitioners should be\ncareful about using LLMs in low-probability situations. More broadly, we\nconclude that we should not evaluate LLMs as if they are humans but should\ninstead treat them as a distinct type of system - one that has been shaped by\nits own particular set of pressures.",
        "pdf_link": "https://arxiv.org/pdf/2309.13638v1.pdf"
    },
    {
        "title": "Resolving References in Visually-Grounded Dialogue via Text Generation",
        "authors": [
            "Bram Willemsen",
            "Livia Qian",
            "Gabriel Skantze"
        ],
        "published": "2023-09-23T17:07:54Z",
        "summary": "Vision-language models (VLMs) have shown to be effective at image retrieval\nbased on simple text queries, but text-image retrieval based on conversational\ninput remains a challenge. Consequently, if we want to use VLMs for reference\nresolution in visually-grounded dialogue, the discourse processing capabilities\nof these models need to be augmented. To address this issue, we propose\nfine-tuning a causal large language model (LLM) to generate definite\ndescriptions that summarize coreferential information found in the linguistic\ncontext of references. We then use a pretrained VLM to identify referents based\non the generated descriptions, zero-shot. We evaluate our approach on a\nmanually annotated dataset of visually-grounded dialogues and achieve results\nthat, on average, exceed the performance of the baselines we compare against.\nFurthermore, we find that using referent descriptions based on larger context\nwindows has the potential to yield higher returns.",
        "pdf_link": "https://arxiv.org/pdf/2309.13430v1.pdf"
    },
    {
        "title": "A Chat About Boring Problems: Studying GPT-based text normalization",
        "authors": [
            "Yang Zhang",
            "Travis M. Bartley",
            "Mariana Graterol-Fuenmayor",
            "Vitaly Lavrukhin",
            "Evelina Bakhturina",
            "Boris Ginsburg"
        ],
        "published": "2023-09-23T16:32:59Z",
        "summary": "Text normalization - the conversion of text from written to spoken form - is\ntraditionally assumed to be an ill-formed task for language models. In this\nwork, we argue otherwise. We empirically show the capacity of Large-Language\nModels (LLM) for text normalization in few-shot scenarios. Combining\nself-consistency reasoning with linguistic-informed prompt engineering, we find\nLLM based text normalization to achieve error rates around 40\\% lower than top\nnormalization systems. Further, upon error analysis, we note key limitations in\nthe conventional design of text normalization tasks. We create a new taxonomy\nof text normalization errors and apply it to results from GPT-3.5-Turbo and\nGPT-4.0. Through this new framework, we can identify strengths and weaknesses\nof GPT-based TN, opening opportunities for future work.",
        "pdf_link": "https://arxiv.org/pdf/2309.13426v2.pdf"
    },
    {
        "title": "Towards LLM-guided Causal Explainability for Black-box Text Classifiers",
        "authors": [
            "Amrita Bhattacharjee",
            "Raha Moraffah",
            "Joshua Garland",
            "Huan Liu"
        ],
        "published": "2023-09-23T11:22:28Z",
        "summary": "With the advent of larger and more complex deep learning models, such as in\nNatural Language Processing (NLP), model qualities like explainability and\ninterpretability, albeit highly desirable, are becoming harder challenges to\ntackle and solve. For example, state-of-the-art models in text classification\nare black-box by design. Although standard explanation methods provide some\ndegree of explainability, these are mostly correlation-based methods and do not\nprovide much insight into the model. The alternative of causal explainability\nis more desirable to achieve but extremely challenging in NLP due to a variety\nof reasons. Inspired by recent endeavors to utilize Large Language Models\n(LLMs) as experts, in this work, we aim to leverage the instruction-following\nand textual understanding capabilities of recent state-of-the-art LLMs to\nfacilitate causal explainability via counterfactual explanation generation for\nblack-box text classifiers. To do this, we propose a three-step pipeline via\nwhich, we use an off-the-shelf LLM to: (1) identify the latent or unobserved\nfeatures in the input text, (2) identify the input features associated with the\nlatent features, and finally (3) use the identified input features to generate\na counterfactual explanation. We experiment with our pipeline on multiple NLP\ntext classification datasets, with several recent LLMs, and present interesting\nand promising findings.",
        "pdf_link": "https://arxiv.org/pdf/2309.13340v2.pdf"
    },
    {
        "title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
        "authors": [
            "Xufeng Zhao",
            "Mengdi Li",
            "Wenhao Lu",
            "Cornelius Weber",
            "Jae Hee Lee",
            "Kun Chu",
            "Stefan Wermter"
        ],
        "published": "2023-09-23T11:21:12Z",
        "summary": "Recent advancements in large language models have showcased their remarkable\ngeneralizability across various domains. However, their reasoning abilities\nstill have significant room for improvement, especially when confronted with\nscenarios requiring multi-step reasoning. Although large language models\npossess extensive knowledge, their reasoning often fails to effectively utilize\nthis knowledge to establish a coherent thinking paradigm. These models\nsometimes show hallucinations as their reasoning procedures are unconstrained\nby logical principles. Aiming at improving the zero-shot chain-of-thought\nreasoning ability of large language models, we propose LoT (Logical Thoughts),\na self-improvement prompting framework that leverages principles rooted in\nsymbolic logic, particularly Reductio ad Absurdum, to systematically verify and\nrectify the reasoning processes step by step. Experimental evaluations\nconducted on language tasks in diverse domains, including arithmetic,\ncommonsense, symbolic, causal inference, and social problems, demonstrate the\nefficacy of enhanced reasoning by logic. The implementation code for LoT can be\naccessed at: https://github.com/xf-zhao/LoT.",
        "pdf_link": "https://arxiv.org/pdf/2309.13339v4.pdf"
    },
    {
        "title": "AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling",
        "authors": [
            "Pivithuru Thejan Amarasinghe",
            "Su Nguyen",
            "Yuan Sun",
            "Damminda Alahakoon"
        ],
        "published": "2023-09-22T23:45:21Z",
        "summary": "Business optimisation refers to the process of finding and implementing\nefficient and cost-effective means of operation to bring a competitive\nadvantage for businesses. Synthesizing problem formulations is an integral part\nof business optimisation, which relies on human expertise to construct problem\nformulations using optimisation languages. Interestingly, with advancements in\nLarge Language Models (LLMs), the human expertise needed in problem formulation\ncan be minimized. However, developing an LLM for problem formulation is\nchallenging, due to training data, token limitations, and lack of appropriate\nperformance metrics. For the requirement of training data, recent attention has\nbeen directed towards fine-tuning pre-trained LLMs for downstream tasks rather\nthan training an LLM from scratch for a specific task. In this paper, we adopt\nan LLM fine-tuning approach and propose an AI-Copilot for business optimisation\nproblem formulation. For token limitations, we introduce modularization and\nprompt engineering techniques to synthesize complex problem formulations as\nmodules that fit into the token limits of LLMs. Additionally, we design\nperformance evaluation metrics that are better suited for assessing the\naccuracy and quality of problem formulations. The experiment results\ndemonstrate that with this approach we can synthesize complex and large problem\nformulations for a typical business optimisation problem in production\nscheduling.",
        "pdf_link": "https://arxiv.org/pdf/2309.13218v3.pdf"
    },
    {
        "title": "Investigating Large Language Models and Control Mechanisms to Improve Text Readability of Biomedical Abstracts",
        "authors": [
            "Zihao Li",
            "Samuel Belkadi",
            "Nicolo Micheletti",
            "Lifeng Han",
            "Matthew Shardlow",
            "Goran Nenadic"
        ],
        "published": "2023-09-22T22:47:32Z",
        "summary": "Biomedical literature often uses complex language and inaccessible\nprofessional terminologies. That is why simplification plays an important role\nin improving public health literacy. Applying Natural Language Processing (NLP)\nmodels to automate such tasks allows for quick and direct accessibility for lay\nreaders. In this work, we investigate the ability of state-of-the-art large\nlanguage models (LLMs) on the task of biomedical abstract simplification, using\nthe publicly available dataset for plain language adaptation of biomedical\nabstracts (\\textbf{PLABA}). The methods applied include domain fine-tuning and\nprompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and\nBART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT,\nand 3) Control-token mechanisms on BART-based models. We used a range of\nautomatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and\nalso conducted human evaluations. BART-Large with Control Token (BART-L-w-CT)\nmechanisms reported the highest SARI score of 46.54 and T5-base reported the\nhighest BERTscore 72.62. In human evaluation, BART-L-w-CTs achieved a better\nsimplicity score over T5-Base (2.9 vs. 2.2), while T5-Base achieved a better\nmeaning preservation score over BART-L-w-CTs (3.1 vs. 2.6). We also categorised\nthe system outputs with examples, hoping this will shed some light for future\nresearch on this task. Our code, fine-tuned models, and data splits are\navailable at \\url{https://github.com/HECTA-UoM/PLABA-MU} \\begin{IEEEkeywords}\nLarge Language Models, Text Simplification, Biomedical NLP, Control Mechanisms,\nHealth Informatics \\end{IEEEkeywords}",
        "pdf_link": "https://arxiv.org/pdf/2309.13202v2.pdf"
    },
    {
        "title": "Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation",
        "authors": [
            "Kai Huang",
            "Hanyun Yin",
            "Heng Huang",
            "Wei Gao"
        ],
        "published": "2023-09-22T21:55:18Z",
        "summary": "Fine-tuning is the most effective way of adapting pre-trained large language\nmodels (LLMs) to downstream applications. With the fast growth of LLM-enabled\nAI applications and democratization of open-souced LLMs, fine-tuning has become\npossible for non-expert individuals, but intensively performed LLM fine-tuning\nworldwide could result in significantly high energy consumption and carbon\nfootprint, which may bring large environmental impact. Mitigating such\nenvironmental impact towards Green AI directly correlates to reducing the FLOPs\nof fine-tuning, but existing techniques on efficient LLM fine-tuning can only\nachieve limited reduction of such FLOPs, due to their ignorance of the\nbackpropagation cost in fine-tuning. To address this limitation, in this paper\nwe present GreenTrainer, a new LLM fine-tuning technique that adaptively\nevaluates different tensors' backpropagation costs and contributions to the\nfine-tuned model accuracy, to minimize the fine-tuning cost by selecting the\nmost appropriate set of tensors in training. Such selection in GreenTrainer is\nmade based on a given objective of FLOPs reduction, which can flexibly adapt to\nthe carbon footprint in energy supply and the need in Green AI. Experiment\nresults over multiple open-sourced LLM models and abstractive summarization\ndatasets show that, compared to fine-tuning the whole LLM model, GreenTrainer\ncan save up to 64% FLOPs in fine-tuning without any noticeable model accuracy\nloss. Compared to the existing fine-tuning techniques such as LoRa,\nGreenTrainer can achieve up to 4% improvement on model accuracy with on-par\nFLOPs reduction.",
        "pdf_link": "https://arxiv.org/pdf/2309.13192v2.pdf"
    },
    {
        "title": "BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP",
        "authors": [
            "Mohsinul Kabir",
            "Mohammed Saidul Islam",
            "Md Tahmid Rahman Laskar",
            "Mir Tafseer Nayeem",
            "M Saiful Bari",
            "Enamul Hoque"
        ],
        "published": "2023-09-22T20:29:34Z",
        "summary": "Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in NLP for their impressive skills in language generation and\nother language-specific tasks. Though LLMs have been evaluated in various\ntasks, mostly in English, they have not yet undergone thorough evaluation in\nunder-resourced languages such as Bengali (Bangla). To this end, this paper\nintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to\nbenchmark their performance in the Bengali language that has modest resources.\nIn this regard, we select various important and diverse Bengali NLP tasks, such\nas text summarization, question answering, paraphrasing, natural language\ninference, transliteration, text classification, and sentiment analysis for\nzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and\nClaude-2. Our experimental results demonstrate that while in some Bengali NLP\ntasks, zero-shot LLMs could achieve performance on par, or even better than\ncurrent SOTA fine-tuned models; in most tasks, their performance is quite poor\n(with the performance of open-source LLMs like LLaMA-2-13b-chat being\nsignificantly bad) in comparison to the current SOTA results. Therefore, it\ncalls for further efforts to develop a better understanding of LLMs in\nmodest-resourced languages like Bengali.",
        "pdf_link": "https://arxiv.org/pdf/2309.13173v2.pdf"
    },
    {
        "title": "Contextual Emotion Estimation from Image Captions",
        "authors": [
            "Vera Yang",
            "Archita Srivastava",
            "Yasaman Etesam",
            "Chuxuan Zhang",
            "Angelica Lim"
        ],
        "published": "2023-09-22T18:44:34Z",
        "summary": "Emotion estimation in images is a challenging task, typically using computer\nvision methods to directly estimate people's emotions using face, body pose and\ncontextual cues. In this paper, we explore whether Large Language Models (LLMs)\ncan support the contextual emotion estimation task, by first captioning images,\nthen using an LLM for inference. First, we must understand: how well do LLMs\nperceive human emotions? And which parts of the information enable them to\ndetermine emotions? One initial challenge is to construct a caption that\ndescribes a person within a scene with information relevant for emotion\nperception. Towards this goal, we propose a set of natural language descriptors\nfor faces, bodies, interactions, and environments. We use them to manually\ngenerate captions and emotion annotations for a subset of 331 images from the\nEMOTIC dataset. These captions offer an interpretable representation for\nemotion estimation, towards understanding how elements of a scene affect\nemotion perception in LLMs and beyond. Secondly, we test the capability of a\nlarge language model to infer an emotion from the resulting image captions. We\nfind that GPT-3.5, specifically the text-davinci-003 model, provides\nsurprisingly reasonable emotion predictions consistent with human annotations,\nbut accuracy can depend on the emotion concept. Overall, the results suggest\npromise in the image captioning and LLM approach.",
        "pdf_link": "https://arxiv.org/pdf/2309.13136v1.pdf"
    },
    {
        "title": "In-context Interference in Chat-based Large Language Models",
        "authors": [
            "Eric Nuertey Coleman",
            "Julio Hurtado",
            "Vincenzo Lomonaco"
        ],
        "published": "2023-09-22T09:18:55Z",
        "summary": "Large language models (LLMs) have had a huge impact on society due to their\nimpressive capabilities and vast knowledge of the world. Various applications\nand tools have been created that allow users to interact with these models in a\nblack-box scenario. However, one limitation of this scenario is that users\ncannot modify the internal knowledge of the model, and the only way to add or\nmodify internal knowledge is by explicitly mentioning it to the model during\nthe current interaction. This learning process is called in-context training,\nand it refers to training that is confined to the user's current session or\ncontext. In-context learning has significant applications, but also has\nlimitations that are seldom studied. In this paper, we present a study that\nshows how the model can suffer from interference between information that\ncontinually flows in the context, causing it to forget previously learned\nknowledge, which can reduce the model's performance. Along with showing the\nproblem, we propose an evaluation benchmark based on the bAbI dataset.",
        "pdf_link": "https://arxiv.org/pdf/2309.12727v1.pdf"
    },
    {
        "title": "Construction contract risk identification based on knowledge-augmented language model",
        "authors": [
            "Saika Wong",
            "Chunmo Zheng",
            "Xing Su",
            "Yinqiu Tang"
        ],
        "published": "2023-09-22T05:27:06Z",
        "summary": "Contract review is an essential step in construction projects to prevent\npotential losses. However, the current methods for reviewing construction\ncontracts lack effectiveness and reliability, leading to time-consuming and\nerror-prone processes. While large language models (LLMs) have shown promise in\nrevolutionizing natural language processing (NLP) tasks, they struggle with\ndomain-specific knowledge and addressing specialized issues. This paper\npresents a novel approach that leverages LLMs with construction contract\nknowledge to emulate the process of contract review by human experts. Our\ntuning-free approach incorporates construction contract domain knowledge to\nenhance language models for identifying construction contract risks. The use of\na natural language when building the domain knowledge base facilitates\npractical implementation. We evaluated our method on real construction\ncontracts and achieved solid performance. Additionally, we investigated how\nlarge language models employ logical thinking during the task and provide\ninsights and recommendations for future research.",
        "pdf_link": "https://arxiv.org/pdf/2309.12626v1.pdf"
    },
    {
        "title": "Studying and improving reasoning in humans and machines",
        "authors": [
            "Nicolas Yax",
            "Hernan Anll\u00f3",
            "Stefano Palminteri"
        ],
        "published": "2023-09-21T21:02:05Z",
        "summary": "In the present study, we investigate and compare reasoning in large language\nmodels (LLM) and humans using a selection of cognitive psychology tools\ntraditionally dedicated to the study of (bounded) rationality. To do so, we\npresented to human participants and an array of pretrained LLMs new variants of\nclassical cognitive experiments, and cross-compared their performances. Our\nresults showed that most of the included models presented reasoning errors akin\nto those frequently ascribed to error-prone, heuristic-based human reasoning.\nNotwithstanding this superficial similarity, an in-depth comparison between\nhumans and LLMs indicated important differences with human-like reasoning, with\nmodels limitations disappearing almost entirely in more recent LLMs releases.\nMoreover, we show that while it is possible to devise strategies to induce\nbetter performance, humans and machines are not equally-responsive to the same\nprompting schemes. We conclude by discussing the epistemological implications\nand challenges of comparing human and machine behavior for both artificial\nintelligence and cognitive psychology.",
        "pdf_link": "https://arxiv.org/pdf/2309.12485v1.pdf"
    },
    {
        "title": "Foundation Metrics for Evaluating Effectiveness of Healthcare Conversations Powered by Generative AI",
        "authors": [
            "Mahyar Abbasian",
            "Elahe Khatibi",
            "Iman Azimi",
            "David Oniani",
            "Zahra Shakeri Hossein Abad",
            "Alexander Thieme",
            "Ram Sriram",
            "Zhongqi Yang",
            "Yanshan Wang",
            "Bryant Lin",
            "Olivier Gevaert",
            "Li-Jia Li",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "published": "2023-09-21T19:36:48Z",
        "summary": "Generative Artificial Intelligence is set to revolutionize healthcare\ndelivery by transforming traditional patient care into a more personalized,\nefficient, and proactive process. Chatbots, serving as interactive\nconversational models, will probably drive this patient-centered transformation\nin healthcare. Through the provision of various services, including diagnosis,\npersonalized lifestyle recommendations, and mental health support, the\nobjective is to substantially augment patient health outcomes, all the while\nmitigating the workload burden on healthcare providers. The life-critical\nnature of healthcare applications necessitates establishing a unified and\ncomprehensive set of evaluation metrics for conversational models. Existing\nevaluation metrics proposed for various generic large language models (LLMs)\ndemonstrate a lack of comprehension regarding medical and health concepts and\ntheir significance in promoting patients' well-being. Moreover, these metrics\nneglect pivotal user-centered aspects, including trust-building, ethics,\npersonalization, empathy, user comprehension, and emotional support. The\npurpose of this paper is to explore state-of-the-art LLM-based evaluation\nmetrics that are specifically applicable to the assessment of interactive\nconversational models in healthcare. Subsequently, we present an comprehensive\nset of evaluation metrics designed to thoroughly assess the performance of\nhealthcare chatbots from an end-user perspective. These metrics encompass an\nevaluation of language processing abilities, impact on real-world clinical\ntasks, and effectiveness in user-interactive conversations. Finally, we engage\nin a discussion concerning the challenges associated with defining and\nimplementing these metrics, with particular emphasis on confounding factors\nsuch as the target audience, evaluation methods, and prompt techniques involved\nin the evaluation process.",
        "pdf_link": "https://arxiv.org/pdf/2309.12444v3.pdf"
    },
    {
        "title": "Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges",
        "authors": [
            "Vinay Samuel",
            "Houda Aynaou",
            "Arijit Ghosh Chowdhury",
            "Karthik Venkat Ramanan",
            "Aman Chadha"
        ],
        "published": "2023-09-21T18:48:02Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive zero shot\nperformance on a wide range of NLP tasks, demonstrating the ability to reason\nand apply commonsense. A relevant application is to use them for creating high\nquality synthetic datasets for downstream tasks. In this work, we probe whether\nGPT-4 can be used to augment existing extractive reading comprehension\ndatasets. Automating data annotation processes has the potential to save large\namounts of time, money and effort that goes into manually labelling datasets.\nIn this paper, we evaluate the performance of GPT-4 as a replacement for human\nannotators for low resource reading comprehension tasks, by comparing\nperformance after fine tuning, and the cost associated with annotation. This\nwork serves to be the first analysis of LLMs as synthetic data augmenters for\nQA systems, highlighting the unique opportunities and challenges. Additionally,\nwe release augmented versions of low resource datasets, that will allow the\nresearch community to create further benchmarks for evaluation of generated\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2309.12426v1.pdf"
    },
    {
        "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent",
        "authors": [
            "Jianing Yang",
            "Xuweiyi Chen",
            "Shengyi Qian",
            "Nikhil Madaan",
            "Madhavan Iyengar",
            "David F. Fouhey",
            "Joyce Chai"
        ],
        "published": "2023-09-21T17:59:45Z",
        "summary": "3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ .",
        "pdf_link": "https://arxiv.org/pdf/2309.12311v1.pdf"
    },
    {
        "title": "Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models",
        "authors": [
            "Levon Haroutunian",
            "Zhuang Li",
            "Lucian Galescu",
            "Philip Cohen",
            "Raj Tumuluri",
            "Gholamreza Haffari"
        ],
        "published": "2023-09-21T17:54:58Z",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language generation. However, their output quality can be inconsistent,\nposing challenges for generating natural language from logical forms (LFs).\nThis task requires the generated outputs to embody the exact semantics of LFs,\nwithout missing any LF semantics or creating any hallucinations. In this work,\nwe tackle this issue by proposing a novel generate-and-rerank approach. Our\napproach involves initially generating a set of candidate outputs by prompting\nan LLM and subsequently reranking them using a task-specific reranker model. In\naddition, we curate a manually collected dataset to evaluate the alignment\nbetween different ranking metrics and human judgements. The chosen ranking\nmetrics are utilized to enhance the training and evaluation of the reranker\nmodel. By conducting extensive experiments on three diverse datasets, we\ndemonstrate that the candidates selected by our reranker outperform those\nselected by baseline methods in terms of semantic consistency and fluency, as\nmeasured by three comprehensive metrics. Our findings provide strong evidence\nfor the effectiveness of our approach in improving the quality of generated\noutputs.",
        "pdf_link": "https://arxiv.org/pdf/2309.12294v1.pdf"
    },
    {
        "title": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"",
        "authors": [
            "Lukas Berglund",
            "Meg Tong",
            "Max Kaufmann",
            "Mikita Balesni",
            "Asa Cooper Stickland",
            "Tomasz Korbak",
            "Owain Evans"
        ],
        "published": "2023-09-21T17:52:19Z",
        "summary": "We expose a surprising failure of generalization in auto-regressive large\nlanguage models (LLMs). If a model is trained on a sentence of the form \"A is\nB\", it will not automatically generalize to the reverse direction \"B is A\".\nThis is the Reversal Curse. For instance, if a model is trained on \"Valentina\nTereshkova was the first woman to travel to space\", it will not automatically\nbe able to answer the question, \"Who was the first woman to travel to space?\".\nMoreover, the likelihood of the correct answer (\"Valentina Tershkova\") will not\nbe higher than for a random name. Thus, models do not generalize a prevalent\npattern in their training set: if \"A is B\" occurs, \"B is A\" is more likely to\noccur. It is worth noting, however, that if \"A is B\" appears in-context, models\ncan deduce the reverse relationship. We provide evidence for the Reversal Curse\nby finetuning GPT-3 and Llama-1 on fictitious statements such as \"Uriah\nHawthorne is the composer of Abyssal Melodies\" and showing that they fail to\ncorrectly answer \"Who composed Abyssal Melodies?\". The Reversal Curse is robust\nacross model sizes and model families and is not alleviated by data\naugmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about\nreal-world celebrities, such as \"Who is Tom Cruise's mother? [A: Mary Lee\nPfeiffer]\" and the reverse \"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly\nanswers questions like the former 79% of the time, compared to 33% for the\nlatter.\n  Code available at: https://github.com/lukasberglund/reversal_curse.",
        "pdf_link": "https://arxiv.org/pdf/2309.12288v3.pdf"
    },
    {
        "title": "Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition",
        "authors": [
            "Junyi Bian",
            "Jiaxuan Zheng",
            "Yuyi Zhang",
            "Shanfeng Zhu"
        ],
        "published": "2023-09-21T17:39:53Z",
        "summary": "Large language models (LLMs) have demonstrated dominating performance in many\nNLP tasks, especially on generative tasks. However, they often fall short in\nsome information extraction tasks, particularly those requiring domain-specific\nknowledge, such as Biomedical Named Entity Recognition (NER). In this paper,\ninspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER\nstep-by-step: break down the NER task into entity span extraction and entity\ntype determination. Additionally, for entity type determination, we inject\nentity knowledge to address the problem that LLM's lack of domain knowledge\nwhen predicting entity category. Experimental results show a significant\nimprovement in our two-step BioNER approach compared to previous few-shot LLM\nbaseline. Additionally, the incorporation of external knowledge significantly\nenhances entity category determination performance.",
        "pdf_link": "https://arxiv.org/pdf/2309.12278v1.pdf"
    },
    {
        "title": "Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",
        "authors": [
            "Beizhe Hu",
            "Qiang Sheng",
            "Juan Cao",
            "Yuhui Shi",
            "Yang Li",
            "Danding Wang",
            "Peng Qi"
        ],
        "published": "2023-09-21T16:47:30Z",
        "summary": "Detecting fake news requires both a delicate sense of diverse clues and a\nprofound understanding of the real-world background, which remains challenging\nfor detectors based on small language models (SLMs) due to their knowledge and\ncapability limitations. Recent advances in large language models (LLMs) have\nshown remarkable performance in various tasks, but whether and how LLMs could\nhelp with fake news detection remains underexplored. In this paper, we\ninvestigate the potential of LLMs in fake news detection. First, we conduct an\nempirical study and find that a sophisticated LLM such as GPT 3.5 could\ngenerally expose fake news and provide desirable multi-perspective rationales\nbut still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis\nattributes such a gap to the LLM's inability to select and integrate rationales\nproperly to conclude. Based on these findings, we propose that current LLMs may\nnot substitute fine-tuned SLMs in fake news detection but can be a good advisor\nfor SLMs by providing multi-perspective instructive rationales. To instantiate\nthis proposal, we design an adaptive rationale guidance network for fake news\ndetection (ARG), in which SLMs selectively acquire insights on news analysis\nfrom the LLMs' rationales. We further derive a rationale-free version of ARG by\ndistillation, namely ARG-D, which services cost-sensitive scenarios without\nquerying LLMs. Experiments on two real-world datasets demonstrate that ARG and\nARG-D outperform three types of baseline methods, including SLM-based,\nLLM-based, and combinations of small and large language models.",
        "pdf_link": "https://arxiv.org/pdf/2309.12247v2.pdf"
    },
    {
        "title": "Code Soliloquies for Accurate Calculations in Large Language Models",
        "authors": [
            "Shashank Sonkar",
            "MyCo Le",
            "Xinghe Chen",
            "Naiming Liu",
            "Debshila Basu Mallick",
            "Richard G. Baraniuk"
        ],
        "published": "2023-09-21T15:16:58Z",
        "summary": "High-quality conversational datasets are crucial for the successful\ndevelopment of Intelligent Tutoring Systems (ITS) that utilize a Large Language\nModel (LLM) backend. Synthetic student-teacher dialogues, generated using\nadvanced GPT-4 models, are a common strategy for creating these datasets.\nHowever, subjects like physics that entail complex calculations pose a\nchallenge. While GPT-4 presents impressive language processing capabilities,\nits limitations in fundamental mathematical reasoning curtail its efficacy for\nsuch subjects. To tackle this limitation, we introduce in this paper an\ninnovative stateful prompt design. Our design orchestrates a mock conversation\nwhere both student and tutorbot roles are simulated by GPT-4. Each student\nresponse triggers an internal monologue, or `code soliloquy' in the\nGPT-tutorbot, which assesses whether its subsequent response would necessitate\ncalculations. If a calculation is deemed necessary, it scripts the relevant\nPython code and uses the Python output to construct a response to the student.\nOur approach notably enhances the quality of synthetic conversation datasets,\nespecially for subjects that are calculation-intensive. Our preliminary Subject\nMatter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA\nmodel, effectively uses Python for computations, which significantly enhances\nthe accuracy and computational reliability of Higgs' responses. Code, models,\nand datasets is available at https://github.com/luffycodes/Tutorbot-Spock-Phys.",
        "pdf_link": "https://arxiv.org/pdf/2309.12161v2.pdf"
    },
    {
        "title": "A knowledge representation approach for construction contract knowledge modeling",
        "authors": [
            "Chunmo Zheng",
            "Saika Wong",
            "Xing Su",
            "Yinqiu Tang"
        ],
        "published": "2023-09-21T14:53:36Z",
        "summary": "The emergence of large language models (LLMs) presents an unprecedented\nopportunity to automate construction contract management, reducing human errors\nand saving significant time and costs. However, LLMs may produce convincing yet\ninaccurate and misleading content due to a lack of domain expertise. To address\nthis issue, expert-driven contract knowledge can be represented in a structured\nmanner to constrain the automatic contract management process. This paper\nintroduces the Nested Contract Knowledge Graph (NCKG), a knowledge\nrepresentation approach that captures the complexity of contract knowledge\nusing a nested structure. It includes a nested knowledge representation\nframework, a NCKG ontology built on the framework, and an implementation\nmethod. Furthermore, we present the LLM-assisted contract review pipeline\nenhanced with external knowledge in NCKG. Our pipeline achieves a promising\nperformance in contract risk reviewing, shedding light on the combination of\nLLM and KG towards more reliable and interpretable contract management.",
        "pdf_link": "https://arxiv.org/pdf/2309.12132v1.pdf"
    },
    {
        "title": "Prompt Tuned Embedding Classification for Multi-Label Industry Sector Allocation",
        "authors": [
            "Valentin Leonhard Buchner",
            "Lele Cao",
            "Jan-Christoph Kalo",
            "Vilhelm von Ehrenheim"
        ],
        "published": "2023-09-21T13:45:32Z",
        "summary": "Prompt Tuning is emerging as a scalable and cost-effective method to\nfine-tune Pretrained Language Models (PLMs), which are often referred to as\nLarge Language Models (LLMs). This study benchmarks the performance and\ncomputational efficiency of Prompt Tuning and baselines for multi-label text\nclassification. This is applied to the challenging task of classifying\ncompanies into an investment firm's proprietary industry taxonomy, supporting\ntheir thematic investment strategy. Text-to-text classification is frequently\nreported to outperform task-specific classification heads, but has several\nlimitations when applied to a multi-label classification problem where each\nlabel consists of multiple tokens: (a) Generated labels may not match any label\nin the label taxonomy; (b) The fine-tuning process lacks permutation invariance\nand is sensitive to the order of the provided labels; (c) The model provides\nbinary decisions rather than appropriate confidence scores. Limitation (a) is\naddressed by applying constrained decoding using Trie Search, which slightly\nimproves classification performance. All limitations (a), (b), and (c) are\naddressed by replacing the PLM's language head with a classification head,\nwhich is referred to as Prompt Tuned Embedding Classification (PTEC). This\nimproves performance significantly, while also reducing computational costs\nduring inference. In our industrial application, the training data is skewed\ntowards well-known companies. We confirm that the model's performance is\nconsistent across both well-known and less-known companies. Our overall results\nindicate the continuing need to adapt state-of-the-art methods to\ndomain-specific tasks, even in the era of PLMs with strong generalization\nabilities. We release our codebase and a benchmarking dataset at\nhttps://github.com/EQTPartners/PTEC.",
        "pdf_link": "https://arxiv.org/pdf/2309.12075v2.pdf"
    },
    {
        "title": "AceGPT, Localizing Large Language Models in Arabic",
        "authors": [
            "Huang Huang",
            "Fei Yu",
            "Jianqing Zhu",
            "Xuening Sun",
            "Hao Cheng",
            "Dingjie Song",
            "Zhihong Chen",
            "Abdulmohsen Alharthi",
            "Bang An",
            "Juncai He",
            "Ziche Liu",
            "Zhiyi Zhang",
            "Junying Chen",
            "Jianquan Li",
            "Benyou Wang",
            "Lian Zhang",
            "Ruoyu Sun",
            "Xiang Wan",
            "Haizhou Li",
            "Jinchao Xu"
        ],
        "published": "2023-09-21T13:20:13Z",
        "summary": "This paper is devoted to the development of a localized Large Language Model\n(LLM) specifically for Arabic, a language imbued with unique cultural\ncharacteristics inadequately addressed by current mainstream models.\nSignificant concerns emerge when addressing cultural sensitivity and local\nvalues. To address this, the paper proposes a comprehensive solution that\nincludes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT)\nutilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside\nReinforcement Learning with AI Feedback (RLAIF) employing a reward model\nattuned to local culture and values. The goal is to cultivate culturally\ncognizant and value-aligned Arabic LLMs capable of accommodating the diverse,\napplication-specific needs of Arabic-speaking communities.\n  Comprehensive evaluations reveal that the resulting model, dubbed `AceGPT',\nsets the state-of-the-art standard for open Arabic LLMs across various\nbenchmarks. Codes, data, and models are in\nhttps://github.com/FreedomIntelligence/AceGPT.",
        "pdf_link": "https://arxiv.org/pdf/2309.12053v5.pdf"
    },
    {
        "title": "Knowledge Sanitization of Large Language Models",
        "authors": [
            "Yoichi Ishibashi",
            "Hidetoshi Shimodaira"
        ],
        "published": "2023-09-21T07:49:55Z",
        "summary": "We explore a knowledge sanitization approach to mitigate the privacy concerns\nassociated with large language models (LLMs). LLMs trained on a large corpus of\nWeb data can memorize and potentially reveal sensitive or confidential\ninformation, raising critical security concerns. Our technique efficiently\nfine-tunes these models using the Low-Rank Adaptation (LoRA) method, prompting\nthem to generate harmless responses such as ``I don't know'' when queried about\nspecific information. Experimental results in a closed-book question-answering\ntask show that our straightforward method not only minimizes particular\nknowledge leakage but also preserves the overall performance of LLMs. These two\nadvantages strengthen the defense against extraction attacks and reduces the\nemission of harmful content such as hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2309.11852v2.pdf"
    },
    {
        "title": "Goal-Oriented Prompt Attack and Safety Evaluation for LLMs",
        "authors": [
            "Chengyuan Liu",
            "Fubang Zhao",
            "Lizhi Qing",
            "Yangyang Kang",
            "Changlong Sun",
            "Kun Kuang",
            "Fei Wu"
        ],
        "published": "2023-09-21T07:07:49Z",
        "summary": "Large Language Models (LLMs) presents significant priority in text\nunderstanding and generation. However, LLMs suffer from the risk of generating\nharmful contents especially while being employed to applications. There are\nseveral black-box attack methods, such as Prompt Attack, which can change the\nbehaviour of LLMs and induce LLMs to generate unexpected answers with harmful\ncontents. Researchers are interested in Prompt Attack and Defense with LLMs,\nwhile there is no publicly available dataset with high successful attacking\nrate to evaluate the abilities of defending prompt attack. In this paper, we\nintroduce a pipeline to construct high-quality prompt attack samples, along\nwith a Chinese prompt attack dataset called CPAD. Our prompts aim to induce\nLLMs to generate unexpected outputs with several carefully designed prompt\nattack templates and widely concerned attacking contents. Different from\nprevious datasets involving safety estimation, we construct the prompts\nconsidering three dimensions: contents, attacking methods and goals.\nEspecially, the attacking goals indicate the behaviour expected after\nsuccessfully attacking the LLMs, thus the responses can be easily evaluated and\nanalysed. We run several popular Chinese LLMs on our dataset, and the results\nshow that our prompts are significantly harmful to LLMs, with around 70% attack\nsuccess rate to GPT-3.5. CPAD is publicly available at\nhttps://github.com/liuchengyuan123/CPAD.",
        "pdf_link": "https://arxiv.org/pdf/2309.11830v2.pdf"
    },
    {
        "title": "LPML: LLM-Prompting Markup Language for Mathematical Reasoning",
        "authors": [
            "Ryutaro Yamauchi",
            "Sho Sonoda",
            "Akiyoshi Sannai",
            "Wataru Kumagai"
        ],
        "published": "2023-09-21T02:46:20Z",
        "summary": "In utilizing large language models (LLMs) for mathematical reasoning,\naddressing the errors in the reasoning and calculation present in the generated\ntext by LLMs is a crucial challenge. In this paper, we propose a novel\nframework that integrates the Chain-of-Thought (CoT) method with an external\ntool (Python REPL). We discovered that by prompting LLMs to generate structured\ntext in XML-like markup language, we could seamlessly integrate CoT and the\nexternal tool and control the undesired behaviors of LLMs. With our approach,\nLLMs can utilize Python computation to rectify errors within CoT. We applied\nour method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and\ndemonstrated that combining CoT and Python REPL through the markup language\nenhances the reasoning capability of LLMs. Our approach enables LLMs to write\nthe markup language and perform advanced mathematical reasoning using only\nzero-shot prompting.",
        "pdf_link": "https://arxiv.org/pdf/2309.13078v2.pdf"
    },
    {
        "title": "LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination",
        "authors": [
            "Kai Zhang",
            "Yangyang Kang",
            "Fubang Zhao",
            "Xiaozhong Liu"
        ],
        "published": "2023-09-21T00:34:33Z",
        "summary": "Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable\nproficiency in comprehending and generating natural language. On the other\nhand, medical assistants hold the potential to offer substantial benefits for\nindividuals. However, the exploration of LLM-based personalized medical\nassistant remains relatively scarce. Typically, patients converse differently\nbased on their background and preferences which necessitates the task of\nenhancing user-oriented medical assistant. While one can fully train an LLM for\nthis objective, the resource consumption is unaffordable. Prior research has\nexplored memory-based methods to enhance the response with aware of previous\nmistakes for new queries during a dialogue session. We contend that a mere\nmemory module is inadequate and fully training an LLM can be excessively\ncostly. In this study, we propose a novel computational bionic memory\nmechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to\npersonalize medical assistants.",
        "pdf_link": "https://arxiv.org/pdf/2309.11696v3.pdf"
    },
    {
        "title": "LLM Guided Inductive Inference for Solving Compositional Problems",
        "authors": [
            "Abhigya Sodani",
            "Lauren Moos",
            "Matthew Mirman"
        ],
        "published": "2023-09-20T23:44:16Z",
        "summary": "While large language models (LLMs) have demonstrated impressive performance\nin question-answering tasks, their performance is limited when the questions\nrequire knowledge that is not included in the model's training data and can\nonly be acquired through direct observation or interaction with the real world.\nExisting methods decompose reasoning tasks through the use of modules invoked\nsequentially, limiting their ability to answer deep reasoning tasks. We\nintroduce a method, Recursion based extensible LLM (REBEL), which handles\nopen-world, deep reasoning tasks by employing automated reasoning techniques\nlike dynamic planning and forward-chaining strategies. REBEL allows LLMs to\nreason via recursive problem decomposition and utilization of external tools.\nThe tools that REBEL uses are specified only by natural language description.\nWe further demonstrate REBEL capabilities on a set of problems that require a\ndeeply nested use of external tools in a compositional and conversational\nsetting.",
        "pdf_link": "https://arxiv.org/pdf/2309.11688v1.pdf"
    },
    {
        "title": "Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation",
        "authors": [
            "Ali Mousavi",
            "Xin Zhan",
            "He Bai",
            "Peng Shi",
            "Theo Rekatsinas",
            "Benjamin Han",
            "Yunyao Li",
            "Jeff Pound",
            "Josh Susskind",
            "Natalie Schluter",
            "Ihab Ilyas",
            "Navdeep Jaitly"
        ],
        "published": "2023-09-20T22:30:20Z",
        "summary": "Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used\nto train forward and reverse neural models that generate text from KG and vice\nversa. However models trained on datasets where KG and text pairs are not\nequivalent can suffer from more hallucination and poorer recall. In this paper,\nwe verify this empirically by generating datasets with different levels of\nnoise and find that noisier datasets do indeed lead to more hallucination. We\nargue that the ability of forward and reverse models trained on a dataset to\ncyclically regenerate source KG or text is a proxy for the equivalence between\nthe KG and the text in the dataset. Using cyclic evaluation we find that\nmanually created WebNLG is much better than automatically created TeKGen and\nT-REx. Guided by these observations, we construct a new, improved dataset\ncalled LAGRANGE using heuristics meant to improve equivalence between KG and\ntext and show the impact of each of the heuristics on cyclic evaluation. We\nalso construct two synthetic datasets using large language models (LLMs), and\nobserve that these are conducive to models that perform significantly well on\ncyclic generation of text, but less so on cyclic generation of KGs, probably\nbecause of a lack of a consistent underlying ontology.",
        "pdf_link": "https://arxiv.org/pdf/2309.11669v1.pdf"
    },
    {
        "title": "Towards Effective Disambiguation for Machine Translation with Large Language Models",
        "authors": [
            "Vivek Iyer",
            "Pinzhen Chen",
            "Alexandra Birch"
        ],
        "published": "2023-09-20T22:22:52Z",
        "summary": "Resolving semantic ambiguity has long been recognised as a central challenge\nin the field of Machine Translation. Recent work on benchmarking translation\nperformance on ambiguous sentences has exposed the limitations of conventional\nNeural Machine Translation (NMT) systems, which fail to handle many such cases.\nLarge language models (LLMs) have emerged as a promising alternative,\ndemonstrating comparable performance to traditional NMT models while\nintroducing new paradigms for controlling the target outputs. In this paper, we\nstudy the capabilities of LLMs to translate \"ambiguous sentences\" - i.e. those\ncontaining highly polysemous words and/or rare word senses. We also propose two\nways to improve their disambiguation capabilities, through a) in-context\nlearning and b) fine-tuning on carefully curated ambiguous datasets.\nExperiments show that our methods can match or outperform state-of-the-art\nsystems such as DeepL and NLLB in four out of five language directions. Our\nresearch provides valuable insights into effectively adapting LLMs to become\nbetter disambiguators during Machine Translation. We release our curated\ndisambiguation corpora and resources at\nhttps://data.statmt.org/ambiguous-europarl.",
        "pdf_link": "https://arxiv.org/pdf/2309.11668v2.pdf"
    },
    {
        "title": "\"It's a Fair Game\", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",
        "authors": [
            "Zhiping Zhang",
            "Michelle Jia",
            "Hao-Ping Lee",
            "Bingsheng Yao",
            "Sauvik Das",
            "Ada Lerner",
            "Dakuo Wang",
            "Tianshi Li"
        ],
        "published": "2023-09-20T21:34:36Z",
        "summary": "The widespread use of Large Language Model (LLM)-based conversational agents\n(CAs), especially in high-stakes domains, raises many privacy concerns.\nBuilding ethical LLM-based CAs that respect user privacy requires an in-depth\nunderstanding of the privacy risks that concern users the most. However,\nexisting research, primarily model-centered, does not provide insight into\nusers' perspectives. To bridge this gap, we analyzed sensitive disclosures in\nreal-world ChatGPT conversations and conducted semi-structured interviews with\n19 LLM-based CA users. We found that users are constantly faced with trade-offs\nbetween privacy, utility, and convenience when using LLM-based CAs. However,\nusers' erroneous mental models and the dark patterns in system design limited\ntheir awareness and comprehension of the privacy risks. Additionally, the\nhuman-like interactions encouraged more sensitive disclosures, which\ncomplicated users' ability to navigate the trade-offs. We discuss practical\ndesign guidelines and the needs for paradigm shifts to protect the privacy of\nLLM-based CA users.",
        "pdf_link": "https://arxiv.org/pdf/2309.11653v2.pdf"
    },
    {
        "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "authors": [
            "Shehzaad Dhuliawala",
            "Mojtaba Komeili",
            "Jing Xu",
            "Roberta Raileanu",
            "Xian Li",
            "Asli Celikyilmaz",
            "Jason Weston"
        ],
        "published": "2023-09-20T17:50:55Z",
        "summary": "Generation of plausible yet incorrect factual information, termed\nhallucination, is an unsolved issue in large language models. We study the\nability of language models to deliberate on the responses they give in order to\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\nwhereby the model first (i) drafts an initial response; then (ii) plans\nverification questions to fact-check its draft; (iii) answers those questions\nindependently so the answers are not biased by other responses; and (iv)\ngenerates its final verified response. In experiments, we show CoVe decreases\nhallucinations across a variety of tasks, from list-based questions from\nWikidata, closed book MultiSpanQA and longform text generation.",
        "pdf_link": "https://arxiv.org/pdf/2309.11495v2.pdf"
    },
    {
        "title": "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning",
        "authors": [
            "Tianbao Xie",
            "Siheng Zhao",
            "Chen Henry Wu",
            "Yitao Liu",
            "Qian Luo",
            "Victor Zhong",
            "Yanchao Yang",
            "Tao Yu"
        ],
        "published": "2023-09-20T17:39:13Z",
        "summary": "Designing reward functions is a longstanding challenge in reinforcement\nlearning (RL); it requires specialized knowledge or domain data, leading to\nhigh costs for development. To address this, we introduce Text2Reward, a\ndata-free framework that automates the generation of dense reward functions\nbased on large language models (LLMs). Given a goal described in natural\nlanguage, Text2Reward generates dense reward functions as an executable program\ngrounded in a compact representation of the environment. Unlike inverse RL and\nrecent work that uses LLMs to write sparse reward codes, Text2Reward produces\ninterpretable, free-form dense reward codes that cover a wide range of tasks,\nutilize existing packages, and allow iterative refinement with human feedback.\nWe evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,\nMetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17\nmanipulation tasks, policies trained with generated reward codes achieve\nsimilar or better task success rates and convergence speed than expert-written\nreward codes. For locomotion tasks, our method learns six novel locomotion\nbehaviors with a success rate exceeding 94%. Furthermore, we show that the\npolicies trained in the simulator with our method can be deployed in the real\nworld. Finally, Text2Reward further improves the policies by refining their\nreward functions with human feedback. Video results are available at\nhttps://text-to-reward.github.io",
        "pdf_link": "https://arxiv.org/pdf/2309.11489v2.pdf"
    },
    {
        "title": "GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction",
        "authors": [
            "Suryanarayanan Balaji",
            "Rishikesh Magar",
            "Yayati Jadhav",
            "Amir Barati Farimani"
        ],
        "published": "2023-09-20T17:21:43Z",
        "summary": "With the emergence of Transformer architectures and their powerful\nunderstanding of textual data, a new horizon has opened up to predict the\nmolecular properties based on text description. While SMILES are the most\ncommon form of representation, they are lacking robustness, rich information\nand canonicity, which limit their effectiveness in becoming generalizable\nrepresentations. Here, we present GPT-MolBERTa, a self-supervised large\nlanguage model (LLM) which uses detailed textual descriptions of molecules to\npredict their properties. A text based description of 326000 molecules were\ncollected using ChatGPT and used to train LLM to learn the representation of\nmolecules. To predict the properties for the downstream tasks, both BERT and\nRoBERTa models were used in the finetuning stage. Experiments show that\nGPT-MolBERTa performs well on various molecule property benchmarks, and\napproaching state of the art performance in regression tasks. Additionally,\nfurther analysis of the attention mechanisms show that GPT-MolBERTa is able to\npick up important information from the input textual data, displaying the\ninterpretability of the model.",
        "pdf_link": "https://arxiv.org/pdf/2310.03030v3.pdf"
    },
    {
        "title": "Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction",
        "authors": [
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "published": "2023-09-20T16:14:10Z",
        "summary": "In Grammatical Error Correction (GEC), it is crucial to ensure the user's\ncomprehension of a reason for correction. Existing studies present tokens,\nexamples, and hints as to the basis for correction but do not directly explain\nthe reasons for corrections. Although methods that use Large Language Models\n(LLMs) to provide direct explanations in natural language have been proposed\nfor various tasks, no such method exists for GEC. Generating explanations for\nGEC corrections involves aligning input and output tokens, identifying\ncorrection points, and presenting corresponding explanations consistently.\nHowever, it is not straightforward to specify a complex format to generate\nexplanations, because explicit control of generation is difficult with prompts.\nThis study introduces a method called controlled generation with Prompt\nInsertion (PI) so that LLMs can explain the reasons for corrections in natural\nlanguage. In PI, LLMs first correct the input text, and then we automatically\nextract the correction points based on the rules. The extracted correction\npoints are sequentially inserted into the LLM's explanation output as prompts,\nguiding the LLMs to generate explanations for the correction points. We also\ncreate an Explainable GEC (XGEC) dataset of correction reasons by annotating\nNUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT\nusing original prompts miss some correction points, the generation control\nusing PI can explicitly guide to describe explanations for all correction\npoints, contributing to improved performance in generating correction reasons.",
        "pdf_link": "https://arxiv.org/pdf/2309.11439v1.pdf"
    },
    {
        "title": "Speak While You Think: Streaming Speech Synthesis During Text Generation",
        "authors": [
            "Avihu Dekel",
            "Slava Shechtman",
            "Raul Fernandez",
            "David Haws",
            "Zvi Kons",
            "Ron Hoory"
        ],
        "published": "2023-09-20T11:00:15Z",
        "summary": "Large Language Models (LLMs) demonstrate impressive capabilities, yet\ninteraction with these models is mostly facilitated through text. Using\nText-To-Speech to synthesize LLM outputs typically results in notable latency,\nwhich is impractical for fluent voice conversations. We propose LLM2Speech, an\narchitecture to synthesize speech while text is being generated by an LLM which\nyields significant latency reduction. LLM2Speech mimics the predictions of a\nnon-streaming teacher model while limiting the exposure to future context in\norder to enable streaming. It exploits the hidden embeddings of the LLM, a\nby-product of the text generation that contains informative semantic context.\nExperimental results show that LLM2Speech maintains the teacher's quality while\nreducing the latency to enable natural conversations.",
        "pdf_link": "https://arxiv.org/pdf/2309.11210v1.pdf"
    },
    {
        "title": "Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering",
        "authors": [
            "Yike Wu",
            "Nan Hu",
            "Sheng Bi",
            "Guilin Qi",
            "Jie Ren",
            "Anhuan Xie",
            "Wei Song"
        ],
        "published": "2023-09-20T10:42:08Z",
        "summary": "Despite their competitive performance on knowledge-intensive tasks, large\nlanguage models (LLMs) still have limitations in memorizing all world knowledge\nespecially long tail knowledge. In this paper, we study the KG-augmented\nlanguage model approach for solving the knowledge graph question answering\n(KGQA) task that requires rich world knowledge. Existing work has shown that\nretrieving KG knowledge to enhance LLMs prompting can significantly improve\nLLMs performance in KGQA. However, their approaches lack a well-formed\nverbalization of KG knowledge, i.e., they ignore the gap between KG\nrepresentations and textual representations. To this end, we propose an\nanswer-sensitive KG-to-Text approach that can transform KG knowledge into\nwell-textualized statements most informative for KGQA. Based on this approach,\nwe propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.\nExperiments on several KGQA benchmarks show that the proposed KG-to-Text\naugmented LLMs approach outperforms previous KG-augmented LLMs approaches\nregarding answer accuracy and usefulness of knowledge statements.",
        "pdf_link": "https://arxiv.org/pdf/2309.11206v2.pdf"
    },
    {
        "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
        "authors": [
            "Haoyu Wang",
            "Guozheng Ma",
            "Cong Yu",
            "Ning Gui",
            "Linrui Zhang",
            "Zhiqi Huang",
            "Suwei Ma",
            "Yongzhe Chang",
            "Sen Zhang",
            "Li Shen",
            "Xueqian Wang",
            "Peilin Zhao",
            "Dacheng Tao"
        ],
        "published": "2023-09-20T09:23:46Z",
        "summary": "The swift advancement in the scales and capabilities of Large Language Models\n(LLMs) positions them as promising tools for a variety of downstream tasks. In\naddition to the pursuit of better performance and the avoidance of violent\nfeedback on a certain prompt, to ensure the responsibility of the LLM, much\nattention is drawn to the robustness of LLMs. However, existing evaluation\nmethods mostly rely on traditional question answering datasets with predefined\nsupervised labels, which do not align with the superior generation capabilities\nof contemporary LLMs. To address this issue, we propose a novel rational\nevaluation approach that leverages pre-trained reward models as diagnostic\ntools to evaluate the longer conversation generated from more challenging open\nquestions by LLMs, which we refer to as the Reward Model for Reasonable\nRobustness Evaluation (TREvaL). Longer conversations manifest the comprehensive\ngrasp of language models in terms of their proficiency in understanding\nquestions, a capability not entirely encompassed by individual words or\nletters, which may exhibit oversimplification and inherent biases. Our\nextensive empirical experiments demonstrate that TREvaL provides an innovative\nmethod for evaluating the robustness of an LLM. Furthermore, our results\ndemonstrate that LLMs frequently exhibit vulnerability to word-level\nperturbations that are commonplace in daily language usage. Notably, we are\nsurprised to discover that robustness tends to decrease as fine-tuning (SFT and\nRLHF) is conducted. The code of TREval is available in\nhttps://github.com/Harry-mic/TREvaL.",
        "pdf_link": "https://arxiv.org/pdf/2309.11166v2.pdf"
    },
    {
        "title": "Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness",
        "authors": [
            "Vipula Rawte",
            "Prachi Priya",
            "S. M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023-09-20T05:04:16Z",
        "summary": "As Large Language Models (LLMs) have advanced, they have brought forth new\nchallenges, with one of the prominent issues being LLM hallucination. While\nvarious mitigation techniques are emerging to address hallucination, it is\nequally crucial to delve into its underlying causes. Consequently, in this\npreliminary exploratory investigation, we examine how linguistic factors in\nprompts, specifically readability, formality, and concreteness, influence the\noccurrence of hallucinations. Our experimental results suggest that prompts\ncharacterized by greater formality and concreteness tend to result in reduced\nhallucination. However, the outcomes pertaining to readability are somewhat\ninconclusive, showing a mixed pattern.",
        "pdf_link": "https://arxiv.org/pdf/2309.11064v1.pdf"
    },
    {
        "title": "In-Context Learning for Text Classification with Many Labels",
        "authors": [
            "Aristides Milios",
            "Siva Reddy",
            "Dzmitry Bahdanau"
        ],
        "published": "2023-09-19T22:41:44Z",
        "summary": "In-context learning (ICL) using large language models for tasks with many\nlabels is challenging due to the limited context window, which makes it\ndifficult to fit a sufficient number of examples in the prompt. In this paper,\nwe use a pre-trained dense retrieval model to bypass this limitation, giving\nthe model only a partial view of the full label space for each inference call.\nTesting with recent open-source LLMs (OPT, LLaMA), we set new state of the art\nperformance in few-shot settings for three common intent classification\ndatasets, with no finetuning. We also surpass fine-tuned performance on\nfine-grained sentiment classification in certain cases. We analyze the\nperformance across number of in-context examples and different model scales,\nshowing that larger models are necessary to effectively and consistently make\nuse of larger context lengths for ICL. By running several ablations, we analyze\nthe model's use of: a) the similarity of the in-context examples to the current\ninput, b) the semantic content of the class names, and c) the correct\ncorrespondence between examples and labels. We demonstrate that all three are\nneeded to varying degrees depending on the domain, contrary to certain recent\nworks.",
        "pdf_link": "https://arxiv.org/pdf/2309.10954v2.pdf"
    },
    {
        "title": "LMDX: Language Model-based Document Information Extraction and Localization",
        "authors": [
            "Vincent Perot",
            "Kai Kang",
            "Florian Luisier",
            "Guolong Su",
            "Xiaoyu Sun",
            "Ramya Sree Boppana",
            "Zilong Wang",
            "Jiaqi Mu",
            "Hao Zhang",
            "Nan Hua"
        ],
        "published": "2023-09-19T22:32:56Z",
        "summary": "Large Language Models (LLM) have revolutionized Natural Language Processing\n(NLP), improving state-of-the-art on many existing tasks and exhibiting\nemergent capabilities. However, LLMs have not yet been successfully applied on\nsemi-structured document information extraction, which is at the core of many\ndocument processing workflows and consists of extracting key entities from a\nvisually rich document (VRD) given a predefined target schema. The main\nobstacles to LLM adoption in that task have been the absence of layout encoding\nwithin LLMs, critical for a high quality extraction, and the lack of a\ngrounding mechanism ensuring the answer is not hallucinated. In this paper, we\nintroduce Language Model-based Document Information Extraction and Localization\n(LMDX), a methodology to adapt arbitrary LLMs for document information\nextraction. LMDX can do extraction of singular, repeated, and hierarchical\nentities, both with and without training data, while providing grounding\nguarantees and localizing the entities within the document. In particular, we\napply LMDX to the PaLM 2-S LLM and evaluate it on VRDU and CORD benchmarks,\nsetting a new state-of-the-art and showing how LMDX enables the creation of\nhigh quality, data-efficient parsers.",
        "pdf_link": "https://arxiv.org/pdf/2309.10952v1.pdf"
    },
    {
        "title": "Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training",
        "authors": [
            "Ruiqi Xu",
            "Yongfeng Huang",
            "Xin Chen",
            "Lin Zhang"
        ],
        "published": "2023-09-19T21:01:40Z",
        "summary": "In this work, we introduce the concept of complex text style transfer tasks,\nand constructed complex text datasets based on two widely applicable scenarios.\nOur dataset is the first large-scale data set of its kind, with 700 rephrased\nsentences and 1,000 sentences from the game Genshin Impact. While large\nlanguage models (LLM) have shown promise in complex text style transfer, they\nhave drawbacks such as data privacy concerns, network instability, and high\ndeployment costs. To address these issues, we explore the effectiveness of\nsmall models (less than T5-3B) with implicit style pre-training through\ncontrastive learning. We also propose a method for automated evaluation of text\ngeneration quality based on alignment with human evaluations using ChatGPT.\nFinally, we compare our approach with existing methods and show that our model\nachieves state-of-art performances of few-shot text style transfer models.",
        "pdf_link": "https://arxiv.org/pdf/2309.10929v1.pdf"
    },
    {
        "title": "FRASIMED: a Clinical French Annotated Resource Produced through Crosslingual BERT-Based Annotation Projection",
        "authors": [
            "Jamil Zaghir",
            "Mina Bjelogrlic",
            "Jean-Philippe Goldman",
            "Souka\u00efna Aananou",
            "Christophe Gaudet-Blavignac",
            "Christian Lovis"
        ],
        "published": "2023-09-19T17:17:28Z",
        "summary": "Natural language processing (NLP) applications such as named entity\nrecognition (NER) for low-resource corpora do not benefit from recent advances\nin the development of large language models (LLMs) where there is still a need\nfor larger annotated datasets. This research article introduces a methodology\nfor generating translated versions of annotated datasets through crosslingual\nannotation projection. Leveraging a language agnostic BERT-based approach, it\nis an efficient solution to increase low-resource corpora with few human\nefforts and by only using already available open data resources. Quantitative\nand qualitative evaluations are often lacking when it comes to evaluating the\nquality and effectiveness of semi-automatic data generation strategies. The\nevaluation of our crosslingual annotation projection approach showed both\neffectiveness and high accuracy in the resulting dataset. As a practical\napplication of this methodology, we present the creation of French Annotated\nResource with Semantic Information for Medical Entities Detection (FRASIMED),\nan annotated corpus comprising 2'051 synthetic clinical cases in French. The\ncorpus is now available for researchers and practitioners to develop and refine\nFrench natural language processing (NLP) applications in the clinical field\n(https://zenodo.org/record/8355629), making it the largest open annotated\ncorpus with linked medical concepts in French.",
        "pdf_link": "https://arxiv.org/pdf/2309.10770v1.pdf"
    },
    {
        "title": "Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome",
        "authors": [
            "Hiromu Yakura"
        ],
        "published": "2023-09-19T16:41:19Z",
        "summary": "Metaphors and sarcasm are precious fruits of our highly-evolved social\ncommunication skills. However, children with Asperger syndrome are known to\nhave difficulties in comprehending sarcasm, even if they possess a certain\nlevel of verbal IQ sufficient for understanding metaphors. Given that, a\nscreening test that scores the ability to understand metaphor and sarcasm has\nbeen used to differentiate Asperger syndrome from other symptoms exhibiting\nakin external behaviors (e.g., attention-deficit/hyperactivity disorder). This\nstudy uses the standardized test to examine the capability of recent large\nlanguage models (LLMs) in understanding human nuanced communication. The\nresults divulged that, whereas their ability to comprehend metaphors has been\nimproved with the increase of the number of model parameters, the improvement\nin sarcasm understanding was not observed. This implies that an alternative\napproach is imperative to imbue LLMs with the capacity to grasp sarcasm, which\nhas been associated with the amygdala, a pivotal cerebral region for emotional\nlearning, in the case of humans.",
        "pdf_link": "https://arxiv.org/pdf/2309.10744v2.pdf"
    },
    {
        "title": "GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models",
        "authors": [
            "Yonggan Fu",
            "Yongan Zhang",
            "Zhongzhi Yu",
            "Sixu Li",
            "Zhifan Ye",
            "Chaojian Li",
            "Cheng Wan",
            "Yingyan Lin"
        ],
        "published": "2023-09-19T16:14:57Z",
        "summary": "The remarkable capabilities and intricate nature of Artificial Intelligence\n(AI) have dramatically escalated the imperative for specialized AI\naccelerators. Nonetheless, designing these accelerators for various AI\nworkloads remains both labor- and time-intensive. While existing design\nexploration and automation tools can partially alleviate the need for extensive\nhuman involvement, they still demand substantial hardware expertise, posing a\nbarrier to non-experts and stifling AI accelerator development. Motivated by\nthe astonishing potential of large language models (LLMs) for generating\nhigh-quality content in response to human language instructions, we embark on\nthis work to examine the possibility of harnessing LLMs to automate AI\naccelerator design. Through this endeavor, we develop GPT4AIGChip, a framework\nintended to democratize AI accelerator design by leveraging human natural\nlanguages instead of domain-specific languages. Specifically, we first perform\nan in-depth investigation into LLMs' limitations and capabilities for AI\naccelerator design, thus aiding our understanding of our current position and\ngarnering insights into LLM-powered automated AI accelerator design.\nFurthermore, drawing inspiration from the above insights, we develop a\nframework called GPT4AIGChip, which features an automated demo-augmented\nprompt-generation pipeline utilizing in-context learning to guide LLMs towards\ncreating high-quality AI accelerator design. To our knowledge, this work is the\nfirst to demonstrate an effective pipeline for LLM-powered automated AI\naccelerator generation. Accordingly, we anticipate that our insights and\nframework can serve as a catalyst for innovations in next-generation\nLLM-powered design automation tools.",
        "pdf_link": "https://arxiv.org/pdf/2309.10730v1.pdf"
    },
    {
        "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
        "authors": [
            "Yucheng Li"
        ],
        "published": "2023-09-19T15:02:58Z",
        "summary": "Data contamination in model evaluation is getting increasingly prevalent as\nthe massive training corpora of large language models often unintentionally\ninclude benchmark samples. Therefore, contamination analysis has became an\ninevitable part of reliable model evaluation. However, existing method of\ncontamination analysis requires the access of the entire training data which is\noften confidential for recent models. This prevent the community to rigorously\naudit these models and conduct accurate assessment of their capability. In this\npaper, we propose a novel method to quantify contamination without the access\nof the full training set, that measure the extent of contamination with\nperplexity. Our analysis provides evidence of significant memorisation of\nrecent foundation models in popular reading comprehension, summarisation\nbenchmarks, while multiple choice appears less contaminated.",
        "pdf_link": "https://arxiv.org/pdf/2309.10677v2.pdf"
    },
    {
        "title": "Model Leeching: An Extraction Attack Targeting LLMs",
        "authors": [
            "Lewis Birch",
            "William Hackett",
            "Stefan Trawicki",
            "Neeraj Suri",
            "Peter Garraghan"
        ],
        "published": "2023-09-19T11:45:29Z",
        "summary": "Model Leeching is a novel extraction attack targeting Large Language Models\n(LLMs), capable of distilling task-specific knowledge from a target LLM into a\nreduced parameter model. We demonstrate the effectiveness of our attack by\nextracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match\n(EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%,\nrespectively for only $50 in API cost. We further demonstrate the feasibility\nof adversarial attack transferability from an extracted model extracted via\nModel Leeching to perform ML attack staging against a target LLM, resulting in\nan 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.",
        "pdf_link": "https://arxiv.org/pdf/2309.10544v1.pdf"
    },
    {
        "title": "Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models",
        "authors": [
            "Qiming Bao",
            "Juho Leinonen",
            "Alex Yuxuan Peng",
            "Wanjun Zhong",
            "Ga\u00ebl Gendron",
            "Timothy Pistotti",
            "Alice Huang",
            "Paul Denny",
            "Michael Witbrock",
            "Jiamou Liu"
        ],
        "published": "2023-09-19T09:04:15Z",
        "summary": "Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.10444v4.pdf"
    },
    {
        "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
        "authors": [
            "Dawei Zhu",
            "Nan Yang",
            "Liang Wang",
            "Yifan Song",
            "Wenhao Wu",
            "Furu Wei",
            "Sujian Li"
        ],
        "published": "2023-09-19T08:03:38Z",
        "summary": "Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.",
        "pdf_link": "https://arxiv.org/pdf/2309.10400v3.pdf"
    },
    {
        "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models",
        "authors": [
            "Yuexiang Zhai",
            "Shengbang Tong",
            "Xiao Li",
            "Mu Cai",
            "Qing Qu",
            "Yong Jae Lee",
            "Yi Ma"
        ],
        "published": "2023-09-19T04:51:13Z",
        "summary": "Following the success of GPT4, there has been a surge in interest in\nmultimodal large language model (MLLM) research. This line of research focuses\non developing general-purpose LLMs through fine-tuning pre-trained LLMs and\nvision models. However, catastrophic forgetting, a notorious phenomenon where\nthe fine-tuned model fails to retain similar performance compared to the\npre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).\nIn this paper, we introduce EMT: Evaluating MulTimodality for evaluating the\ncatastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.\nWe first apply EMT to evaluate several open-source fine-tuned MLLMs and we\ndiscover that almost all evaluated MLLMs fail to retain the same performance\nlevels as their vision encoders on standard image classification tasks.\nMoreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess\nperformance throughout the fine-tuning. Interestingly, our results suggest that\nearly-stage fine-tuning on an image dataset improves performance across other\nimage datasets, by enhancing the alignment of text and visual features.\nHowever, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in\na significant loss of generalizability, even when the image encoder remains\nfrozen. Our results suggest that MLLMs have yet to demonstrate performance on\npar with their vision models on standard image classification tasks and the\ncurrent MLLM fine-tuning procedure still has room for improvement.",
        "pdf_link": "https://arxiv.org/pdf/2309.10313v4.pdf"
    },
    {
        "title": "LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins",
        "authors": [
            "Umar Iqbal",
            "Tadayoshi Kohno",
            "Franziska Roesner"
        ],
        "published": "2023-09-19T02:20:10Z",
        "summary": "Large language model (LLM) platforms, such as ChatGPT, have recently begun\noffering a plugin ecosystem to interface with third-party services on the\ninternet. While these plugins extend the capabilities of LLM platforms, they\nare developed by arbitrary third parties and thus cannot be implicitly trusted.\nPlugins also interface with LLM platforms and users using natural language,\nwhich can have imprecise interpretations. In this paper, we propose a framework\nthat lays a foundation for LLM platform designers to analyze and improve the\nsecurity, privacy, and safety of current and future plugin-integrated LLM\nplatforms. Our framework is a formulation of an attack taxonomy that is\ndeveloped by iteratively exploring how LLM platform stakeholders could leverage\ntheir capabilities and responsibilities to mount attacks against each other. As\npart of our iterative process, we apply our framework in the context of\nOpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the\npotential for the types of issues that we outline in our attack taxonomy. We\nconclude by discussing novel challenges and by providing recommendations to\nimprove the security, privacy, and safety of present and future LLM-based\ncomputing platforms.",
        "pdf_link": "https://arxiv.org/pdf/2309.10254v1.pdf"
    },
    {
        "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
        "authors": [
            "Jiahao Yu",
            "Xingwei Lin",
            "Zheng Yu",
            "Xinyu Xing"
        ],
        "published": "2023-09-19T02:19:48Z",
        "summary": "Large language models (LLMs) have recently experienced tremendous popularity\nand are widely used from casual conversations to AI-driven programming.\nHowever, despite their considerable success, LLMs are not entirely reliable and\ncan give detailed guidance on how to conduct harmful or illegal activities.\nWhile safety measures can reduce the risk of such outputs, adversarial\njailbreak attacks can still exploit LLMs to produce harmful content. These\njailbreak templates are typically manually crafted, making large-scale testing\nchallenging.\n  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing\nframework inspired by the AFL fuzzing framework. Instead of manual engineering,\nGPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.\nAt its core, GPTFuzz starts with human-written templates as initial seeds, then\nmutates them to produce new templates. We detail three key components of\nGPTFuzz: a seed selection strategy for balancing efficiency and variability,\nmutate operators for creating semantically equivalent or similar sentences, and\na judgment model to assess the success of a jailbreak attack.\n  We evaluate GPTFuzz against various commercial and open-source LLMs,\nincluding ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our\nresults indicate that GPTFuzz consistently produces jailbreak templates with a\nhigh success rate, surpassing human-crafted templates. Remarkably, GPTFuzz\nachieves over 90% attack success rates against ChatGPT and Llama-2 models, even\nwith suboptimal initial seed templates. We anticipate that GPTFuzz will be\ninstrumental for researchers and practitioners in examining LLM robustness and\nwill encourage further exploration into enhancing LLM safety.",
        "pdf_link": "https://arxiv.org/pdf/2309.10253v2.pdf"
    },
    {
        "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal",
        "authors": [
            "Baolin Peng",
            "Linfeng Song",
            "Ye Tian",
            "Lifeng Jin",
            "Haitao Mi",
            "Dong Yu"
        ],
        "published": "2023-09-18T23:06:32Z",
        "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet aligning these models with human values and preferences using RLHF remains\na significant challenge. This challenge is characterized by various\ninstabilities, such as reward hacking and catastrophic forgetting. In this\ntechnical report, we propose two innovations to stabilize RLHF training: 1)\nAdvantage Model, which directly models advantage score i.e., extra reward\ncompared to the expected rewards and regulates score distributions across tasks\nto prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic\nforgetting by strategically selecting data for PPO training and knowledge\nrehearsing. Our experimental analysis on public and proprietary datasets\nreveals that the proposed methods not only increase stability in RLHF training\nbut also achieve higher reward scores and win rates.",
        "pdf_link": "https://arxiv.org/pdf/2309.10202v1.pdf"
    },
    {
        "title": "RadOnc-GPT: A Large Language Model for Radiation Oncology",
        "authors": [
            "Zhengliang Liu",
            "Peilong Wang",
            "Yiwei Li",
            "Jason Holmes",
            "Peng Shu",
            "Lian Zhang",
            "Chenbin Liu",
            "Ninghao Liu",
            "Dajiang Zhu",
            "Xiang Li",
            "Quanzheng Li",
            "Samir H. Patel",
            "Terence T. Sio",
            "Tianming Liu",
            "Wei Liu"
        ],
        "published": "2023-09-18T21:15:02Z",
        "summary": "This paper presents RadOnc-GPT, a large language model specialized for\nradiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on\na large dataset of radiation oncology patient records from the Mayo Clinic in\nArizona. The model employs instruction tuning on three key tasks - generating\nradiotherapy treatment regimens, determining optimal radiation modalities, and\nproviding diagnostic descriptions/ICD codes based on patient diagnostic\ndetails. Evaluations conducted by comparing RadOnc-GPT outputs to general large\nlanguage model outputs showed higher ROUGE scores in these three tasks. The\nstudy demonstrated the potential of using large language models fine-tuned\nusing domain-specific knowledge like RadOnc-GPT to achieve transformational\ncapabilities in highly specialized healthcare fields such as radiation\noncology. However, our model's clinical relevance requires confirmation, and it\nspecializes in only the aforementioned three specific tasks and lacks broader\napplicability. Furthermore, its evaluation through ROUGE scores might not\nreflect the true semantic and clinical accuracy - challenges we intend to\naddress in future research.",
        "pdf_link": "https://arxiv.org/pdf/2309.10160v3.pdf"
    },
    {
        "title": "Prompt a Robot to Walk with Large Language Models",
        "authors": [
            "Yen-Jen Wang",
            "Bike Zhang",
            "Jianyu Chen",
            "Koushil Sreenath"
        ],
        "published": "2023-09-18T17:50:17Z",
        "summary": "Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .",
        "pdf_link": "https://arxiv.org/pdf/2309.09969v2.pdf"
    },
    {
        "title": "Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding",
        "authors": [
            "Andr\u00e9 Storhaug",
            "Jingyue Li",
            "Tianyuan Hu"
        ],
        "published": "2023-09-18T14:47:34Z",
        "summary": "Auto-completing code enables developers to speed up coding significantly.\nRecent advances in transformer-based large language model (LLM) technologies\nhave been applied to code synthesis. However, studies show that many of such\nsynthesized codes contain vulnerabilities. We propose a novel\nvulnerability-constrained decoding approach to reduce the amount of vulnerable\ncode generated by such models. Using a small dataset of labeled vulnerable\nlines of code, we fine-tune an LLM to include vulnerability labels when\ngenerating code, acting as an embedded classifier. Then, during decoding, we\ndeny the model to generate these labels to avoid generating vulnerable code. To\nevaluate the method, we chose to automatically complete Ethereum Blockchain\nsmart contracts (SCs) as the case study due to the strict requirements of SC\nsecurity. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397\nEthereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning\ntook more than one week using ten GPUs. The results showed that our fine-tuned\nmodel could synthesize SCs with an average BLEU (BiLingual Evaluation\nUnderstudy) score of 0.557. However, many codes in the auto-completed SCs were\nvulnerable. Using the code before the vulnerable line of 176 SCs containing\ndifferent types of vulnerabilities to auto-complete the code, we found that\nmore than 70% of the auto-completed codes were insecure. Thus, we further\nfine-tuned the model on other 941 vulnerable SCs containing the same types of\nvulnerabilities and applied vulnerability-constrained decoding. The fine-tuning\ntook only one hour with four GPUs. We then auto-completed the 176 SCs again and\nfound that our approach could identify 62% of the code to be generated as\nvulnerable and avoid generating 67% of them, indicating the approach could\nefficiently and effectively avoid vulnerabilities in the auto-completed code.",
        "pdf_link": "https://arxiv.org/pdf/2309.09826v2.pdf"
    },
    {
        "title": "Bias of AI-Generated Content: An Examination of News Produced by Large Language Models",
        "authors": [
            "Xiao Fang",
            "Shangkun Che",
            "Minjia Mao",
            "Hongzhe Zhang",
            "Ming Zhao",
            "Xiaohang Zhao"
        ],
        "published": "2023-09-18T14:47:24Z",
        "summary": "Large language models (LLMs) have the potential to transform our lives and\nwork through the content they generate, known as AI-Generated Content (AIGC).\nTo harness this transformation, we need to understand the limitations of LLMs.\nHere, we investigate the bias of AIGC produced by seven representative LLMs,\nincluding ChatGPT and LLaMA. We collect news articles from The New York Times\nand Reuters, both known for their dedication to provide unbiased news. We then\napply each examined LLM to generate news content with headlines of these news\narticles as prompts, and evaluate the gender and racial biases of the AIGC\nproduced by the LLM by comparing the AIGC and the original news articles. We\nfurther analyze the gender bias of each LLM under biased prompts by adding\ngender-biased messages to prompts constructed from these news headlines. Our\nstudy reveals that the AIGC produced by each examined LLM demonstrates\nsubstantial gender and racial biases. Moreover, the AIGC generated by each LLM\nexhibits notable discrimination against females and individuals of the Black\nrace. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowest\nlevel of bias, and ChatGPT is the sole model capable of declining content\ngeneration when provided with biased prompts.",
        "pdf_link": "https://arxiv.org/pdf/2309.09825v3.pdf"
    },
    {
        "title": "Proposition from the Perspective of Chinese Language: A Chinese Proposition Classification Evaluation Benchmark",
        "authors": [
            "Conghui Niu",
            "Mengyang Hu",
            "Lin Bo",
            "Xiaoli He",
            "Dong Yu",
            "Pengyuan Liu"
        ],
        "published": "2023-09-18T09:18:39Z",
        "summary": "Existing propositions often rely on logical constants for classification.\nCompared with Western languages that lean towards hypotaxis such as English,\nChinese often relies on semantic or logical understanding rather than logical\nconnectives in daily expressions, exhibiting the characteristics of parataxis.\nHowever, existing research has rarely paid attention to this issue. And\naccurately classifying these propositions is crucial for natural language\nunderstanding and reasoning. In this paper, we put forward the concepts of\nexplicit and implicit propositions and propose a comprehensive multi-level\nproposition classification system based on linguistics and logic.\nCorrespondingly, we create a large-scale Chinese proposition dataset PEACE from\nmultiple domains, covering all categories related to propositions. To evaluate\nthe Chinese proposition classification ability of existing models and explore\ntheir limitations, We conduct evaluations on PEACE using several different\nmethods including the Rule-based method, SVM, BERT, RoBERTA, and ChatGPT.\nResults show the importance of properly modeling the semantic features of\npropositions. BERT has relatively good proposition classification capability,\nbut lacks cross-domain transferability. ChatGPT performs poorly, but its\nclassification ability can be improved by providing more proposition\ninformation. Many issues are still far from being resolved and require further\nstudy.",
        "pdf_link": "https://arxiv.org/pdf/2309.09602v1.pdf"
    },
    {
        "title": "Progressive Text-to-Image Diffusion with Soft Latent Direction",
        "authors": [
            "YuTeng Ye",
            "Jiale Cai",
            "Hang Zhou",
            "Guanwen Li",
            "Youjia Zhang",
            "Zikai Song",
            "Chenxing Gao",
            "Junqing Yu",
            "Wei Yang"
        ],
        "published": "2023-09-18T04:01:25Z",
        "summary": "In spite of the rapidly evolving landscape of text-to-image generation, the\nsynthesis and manipulation of multiple entities while adhering to specific\nrelational constraints pose enduring challenges. This paper introduces an\ninnovative progressive synthesis and editing operation that systematically\nincorporates entities into the target image, ensuring their adherence to\nspatial and relational constraints at each sequential step. Our key insight\nstems from the observation that while a pre-trained text-to-image diffusion\nmodel adeptly handles one or two entities, it often falters when dealing with a\ngreater number. To address this limitation, we propose harnessing the\ncapabilities of a Large Language Model (LLM) to decompose intricate and\nprotracted text descriptions into coherent directives adhering to stringent\nformats. To facilitate the execution of directives involving distinct semantic\noperations-namely insertion, editing, and erasing-we formulate the Stimulus,\nResponse, and Fusion (SRF) framework. Within this framework, latent regions are\ngently stimulated in alignment with each operation, followed by the fusion of\nthe responsive latent components to achieve cohesive entity manipulation. Our\nproposed framework yields notable advancements in object synthesis,\nparticularly when confronted with intricate and lengthy textual inputs.\nConsequently, it establishes a new benchmark for text-to-image generation\ntasks, further elevating the field's performance standards.",
        "pdf_link": "https://arxiv.org/pdf/2309.09466v2.pdf"
    },
    {
        "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM",
        "authors": [
            "Bochuan Cao",
            "Yuanpu Cao",
            "Lu Lin",
            "Jinghui Chen"
        ],
        "published": "2023-09-18T02:07:22Z",
        "summary": "Recently, Large Language Models (LLMs) have made significant advancements and\nare now widely used across various domains. Unfortunately, there has been a\nrising concern that LLMs can be misused to generate harmful or malicious\ncontent. Though a line of research has focused on aligning LLMs with human\nvalues and preventing them from producing inappropriate content, such\nalignments are usually vulnerable and can be bypassed by alignment-breaking\nattacks via adversarially optimized or handcrafted jailbreaking prompts. In\nthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against\npotential alignment-breaking attacks. RA-LLM can be directly constructed upon\nan existing aligned LLM with a robust alignment checking function, without\nrequiring any expensive retraining or fine-tuning process of the original LLM.\nFurthermore, we also provide a theoretical analysis for RA-LLM to verify its\neffectiveness in defending against alignment-breaking attacks. Through\nreal-world experiments on open-source large language models, we demonstrate\nthat RA-LLM can successfully defend against both state-of-the-art adversarial\nprompts and popular handcrafted jailbreaking prompts by reducing their attack\nsuccess rates from nearly 100% to around 10% or less.",
        "pdf_link": "https://arxiv.org/pdf/2309.14348v2.pdf"
    },
    {
        "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
        "authors": [
            "Thuat Nguyen",
            "Chien Van Nguyen",
            "Viet Dac Lai",
            "Hieu Man",
            "Nghia Trung Ngo",
            "Franck Dernoncourt",
            "Ryan A. Rossi",
            "Thien Huu Nguyen"
        ],
        "published": "2023-09-17T23:49:10Z",
        "summary": "The driving factors behind the development of large language models (LLMs)\nwith impressive learning capabilities are their colossal model sizes and\nextensive training datasets. Along with the progress in natural language\nprocessing, LLMs have been frequently made accessible to the public to foster\ndeeper investigation and applications. However, when it comes to training\ndatasets for these LLMs, especially the recent state-of-the-art models, they\nare often not fully disclosed. Creating training data for high-performing LLMs\ninvolves extensive cleaning and deduplication to ensure the necessary level of\nquality. The lack of transparency for training data has thus hampered research\non attributing and addressing hallucination and bias issues in LLMs, hindering\nreplication efforts and further advancements in the community. These challenges\nbecome even more pronounced in multilingual learning scenarios, where the\navailable multilingual text datasets are often inadequately collected and\ncleaned. Consequently, there is a lack of open-source and readily usable\ndataset to effectively train LLMs in multiple languages. To overcome this\nissue, we present CulturaX, a substantial multilingual dataset with 6.3\ntrillion tokens in 167 languages, tailored for LLM development. Our dataset\nundergoes meticulous cleaning and deduplication through a rigorous pipeline of\nmultiple stages to accomplish the best quality for model training, including\nlanguage identification, URL-based filtering, metric-based cleaning, document\nrefinement, and data deduplication. CulturaX is fully released to the public in\nHuggingFace to facilitate research and advancements in multilingual LLMs:\nhttps://huggingface.co/datasets/uonlp/CulturaX.",
        "pdf_link": "https://arxiv.org/pdf/2309.09400v1.pdf"
    },
    {
        "title": "Language models are susceptible to incorrect patient self-diagnosis in medical applications",
        "authors": [
            "Rojin Ziaei",
            "Samuel Schmidgall"
        ],
        "published": "2023-09-17T19:56:39Z",
        "summary": "Large language models (LLMs) are becoming increasingly relevant as a\npotential tool for healthcare, aiding communication between clinicians,\nresearchers, and patients. However, traditional evaluations of LLMs on medical\nexam questions do not reflect the complexity of real patient-doctor\ninteractions. An example of this complexity is the introduction of patient\nself-diagnosis, where a patient attempts to diagnose their own medical\nconditions from various sources. While the patient sometimes arrives at an\naccurate conclusion, they more often are led toward misdiagnosis due to the\npatient's over-emphasis on bias validating information. In this work we present\na variety of LLMs with multiple-choice questions from United States medical\nboard exams which are modified to include self-diagnostic reports from\npatients. Our findings highlight that when a patient proposes incorrect\nbias-validating information, the diagnostic accuracy of LLMs drop dramatically,\nrevealing a high susceptibility to errors in self-diagnosis.",
        "pdf_link": "https://arxiv.org/pdf/2309.09362v1.pdf"
    },
    {
        "title": "Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model",
        "authors": [
            "Ziqi Yang",
            "Xuhai Xu",
            "Bingsheng Yao",
            "Shao Zhang",
            "Ethan Rogers",
            "Stephen Intille",
            "Nawar Shara",
            "Guodong Gordon Gao",
            "Dakuo Wang"
        ],
        "published": "2023-09-17T19:46:03Z",
        "summary": "Despite the plethora of telehealth applications to assist home-based older\nadults and healthcare providers, basic messaging and phone calls are still the\nmost common communication methods, which suffer from limited availability,\ninformation loss, and process inefficiencies. One promising solution to\nfacilitate patient-provider communication is to leverage large language models\n(LLMs) with their powerful natural conversation and summarization capability.\nHowever, there is a limited understanding of LLMs' role during the\ncommunication. We first conducted two interview studies with both older adults\n(N=10) and healthcare providers (N=9) to understand their needs and\nopportunities for LLMs in patient-provider asynchronous communication. Based on\nthe insights, we built an LLM-powered communication system, Talk2Care, and\ndesigned interactive components for both groups: (1) For older adults, we\nleveraged the convenience and accessibility of voice assistants (VAs) and built\nan LLM-powered VA interface for effective information collection. (2) For\nhealth providers, we built an LLM-based dashboard to summarize and present\nimportant health information based on older adults' conversations with the VA.\nWe further conducted two user studies with older adults and providers to\nevaluate the usability of the system. The results showed that Talk2Care could\nfacilitate the communication process, enrich the health information collected\nfrom older adults, and considerably save providers' efforts and time. We\nenvision our work as an initial exploration of LLMs' capability in the\nintersection of healthcare and interpersonal communication.",
        "pdf_link": "https://arxiv.org/pdf/2309.09357v5.pdf"
    },
    {
        "title": "Can Large Language Models Understand Real-World Complex Instructions?",
        "authors": [
            "Qianyu He",
            "Jie Zeng",
            "Wenhao Huang",
            "Lina Chen",
            "Jin Xiao",
            "Qianxi He",
            "Xunzhe Zhou",
            "Lida Chen",
            "Xintao Wang",
            "Yuncheng Huang",
            "Haoning Ye",
            "Zihan Li",
            "Shisong Chen",
            "Yikai Zhang",
            "Zhouhong Gu",
            "Jiaqing Liang",
            "Yanghua Xiao"
        ],
        "published": "2023-09-17T04:18:39Z",
        "summary": "Large language models (LLMs) can understand human instructions, showing their\npotential for pragmatic applications beyond traditional NLP tasks. However,\nthey still struggle with complex instructions, which can be either complex task\ndescriptions that require multiple tasks and constraints, or complex input that\ncontains long context, noise, heterogeneous information and multi-turn format.\nDue to these features, LLMs often ignore semantic constraints from task\ndescriptions, generate incorrect formats, violate length or sample count\nconstraints, and be unfaithful to the input text. Existing benchmarks are\ninsufficient to assess LLMs' ability to understand complex instructions, as\nthey are close-ended and simple. To bridge this gap, we propose CELLO, a\nbenchmark for evaluating LLMs' ability to follow complex instructions\nsystematically. We design eight features for complex instructions and construct\na comprehensive evaluation dataset from real-world scenarios. We also establish\nfour criteria and develop corresponding metrics, as current ones are\ninadequate, biased or too strict and coarse-grained. We compare the performance\nof representative Chinese-oriented and English-oriented models in following\ncomplex instructions through extensive experiments. Resources of CELLO are\npublicly available at https://github.com/Abbey4799/CELLO.",
        "pdf_link": "https://arxiv.org/pdf/2309.09150v2.pdf"
    },
    {
        "title": "RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification",
        "authors": [
            "Hai-Long Nguyen",
            "Thi-Kieu-Trang Pham",
            "Thai-Son Le",
            "Tan-Minh Nguyen",
            "Thi-Hai-Yen Vuong",
            "Ha-Thanh Nguyen"
        ],
        "published": "2023-09-16T18:35:08Z",
        "summary": "In this study, we present a novel and challenging multilabel Vietnamese\ndataset (RMDM) designed to assess the performance of large language models\n(LLMs), in verifying electronic information related to legal contexts, focusing\non fake news as potential input for electronic evidence. The RMDM dataset\ncomprises four labels: real, mis, dis, and mal, representing real information,\nmisinformation, disinformation, and mal-information, respectively. By including\nthese diverse labels, RMDM captures the complexities of differing fake news\ncategories and offers insights into the abilities of different language models\nto handle various types of information that could be part of electronic\nevidence. The dataset consists of a total of 1,556 samples, with 389 samples\nfor each label. Preliminary tests on the dataset using GPT-based and BERT-based\nmodels reveal variations in the models' performance across different labels,\nindicating that the dataset effectively challenges the ability of various\nlanguage models to verify the authenticity of such information. Our findings\nsuggest that verifying electronic information related to legal contexts,\nincluding fake news, remains a difficult problem for language models,\nwarranting further attention from the research community to advance toward more\nreliable AI models for potential legal applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.09071v1.pdf"
    },
    {
        "title": "Rethinking STS and NLI in Large Language Models",
        "authors": [
            "Yuxia Wang",
            "Minghan Wang",
            "Preslav Nakov"
        ],
        "published": "2023-09-16T11:58:39Z",
        "summary": "Recent years have seen the rise of large language models (LLMs), where\npractitioners use task-specific prompts; this was shown to be effective for a\nvariety of tasks. However, when applied to semantic textual similarity (STS)\nand natural language inference (NLI), the effectiveness of LLMs turns out to be\nlimited by low-resource domain accuracy, model overconfidence, and difficulty\nto capture the disagreements between human judgements. With this in mind, here\nwe try to rethink STS and NLI in the era of LLMs. We first evaluate the\nperformance of STS and NLI in the clinical/biomedical domain, and then we\nassess LLMs' predictive confidence and their capability of capturing collective\nhuman opinions. We find that these old problems are still to be properly\naddressed in the era of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.08969v2.pdf"
    },
    {
        "title": "Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?",
        "authors": [
            "Xiangru Tang",
            "Yiming Zong",
            "Jason Phang",
            "Yilun Zhao",
            "Wangchunshu Zhou",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "published": "2023-09-16T11:31:58Z",
        "summary": "Despite the remarkable capabilities of Large Language Models (LLMs) like\nGPT-4, producing complex, structured tabular data remains challenging. Our\nstudy assesses LLMs' proficiency in structuring tables and introduces a novel\nfine-tuning method, cognizant of data structures, to bolster their performance.\nWe unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs\n(GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and\nLaTeX formats. Our proposed FormatCoT aids in crafting format-specific\ninstructions from the intended outputs to populate this benchmark. Addressing\nthe gap in task-centered evaluation, we propose two innovative metrics, P-Score\n(Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM\nperformance. Our experiments show that applying our structure-aware fine-tuning\nto LLaMA-7B leads to substantial performance gains, outshining its LLM\ncounterparts across most measures. In-depth error analysis and creating an\nability map across six dimensions -- coverage, formatting, reasoning,\ncomprehension, pragmatics, and hallucination -- highlight areas for future\nenhancements and suggest forthcoming research trajectories. Our code and models\ncan be found at https://github.com/gersteinlab/Struc-Bench.",
        "pdf_link": "https://arxiv.org/pdf/2309.08963v3.pdf"
    },
    {
        "title": "ODSum: New Benchmarks for Open Domain Multi-Document Summarization",
        "authors": [
            "Yijie Zhou",
            "Kejian Shi",
            "Wencai Zhang",
            "Yixin Liu",
            "Yilun Zhao",
            "Arman Cohan"
        ],
        "published": "2023-09-16T11:27:34Z",
        "summary": "Open-domain Multi-Document Summarization (ODMDS) is a critical tool for\ncondensing vast arrays of documents into coherent, concise summaries. With a\nmore inter-related document set, there does not necessarily exist a correct\nanswer for the retrieval, making it hard to measure the retrieving performance.\nWe propose a rule-based method to process query-based document summarization\ndatasets into ODMDS datasets. Based on this method, we introduce a novel\ndataset, ODSum, a sophisticated case with its document index interdependent and\noften interrelated. We tackle ODMDS with the \\textit{retrieve-then-summarize}\nmethod, and the performance of a list of retrievers and summarizers is\ninvestigated. Through extensive experiments, we identify variances in\nevaluation metrics and provide insights into their reliability. We also found\nthat LLMs suffer great performance loss from retrieving errors. We further\nexperimented methods to improve the performance as well as investigate their\nrobustness against imperfect retrieval. We will release our data and code at\nhttps://github.com/yale-nlp/ODSum.",
        "pdf_link": "https://arxiv.org/pdf/2309.08960v1.pdf"
    },
    {
        "title": "PDFTriage: Question Answering over Long, Structured Documents",
        "authors": [
            "Jon Saad-Falcon",
            "Joe Barrow",
            "Alexa Siu",
            "Ani Nenkova",
            "David Seunghyun Yoon",
            "Ryan A. Rossi",
            "Franck Dernoncourt"
        ],
        "published": "2023-09-16T04:29:05Z",
        "summary": "Large Language Models (LLMs) have issues with document question answering\n(QA) in situations where the document is unable to fit in the small context\nlength of an LLM. To overcome this issue, most existing works focus on\nretrieving the relevant context from the document, representing them as plain\ntext. However, documents such as PDFs, web pages, and presentations are\nnaturally structured with different pages, tables, sections, and so on.\nRepresenting such structured documents as plain text is incongruous with the\nuser's mental model of these documents with rich structure. When a system has\nto query the document for context, this incongruity is brought to the fore, and\nseemingly trivial questions can trip up the QA system. To bridge this\nfundamental gap in handling structured documents, we propose an approach called\nPDFTriage that enables models to retrieve the context based on either structure\nor content. Our experiments demonstrate the effectiveness of the proposed\nPDFTriage-augmented models across several classes of questions where existing\nretrieval-augmented LLMs fail. To facilitate further research on this\nfundamental problem, we release our benchmark dataset consisting of 900+\nhuman-generated questions over 80 structured documents from 10 different\ncategories of question types for document QA. Our code and datasets will be\nreleased soon on Github.",
        "pdf_link": "https://arxiv.org/pdf/2309.08872v2.pdf"
    },
    {
        "title": "Rethinking Learning Rate Tuning in the Era of Large Language Models",
        "authors": [
            "Hongpeng Jin",
            "Wenqi Wei",
            "Xuyu Wang",
            "Wenbin Zhang",
            "Yanzhao Wu"
        ],
        "published": "2023-09-16T03:37:00Z",
        "summary": "Large Language Models (LLMs) represent the recent success of deep learning in\nachieving remarkable human-like predictive performance. It has become a\nmainstream strategy to leverage fine-tuning to adapt LLMs for various\nreal-world applications due to the prohibitive expenses associated with LLM\ntraining. The learning rate is one of the most important hyperparameters in LLM\nfine-tuning with direct impacts on both fine-tuning efficiency and fine-tuned\nLLM quality. Existing learning rate policies are primarily designed for\ntraining traditional deep neural networks (DNNs), which may not work well for\nLLM fine-tuning. We reassess the research challenges and opportunities of\nlearning rate tuning in the coming era of Large Language Models. This paper\nmakes three original contributions. First, we revisit existing learning rate\npolicies to analyze the critical challenges of learning rate tuning in the era\nof LLMs. Second, we present LRBench++ to benchmark learning rate policies and\nfacilitate learning rate tuning for both traditional DNNs and LLMs. Third, our\nexperimental analysis with LRBench++ demonstrates the key differences between\nLLM fine-tuning and traditional DNN training and validates our analysis.",
        "pdf_link": "https://arxiv.org/pdf/2309.08859v1.pdf"
    },
    {
        "title": "S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs",
        "authors": [
            "Sarkar Snigdha Sarathi Das",
            "Chirag Shah",
            "Mengting Wan",
            "Jennifer Neville",
            "Longqi Yang",
            "Reid Andersen",
            "Georg Buscher",
            "Tara Safavi"
        ],
        "published": "2023-09-16T00:59:23Z",
        "summary": "The traditional Dialogue State Tracking (DST) problem aims to track user\npreferences and intents in user-agent conversations. While sufficient for\ntask-oriented dialogue systems supporting narrow domain applications, the\nadvent of Large Language Model (LLM)-based chat systems has introduced many\nreal-world intricacies in open-domain dialogues. These intricacies manifest in\nthe form of increased complexity in contextual interactions, extended dialogue\nsessions encompassing a diverse array of topics, and more frequent contextual\nshifts. To handle these intricacies arising from evolving LLM-based chat\nsystems, we propose joint dialogue segmentation and state tracking per segment\nin open-domain dialogue systems. Assuming a zero-shot setting appropriate to a\ntrue open-domain dialogue system, we propose S3-DST, a structured prompting\ntechnique that harnesses Pre-Analytical Recollection, a novel grounding\nmechanism we designed for improving long context tracking. To demonstrate the\nefficacy of our proposed approach in joint segmentation and state tracking, we\nevaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as\nwell as publicly available DST and segmentation datasets. Across all datasets\nand settings, S3-DST consistently outperforms the state-of-the-art,\ndemonstrating its potency and robustness the next generation of LLM-based chat\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2309.08827v1.pdf"
    },
    {
        "title": "Fake News Detectors are Biased against Texts Generated by Large Language Models",
        "authors": [
            "Jinyan Su",
            "Terry Yue Zhuo",
            "Jonibek Mansurov",
            "Di Wang",
            "Preslav Nakov"
        ],
        "published": "2023-09-15T18:04:40Z",
        "summary": "The spread of fake news has emerged as a critical challenge, undermining\ntrust and posing threats to society. In the era of Large Language Models\n(LLMs), the capability to generate believable fake content has intensified\nthese concerns. In this study, we present a novel paradigm to evaluate fake\nnews detectors in scenarios involving both human-written and LLM-generated\nmisinformation. Intriguingly, our findings reveal a significant bias in many\nexisting detectors: they are more prone to flagging LLM-generated content as\nfake news while often misclassifying human-written fake news as genuine. This\nunexpected bias appears to arise from distinct linguistic patterns inherent to\nLLM outputs. To address this, we introduce a mitigation strategy that leverages\nadversarial training with LLM-paraphrased genuine news. The resulting model\nyielded marked improvements in detection accuracy for both human and\nLLM-generated news. To further catalyze research in this domain, we release two\ncomprehensive datasets, \\texttt{GossipCop++} and \\texttt{PolitiFact++}, thus\namalgamating human-validated articles with LLM-generated fake and real news.",
        "pdf_link": "https://arxiv.org/pdf/2309.08674v1.pdf"
    },
    {
        "title": "Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings",
        "authors": [
            "Chen Cecilia Liu",
            "Fajri Koto",
            "Timothy Baldwin",
            "Iryna Gurevych"
        ],
        "published": "2023-09-15T17:45:28Z",
        "summary": "Large language models (LLMs) are highly adept at question answering and\nreasoning tasks, but when reasoning in a situational context, human\nexpectations vary depending on the relevant cultural common ground. As\nlanguages are associated with diverse cultures, LLMs should also be\nculturally-diverse reasoners. In this paper, we study the ability of a wide\nrange of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and\nsayings in a conversational context. Our experiments reveal that: (1) mLLMs\n\"know\" limited proverbs and memorizing proverbs does not mean understanding\nthem within a conversational context; (2) mLLMs struggle to reason with\nfigurative proverbs and sayings, and when asked to select the wrong answer\n(instead of asking it to select the correct answer); and (3) there is a\n\"culture gap\" in mLLMs when reasoning about proverbs and sayings translated\nfrom other languages. We construct and release our evaluation dataset MAPS\n(MulticultrAl Proverbs and Sayings) for proverb understanding with\nconversational context for six different languages.",
        "pdf_link": "https://arxiv.org/pdf/2309.08591v2.pdf"
    },
    {
        "title": "Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West",
        "authors": [
            "Khyati Khandelwal",
            "Manuel Tonneau",
            "Andrew M. Bean",
            "Hannah Rose Kirk",
            "Scott A. Hale"
        ],
        "published": "2023-09-15T17:38:41Z",
        "summary": "Large Language Models (LLMs), now used daily by millions of users, can encode\nsocietal biases, exposing their users to representational harms. A large body\nof scholarship on LLM bias exists but it predominantly adopts a Western-centric\nframe and attends comparatively less to bias levels and potential harms in the\nGlobal South. In this paper, we quantify stereotypical bias in popular LLMs\naccording to an Indian-centric frame and compare bias levels between the Indian\nand Western contexts. To do this, we develop a novel dataset which we call\nIndian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and\nanti-stereotypical examples for caste and religion contexts. We find that the\nmajority of LLMs tested are strongly biased towards stereotypes in the Indian\ncontext, especially as compared to the Western context. We finally investigate\nInstruction Prompting as a simple intervention to mitigate such bias and find\nthat it significantly reduces both stereotypical and anti-stereotypical biases\nin the majority of cases for GPT-3.5. The findings of this work highlight the\nneed for including more diverse voices when evaluating LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.08573v1.pdf"
    },
    {
        "title": "Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata",
        "authors": [
            "Bohui Zhang",
            "Ioannis Reklos",
            "Nitisha Jain",
            "Albert Mero\u00f1o Pe\u00f1uela",
            "Elena Simperl"
        ],
        "published": "2023-09-15T15:51:14Z",
        "summary": "In this work, we explore the use of Large Language Models (LLMs) for\nknowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.\nFor this task, given subject and relation pairs sourced from Wikidata, we\nutilize pre-trained LLMs to produce the relevant objects in string format and\nlink them to their respective Wikidata QIDs. We developed a pipeline using LLMs\nfor Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata\nentity mapping. The method achieved a macro-averaged F1-score of 0.701 across\nthe properties, with the scores varying from 1.00 to 0.328. These results\ndemonstrate that the knowledge of LLMs varies significantly depending on the\ndomain and that further experimentation is required to determine the\ncircumstances under which LLMs can be used for automatic Knowledge Base (e.g.,\nWikidata) completion and correction. The investigation of the results also\nsuggests the promising contribution of LLMs in collaborative knowledge\nengineering. LLMKE won Track 2 of the challenge. The implementation is\navailable at https://github.com/bohuizhang/LLMKE.",
        "pdf_link": "https://arxiv.org/pdf/2309.08491v1.pdf"
    },
    {
        "title": "Adversarial Attacks on Tables with Entity Swap",
        "authors": [
            "Aneta Koleva",
            "Martin Ringsquandl",
            "Volker Tresp"
        ],
        "published": "2023-09-15T15:03:33Z",
        "summary": "The capabilities of large language models (LLMs) have been successfully\napplied in the context of table representation learning. The recently proposed\ntabular language models have reported state-of-the-art results across various\ntasks for table interpretation. However, a closer look into the datasets\ncommonly used for evaluation reveals an entity leakage from the train set into\nthe test set. Motivated by this observation, we explore adversarial attacks\nthat represent a more realistic inference setup. Adversarial attacks on text\nhave been shown to greatly affect the performance of LLMs, but currently, there\nare no attacks targeting tabular language models. In this paper, we propose an\nevasive entity-swap attack for the column type annotation (CTA) task. Our CTA\nattack is the first black-box attack on tables, where we employ a\nsimilarity-based sampling strategy to generate adversarial examples. The\nexperimental results show that the proposed attack generates up to a 70% drop\nin performance.",
        "pdf_link": "https://arxiv.org/pdf/2309.08650v1.pdf"
    },
    {
        "title": "Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases",
        "authors": [
            "Yiheng Shu",
            "Zhiwei Yu"
        ],
        "published": "2023-09-15T12:06:45Z",
        "summary": "Language models (LMs) have already demonstrated remarkable abilities in\nunderstanding and generating both natural and formal language. Despite these\nadvances, their integration with real-world environments such as large-scale\nknowledge bases (KBs) remains an underdeveloped area, affecting applications\nsuch as semantic parsing and indulging in \"hallucinated\" information. This\npaper is an experimental investigation aimed at uncovering the robustness\nchallenges that LMs encounter when tasked with knowledge base question\nanswering (KBQA). The investigation covers scenarios with inconsistent data\ndistribution between training and inference, such as generalization to unseen\ndomains, adaptation to various language variations, and transferability across\ndifferent datasets. Our comprehensive experiments reveal that even when\nemployed with our proposed data augmentation techniques, advanced small and\nlarge language models exhibit poor performance in various dimensions. While the\nLM is a promising technology, the robustness of the current form in dealing\nwith complex environments is fragile and of limited practicality because of the\ndata distribution issue. This calls for future research on data collection and\nLM learning paradims.",
        "pdf_link": "https://arxiv.org/pdf/2309.08345v3.pdf"
    },
    {
        "title": "CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending",
        "authors": [
            "Shiyi Zhu",
            "Jing Ye",
            "Wei Jiang",
            "Siqiao Xue",
            "Qi Zhang",
            "Yifan Wu",
            "Jianguo Li"
        ],
        "published": "2023-09-15T09:36:51Z",
        "summary": "Self-attention and position embedding are two key modules in\ntransformer-based Large Language Models (LLMs). However, the potential\nrelationship between them is far from well studied, especially for long context\nwindow extending. In fact, anomalous behaviors harming long context\nextrapolation exist between Rotary Position Embedding (RoPE) and vanilla\nself-attention unveiled by our work. To address this issue, we propose a novel\nattention mechanism, CoCA (Collinear Constrained Attention). Specifically, we\nenforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE\nand self-attention. While only adding minimal computational and spatial\ncomplexity, this integration significantly enhances long context window\nextrapolation ability. We provide an optimized implementation, making it a\ndrop-in replacement for any existing transformer-based models. Extensive\nexperiments show that CoCA performs extraordinarily well in extending context\nwindows. A CoCA-based GPT model, trained with a context length of 512, can\nseamlessly extend the context window up to 32K (60$\\times$), without any\nfine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve\nextrapolation up to 32K within only 2K training length. Our code is publicly\navailable at: https://github.com/codefuse-ai/Collinear-Constrained-Attention",
        "pdf_link": "https://arxiv.org/pdf/2309.08646v3.pdf"
    },
    {
        "title": "Investigating Answerability of LLMs for Long-Form Question Answering",
        "authors": [
            "Meghana Moorthy Bhat",
            "Rui Meng",
            "Ye Liu",
            "Yingbo Zhou",
            "Semih Yavuz"
        ],
        "published": "2023-09-15T07:22:56Z",
        "summary": "As we embark on a new era of LLMs, it becomes increasingly crucial to\nunderstand their capabilities, limitations, and differences. Toward making\nfurther progress in this direction, we strive to build a deeper understanding\nof the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective\nopen-source LLMs and their distilled counterparts. To this end, we specifically\nfocus on long-form question answering (LFQA) because it has several practical\nand impactful applications (e.g., troubleshooting, customer service, etc.) yet\nis still understudied and challenging for LLMs. We propose a\nquestion-generation method from abstractive summaries and show that generating\nfollow-up questions from summaries of long documents can create a challenging\nsetting for LLMs to reason and infer from long contexts. Our experimental\nresults confirm that: (1) our proposed method of generating questions from\nabstractive summaries pose a challenging setup for LLMs and shows performance\ngaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2)\nopen-source LLMs exhibit decreased reliance on context for generated questions\nfrom the original document, but their generation capabilities drop\nsignificantly on generated questions from summaries -- especially for longer\ncontexts (>1024 tokens)",
        "pdf_link": "https://arxiv.org/pdf/2309.08210v1.pdf"
    },
    {
        "title": "Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level",
        "authors": [
            "Jingzhe Ding",
            "Yan Cen",
            "Xinyuan Wei"
        ],
        "published": "2023-09-15T06:13:06Z",
        "summary": "Our work demonstrates that large language model (LLM) pre-trained on texts\ncan not only solve pure math word problems, but also physics word problems,\nwhose solution requires calculation and inference based on prior physical\nknowledge. We collect and annotate the first physics word problem\ndataset-PhysQA, which contains over 1000 junior high school physics word\nproblems (covering Kinematics, Mass&Density, Mechanics, Heat, Electricity).\nThen we use OpenAI' s GPT3.5 to generate the answer of these problems and found\nthat GPT3.5 could automatically solve 49.3% of the problems through zero-shot\nlearning and 73.2% through few-shot learning. This result demonstrates that by\nusing similar problems and their answers as prompt, LLM could solve elementary\nphysics word problems approaching human level performance. In addition to\nsolving problems, GPT3.5 can also summarize the knowledge or topics covered by\nthe problems, provide relevant explanations, and generate new physics word\nproblems based on the input. Our work is the first research to focus on the\nautomatic solving, explanation, and generation of physics word problems across\nvarious types and scenarios, and we achieve an acceptable and state-of-the-art\naccuracy. This underscores the potential of LLMs for further applications in\nsecondary education.",
        "pdf_link": "https://arxiv.org/pdf/2309.08182v2.pdf"
    },
    {
        "title": "FedJudge: Federated Legal Large Language Model",
        "authors": [
            "Linan Yue",
            "Qi Liu",
            "Yichao Du",
            "Weibo Gao",
            "Ye Liu",
            "Fangzhou Yao"
        ],
        "published": "2023-09-15T05:45:44Z",
        "summary": "Large Language Models (LLMs) have gained prominence in the field of Legal\nIntelligence, offering potential applications in assisting legal professionals\nand laymen. However, the centralized training of these Legal LLMs raises data\nprivacy concerns, as legal data is distributed among various institutions\ncontaining sensitive individual information. This paper addresses this\nchallenge by exploring the integration of Legal LLMs with Federated Learning\n(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on\ndevices or clients, and their parameters are aggregated and distributed on a\ncentral server, ensuring data privacy without directly sharing raw data.\nHowever, computation and communication overheads hinder the full fine-tuning of\nLLMs under the FL setting. Moreover, the distribution shift of legal data\nreduces the effectiveness of FL methods. To this end, in this paper, we propose\nthe first Federated Legal Large Language Model (FedJudge) framework, which\nfine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudge\nutilizes parameter-efficient fine-tuning methods to update only a few\nadditional parameters during the FL training. Besides, we explore the continual\nlearning methods to preserve the global model's important parameters when\ntraining local clients to mitigate the problem of data shifts. Extensive\nexperimental results on three real-world datasets clearly validate the\neffectiveness of FedJudge. Code is released at\nhttps://github.com/yuelinan/FedJudge.",
        "pdf_link": "https://arxiv.org/pdf/2309.08173v3.pdf"
    },
    {
        "title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
        "authors": [
            "Kaixin Ma",
            "Hongming Zhang",
            "Hongwei Wang",
            "Xiaoman Pan",
            "Wenhao Yu",
            "Dong Yu"
        ],
        "published": "2023-09-15T05:44:08Z",
        "summary": "Large language models (LLMs) have been successfully adapted for interactive\ndecision-making tasks like web navigation. While achieving decent performance,\nprevious methods implicitly assume a forward-only execution mode for the model,\nwhere they only provide oracle trajectories as in-context examples to guide the\nmodel on how to reason in the environment. Consequently, the model could not\nhandle more challenging scenarios not covered in the in-context examples, e.g.,\nmistakes, leading to sub-optimal performance. To address this issue, we propose\nto model the interactive task as state space exploration, where the LLM agent\ntransitions among a pre-defined set of states by performing actions to complete\nthe task. This formulation enables flexible backtracking, allowing the model to\nrecover from errors easily. We evaluate our proposed LLM Agent with State-Space\nExploRation (LASER) on both the WebShop task and amazon.com. Experimental\nresults show that LASER significantly outperforms previous methods and closes\nthe gap with human performance on the web navigation task.",
        "pdf_link": "https://arxiv.org/pdf/2309.08172v2.pdf"
    },
    {
        "title": "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding",
        "authors": [
            "Jun Zhang",
            "Jue Wang",
            "Huan Li",
            "Lidan Shou",
            "Ke Chen",
            "Gang Chen",
            "Sharad Mehrotra"
        ],
        "published": "2023-09-15T05:34:32Z",
        "summary": "We present a novel inference scheme, self-speculative decoding, for\naccelerating Large Language Models (LLMs) without the need for an auxiliary\nmodel. This approach is characterized by a two-stage process: drafting and\nverification. The drafting stage generates draft tokens at a slightly lower\nquality but more quickly, which is achieved by selectively skipping certain\nintermediate layers during drafting Subsequently, the verification stage\nemploys the original LLM to validate those draft output tokens in one forward\npass. This process ensures the final output remains identical to that produced\nby the unaltered LLM, thereby maintaining output quality. The proposed method\nrequires no additional neural network training and no extra memory footprint,\nmaking it a plug-and-play and cost-effective solution for inference\nacceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a\nspeedup up to 1.73$\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2309.08168v1.pdf"
    },
    {
        "title": "Self-Assessment Tests are Unreliable Measures of LLM Personality",
        "authors": [
            "Akshat Gupta",
            "Xiaoyang Song",
            "Gopala Anumanchipalli"
        ],
        "published": "2023-09-15T05:19:39Z",
        "summary": "As large language models (LLM) evolve in their capabilities, various recent\nstudies have tried to quantify their behavior using psychological tools created\nto study human behavior. One such example is the measurement of \"personality\"\nof LLMs using self-assessment personality tests developed to measure human\npersonality. Yet almost none of these works verify the applicability of these\ntests on LLMs. In this paper, we analyze the reliability of LLM personality\nscores obtained from self-assessment personality tests using two simple\nexperiments. We first introduce the property of prompt sensitivity, where three\nsemantically equivalent prompts representing three intuitive ways of\nadministering self-assessment tests on LLMs are used to measure the personality\nof the same LLM. We find that all three prompts lead to very different\npersonality scores, a difference that is statistically significant for all\ntraits in a large majority of scenarios. We then introduce the property of\noption-order symmetry for personality measurement of LLMs. Since most of the\nself-assessment tests exist in the form of multiple choice question (MCQ)\nquestions, we argue that the scores should also be robust to not just the\nprompt template but also the order in which the options are presented. This\ntest unsurprisingly reveals that the self-assessment test scores are not robust\nto the order of the options. These simple tests, done on ChatGPT and three\nLlama2 models of different sizes, show that self-assessment personality tests\ncreated for humans are unreliable measures of personality in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.08163v2.pdf"
    },
    {
        "title": "Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies",
        "authors": [
            "Chirag Shah",
            "Ryen W. White",
            "Reid Andersen",
            "Georg Buscher",
            "Scott Counts",
            "Sarkar Snigdha Sarathi Das",
            "Ali Montazer",
            "Sathish Manivannan",
            "Jennifer Neville",
            "Xiaochuan Ni",
            "Nagu Rangan",
            "Tara Safavi",
            "Siddharth Suri",
            "Mengting Wan",
            "Leijie Wang",
            "Longqi Yang"
        ],
        "published": "2023-09-14T20:46:48Z",
        "summary": "Log data can reveal valuable information about how users interact with Web\nsearch services, what they want, and how satisfied they are. However, analyzing\nuser intents in log data is not easy, especially for emerging forms of Web\nsearch such as AI-driven chat. To understand user intents from log data, we\nneed a way to label them with meaningful categories that capture their\ndiversity and dynamics. Existing methods rely on manual or machine-learned\nlabeling, which are either expensive or inflexible for large and dynamic\ndatasets. We propose a novel solution using large language models (LLMs), which\ncan generate rich and relevant concepts, descriptions, and examples for user\nintents. However, using LLMs to generate a user intent taxonomy and apply it\nfor log analysis can be problematic for two main reasons: (1) such a taxonomy\nis not externally validated; and (2) there may be an undesirable feedback loop.\nTo address this, we propose a new methodology with human experts and assessors\nto verify the quality of the LLM-generated taxonomy. We also present an\nend-to-end pipeline that uses an LLM with human-in-the-loop to produce, refine,\nand apply labels for user intent analysis in log data. We demonstrate its\neffectiveness by uncovering new insights into user intents from search and chat\nlogs from the Microsoft Bing commercial search engine. The proposed work's\nnovelty stems from the method for generating purpose-driven user intent\ntaxonomies with strong validation. This method not only helps remove\nmethodological and practical bottlenecks from intent-focused research, but also\nprovides a new framework for generating, validating, and applying other kinds\nof taxonomies in a scalable and adaptable way with minimal human effort.",
        "pdf_link": "https://arxiv.org/pdf/2309.13063v2.pdf"
    },
    {
        "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
        "authors": [
            "Haozhe Zhao",
            "Zefan Cai",
            "Shuzheng Si",
            "Xiaojian Ma",
            "Kaikai An",
            "Liang Chen",
            "Zixuan Liu",
            "Sheng Wang",
            "Wenjuan Han",
            "Baobao Chang"
        ],
        "published": "2023-09-14T17:59:17Z",
        "summary": "Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC",
        "pdf_link": "https://arxiv.org/pdf/2309.07915v3.pdf"
    },
    {
        "title": "Ambiguity-Aware In-Context Learning with Large Language Models",
        "authors": [
            "Lingyu Gao",
            "Aditi Chaudhary",
            "Krishna Srinivasan",
            "Kazuma Hashimoto",
            "Karthik Raman",
            "Michael Bendersky"
        ],
        "published": "2023-09-14T17:48:34Z",
        "summary": "In-context learning (ICL) i.e. showing LLMs only a few task-specific\ndemonstrations has led to downstream gains with no task-specific fine-tuning\nrequired. However, LLMs are sensitive to the choice of prompts, and therefore a\ncrucial research question is how to select good demonstrations for ICL. One\neffective strategy is leveraging semantic similarity between the ICL\ndemonstrations and test inputs by using a text retriever, which however is\nsub-optimal as that does not consider the LLM's existing knowledge about that\ntask. From prior work (Lyu et al., 2023), we already know that labels paired\nwith the demonstrations bias the model predictions. This leads us to our\nhypothesis whether considering LLM's existing knowledge about the task,\nespecially with respect to the output label space can help in a better\ndemonstration selection strategy. Through extensive experimentation on three\ntext classification tasks, we find that it is beneficial to not only choose\nsemantically similar ICL demonstrations but also to choose those demonstrations\nthat help resolve the inherent label ambiguity surrounding the test example.\nInterestingly, we find that including demonstrations that the LLM previously\nmis-classified and also fall on the test example's decision boundary, brings\nthe most performance gain.",
        "pdf_link": "https://arxiv.org/pdf/2309.07900v2.pdf"
    },
    {
        "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions",
        "authors": [
            "Federico Bianchi",
            "Mirac Suzgun",
            "Giuseppe Attanasio",
            "Paul R\u00f6ttger",
            "Dan Jurafsky",
            "Tatsunori Hashimoto",
            "James Zou"
        ],
        "published": "2023-09-14T17:23:37Z",
        "summary": "Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks and generally become more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not harmlessness, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) when fine-tuning a model like LLaMA can substantially\nimprove its safety. Our safety-tuning does not make models significantly less\ncapable or helpful as measured by standard benchmarks. However, we do find\nexaggerated safety behaviours, where too much safety-tuning makes models refuse\nperfectly safe prompts if they superficially resemble unsafe ones. As a whole,\nour results illustrate trade-offs in training LLMs to be helpful and training\nthem to be safe.",
        "pdf_link": "https://arxiv.org/pdf/2309.07875v3.pdf"
    },
    {
        "title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
        "authors": [
            "Huayang Li",
            "Siheng Li",
            "Deng Cai",
            "Longyue Wang",
            "Lemao Liu",
            "Taro Watanabe",
            "Yujiu Yang",
            "Shuming Shi"
        ],
        "published": "2023-09-14T15:34:01Z",
        "summary": "Large language models with instruction-following abilities have\nrevolutionized the field of artificial intelligence. These models show\nexceptional generalizability to tackle various real-world tasks through their\nnatural language interfaces. However, their performance heavily relies on\nhigh-quality exemplar data, which is often difficult to obtain. This challenge\nis further exacerbated when it comes to multimodal instruction following. We\nintroduce TextBind, an almost annotation-free framework for empowering larger\nlanguage models with the multi-turn interleaved multimodal\ninstruction-following capabilities. Our approach requires only image-caption\npairs and generates multi-turn multimodal instruction-response conversations\nfrom a language model. To accommodate interleaved image-text inputs and\noutputs, we devise MIM, a language model-centric architecture that seamlessly\nintegrates image encoder and decoder models. We release our dataset, model, and\ndemo to foster future research in the area of multimodal instruction following.",
        "pdf_link": "https://arxiv.org/pdf/2309.08637v4.pdf"
    },
    {
        "title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
        "authors": [
            "Shentong Mo",
            "Miao Xin"
        ],
        "published": "2023-09-14T13:14:51Z",
        "summary": "While the recently introduced Tree of Thoughts (ToT) has heralded\nadvancements in allowing Large Language Models (LLMs) to reason through\nforesight and backtracking for global decision-making, it has overlooked the\ninherent local uncertainties in intermediate decision points or \"thoughts\".\nThese local uncertainties, intrinsic to LLMs given their potential for diverse\nresponses, remain a significant concern in the reasoning process. Addressing\nthis pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a\nreasoning framework tailored for LLMs. Our TouT effectively leverages Monte\nCarlo Dropout to quantify uncertainty scores associated with LLMs' diverse\nlocal responses at these intermediate steps. By marrying this local uncertainty\nquantification with global search algorithms, TouT enhances the model's\nprecision in response generation. We substantiate our approach with rigorous\nexperiments on two demanding planning tasks: Game of 24 and Mini Crosswords.\nThe empirical evidence underscores TouT's superiority over both ToT and\nchain-of-thought prompting methods.",
        "pdf_link": "https://arxiv.org/pdf/2309.07694v1.pdf"
    },
    {
        "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text",
        "authors": [
            "Mahdi Dhaini",
            "Wessel Poelman",
            "Ege Erdogan"
        ],
        "published": "2023-09-14T13:05:20Z",
        "summary": "While recent advancements in the capabilities and widespread accessibility of\ngenerative language models, such as ChatGPT (OpenAI, 2022), have brought about\nvarious benefits by generating fluent human-like text, the task of\ndistinguishing between human- and large language model (LLM) generated text has\nemerged as a crucial problem. These models can potentially deceive by\ngenerating artificial text that appears to be human-generated. This issue is\nparticularly significant in domains such as law, education, and science, where\nensuring the integrity of text is of the utmost importance. This survey\nprovides an overview of the current approaches employed to differentiate\nbetween texts generated by humans and ChatGPT. We present an account of the\ndifferent datasets constructed for detecting ChatGPT-generated text, the\nvarious methods utilized, what qualitative analyses into the characteristics of\nhuman versus ChatGPT-generated text have been performed, and finally, summarize\nour findings into general insights",
        "pdf_link": "https://arxiv.org/pdf/2309.07689v1.pdf"
    },
    {
        "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?",
        "authors": [
            "Rishav Hada",
            "Varun Gumma",
            "Adrian de Wynter",
            "Harshita Diddee",
            "Mohamed Ahmed",
            "Monojit Choudhury",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "published": "2023-09-14T06:41:58Z",
        "summary": "Large Language Models (LLMs) excel in various Natural Language Processing\n(NLP) tasks, yet their evaluation, particularly in languages beyond the top\n$20$, remains inadequate due to existing benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score other models' outputs emerges as\na viable solution, addressing the constraints tied to human annotators and\nestablished benchmarks. In this study, we explore the potential of LLM-based\nevaluators, specifically GPT-4 in enhancing multilingual evaluation by\ncalibrating them against $20$K human judgments across three text-generation\ntasks, five metrics, and eight languages. Our analysis reveals a bias in\nGPT4-based evaluators towards higher scores, underscoring the necessity of\ncalibration with native speaker judgments, especially in low-resource and\nnon-Latin script languages, to ensure accurate evaluation of LLM performance\nacross diverse languages.",
        "pdf_link": "https://arxiv.org/pdf/2309.07462v2.pdf"
    },
    {
        "title": "ChatGPT MT: Competitive for High- (but not Low-) Resource Languages",
        "authors": [
            "Nathaniel R. Robinson",
            "Perez Ogayo",
            "David R. Mortensen",
            "Graham Neubig"
        ],
        "published": "2023-09-14T04:36:00Z",
        "summary": "Large language models (LLMs) implicitly learn to perform a range of language\ntasks, including machine translation (MT). Previous studies explore aspects of\nLLMs' MT capabilities. However, there exist a wide variety of languages for\nwhich recent LLM MT performance has never before been evaluated. Without\npublished experimental evidence on the matter, it is difficult for speakers of\nthe world's diverse languages to know how and whether they can use LLMs for\ntheir languages. We present the first experimental evidence for an expansive\nset of 204 languages, along with MT cost analysis, using the FLORES-200\nbenchmark. Trends reveal that GPT models approach or exceed traditional MT\nmodel performance for some high-resource languages (HRLs) but consistently lag\nfor low-resource languages (LRLs), under-performing traditional MT for 84.1% of\nlanguages we covered. Our analysis reveals that a language's resource level is\nthe most important feature in determining ChatGPT's relative ability to\ntranslate it, and suggests that ChatGPT is especially disadvantaged for LRLs\nand African languages.",
        "pdf_link": "https://arxiv.org/pdf/2309.07423v1.pdf"
    },
    {
        "title": "An Assessment of ChatGPT on Log Data",
        "authors": [
            "Priyanka Mudgal",
            "Rita Wouhaybi"
        ],
        "published": "2023-09-14T04:09:27Z",
        "summary": "Recent development of large language models (LLMs), such as ChatGPT has been\nwidely applied to a wide range of software engineering tasks. Many papers have\nreported their analysis on the potential advantages and limitations of ChatGPT\nfor writing code, summarization, text generation, etc. However, the analysis of\nthe current state of ChatGPT for log processing has received little attention.\nLogs generated by large-scale software systems are complex and hard to\nunderstand. Despite their complexity, they provide crucial information for\nsubject matter experts to understand the system status and diagnose problems of\nthe systems. In this paper, we investigate the current capabilities of ChatGPT\nto perform several interesting tasks on log data, while also trying to identify\nits main shortcomings. Our findings show that the performance of the current\nversion of ChatGPT for log processing is limited, with a lack of consistency in\nresponses and scalability issues. We also outline our views on how we perceive\nthe role of LLMs in the log processing discipline and possible next steps to\nimprove the current capabilities of ChatGPT and the future LLMs in this area.\nWe believe our work can contribute to future academic research to address the\nidentified issues.",
        "pdf_link": "https://arxiv.org/pdf/2309.07938v1.pdf"
    },
    {
        "title": "Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos",
        "authors": [
            "Fen Fang",
            "Yun Liu",
            "Ali Koksal",
            "Qianli Xu",
            "Joo-Hwee Lim"
        ],
        "published": "2023-09-14T03:25:37Z",
        "summary": "A key challenge with procedure planning in instructional videos lies in how\nto handle a large decision space consisting of a multitude of action types that\nbelong to various tasks. To understand real-world video content, an AI agent\nmust proficiently discern these action types (e.g., pour milk, pour water, open\nlid, close lid, etc.) based on brief visual observation. Moreover, it must\nadeptly capture the intricate semantic relation of the action types and task\ngoals, along with the variable action sequences. Recently, notable progress has\nbeen made via the integration of diffusion models and visual representation\nlearning to address the challenge. However, existing models employ rudimentary\nmechanisms to utilize task information to manage the decision space. To\novercome this limitation, we introduce a simple yet effective enhancement - a\nmasked diffusion model. The introduced mask acts akin to a task-oriented\nattention filter, enabling the diffusion/denoising process to concentrate on a\nsubset of action types. Furthermore, to bolster the accuracy of task\nclassification, we harness more potent visual representation learning\ntechniques. In particular, we learn a joint visual-text embedding, where a text\nembedding is generated by prompting a pre-trained vision-language model to\nfocus on human actions. We evaluate the method on three public datasets and\nachieve state-of-the-art performance on multiple metrics. Code is available at\nhttps://github.com/ffzzy840304/Masked-PDPP.",
        "pdf_link": "https://arxiv.org/pdf/2309.07409v1.pdf"
    },
    {
        "title": "Less is More for Long Document Summary Evaluation by LLMs",
        "authors": [
            "Yunshu Wu",
            "Hayate Iso",
            "Pouya Pezeshkpour",
            "Nikita Bhutani",
            "Estevam Hruschka"
        ],
        "published": "2023-09-14T01:59:15Z",
        "summary": "Large Language Models (LLMs) have shown promising performance in summary\nevaluation tasks, yet they face challenges such as high computational costs and\nthe Lost-in-the-Middle problem where important information in the middle of\nlong documents is often overlooked. To address these issues, this paper\nintroduces a novel approach, Extract-then-Evaluate, which involves extracting\nkey sentences from a long source document and then evaluating the summary by\nprompting LLMs. The results reveal that the proposed method not only\nsignificantly reduces evaluation costs but also exhibits a higher correlation\nwith human evaluations. Furthermore, we provide practical recommendations for\noptimal document length and sentence extraction methods, contributing to the\ndevelopment of cost-effective yet more accurate methods for LLM-based text\ngeneration evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2309.07382v2.pdf"
    },
    {
        "title": "In-Contextual Gender Bias Suppression for Large Language Models",
        "authors": [
            "Daisuke Oba",
            "Masahiro Kaneko",
            "Danushka Bollegala"
        ],
        "published": "2023-09-13T18:39:08Z",
        "summary": "Despite their impressive performance in a wide range of NLP tasks, Large\nLanguage Models (LLMs) have been reported to encode worrying-levels of gender\nbiases. Prior work has proposed debiasing methods that require human labelled\nexamples, data augmentation and fine-tuning of LLMs, which are computationally\ncostly. Moreover, one might not even have access to the model parameters for\nperforming debiasing such as in the case of closed LLMs such as GPT-4. To\naddress this challenge, we propose bias suppression that prevents biased\ngenerations of LLMs by simply providing textual preambles constructed from\nmanually designed templates and real-world statistics, without accessing to\nmodel parameters. We show that, using CrowsPairs dataset, our textual preambles\ncovering counterfactual statements can suppress gender biases in English LLMs\nsuch as LLaMA2. Moreover, we find that gender-neutral descriptions of\ngender-biased objects can also suppress their gender biases. Moreover, we show\nthat bias suppression has acceptable adverse effect on downstream task\nperformance with HellaSwag and COPA.",
        "pdf_link": "https://arxiv.org/pdf/2309.07251v2.pdf"
    },
    {
        "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
        "authors": [
            "Yuhui Li",
            "Fangyun Wei",
            "Jinjing Zhao",
            "Chao Zhang",
            "Hongyang Zhang"
        ],
        "published": "2023-09-13T17:59:09Z",
        "summary": "Large language models (LLMs) often demonstrate inconsistencies with human\npreferences. Previous research typically gathered human preference data and\nthen aligned the pre-trained models using reinforcement learning or instruction\ntuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without\nrequiring alignment data is more appealing. This work explores the potential of\nthe latter setting. We discover that by integrating self-evaluation and rewind\nmechanisms, unaligned LLMs can directly produce responses consistent with human\npreferences via self-boosting. We introduce a novel inference method,\nRewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to\nevaluate their own generation and use the evaluation results to guide rewind\nand generation for AI safety. Notably, RAIN operates without the need of extra\ndata for model alignment and abstains from any training, gradient computation,\nor parameter updates. Experimental results evaluated by GPT-4 and humans\ndemonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the\nharmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while\nmaintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the\ntruthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.",
        "pdf_link": "https://arxiv.org/pdf/2309.07124v2.pdf"
    },
    {
        "title": "Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding",
        "authors": [
            "Rico Sennrich",
            "Jannis Vamvas",
            "Alireza Mohammadshahi"
        ],
        "published": "2023-09-13T17:15:27Z",
        "summary": "Hallucinations and off-target translation remain unsolved problems in MT,\nespecially for low-resource languages and massively multilingual models. In\nthis paper, we introduce two related methods to mitigate these failure cases\nwith a modified decoding objective, without either requiring retraining or\nexternal models. In source-contrastive decoding, we search for a translation\nthat is probable given the correct input, but improbable given a random input\nsegment. In language-contrastive decoding, we search for a translation that is\nprobable, but improbable given the wrong language indicator token. Experiments\non the massively multilingual models M2M-100 (418M) and SMaLL-100 show that\nthese methods suppress hallucinations and off-target translations, reducing the\nnumber of translations with segment-level chrF2 below 10 by 67-83% on average,\nand the number of translations with oscillatory hallucinations by 75-92% on\naverage, across 57 tested translation directions. In a proof of concept on\nout-of-English translation, we also show that we can suppress off-target\ntranslations with large language models. We release our source code at\nhttps://github.com/ZurichNLP/ContraDecode.",
        "pdf_link": "https://arxiv.org/pdf/2309.07098v2.pdf"
    },
    {
        "title": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
        "authors": [
            "Zhexin Zhang",
            "Leqi Lei",
            "Lindong Wu",
            "Rui Sun",
            "Yongkang Huang",
            "Chong Long",
            "Xiao Liu",
            "Xuanyu Lei",
            "Jie Tang",
            "Minlie Huang"
        ],
        "published": "2023-09-13T15:56:50Z",
        "summary": "With the rapid development of Large Language Models (LLMs), increasing\nattention has been paid to their safety concerns. Consequently, evaluating the\nsafety of LLMs has become an essential task for facilitating the broad\napplications of LLMs. Nevertheless, the absence of comprehensive safety\nevaluation benchmarks poses a significant impediment to effectively assess and\nenhance the safety of LLMs. In this work, we present SafetyBench, a\ncomprehensive benchmark for evaluating the safety of LLMs, which comprises\n11,435 diverse multiple choice questions spanning across 7 distinct categories\nof safety concerns. Notably, SafetyBench also incorporates both Chinese and\nEnglish data, facilitating the evaluation in both languages. Our extensive\ntests over 25 popular Chinese and English LLMs in both zero-shot and few-shot\nsettings reveal a substantial performance advantage for GPT-4 over its\ncounterparts, and there is still significant room for improving the safety of\ncurrent LLMs. We believe SafetyBench will enable fast and comprehensive\nevaluation of LLMs' safety, and foster the development of safer LLMs. Data and\nevaluation guidelines are available at https://github.com/thu-coai/SafetyBench.\nSubmission entrance and leaderboard are available at\nhttps://llmbench.ai/safety.",
        "pdf_link": "https://arxiv.org/pdf/2309.07045v1.pdf"
    },
    {
        "title": "Generative AI",
        "authors": [
            "Stefan Feuerriegel",
            "Jochen Hartmann",
            "Christian Janiesch",
            "Patrick Zschech"
        ],
        "published": "2023-09-13T08:21:59Z",
        "summary": "The term \"generative AI\" refers to computational techniques that are capable\nof generating seemingly new, meaningful content such as text, images, or audio\nfrom training data. The widespread diffusion of this technology with examples\nsuch as Dall-E 2, GPT-4, and Copilot is currently revolutionizing the way we\nwork and communicate with each other. In this article, we provide a\nconceptualization of generative AI as an entity in socio-technical systems and\nprovide examples of models, systems, and applications. Based on that, we\nintroduce limitations of current generative AI and provide an agenda for\nBusiness & Information Systems Engineering (BISE) research. Different from\nprevious works, we focus on generative AI in the context of information\nsystems, and, to this end, we discuss several opportunities and challenges that\nare unique to the BISE community and make suggestions for impactful directions\nfor BISE research.",
        "pdf_link": "https://arxiv.org/pdf/2309.07930v1.pdf"
    },
    {
        "title": "Scaled Prompt-Tuning for Few-Shot Natural Language Generation",
        "authors": [
            "Ting Hu",
            "Christoph Meinel",
            "Haojin Yang"
        ],
        "published": "2023-09-13T07:12:31Z",
        "summary": "The increasingly Large Language Models (LLMs) demonstrate stronger language\nunderstanding and generation capabilities, while the memory demand and\ncomputation cost of fine-tuning LLMs on downstream tasks are non-negligible.\nBesides, fine-tuning generally requires a certain amount of data from\nindividual tasks whilst data collection cost is another issue to consider in\nreal-world applications. In this work, we focus on Parameter-Efficient\nFine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG),\nwhich freeze most parameters in LLMs and tune a small subset of parameters in\nfew-shot cases so that memory footprint, training cost, and labeling cost are\nreduced while maintaining or even improving the performance. We propose a\nScaled Prompt-Tuning (SPT) method which surpasses conventional PT with better\nperformance and generalization ability but without an obvious increase in\ntraining cost. Further study on intermediate SPT suggests the superior\ntransferability of SPT in few-shot scenarios, providing a recipe for\ndata-deficient and computation-limited circumstances. Moreover, a comprehensive\ncomparison of existing PEFT methods reveals that certain approaches exhibiting\ndecent performance with modest training cost such as Prefix-Tuning in prior\nstudy could struggle in few-shot NLG tasks, especially on challenging datasets.",
        "pdf_link": "https://arxiv.org/pdf/2309.06759v1.pdf"
    },
    {
        "title": "TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",
        "authors": [
            "Siyao Zhang",
            "Daocheng Fu",
            "Zhao Zhang",
            "Bin Yu",
            "Pinlong Cai"
        ],
        "published": "2023-09-13T04:47:43Z",
        "summary": "With the promotion of chatgpt to the public, Large language models indeed\nshowcase remarkable common sense, reasoning, and planning skills, frequently\nproviding insightful guidance. These capabilities hold significant promise for\ntheir application in urban traffic management and control. However, LLMs\nstruggle with addressing traffic issues, especially processing numerical data\nand interacting with simulations, limiting their potential in solving\ntraffic-related challenges. In parallel, specialized traffic foundation models\nexist but are typically designed for specific tasks with limited input-output\ninteractions. Combining these models with LLMs presents an opportunity to\nenhance their capacity for tackling complex traffic-related problems and\nproviding insightful suggestions. To bridge this gap, we present TrafficGPT, a\nfusion of ChatGPT and traffic foundation models. This integration yields the\nfollowing key enhancements: 1) empowering ChatGPT with the capacity to view,\nanalyze, process traffic data, and provide insightful decision support for\nurban transportation system management; 2) facilitating the intelligent\ndeconstruction of broad and complex tasks and sequential utilization of traffic\nfoundation models for their gradual completion; 3) aiding human decision-making\nin traffic control through natural language dialogues; and 4) enabling\ninteractive feedback and solicitation of revised outcomes. By seamlessly\nintertwining large language model and traffic expertise, TrafficGPT not only\nadvances traffic management but also offers a novel approach to leveraging AI\ncapabilities in this domain. The TrafficGPT demo can be found in\nhttps://github.com/lijlansg/TrafficGPT.git.",
        "pdf_link": "https://arxiv.org/pdf/2309.06719v1.pdf"
    },
    {
        "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics",
        "authors": [
            "Jiayang Song",
            "Zhehua Zhou",
            "Jiawei Liu",
            "Chunrong Fang",
            "Zhan Shu",
            "Lei Ma"
        ],
        "published": "2023-09-13T02:56:56Z",
        "summary": "Although Deep Reinforcement Learning (DRL) has achieved notable success in\nnumerous robotic applications, designing a high-performing reward function\nremains a challenging task that often requires substantial manual input.\nRecently, Large Language Models (LLMs) have been extensively adopted to address\ntasks demanding in-depth common-sense knowledge, such as reasoning and\nplanning. Recognizing that reward function design is also inherently linked to\nsuch knowledge, LLM offers a promising potential in this context. Motivated by\nthis, we propose in this work a novel LLM framework with a self-refinement\nmechanism for automated reward function design. The framework commences with\nthe LLM formulating an initial reward function based on natural language\ninputs. Then, the performance of the reward function is assessed, and the\nresults are presented back to the LLM for guiding its self-refinement process.\nWe examine the performance of our proposed framework through a variety of\ncontinuous robotic control tasks across three diverse robotic systems. The\nresults indicate that our LLM-designed reward functions are able to rival or\neven surpass manually designed reward functions, highlighting the efficacy and\napplicability of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2309.06687v2.pdf"
    },
    {
        "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL",
        "authors": [
            "Hao Sun",
            "Alihan H\u00fcy\u00fck",
            "Mihaela van der Schaar"
        ],
        "published": "2023-09-13T01:12:52Z",
        "summary": "In this study, we aim to enhance the arithmetic reasoning ability of Large\nLanguage Models (LLMs) through zero-shot prompt optimization. We identify a\npreviously overlooked objective of query dependency in such optimization and\nelucidate two ensuing challenges that impede the successful and economical\ndesign of prompt optimization techniques. One primary issue is the absence of\nan effective method to evaluate prompts during inference when the golden answer\nis unavailable. Concurrently, learning via interactions with the LLMs to\nnavigate the expansive natural language prompting space proves to be\nresource-intensive. To address this, we introduce Prompt-OIRL, which harnesses\noffline inverse reinforcement learning to draw insights from offline prompting\ndemonstration data. Such data exists as by-products when diverse prompts are\nbenchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent\nprompt optimization objective is achieved by first learning an offline reward\nmodel. This model can evaluate any query-prompt pairs without accessing LLMs.\nSubsequently, a best-of-N strategy is deployed to recommend the optimal prompt.\nOur experimental evaluations across various LLM scales and arithmetic reasoning\ndatasets underscore both the efficacy and economic viability of the proposed\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2309.06553v4.pdf"
    },
    {
        "title": "Statistical Rejection Sampling Improves Preference Optimization",
        "authors": [
            "Tianqi Liu",
            "Yao Zhao",
            "Rishabh Joshi",
            "Misha Khalman",
            "Mohammad Saleh",
            "Peter J. Liu",
            "Jialu Liu"
        ],
        "published": "2023-09-13T01:07:25Z",
        "summary": "Improving the alignment of language models with human preferences remains an\nactive research challenge. Previous approaches have primarily utilized\nReinforcement Learning from Human Feedback (RLHF) via online RL methods such as\nProximal Policy Optimization (PPO). Recently, offline methods such as Sequence\nLikelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have\nemerged as attractive alternatives, offering improvements in stability and\nscalability while maintaining competitive performance. SLiC refines its loss\nfunction using sequence pairs sampled from a supervised fine-tuned (SFT)\npolicy, while DPO directly optimizes language models based on preference data,\nforegoing the need for a separate reward model. However, the maximum likelihood\nestimator (MLE) of the target optimal policy requires labeled preference pairs\nsampled from that policy. DPO's lack of a reward model constrains its ability\nto sample preference pairs from the optimal policy, and SLiC is restricted to\nsampling preference pairs only from the SFT policy. To address these\nlimitations, we introduce a novel approach called Statistical Rejection\nSampling Optimization (RSO) that aims to source preference data from the target\noptimal policy using rejection sampling, enabling a more accurate estimation of\nthe optimal policy. We also propose a unified framework that enhances the loss\nfunctions used in both SLiC and DPO from a preference modeling standpoint.\nThrough extensive experiments across three diverse tasks, we demonstrate that\nRSO consistently outperforms both SLiC and DPO on evaluations from both Large\nLanguage Model (LLM) and human raters.",
        "pdf_link": "https://arxiv.org/pdf/2309.06657v2.pdf"
    },
    {
        "title": "Exploring Large Language Models for Ontology Alignment",
        "authors": [
            "Yuan He",
            "Jiaoyan Chen",
            "Hang Dong",
            "Ian Horrocks"
        ],
        "published": "2023-09-12T17:01:02Z",
        "summary": "This work investigates the applicability of recent generative Large Language\nModels (LLMs), such as the GPT series and Flan-T5, to ontology alignment for\nidentifying concept equivalence mappings across ontologies. To test the\nzero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging\nsubsets from two equivalence matching datasets of the OAEI Bio-ML track, taking\ninto account concept labels and structural contexts. Preliminary findings\nsuggest that LLMs have the potential to outperform existing ontology alignment\nsystems like BERTMap, given careful framework and prompt design.",
        "pdf_link": "https://arxiv.org/pdf/2309.07172v1.pdf"
    },
    {
        "title": "The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models",
        "authors": [
            "Dimitris Spathis",
            "Fahim Kawsar"
        ],
        "published": "2023-09-12T13:51:29Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\nacross diverse tasks, leading individuals to increasingly use them as personal\nassistants and universal computing engines. Nevertheless, a notable obstacle\nemerges when feeding numerical/temporal data into these models, such as data\nsourced from wearables or electronic health records. LLMs employ tokenizers in\ntheir input that break down text into smaller units. However, tokenizers are\nnot designed to represent numerical values and might struggle to understand\nrepetitive patterns and context, treating consecutive values as separate tokens\nand disregarding their temporal relationships. Here, we discuss recent works\nthat employ LLMs for human-centric tasks such as in mobile health sensing and\npresent a case study showing that popular LLMs tokenize temporal data\nincorrectly. To address that, we highlight potential solutions such as prompt\ntuning with lightweight embedding layers as well as multimodal adapters, that\ncan help bridge this \"modality gap\". While the capability of language models to\ngeneralize to other modalities with minimal or no finetuning is exciting, this\npaper underscores the fact that their outputs cannot be meaningful if they\nstumble over input nuances.",
        "pdf_link": "https://arxiv.org/pdf/2309.06236v1.pdf"
    },
    {
        "title": "Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions",
        "authors": [
            "Myriam Safrai",
            "Amos Azaria"
        ],
        "published": "2023-09-12T05:54:45Z",
        "summary": "As Large Language Models (LLMs) are predictive models building their response\nbased on the words in the prompts, there is a risk that small talk and\nirrelevant information may alter the response and the suggestion given.\nTherefore, this study aims to investigate the impact of medical data mixed with\nsmall talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3\nquestions were used as a model for relevant medical data. We use both multiple\nchoice and open ended questions. We gathered small talk sentences from human\nparticipants using the Mechanical Turk platform. Both sets of USLME questions\nwere arranged in a pattern where each sentence from the original questions was\nfollowed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both\nsets of questions with and without the small talk sentences. A board-certified\nphysician analyzed the answers by ChatGPT and compared them to the formal\ncorrect answer. The analysis results demonstrate that the ability of\nChatGPT-3.5 to answer correctly was impaired when small talk was added to\nmedical data for multiple-choice questions (72.1\\% vs. 68.9\\%) and open\nquestions (61.5\\% vs. 44.3\\%; p=0.01), respectively. In contrast, small talk\nphrases did not impair ChatGPT-4 ability in both types of questions (83.6\\% and\n66.2\\%, respectively). According to these results, ChatGPT-4 seems more\naccurate than the earlier 3.5 version, and it appears that small talk does not\nimpair its capability to provide medical recommendations. Our results are an\nimportant first step in understanding the potential and limitations of\nutilizing ChatGPT and other LLMs for physician-patient interactions, which\ninclude casual conversations.",
        "pdf_link": "https://arxiv.org/pdf/2309.08625v1.pdf"
    },
    {
        "title": "The Moral Machine Experiment on Large Language Models",
        "authors": [
            "Kazuhiro Takemoto"
        ],
        "published": "2023-09-12T04:49:39Z",
        "summary": "As large language models (LLMs) become more deeply integrated into various\nsectors, understanding how they make moral judgments has become crucial,\nparticularly in the realm of autonomous driving. This study utilized the Moral\nMachine framework to investigate the ethical decision-making tendencies of\nprominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their\nresponses to human preferences. While LLMs' and humans' preferences such as\nprioritizing humans over pets and favoring saving more lives are broadly\naligned, PaLM 2 and Llama 2, especially, evidence distinct deviations.\nAdditionally, despite the qualitative similarities between the LLM and human\npreferences, there are significant quantitative disparities, suggesting that\nLLMs might lean toward more uncompromising decisions, compared to the milder\ninclinations of humans. These insights elucidate the ethical frameworks of LLMs\nand their potential implications for autonomous driving.",
        "pdf_link": "https://arxiv.org/pdf/2309.05958v1.pdf"
    },
    {
        "title": "Balanced and Explainable Social Media Analysis for Public Health with Large Language Models",
        "authors": [
            "Yan Jiang",
            "Ruihong Qiu",
            "Yi Zhang",
            "Peng-Fei Zhang"
        ],
        "published": "2023-09-12T04:15:34Z",
        "summary": "As social media becomes increasingly popular, more and more public health\nactivities emerge, which is worth noting for pandemic monitoring and government\ndecision-making. Current techniques for public health analysis involve popular\nmodels such as BERT and large language models (LLMs). Although recent progress\nin LLMs has shown a strong ability to comprehend knowledge by being fine-tuned\non specific domain datasets, the costs of training an in-domain LLM for every\nspecific public health task are especially expensive. Furthermore, such kinds\nof in-domain datasets from social media are generally highly imbalanced, which\nwill hinder the efficiency of LLMs tuning. To tackle these challenges, the data\nimbalance issue can be overcome by sophisticated data augmentation methods for\nsocial media datasets. In addition, the ability of the LLMs can be effectively\nutilised by prompting the model properly. In light of the above discussion, in\nthis paper, a novel ALEX framework is proposed for social media analysis on\npublic health. Specifically, an augmentation pipeline is developed to resolve\nthe data imbalance issue. Furthermore, an LLMs explanation mechanism is\nproposed by prompting an LLM with the predicted results from BERT models.\nExtensive experiments conducted on three tasks at the Social Media Mining for\nHealth 2023 (SMM4H) competition with the first ranking in two tasks demonstrate\nthe superior performance of the proposed ALEX method. Our code has been\nreleased in https://github.com/YanJiangJerry/ALEX.",
        "pdf_link": "https://arxiv.org/pdf/2309.05951v1.pdf"
    },
    {
        "title": "Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs",
        "authors": [
            "Walid S. Saba"
        ],
        "published": "2023-09-12T02:14:05Z",
        "summary": "In our opinion the exuberance surrounding the relative success of data-driven\nlarge language models (LLMs) is slightly misguided and for several reasons (i)\nLLMs cannot be relied upon for factual information since for LLMs all ingested\ntext (factual or non-factual) was created equal; (ii) due to their subsymbolic\nna-ture, whatever 'knowledge' these models acquire about language will always\nbe buried in billions of microfeatures (weights), none of which is meaningful\non its own; and (iii) LLMs will often fail to make the correct inferences in\nseveral linguistic contexts (e.g., nominal compounds, copredication, quantifier\nscope ambi-guities, intensional contexts. Since we believe the relative success\nof data-driven large language models (LLMs) is not a reflection on the symbolic\nvs. subsymbol-ic debate but a reflection on applying the successful strategy of\na bottom-up reverse engineering of language at scale, we suggest in this paper\napplying the effective bottom-up strategy in a symbolic setting resulting in\nsymbolic, explainable, and ontologically grounded language models.",
        "pdf_link": "https://arxiv.org/pdf/2309.05918v3.pdf"
    },
    {
        "title": "Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing",
        "authors": [
            "Nunzio Lor\u00e8",
            "Babak Heydari"
        ],
        "published": "2023-09-12T00:54:15Z",
        "summary": "This paper investigates the strategic decision-making capabilities of three\nLarge Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, within the framework\nof game theory. Utilizing four canonical two-player games -- Prisoner's\nDilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we explore how these\nmodels navigate social dilemmas, situations where players can either cooperate\nfor a collective benefit or defect for individual gain. Crucially, we extend\nour analysis to examine the role of contextual framing, such as diplomatic\nrelations or casual friendships, in shaping the models' decisions. Our findings\nreveal a complex landscape: while GPT-3.5 is highly sensitive to contextual\nframing, it shows limited ability to engage in abstract strategic reasoning.\nBoth GPT-4 and LLaMa-2 adjust their strategies based on game structure and\ncontext, but LLaMa-2 exhibits a more nuanced understanding of the games'\nunderlying mechanics. These results highlight the current limitations and\nvaried proficiencies of LLMs in strategic decision-making, cautioning against\ntheir unqualified use in tasks requiring complex strategic reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2309.05898v1.pdf"
    },
    {
        "title": "PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis",
        "authors": [
            "Dylan Zhang",
            "Xuchao Zhang",
            "Chetan Bansal",
            "Pedro Las-Casas",
            "Rodrigo Fonseca",
            "Saravan Rajmohan"
        ],
        "published": "2023-09-11T21:24:00Z",
        "summary": "Major cloud providers have employed advanced AI-based solutions like large\nlanguage models to aid humans in identifying the root causes of cloud\nincidents. Despite the growing prevalence of AI-driven assistants in the root\ncause analysis process, their effectiveness in assisting on-call engineers is\nconstrained by low accuracy due to the intrinsic difficulty of the task, a\npropensity for LLM-based approaches to hallucinate, and difficulties in\ndistinguishing these well-disguised hallucinations. To address this challenge,\nwe propose to perform confidence estimation for the predictions to help on-call\nengineers make decisions on whether to adopt the model prediction. Considering\nthe black-box nature of many LLM-based root cause predictors, fine-tuning or\ntemperature-scaling-based approaches are inapplicable. We therefore design an\ninnovative confidence estimation framework based on prompting\nretrieval-augmented large language models (LLMs) that demand a minimal amount\nof information from the root cause predictor. This approach consists of two\nscoring phases: the LLM-based confidence estimator first evaluates its\nconfidence in making judgments in the face of the current incident that\nreflects its ``grounded-ness\" level in reference data, then rates the root\ncause prediction based on historical references. An optimization step combines\nthese two scores for a final confidence assignment. We show that our method is\nable to produce calibrated confidence estimates for predicted root causes,\nvalidate the usefulness of retrieved historical data and the prompting strategy\nas well as the generalizability across different root cause prediction models.\nOur study takes an important move towards reliably and effectively embedding\nLLMs into cloud incident management systems.",
        "pdf_link": "https://arxiv.org/pdf/2309.05833v3.pdf"
    },
    {
        "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models",
        "authors": [
            "Mansi Sakarvadia",
            "Aswathy Ajith",
            "Arham Khan",
            "Daniel Grzenda",
            "Nathaniel Hudson",
            "Andr\u00e9 Bauer",
            "Kyle Chard",
            "Ian Foster"
        ],
        "published": "2023-09-11T16:39:30Z",
        "summary": "Answering multi-hop reasoning questions requires retrieving and synthesizing\ninformation from diverse sources. Large Language Models (LLMs) struggle to\nperform such reasoning consistently. Here we propose an approach to pinpoint\nand rectify multi-hop reasoning failures through targeted memory injections on\nLLM attention heads. First, we analyze the per-layer activations of GPT-2\nmodels in response to single and multi-hop prompts. We then propose a mechanism\nthat allows users to inject pertinent prompt-specific information, which we\nrefer to as \"memories,\" at critical LLM locations during inference. By thus\nenabling the LLM to incorporate additional relevant information during\ninference, we enhance the quality of multi-hop prompt completions. We show\nempirically that a simple, efficient, and targeted memory injection into a key\nattention layer can often increase the probability of the desired next token in\nmulti-hop tasks, by up to 424%.",
        "pdf_link": "https://arxiv.org/pdf/2309.05605v3.pdf"
    },
    {
        "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
        "authors": [
            "Wenhua Cheng",
            "Weiwei Zhang",
            "Haihao Shen",
            "Yiyang Cai",
            "Xin He",
            "Kaokao Lv"
        ],
        "published": "2023-09-11T14:58:23Z",
        "summary": "Large Language Models (LLMs) have proven their exceptional capabilities in\nperforming language-related tasks. However, their deployment poses significant\nchallenges due to their considerable memory and storage requirements. In\nresponse to this issue, weight-only quantization, particularly 3 and 4-bit\nweight-only quantization, has emerged as one of the most viable solutions. As\nthe number of bits decreases, the quantization grid broadens, thus emphasizing\nthe importance of up and down rounding. While previous studies have\ndemonstrated that fine-tuning up and down rounding with the addition of\nperturbations can enhance accuracy in some scenarios, our study is driven by\nthe precise and limited boundary of these perturbations, where only the\nthreshold for altering the rounding value is of significance. Consequently, we\npropose a concise and highly effective approach for optimizing the weight\nrounding task. Our method, named SignRound, involves lightweight block-wise\ntuning using signed gradient descent, enabling us to achieve outstanding\nresults within 400 steps. SignRound competes impressively against recent\nmethods without introducing additional inference overhead. The source code will\nbe publicly available at \\url{https://github.com/intel/neural-compressor} soon.",
        "pdf_link": "https://arxiv.org/pdf/2309.05516v2.pdf"
    },
    {
        "title": "Evaluating the Deductive Competence of Large Language Models",
        "authors": [
            "S. M. Seals",
            "Valerie L. Shalin"
        ],
        "published": "2023-09-11T13:47:07Z",
        "summary": "The development of highly fluent large language models (LLMs) has prompted\nincreased interest in assessing their reasoning and problem-solving\ncapabilities. We investigate whether several LLMs can solve a classic type of\ndeductive reasoning problem from the cognitive science literature. The tested\nLLMs have limited abilities to solve these problems in their conventional form.\nWe performed follow up experiments to investigate if changes to the\npresentation format and content improve model performance. We do find\nperformance differences between conditions; however, they do not improve\noverall performance. Moreover, we find that performance interacts with\npresentation format and content in unexpected ways that differ from human\nperformance. Overall, our results suggest that LLMs have unique reasoning\nbiases that are only partially predicted from human reasoning performance.",
        "pdf_link": "https://arxiv.org/pdf/2309.05452v1.pdf"
    },
    {
        "title": "Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis",
        "authors": [
            "Li Du",
            "Yequan Wang",
            "Xingrun Xing",
            "Yiqun Ya",
            "Xiang Li",
            "Xin Jiang",
            "Xuezhi Fang"
        ],
        "published": "2023-09-11T03:35:00Z",
        "summary": "Although demonstrating superb performance on various NLP tasks, large\nlanguage models (LLMs) still suffer from the hallucination problem, which\nthreatens the reliability of LLMs. To measure the level of hallucination of\nLLMs, previous works first categorize the hallucination according to the\nphenomenon similarity, then quantify the proportion that model outputs contain\nhallucinatory contents. However, such hallucination rates could easily be\ndistorted by confounders. Moreover, such hallucination rates could not reflect\nthe reasons for the hallucination, as similar hallucinatory phenomena may\noriginate from different sources. To address these issues, we propose to\ncombine the hallucination level quantification and hallucination reason\ninvestigation through an association analysis, which builds the relationship\nbetween the hallucination rate of LLMs with a set of risk factors. In this way,\nwe are able to observe the hallucination level under each value of each risk\nfactor, examining the contribution and statistical significance of each risk\nfactor, meanwhile excluding the confounding effect of other factors.\nAdditionally, by recognizing the risk factors according to a taxonomy of model\ncapability, we reveal a set of potential deficiencies in commonsense\nmemorization, relational reasoning, and instruction following, which may\nfurther provide guidance for the pretraining and supervised fine-tuning process\nof LLMs to mitigate the hallucination.",
        "pdf_link": "https://arxiv.org/pdf/2309.05217v1.pdf"
    },
    {
        "title": "Does Writing with Language Models Reduce Content Diversity?",
        "authors": [
            "Vishakh Padmakumar",
            "He He"
        ],
        "published": "2023-09-11T02:16:47Z",
        "summary": "Large language models (LLMs) have led to a surge in collaborative writing\nwith model assistance. As different users incorporate suggestions from the same\nmodel, there is a risk of decreased diversity in the produced content,\npotentially limiting diverse perspectives in public discourse. In this work, we\nmeasure the impact of co-writing on diversity via a controlled experiment,\nwhere users write argumentative essays in three setups -- using a base LLM\n(GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We\ndevelop a set of diversity metrics and find that writing with InstructGPT (but\nnot the GPT3) results in a statistically significant reduction in diversity.\nSpecifically, it increases the similarity between the writings of different\nauthors and reduces the overall lexical and content diversity. We additionally\nfind that this effect is mainly attributable to InstructGPT contributing less\ndiverse text to co-written essays. In contrast, the user-contributed text\nremains unaffected by model collaboration. This suggests that the recent\nimprovement in generation quality from adapting models to human feedback might\ncome at the cost of more homogeneous and less diverse content.",
        "pdf_link": "https://arxiv.org/pdf/2309.05196v2.pdf"
    },
    {
        "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
        "authors": [
            "Zhengxiang Shi",
            "Aldo Lipani"
        ],
        "published": "2023-09-11T00:02:05Z",
        "summary": "Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving substantial memory and time costs compared to vanilla\nPT and its variants, without changing trainable parameter sizes. Through\nextensive experiments on 23 natural language processing (NLP) and\nvision-language (VL) tasks, we demonstrate that DePT outperforms\nstate-of-the-art PEFT approaches, including the full fine-tuning baseline, in\nsome scenarios. Additionally, we empirically show that DEPT grows more\nefficient as the model size increases. Our further study reveals that DePT\nintegrates seamlessly with parameter-efficient transfer learning in the\nfew-shot learning setting and highlights its adaptability to various model\narchitectures and sizes.",
        "pdf_link": "https://arxiv.org/pdf/2309.05173v5.pdf"
    },
    {
        "title": "Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps",
        "authors": [
            "Yaonai Wei",
            "Tuo Zhang",
            "Han Zhang",
            "Tianyang Zhong",
            "Lin Zhao",
            "Zhengliang Liu",
            "Chong Ma",
            "Songyao Zhang",
            "Muheng Shang",
            "Lei Du",
            "Xiao Li",
            "Tianming Liu",
            "Junwei Han"
        ],
        "published": "2023-09-10T13:06:45Z",
        "summary": "Over decades, neuroscience has accumulated a wealth of research results in\nthe text modality that can be used to explore cognitive processes.\nMeta-analysis is a typical method that successfully establishes a link from\ntext queries to brain activation maps using these research results, but it\nstill relies on an ideal query environment. In practical applications, text\nqueries used for meta-analyses may encounter issues such as semantic redundancy\nand ambiguity, resulting in an inaccurate mapping to brain images. On the other\nhand, large language models (LLMs) like ChatGPT have shown great potential in\ntasks such as context understanding and reasoning, displaying a high degree of\nconsistency with human natural language. Hence, LLMs could improve the\nconnection between text modality and neuroscience, resolving existing\nchallenges of meta-analyses. In this study, we propose a method called\nChat2Brain that combines LLMs to basic text-2-image model, known as Text2Brain,\nto map open-ended semantic queries to brain activation maps in data-scarce and\ncomplex query environments. By utilizing the understanding and reasoning\ncapabilities of LLMs, the performance of the mapping model is optimized by\ntransferring text queries to semantic queries. We demonstrate that Chat2Brain\ncan synthesize anatomically plausible neural activation patterns for more\ncomplex tasks of text queries.",
        "pdf_link": "https://arxiv.org/pdf/2309.05021v1.pdf"
    },
    {
        "title": "Towards LLM-based Autograding for Short Textual Answers",
        "authors": [
            "Johannes Schneider",
            "Bernd Schenk",
            "Christina Niklaus",
            "Michaelis Vlachos"
        ],
        "published": "2023-09-09T22:25:56Z",
        "summary": "Grading of exams is an important, labor intensive, subjective, repetitive and\nfrequently challenging task. The feasibility of autograding textual responses\nhas greatly increased thanks to the availability of large language models\n(LLMs) such as ChatGPT and because of the substantial influx of data brought\nabout by digitalization. However, entrusting AI models with decision-making\nroles raises ethical considerations, mainly stemming from potential biases and\nissues related to generating false information. Thus, in this manuscript we\nprovide an evaluation of a large language model for the purpose of autograding,\nwhile also highlighting how LLMs can support educators in validating their\ngrading procedures. Our evaluation is targeted towards automatic short textual\nanswers grading (ASAG), spanning various languages and examinations from two\ndistinct courses. Our findings suggest that while \"out-of-the-box\" LLMs provide\na valuable tool to provide a complementary perspective, their readiness for\nindependent automated grading remains a work in progress, necessitating human\noversight.",
        "pdf_link": "https://arxiv.org/pdf/2309.11508v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
        "authors": [
            "Pranay Dighe",
            "Yi Su",
            "Shangshang Zheng",
            "Yunshu Liu",
            "Vineet Garg",
            "Xiaochuan Niu",
            "Ahmed Tewfik"
        ],
        "published": "2023-09-09T17:02:33Z",
        "summary": "While large language models excel in a variety of natural language processing\n(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they\nmust either rely on off-the-shelf automatic speech recognition (ASR) systems\nfor transcription, or be equipped with an in-built speech modality. This work\nfocuses on the former scenario, where LLM's accuracy on SLU tasks is\nconstrained by the accuracy of a fixed ASR system on the spoken input.\nSpecifically, we tackle speech-intent classification task, where a high\nword-error-rate can limit the LLM's ability to understand the spoken intent.\nInstead of chasing a high accuracy by designing complex or specialized\narchitectures regardless of deployment costs, we seek to answer how far we can\ngo without substantially changing the underlying ASR and LLM, which can\npotentially be shared by multiple unrelated tasks. To this end, we propose\nprompting the LLM with an n-best list of ASR hypotheses instead of only the\nerror-prone 1-best hypothesis. We explore prompt-engineering to explain the\nconcept of n-best lists to the LLM; followed by the finetuning of Low-Rank\nAdapters on the downstream tasks. Our approach using n-best lists proves to be\neffective on a device-directed speech detection task as well as on a keyword\nspotting task, where systems using n-best list prompts outperform those using\n1-best ASR hypothesis; thus paving the way for an efficient method to exploit\nASR uncertainty via LLMs for speech-based applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.04842v2.pdf"
    },
    {
        "title": "Code-Style In-Context Learning for Knowledge-Based Question Answering",
        "authors": [
            "Zhijie Nie",
            "Richong Zhang",
            "Zhongyuan Wang",
            "Xudong Liu"
        ],
        "published": "2023-09-09T06:27:00Z",
        "summary": "Current methods for Knowledge-Based Question Answering (KBQA) usually rely on\ncomplex training techniques and model frameworks, leading to many limitations\nin practical applications. Recently, the emergence of In-Context Learning (ICL)\ncapabilities in Large Language Models (LLMs) provides a simple and\ntraining-free semantic parsing paradigm for KBQA: Given a small number of\nquestions and their labeled logical forms as demo examples, LLMs can understand\nthe task intent and generate the logic form for a new question. However,\ncurrent powerful LLMs have little exposure to logic forms during pre-training,\nresulting in a high format error rate. To solve this problem, we propose a\ncode-style in-context learning method for KBQA, which converts the generation\nprocess of unfamiliar logical form into the more familiar code generation\nprocess for LLMs. Experimental results on three mainstream datasets show that\nour method dramatically mitigated the formatting error problem in generating\nlogic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the\nfew-shot setting. The code and supplementary files are released at\nhttps://github.com/Arthurizijar/KB-Coder .",
        "pdf_link": "https://arxiv.org/pdf/2309.04695v2.pdf"
    },
    {
        "title": "Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges",
        "authors": [
            "Hiba Ahsan",
            "Denis Jered McInerney",
            "Jisoo Kim",
            "Christopher Potter",
            "Geoffrey Young",
            "Silvio Amir",
            "Byron C. Wallace"
        ],
        "published": "2023-09-08T18:44:47Z",
        "summary": "Unstructured data in Electronic Health Records (EHRs) often contains critical\ninformation -- complementary to imaging -- that could inform radiologists'\ndiagnoses. But the large volume of notes often associated with patients\ntogether with time constraints renders manually identifying relevant evidence\npractically infeasible. In this work we propose and evaluate a zero-shot\nstrategy for using LLMs as a mechanism to efficiently retrieve and summarize\nunstructured evidence in patient EHR relevant to a given query. Our method\nentails tasking an LLM to infer whether a patient has, or is at risk of, a\nparticular condition on the basis of associated notes; if so, we ask the model\nto summarize the supporting evidence. Under expert evaluation, we find that\nthis LLM-based approach provides outputs consistently preferred to a pre-LLM\ninformation retrieval baseline. Manual evaluation is expensive, so we also\npropose and validate a method using an LLM to evaluate (other) LLM outputs for\nthis task, allowing us to scale up evaluation. Our findings indicate the\npromise of LLMs as interfaces to EHR, but also highlight the outstanding\nchallenge posed by \"hallucinations\". In this setting, however, we show that\nmodel confidence in outputs strongly correlates with faithful summaries,\noffering a practical means to limit confabulations.",
        "pdf_link": "https://arxiv.org/pdf/2309.04550v2.pdf"
    },
    {
        "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models",
        "authors": [
            "Yangyi Chen",
            "Karan Sikka",
            "Michael Cogswell",
            "Heng Ji",
            "Ajay Divakaran"
        ],
        "published": "2023-09-08T17:49:44Z",
        "summary": "Vision-language models (VLMs) have recently demonstrated strong efficacy as\nvisual assistants that can parse natural queries about the visual content and\ngenerate human-like outputs. In this work, we explore the ability of these\nmodels to demonstrate human-like reasoning based on the perceived information.\nTo address a crucial concern regarding the extent to which their reasoning\ncapabilities are fully consistent and grounded, we also measure the reasoning\nconsistency of these models. We achieve this by proposing a chain-of-thought\n(CoT) based consistency measure. However, such an evaluation requires a\nbenchmark that encompasses both high-level inference and detailed reasoning\nchains, which is costly. We tackle this challenge by proposing a\nLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously\nensuring the generation of a high-quality dataset. Based on this pipeline and\nthe existing coarse-grained annotated dataset, we build the CURE benchmark to\nmeasure both the zero-shot reasoning performance and consistency of VLMs. We\nevaluate existing state-of-the-art VLMs, and find that even the best-performing\nmodel is unable to demonstrate strong visual reasoning capabilities and\nconsistency, indicating that substantial efforts are required to enable VLMs to\nperform visual reasoning as systematically and consistently as humans. As an\nearly step, we propose a two-stage training framework aimed at improving both\nthe reasoning performance and consistency of VLMs. The first stage involves\nemploying supervised fine-tuning of VLMs using step-by-step reasoning samples\nautomatically generated by LLMs. In the second stage, we further augment the\ntraining process by incorporating feedback provided by LLMs to produce\nreasoning chains that are highly consistent and grounded. We empirically\nhighlight the effectiveness of our framework in both reasoning performance and\nconsistency.",
        "pdf_link": "https://arxiv.org/pdf/2309.04461v2.pdf"
    },
    {
        "title": "LLMCad: Fast and Scalable On-device Large Language Model Inference",
        "authors": [
            "Daliang Xu",
            "Wangsong Yin",
            "Xin Jin",
            "Ying Zhang",
            "Shiyun Wei",
            "Mengwei Xu",
            "Xuanzhe Liu"
        ],
        "published": "2023-09-08T10:44:19Z",
        "summary": "Generative tasks, such as text generation and question answering, hold a\ncrucial position in the realm of mobile applications. Due to their sensitivity\nto privacy concerns, there is a growing demand for their execution directly on\nmobile devices. Currently, the execution of these generative tasks heavily\ndepends on Large Language Models (LLMs). Nevertheless, the limited memory\ncapacity of these devices presents a formidable challenge to the scalability of\nsuch models.\n  In our research, we introduce LLMCad, an innovative on-device inference\nengine specifically designed for efficient generative Natural Language\nProcessing (NLP) tasks. The core idea behind LLMCad revolves around model\ncollaboration: a compact LLM, residing in memory, takes charge of generating\nthe most straightforward tokens, while a high-precision LLM steps in to\nvalidate these tokens and rectify any identified errors. LLMCad incorporates\nthree novel techniques: (1) Instead of generating candidate tokens in a\nsequential manner, LLMCad employs the smaller LLM to construct a token tree,\nencompassing a wider range of plausible token pathways. Subsequently, the\nlarger LLM can efficiently validate all of these pathways simultaneously. (2)\nIt employs a self-adjusting fallback strategy, swiftly initiating the\nverification process whenever the smaller LLM generates an erroneous token. (3)\nTo ensure a continuous flow of token generation, LLMCad speculatively generates\ntokens during the verification process by implementing a compute-IO pipeline.\nThrough an extensive series of experiments, LLMCad showcases an impressive\ntoken generation speed, achieving rates up to 9.3x faster than existing\ninference engines.",
        "pdf_link": "https://arxiv.org/pdf/2309.04255v1.pdf"
    },
    {
        "title": "Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems",
        "authors": [
            "Dongyub Lee",
            "Taesun Whang",
            "Chanhee Lee",
            "Heuiseok Lim"
        ],
        "published": "2023-09-08T09:39:53Z",
        "summary": "Large language models (LLMs) have emerged as versatile tools in various daily\napplications. However, they are fraught with issues that undermine their\nutility and trustworthiness. These include the incorporation of erroneous\nreferences (citation), the generation of hallucinated information\n(correctness), and the inclusion of superfluous or omission of crucial details\n(fluency). To ameliorate these concerns, this study makes several key\ncontributions. First, we build a dataset to train a critic model capable of\nevaluating the citation, correctness, and fluency of responses generated by\nLLMs in QA systems. Second, we propose an automated feedback mechanism that\nleverages the critic model to offer real-time feedback on heterogeneous aspects\nof generated text. Third, we introduce a feedback learning loop that uses this\ncritic model to iteratively improve the performance of the LLM responsible for\nresponse generation. Experimental results demonstrate the efficacy of our\napproach, showing substantial improvements in citation and fluency metrics for\nChatGPT, including a 4% precision increase in citation and an approximately 8%\nenhancement in the MAUVE metric for fluency, while maintaining high levels of\ncorrectness.",
        "pdf_link": "https://arxiv.org/pdf/2309.06384v1.pdf"
    },
    {
        "title": "UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media",
        "authors": [
            "Yan Jiang",
            "Ruihong Qiu",
            "Yi Zhang",
            "Zi Huang"
        ],
        "published": "2023-09-08T08:54:55Z",
        "summary": "As social media becomes increasingly popular, more and more activities\nrelated to public health emerge. Current techniques for public health analysis\ninvolve popular models such as BERT and large language models (LLMs). However,\nthe costs of training in-domain LLMs for public health are especially\nexpensive. Furthermore, such kinds of in-domain datasets from social media are\ngenerally imbalanced. To tackle these challenges, the data imbalance issue can\nbe overcome by data augmentation and balanced training. Moreover, the ability\nof the LLMs can be effectively utilized by prompting the model properly. In\nthis paper, a novel ALEX framework is proposed to improve the performance of\npublic health analysis on social media by adopting an LLMs explanation\nmechanism. Results show that our ALEX model got the best performance among all\nsubmissions in both Task 2 and Task 4 with a high score in Task 1 in Social\nMedia Mining for Health 2023 (SMM4H)[1]. Our code has been released at https://\ngithub.com/YanJiangJerry/ALEX.",
        "pdf_link": "https://arxiv.org/pdf/2309.04213v2.pdf"
    },
    {
        "title": "Don't Ignore Dual Logic Ability of LLMs while Privatizing: A Data-Intensive Analysis in Medical Domain",
        "authors": [
            "Yanrui Du",
            "Sendong Zhao",
            "Muzhen Cai",
            "Ming Ma",
            "Danyang Zhao",
            "Jiawei Cao",
            "Bing Qin"
        ],
        "published": "2023-09-08T08:20:46Z",
        "summary": "Extensive studies have been devoted to privatizing general-domain Large\nLanguage Models (LLMs) as Domain-Specific LLMs via feeding specific-domain\ndata. However, these privatization efforts often ignored a critical aspect:\nDual Logic Ability, which is a core reasoning ability for LLMs. The dual logic\nability of LLMs ensures that they can maintain a consistent stance when\nconfronted with both positive and negative statements about the same fact. Our\nstudy focuses on how the dual logic ability of LLMs is affected during the\nprivatization process in the medical domain. We conduct several experiments to\nanalyze the dual logic ability of LLMs by examining the consistency of the\nstance in responses to paired questions about the same fact. In our\nexperiments, interestingly, we observed a significant decrease in the dual\nlogic ability of existing LLMs after privatization. Besides, our results\nindicate that incorporating general domain dual logic data into LLMs not only\nenhances LLMs' dual logic ability but also further improves their accuracy.\nThese findings underscore the importance of prioritizing LLMs' dual logic\nability during the privatization process. Our study establishes a benchmark for\nfuture research aimed at exploring LLMs' dual logic ability during the\nprivatization process and offers valuable guidance for privatization efforts in\nreal-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.04198v3.pdf"
    },
    {
        "title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese",
        "authors": [
            "Haochun Wang",
            "Sendong Zhao",
            "Zewen Qiang",
            "Zijian Li",
            "Nuwa Xi",
            "Yanrui Du",
            "MuZhen Cai",
            "Haoqiang Guo",
            "Yuhan Chen",
            "Haoming Xu",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023-09-08T07:42:57Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable success in diverse\nnatural language processing (NLP) tasks in general domains. However, LLMs\nsometimes generate responses with the hallucination about medical facts due to\nlimited domain knowledge. Such shortcomings pose potential risks in the\nutilization of LLMs within medical contexts. To address this challenge, we\npropose knowledge-tuning, which leverages structured medical knowledge bases\nfor the LLMs to grasp domain knowledge efficiently and facilitate reliable\nresponse generation. We also release cMedKnowQA, a Chinese medical knowledge\nquestion-answering dataset constructed from medical knowledge bases to assess\nthe medical knowledge proficiency of LLMs. Experimental results show that the\nLLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of\naccuracy in response generation compared with vanilla instruction-tuning and\noffer a new reliable way for the domain adaptation of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.04175v1.pdf"
    },
    {
        "title": "Matching Table Metadata with Business Glossaries Using Large Language Models",
        "authors": [
            "Elita Lobo",
            "Oktie Hassanzadeh",
            "Nhan Pham",
            "Nandana Mihindukulasooriya",
            "Dharmashankar Subramanian",
            "Horst Samulowitz"
        ],
        "published": "2023-09-08T02:23:59Z",
        "summary": "Enterprises often own large collections of structured data in the form of\nlarge databases or an enterprise data lake. Such data collections come with\nlimited metadata and strict access policies that could limit access to the data\ncontents and, therefore, limit the application of classic retrieval and\nanalysis solutions. As a result, there is a need for solutions that can\neffectively utilize the available metadata. In this paper, we study the problem\nof matching table metadata to a business glossary containing data labels and\ndescriptions. The resulting matching enables the use of an available or curated\nbusiness glossary for retrieval and analysis without or before requesting\naccess to the data contents. One solution to this problem is to use\nmanually-defined rules or similarity measures on column names and glossary\ndescriptions (or their vector embeddings) to find the closest match. However,\nsuch approaches need to be tuned through manual labeling and cannot handle many\nbusiness glossaries that contain a combination of simple as well as complex and\nlong descriptions. In this work, we leverage the power of large language models\n(LLMs) to design generic matching methods that do not require manual tuning and\ncan identify complex relations between column names and glossaries. We propose\nmethods that utilize LLMs in two ways: a) by generating additional context for\ncolumn names that can aid with matching b) by using LLMs to directly infer if\nthere is a relation between column names and glossary descriptions. Our\npreliminary experimental results show the effectiveness of our proposed\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2309.11506v1.pdf"
    },
    {
        "title": "Evaluation of large language models for discovery of gene set function",
        "authors": [
            "Mengzhou Hu",
            "Sahar Alkhairy",
            "Ingoo Lee",
            "Rudolf T. Pillich",
            "Dylan Fong",
            "Kevin Smith",
            "Robin Bachelder",
            "Trey Ideker",
            "Dexter Pratt"
        ],
        "published": "2023-09-07T21:10:48Z",
        "summary": "Gene set analysis is a mainstay of functional genomics, but it relies on\ncurated databases of gene functions that are incomplete. Here we evaluate five\nLarge Language Models (LLMs) for their ability to discover the common\nbiological functions represented by a gene set, substantiated by supporting\nrationale, citations and a confidence assessment. Benchmarking against\ncanonical gene sets from the Gene Ontology, GPT-4 confidently recovered the\ncurated name or a more general concept (73% of cases), while benchmarking\nagainst random gene sets correctly yielded zero confidence. Gemini-Pro and\nMixtral-Instruct showed ability in naming but were falsely confident for random\nsets, whereas Llama2-70b had poor performance overall. In gene sets derived\nfrom 'omics data, GPT-4 identified novel functions not reported by classical\nfunctional enrichment (32% of cases), which independent review indicated were\nlargely verifiable and not hallucinations. The ability to rapidly synthesize\ncommon gene functions positions LLMs as valuable 'omics assistants.",
        "pdf_link": "https://arxiv.org/pdf/2309.04019v2.pdf"
    },
    {
        "title": "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
        "authors": [
            "Amrita Bhattacharjee",
            "Tharindu Kumarage",
            "Raha Moraffah",
            "Huan Liu"
        ],
        "published": "2023-09-07T19:51:30Z",
        "summary": "Large language models (LLMs) are increasingly being used for generating text\nin a variety of use cases, including journalistic news articles. Given the\npotential malicious nature in which these LLMs can be used to generate\ndisinformation at scale, it is important to build effective detectors for such\nAI-generated text. Given the surge in development of new LLMs, acquiring\nlabeled training data for supervised detectors is a bottleneck. However, there\nmight be plenty of unlabeled text data available, without information on which\ngenerator it came from. In this work we tackle this data problem, in detecting\nAI-generated news text, and frame the problem as an unsupervised domain\nadaptation task. Here the domains are the different text generators, i.e. LLMs,\nand we assume we have access to only the labeled source data and unlabeled\ntarget data. We develop a Contrastive Domain Adaptation framework, called\nConDA, that blends standard domain adaptation techniques with the\nrepresentation power of contrastive learning to learn domain invariant\nrepresentations that are effective for the final unsupervised detection task.\nOur experiments demonstrate the effectiveness of our framework, resulting in\naverage performance gains of 31.7% from the best performing baselines, and\nwithin 0.8% margin of a fully supervised detector. All our code and data is\navailable at https://github.com/AmritaBh/ConDA-gen-text-detection.",
        "pdf_link": "https://arxiv.org/pdf/2309.03992v2.pdf"
    },
    {
        "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
        "authors": [
            "Yung-Sung Chuang",
            "Yujia Xie",
            "Hongyin Luo",
            "Yoon Kim",
            "James Glass",
            "Pengcheng He"
        ],
        "published": "2023-09-07T17:45:31Z",
        "summary": "Despite their impressive capabilities, large language models (LLMs) are prone\nto hallucinations, i.e., generating content that deviates from facts seen\nduring pretraining. We propose a simple decoding strategy for reducing\nhallucinations with pretrained LLMs that does not require conditioning on\nretrieved external knowledge nor additional fine-tuning. Our approach obtains\nthe next-token distribution by contrasting the differences in logits obtained\nfrom projecting the later layers versus earlier layers to the vocabulary space,\nexploiting the fact that factual knowledge in an LLMs has generally been shown\nto be localized to particular transformer layers. We find that this Decoding by\nContrasting Layers (DoLa) approach is able to better surface factual knowledge\nand reduce the generation of incorrect facts. DoLa consistently improves the\ntruthfulness across multiple choices tasks and open-ended generation tasks, for\nexample improving the performance of LLaMA family models on TruthfulQA by\n12-17% absolute points, demonstrating its potential in making LLMs reliably\ngenerate truthful facts.",
        "pdf_link": "https://arxiv.org/pdf/2309.03883v2.pdf"
    },
    {
        "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
        "authors": [
            "Chujie Zheng",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Minlie Huang"
        ],
        "published": "2023-09-07T17:44:56Z",
        "summary": "Multiple choice questions (MCQs) serve as a common yet important task format\nin the evaluation of large language models (LLMs). This work shows that modern\nLLMs are vulnerable to option position changes in MCQs due to their inherent\n\"selection bias\", namely, they prefer to select specific option IDs as answers\n(like \"Option A\"). Through extensive empirical analyses with 20 LLMs on three\nbenchmarks, we pinpoint that this behavioral bias primarily stems from LLMs'\ntoken bias, where the model a priori assigns more probabilistic mass to\nspecific option ID tokens (e.g., A/B/C/D) when predicting answers from the\noption IDs. To mitigate selection bias, we propose a label-free, inference-time\ndebiasing method, called PriDe, which separates the model's prior bias for\noption IDs from the overall prediction distribution. PriDe first estimates the\nprior by permutating option contents on a small number of test samples, and\nthen applies the estimated prior to debias the remaining samples. We\ndemonstrate that it achieves interpretable and transferable debiasing with high\ncomputational efficiency. We hope this work can draw broader research attention\nto the bias and robustness of modern LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.03882v4.pdf"
    },
    {
        "title": "OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs",
        "authors": [
            "Patrick Haller",
            "Ansar Aynetdinov",
            "Alan Akbik"
        ],
        "published": "2023-09-07T17:41:01Z",
        "summary": "Instruction-tuned Large Language Models (LLMs) have recently showcased\nremarkable ability to generate fitting responses to natural language\ninstructions. However, an open research question concerns the inherent biases\nof trained models and their responses. For instance, if the data used to tune\nan LLM is dominantly written by persons with a specific political bias, we\nmight expect generated answers to share this bias. Current research work seeks\nto de-bias such models, or suppress potentially biased answers. With this\ndemonstration, we take a different view on biases in instruction-tuning: Rather\nthan aiming to suppress them, we aim to make them explicit and transparent. To\nthis end, we present OpinionGPT, a web demo in which users can ask questions\nand select all biases they wish to investigate. The demo will answer this\nquestion using a model fine-tuned on text representing each of the selected\nbiases, allowing side-by-side comparison. To train the underlying model, we\nidentified 11 different biases (political, geographic, gender, age) and derived\nan instruction-tuning corpus in which each answer was written by members of one\nof these demographics. This paper presents OpinionGPT, illustrates how we\ntrained the bias-aware model and showcases the web application (available at\nhttps://opiniongpt.informatik.hu-berlin.de).",
        "pdf_link": "https://arxiv.org/pdf/2309.03876v1.pdf"
    },
    {
        "title": "Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media",
        "authors": [
            "Hongzhi Qi",
            "Qing Zhao",
            "Changwei Song",
            "Wei Zhai",
            "Dan Luo",
            "Shuo Liu",
            "Yi Jing Yu",
            "Fan Wang",
            "Huijing Zou",
            "Bing Xiang Yang",
            "Jianqiang Li",
            "Guanghui Fu"
        ],
        "published": "2023-09-07T08:50:46Z",
        "summary": "In the realm of social media, users frequently convey personal sentiments,\nwith some potentially indicating cognitive distortions or suicidal tendencies.\nTimely recognition of such signs is pivotal for effective interventions. In\nresponse, we introduce two novel annotated datasets from Chinese social media,\nfocused on cognitive distortions and suicidal risk classification. We propose a\ncomprehensive benchmark using both supervised learning and large language\nmodels, especially from the GPT series, to evaluate performance on these\ndatasets. To assess the capabilities of the large language models, we employed\nthree strategies: zero-shot, few-shot, and fine-tuning. Furthermore, we deeply\nexplored and analyzed the performance of these large language models from a\npsychological perspective, shedding light on their strengths and limitations in\nidentifying and understanding complex human emotions. Our evaluations\nunderscore a performance difference between the two approaches, with the models\noften challenged by subtle category distinctions. While GPT-4 consistently\ndelivered strong results, GPT-3.5 showed marked improvement in suicide risk\nclassification after fine-tuning. This research is groundbreaking in its\nevaluation of large language models for Chinese social media tasks,\naccentuating the models' potential in psychological contexts. All datasets and\ncode are made available.",
        "pdf_link": "https://arxiv.org/pdf/2309.03564v2.pdf"
    },
    {
        "title": "Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences",
        "authors": [
            "Sai Koneru",
            "Jian Wu",
            "Sarah Rajtmajer"
        ],
        "published": "2023-09-07T04:15:17Z",
        "summary": "Hypothesis formulation and testing are central to empirical research. A\nstrong hypothesis is a best guess based on existing evidence and informed by a\ncomprehensive view of relevant literature. However, with exponential increase\nin the number of scientific articles published annually, manual aggregation and\nsynthesis of evidence related to a given hypothesis is a challenge. Our work\nexplores the ability of current large language models (LLMs) to discern\nevidence in support or refute of specific hypotheses based on the text of\nscientific abstracts. We share a novel dataset for the task of scientific\nhypothesis evidencing using community-driven annotations of studies in the\nsocial sciences. We compare the performance of LLMs to several state-of-the-art\nbenchmarks and highlight opportunities for future research in this area. The\ndataset is available at\nhttps://github.com/Sai90000/ScientificHypothesisEvidencing.git",
        "pdf_link": "https://arxiv.org/pdf/2309.06578v3.pdf"
    },
    {
        "title": "XGen-7B Technical Report",
        "authors": [
            "Erik Nijkamp",
            "Tian Xie",
            "Hiroaki Hayashi",
            "Bo Pang",
            "Congying Xia",
            "Chen Xing",
            "Jesse Vig",
            "Semih Yavuz",
            "Philippe Laban",
            "Ben Krause",
            "Senthil Purushwalkam",
            "Tong Niu",
            "Wojciech Kry\u015bci\u0144ski",
            "Lidiya Murakhovs'ka",
            "Prafulla Kumar Choubey",
            "Alex Fabbri",
            "Ye Liu",
            "Rui Meng",
            "Lifu Tu",
            "Meghana Bhat",
            "Chien-Sheng Wu",
            "Silvio Savarese",
            "Yingbo Zhou",
            "Shafiq Joty",
            "Caiming Xiong"
        ],
        "published": "2023-09-07T02:20:03Z",
        "summary": "Large Language Models (LLMs) have become ubiquitous across various domains,\ntransforming the way we interact with information and conduct research.\nHowever, most high-performing LLMs remain confined behind proprietary walls,\nhindering scientific progress. Most open-source LLMs, on the other hand, are\nlimited in their ability to support longer sequence lengths, which is a key\nrequirement for many tasks that require inference over an input context. To\naddress this, we have trained XGen, a series of 7B parameter models on up to 8K\nsequence length for up to 1.5T tokens. We have also finetuned the XGen models\non public-domain instructional data, creating their instruction-tuned\ncounterparts (XGen-Inst). We open-source our models for both research\nadvancements and commercial applications. Our evaluation on standard benchmarks\nshows that XGen models achieve comparable or better results when compared with\nstate-of-the-art open-source LLMs. Our targeted evaluation on long sequence\nmodeling tasks shows the benefits of our 8K-sequence models over 2K-sequence\nopen-source LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.03450v1.pdf"
    },
    {
        "title": "Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Xuchao Zhang",
            "Yanchi Liu",
            "Wei Cheng",
            "Haoyu Wang",
            "Zhengzhang Chen",
            "Takao Osaki",
            "Katsushi Matsuda",
            "Haifeng Chen",
            "Liang Zhao"
        ],
        "published": "2023-09-07T01:35:24Z",
        "summary": "Open Information Extraction (OIE) task aims at extracting structured facts\nfrom unstructured text, typically in the form of (subject, relation, object)\ntriples. Despite the potential of large language models (LLMs) like ChatGPT as\na general task solver, they lag behind state-of-the-art (supervised) methods in\nOIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant\ncontext from relevant relations and generate structured output due to the\nrestrictions on fine-tuning the model. Second, LLMs generates responses\nautoregressively based on probability, which makes the predicted relations lack\nconfidence. In this paper, we assess the capabilities of LLMs in improving the\nOIE task. Particularly, we propose various in-context learning strategies to\nenhance LLM's instruction-following ability and a demonstration uncertainty\nquantification module to enhance the confidence of the generated relations. Our\nexperiments on three OIE benchmark datasets show that our approach holds its\nown against established supervised methods, both quantitatively and\nqualitatively.",
        "pdf_link": "https://arxiv.org/pdf/2309.03433v1.pdf"
    },
    {
        "title": "Large Language Models as Optimizers",
        "authors": [
            "Chengrun Yang",
            "Xuezhi Wang",
            "Yifeng Lu",
            "Hanxiao Liu",
            "Quoc V. Le",
            "Denny Zhou",
            "Xinyun Chen"
        ],
        "published": "2023-09-07T00:07:15Z",
        "summary": "Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to prompt optimization where the goal is to\nfind instructions that maximize the task accuracy. With a variety of LLMs, we\ndemonstrate that the best prompts optimized by OPRO outperform human-designed\nprompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at\nhttps://github.com/google-deepmind/opro.",
        "pdf_link": "https://arxiv.org/pdf/2309.03409v2.pdf"
    },
    {
        "title": "Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks",
        "authors": [
            "Priyam Mazumdar",
            "Aiman Soliman",
            "Volodymyr Kindratenko",
            "Luigi Marini",
            "Kenton McHenry"
        ],
        "published": "2023-09-06T21:20:10Z",
        "summary": "The lack of quality labeled data is one of the main bottlenecks for training\nDeep Learning models. As the task increases in complexity, there is a higher\npenalty for overfitting and unstable learning. The typical paradigm employed\ntoday is Self-Supervised learning, where the model attempts to learn from a\nlarge corpus of unstructured and unlabeled data and then transfer that\nknowledge to the required task. Some notable examples of self-supervision in\nother modalities are BERT for Large Language Models, Wav2Vec for Speech\nRecognition, and the Masked AutoEncoder for Vision, which all utilize\nTransformers to solve a masked prediction task. GeoAI is uniquely poised to\ntake advantage of the self-supervised methodology due to the decades of data\ncollected, little of which is precisely and dependably annotated. Our goal is\nto extract building and road segmentations from Digital Elevation Models (DEM)\nthat provide a detailed topography of the earths surface. The proposed\narchitecture is the Masked Autoencoder pre-trained on ImageNet (with the\nlimitation that there is a large domain discrepancy between ImageNet and DEM)\nwith an UperNet Head for decoding segmentations. We tested this model with 450\nand 50 training images only, utilizing roughly 5% and 0.5% of the original data\nrespectively. On the building segmentation task, this model obtains an 82.1%\nIntersection over Union (IoU) with 450 Images and 69.1% IoU with only 50\nimages. On the more challenging road detection task the model obtains an 82.7%\nIoU with 450 images and 73.2% IoU with only 50 images. Any hand-labeled dataset\nmade today about the earths surface will be immediately obsolete due to the\nconstantly changing nature of the landscape. This motivates the clear necessity\nfor data-efficient learners that can be used for a wide variety of downstream\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2309.03367v1.pdf"
    },
    {
        "title": "Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity",
        "authors": [
            "Aliya Amirova",
            "Theodora Fteropoulli",
            "Nafiso Ahmed",
            "Martin R. Cowie",
            "Joel Z. Leibo"
        ],
        "published": "2023-09-06T15:00:44Z",
        "summary": "Today, using Large-scale generative Language Models (LLMs) it is possible to\nsimulate free responses to interview questions like those traditionally\nanalyzed using qualitative research methods. Qualitative methodology\nencompasses a broad family of techniques involving manual analysis of\nopen-ended interviews or conversations conducted freely in natural language.\nHere we consider whether artificial \"silicon participants\" generated by LLMs\nmay be productively studied using qualitative methods aiming to produce\ninsights that could generalize to real human populations. The key concept in\nour analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023)\ncapturing the degree to which LLM-generated outputs mirror human\nsub-populations' beliefs and attitudes. By definition, high algorithmic\nfidelity suggests latent beliefs elicited from LLMs may generalize to real\nhumans, whereas low algorithmic fidelity renders such research invalid. Here we\nused an LLM to generate interviews with silicon participants matching specific\ndemographic characteristics one-for-one with a set of human participants. Using\nframework-based qualitative analysis, we showed the key themes obtained from\nboth human and silicon participants were strikingly similar. However, when we\nanalyzed the structure and tone of the interviews we found even more striking\ndifferences. We also found evidence of the hyper-accuracy distortion described\nby Aher et al. (2023). We conclude that the LLM we tested (GPT-3.5) does not\nhave sufficient algorithmic fidelity to expect research on it to generalize to\nhuman populations. However, the rapid pace of LLM research makes it plausible\nthis could change in the future. Thus we stress the need to establish epistemic\nnorms now around how to assess validity of LLM-based qualitative research,\nespecially concerning the need to ensure representation of heterogeneous lived\nexperiences.",
        "pdf_link": "https://arxiv.org/pdf/2309.06364v3.pdf"
    },
    {
        "title": "Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection",
        "authors": [
            "Yu Chen",
            "Tingxin Li",
            "Huiming Liu",
            "Yang Yu"
        ],
        "published": "2023-09-06T14:54:11Z",
        "summary": "Numerous companies have started offering services based on large language\nmodels (LLM), such as ChatGPT, which inevitably raises privacy concerns as\nusers' prompts are exposed to the model provider. Previous research on secure\nreasoning using multi-party computation (MPC) has proven to be impractical for\nLLM applications due to its time-consuming and communication-intensive nature.\nWhile lightweight anonymization techniques can protect private information in\nprompts through substitution or masking, they fail to recover sensitive data\nreplaced in the LLM-generated results. In this paper, we expand the application\nscenarios of anonymization techniques by training a small local model to\nde-anonymize the LLM's returned results with minimal computational overhead. We\nintroduce the HaS framework, where \"H(ide)\" and \"S(eek)\" represent its two core\nprocesses: hiding private entities for anonymization and seeking private\nentities for de-anonymization, respectively. To quantitatively assess HaS's\nprivacy protection performance, we propose both black-box and white-box\nadversarial models. Furthermore, we conduct experiments to evaluate HaS's\nusability in translation and classification tasks. The experimental findings\ndemonstrate that the HaS framework achieves an optimal balance between privacy\nprotection and utility.",
        "pdf_link": "https://arxiv.org/pdf/2309.03057v1.pdf"
    },
    {
        "title": "Aligning Large Language Models for Clinical Tasks",
        "authors": [
            "Supun Manathunga",
            "Isuru Hettigoda"
        ],
        "published": "2023-09-06T10:20:06Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable adaptability,\nshowcasing their capacity to excel in tasks for which they were not explicitly\ntrained. However, despite their impressive natural language processing (NLP)\ncapabilities, effective alignment of LLMs remains a crucial challenge when\ndeploying them for specific clinical applications. The ability to generate\nresponses with factually accurate content and to engage in non-trivial\nreasoning steps are crucial for the LLMs to be eligible for applications in\nclinical medicine. Employing a combination of techniques including\ninstruction-tuning and in-prompt strategies like few-shot and chain-of-thought\nprompting has significantly enhanced the performance of LLMs. Our proposed\nalignment strategy for medical question-answering, known as\n'expand-guess-refine', offers a parameter and data-efficient solution. A\npreliminary analysis of this method demonstrated outstanding performance,\nachieving a score of 70.63% on a subset of questions sourced from the USMLE\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2309.02884v2.pdf"
    },
    {
        "title": "Norm Tweaking: High-performance Low-bit Quantization of Large Language Models",
        "authors": [
            "Liang Li",
            "Qingyuan Li",
            "Bo Zhang",
            "Xiangxiang Chu"
        ],
        "published": "2023-09-06T06:51:15Z",
        "summary": "As the size of large language models (LLMs) continues to grow, model\ncompression without sacrificing accuracy has become a crucial challenge for\ndeployment. While some quantization methods, such as GPTQ, have made progress\nin achieving acceptable 4-bit weight-only quantization, attempts at lower-bit\nquantization often result in severe performance degradation. In this paper, we\nintroduce a technique called norm tweaking, which can be used as a plugin in\ncurrent PTQ methods to achieve high precision while being cost-efficient. Our\napproach is inspired by the observation that rectifying the quantized\nactivation distribution to match its float counterpart can readily restore\naccuracy for LLMs. To achieve this, we carefully design a tweaking strategy\nthat includes calibration data generation and channel-wise distance constraint\nto update the weights of normalization layers for better generalization. We\nconduct extensive experiments on various datasets using several open-sourced\nLLMs. Our method demonstrates significant improvements in both weight-only\nquantization and joint quantization of weights and activations, surpassing\nexisting PTQ methods. On GLM-130B and OPT-66B, our method even achieves the\nsame level of accuracy at 2-bit quantization as their float ones. Our simple\nand effective approach makes it more practical for real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.02784v2.pdf"
    },
    {
        "title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
        "authors": [
            "Zonglin Yang",
            "Xinya Du",
            "Junxian Li",
            "Jie Zheng",
            "Soujanya Poria",
            "Erik Cambria"
        ],
        "published": "2023-09-06T05:19:41Z",
        "summary": "Hypothetical induction is recognized as the main reasoning type when\nscientists make observations about the world and try to propose hypotheses to\nexplain those observations. Past research on hypothetical induction is under a\nconstrained setting: (1) the observation annotations in the dataset are\ncarefully manually handpicked sentences (resulting in a close-domain setting);\nand (2) the ground truth hypotheses are mostly commonsense knowledge, making\nthe task less challenging. In this work, we tackle these problems by proposing\nthe first NLP dataset for social science academic hypotheses discovery,\nconsisting of 50 recent top social science publications; and a raw web corpus\nthat contains enough information to make it possible to develop all the\nresearch hypotheses in the 50 papers. The final goal is to create systems that\nautomatically generate valid, novel, and helpful scientific hypotheses, given\nonly a pile of raw web corpus. Different from the previous settings, the new\ndataset requires (1) using open-domain data (raw web corpus) as observations;\nand (2) proposing hypotheses even new to humanity. A multi-module framework is\ndeveloped for the task, as well as three different feedback mechanisms that\nempirically show performance gain over the base framework. Finally, our\nframework exhibits superior performance in terms of both GPT-4 based evaluation\nand expert-based evaluation.To the best of our knowledge, this is the first\nwork showing that LLMs are able to generate novel (\"not existing in the\nliterature\") and valid (\"reflecting reality\") scientific hypotheses.",
        "pdf_link": "https://arxiv.org/pdf/2309.02726v2.pdf"
    },
    {
        "title": "Zero-Resource Hallucination Prevention for Large Language Models",
        "authors": [
            "Junyu Luo",
            "Cao Xiao",
            "Fenglong Ma"
        ],
        "published": "2023-09-06T01:57:36Z",
        "summary": "The prevalent use of large language models (LLMs) in various domains has\ndrawn attention to the issue of \"hallucination,\" which refers to instances\nwhere LLMs generate factually inaccurate or ungrounded information. Existing\ntechniques for hallucination detection in language assistants rely on intricate\nfuzzy, specific free-language-based chain of thought (CoT) techniques or\nparameter-based methods that suffer from interpretability issues. Additionally,\nthe methods that identify hallucinations post-generation could not prevent\ntheir occurrence and suffer from inconsistent performance due to the influence\nof the instruction format and model style. In this paper, we introduce a novel\npre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which\nfocuses on evaluating the model's familiarity with the concepts present in the\ninput instruction and withholding the generation of response in case of\nunfamiliar concepts. This approach emulates the human ability to refrain from\nresponding to unfamiliar topics, thus reducing hallucinations. We validate\nSELF-FAMILIARITY across four different large language models, demonstrating\nconsistently superior performance compared to existing techniques. Our findings\npropose a significant shift towards preemptive strategies for hallucination\nmitigation in LLM assistants, promising improvements in reliability,\napplicability, and interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2309.02654v3.pdf"
    },
    {
        "title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
        "authors": [
            "Jensen Gao",
            "Bidipta Sarkar",
            "Fei Xia",
            "Ted Xiao",
            "Jiajun Wu",
            "Brian Ichter",
            "Anirudha Majumdar",
            "Dorsa Sadigh"
        ],
        "published": "2023-09-05T20:21:03Z",
        "summary": "Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.",
        "pdf_link": "https://arxiv.org/pdf/2309.02561v4.pdf"
    },
    {
        "title": "Automating Behavioral Testing in Machine Translation",
        "authors": [
            "Javier Ferrando",
            "Matthias Sperber",
            "Hendra Setiawan",
            "Dominic Telaar",
            "Sa\u0161a Hasan"
        ],
        "published": "2023-09-05T19:40:45Z",
        "summary": "Behavioral testing in NLP allows fine-grained evaluation of systems by\nexamining their linguistic capabilities through the analysis of input-output\nbehavior. Unfortunately, existing work on behavioral testing in Machine\nTranslation (MT) is currently restricted to largely handcrafted tests covering\na limited range of capabilities and languages. To address this limitation, we\npropose to use Large Language Models (LLMs) to generate a diverse set of source\nsentences tailored to test the behavior of MT models in a range of situations.\nWe can then verify whether the MT model exhibits the expected behavior through\nmatching candidate sets that are also generated using LLMs. Our approach aims\nto make behavioral testing of MT systems practical while requiring only minimal\nhuman effort. In our experiments, we apply our proposed evaluation framework to\nassess multiple available MT systems, revealing that while in general\npass-rates follow the trends observable from traditional accuracy-based\nmetrics, our method was able to uncover several important differences and\npotential bugs that go unnoticed when relying only on accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2309.02553v3.pdf"
    },
    {
        "title": "Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices",
        "authors": [
            "Bojia Zi",
            "Xianbiao Qi",
            "Lingzhi Wang",
            "Jianan Wang",
            "Kam-Fai Wong",
            "Lei Zhang"
        ],
        "published": "2023-09-05T17:40:34Z",
        "summary": "In this paper, we present Delta-LoRA, which is a novel parameter-efficient\napproach to fine-tune large language models (LLMs). In contrast to LoRA and\nother low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates\nthe low-rank matrices $\\bA$ and $\\bB$, but also propagate the learning to the\npre-trained weights $\\bW$ via updates utilizing the delta of the product of two\nlow-rank matrices ($\\bA^{(t+1)}\\bB^{(t+1)} - \\bA^{(t)}\\bB^{(t)}$). Such a\nstrategy effectively addresses the limitation that the incremental update of\nlow-rank matrices is inadequate for learning representations capable for\ndownstream tasks. Moreover, as the update of $\\bW$ does not need to compute the\ngradients of $\\bW$ and store their momentums, Delta-LoRA shares comparable\nmemory requirements and computational costs with LoRA. Extensive experiments\nshow that Delta-LoRA significantly outperforms existing low-rank adaptation\nmethods. We further support these results with comprehensive analyses that\nunderscore the effectiveness of Delta-LoRA.",
        "pdf_link": "https://arxiv.org/pdf/2309.02411v1.pdf"
    },
    {
        "title": "PromptTTS 2: Describing and Generating Voices with Text Prompt",
        "authors": [
            "Yichong Leng",
            "Zhifang Guo",
            "Kai Shen",
            "Xu Tan",
            "Zeqian Ju",
            "Yanqing Liu",
            "Yufei Liu",
            "Dongchao Yang",
            "Leying Zhang",
            "Kaitao Song",
            "Lei He",
            "Xiang-Yang Li",
            "Sheng Zhao",
            "Tao Qin",
            "Jiang Bian"
        ],
        "published": "2023-09-05T14:45:27Z",
        "summary": "Speech conveys more information than text, as the same word can be uttered in\nvarious voices to convey diverse information. Compared to traditional\ntext-to-speech (TTS) methods relying on speech prompts (reference speech) for\nvoice variability, using text prompts (descriptions) is more user-friendly\nsince speech prompts can be hard to find or may not exist at all. TTS\napproaches based on the text prompt face two main challenges: 1) the\none-to-many problem, where not all details about voice variability can be\ndescribed in the text prompt, and 2) the limited availability of text prompt\ndatasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address\nthese challenges with a variation network to provide variability information of\nvoice not captured by text prompts, and a prompt generation pipeline to utilize\nthe large language models (LLM) to compose high quality text prompts.\nSpecifically, the variation network predicts the representation extracted from\nthe reference speech (which contains full information about voice variability)\nbased on the text prompt representation. For the prompt generation pipeline, it\ngenerates text prompts for speech with a speech language understanding model to\nrecognize voice attributes (e.g., gender, speed) from speech and a large\nlanguage model to formulate text prompts based on the recognition results.\nExperiments on a large-scale (44K hours) speech dataset demonstrate that\ncompared to the previous works, PromptTTS 2 generates voices more consistent\nwith text prompts and supports the sampling of diverse voice variability,\nthereby offering users more choices on voice generation. Additionally, the\nprompt generation pipeline produces high-quality text prompts, eliminating the\nlarge labeling cost. The demo page of PromptTTS 2 is available online.",
        "pdf_link": "https://arxiv.org/pdf/2309.02285v2.pdf"
    },
    {
        "title": "Sample Size in Natural Language Processing within Healthcare Research",
        "authors": [
            "Jaya Chaturvedi",
            "Diana Shamsutdinova",
            "Felix Zimmer",
            "Sumithra Velupillai",
            "Daniel Stahl",
            "Robert Stewart",
            "Angus Roberts"
        ],
        "published": "2023-09-05T13:42:43Z",
        "summary": "Sample size calculation is an essential step in most data-based disciplines.\nLarge enough samples ensure representativeness of the population and determine\nthe precision of estimates. This is true for most quantitative studies,\nincluding those that employ machine learning methods, such as natural language\nprocessing, where free-text is used to generate predictions and classify\ninstances of text. Within the healthcare domain, the lack of sufficient corpora\nof previously collected data can be a limiting factor when determining sample\nsizes for new studies. This paper tries to address the issue by making\nrecommendations on sample sizes for text classification tasks in the healthcare\ndomain.\n  Models trained on the MIMIC-III database of critical care records from Beth\nIsrael Deaconess Medical Center were used to classify documents as having or\nnot having Unspecified Essential Hypertension, the most common diagnosis code\nin the database. Simulations were performed using various classifiers on\ndifferent sample sizes and class proportions. This was repeated for a\ncomparatively less common diagnosis code within the database of diabetes\nmellitus without mention of complication.\n  Smaller sample sizes resulted in better results when using a K-nearest\nneighbours classifier, whereas larger sample sizes provided better results with\nsupport vector machines and BERT models. Overall, a sample size larger than\n1000 was sufficient to provide decent performance metrics.\n  The simulations conducted within this study provide guidelines that can be\nused as recommendations for selecting appropriate sample sizes and class\nproportions, and for predicting expected performance, when building classifiers\nfor textual healthcare data. The methodology used here can be modified for\nsample size estimates calculations with other datasets.",
        "pdf_link": "https://arxiv.org/pdf/2309.02237v1.pdf"
    },
    {
        "title": "Language Models for Novelty Detection in System Call Traces",
        "authors": [
            "Quentin Fournier",
            "Daniel Aloise",
            "Leandro R. Costa"
        ],
        "published": "2023-09-05T13:11:40Z",
        "summary": "Due to the complexity of modern computer systems, novel and unexpected\nbehaviors frequently occur. Such deviations are either normal occurrences, such\nas software updates and new user activities, or abnormalities, such as\nmisconfigurations, latency issues, intrusions, and software bugs. Regardless,\nnovel behaviors are of great interest to developers, and there is a genuine\nneed for efficient and effective methods to detect them. Nowadays, researchers\nconsider system calls to be the most fine-grained and accurate source of\ninformation to investigate the behavior of computer systems. Accordingly, this\npaper introduces a novelty detection methodology that relies on a probability\ndistribution over sequences of system calls, which can be seen as a language\nmodel. Language models estimate the likelihood of sequences, and since\nnovelties deviate from previously observed behaviors by definition, they would\nbe unlikely under the model. Following the success of neural networks for\nlanguage models, three architectures are evaluated in this work: the widespread\nLSTM, the state-of-the-art Transformer, and the lower-complexity Longformer.\nHowever, large neural networks typically require an enormous amount of data to\nbe trained effectively, and to the best of our knowledge, no massive modern\ndatasets of kernel traces are publicly available. This paper addresses this\nlimitation by introducing a new open-source dataset of kernel traces comprising\nover 2 million web requests with seven distinct behaviors. The proposed\nmethodology requires minimal expert hand-crafting and achieves an F-score and\nAuROC greater than 95% on most novelties while being data- and task-agnostic.\nThe source code and trained models are publicly available on GitHub while the\ndatasets are available on Zenodo.",
        "pdf_link": "https://arxiv.org/pdf/2309.02206v1.pdf"
    },
    {
        "title": "An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models",
        "authors": [
            "Yusheng Liao",
            "Yutong Meng",
            "Hongcheng Liu",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "published": "2023-09-05T09:24:48Z",
        "summary": "Large language models (LLMs) have achieved significant success in interacting\nwith human. However, recent studies have revealed that these models often\nsuffer from hallucinations, leading to overly confident but incorrect\njudgments. This limits their application in the medical domain, where tasks\nrequire the utmost accuracy. This paper introduces an automated evaluation\nframework that assesses the practical capabilities of LLMs as virtual doctors\nduring multi-turn consultations. Consultation tasks are designed to require\nLLMs to be aware of what they do not know, to inquire about missing medical\ninformation from patients, and to ultimately make diagnoses. To evaluate the\nperformance of LLMs for these tasks, a benchmark is proposed by reformulating\nmedical multiple-choice questions from the United States Medical Licensing\nExaminations (USMLE), and comprehensive evaluation metrics are developed and\nevaluated on three constructed test sets. A medical consultation training set\nis further constructed to improve the consultation ability of LLMs. The results\nof the experiments show that fine-tuning with the training set can alleviate\nhallucinations and improve LLMs' performance on the proposed benchmark.\nExtensive experiments and ablation studies are conducted to validate the\neffectiveness and robustness of the proposed framework.",
        "pdf_link": "https://arxiv.org/pdf/2309.02077v1.pdf"
    },
    {
        "title": "Data-Juicer: A One-Stop Data Processing System for Large Language Models",
        "authors": [
            "Daoyuan Chen",
            "Yilun Huang",
            "Zhijian Ma",
            "Hesen Chen",
            "Xuchen Pan",
            "Ce Ge",
            "Dawei Gao",
            "Yuexiang Xie",
            "Zhaoyang Liu",
            "Jinyang Gao",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "published": "2023-09-05T08:22:07Z",
        "summary": "The immense evolution in Large Language Models (LLMs) has underscored the\nimportance of massive, heterogeneous, and high-quality data. A data recipe is a\nmixture of data from different sources for training LLMs, which plays a vital\nrole in LLMs' performance. Existing open-source tools for LLM data processing\nare mostly tailored for specific data recipes. To continuously uncover the\npotential of LLMs, incorporate data from new sources, and improve LLMs'\nperformance, we build a new system named Data-Juicer, with which we can\nefficiently generate diverse data recipes, explore different possibilities in\nforming data mixtures, and evaluate their effects on model performance.\nDifferent from traditional data-analytics pipelines, Data-Juicer faces some\nunique challenges. Firstly, the possible data sources for forming data recipes\nare truly heterogeneous and massive with various qualities. Secondly, it is\nextremely expensive to precisely evaluate data recipes' impact on LLMs'\nperformance. Thirdly, the end users of Data-Juicer, model developers, need\nsufficient flexibility to configure and evaluate different data recipes.\n  Data-Juicer features a fine-grained abstraction of pipelines for constructing\ndata recipes, with over 50 built-in operators for easy composition and\nextension. By incorporating visualization and auto-evaluation capabilities,\nData-Juicer enables a timely feedback loop for both LLM pre-training and\nfine-tuning. Further, Data-Juicer is optimized and integrated with ecosystems\nfor LLM training, evaluation, and distributed computing. The data recipes\nderived with Data-Juicer gain notable improvements on state-of-the-art LLMs, by\nup to 7.45% increase in averaged score across 16 LLM benchmarks and 17.5%\nhigher win rate in pair-wise GPT-4 evaluations. Our system, data recipes, and\ntutorials are released, calling for broader data-centric research on training\nand understanding LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.02033v3.pdf"
    },
    {
        "title": "On the Planning, Search, and Memorization Capabilities of Large Language Models",
        "authors": [
            "Yunhao Yang",
            "Anshul Tomar"
        ],
        "published": "2023-09-05T00:19:31Z",
        "summary": "The rapid advancement of large language models, such as the Generative\nPre-trained Transformer (GPT) series, has had significant implications across\nvarious disciplines. In this study, we investigate the potential of the\nstate-of-the-art large language model (GPT-4) for planning tasks. We explore\nits effectiveness in multiple planning subfields, highlighting both its\nstrengths and limitations. Through a comprehensive examination, we identify\nareas where large language models excel in solving planning problems and reveal\nthe constraints that limit their applicability. Our empirical analysis focuses\non GPT-4's performance in planning domain extraction, graph search path\nplanning, and adversarial planning. We then propose a way of fine-tuning a\ndomain-specific large language model to improve its Chain of Thought (CoT)\ncapabilities for the above-mentioned tasks. The results provide valuable\ninsights into the potential applications of large language models in the\nplanning domain and pave the way for future research to overcome their\nlimitations and expand their capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2309.01868v1.pdf"
    },
    {
        "title": "Softmax Bias Correction for Quantized Generative Models",
        "authors": [
            "Nilesh Prasad Pandey",
            "Marios Fournarakis",
            "Chirag Patel",
            "Markus Nagel"
        ],
        "published": "2023-09-04T17:29:31Z",
        "summary": "Post-training quantization (PTQ) is the go-to compression technique for large\ngenerative models, such as stable diffusion or large language models. PTQ\nmethods commonly keep the softmax activation in higher precision as it has been\nshown to be very sensitive to quantization noise. However, this can lead to a\nsignificant runtime and power overhead during inference on resource-constraint\nedge devices. In this work, we investigate the source of the softmax\nsensitivity to quantization and show that the quantization operation leads to a\nlarge bias in the softmax output, causing accuracy degradation. To overcome\nthis issue, we propose an offline bias correction technique that improves the\nquantizability of softmax without additional compute during deployment, as it\ncan be readily absorbed into the quantization parameters. We demonstrate the\neffectiveness of our method on stable diffusion v1.5 and 125M-size OPT language\nmodel, achieving significant accuracy improvement for 8-bit quantized softmax.",
        "pdf_link": "https://arxiv.org/pdf/2309.01729v1.pdf"
    },
    {
        "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
        "authors": [
            "Raz Lapid",
            "Ron Langberg",
            "Moshe Sipper"
        ],
        "published": "2023-09-04T08:54:20Z",
        "summary": "Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.",
        "pdf_link": "https://arxiv.org/pdf/2309.01446v3.pdf"
    },
    {
        "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
        "authors": [
            "Jiawei Chen",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023-09-04T08:28:44Z",
        "summary": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating\nthe hallucination of large language models (LLMs). However, existing research\nlacks rigorous evaluation of the impact of retrieval-augmented generation on\ndifferent large language models, which make it challenging to identify the\npotential bottlenecks in the capabilities of RAG for different LLMs. In this\npaper, we systematically investigate the impact of Retrieval-Augmented\nGeneration on large language models. We analyze the performance of different\nlarge language models in 4 fundamental abilities required for RAG, including\nnoise robustness, negative rejection, information integration, and\ncounterfactual robustness. To this end, we establish Retrieval-Augmented\nGeneration Benchmark (RGB), a new corpus for RAG evaluation in both English and\nChinese. RGB divides the instances within the benchmark into 4 separate\ntestbeds based on the aforementioned fundamental abilities required to resolve\nthe case. Then we evaluate 6 representative LLMs on RGB to diagnose the\nchallenges of current LLMs when applying RAG. Evaluation reveals that while\nLLMs exhibit a certain degree of noise robustness, they still struggle\nsignificantly in terms of negative rejection, information integration, and\ndealing with false information. The aforementioned assessment outcomes indicate\nthat there is still a considerable journey ahead to effectively apply RAG to\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.01431v2.pdf"
    },
    {
        "title": "Representations Matter: Embedding Modes of Large Language Models using Dynamic Mode Decomposition",
        "authors": [
            "Mohamed Akrout"
        ],
        "published": "2023-09-03T19:10:18Z",
        "summary": "Existing large language models (LLMs) are known for generating \"hallucinated\"\ncontent, namely a fabricated text of plausibly looking, yet unfounded, facts.\nTo identify when these hallucination scenarios occur, we examine the properties\nof the generated text in the embedding space. Specifically, we draw inspiration\nfrom the dynamic mode decomposition (DMD) tool in analyzing the pattern\nevolution of text embeddings across sentences. We empirically demonstrate how\nthe spectrum of sentence embeddings over paragraphs is constantly low-rank for\nthe generated text, unlike that of the ground-truth text. Importantly, we find\nthat evaluation cases having LLM hallucinations correspond to ground-truth\nembedding patterns with a higher number of modes being poorly approximated by\nthe few modes associated with LLM embedding patterns. In analogy to near-field\nelectromagnetic evanescent waves, the embedding DMD eigenmodes of the generated\ntext with hallucinations vanishes quickly across sentences as opposed to those\nof the ground-truth text. This suggests that the hallucinations result from\nboth the generation techniques and the underlying representation.",
        "pdf_link": "https://arxiv.org/pdf/2309.01245v1.pdf"
    },
    {
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
        "authors": [
            "Yue Zhang",
            "Yafu Li",
            "Leyang Cui",
            "Deng Cai",
            "Lemao Liu",
            "Tingchen Fu",
            "Xinting Huang",
            "Enbo Zhao",
            "Yu Zhang",
            "Yulong Chen",
            "Longyue Wang",
            "Anh Tuan Luu",
            "Wei Bi",
            "Freda Shi",
            "Shuming Shi"
        ],
        "published": "2023-09-03T16:56:48Z",
        "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.",
        "pdf_link": "https://arxiv.org/pdf/2309.01219v2.pdf"
    },
    {
        "title": "FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs",
        "authors": [
            "Zhenheng Tang",
            "Yuxin Wang",
            "Xin He",
            "Longteng Zhang",
            "Xinglin Pan",
            "Qiang Wang",
            "Rongfei Zeng",
            "Kaiyong Zhao",
            "Shaohuai Shi",
            "Bingsheng He",
            "Xiaowen Chu"
        ],
        "published": "2023-09-03T13:27:56Z",
        "summary": "The rapid growth of memory and computation requirements of large language\nmodels (LLMs) has outpaced the development of hardware, hindering people who\nlack large-scale high-end GPUs from training or deploying LLMs. However,\nconsumer-level GPUs, which constitute a larger market share, are typically\noverlooked in LLM due to their weaker computing performance, smaller storage\ncapacity, and lower communication bandwidth. Additionally, users may have\nprivacy concerns when interacting with remote LLMs. In this paper, we envision\na decentralized system unlocking the potential vast untapped consumer-level\nGPUs in pre-training, inference and fine-tuning of LLMs with privacy\nprotection. However, this system faces critical challenges, including limited\nCPU and GPU memory, low network bandwidth, the variability of peer and device\nheterogeneity. To address these challenges, our system design incorporates: 1)\na broker with backup pool to implement dynamic join and quit of computing\nproviders; 2) task scheduling with hardware performance to improve system\nefficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) to\nachieve model and task universality; 4) abstracting intermediate represention\nand execution planes to ensure compatibility of various devices and deep\nlearning (DL) frameworks. Our performance analysis demonstrates that 50 RTX\n3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which are\nsignificantly more expensive.",
        "pdf_link": "https://arxiv.org/pdf/2309.01172v1.pdf"
    },
    {
        "title": "Bias Testing and Mitigation in LLM-based Code Generation",
        "authors": [
            "Dong Huang",
            "Qingwen Bu",
            "Jie Zhang",
            "Xiaofei Xie",
            "Junjie Chen",
            "Heming Cui"
        ],
        "published": "2023-09-03T07:14:49Z",
        "summary": "Utilizing state-of-the-art Large Language Models (LLMs), automatic code\ngeneration models play a pivotal role in enhancing the productivity of software\ndevelopment procedures. As the adoption of LLMs becomes more widespread in\nsoftware coding ecosystems, a pressing issue has emerged: does the generated\ncode contain social bias and unfairness, such as those related to age, gender,\nand race? This issue concerns the integrity, fairness, and ethical foundation\nof software applications that depend on the code generated by these models, yet\nis under-explored in the literature. This paper presents a novel bias testing\nframework that is specifically designed for code generation tasks. Based on\nthis framework, we conduct an extensive evaluation of the bias in code\ngenerated by five state-of-the-art LLMs. Our findings reveal that 20.29% to\n44.93% code functions generated by the models under study are biased when\nhandling bias sensitive tasks (i.e., tasks that involve sensitive attributes\nsuch as age and gender). This indicates that the existing LLMs can be unfair in\ncode generation, posing risks of unintended and harmful software behaviors. To\nmitigate bias for code generation models, we evaluate five bias mitigation\nprompt strategies, i.e., utilizing bias testing results to refine the code\n(zero-shot), one-, few-shot, and two Chain-of-Thought (CoT) prompts. Our\nevaluation results illustrate that these strategies are all effective in\nmitigating bias. Overall, one-shot and few-shot learning are the two most\neffective. For GPT-4, 80% to 90% code bias can be removed with one-shot\nlearning.",
        "pdf_link": "https://arxiv.org/pdf/2309.14345v2.pdf"
    },
    {
        "title": "Explainability for Large Language Models: A Survey",
        "authors": [
            "Haiyan Zhao",
            "Hanjie Chen",
            "Fan Yang",
            "Ninghao Liu",
            "Huiqi Deng",
            "Hengyi Cai",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Mengnan Du"
        ],
        "published": "2023-09-02T22:14:26Z",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language processing. However, their internal mechanisms are still\nunclear and this lack of transparency poses unwanted risks for downstream\napplications. Therefore, understanding and explaining these models is crucial\nfor elucidating their behaviors, limitations, and social impacts. In this\npaper, we introduce a taxonomy of explainability techniques and provide a\nstructured overview of methods for explaining Transformer-based language\nmodels. We categorize techniques based on the training paradigms of LLMs:\ntraditional fine-tuning-based paradigm and prompting-based paradigm. For each\nparadigm, we summarize the goals and dominant approaches for generating local\nexplanations of individual predictions and global explanations of overall model\nknowledge. We also discuss metrics for evaluating generated explanations, and\ndiscuss how explanations can be leveraged to debug models and improve\nperformance. Lastly, we examine key challenges and emerging opportunities for\nexplanation techniques in the era of LLMs in comparison to conventional machine\nlearning models.",
        "pdf_link": "https://arxiv.org/pdf/2309.01029v3.pdf"
    },
    {
        "title": "eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models",
        "authors": [
            "Minsik Cho",
            "Keivan A. Vahid",
            "Qichen Fu",
            "Saurabh Adya",
            "Carlo C Del Mundo",
            "Mohammad Rastegari",
            "Devang Naik",
            "Peter Zatloukal"
        ],
        "published": "2023-09-02T15:16:35Z",
        "summary": "Since Large Language Models or LLMs have demonstrated high-quality\nperformance on many complex language tasks, there is a great interest in\nbringing these LLMs to mobile devices for faster responses and better privacy\nprotection. However, the size of LLMs (i.e., billions of parameters) requires\nhighly effective compression to fit into storage-limited devices. Among many\ncompression techniques, weight-clustering, a form of non-linear quantization,\nis one of the leading candidates for LLM compression, and supported by modern\nsmartphones. Yet, its training overhead is prohibitively significant for LLM\nfine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown\nthe state-of-the-art trade-off between compression ratio and accuracy\nregression, but its large memory complexity makes it nearly impossible to apply\nto train-time LLM compression. In this paper, we propose a memory-efficient DKM\nimplementation, eDKM powered by novel techniques to reduce the memory footprint\nof DKM by orders of magnitudes. For a given tensor to be saved on CPU for the\nbackward pass of DKM, we compressed the tensor by applying uniquification and\nsharding after checking if there is no duplicated tensor previously copied to\nCPU. Our experimental results demonstrate that \\prjname can fine-tune and\ncompress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) with\nthe Alpaca dataset by reducing the train-time memory footprint of a decoder\nlayer by 130$\\times$, while delivering good accuracy on broader LLM benchmarks\n(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).",
        "pdf_link": "https://arxiv.org/pdf/2309.00964v2.pdf"
    },
    {
        "title": "Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports",
        "authors": [
            "Tom van Sonsbeek",
            "Xiantong Zhen",
            "Marcel Worring"
        ],
        "published": "2023-09-02T11:46:41Z",
        "summary": "The way we analyse clinical texts has undergone major changes over the last\nyears. The introduction of language models such as BERT led to adaptations for\nthe (bio)medical domain like PubMedBERT and ClinicalBERT. These models rely on\nlarge databases of archived medical documents. While performing well in terms\nof accuracy, both the lack of interpretability and limitations to transfer\nacross languages limit their use in clinical setting. We introduce a novel\nlight-weight graph-based embedding method specifically catering radiology\nreports. It takes into account the structure and composition of the report,\nwhile also connecting medical terms in the report through the multi-lingual\nSNOMED Clinical Terms knowledge base. The resulting graph embedding uncovers\nthe underlying relationships among clinical terms, achieving a representation\nthat is better understandable for clinicians and clinically more accurate,\nwithout reliance on large pre-training datasets. We show the use of this\nembedding on two tasks namely disease classification of X-ray reports and image\nclassification. For disease classification our model is competitive with its\nBERT-based counterparts, while being magnitudes smaller in size and training\ndata requirements. For image classification, we show the effectiveness of the\ngraph embedding leveraging cross-modal knowledge transfer and show how this\nmethod is usable across different languages.",
        "pdf_link": "https://arxiv.org/pdf/2309.00917v2.pdf"
    },
    {
        "title": "Large Process Models: Business Process Management in the Age of Generative AI",
        "authors": [
            "Timotheus Kampik",
            "Christian Warmuth",
            "Adrian Rebmann",
            "Ron Agam",
            "Lukas N. P. Egger",
            "Andreas Gerber",
            "Johannes Hoffart",
            "Jonas Kolk",
            "Philipp Herzig",
            "Gero Decker",
            "Han van der Aa",
            "Artem Polyvyanyy",
            "Stefanie Rinderle-Ma",
            "Ingo Weber",
            "Matthias Weidlich"
        ],
        "published": "2023-09-02T10:32:53Z",
        "summary": "The continued success of Large Language Models (LLMs) and other generative\nartificial intelligence approaches highlights the advantages that large\ninformation corpora can have over rigidly defined symbolic models, but also\nserves as a proof-point of the challenges that purely statistics-based\napproaches have in terms of safety and trustworthiness. As a framework for\ncontextualizing the potential, as well as the limitations of LLMs and other\nfoundation model-based technologies, we propose the concept of a Large Process\nModel (LPM) that combines the correlation power of LLMs with the analytical\nprecision and reliability of knowledge-based systems and automated reasoning\napproaches. LPMs are envisioned to directly utilize the wealth of process\nmanagement experience that experts have accumulated, as well as process\nperformance data of organizations with diverse characteristics, e.g., regarding\nsize, region, or industry. In this vision, the proposed LPM would allow\norganizations to receive context-specific (tailored) process and other business\nmodels, analytical deep-dives, and improvement recommendations. As such, they\nwould allow to substantially decrease the time and effort required for business\ntransformation, while also allowing for deeper, more impactful, and more\nactionable insights than previously possible. We argue that implementing an LPM\nis feasible, but also highlight limitations and research challenges that need\nto be solved to implement particular aspects of the LPM vision.",
        "pdf_link": "https://arxiv.org/pdf/2309.00900v2.pdf"
    },
    {
        "title": "LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs",
        "authors": [
            "Md Adnan Arefeen",
            "Biplob Debnath",
            "Srimat Chakradhar"
        ],
        "published": "2023-09-02T06:33:18Z",
        "summary": "Question-answering (QA) is a significant application of Large Language Models\n(LLMs), shaping chatbot capabilities across healthcare, education, and customer\nservice. However, widespread LLM integration presents a challenge for small\nbusinesses due to the high expenses of LLM API usage. Costs rise rapidly when\ndomain-specific data (context) is used alongside queries for accurate\ndomain-specific LLM responses. One option is to summarize the context by using\nLLMs and reduce the context. However, this can also filter out useful\ninformation that is necessary to answer some domain-specific queries. In this\npaper, we shift from human-oriented summarizers to AI model-friendly summaries.\nOur approach, LeanContext, efficiently extracts $k$ key sentences from the\ncontext that are closely aligned with the query. The choice of $k$ is neither\nstatic nor random; we introduce a reinforcement learning technique that\ndynamically determines $k$ based on the query and context. The rest of the less\nimportant sentences are reduced using a free open source text reduction method.\nWe evaluate LeanContext against several recent query-aware and query-unaware\ncontext reduction approaches on prominent datasets (arxiv papers and BBC news\narticles). Despite cost reductions of $37.29\\%$ to $67.81\\%$, LeanContext's\nROUGE-1 score decreases only by $1.41\\%$ to $2.65\\%$ compared to a baseline\nthat retains the entire context (no summarization). Additionally, if free\npretrained LLM-based summarizers are used to reduce context (into human\nconsumable summaries), LeanContext can further modify the reduced context to\nenhance the accuracy (ROUGE-1 score) by $13.22\\%$ to $24.61\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2309.00841v1.pdf"
    },
    {
        "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
        "authors": [
            "Neel Jain",
            "Avi Schwarzschild",
            "Yuxin Wen",
            "Gowthami Somepalli",
            "John Kirchenbauer",
            "Ping-yeh Chiang",
            "Micah Goldblum",
            "Aniruddha Saha",
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "published": "2023-09-01T17:59:44Z",
        "summary": "As Large Language Models quickly become ubiquitous, it becomes critical to\nunderstand their security vulnerabilities. Recent work shows that text\noptimizers can produce jailbreaking prompts that bypass moderation and\nalignment. Drawing from the rich body of work on adversarial machine learning,\nwe approach these attacks with three questions: What threat models are\npractically useful in this domain? How do baseline defense techniques perform\nin this new domain? How does LLM security differ from computer vision?\n  We evaluate several baseline defense strategies against leading adversarial\nattacks on LLMs, discussing the various settings in which each is feasible and\neffective. Particularly, we look at three types of defenses: detection\n(perplexity based), input preprocessing (paraphrase and retokenization), and\nadversarial training. We discuss white-box and gray-box settings and discuss\nthe robustness-performance trade-off for each of the defenses considered. We\nfind that the weakness of existing discrete optimizers for text, combined with\nthe relatively high costs of optimization, makes standard adaptive attacks more\nchallenging for LLMs. Future research will be needed to uncover whether more\npowerful optimizers can be developed, or whether the strength of filtering and\npreprocessing defenses is greater in the LLMs domain than it has been in\ncomputer vision.",
        "pdf_link": "https://arxiv.org/pdf/2309.00614v2.pdf"
    },
    {
        "title": "Taken out of context: On measuring situational awareness in LLMs",
        "authors": [
            "Lukas Berglund",
            "Asa Cooper Stickland",
            "Mikita Balesni",
            "Max Kaufmann",
            "Meg Tong",
            "Tomasz Korbak",
            "Daniel Kokotajlo",
            "Owain Evans"
        ],
        "published": "2023-09-01T17:27:37Z",
        "summary": "We aim to better understand the emergence of `situational awareness' in large\nlanguage models (LLMs). A model is situationally aware if it's aware that it's\na model and can recognize whether it's currently in testing or deployment.\nToday's LLMs are tested for safety and alignment before they are deployed. An\nLLM could exploit situational awareness to achieve a high score on safety\ntests, while taking harmful actions after deployment. Situational awareness may\nemerge unexpectedly as a byproduct of model scaling. One way to better foresee\nthis emergence is to run scaling experiments on abilities necessary for\nsituational awareness. As such an ability, we propose `out-of-context\nreasoning' (in contrast to in-context learning). We study out-of-context\nreasoning experimentally. First, we finetune an LLM on a description of a test\nwhile providing no examples or demonstrations. At test time, we assess whether\nthe model can pass the test. To our surprise, we find that LLMs succeed on this\nout-of-context reasoning task. Their success is sensitive to the training setup\nand only works when we apply data augmentation. For both GPT-3 and LLaMA-1,\nperformance improves with model size. These findings offer a foundation for\nfurther empirical study, towards predicting and potentially controlling the\nemergence of situational awareness in LLMs. Code is available at:\nhttps://github.com/AsaCooperStickland/situational-awareness-evals.",
        "pdf_link": "https://arxiv.org/pdf/2309.00667v1.pdf"
    },
    {
        "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
        "authors": [
            "Haotian Xu"
        ],
        "published": "2023-09-01T13:10:54Z",
        "summary": "Large language models (LLMs) demonstrate impressive language understanding\nand contextual learning abilities, making them suitable for natural language\nprocessing (NLP) tasks and complex mathematical reasoning. However, when\napplied to mathematical reasoning tasks, LLMs often struggle to generate\ncorrect reasoning steps and answers despite having high probabilities for the\nsolutions. To overcome this limitation and enhance the mathematical reasoning\ncapabilities of fine-tuned LLMs without additional fine-tuning steps, we\npropose a method that incorporates Monte Carlo Tree Search (MCTS) and a\nlightweight energy function to rank decision steps and enable immediate\nreaction and precise reasoning. Specifically, we re-formulate the fine-tuned\nLLMs into a Residual-based Energy Model (Residual-EBM) and employ noise\ncontrastive estimation to estimate the energy function's parameters. We then\nutilize MCTS with the energy function as a path verifier to search the output\nspace and evaluate the reasoning path. Through extensive experiments on two\nmathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the\nexceptional capabilities of our method, which significantly improves the pass@1\nmetric of the fine-tuned model without requiring additional fine-tuning or\nreinforcement learning with human feedback alignment.",
        "pdf_link": "https://arxiv.org/pdf/2309.03224v3.pdf"
    },
    {
        "title": "BatchPrompt: Accomplish more with less",
        "authors": [
            "Jianzhe Lin",
            "Maurice Diesendruck",
            "Liang Du",
            "Robin Abraham"
        ],
        "published": "2023-09-01T10:44:36Z",
        "summary": "As the ever-increasing token limits of large language models (LLMs) have\nenabled long context as input, prompting with single data samples might no\nlonger an efficient way. A straightforward strategy improving efficiency is to\nbatch data within the token limit (e.g., 8k for gpt-3.5-turbo; 32k for GPT-4),\nwhich we call BatchPrompt. We have two initial observations for prompting with\nbatched data. First, we find that prompting with batched data in longer\ncontexts will inevitably lead to worse performance, compared to single-data\nprompting. Second, the performance of the language model is significantly\ncorrelated with the positions and order of the batched data, due to the\ncorresponding change in decoder context. To retain efficiency and overcome\nperformance loss, we propose Batch Permutation and Ensembling (BPE), and a\nnovel Self-reflection-guided EArly Stopping (SEAS) technique. Our comprehensive\nexperimental evaluation demonstrates that BPE can boost the performance of\nBatchPrompt with a striking margin on a range of popular NLP tasks, including\nquestion answering (Boolq), textual entailment (RTE), and duplicate questions\nidentification (QQP). These performances are even competitive with/higher than\nsingle-data prompting(SinglePrompt), while BatchPrompt requires much fewer LLM\ncalls and input tokens (For SinglePrompt v.s. BatchPrompt with batch size 32,\nusing just 9%-16% the number of LLM calls, Boolq accuracy 90.6% to 90.9% with\n27.4% tokens, QQP accuracy 87.2% to 88.4% with 18.6% tokens, RTE accuracy 91.5%\nto 91.1% with 30.8% tokens). To the best of our knowledge, this is the first\nwork to technically improve prompting efficiency of large language models. We\nhope our simple yet effective approach will shed light on the future research\nof large language models. The code will be released.",
        "pdf_link": "https://arxiv.org/pdf/2309.00384v2.pdf"
    },
    {
        "title": "Why do universal adversarial attacks work on large language models?: Geometry might be the answer",
        "authors": [
            "Varshini Subhash",
            "Anna Bialas",
            "Weiwei Pan",
            "Finale Doshi-Velez"
        ],
        "published": "2023-09-01T05:09:49Z",
        "summary": "Transformer based large language models with emergent capabilities are\nbecoming increasingly ubiquitous in society. However, the task of understanding\nand interpreting their internal workings, in the context of adversarial\nattacks, remains largely unsolved. Gradient-based universal adversarial attacks\nhave been shown to be highly effective on large language models and potentially\ndangerous due to their input-agnostic nature. This work presents a novel\ngeometric perspective explaining universal adversarial attacks on large\nlanguage models. By attacking the 117M parameter GPT-2 model, we find evidence\nindicating that universal adversarial triggers could be embedding vectors which\nmerely approximate the semantic information in their adversarial training\nregion. This hypothesis is supported by white-box model analysis comprising\ndimensionality reduction and similarity measurement of hidden representations.\nWe believe this new geometric perspective on the underlying mechanism driving\nuniversal attacks could help us gain deeper insight into the internal workings\nand failure modes of LLMs, thus enabling their mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2309.00254v1.pdf"
    },
    {
        "title": "FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking",
        "authors": [
            "Tsun-Hin Cheung",
            "Kin-Man Lam"
        ],
        "published": "2023-09-01T04:14:39Z",
        "summary": "Automatic fact-checking plays a crucial role in combating the spread of\nmisinformation. Large Language Models (LLMs) and Instruction-Following\nvariants, such as InstructGPT and Alpaca, have shown remarkable performance in\nvarious natural language processing tasks. However, their knowledge may not\nalways be up-to-date or sufficient, potentially leading to inaccuracies in\nfact-checking. To address this limitation, we propose combining the power of\ninstruction-following language models with external evidence retrieval to\nenhance fact-checking performance. Our approach involves leveraging search\nengines to retrieve relevant evidence for a given input claim. This external\nevidence serves as valuable supplementary information to augment the knowledge\nof the pretrained language model. Then, we instruct-tune an open-sourced\nlanguage model, called LLaMA, using this evidence, enabling it to predict the\nveracity of the input claim more accurately. To evaluate our method, we\nconducted experiments on two widely used fact-checking datasets: RAWFC and\nLIAR. The results demonstrate that our approach achieves state-of-the-art\nperformance in fact-checking tasks. By integrating external evidence, we bridge\nthe gap between the model's knowledge and the most up-to-date and sufficient\ncontext available, leading to improved fact-checking outcomes. Our findings\nhave implications for combating misinformation and promoting the dissemination\nof accurate information on online platforms. Our released materials are\naccessible at: https://thcheung.github.io/factllama.",
        "pdf_link": "https://arxiv.org/pdf/2309.00240v1.pdf"
    },
    {
        "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
        "authors": [
            "Zhichao Huang",
            "Chutong Meng",
            "Tom Ko"
        ],
        "published": "2023-08-31T23:26:10Z",
        "summary": "With recent rapid growth of large language models (LLMs), discrete speech\ntokenization has played an important role for injecting speech into LLMs.\nHowever, this discretization gives rise to a loss of information, consequently\nimpairing overall performance. To improve the performance of these discrete\nspeech tokens, we present RepCodec, a novel speech representation codec for\nsemantic speech tokenization. In contrast to audio codecs which reconstruct the\nraw audio, RepCodec learns a vector quantization codebook through\nreconstructing speech representations from speech encoders like HuBERT or\ndata2vec. Together, the speech encoder, the codec encoder and the vector\nquantization codebook form a pipeline for converting speech waveforms into\nsemantic tokens. The extensive experiments illustrate that RepCodec, by virtue\nof its enhanced information retention capacity, significantly outperforms the\nwidely used k-means clustering approach in both speech understanding and\ngeneration. Furthermore, this superiority extends across various speech\nencoders and languages, affirming the robustness of RepCodec. We believe our\nmethod can facilitate large language modeling research on speech processing.",
        "pdf_link": "https://arxiv.org/pdf/2309.00169v1.pdf"
    },
    {
        "title": "LLM in the Shell: Generative Honeypots",
        "authors": [
            "Muris Sladi\u0107",
            "Veronica Valeros",
            "Carlos Catania",
            "Sebastian Garcia"
        ],
        "published": "2023-08-31T22:05:46Z",
        "summary": "Honeypots are essential tools in cybersecurity. However, most of them (even\nthe high-interaction ones) lack the required realism to engage and fool human\nattackers. This limitation makes them easily discernible, hindering their\neffectiveness. This work introduces a novel method to create dynamic and\nrealistic software honeypots based on Large Language Models. Preliminary\nresults indicate that LLMs can create credible and dynamic honeypots capable of\naddressing important limitations of previous honeypots, such as deterministic\nresponses, lack of adaptability, etc. We evaluated the realism of each command\nby conducting an experiment with human attackers who needed to say if the\nanswer from the honeypot was fake or not. Our proposed honeypot, called shelLM,\nreached an accuracy of 0.92. The source code and prompts necessary for\nreplicating the experiments have been made publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2309.00155v2.pdf"
    },
    {
        "title": "Towards Multilingual Automatic Dialogue Evaluation",
        "authors": [
            "John Mendon\u00e7a",
            "Alon Lavie",
            "Isabel Trancoso"
        ],
        "published": "2023-08-31T15:15:26Z",
        "summary": "The main limiting factor in the development of robust multilingual dialogue\nevaluation metrics is the lack of multilingual data and the limited\navailability of open sourced multilingual dialogue systems. In this work, we\npropose a workaround for this lack of data by leveraging a strong multilingual\npretrained LLM and augmenting existing English dialogue data using Machine\nTranslation. We empirically show that the naive approach of finetuning a\npretrained multilingual encoder model with translated data is insufficient to\noutperform the strong baseline of finetuning a multilingual model with only\nsource data. Instead, the best approach consists in the careful curation of\ntranslated data using MT Quality Estimation metrics, excluding low quality\ntranslations that hinder its performance.",
        "pdf_link": "https://arxiv.org/pdf/2308.16795v1.pdf"
    },
    {
        "title": "Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection",
        "authors": [
            "Kairui Hu",
            "Ming Yan",
            "Joey Tianyi Zhou",
            "Ivor W. Tsang",
            "Wen Haw Chong",
            "Yong Keong Yap"
        ],
        "published": "2023-08-31T14:31:48Z",
        "summary": "Stance detection aims to identify the attitude expressed in a document\ntowards a given target. Techniques such as Chain-of-Thought (CoT) prompting\nhave advanced this task, enhancing a model's reasoning capabilities through the\nderivation of intermediate rationales. However, CoT relies primarily on a\nmodel's pre-trained internal knowledge during reasoning, thereby neglecting the\nvaluable external information that is previously unknown to the model. This\nomission, especially within the unsupervised reasoning process, can affect the\nmodel's overall performance. Moreover, while CoT enhances Large Language Models\n(LLMs), smaller LMs, though efficient operationally, face challenges in\ndelivering nuanced reasoning. In response to these identified gaps, we\nintroduce the Ladder-of-Thought (LoT) for the stance detection task.\nConstructed through a dual-phase Progressive Optimization Framework, LoT\ndirects the small LMs to assimilate high-quality external knowledge, refining\nthe intermediate rationales produced. These bolstered rationales subsequently\nserve as the foundation for more precise predictions - akin to how a ladder\nfacilitates reaching elevated goals. LoT achieves a balance between efficiency\nand performance. Our empirical evaluations underscore LoT's efficacy, marking a\n16% improvement over GPT-3.5 and a 10% enhancement compared to GPT-3.5 with CoT\non stance detection task.",
        "pdf_link": "https://arxiv.org/pdf/2308.16763v2.pdf"
    },
    {
        "title": "Context Aware Query Rewriting for Text Rankers using LLM",
        "authors": [
            "Abhijit Anand",
            "Venktesh V",
            "Vinay Setty",
            "Avishek Anand"
        ],
        "published": "2023-08-31T14:19:50Z",
        "summary": "Query rewriting refers to an established family of approaches that are\napplied to underspecified and ambiguous queries to overcome the vocabulary\nmismatch problem in document ranking. Queries are typically rewritten during\nquery processing time for better query modelling for the downstream ranker.\nWith the advent of large-language models (LLMs), there have been initial\ninvestigations into using generative approaches to generate pseudo documents to\ntackle this inherent vocabulary gap. In this work, we analyze the utility of\nLLMs for improved query rewriting for text ranking tasks. We find that there\nare two inherent limitations of using LLMs as query re-writers -- concept drift\nwhen using only queries as prompts and large inference costs during query\nprocessing. We adopt a simple, yet surprisingly effective, approach called\ncontext aware query rewriting (CAR) to leverage the benefits of LLMs for query\nunderstanding. Firstly, we rewrite ambiguous training queries by context-aware\nprompting of LLMs, where we use only relevant documents as context.Unlike\nexisting approaches, we use LLM-based query rewriting only during the training\nphase. Eventually, a ranker is fine-tuned on the rewritten queries instead of\nthe original queries during training. In our extensive experiments, we find\nthat fine-tuning a ranker using re-written queries offers a significant\nimprovement of up to 33% on the passage ranking task and up to 28% on the\ndocument ranking task when compared to the baseline performance of using\noriginal queries.",
        "pdf_link": "https://arxiv.org/pdf/2308.16753v1.pdf"
    },
    {
        "title": "Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis",
        "authors": [
            "Nayeon Lee",
            "Chani Jung",
            "Junho Myung",
            "Jiho Jin",
            "Jose Camacho-Collados",
            "Juho Kim",
            "Alice Oh"
        ],
        "published": "2023-08-31T13:14:47Z",
        "summary": "Warning: this paper contains content that may be offensive or upsetting.\n  Most hate speech datasets neglect the cultural diversity within a single\nlanguage, resulting in a critical shortcoming in hate speech detection. To\naddress this, we introduce CREHate, a CRoss-cultural English Hate speech\ndataset. To construct CREHate, we follow a two-step procedure: 1) cultural post\ncollection and 2) cross-cultural annotation. We sample posts from the SBIC\ndataset, which predominantly represents North America, and collect posts from\nfour geographically diverse English-speaking countries (Australia, United\nKingdom, Singapore, and South Africa) using culturally hateful keywords we\nretrieve from our survey. Annotations are collected from the four countries\nplus the United States to establish representative labels for each country. Our\nanalysis highlights statistically significant disparities across countries in\nhate speech annotations. Only 56.2% of the posts in CREHate achieve consensus\namong all countries, with the highest pairwise label difference rate of 26%.\nQualitative analysis shows that label disagreement occurs mostly due to\ndifferent interpretations of sarcasm and the personal bias of annotators on\ndivisive topics. Lastly, we evaluate large language models (LLMs) under a\nzero-shot setting and show that current LLMs tend to show higher accuracies on\nAnglosphere country labels in CREHate. Our dataset and codes are available at:\nhttps://github.com/nlee0212/CREHate",
        "pdf_link": "https://arxiv.org/pdf/2308.16705v3.pdf"
    },
    {
        "title": "Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering",
        "authors": [
            "Lars-Peter Meyer",
            "Johannes Frey",
            "Kurt Junghanns",
            "Felix Brei",
            "Kirill Bulert",
            "Sabine Gr\u00fcnder-Fahrer",
            "Michael Martin"
        ],
        "published": "2023-08-31T10:31:19Z",
        "summary": "As the field of Large Language Models (LLMs) evolves at an accelerated pace,\nthe critical need to assess and monitor their performance emerges. We introduce\na benchmarking framework focused on knowledge graph engineering (KGE)\naccompanied by three challenges addressing syntax and error correction, facts\nextraction and dataset generation. We show that while being a useful tool, LLMs\nare yet unfit to assist in knowledge graph generation with zero-shot prompting.\nConsequently, our LLM-KG-Bench framework provides automatic evaluation and\nstorage of LLM responses as well as statistical data and visualization tools to\nsupport tracking of prompt engineering and model performance.",
        "pdf_link": "https://arxiv.org/pdf/2308.16622v1.pdf"
    },
    {
        "title": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills",
        "authors": [
            "Amey Agrawal",
            "Ashish Panwar",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Bhargav S. Gulavani",
            "Ramachandran Ramjee"
        ],
        "published": "2023-08-31T00:03:02Z",
        "summary": "Large Language Model (LLM) inference consists of two distinct phases -\nprefill phase which processes the input prompt and decode phase which generates\noutput tokens autoregressively. While the prefill phase effectively saturates\nGPU compute at small batch sizes, the decode phase results in low compute\nutilization as it generates one token at a time per request. The varying\nprefill and decode times also lead to imbalance across micro-batches when using\npipeline parallelism, resulting in further inefficiency due to bubbles.\n  We present SARATHI to address these challenges. SARATHI employs\nchunked-prefills, which splits a prefill request into equal sized chunks, and\ndecode-maximal batching, which constructs a batch using a single prefill chunk\nand populates the remaining slots with decodes. During inference, the prefill\nchunk saturates GPU compute, while the decode requests 'piggyback' and cost up\nto an order of magnitude less compared to a decode-only batch. Chunked-prefills\nallows constructing multiple decode-maximal batches from a single prefill\nrequest, maximizing coverage of decodes that can piggyback. Furthermore, the\nuniform compute design of these batches ameliorates the imbalance between\nmicro-batches, significantly reducing pipeline bubbles.\n  Our techniques yield significant improvements in inference performance across\nmodels and hardware. For the LLaMA-13B model on A6000 GPU, SARATHI improves\ndecode throughput by up to 10x, and accelerates end-to-end throughput by up to\n1.33x. For LLaMa-33B on A100 GPU, we achieve 1.25x higher end-to-end-throughput\nand up to 4.25x higher decode throughput. When used with pipeline parallelism\non GPT-3, SARATHI reduces bubbles by 6.29x, resulting in an end-to-end\nthroughput improvement of 1.91x.",
        "pdf_link": "https://arxiv.org/pdf/2308.16369v1.pdf"
    },
    {
        "title": "Large Language Models as Data Preprocessors",
        "authors": [
            "Haochen Zhang",
            "Yuyang Dong",
            "Chuan Xiao",
            "Masafumi Oyamada"
        ],
        "published": "2023-08-30T23:28:43Z",
        "summary": "Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's\nLLaMA variants, have marked a significant advancement in artificial\nintelligence. Trained on vast amounts of text data, LLMs are capable of\nunderstanding and generating human-like text across a diverse range of topics.\nThis study expands on the applications of LLMs, exploring their potential in\ndata preprocessing, a critical stage in data mining and analytics applications.\nWe delve into the applicability of state-of-the-art LLMs such as GPT-3.5,\nGPT-4, and Vicuna-13B for error detection, data imputation, schema matching,\nand entity matching tasks. Alongside showcasing the inherent capabilities of\nLLMs, we highlight their limitations, particularly in terms of computational\nexpense and inefficiency. We propose an LLM-based framework for data\npreprocessing, which integrates cutting-edge prompt engineering techniques,\ncoupled with traditional methods like contextualization and feature selection,\nto improve the performance and efficiency of these models. The effectiveness of\nLLMs in data preprocessing is evaluated through an experimental study spanning\n12 datasets. GPT-4 emerged as a standout, achieving 100\\% accuracy or F1 score\non 4 datasets, suggesting LLMs' immense potential in these tasks. Despite\ncertain limitations, our study underscores the promise of LLMs in this domain\nand anticipates future developments to overcome current hurdles.",
        "pdf_link": "https://arxiv.org/pdf/2308.16361v1.pdf"
    },
    {
        "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
        "authors": [
            "Chi Han",
            "Qifan Wang",
            "Hao Peng",
            "Wenhan Xiong",
            "Yu Chen",
            "Heng Ji",
            "Sinong Wang"
        ],
        "published": "2023-08-30T16:47:51Z",
        "summary": "Today's large language models (LLMs) typically train on short text segments\n(e.g., <4K tokens) due to the quadratic complexity of their Transformer\narchitectures. As a result, their performance suffers drastically on inputs\nlonger than those encountered during training, substantially limiting their\napplications in real-world tasks involving long contexts such as encoding\nscientific articles, code repositories, or long dialogues. Through theoretical\nanalysis and empirical investigation, this work identifies three major factors\ncontributing to this length generalization failure. Our theoretical analysis\nfurther reveals that commonly used techniques like truncating the attention\nwindow or relative positional encodings are inadequate to address them.\nAnswering these challenges, we propose LM-Infinite, a simple and effective\nmethod for enhancing LLMs' capabilities of handling long contexts. LM-Infinite\nis highly flexible and can be used with most modern LLMs off-the-shelf. Without\nany parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments\nto generalize to up to 200M length inputs while retaining perplexity. It also\nimproves performance on downstream tasks such as Passkey Retrieval and Qasper\nin the zero-shot setting. LM-Infinite brings substantial efficiency\nimprovements: it achieves 2.7x decoding speed up and 7.5x memory saving over\nthe original model. Our code will be publicly available upon publication.",
        "pdf_link": "https://arxiv.org/pdf/2308.16137v6.pdf"
    },
    {
        "title": "FPTQ: Fine-grained Post-Training Quantization for Large Language Models",
        "authors": [
            "Qingyuan Li",
            "Yifan Zhang",
            "Liang Li",
            "Peng Yao",
            "Bo Zhang",
            "Xiangxiang Chu",
            "Yerui Sun",
            "Li Du",
            "Yuchen Xie"
        ],
        "published": "2023-08-30T12:18:18Z",
        "summary": "In the era of large-scale language models, the substantial parameter size\nposes significant challenges for deployment. Being a prevalent compression\ntechnique, quantization has emerged as the mainstream practice to tackle this\nissue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and\nactivations in such bit widths). In this study, we propose a novel W4A8\npost-training quantization method for the available open-sourced LLMs, which\ncombines the advantages of both two recipes. Therefore, we can leverage the\nbenefit in the I/O utilization of 4-bit weight quantization and the\nacceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces\nnotorious performance degradation. As a remedy, we involve layerwise activation\nquantization strategies which feature a novel logarithmic equalization for most\nintractable layers, and we combine them with fine-grained weight quantization.\nWithout whistles and bells, we eliminate the necessity for further fine-tuning\nand obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, and\nLLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization is\nachievable for the deployment of large language models, fostering their\nwide-spreading real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2308.15987v1.pdf"
    },
    {
        "title": "Quantifying and Analyzing Entity-level Memorization in Large Language Models",
        "authors": [
            "Zhenhong Zhou",
            "Jiuyang Xiang",
            "Chaomeng Chen",
            "Sen Su"
        ],
        "published": "2023-08-30T03:06:47Z",
        "summary": "Large language models (LLMs) have been proven capable of memorizing their\ntraining data, which can be extracted through specifically designed prompts. As\nthe scale of datasets continues to grow, privacy risks arising from\nmemorization have attracted increasing attention. Quantifying language model\nmemorization helps evaluate potential privacy risks. However, prior works on\nquantifying memorization require access to the precise original data or incur\nsubstantial computational overhead, making it difficult for applications in\nreal-world language models. To this end, we propose a fine-grained,\nentity-level definition to quantify memorization with conditions and metrics\ncloser to real-world scenarios. In addition, we also present an approach for\nefficiently extracting sensitive entities from autoregressive language models.\nWe conduct extensive experiments based on the proposed, probing language\nmodels' ability to reconstruct sensitive entities under different settings. We\nfind that language models have strong memorization at the entity level and are\nable to reproduce the training data even with partial leakages. The results\ndemonstrate that LLMs not only memorize their training data but also understand\nassociations between entities. These findings necessitate that trainers of LLMs\nexercise greater prudence regarding model memorization, adopting memorization\nmitigation techniques to preclude privacy violations.",
        "pdf_link": "https://arxiv.org/pdf/2308.15727v2.pdf"
    },
    {
        "title": "Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model",
        "authors": [
            "Kazuki Hori",
            "Kanata Suzuki",
            "Tetsuya Ogata"
        ],
        "published": "2023-08-30T00:54:44Z",
        "summary": "The application of the Large Language Model (LLM) to robot action planning\nhas been actively studied. The instructions given to the LLM by natural\nlanguage may include ambiguity and lack of information depending on the task\ncontext. It is possible to adjust the output of LLM by making the instruction\ninput more detailed; however, the design cost is high. In this paper, we\npropose the interactive robot action planning method that allows the LLM to\nanalyze and gather missing information by asking questions to humans. The\nmethod can minimize the design cost of generating precise robot instructions.\nWe demonstrated the effectiveness of our method through concrete examples in\ncooking tasks. However, our experiments also revealed challenges in robot\naction planning with LLM, such as asking unimportant questions and assuming\ncrucial information without asking. Shedding light on these issues provides\nvaluable insights for future research on utilizing LLM for robotics.",
        "pdf_link": "https://arxiv.org/pdf/2308.15684v2.pdf"
    },
    {
        "title": "Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",
        "authors": [
            "Angus Addlesee",
            "Weronika Siei\u0144ska",
            "Nancie Gunson",
            "Daniel Hern\u00e1ndez Garcia",
            "Christian Dondrup",
            "Oliver Lemon"
        ],
        "published": "2023-08-29T11:40:03Z",
        "summary": "This paper evaluates the extent to which current Large Language Models (LLMs)\ncan capture task-oriented multi-party conversations (MPCs). We have recorded\nand transcribed 29 MPCs between patients, their companions, and a social robot\nin a hospital. We then annotated this corpus for multi-party goal-tracking and\nintent-slot recognition. People share goals, answer each other's goals, and\nprovide other people's goals in MPCs - none of which occur in dyadic\ninteractions. To understand user goals in MPCs, we compared three methods in\nzero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks\nto train DialogLM using LED, and employed prompt engineering techniques with\nGPT-3.5-turbo, to determine which approach can complete this novel task with\nlimited data. GPT-3.5-turbo significantly outperformed the others in a few-shot\nsetting. The `reasoning' style prompt, when given 7% of the corpus as example\nannotated conversations, was the best performing method. It correctly annotated\n62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition\nMPCs. A `story' style prompt increased model hallucination, which could be\ndetrimental if deployed in safety-critical settings. We conclude that\nmulti-party conversations still challenge state-of-the-art LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.15231v1.pdf"
    },
    {
        "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models",
        "authors": [
            "Junyang Wang",
            "Yiyang Zhou",
            "Guohai Xu",
            "Pengcheng Shi",
            "Chenlin Zhao",
            "Haiyang Xu",
            "Qinghao Ye",
            "Ming Yan",
            "Ji Zhang",
            "Jihua Zhu",
            "Jitao Sang",
            "Haoyu Tang"
        ],
        "published": "2023-08-29T08:51:24Z",
        "summary": "Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.",
        "pdf_link": "https://arxiv.org/pdf/2308.15126v3.pdf"
    },
    {
        "title": "SwapMoE: Efficient Memory-Constrained Serving of Large Sparse MoE Models via Dynamic Expert Pruning and Swapping",
        "authors": [
            "Rui Kong",
            "Yuanchun Li",
            "Qingtian Feng",
            "Weijun Wang",
            "Linghe Kong",
            "Yunxin Liu"
        ],
        "published": "2023-08-29T05:25:21Z",
        "summary": "Mixture of experts (MoE) is a popular technique to improve capacity of large\nmodels with conditionally-activated parallel neural network modules (experts).\nDue to its remarkable scaling performance with sparse computation, it is widely\nused in modern Large Language Models (LLMs) and Large Vision Models (LVMs).\nHowever, serving such large models on edge devices is challenging due to memory\nconstraints. Typical solutions like memory swapping or weight pruning may lead\nto significantly higher latency or severe accuracy loss.\n  In this paper, we introduce SwapMoE, a framework for efficient continuous\nMoE-based large models serving with tunable memory budgets. The main idea of\nSwapMoE is to keep a small dynamic set of important experts, namely Virtual\nExperts, in the main memory for inference, while seamlessly maintaining how the\nVirtual Experts map to the actual experts. We use a profiling-guided planner to\nallocate the resources for SwapMoE that can fully utilize the memory budgets\nand bandwidth, and an importance-aware scheduler to efficiently identify,\nupdate, and use the Virtual Experts for accurate inference.\n  To evaluate SwapMoE, we conduct experiments on multiple edge devices with\nstate-of-the-art MoE-based Large Language Models and Large Vision Models. The\nresults demonstrate remarkable performance of SwapMoE under various memory\nconstraints. Specifically, SwapMoE can enable running large MoE models under\ntight memory budgets with similar latency to pruned compact models, while with\nsignificantly higher accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2308.15030v2.pdf"
    },
    {
        "title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models",
        "authors": [
            "Qingyue Wang",
            "Liang Ding",
            "Yanan Cao",
            "Zhiliang Tian",
            "Shi Wang",
            "Dacheng Tao",
            "Li Guo"
        ],
        "published": "2023-08-29T04:59:53Z",
        "summary": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts will be released later.",
        "pdf_link": "https://arxiv.org/pdf/2308.15022v2.pdf"
    },
    {
        "title": "LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks",
        "authors": [
            "Haokun Liu",
            "Yaonan Zhu",
            "Kenji Kato",
            "Izumi Kondo",
            "Tadayoshi Aoyama",
            "Yasuhisa Hasegawa"
        ],
        "published": "2023-08-29T01:54:49Z",
        "summary": "This paper presents a novel approach to enhance autonomous robotic\nmanipulation using the Large Language Model (LLM) for logical inference,\nconverting high-level language commands into sequences of executable motion\nfunctions. The proposed system combines the advantage of LLM with YOLO-based\nenvironmental perception to enable robots to autonomously make reasonable\ndecisions and task planning based on the given commands. Additionally, to\naddress the potential inaccuracies or illogical actions arising from LLM, a\ncombination of teleoperation and Dynamic Movement Primitives (DMP) is employed\nfor action correction. This integration aims to improve the practicality and\ngeneralizability of the LLM-based human-robot collaboration system.",
        "pdf_link": "https://arxiv.org/pdf/2308.14972v1.pdf"
    },
    {
        "title": "Uncovering the Hidden Cost of Model Compression",
        "authors": [
            "Diganta Misra",
            "Muawiz Chaudhary",
            "Agam Goyal",
            "Bharat Runwal",
            "Pin Yu Chen"
        ],
        "published": "2023-08-29T01:47:49Z",
        "summary": "In an age dominated by resource-intensive foundation models, the ability to\nefficiently adapt to downstream tasks is crucial. Visual Prompting (VP),\ndrawing inspiration from the prompting techniques employed in Large Language\nModels (LLMs), has emerged as a pivotal method for transfer learning in the\nrealm of computer vision. As the importance of efficiency continues to rise,\nresearch into model compression has become indispensable in alleviating the\ncomputational burdens associated with training and deploying over-parameterized\nneural networks. A primary objective in model compression is to develop sparse\nand/or quantized models capable of matching or even surpassing the performance\nof their over-parameterized, full-precision counterparts. Although previous\nstudies have explored the effects of model compression on transfer learning,\nits impact on visual prompting-based transfer remains unclear. This study aims\nto bridge this gap, shedding light on the fact that model compression\ndetrimentally impacts the performance of visual prompting-based transfer,\nparticularly evident in scenarios with low data volume. Furthermore, our\nfindings underscore the adverse influence of sparsity on the calibration of\ndownstream visual-prompted models. However, intriguingly, we also illustrate\nthat such negative effects on calibration are not present when models are\ncompressed via quantization. This empirical investigation underscores the need\nfor a nuanced understanding beyond mere accuracy in sparse and quantized\nsettings, thereby paving the way for further exploration in Visual Prompting\ntechniques tailored for sparse and quantized models.",
        "pdf_link": "https://arxiv.org/pdf/2308.14969v3.pdf"
    },
    {
        "title": "Gender bias and stereotypes in Large Language Models",
        "authors": [
            "Hadas Kotek",
            "Rikker Dockum",
            "David Q. Sun"
        ],
        "published": "2023-08-28T22:32:05Z",
        "summary": "Large Language Models (LLMs) have made substantial progress in the past\nseveral months, shattering state-of-the-art benchmarks in many domains. This\npaper investigates LLMs' behavior with respect to gender stereotypes, a known\nissue for prior models. We use a simple paradigm to test the presence of gender\nbias, building on but differing from WinoBias, a commonly used gender bias\ndataset, which is likely to be included in the training data of current LLMs.\nWe test four recently published LLMs and demonstrate that they express biased\nassumptions about men and women's occupations. Our contributions in this paper\nare as follows: (a) LLMs are 3-6 times more likely to choose an occupation that\nstereotypically aligns with a person's gender; (b) these choices align with\npeople's perceptions better than with the ground truth as reflected in official\njob statistics; (c) LLMs in fact amplify the bias beyond what is reflected in\nperceptions or the ground truth; (d) LLMs ignore crucial ambiguities in\nsentence structure 95% of the time in our study items, but when explicitly\nprompted, they recognize the ambiguity; (e) LLMs provide explanations for their\nchoices that are factually inaccurate and likely obscure the true reason behind\ntheir predictions. That is, they provide rationalizations of their biased\nbehavior. This highlights a key property of these models: LLMs are trained on\nimbalanced datasets; as such, even with the recent successes of reinforcement\nlearning with human feedback, they tend to reflect those imbalances back at us.\nAs with other types of societal biases, we suggest that LLMs must be carefully\ntested to ensure that they treat minoritized individuals and communities\nequitably.",
        "pdf_link": "https://arxiv.org/pdf/2308.14921v1.pdf"
    },
    {
        "title": "Identifying and Mitigating the Security Risks of Generative AI",
        "authors": [
            "Clark Barrett",
            "Brad Boyd",
            "Elie Burzstein",
            "Nicholas Carlini",
            "Brad Chen",
            "Jihye Choi",
            "Amrita Roy Chowdhury",
            "Mihai Christodorescu",
            "Anupam Datta",
            "Soheil Feizi",
            "Kathleen Fisher",
            "Tatsunori Hashimoto",
            "Dan Hendrycks",
            "Somesh Jha",
            "Daniel Kang",
            "Florian Kerschbaum",
            "Eric Mitchell",
            "John Mitchell",
            "Zulfikar Ramzan",
            "Khawaja Shams",
            "Dawn Song",
            "Ankur Taly",
            "Diyi Yang"
        ],
        "published": "2023-08-28T18:51:09Z",
        "summary": "Every major technical invention resurfaces the dual-use dilemma -- the new\ntechnology has the potential to be used for good as well as for harm.\nGenerative AI (GenAI) techniques, such as large language models (LLMs) and\ndiffusion models, have shown remarkable capabilities (e.g., in-context\nlearning, code-completion, and text-to-image generation and editing). However,\nGenAI can be used just as well by attackers to generate new attacks and\nincrease the velocity and efficacy of existing attacks.\n  This paper reports the findings of a workshop held at Google (co-organized by\nStanford University and the University of Wisconsin-Madison) on the dual-use\ndilemma posed by GenAI. This paper is not meant to be comprehensive, but is\nrather an attempt to synthesize some of the interesting findings from the\nworkshop. We discuss short-term and long-term goals for the community on this\ntopic. We hope this paper provides both a launching point for a discussion on\nthis important topic as well as interesting problems that the research\ncommunity can work to address.",
        "pdf_link": "https://arxiv.org/pdf/2308.14840v4.pdf"
    },
    {
        "title": "Distilled GPT for Source Code Summarization",
        "authors": [
            "Chia-Yi Su",
            "Collin McMillan"
        ],
        "published": "2023-08-28T17:34:07Z",
        "summary": "A code summary is a brief natural language description of source code.\nSummaries are usually only a single sentence long, and yet form the backbone of\ndeveloper documentation. A short descriptions such as \"changes all visible\npolygons to the color blue\" can give a programmer a high-level idea of what\ncode does without the effort of reading the code itself. Recently, products\nbased on Large Language Models such as ChatGPT have demonstrated a strong\nability to write these descriptions automatically. However, to use these tools,\nprogrammers must send their code to untrusted third parties for processing\n(e.g., via an API call). This loss of custody is not acceptable to many\norganizations. In this paper, we present an alternative: we train an open\nsource model using sample output generated by GPT-3.5 in a process related to\nknowledge distillation. Our model is small enough (350m parameters) to be run\non a single 16gb GPU, yet we show in our evaluation that it is large enough to\nmimic GPT-3.5 on this task.",
        "pdf_link": "https://arxiv.org/pdf/2308.14731v2.pdf"
    },
    {
        "title": "Challenges of GPT-3-based Conversational Agents for Healthcare",
        "authors": [
            "Fabian Lechner",
            "Allison Lahnala",
            "Charles Welch",
            "Lucie Flek"
        ],
        "published": "2023-08-28T15:12:34Z",
        "summary": "The potential to provide patients with faster information access while\nallowing medical specialists to concentrate on critical tasks makes medical\ndomain dialog agents appealing. However, the integration of large-language\nmodels (LLMs) into these agents presents certain limitations that may result in\nserious consequences. This paper investigates the challenges and risks of using\nGPT-3-based models for medical question-answering (MedQA). We perform several\nevaluations contextualized in terms of standard medical principles. We provide\na procedure for manually designing patient queries to stress-test high-risk\nlimitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to\nrespond adequately to these queries, generating erroneous medical information,\nunsafe recommendations, and content that may be considered offensive.",
        "pdf_link": "https://arxiv.org/pdf/2308.14641v2.pdf"
    },
    {
        "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
        "authors": [
            "Yushi Bai",
            "Xin Lv",
            "Jiajie Zhang",
            "Hongchang Lyu",
            "Jiankai Tang",
            "Zhidian Huang",
            "Zhengxiao Du",
            "Xiao Liu",
            "Aohan Zeng",
            "Lei Hou",
            "Yuxiao Dong",
            "Jie Tang",
            "Juanzi Li"
        ],
        "published": "2023-08-28T11:53:40Z",
        "summary": "Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.",
        "pdf_link": "https://arxiv.org/pdf/2308.14508v1.pdf"
    },
    {
        "title": "Biomedical Entity Linking with Triple-aware Pre-Training",
        "authors": [
            "Xi Yan",
            "Cedric M\u00f6ller",
            "Ricardo Usbeck"
        ],
        "published": "2023-08-28T09:06:28Z",
        "summary": "Linking biomedical entities is an essential aspect in biomedical natural\nlanguage processing tasks, such as text mining and question answering. However,\na difficulty of linking the biomedical entities using current large language\nmodels (LLM) trained on a general corpus is that biomedical entities are\nscarcely distributed in texts and therefore have been rarely seen during\ntraining by the LLM. At the same time, those LLMs are not aware of high level\nsemantic connection between different biomedical entities, which are useful in\nidentifying similar concepts in different textual contexts. To cope with\naforementioned problems, some recent works focused on injecting knowledge graph\ninformation into LLMs. However, former methods either ignore the relational\nknowledge of the entities or lead to catastrophic forgetting. Therefore, we\npropose a novel framework to pre-train the powerful generative LLM by a corpus\nsynthesized from a KG. In the evaluations we are unable to confirm the benefit\nof including synonym, description or relational information.",
        "pdf_link": "https://arxiv.org/pdf/2308.14429v1.pdf"
    },
    {
        "title": "EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models",
        "authors": [
            "Rongjie Yi",
            "Liwei Guo",
            "Shiyun Wei",
            "Ao Zhou",
            "Shangguang Wang",
            "Mengwei Xu"
        ],
        "published": "2023-08-28T06:56:08Z",
        "summary": "Large Language Models (LLMs) such as GPTs and LLaMa have ushered in a\nrevolution in machine intelligence, owing to their exceptional capabilities in\na wide range of machine learning tasks. However, the transition of LLMs from\ndata centers to edge devices presents a set of challenges and opportunities.\nWhile this shift can enhance privacy and availability, it is hampered by the\nenormous parameter sizes of these models, leading to impractical runtime costs.\nIn light of these considerations, we introduce EdgeMoE, the first on-device\ninference engine tailored for mixture-of-expert (MoE) LLMs, a popular variant\nof sparse LLMs that exhibit nearly constant computational complexity as their\nparameter size scales. EdgeMoE achieves both memory and computational\nefficiency by strategically partitioning the model across the storage\nhierarchy. Specifically, non-expert weights are stored in the device's memory,\nwhile expert weights are kept in external storage and are fetched into memory\nonly when they are activated. This design is underpinned by a crucial insight\nthat expert weights, though voluminous, are infrequently accessed due to sparse\nactivation patterns. To further mitigate the overhead associated with expert\nI/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wise\nbitwidth adaptation: This method reduces the size of expert weights with an\nacceptable level of accuracy loss. (2) Expert management: It predicts the\nexperts that will be activated in advance and preloads them into the\ncompute-I/O pipeline, thus further optimizing the process. In empirical\nevaluations conducted on well-established MoE LLMs and various edge devices,\nEdgeMoE demonstrates substantial memory savings and performance improvements\nwhen compared to competitive baseline solutions.",
        "pdf_link": "https://arxiv.org/pdf/2308.14352v1.pdf"
    },
    {
        "title": "Evaluating the Robustness to Instructions of Large Language Models",
        "authors": [
            "Yuansheng Ni",
            "Sichao Jiang",
            "Xinyu wu",
            "Hui Shen",
            "Yuli Zhou"
        ],
        "published": "2023-08-28T04:57:07Z",
        "summary": "Recently, Instruction fine-tuning has risen to prominence as a potential\nmethod for enhancing the zero-shot capabilities of Large Language Models (LLMs)\non novel tasks. This technique has shown an exceptional ability to boost the\nperformance of moderately sized LLMs, sometimes even reaching performance\nlevels comparable to those of much larger model variants. The focus is on the\nrobustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an\nexploration of six models including Alpaca, Vicuna, WizardLM, and Traditional\nTask-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction\ndatasets as case studies. We carried out a comprehensive evaluation of these\ninstruction-following LLMs which have been tuned based on open-domain\ninstructions and task-oriented instructions. The main discussion is their\nperformance and robustness towards instructions. We have observed that in most\ncases, the model's performance in dealing with unfamiliar instructions tends to\nworsen significantly, and the robustness of the model for RE instructions\ndeteriorates compared to QA. Further, we discovered that up until a certain\nparameter size threshold (3B), the performance of the FLAN-T5 model improves as\nthe parameter count increases. The robustness of different scales of FLAN-T5\nmodels to RE instruction is worse than the robustness to QA instruction.",
        "pdf_link": "https://arxiv.org/pdf/2308.14306v3.pdf"
    },
    {
        "title": "Symbolic and Language Agnostic Large Language Models",
        "authors": [
            "Walid S. Saba"
        ],
        "published": "2023-08-27T20:24:33Z",
        "summary": "We argue that the relative success of large language models (LLMs) is not a\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\nan appropriate strategy of bottom-up reverse engineering of language at scale.\nHowever, due to the subsymbolic nature of these models whatever knowledge these\nsystems acquire about language will always be buried in millions of\nmicrofeatures (weights) none of which is meaningful on its own. Moreover, and\ndue to their stochastic nature, these models will often fail in capturing\nvarious inferential aspects that are prevalent in natural language. What we\nsuggest here is employing the successful bottom-up strategy in a symbolic\nsetting, producing symbolic, language agnostic and ontologically grounded large\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2308.14199v1.pdf"
    },
    {
        "title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations",
        "authors": [
            "Leonardo Ranaldi",
            "Giulia Pucci",
            "Andre Freitas"
        ],
        "published": "2023-08-27T19:22:12Z",
        "summary": "The language ability of Large Language Models (LLMs) is often unbalanced\ntowards English because of the imbalance in the distribution of the\npre-training data. This disparity is demanded in further fine-tuning and\naffecting the cross-lingual abilities of LLMs. In this paper, we propose to\nempower Instructiontuned LLMs (It-LLMs) in languages other than English by\nbuilding semantic alignment between them. Hence, we propose CrossAlpaca, an\nIt-LLM with cross-lingual instruction-following and Translation-following\ndemonstrations to improve semantic alignment between languages. We validate our\napproach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA\nand adapted versions of MMLU and BBH. Our models, tested over six different\nlanguages, outperform the It-LLMs tuned on monolingual data. The final results\nshow that instruction tuning on non-English data is not enough and that\nsemantic alignment can be further improved by Translation-following\ndemonstrations.",
        "pdf_link": "https://arxiv.org/pdf/2308.14186v1.pdf"
    },
    {
        "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
        "authors": [
            "Kaiyuan Gao",
            "Sunan He",
            "Zhenyu He",
            "Jiacheng Lin",
            "QiZhi Pei",
            "Jie Shao",
            "Wei Zhang"
        ],
        "published": "2023-08-27T16:14:19Z",
        "summary": "Generative pre-trained transformer (GPT) models have revolutionized the field\nof natural language processing (NLP) with remarkable performance in various\ntasks and also extend their power to multimodal domains. Despite their success,\nlarge GPT models like GPT-4 face inherent limitations such as considerable\nsize, high computational requirements, complex deployment processes, and closed\ndevelopment loops. These constraints restrict their widespread adoption and\nraise concerns regarding their responsible development and usage. The need for\nuser-friendly, relatively small, and open-sourced alternative GPT models arises\nfrom the desire to overcome these limitations while retaining high performance.\nIn this survey paper, we provide an examination of alternative open-sourced\nmodels of large GPTs, focusing on user-friendly and relatively small models\nthat facilitate easier deployment and accessibility. Through this extensive\nsurvey, we aim to equip researchers, practitioners, and enthusiasts with a\nthorough understanding of user-friendly and relatively small open-sourced\nmodels of large GPTs, their current state, challenges, and future research\ndirections, inspiring the development of more efficient, accessible, and\nversatile GPT models that cater to the broader scientific community and advance\nthe field of general artificial intelligence. The source contents are\ncontinuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.",
        "pdf_link": "https://arxiv.org/pdf/2308.14149v1.pdf"
    },
    {
        "title": "Detecting Language Model Attacks with Perplexity",
        "authors": [
            "Gabriel Alon",
            "Michael Kamfonas"
        ],
        "published": "2023-08-27T15:20:06Z",
        "summary": "A novel hack involving Large Language Models (LLMs) has emerged, exploiting\nadversarial suffixes to deceive models into generating perilous responses. Such\njailbreaks can trick LLMs into providing intricate instructions to a malicious\nuser for creating explosives, orchestrating a bank heist, or facilitating the\ncreation of offensive content. By evaluating the perplexity of queries with\nadversarial suffixes using an open-source LLM (GPT-2), we found that they have\nexceedingly high perplexity values. As we explored a broad range of regular\n(non-adversarial) prompt varieties, we concluded that false positives are a\nsignificant challenge for plain perplexity filtering. A Light-GBM trained on\nperplexity and token length resolved the false positives and correctly detected\nmost adversarial attacks in the test set.",
        "pdf_link": "https://arxiv.org/pdf/2308.14132v3.pdf"
    },
    {
        "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
        "authors": [
            "Chengkun Wei",
            "Wenlong Meng",
            "Zhikun Zhang",
            "Min Chen",
            "Minghu Zhao",
            "Wenjing Fang",
            "Lei Wang",
            "Zihui Zhang",
            "Wenzhi Chen"
        ],
        "published": "2023-08-26T15:21:47Z",
        "summary": "Prompt-tuning has emerged as an attractive paradigm for deploying large-scale\nlanguage models due to its strong downstream task performance and efficient\nmultitask serving ability. Despite its wide adoption, we empirically show that\nprompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside\nin the pretrained models and can affect arbitrary downstream tasks. The\nstate-of-the-art backdoor detection approaches cannot defend against\ntask-agnostic backdoors since they hardly converge in reversing the backdoor\ntriggers. To address this issue, we propose LMSanitator, a novel approach for\ndetecting and removing task-agnostic backdoors on Transformer models. Instead\nof directly inverting the triggers, LMSanitator aims to invert the predefined\nattack vectors (pretrained models' output when the input is embedded with\ntriggers) of the task-agnostic backdoors, which achieves much better\nconvergence performance and backdoor detection accuracy. LMSanitator further\nleverages prompt-tuning's property of freezing the pretrained model to perform\naccurate and fast output monitoring and input purging during the inference\nphase. Extensive experiments on multiple language models and NLP tasks\nillustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves\n92.8% backdoor detection accuracy on 960 models and decreases the attack\nsuccess rate to less than 1% in most scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2308.13904v2.pdf"
    },
    {
        "title": "Planning with Logical Graph-based Language Model for Instruction Generation",
        "authors": [
            "Fan Zhang",
            "Kebing Jin",
            "Hankz Hankui Zhuo"
        ],
        "published": "2023-08-26T06:28:14Z",
        "summary": "Despite the superior performance of large language models to generate natural\nlanguage texts, it is hard to generate texts with correct logic according to a\ngiven task, due to the difficulties for neural models to capture implied rules\nfrom free-form texts. In this paper, we propose a novel graph-based language\nmodel, Logical-GLM, to infuse logic into language models for more valid text\ngeneration and interpretability. Specifically, we first capture information\nfrom natural language instructions and construct logical bayes graphs that\ngenerally describe domains. Next, we generate logical skeletons to guide\nlanguage model training, infusing domain knowledge into language models.\nFinally, we alternately optimize the searching policy of graphs and language\nmodels until convergence. The experimental results show that Logical-GLM is\nboth effective and efficient compared with traditional language models, despite\nusing smaller-scale training data and fewer parameters. Our approach can\ngenerate instructional texts with more correct logic owing to the internalized\ndomain knowledge. Moreover, the usage of logical graphs reflects the inner\nmechanism of the language models, which improves the interpretability of\nblack-box models.",
        "pdf_link": "https://arxiv.org/pdf/2308.13782v1.pdf"
    },
    {
        "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
        "authors": [
            "Charles O'Neill",
            "Jack Miller",
            "Ioana Ciuca",
            "Yuan-Sen Ting",
            "Thang Bui"
        ],
        "published": "2023-08-26T05:20:58Z",
        "summary": "In this paper, we tackle the emerging challenge of unintended harmful content\ngeneration in Large Language Models (LLMs) with a novel dual-stage optimisation\ntechnique using adversarial fine-tuning. Our two-pronged approach employs an\nadversarial model, fine-tuned to generate potentially harmful prompts, and a\njudge model, iteratively optimised to discern these prompts. In this\nadversarial cycle, the two models seek to outperform each other in the\nprompting phase, generating a dataset of rich examples which are then used for\nfine-tuning. This iterative application of prompting and fine-tuning allows\ncontinuous refinement and improved performance. The performance of our approach\nis evaluated through classification accuracy on a dataset consisting of\nproblematic prompts not detected by GPT-4, as well as a selection of\ncontentious but unproblematic prompts. We show considerable increase in\nclassification accuracy of the judge model on this challenging dataset as it\nundergoes the optimisation process. Furthermore, we show that a rudimentary\nmodel \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set\nthan GPT-4 after only a few rounds of this process, and that this fine-tuning\nimproves performance in parallel tasks such as toxic comment identification.",
        "pdf_link": "https://arxiv.org/pdf/2308.13768v1.pdf"
    },
    {
        "title": "Rethinking Language Models as Symbolic Knowledge Graphs",
        "authors": [
            "Vishwas Mruthyunjaya",
            "Pouya Pezeshkpour",
            "Estevam Hruschka",
            "Nikita Bhutani"
        ],
        "published": "2023-08-25T21:25:08Z",
        "summary": "Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric\napplications such as search, question answering and recommendation. As\ncontemporary language models (LMs) trained on extensive textual data have\ngained prominence, researchers have extensively explored whether the parametric\nknowledge within these models can match up to that present in knowledge graphs.\nVarious methodologies have indicated that enhancing the size of the model or\nthe volume of training data enhances its capacity to retrieve symbolic\nknowledge, often with minimal or no human supervision. Despite these\nadvancements, there is a void in comprehensively evaluating whether LMs can\nencompass the intricate topological and semantic attributes of KGs, attributes\ncrucial for reasoning processes. In this work, we provide an exhaustive\nevaluation of language models of varying sizes and capabilities. We construct\nnine qualitative benchmarks that encompass a spectrum of attributes including\nsymmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths,\nentity-centricity, bias and ambiguity. Additionally, we propose novel\nevaluation metrics tailored for each of these attributes. Our extensive\nevaluation of various LMs shows that while these models exhibit considerable\npotential in recalling factual information, their ability to capture intricate\ntopological and semantic traits of KGs remains significantly constrained. We\nnote that our proposed evaluation metrics are more reliable in evaluating these\nabilities than the existing metrics. Lastly, some of our benchmarks challenge\nthe common notion that larger LMs (e.g., GPT-4) universally outshine their\nsmaller counterparts (e.g., BERT).",
        "pdf_link": "https://arxiv.org/pdf/2308.13676v1.pdf"
    },
    {
        "title": "Large Language Models Should Ask Clarifying Questions to Increase Confidence in Generated Code",
        "authors": [
            "Jie JW Wu"
        ],
        "published": "2023-08-25T17:33:05Z",
        "summary": "Large language models (LLMs) have significantly improved the ability to\nperform tasks in the field of code generation. However, there is still a gap\nbetween LLMs being capable coders and being top-tier software engineers. Based\non the observation that toplevel software engineers often ask clarifying\nquestions to reduce ambiguity in both requirements and coding solutions, I\nargue that the same should be applied to LLMs for code generation tasks. By\nasking probing questions in various topics before generating the final code,\nthe challenges of programming with LLMs, such as unclear intent specification,\nlack of computational thinking, and undesired code quality, may be alleviated.\nThis, in turn, increases confidence in the generated code. In this work, I\nexplore how to leverage better communication skills to achieve greater\nconfidence in generated code. I propose a communication-centered process that\nuses an LLM-generated communicator to identify issues with high ambiguity or\nlow confidence in problem descriptions and generated code. I then ask\nclarifying questions to obtain responses from users for refining the code.",
        "pdf_link": "https://arxiv.org/pdf/2308.13507v2.pdf"
    },
    {
        "title": "The Poison of Alignment",
        "authors": [
            "Aibek Bekbayev",
            "Sungbae Chun",
            "Yerzat Dulat",
            "James Yamazaki"
        ],
        "published": "2023-08-25T15:51:15Z",
        "summary": "From the perspective of content safety issues, alignment has shown to limit\nlarge language models' (LLMs) harmful content generation. This intentional\nmethod of reinforcing models to not respond to certain user inputs seem to be\npresent in many modern open-source instruction tuning datasets such as\nOpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned\nmodel's performance affected by the presence of alignment in supervised\nfine-tuning dataset. To be specific, we noticed that alignment acts as if it is\npoisoning the instruction dataset. Experimentally, we demonstrate that aligned\nanswers significantly worsen the performance of the resulting fine-tuned\nmodel's on various reasoning benchmarks such as Big Bench (BBH), Massive\nMultitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning\nOver Paragraphs (DROP), performing worse than the counterpart tuned without\nalignment by 4-33%.",
        "pdf_link": "https://arxiv.org/pdf/2308.13449v1.pdf"
    },
    {
        "title": "Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions",
        "authors": [
            "Reem I. Masoud",
            "Ziquan Liu",
            "Martin Ferianc",
            "Philip Treleaven",
            "Miguel Rodrigues"
        ],
        "published": "2023-08-25T14:50:13Z",
        "summary": "The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals from\nvarious cultural norms. Existing work investigated political and social biases\nand public opinions rather than their cultural values. To address this\nlimitation, the proposed Cultural Alignment Test (CAT) quantifies cultural\nalignment using Hofstede's cultural dimension framework, which offers an\nexplanatory cross-cultural comparison through the latent variable analysis. We\napply our approach to assess the cultural values embedded in state-of-the-art\nLLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United\nStates (US), Saudi Arabia, China, and Slovakia, using different prompting\nstyles and hyperparameter settings. Our results not only quantify cultural\nalignment of LLMs with certain countries, but also reveal the difference\nbetween LLMs in explanatory cultural dimensions. While all LLMs did not provide\nsatisfactory results in understanding cultural values, GPT-4 exhibited the\nhighest CAT score for the cultural values of the US.",
        "pdf_link": "https://arxiv.org/pdf/2309.12342v1.pdf"
    },
    {
        "title": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering",
        "authors": [
            "Keheng Wang",
            "Feiyu Duan",
            "Sirui Wang",
            "Peiguang Li",
            "Yunsen Xian",
            "Chuantao Yin",
            "Wenge Rong",
            "Zhang Xiong"
        ],
        "published": "2023-08-25T09:23:55Z",
        "summary": "Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown\nimpressive reasoning ability in various downstream tasks. Even so, suffering\nfrom hallucinations and the inability to access external knowledge, LLMs often\ncome with incorrect or unfaithful intermediate reasoning steps, especially in\nthe context of answering knowledge-intensive tasks such as KBQA. To alleviate\nthis issue, we propose a framework called Knowledge-Driven Chain-of-Thought\n(KD-CoT) to verify and modify reasoning traces in CoT via interaction with\nexternal knowledge, and thus overcome the hallucinations and error propagation.\nConcretely, we formulate the CoT rationale process of LLMs into a structured\nmulti-round QA format. In each round, LLMs interact with a QA system that\nretrieves external knowledge and produce faithful reasoning traces based on\nretrieved precise answers. The structured CoT reasoning of LLMs is facilitated\nby our developed KBQA CoT collection, which serves as in-context learning\ndemonstrations and can also be utilized as feedback augmentation to train a\nrobust retriever. Extensive experiments on WebQSP and ComplexWebQuestion\ndatasets demonstrate the effectiveness of proposed KD-CoT in task-solving\nreasoning generation, which outperforms the vanilla CoT ICL with an absolute\nsuccess rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented\nretriever outperforms the state-of-the-art baselines for retrieving knowledge,\nachieving significant improvement in Hit and recall performance. Our code and\ndata are released on https://github.com/AdelWang/KD-CoT/tree/main.",
        "pdf_link": "https://arxiv.org/pdf/2308.13259v2.pdf"
    },
    {
        "title": "Measuring Spurious Correlation in Classification: 'Clever Hans' in Translationese",
        "authors": [
            "Angana Borah",
            "Daria Pylypenko",
            "Cristina Espana-Bonet",
            "Josef van Genabith"
        ],
        "published": "2023-08-25T04:19:58Z",
        "summary": "Recent work has shown evidence of 'Clever Hans' behavior in high-performance\nneural translationese classifiers, where BERT-based classifiers capitalize on\nspurious correlations, in particular topic information, between data and target\nclassification labels, rather than genuine translationese signals.\nTranslationese signals are subtle (especially for professional translation) and\ncompete with many other signals in the data such as genre, style, author, and,\nin particular, topic. This raises the general question of how much of the\nperformance of a classifier is really due to spurious correlations in the data\nversus the signals actually targeted for by the classifier, especially for\nsubtle target signals and in challenging (low resource) data settings. We focus\non topic-based spurious correlation and approach the question from two\ndirections: (i) where we have no knowledge about spurious topic information and\nits distribution in the data, (ii) where we have some indication about the\nnature of spurious topic correlations. For (i) we develop a measure from first\nprinciples capturing alignment of unsupervised topics with target\nclassification labels as an indication of spurious topic information in the\ndata. We show that our measure is the same as purity in clustering and propose\na 'topic floor' (as in a 'noise floor') for classification. For (ii) we\ninvestigate masking of known spurious topic carriers in classification. Both\n(i) and (ii) contribute to quantifying and (ii) to mitigating spurious\ncorrelations.",
        "pdf_link": "https://arxiv.org/pdf/2308.13170v1.pdf"
    },
    {
        "title": "Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4",
        "authors": [
            "Maroa Mumtarin",
            "Md Samiullah Chowdhury",
            "Jonathan Wood"
        ],
        "published": "2023-08-25T00:09:16Z",
        "summary": "In traffic safety research, extracting information from crash narratives\nusing text analysis is a common practice. With recent advancements of large\nlanguage models (LLM), it would be useful to know how the popular LLM\ninterfaces perform in classifying or extracting information from crash\nnarratives. To explore this, our study has used the three most popular publicly\navailable LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their\nusefulness and boundaries in extracting information and answering queries\nrelated to accidents from 100 crash narratives from Iowa and Kansas. During the\ninvestigation, their capabilities and limitations were assessed and their\nresponses to the queries were compared. Five questions were asked related to\nthe narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has\nthe crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5)\nWhat are the sequence of harmful events in the crash? For questions 1 through\n4, the overall similarity among the LLMs were 70%, 35%, 96% and 89%,\nrespectively. The similarities were higher while answering direct questions\nrequiring binary responses and significantly lower for complex questions. To\ncompare the responses to question 5, network diagram and centrality measures\nwere analyzed. The network diagram from the three LLMs were not always similar\nalthough they sometimes have the same influencing events with high in-degree,\nout-degree and betweenness centrality. This study suggests using multiple\nmodels to extract viable information from narratives. Also, caution must be\npracticed while using these interfaces to obtain crucial safety related\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2308.13563v1.pdf"
    },
    {
        "title": "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal",
        "authors": [
            "Matej Ze\u010devi\u0107",
            "Moritz Willig",
            "Devendra Singh Dhami",
            "Kristian Kersting"
        ],
        "published": "2023-08-24T20:23:13Z",
        "summary": "Some argue scale is all what is needed to achieve AI, covering even causal\nmodels. We make it clear that large language models (LLMs) cannot be causal and\ngive reason onto why sometimes we might feel otherwise. To this end, we define\nand exemplify a new subgroup of Structural Causal Model (SCM) that we call meta\nSCM which encode causal facts about other SCM within their variables. We\nconjecture that in the cases where LLM succeed in doing causal inference,\nunderlying was a respective meta SCM that exposed correlations between causal\nfacts in natural language on whose data the LLM was ultimately trained. If our\nhypothesis holds true, then this would imply that LLMs are like parrots in that\nthey simply recite the causal knowledge embedded in the data. Our empirical\nanalysis provides favoring evidence that current LLMs are even weak `causal\nparrots.'",
        "pdf_link": "https://arxiv.org/pdf/2308.13067v1.pdf"
    },
    {
        "title": "ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching",
        "authors": [
            "M. Caner Tol",
            "Berk Sunar"
        ],
        "published": "2023-08-24T20:04:36Z",
        "summary": "Security critical software, e.g., OpenSSL, comes with numerous side-channel\nleakages left unpatched due to a lack of resources or experts. The situation\nwill only worsen as the pace of code development accelerates, with developers\nrelying on Large Language Models (LLMs) to automatically generate code. In this\nwork, we explore the use of LLMs in generating patches for vulnerable code with\nmicroarchitectural side-channel leakages. For this, we investigate the\ngenerative abilities of powerful LLMs by carefully crafting prompts following a\nzero-shot learning approach. All generated code is dynamically analyzed by\nleakage detection tools, which are capable of pinpointing information leakage\nat the instruction level leaked either from secret dependent accesses or\nbranches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts\nare used to generate candidate replacements for vulnerable code, which are then\nanalyzed for correctness and for leakage resilience. From a cost/performance\nperspective, the GPT4-based configuration costs in API calls a mere few cents\nper vulnerability fixed. Our results show that LLM-based patching is far more\ncost-effective and thus provides a scalable solution. Finally, the framework we\npropose will improve in time, especially as vulnerability detection tools and\nLLMs mature.",
        "pdf_link": "https://arxiv.org/pdf/2308.13062v1.pdf"
    },
    {
        "title": "POLCA: Power Oversubscription in LLM Cloud Providers",
        "authors": [
            "Pratyush Patel",
            "Esha Choukse",
            "Chaojie Zhang",
            "\u00cd\u00f1igo Goiri",
            "Brijesh Warrier",
            "Nithish Mahalingam",
            "Ricardo Bianchini"
        ],
        "published": "2023-08-24T16:32:34Z",
        "summary": "Recent innovation in large language models (LLMs), and their myriad use-cases\nhave rapidly driven up the compute capacity demand for datacenter GPUs. Several\ncloud providers and other enterprises have made substantial plans of growth in\ntheir datacenters to support these new workloads. One of the key bottleneck\nresources in datacenters is power, and given the increasing model sizes of\nLLMs, they are becoming increasingly power intensive. In this paper, we show\nthat there is a significant opportunity to oversubscribe power in LLM clusters.\nPower oversubscription improves the power efficiency of these datacenters,\nallowing more deployable servers per datacenter, and reduces the deployment\ntime, since building new datacenters is slow.\n  We extensively characterize the power consumption patterns of a variety of\nLLMs and their configurations. We identify the differences between the\ninference and training power consumption patterns. Based on our analysis of\nthese LLMs, we claim that the average and peak power utilization in LLM\nclusters for inference should not be very high. Our deductions align with the\ndata from production LLM clusters, revealing that inference workloads offer\nsubstantial headroom for power oversubscription. However, the stringent set of\ntelemetry and controls that GPUs offer in a virtualized environment, makes it\nchallenging to have a reliable and robust power oversubscription mechanism.\n  We propose POLCA, our framework for power oversubscription that is robust,\nreliable, and readily deployable for GPU clusters. Using open-source models to\nreplicate the power patterns observed in production, we simulate POLCA and\ndemonstrate that we can deploy 30% more servers in the same GPU cluster for\ninference, with minimal performance loss",
        "pdf_link": "https://arxiv.org/pdf/2308.12908v1.pdf"
    },
    {
        "title": "Large Language Models Vote: Prompting for Rare Disease Identification",
        "authors": [
            "David Oniani",
            "Jordan Hilsman",
            "Hang Dong",
            "Fengyi Gao",
            "Shiven Verma",
            "Yanshan Wang"
        ],
        "published": "2023-08-24T16:09:13Z",
        "summary": "The emergence of generative Large Language Models (LLMs) emphasizes the need\nfor accurate and efficient prompting approaches. LLMs are often applied in\nFew-Shot Learning (FSL) contexts, where tasks are executed with minimal\ntraining data. FSL has become popular in many Artificial Intelligence (AI)\nsubdomains, including AI for health. Rare diseases affect a small fraction of\nthe population. Rare disease identification from clinical notes inherently\nrequires FSL techniques due to limited data availability. Manual data\ncollection and annotation is both expensive and time-consuming. In this paper,\nwe propose Models-Vote Prompting (MVP), a flexible prompting approach for\nimproving the performance of LLM queries in FSL settings. MVP works by\nprompting numerous LLMs to perform the same tasks and then conducting a\nmajority vote on the resulting outputs. This method achieves improved results\nto any one model in the ensemble on one-shot rare disease identification and\nclassification tasks. We also release a novel rare disease dataset for FSL,\navailable to those who signed the MIMIC-IV Data Use Agreement (DUA).\nFurthermore, in using MVP, each model is prompted multiple times, substantially\nincreasing the time needed for manual annotation, and to address this, we\nassess the feasibility of using JSON for automating generative LLM evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2308.12890v3.pdf"
    },
    {
        "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",
        "authors": [
            "Maximilian Mozes",
            "Xuanli He",
            "Bennett Kleinberg",
            "Lewis D. Griffin"
        ],
        "published": "2023-08-24T14:45:50Z",
        "summary": "Spurred by the recent rapid increase in the development and distribution of\nlarge language models (LLMs) across industry and academia, much recent work has\ndrawn attention to safety- and security-related threats and vulnerabilities of\nLLMs, including in the context of potentially criminal activities.\nSpecifically, it has been shown that LLMs can be misused for fraud,\nimpersonation, and the generation of malware; while other authors have\nconsidered the more general problem of AI alignment. It is important that\ndevelopers and practitioners alike are aware of security-related problems with\nsuch models. In this paper, we provide an overview of existing - predominantly\nscientific - efforts on identifying and mitigating threats and vulnerabilities\narising from LLMs. We present a taxonomy describing the relationship between\nthreats caused by the generative capabilities of LLMs, prevention measures\nintended to address such threats, and vulnerabilities arising from imperfect\nprevention measures. With our work, we hope to raise awareness of the\nlimitations of LLMs in light of such security concerns, among both experienced\ndevelopers and novel users of such technologies.",
        "pdf_link": "https://arxiv.org/pdf/2308.12833v1.pdf"
    },
    {
        "title": "VIGC: Visual Instruction Generation and Correction",
        "authors": [
            "Bin Wang",
            "Fan Wu",
            "Xiao Han",
            "Jiahui Peng",
            "Huaping Zhong",
            "Pan Zhang",
            "Xiaoyi Dong",
            "Weijia Li",
            "Wei Li",
            "Jiaqi Wang",
            "Conghui He"
        ],
        "published": "2023-08-24T11:21:05Z",
        "summary": "The integration of visual encoders and large language models (LLMs) has\ndriven recent progress in multimodal large language models (MLLMs). However,\nthe scarcity of high-quality instruction-tuning data for vision-language tasks\nremains a challenge. The current leading paradigm, such as LLaVA, relies on\nlanguage-only GPT-4 to generate data, which requires pre-annotated image\ncaptions and detection bounding boxes, suffering from understanding image\ndetails. A practical solution to this problem would be to utilize the available\nmultimodal large language models (MLLMs) to generate instruction data for\nvision-language tasks. However, it's worth noting that the currently accessible\nMLLMs are not as powerful as their LLM counterparts, as they tend to produce\ninadequate responses and generate false information. As a solution for\naddressing the current issue, this paper proposes the Visual Instruction\nGeneration and Correction (VIGC) framework that enables multimodal large\nlanguage models to generate instruction-tuning data and progressively enhance\nits quality on-the-fly. Specifically, Visual Instruction Generation (VIG)\nguides the vision-language model to generate diverse instruction-tuning data.\nTo ensure generation quality, Visual Instruction Correction (VIC) adopts an\niterative update mechanism to correct any inaccuracies in data produced by VIG,\neffectively reducing the risk of hallucination. Leveraging the diverse,\nhigh-quality data generated by VIGC, we finetune mainstream models and validate\ndata quality based on various evaluations. Experimental results demonstrate\nthat VIGC not only compensates for the shortcomings of language-only data\ngeneration methods, but also effectively enhances the benchmark performance.\nThe models, datasets, and code are available at\nhttps://opendatalab.github.io/VIGC.",
        "pdf_link": "https://arxiv.org/pdf/2308.12714v3.pdf"
    },
    {
        "title": "Improving Translation Faithfulness of Large Language Models via Augmenting Instructions",
        "authors": [
            "Yijie Chen",
            "Yijin Liu",
            "Fandong Meng",
            "Yufeng Chen",
            "Jinan Xu",
            "Jie Zhou"
        ],
        "published": "2023-08-24T09:32:29Z",
        "summary": "Large Language Models (LLMs) present strong general capabilities, and a\ncurrent compelling challenge is stimulating their specialized capabilities,\nsuch as machine translation, through low-cost instruction tuning. The standard\ninstruction-following data is sequentially organized as the concatenation of an\ninstruction, an input, and a response. As the attention mechanism of LLMs has\nlimitations on local focus, LLMs tend to focus more on the words or sentences\nnearby at each position. This leads to a high risk of instruction forgetting\nduring decoding. To alleviate the above issues, We propose SWIE\n(Segment-Weighted Instruction Embedding) and an instruction-following dataset\nOVERMISS. SWIE improves the model instruction understanding by adding a global\ninstruction representation on the following input and response representations.\nOVERMISS improves model faithfulness by comparing over-translation and\nmiss-translation results with the correct translation. We apply our methods to\ntwo main-stream open-source LLMs, BLOOM and LLaMA. The experimental results\ndemonstrate significant improvements in translation performance with SWIE based\non BLOOMZ-3b, particularly in zero-shot and long text translations due to\nreduced instruction forgetting risk. Additionally, OVERMISS outperforms the\nbaseline in translation performance (e.g. an increase in BLEU scores from 0.69\nto 3.12 and an average improvement of 0.48 percentage comet scores for\nLLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE\n(e.g. the BLUE scores increase up to 0.56 from English to German across three\ndifferent backbones), and both exhibit improvements in the faithfulness metric\nbased on word alignment.",
        "pdf_link": "https://arxiv.org/pdf/2308.12674v1.pdf"
    },
    {
        "title": "Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs",
        "authors": [
            "Ye Liu",
            "Semih Yavuz",
            "Rui Meng",
            "Meghana Moorthy",
            "Shafiq Joty",
            "Caiming Xiong",
            "Yingbo Zhou"
        ],
        "published": "2023-08-24T05:26:54Z",
        "summary": "The integration of retrieved passages and large language models (LLMs), such\nas ChatGPTs, has significantly contributed to improving open-domain question\nanswering. However, there is still a lack of exploration regarding the optimal\napproach for incorporating retrieved passages into the answer generation\nprocess. This paper aims to fill this gap by investigating different methods of\ncombining retrieved passages with LLMs to enhance answer generation. We begin\nby examining the limitations of a commonly-used concatenation approach.\nSurprisingly, this approach often results in generating \"unknown\" outputs, even\nwhen the correct document is among the top-k retrieved passages. To address\nthis issue, we explore four alternative strategies for integrating the\nretrieved passages with the LLMs. These strategies include two single-round\nmethods that utilize chain-of-thought reasoning and two multi-round strategies\nthat incorporate feedback loops. Through comprehensive analyses and\nexperiments, we provide insightful observations on how to effectively leverage\nretrieved passages to enhance the answer generation capability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.12574v2.pdf"
    },
    {
        "title": "Benchmarking Causal Study to Interpret Large Language Models for Source Code",
        "authors": [
            "Daniel Rodriguez-Cardenas",
            "David N. Palacio",
            "Dipin Khati",
            "Henry Burke",
            "Denys Poshyvanyk"
        ],
        "published": "2023-08-23T20:32:12Z",
        "summary": "One of the most common solutions adopted by software researchers to address\ncode generation is by training Large Language Models (LLMs) on massive amounts\nof source code. Although a number of studies have shown that LLMs have been\neffectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu),\nprevious research has largely overlooked the role of Causal Inference as a\nfundamental component of the interpretability of LLMs' performance. Existing\nbenchmarks and datasets are meant to highlight the difference between the\nexpected and the generated outcome, but do not take into account confounding\nvariables (e.g., lines of code, prompt size) that equally influence the\naccuracy metrics. The fact remains that, when dealing with generative software\ntasks by LLMs, no benchmark is available to tell researchers how to quantify\nneither the causal effect of SE-based treatments nor the correlation of\nconfounders to the model's performance. In an effort to bring statistical rigor\nto the evaluation of LLMs, this paper introduces a benchmarking strategy named\nGaleras comprised of curated testbeds for three SE tasks (i.e., code\ncompletion, code summarization, and commit generation) to help aid the\ninterpretation of LLMs' performance. We illustrate the insights of our\nbenchmarking strategy by conducting a case study on the performance of ChatGPT\nunder distinct prompt engineering methods. The results of the case study\ndemonstrate the positive causal influence of prompt semantics on ChatGPT's\ngenerative performance by an average treatment effect of $\\approx 3\\%$.\nMoreover, it was found that confounders such as prompt size are highly\ncorrelated with accuracy metrics ($\\approx 0.412\\%$). The end result of our\ncase study is to showcase causal inference evaluations, in practice, to reduce\nconfounding bias. By reducing the bias, we offer an interpretable solution for\nthe accuracy metric under analysis.",
        "pdf_link": "https://arxiv.org/pdf/2308.12415v1.pdf"
    },
    {
        "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification",
        "authors": [
            "Kushal Tirumala",
            "Daniel Simig",
            "Armen Aghajanyan",
            "Ari S. Morcos"
        ],
        "published": "2023-08-23T17:58:14Z",
        "summary": "Over recent years, an increasing amount of compute and data has been poured\ninto training large language models (LLMs), usually by doing one-pass learning\non as many tokens as possible randomly selected from large-scale web corpora.\nWhile training on ever-larger portions of the internet leads to consistent\nperformance improvements, the size of these improvements diminishes with scale,\nand there has been little work exploring the effect of data selection on\npre-training and downstream performance beyond simple de-duplication methods\nsuch as MinHash. Here, we show that careful data selection (on top of\nde-duplicated data) via pre-trained model embeddings can speed up training (20%\nefficiency gains) and improves average downstream accuracy on 16 NLP tasks (up\nto 2%) at the 6.7B model scale. Furthermore, we show that repeating data\nintelligently consistently outperforms baseline training (while repeating\nrandom data performs worse than baseline training). Our results indicate that\nclever data selection can significantly improve LLM pre-training, calls into\nquestion the common practice of training for a single epoch on as much data as\npossible, and demonstrates a path to keep improving our models past the limits\nof randomly sampling web data.",
        "pdf_link": "https://arxiv.org/pdf/2308.12284v1.pdf"
    },
    {
        "title": "Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models",
        "authors": [
            "Nancy Tyagi",
            "Aidin Shiri",
            "Surjodeep Sarkar",
            "Abhishek Kumar Umrawal",
            "Manas Gaur"
        ],
        "published": "2023-08-23T17:40:35Z",
        "summary": "Foundational Language Models (FLMs) have advanced natural language processing\n(NLP) research. Current researchers are developing larger FLMs (e.g., XLNet,\nT5) to enable contextualized language representation, classification, and\ngeneration. While developing larger FLMs has been of significant advantage, it\nis also a liability concerning hallucination and predictive uncertainty.\nFundamentally, larger FLMs are built on the same foundations as smaller FLMs\n(e.g., BERT); hence, one must recognize the potential of smaller FLMs which can\nbe realized through an ensemble. In the current research, we perform a reality\ncheck on FLMs and their ensemble on benchmark and real-world datasets. We\nhypothesize that the ensembling of FLMs can influence the individualistic\nattention of FLMs and unravel the strength of coordination and cooperation of\ndifferent FLMs. We utilize BERT and define three other ensemble techniques:\n{Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a\nknowledge-guided reinforcement learning approach. We discovered that the\nsuggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by\na factor of many times using datasets that show the usefulness of NLP in\nsensitive fields, such as mental health.",
        "pdf_link": "https://arxiv.org/pdf/2308.12272v1.pdf"
    },
    {
        "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
        "authors": [
            "Vijay Viswanathan",
            "Chenyang Zhao",
            "Amanda Bertsch",
            "Tongshuang Wu",
            "Graham Neubig"
        ],
        "published": "2023-08-23T17:28:21Z",
        "summary": "Large language models (LLMs) enable system builders today to create competent\nNLP systems through prompting, where they only need to describe the task in\nnatural language and provide a few examples. However, in other ways, LLMs are a\nstep backward from traditional special-purpose NLP models; they require\nextensive computational resources for deployment and can be gated behind APIs.\nIn this paper, we propose Prompt2Model, a general-purpose method that takes a\nnatural language task description like the prompts provided to LLMs, and uses\nit to train a special-purpose model that is conducive to deployment. This is\ndone through a multi-step process of retrieval of existing datasets and\npretrained models, dataset generation using LLMs, and supervised fine-tuning on\nthese retrieved and generated datasets. Over three tasks, we demonstrate that\ngiven the same few-shot prompt as input, Prompt2Model trains models that\noutperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%\nwhile being up to 700 times smaller. We also show that this data can be used to\nobtain reliable performance estimates of model performance, enabling model\ndevelopers to assess model reliability before deployment. Prompt2Model is\navailable open-source at https://github.com/neulab/prompt2model.",
        "pdf_link": "https://arxiv.org/pdf/2308.12261v1.pdf"
    },
    {
        "title": "Evaluation of Faithfulness Using the Longest Supported Subsequence",
        "authors": [
            "Anirudh Mittal",
            "Timo Schick",
            "Mikel Artetxe",
            "Jane Dwivedi-Yu"
        ],
        "published": "2023-08-23T14:18:44Z",
        "summary": "As increasingly sophisticated language models emerge, their trustworthiness\nbecomes a pivotal issue, especially in tasks such as summarization and\nquestion-answering. Ensuring their responses are contextually grounded and\nfaithful is challenging due to the linguistic diversity and the myriad of\npossible answers. In this paper, we introduce a novel approach to evaluate\nfaithfulness of machine-generated text by computing the longest noncontinuous\nsubstring of the claim that is supported by the context, which we refer to as\nthe Longest Supported Subsequence (LSS). Using a new human-annotated dataset,\nwe finetune a model to generate LSS. We introduce a new method of evaluation\nand demonstrate that these metrics correlate better with human ratings when LSS\nis employed, as opposed to when it is not. Our proposed metric demonstrates an\n18% enhancement over the prevailing state-of-the-art metric for faithfulness on\nour dataset. Our metric consistently outperforms other metrics on a\nsummarization dataset across six different models. Finally, we compare several\npopular Large Language Models (LLMs) for faithfulness using this metric. We\nrelease the human-annotated dataset built for predicting LSS and our fine-tuned\nmodel for evaluating faithfulness.",
        "pdf_link": "https://arxiv.org/pdf/2308.12157v1.pdf"
    },
    {
        "title": "PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine",
        "authors": [
            "Chenrui Zhang",
            "Lin Liu",
            "Jinpeng Wang",
            "Chuyuan Wang",
            "Xiao Sun",
            "Hongyu Wang",
            "Mingchen Cai"
        ],
        "published": "2023-08-23T09:46:37Z",
        "summary": "As an effective tool for eliciting the power of Large Language Models (LLMs),\nprompting has recently demonstrated unprecedented abilities across a variety of\ncomplex tasks. To further improve the performance, prompt ensemble has\nattracted substantial interest for tackling the hallucination and instability\nof LLMs. However, existing methods usually adopt a two-stage paradigm, which\nrequires a pre-prepared set of prompts with substantial manual effort, and is\nunable to perform directed optimization for different weak learners. In this\npaper, we propose a simple, universal, and automatic method named PREFER (Pompt\nEnsemble learning via Feedback-Reflect-Refine) to address the stated\nlimitations. Specifically, given the fact that weak learners are supposed to\nfocus on hard examples during boosting, PREFER builds a feedback mechanism for\nreflecting on the inadequacies of existing weak learners. Based on this, the\nLLM is required to automatically synthesize new prompts for iterative\nrefinement. Moreover, to enhance stability of the prompt effect evaluation, we\npropose a novel prompt bagging method involving forward and backward thinking,\nwhich is superior to majority voting and is beneficial for both feedback and\nweight calculation in boosting. Extensive experiments demonstrate that our\nPREFER achieves state-of-the-art performance in multiple types of tasks by a\nsignificant margin. We have made our code publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2308.12033v1.pdf"
    },
    {
        "title": "From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models",
        "authors": [
            "Jing Yao",
            "Xiaoyuan Yi",
            "Xiting Wang",
            "Jindong Wang",
            "Xing Xie"
        ],
        "published": "2023-08-23T09:11:13Z",
        "summary": "Big models, exemplified by Large Language Models (LLMs), are models typically\npre-trained on massive data and comprised of enormous parameters, which not\nonly obtain significantly improved performance across diverse tasks but also\npresent emergent capabilities absent in smaller models. However, the growing\nintertwining of big models with everyday human lives poses potential risks and\nmight cause serious social harm. Therefore, many efforts have been made to\nalign LLMs with humans to make them better follow user instructions and satisfy\nhuman preferences. Nevertheless, `what to align with' has not been fully\ndiscussed, and inappropriate alignment goals might even backfire. In this\npaper, we conduct a comprehensive survey of different alignment goals in\nexisting work and trace their evolution paths to help identify the most\nessential goal. Particularly, we investigate related works from two\nperspectives: the definition of alignment goals and alignment evaluation. Our\nanalysis encompasses three distinct levels of alignment goals and reveals a\ngoal transformation from fundamental abilities to value orientation, indicating\nthe potential of intrinsic human values as the alignment goal for enhanced\nLLMs. Based on such results, we further discuss the challenges of achieving\nsuch intrinsic value alignment and provide a collection of available resources\nfor future research on the alignment of big models.",
        "pdf_link": "https://arxiv.org/pdf/2308.12014v2.pdf"
    },
    {
        "title": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs",
        "authors": [
            "Ziyi Tang",
            "Ruilin Wang",
            "Weixing Chen",
            "Keze Wang",
            "Yang Liu",
            "Tianshui Chen",
            "Liang Lin"
        ],
        "published": "2023-08-23T04:59:21Z",
        "summary": "Despite advancements in LLMs, knowledge-based reasoning remains a\nlongstanding issue due to the fragility of knowledge recall and inference.\nExisting methods primarily encourage LLMs to autonomously plan and solve\nproblems or to extensively sample reasoning chains without addressing the\nconceptual and inferential fallacies. Attempting to alleviate inferential\nfallacies and drawing inspiration from multi-agent collaboration, we present a\nframework to increase faithfulness and causality for knowledge-based reasoning.\nSpecifically, we propose to employ multiple intelligent agents (i.e., reasoners\nand an evaluator) to work collaboratively in a reasoning-and-consensus paradigm\nfor elevated reasoning faithfulness. The reasoners focus on providing solutions\nwith human-like causality to solve open-domain problems. On the other hand, the\n\\textit{evaluator} agent scrutinizes if a solution is deducible from a\nnon-causal perspective and if it still holds when challenged by a\ncounterfactual candidate. According to the extensive and comprehensive\nevaluations on a variety of knowledge reasoning tasks (e.g., science question\nanswering and commonsense reasoning), our framework outperforms all compared\nstate-of-the-art approaches by large margins.",
        "pdf_link": "https://arxiv.org/pdf/2308.11914v2.pdf"
    },
    {
        "title": "Dcc --help: Generating Context-Aware Compiler Error Explanations with Large Language Models",
        "authors": [
            "Andrew Taylor",
            "Alexandra Vassar",
            "Jake Renzella",
            "Hammond Pearce"
        ],
        "published": "2023-08-23T02:36:19Z",
        "summary": "In the challenging field of introductory programming, high enrollments and\nfailure rates drive us to explore tools and systems to enhance student\noutcomes, especially automated tools that scale to large cohorts. This paper\npresents and evaluates the dcc --help tool, an integration of a Large Language\nModel (LLM) into the Debugging C Compiler (DCC) to generate unique,\nnovice-focused explanations tailored to each error. dcc --help prompts an LLM\nwith contextual information of compile- and run-time error occurrences,\nincluding the source code, error location and standard compiler error message.\nThe LLM is instructed to generate novice-focused, actionable error explanations\nand guidance, designed to help students understand and resolve problems without\nproviding solutions. dcc --help was deployed to our CS1 and CS2 courses, with\n2,565 students using the tool over 64,000 times in ten weeks. We analysed a\nsubset of these error/explanation pairs to evaluate their properties, including\nconceptual correctness, relevancy, and overall quality. We found that the\nLLM-generated explanations were conceptually accurate in 90% of compile-time\nand 75% of run-time cases, but often disregarded the instruction not to provide\nsolutions in code. Our findings, observations and reflections following\ndeployment indicate that dcc-help provides novel opportunities for scaffolding\nstudents' introduction to programming.",
        "pdf_link": "https://arxiv.org/pdf/2308.11873v2.pdf"
    },
    {
        "title": "Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test",
        "authors": [
            "Saba Rahimi",
            "Tucker Balch",
            "Manuela Veloso"
        ],
        "published": "2023-08-22T23:18:53Z",
        "summary": "Large language models such as Open AI's Generative Pre-trained Transformer\n(GPT) models are proficient at answering questions, but their knowledge is\nconfined to the information present in their training data. This limitation\nrenders them ineffective when confronted with questions about recent\ndevelopments or non-public documents. Our research proposes a method that\nenables GPT models to answer questions by employing context from an information\nsource not previously included in their training data. The methodology includes\npreprocessing of contextual information, the embedding of contexts and queries,\nconstructing prompt through the integration of context embeddings, and\ngenerating answers using GPT models. We applied this method in a controlled\ntest scenario using the California Driver's Handbook as the information source.\nThe GPT-3 model achieved a 96% passing score on a set of 50 sample driving\nknowledge test questions. In contrast, without context, the model's passing\nscore fell to 82%. However, the model still fails to answer some questions\ncorrectly even with providing library of context, highlighting room for\nimprovement. The research also examined the impact of prompt length and context\nformat, on the model's performance. Overall, the study provides insights into\nthe limitations and potential improvements for GPT models in question-answering\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2308.11827v1.pdf"
    },
    {
        "title": "Towards an On-device Agent for Text Rewriting",
        "authors": [
            "Yun Zhu",
            "Yinxiao Liu",
            "Felix Stahlberg",
            "Shankar Kumar",
            "Yu-hui Chen",
            "Liangchen Luo",
            "Lei Shu",
            "Renjie Liu",
            "Jindong Chen",
            "Lei Meng"
        ],
        "published": "2023-08-22T22:18:38Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities for\ntext rewriting. Nonetheless, the large sizes of these models make them\nimpractical for on-device inference, which would otherwise allow for enhanced\nprivacy and economical inference. Creating a smaller yet potent language model\nfor text rewriting presents a formidable challenge because it requires\nbalancing the need for a small size with the need to retain the emergent\ncapabilities of the LLM, that requires costly data collection. To address the\nabove challenge, we introduce a new instruction tuning approach for building a\nmobile-centric text rewriting model. Our strategies enable the generation of\nhigh quality training data without any human labeling. In addition, we propose\na heuristic reinforcement learning framework which substantially enhances\nperformance without requiring preference data. To further bridge the\nperformance gap with the larger server-side model, we propose an effective\napproach that combines the mobile rewrite agent with the server model using a\ncascade. To tailor the text rewriting tasks to mobile scenarios, we introduce\nMessageRewriteEval, a benchmark that focuses on text rewriting for messages\nthrough natural language instructions. Through empirical experiments, we\ndemonstrate that our on-device model surpasses the current state-of-the-art\nLLMs in text rewriting while maintaining a significantly reduced model size.\nNotably, we show that our proposed cascading approach improves model\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2308.11807v1.pdf"
    },
    {
        "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models",
        "authors": [
            "Mohamed Elaraby",
            "Mengyin Lu",
            "Jacob Dunn",
            "Xueying Zhang",
            "Yu Wang",
            "Shizhu Liu",
            "Pingchuan Tian",
            "Yuping Wang",
            "Yuxuan Wang"
        ],
        "published": "2023-08-22T20:12:49Z",
        "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP). Although convenient for research and practical applications, open-source\nLLMs with fewer parameters often suffer from severe hallucinations compared to\ntheir larger counterparts. This paper focuses on measuring and reducing\nhallucinations in BLOOM 7B, a representative of such weaker open-source LLMs\nthat are publicly available for research and commercial applications. We\nintroduce HaloCheck, a lightweight BlackBox knowledge-free framework designed\nto quantify the severity of hallucinations in LLMs. Additionally, we explore\ntechniques like knowledge injection and teacher-student approaches to alleviate\nhallucinations in low-parameter LLMs. Our experiments effectively demonstrate\nthe reduction of hallucinations in challenging domains for these LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.11764v4.pdf"
    },
    {
        "title": "Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation",
        "authors": [
            "Yifei Su",
            "Dong An",
            "Yuan Xu",
            "Kehan Chen",
            "Yan Huang"
        ],
        "published": "2023-08-22T16:45:35Z",
        "summary": "This report details the methods of the winning entry of the AVDN Challenge in\nICCV CLVL 2023. The competition addresses the Aerial Navigation from Dialog\nHistory (ANDH) task, which requires a drone agent to associate dialog history\nwith aerial observations to reach the destination. For better cross-modal\ngrounding abilities of the drone agent, we propose a Target-Grounded\nGraph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leverages\na graph-aware transformer to capture spatiotemporal dependency, which benefits\nnavigation state tracking and robust action planning. In addition,an auxiliary\nvisual grounding task is devised to boost the agent's awareness of referred\nlandmarks. Moreover, a hybrid augmentation strategy based on large language\nmodels is utilized to mitigate data scarcity limitations. Our TG-GAT framework\nwon the AVDN Challenge, with 2.2% and 3.0% absolute improvements over the\nbaseline on SPL and SR metrics, respectively. The code is available at\nhttps://github.com/yifeisu/TG-GAT.",
        "pdf_link": "https://arxiv.org/pdf/2308.11561v5.pdf"
    },
    {
        "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
        "authors": [
            "Pouya Pezeshkpour",
            "Estevam Hruschka"
        ],
        "published": "2023-08-22T14:54:59Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious NLP tasks. However, previous works have shown these models are\nsensitive towards prompt wording, and few-shot demonstrations and their order,\nposing challenges to fair assessment of these models. As these models become\nmore powerful, it becomes imperative to understand and address these\nlimitations. In this paper, we focus on LLMs robustness on the task of\nmultiple-choice questions -- commonly adopted task to study reasoning and\nfact-retrieving capability of LLMs. Investigating the sensitivity of LLMs\ntowards the order of options in multiple-choice questions, we demonstrate a\nconsiderable performance gap of approximately 13% to 75% in LLMs on different\nbenchmarks, when answer options are reordered, even when using demonstrations\nin a few-shot setting. Through a detailed analysis, we conjecture that this\nsensitivity arises when LLMs are uncertain about the prediction between the\ntop-2/3 choices, and specific options placements may favor certain prediction\nbetween those top choices depending on the question caused by positional bias.\nWe also identify patterns in top-2 choices that amplify or mitigate the model's\nbias toward option placement. We found that for amplifying bias, the optimal\nstrategy involves positioning the top two choices as the first and last\noptions. Conversely, to mitigate bias, we recommend placing these choices among\nthe adjacent options. To validate our conjecture, we conduct various\nexperiments and adopt two approaches to calibrate LLMs' predictions, leading to\nup to 8 percentage points improvement across different models and benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2308.11483v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis",
        "authors": [
            "Chang Liu",
            "Bo Wu"
        ],
        "published": "2023-08-22T06:32:07Z",
        "summary": "Large Language Models (LLMs) have garnered considerable interest within both\nacademic and industrial. Yet, the application of LLMs to graph data remains\nunder-explored. In this study, we evaluate the capabilities of four LLMs in\naddressing several analytical problems with graph data. We employ four distinct\nevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.\nOur results show that: 1) LLMs effectively comprehend graph data in natural\nlanguage and reason with graph topology. 2) GPT models can generate logical and\ncoherent results, outperforming alternatives in correctness. 3) All examined\nLLMs face challenges in structural reasoning, with techniques like zero-shot\nchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT\nmodels often produce erroneous answers in multi-answer tasks, raising concerns\nin fidelity. 5) GPT models exhibit elevated confidence in their outputs,\npotentially hindering their rectification capacities. Notably, GPT-4 has\ndemonstrated the capacity to rectify responses from GPT-3.5-turbo and its own\nprevious iterations. The code is available at:\nhttps://github.com/Ayame1006/LLMtoGraph.",
        "pdf_link": "https://arxiv.org/pdf/2308.11224v2.pdf"
    },
    {
        "title": "LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning",
        "authors": [
            "Junyi Lu",
            "Lei Yu",
            "Xiaojia Li",
            "Li Yang",
            "Chun Zuo"
        ],
        "published": "2023-08-22T03:10:40Z",
        "summary": "The automation of code review activities, a long-standing pursuit in software\nengineering, has been primarily addressed by numerous domain-specific\npre-trained models. Despite their success, these models frequently demand\nextensive resources for pre-training from scratch. In contrast, Large Language\nModels (LLMs) provide an intriguing alternative, given their remarkable\ncapabilities when supplemented with domain-specific knowledge. However, their\npotential for automating code review tasks remains largely unexplored.\n  In response to this research gap, we present LLaMA-Reviewer, an innovative\nframework that leverages the capabilities of LLaMA, a popular LLM, in the realm\nof code review. Mindful of resource constraints, this framework employs\nparameter-efficient fine-tuning (PEFT) methods, delivering high performance\nwhile using less than 1% of trainable parameters.\n  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse,\npublicly available datasets. Notably, even with the smallest LLaMA base model\nconsisting of 6.7B parameters and a limited number of tuning epochs,\nLLaMA-Reviewer equals the performance of existing code-review-focused models.\n  The ablation experiments provide insights into the influence of various\nfine-tuning process components, including input representation, instruction\ntuning, and different PEFT methods. To foster continuous progress in this\nfield, the code and all PEFT-weight plugins have been made open-source.",
        "pdf_link": "https://arxiv.org/pdf/2308.11148v2.pdf"
    },
    {
        "title": "Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models",
        "authors": [
            "Alex Nyffenegger",
            "Matthias St\u00fcrmer",
            "Joel Niklaus"
        ],
        "published": "2023-08-22T00:57:36Z",
        "summary": "Anonymity of both natural and legal persons in court rulings is a critical\naspect of privacy protection in the European Union and Switzerland. With the\nadvent of LLMs, concerns about large-scale re-identification of anonymized\npersons are growing. In accordance with the Federal Supreme Court of\nSwitzerland, we explore the potential of LLMs to re-identify individuals in\ncourt rulings by constructing a proof-of-concept using actual legal data from\nthe Swiss federal supreme court. Following the initial experiment, we\nconstructed an anonymized Wikipedia dataset as a more rigorous testing ground\nto further investigate the findings. With the introduction and application of\nthe new task of re-identifying people in texts, we also introduce new metrics\nto measure performance. We systematically analyze the factors that influence\nsuccessful re-identifications, identifying model size, input length, and\ninstruction tuning among the most critical determinants. Despite high\nre-identification rates on Wikipedia, even the best LLMs struggled with court\ndecisions. The complexity is attributed to the lack of test datasets, the\nnecessity for substantial training resources, and data sparsity in the\ninformation used for re-identification. In conclusion, this study demonstrates\nthat re-identification using LLMs may not be feasible for now, but as the\nproof-of-concept on Wikipedia showed, it might become possible in the future.\nWe hope that our system can help enhance the confidence in the security of\nanonymized decisions, thus leading to the courts being more confident to\npublish decisions.",
        "pdf_link": "https://arxiv.org/pdf/2308.11103v1.pdf"
    },
    {
        "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs",
        "authors": [
            "Arka Pal",
            "Deep Karkhanis",
            "Manley Roberts",
            "Samuel Dooley",
            "Arvind Sundararajan",
            "Siddartha Naidu"
        ],
        "published": "2023-08-21T17:30:16Z",
        "summary": "Modern large language models (LLMs) that rely on attention mechanisms are\ntypically trained with fixed context lengths which enforce upper limits on the\nlength of input sequences that they can handle at evaluation time. To use these\nmodels on sequences longer than the train-time context length, one might employ\ntechniques from the growing family of context length extrapolation methods --\nmost of which focus on modifying the system of positional encodings used in the\nattention mechanism to indicate where tokens or activations are located in the\ninput sequence. We conduct a wide survey of existing methods of context length\nextrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own\ndesign as well -- in particular, a new truncation strategy for modifying the\nbasis for the position encoding.\n  We test these methods using three new evaluation tasks (FreeFormQA,\nAlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to\nbe less fine-grained as a measure of long context performance of LLMs. We\nrelease the three tasks publicly as datasets on HuggingFace. We discover that\nlinear scaling is the best method for extending context length, and show that\nfurther gains can be achieved by using longer scales at evaluation time. We\nalso discover promising extrapolation capabilities in the truncated basis. To\nsupport further research in this area, we release three new 13B parameter\nlong-context models which we call Giraffe: 4k and 16k context models trained\nfrom base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We\nalso release the code to replicate our results.",
        "pdf_link": "https://arxiv.org/pdf/2308.10882v1.pdf"
    },
    {
        "title": "Unreflected Acceptance -- Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education",
        "authors": [
            "Lars Krupp",
            "Steffen Steinert",
            "Maximilian Kiefer-Emmanouilidis",
            "Karina E. Avila",
            "Paul Lukowicz",
            "Jochen Kuhn",
            "Stefan K\u00fcchemann",
            "Jakob Karolus"
        ],
        "published": "2023-08-21T16:14:34Z",
        "summary": "Large language models (LLMs) have recently gained popularity. However, the\nimpact of their general availability through ChatGPT on sensitive areas of\neveryday life, such as education, remains unclear. Nevertheless, the societal\nimpact on established educational methods is already being experienced by both\nstudents and educators. Our work focuses on higher physics education and\nexamines problem solving strategies. In a study, students with a background in\nphysics were assigned to solve physics exercises, with one group having access\nto an internet search engine (N=12) and the other group being allowed to use\nChatGPT (N=27). We evaluated their performance, strategies, and interaction\nwith the provided tools. Our results showed that nearly half of the solutions\nprovided with the support of ChatGPT were mistakenly assumed to be correct by\nthe students, indicating that they overly trusted ChatGPT even in their field\nof expertise. Likewise, in 42% of cases, students used copy & paste to query\nChatGPT -- an approach only used in 4% of search engine queries -- highlighting\nthe stark differences in interaction behavior between the groups and indicating\nlimited reflection when using ChatGPT. In our work, we demonstrated a need to\n(1) guide students on how to interact with LLMs and (2) create awareness of\npotential shortcomings for users.",
        "pdf_link": "https://arxiv.org/pdf/2309.03087v1.pdf"
    },
    {
        "title": "Fact-checking information generated by a large language model can decrease news discernment",
        "authors": [
            "Matthew R. DeVerna",
            "Harry Yaojun Yan",
            "Kai-Cheng Yang",
            "Filippo Menczer"
        ],
        "published": "2023-08-21T15:47:37Z",
        "summary": "Fact checking can be an effective strategy against misinformation, but its\nimplementation at scale is impeded by the overwhelming volume of information\nonline. Recent artificial intelligence (AI) language models have shown\nimpressive ability in fact-checking tasks, but how humans interact with\nfact-checking information provided by these models is unclear. Here, we\ninvestigate the impact of fact-checking information generated by a popular\nlarge language model (LLM) on belief in, and sharing intent of, political news\nin a preregistered randomized control experiment. Although the LLM performs\nreasonably well in debunking false headlines, we find that it does not\nsignificantly affect participants' ability to discern headline accuracy or\nshare accurate news. Subsequent analysis reveals that the AI fact-checker is\nharmful in specific cases: it decreases beliefs in true headlines that it\nmislabels as false and increases beliefs in false headlines that it is unsure\nabout. On the positive side, the AI fact-checking information increases sharing\nintents for correctly labeled true headlines. When participants are given the\noption to view LLM fact checks and choose to do so, they are significantly more\nlikely to share both true and false news but only more likely to believe false\nnews. Our findings highlight an important source of potential harm stemming\nfrom AI applications and underscore the critical need for policies to prevent\nor mitigate such unintended consequences.",
        "pdf_link": "https://arxiv.org/pdf/2308.10800v3.pdf"
    },
    {
        "title": "SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding",
        "authors": [
            "Tianyu Yu",
            "Chengyue Jiang",
            "Chao Lou",
            "Shen Huang",
            "Xiaobin Wang",
            "Wei Liu",
            "Jiong Cai",
            "Yangning Li",
            "Yinghui Li",
            "Kewei Tu",
            "Hai-Tao Zheng",
            "Ningyu Zhang",
            "Pengjun Xie",
            "Fei Huang",
            "Yong Jiang"
        ],
        "published": "2023-08-21T07:31:19Z",
        "summary": "Large language models (LLMs) have shown impressive ability for open-domain\nNLP tasks. However, LLMs are sometimes too footloose for natural language\nunderstanding (NLU) tasks which always have restricted output and input format.\nTheir performances on NLU tasks are highly related to prompts or demonstrations\nand are shown to be poor at performing several representative NLU tasks, such\nas event extraction and entity typing. To this end, we present SeqGPT, a\nbilingual (i.e., English and Chinese) open-source autoregressive model\nspecially enhanced for open-domain natural language understanding. We express\nall NLU tasks with two atomic tasks, which define fixed instructions to\nrestrict the input and output format but still ``open'' for arbitrarily varied\nlabel sets. The model is first instruction-tuned with extremely fine-grained\nlabeled data synthesized by ChatGPT and then further fine-tuned by 233\ndifferent atomic tasks from 152 datasets across various domains. The\nexperimental results show that SeqGPT has decent classification and extraction\nability, and is capable of performing language understanding tasks on unseen\ndomains. We also conduct empirical studies on the scaling of data and model\nsize as well as on the transfer across tasks. Our model is accessible at\nhttps://github.com/Alibaba-NLP/SeqGPT.",
        "pdf_link": "https://arxiv.org/pdf/2308.10529v1.pdf"
    },
    {
        "title": "Dataset Quantization",
        "authors": [
            "Daquan Zhou",
            "Kai Wang",
            "Jianyang Gu",
            "Xiangyu Peng",
            "Dongze Lian",
            "Yifan Zhang",
            "Yang You",
            "Jiashi Feng"
        ],
        "published": "2023-08-21T07:24:29Z",
        "summary": "State-of-the-art deep neural networks are trained with large amounts\n(millions or even billions) of data. The expensive computation and memory costs\nmake it difficult to train them on limited hardware resources, especially for\nrecent popular large language models (LLM) and computer vision models (CV).\nRecent popular dataset distillation methods are thus developed, aiming to\nreduce the number of training samples via synthesizing small-scale datasets via\ngradient matching. However, as the gradient calculation is coupled with the\nspecific network architecture, the synthesized dataset is biased and performs\npoorly when used for training unseen architectures. To address these\nlimitations, we present dataset quantization (DQ), a new framework to compress\nlarge-scale datasets into small subsets which can be used for training any\nneural network architectures. Extensive experiments demonstrate that DQ is able\nto generate condensed small datasets for training unseen network architectures\nwith state-of-the-art compression ratios for lossless model training. To the\nbest of our knowledge, DQ is the first method that can successfully distill\nlarge-scale datasets such as ImageNet-1k with a state-of-the-art compression\nratio. Notably, with 60% data from ImageNet and 20% data from Alpaca's\ninstruction tuning data, the models can be trained with negligible or no\nperformance drop for both vision tasks (including classification, semantic\nsegmentation, and object detection) as well as language tasks (including\ninstruction tuning tasks such as BBH and DROP).",
        "pdf_link": "https://arxiv.org/pdf/2308.10524v1.pdf"
    },
    {
        "title": "An Examination of the Compositionality of Large Generative Vision-Language Models",
        "authors": [
            "Teli Ma",
            "Rong Li",
            "Junwei Liang"
        ],
        "published": "2023-08-21T06:50:29Z",
        "summary": "With the success of Large Language Models (LLMs), many Generative\nVision-Language Models (GVLMs) have been constructed via multimodal instruction\ntuning. However, the performance of GVLMs in multimodal compositional reasoning\nremains under-explored. In this paper, we examine both the evaluation metrics\n(VisualGPTScore, etc.) and current benchmarks for evaluating the\ncompositionality of GVLMs. We identify the syntactical bias in current\nbenchmarks, which is exploited by the linguistic capability of GVLMs. The bias\nrenders VisualGPTScore an insufficient metric for assessing GVLMs. To combat\nthis, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such\nbias for mitigation. A challenging new task is subsequently added to evaluate\nthe robustness of GVLMs against inherent inclination toward syntactical\ncorrectness. Using the bias-mitigated datasets and the new task, we propose a\nnovel benchmark, namely SyntActically DE-biased benchmark (SADE). Our study\nprovides an unbiased benchmark for the compositionality of GVLMs, facilitating\nfuture research in this direction (Code and dataset are available at\nhttps://github.com/TeleeMa/SADE).",
        "pdf_link": "https://arxiv.org/pdf/2308.10509v2.pdf"
    },
    {
        "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models",
        "authors": [
            "Martin Weyssow",
            "Xin Zhou",
            "Kisub Kim",
            "David Lo",
            "Houari Sahraoui"
        ],
        "published": "2023-08-21T04:31:06Z",
        "summary": "Large Language Models (LLMs) demonstrate impressive capabilities to generate\naccurate code snippets given natural language intents in zero-shot, i.e.,\nwithout the need for specific fine-tuning. While prior studies have highlighted\nthe advantages of fine-tuning LLMs, this process incurs high computational\ncosts, making it impractical in resource-scarce environments, particularly for\nmodels with billions of parameters. To address these challenges, previous\nresearch explored In-Context Learning (ICL) as a strategy to guide the LLM\ngenerative process with task-specific prompt examples. However, ICL introduces\ninconveniences, such as the need for designing contextually relevant prompts\nand the absence of learning task-specific parameters, thereby limiting\ndownstream task performance. In this context, we foresee Parameter-Efficient\nFine-Tuning (PEFT) techniques as a promising approach to efficiently specialize\nLLMs to task-specific data while maintaining reasonable resource consumption.\nIn this paper, we deliver a comprehensive study of PEFT techniques for LLMs\nunder the automated code generation scenario. Our comprehensive investigation\nof PEFT techniques for LLMs reveals their superiority and potential over ICL\nacross a diverse set of LLMs. Additionally, we demonstrate the extended\ncapabilities of PEFT, showcasing its ability to learn from two distinct\ndatasets jointly without compromising performance. Furthermore, our study\nhighlights the potential for tuning larger LLMs and significant reductions in\nmemory usage by combining PEFT with quantization. Therefore, this study opens\nopportunities for broader applications of PEFT in software engineering\nscenarios. Our code is available at\nhttps://github.com/martin-wey/peft-llm-code/.",
        "pdf_link": "https://arxiv.org/pdf/2308.10462v2.pdf"
    },
    {
        "title": "Dynamic Strategy Chain: Dynamic Zero-Shot CoT for Long Mental Health Support Generation",
        "authors": [
            "Qi Chen",
            "Dexi Liu"
        ],
        "published": "2023-08-21T03:31:20Z",
        "summary": "Long counseling Text Generation for Mental health support (LTGM), an\ninnovative and challenging task, aims to provide help-seekers with mental\nhealth support through a comprehensive and more acceptable response. The\ncombination of chain-of-thought (CoT) prompting and Large Language Models\n(LLMs) is employed and get the SOTA performance on various NLP tasks,\nespecially on text generation tasks. Zero-shot CoT prompting is one of the most\ncommon methods in CoT prompting. However, in the LTGM task, Zero-shot CoT\nprompting can not simulate a counselor or provide personalized strategies\nwithout effective mental health counseling strategy prompts. To tackle this\nchallenge, we propose a zero-shot Dynamic Strategy Chain (DSC) prompting\nmethod. Firstly, we utilize GPT2 to learn the responses written by mental\nhealth counselors and dynamically generate mental health counseling strategies\ntailored to the help-seekers' needs. Secondly, the Zero-shot DSC prompting is\nconstructed according to mental health counseling strategies and the\nhelp-seekers' post. Finally, the Zero-shot DSC prompting is employed to guide\nLLMs in generating more human-like responses for the help-seekers. Both\nautomatic and manual evaluations demonstrate that Zero-shot DSC prompting can\ndeliver more human-like responses than CoT prompting methods on LTGM tasks.",
        "pdf_link": "https://arxiv.org/pdf/2308.10444v1.pdf"
    },
    {
        "title": "Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions",
        "authors": [
            "Wesley Tann",
            "Yuancheng Liu",
            "Jun Heng Sim",
            "Choon Meng Seah",
            "Ee-Chien Chang"
        ],
        "published": "2023-08-21T03:30:21Z",
        "summary": "The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves\nparticipants finding text strings or ``flags'' by exploiting system\nvulnerabilities. Large Language Models (LLMs) are natural-language models\ntrained on vast amounts of words to understand and generate text; they can\nperform well on many CTF challenges. Such LLMs are freely available to\nstudents. In the context of CTF exercises in the classroom, this raises\nconcerns about academic integrity. Educators must understand LLMs' capabilities\nto modify their teaching to accommodate generative AI assistance. This research\ninvestigates the effectiveness of LLMs, particularly in the realm of CTF\nchallenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,\nGoogle Bard, and Microsoft Bing. First, we assess the LLMs' question-answering\nperformance on five Cisco certifications with varying difficulty levels. Next,\nwe qualitatively study the LLMs' abilities in solving CTF challenges to\nunderstand their limitations. We report on the experience of using the LLMs for\nseven test cases in all five types of CTF challenges. In addition, we\ndemonstrate how jailbreak prompts can bypass and break LLMs' ethical\nsafeguards. The paper concludes by discussing LLM's impact on CTF exercises and\nits implications.",
        "pdf_link": "https://arxiv.org/pdf/2308.10443v1.pdf"
    },
    {
        "title": "LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient Querying",
        "authors": [
            "Thommen George Karimpanal",
            "Laknath Buddhika Semage",
            "Santu Rana",
            "Hung Le",
            "Truyen Tran",
            "Sunil Gupta",
            "Svetha Venkatesh"
        ],
        "published": "2023-08-21T02:07:35Z",
        "summary": "Large language models (LLMs) have recently demonstrated their impressive\nability to provide context-aware responses via text. This ability could\npotentially be used to predict plausible solutions in sequential decision\nmaking tasks pertaining to pattern completion. For example, by observing a\npartial stack of cubes, LLMs can predict the correct sequence in which the\nremaining cubes should be stacked by extrapolating the observed patterns (e.g.,\ncube sizes, colors or other attributes) in the partial stack. In this work, we\nintroduce LaGR (Language-Guided Reinforcement learning), which uses this\npredictive ability of LLMs to propose solutions to tasks that have been\npartially completed by a primary reinforcement learning (RL) agent, in order to\nsubsequently guide the latter's training. However, as RL training is generally\nnot sample-efficient, deploying this approach would inherently imply that the\nLLM be repeatedly queried for solutions; a process that can be expensive and\ninfeasible. To address this issue, we introduce SEQ (sample efficient\nquerying), where we simultaneously train a secondary RL agent to decide when\nthe LLM should be queried for solutions. Specifically, we use the quality of\nthe solutions emanating from the LLM as the reward to train this agent. We show\nthat our proposed framework LaGR-SEQ enables more efficient primary RL\ntraining, while simultaneously minimizing the number of queries to the LLM. We\ndemonstrate our approach on a series of tasks and highlight the advantages of\nour approach, along with its limitations and potential future research\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2308.13542v1.pdf"
    },
    {
        "title": "Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts",
        "authors": [
            "Fan Gao",
            "Hang Jiang",
            "Rui Yang",
            "Qingcheng Zeng",
            "Jinghui Lu",
            "Moritz Blum",
            "Dairui Liu",
            "Tianwei She",
            "Yuang Jiang",
            "Irene Li"
        ],
        "published": "2023-08-21T01:32:45Z",
        "summary": "Educational materials such as survey articles in specialized fields like\ncomputer science traditionally require tremendous expert inputs and are\ntherefore expensive to create and update. Recently, Large Language Models\n(LLMs) have achieved significant success across various general tasks. However,\ntheir effectiveness and limitations in the education domain are yet to be fully\nexplored. In this work, we examine the proficiency of LLMs in generating\nsuccinct survey articles specific to the niche field of NLP in computer\nscience, focusing on a curated list of 99 topics. Automated benchmarks reveal\nthat GPT-4 surpasses its predecessors like GPT-3.5, PaLM2, and LLaMa2 in\ncomparison to the established ground truth. We compare both human and GPT-based\nevaluation scores and provide in-depth analysis. While our findings suggest\nthat GPT-created surveys are more contemporary and accessible than\nhuman-authored ones, certain limitations were observed. Notably, GPT-4, despite\noften delivering outstanding content, occasionally exhibited lapses like\nmissing details or factual errors. At last, we compared the rating behavior\nbetween humans and GPT-4 and found systematic bias in using GPT evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2308.10410v3.pdf"
    },
    {
        "title": "FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models",
        "authors": [
            "Yanhong Bai",
            "Jiabao Zhao",
            "Jinxin Shi",
            "Tingjiang Wei",
            "Xingjiao Wu",
            "Liang He"
        ],
        "published": "2023-08-21T00:25:17Z",
        "summary": "Detecting stereotypes and biases in Large Language Models (LLMs) can enhance\nfairness and reduce adverse impacts on individuals or groups when these LLMs\nare applied. However, the majority of existing methods focus on measuring the\nmodel's preference towards sentences containing biases and stereotypes within\ndatasets, which lacks interpretability and cannot detect implicit biases and\nstereotypes in the real world. To address this gap, this paper introduces a\nfour-stage framework to directly evaluate stereotypes and biases in the\ngenerated content of LLMs, including direct inquiry testing, serial or adapted\nstory testing, implicit association testing, and unknown situation testing.\nAdditionally, the paper proposes multi-dimensional evaluation metrics and\nexplainable zero-shot prompts for automated evaluation. Using the education\nsector as a case study, we constructed the Edu-FairMonitor based on the\nfour-stage framework, which encompasses 12,632 open-ended questions covering\nnine sensitive factors and 26 educational scenarios. Experimental results\nreveal varying degrees of stereotypes and biases in five LLMs evaluated on\nEdu-FairMonitor. Moreover, the results of our proposed automated evaluation\nmethod have shown a high correlation with human annotations.",
        "pdf_link": "https://arxiv.org/pdf/2308.10397v2.pdf"
    },
    {
        "title": "A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability",
        "authors": [
            "Ming Jin",
            "Bilgehan Sel",
            "Fnu Hardeep",
            "Wotao Yin"
        ],
        "published": "2023-08-20T22:42:04Z",
        "summary": "This paper outlines a natural conversational approach to solving personalized\nenergy-related problems using large language models (LLMs). We focus on\ncustomizable optimization problems that necessitate repeated solving with\nslight variations in modeling and are user-specific, hence posing a challenge\nto devising a one-size-fits-all model. We put forward a strategy that augments\nan LLM with an optimization solver, enhancing its proficiency in understanding\nand responding to user specifications and preferences while providing nonlinear\nreasoning capabilities. Our approach pioneers the novel concept of human-guided\noptimization autoformalism, translating a natural language task specification\nautomatically into an optimization instance. This enables LLMs to analyze,\nexplain, and tackle a variety of instance-specific energy-related problems,\npushing beyond the limits of current prompt-based techniques.\n  Our research encompasses various commonplace tasks in the energy sector, from\nelectric vehicle charging and Heating, Ventilation, and Air Conditioning (HVAC)\ncontrol to long-term planning problems such as cost-benefit evaluations for\ninstalling rooftop solar photovoltaics (PVs) or heat pumps. This pilot study\nmarks an essential stride towards the context-based formulation of optimization\nusing LLMs, with the potential to democratize optimization processes. As a\nresult, stakeholders are empowered to optimize their energy consumption,\npromoting sustainable energy practices customized to personal needs and\npreferences.",
        "pdf_link": "https://arxiv.org/pdf/2308.10380v2.pdf"
    },
    {
        "title": "Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation",
        "authors": [
            "Li Zhong",
            "Zilong Wang"
        ],
        "published": "2023-08-20T18:36:28Z",
        "summary": "Recently, the large language models (LLMs) have shown extraordinary ability\nin understanding natural language and generating programming code. It has been\na common practice of software engineers to consult LLMs when encountering\ncoding questions. Although efforts have been made to avoid syntax errors and\nalign the code with the intended semantics, the reliability and robustness of\nthe code generationfrom LLMs have not yet been thoroughly studied. The\nexecutable code is not equivalent to the reliable and robust code, especially\nin the context of real-world software development. The misuse of APIs in the\ngenerated code could lead to severe problem, such as resource leaks, program\ncrashes. To make things worse, the users of LLM code generation services are\nactually the developers that are most vulnerable to these code that seems right\n-- They are always novice developers that are not familiar with the APIs that\nLLMs generate code for them. Therefore, they could hardly tell the misuse in\nthe code generated by LLMs, which further facilitates the incorrect code\napplied in real-world software. Existing code evaluation benchmark and datasets\nfocus on crafting small tasks such as programming questions in coding\ninterviews, which however deviates from the problem that developers would ask\nLLM for real-world coding help. To fill the missing piece, in this work, we\npropose a dataset RobustAPI for evaluating the reliability and robustness of\ncode generated by LLMs. We collect 1208 coding questions from StackOverflow on\n24 representative Java APIs. We summarize thecommon misuse patterns of these\nAPIs and evaluate them oncurrent popular LLMs. The evaluation results show that\nevenfor GPT-4, 62% of the generated code contains API misuses,which would cause\nunexpected consequences if the code isintroduced into real-world software.",
        "pdf_link": "https://arxiv.org/pdf/2308.10335v5.pdf"
    },
    {
        "title": "How Good Are Large Language Models at Out-of-Distribution Detection?",
        "authors": [
            "Bo Liu",
            "Liming Zhan",
            "Zexin Lu",
            "Yujie Feng",
            "Lei Xue",
            "Xiao-Ming Wu"
        ],
        "published": "2023-08-20T13:15:18Z",
        "summary": "Out-of-distribution (OOD) detection plays a vital role in enhancing the\nreliability of machine learning (ML) models. The emergence of large language\nmodels (LLMs) has catalyzed a paradigm shift within the ML community,\nshowcasing their exceptional capabilities across diverse natural language\nprocessing tasks. While existing research has probed OOD detection with\nrelative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark\ndifferences in scales, pre-training objectives, and inference paradigms call\ninto question the applicability of these findings to LLMs. This paper embarks\non a pioneering empirical investigation of OOD detection in the domain of LLMs,\nfocusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate\ncommonly-used OOD detectors, scrutinizing their performance in both zero-grad\nand fine-tuning scenarios. Notably, we alter previous discriminative\nin-distribution fine-tuning into generative fine-tuning, aligning the\npre-training objective of LLMs with downstream tasks. Our findings unveil that\na simple cosine distance OOD detector demonstrates superior efficacy,\noutperforming other OOD detectors. We provide an intriguing explanation for\nthis phenomenon by highlighting the isotropic nature of the embedding spaces of\nLLMs, which distinctly contrasts with the anisotropic property observed in\nsmaller BERT family models. The new insight enhances our understanding of how\nLLMs detect OOD data, thereby enhancing their adaptability and reliability in\ndynamic environments.",
        "pdf_link": "https://arxiv.org/pdf/2308.10261v3.pdf"
    },
    {
        "title": "A Survey on Fairness in Large Language Models",
        "authors": [
            "Yingji Li",
            "Mengnan Du",
            "Rui Song",
            "Xin Wang",
            "Ying Wang"
        ],
        "published": "2023-08-20T03:30:22Z",
        "summary": "Large Language Models (LLMs) have shown powerful performance and development\nprospects and are widely deployed in the real world. However, LLMs can capture\nsocial biases from unprocessed training data and propagate the biases to\ndownstream tasks. Unfair LLM systems have undesirable social impacts and\npotential harms. In this paper, we provide a comprehensive review of related\nresearch on fairness in LLMs. Considering the influence of parameter magnitude\nand training paradigm on research strategy, we divide existing fairness\nresearch into oriented to medium-sized LLMs under pre-training and fine-tuning\nparadigms and oriented to large-sized LLMs under prompting paradigms. First,\nfor medium-sized LLMs, we introduce evaluation metrics and debiasing methods\nfrom the perspectives of intrinsic bias and extrinsic bias, respectively. Then,\nfor large-sized LLMs, we introduce recent fairness research, including fairness\nevaluation, reasons for bias, and debiasing methods. Finally, we discuss and\nprovide insight on the challenges and future directions for the development of\nfairness in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.10149v2.pdf"
    },
    {
        "title": "March in Chat: Interactive Prompting for Remote Embodied Referring Expression",
        "authors": [
            "Yanyuan Qiao",
            "Yuankai Qi",
            "Zheng Yu",
            "Jing Liu",
            "Qi Wu"
        ],
        "published": "2023-08-20T03:00:20Z",
        "summary": "Many Vision-and-Language Navigation (VLN) tasks have been proposed in recent\nyears, from room-based to object-based and indoor to outdoor. The REVERIE\n(Remote Embodied Referring Expression) is interesting since it only provides\nhigh-level instructions to the agent, which are closer to human commands in\npractice. Nevertheless, this poses more challenges than other VLN tasks since\nit requires agents to infer a navigation plan only based on a short\ninstruction. Large Language Models (LLMs) show great potential in robot action\nplanning by providing proper prompts. Still, this strategy has not been\nexplored under the REVERIE settings. There are several new challenges. For\nexample, the LLM should be environment-aware so that the navigation plan can be\nadjusted based on the current visual observation. Moreover, the LLM planned\nactions should be adaptable to the much larger and more complex REVERIE\nenvironment. This paper proposes a March-in-Chat (MiC) model that can talk to\nthe LLM on the fly and plan dynamically based on a newly proposed\nRoom-and-Object Aware Scene Perceiver (ROASP). Our MiC model outperforms the\nprevious state-of-the-art by large margins by SPL and RGSPL metrics on the\nREVERIE benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2308.10141v1.pdf"
    },
    {
        "title": "UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding",
        "authors": [
            "Hao Feng",
            "Zijian Wang",
            "Jingqun Tang",
            "Jinghui Lu",
            "Wengang Zhou",
            "Houqiang Li",
            "Can Huang"
        ],
        "published": "2023-08-19T17:32:34Z",
        "summary": "In the era of Large Language Models (LLMs), tremendous strides have been made\nin the field of multimodal understanding. However, existing advanced algorithms\nare limited to effectively utilizing the immense representation capabilities\nand rich world knowledge inherent to these large pre-trained models, and the\nbeneficial connections among tasks within the context of text-rich scenarios\nhave not been sufficiently explored. In this work, we introduce UniDoc, a novel\nmultimodal model equipped with text detection and recognition capabilities,\nwhich are deficient in existing approaches. Moreover, UniDoc capitalizes on the\nbeneficial interactions among tasks to enhance the performance of each\nindividual task. To implement UniDoc, we perform unified multimodal instruct\ntuning on the contributed large-scale instruction following datasets.\nQuantitative and qualitative experimental results show that UniDoc sets\nstate-of-the-art scores across multiple challenging benchmarks. To the best of\nour knowledge, this is the first large multimodal model capable of simultaneous\ntext detection, recognition, spotting, and understanding.",
        "pdf_link": "https://arxiv.org/pdf/2308.11592v2.pdf"
    },
    {
        "title": "GameEval: Evaluating LLMs on Conversational Games",
        "authors": [
            "Dan Qiao",
            "Chenfei Wu",
            "Yaobo Liang",
            "Juntao Li",
            "Nan Duan"
        ],
        "published": "2023-08-19T14:33:40Z",
        "summary": "The rapid advancements in large language models (LLMs) have presented\nchallenges in evaluating those models. Existing evaluation methods are either\nreference-based or preference based, which inevitably need human intervention\nor introduce test bias caused by evaluator models. In this paper, we propose\nGameEval, a novel approach to evaluating LLMs through goal-driven\nconversational games, overcoming the limitations of previous methods. GameEval\ntreats LLMs as game players and assigns them distinct roles with specific goals\nachieved by launching conversations of various forms, including discussion,\nquestion answering, and voting. We design three unique games with cooperative\nor adversarial objectives, accompanied by corresponding evaluation metrics, to\nshow how this new paradigm comprehensively evaluates model performance.Through\nextensive experiments, we show that GameEval can effectively differentiate the\ncapabilities of various LLMs, providing a comprehensive assessment of their\nintegrated abilities to solve complex problems. Our public anonymous code is\navailable at https://github.com/GameEval/GameEval.",
        "pdf_link": "https://arxiv.org/pdf/2308.10032v1.pdf"
    },
    {
        "title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models",
        "authors": [
            "Liwen Zhang",
            "Weige Cai",
            "Zhaowei Liu",
            "Zhi Yang",
            "Wei Dai",
            "Yujie Liao",
            "Qianru Qin",
            "Yifei Li",
            "Xingyu Liu",
            "Zhiqiang Liu",
            "Zhoufan Zhu",
            "Anbo Wu",
            "Xin Guo",
            "Yun Chen"
        ],
        "published": "2023-08-19T10:38:00Z",
        "summary": "Large language models (LLMs) have demonstrated exceptional performance in\nvarious natural language processing tasks, yet their efficacy in more\nchallenging and domain-specific tasks remains largely unexplored. This paper\npresents FinEval, a benchmark specifically designed for the financial domain\nknowledge in the LLMs. FinEval is a collection of high-quality multiple-choice\nquestions covering Finance, Economy, Accounting, and Certificate. It includes\n4,661 questions spanning 34 different academic subjects. To ensure a\ncomprehensive model performance evaluation, FinEval employs a range of prompt\ntypes, including zero-shot and few-shot prompts, as well as answer-only and\nchain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs\non FinEval, the results show that only GPT-4 achieved an accuracy close to 70%\nin different prompt settings, indicating significant growth potential for LLMs\nin the financial domain knowledge. Our work offers a more comprehensive\nfinancial knowledge evaluation benchmark, utilizing data of mock exams and\ncovering a wide range of evaluated LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.09975v1.pdf"
    },
    {
        "title": "Tackling Vision Language Tasks Through Learning Inner Monologues",
        "authors": [
            "Diji Yang",
            "Kezhen Chen",
            "Jinmeng Rao",
            "Xiaoyuan Guo",
            "Yawen Zhang",
            "Jie Yang",
            "Yi Zhang"
        ],
        "published": "2023-08-19T10:10:49Z",
        "summary": "Visual language tasks require AI models to comprehend and reason with both\nvisual and textual content. Driven by the power of Large Language Models\n(LLMs), two prominent methods have emerged: (1) the hybrid integration between\nLLMs and Vision-Language Models (VLMs), where visual inputs are firstly\nconverted into language descriptions by VLMs, serving as inputs for LLMs to\ngenerate final answer(s); (2) visual feature alignment in language space, where\nvisual inputs are encoded as embeddings and projected to LLMs' language space\nvia further supervised fine-tuning. The first approach provides light training\ncosts and interpretability but is hard to be optimized in an end-to-end\nfashion. The second approach presents decent performance, but feature alignment\nusually requires large amounts of training data and lacks interpretability. To\ntackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal\nOptimization (IMMO), to solve complex vision language problems by simulating\ninner monologue processes, a cognitive process in which an individual engages\nin silent verbal communication with themselves. We enable LLMs and VLMs to\ninteract through natural language conversation and propose to use a two-stage\ntraining process to learn how to do the inner monologue (self-asking questions\nand answering questions). IMMO is evaluated on two popular tasks and the\nresults suggest by emulating the cognitive phenomenon of internal dialogue, our\napproach can enhance reasoning and explanation abilities, contributing to the\nmore effective fusion of vision and language models. More importantly, instead\nof using predefined human-crafted monologues, IMMO learns this process within\nthe deep learning models, promising wider applicability to many different AI\nproblems beyond vision language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2308.09970v1.pdf"
    },
    {
        "title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs",
        "authors": [
            "Federico Cassano",
            "John Gouwar",
            "Francesca Lucchetti",
            "Claire Schlesinger",
            "Anders Freeman",
            "Carolyn Jane Anderson",
            "Molly Q Feldman",
            "Michael Greenberg",
            "Abhinav Jangda",
            "Arjun Guha"
        ],
        "published": "2023-08-19T03:19:01Z",
        "summary": "Over the past few years, Large Language Models of Code (Code LLMs) have\nstarted to have a significant impact on programming practice. Code LLMs are\nalso emerging as building blocks for research in programming languages and\nsoftware engineering. However, Code LLMs produce impressive results on\nprogramming languages that are well represented in their training data (e.g.,\nJava, Python, or JavaScript), but struggle with low-resource languages that\nhave limited training data available. Low resource languages include OCaml,\nRacket, and several others.\n  This paper presents an effective approach for boosting the performance of\nCode LLMs on low-resource languages using semi-synthetic data. Our approach,\nMultiPL-T, translates training data from high-resource languages into training\ndata for low-resource languages in the following way. 1) We use a Code LLM to\nsynthesize tests for commented code from a high-resource language, filtering\nout faulty tests and code with low test coverage. 2) We use a Code LLM to\ntranslate Python code to a target low-resource language, and use tests to\nvalidate the translation. We apply this approach to generate tens of thousands\nof validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore,\nwe use an open model (StarCoderBase) with open training data (The Stack), which\nallows us to decontaminate benchmarks, train models without violating licenses,\nand run experiments that could not otherwise be done.\n  With MultiPL-T generated data, we present fine-tuned versions of\nStarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On\nestablished benchmarks (MultiPL-E), these models outperform other open Code\nLLMs. The MultiPL-T approach is easy to apply to new languages, and is\nsignificantly more efficient and effective than alternatives such as training\nlonger.",
        "pdf_link": "https://arxiv.org/pdf/2308.09895v5.pdf"
    },
    {
        "title": "Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis",
        "authors": [
            "Oscar J. Romero",
            "John Zimmerman",
            "Aaron Steinfeld",
            "Anthony Tomasic"
        ],
        "published": "2023-08-18T21:42:47Z",
        "summary": "This paper explores the integration of two AI subdisciplines employed in the\ndevelopment of artificial agents that exhibit intelligent behavior: Large\nLanguage Models (LLMs) and Cognitive Architectures (CAs). We present three\nintegration approaches, each grounded in theoretical models and supported by\npreliminary empirical evidence. The modular approach, which introduces four\nmodels with varying degrees of integration, makes use of chain-of-thought\nprompting, and draws inspiration from augmented LLMs, the Common Model of\nCognition, and the simulation theory of cognition. The agency approach,\nmotivated by the Society of Mind theory and the LIDA cognitive architecture,\nproposes the formation of agent collections that interact at micro and macro\ncognitive levels, driven by either LLMs or symbolic components. The\nneuro-symbolic approach, which takes inspiration from the CLARION cognitive\narchitecture, proposes a model where bottom-up learning extracts symbolic\nrepresentations from an LLM layer and top-down guidance utilizes symbolic\nrepresentations to direct prompt engineering in the LLM layer. These approaches\naim to harness the strengths of both LLMs and CAs, while mitigating their\nweaknesses, thereby advancing the development of more robust AI systems. We\ndiscuss the tradeoffs and challenges associated with each approach.",
        "pdf_link": "https://arxiv.org/pdf/2308.09830v3.pdf"
    },
    {
        "title": "Learning Representations on Logs for AIOps",
        "authors": [
            "Pranjal Gupta",
            "Harshit Kumar",
            "Debanjana Kar",
            "Karan Bhukar",
            "Pooja Aggarwal",
            "Prateeti Mohapatra"
        ],
        "published": "2023-08-18T20:34:46Z",
        "summary": "AI for IT Operations (AIOps) is a powerful platform that Site Reliability\nEngineers (SREs) use to automate and streamline operational workflows with\nminimal human intervention. Automated log analysis is a critical task in AIOps\nas it provides key insights for SREs to identify and address ongoing faults.\nTasks such as log format detection, log classification, and log parsing are key\ncomponents of automated log analysis. Most of these tasks require supervised\nlearning; however, there are multiple challenges due to limited labelled log\ndata and the diverse nature of log data. Large Language Models (LLMs) such as\nBERT and GPT3 are trained using self-supervision on a vast amount of unlabeled\ndata. These models provide generalized representations that can be effectively\nused for various downstream tasks with limited labelled data. Motivated by the\nsuccess of LLMs in specific domains like science and biology, this paper\nintroduces a LLM for log data which is trained on public and proprietary log\ndata. The results of our experiments demonstrate that the proposed LLM\noutperforms existing models on multiple downstream tasks. In summary, AIOps\npowered by LLMs offers an efficient and effective solution for automating log\nanalysis tasks and enabling SREs to focus on higher-level tasks. Our proposed\nLLM, trained on public and proprietary log data, offers superior performance on\nmultiple downstream tasks, making it a valuable addition to the AIOps platform.",
        "pdf_link": "https://arxiv.org/pdf/2308.11526v1.pdf"
    },
    {
        "title": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
        "authors": [
            "Rishabh Bhardwaj",
            "Soujanya Poria"
        ],
        "published": "2023-08-18T16:27:04Z",
        "summary": "Larger language models (LLMs) have taken the world by storm with their\nmassive multi-tasking capabilities simply by optimizing over a next-word\nprediction objective. With the emergence of their properties and encoded\nknowledge, the risk of LLMs producing harmful outputs increases, making them\nunfit for scalable deployment for the public. In this work, we propose a new\nsafety evaluation benchmark RED-EVAL that carries out red-teaming. We show that\neven widely deployed models are susceptible to the Chain of Utterances-based\n(CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and\nChatGPT to unethically respond to more than 65% and 73% of harmful queries. We\nalso demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in\ngenerating harmful responses in more than 86% of the red-teaming attempts.\nNext, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It\nconstitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting,\nwe collect a dataset that consists of 1.9K harmful questions covering a wide\nrange of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2)\nSAFE-ALIGN: We demonstrate how the conversational dataset can be used for the\nsafety alignment of LLMs by minimizing the negative log-likelihood over helpful\nresponses and penalizing over harmful responses by gradient accent over sample\nloss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely\naligned when evaluated on RED-EVAL and HHH benchmarks while preserving the\nutility of the baseline models (TruthfulQA, MMLU, and BBH).",
        "pdf_link": "https://arxiv.org/pdf/2308.09662v3.pdf"
    },
    {
        "title": "Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning",
        "authors": [
            "Pengbo Hu",
            "Ji Qi",
            "Xingyu Li",
            "Hong Li",
            "Xinqi Wang",
            "Bing Quan",
            "Ruiyu Wang",
            "Yi Zhou"
        ],
        "published": "2023-08-18T16:21:40Z",
        "summary": "There emerges a promising trend of using large language models (LLMs) to\ngenerate code-like plans for complex inference tasks such as visual reasoning.\nThis paradigm, known as LLM-based planning, provides flexibility in problem\nsolving and endows better interpretability. However, current research is mostly\nlimited to basic scenarios of simple questions that can be straightforward\nanswered in a few inference steps. Planning for the more challenging multi-hop\nvisual reasoning tasks remains under-explored. Specifically, under multi-hop\nreasoning situations, the trade-off between accuracy and the complexity of\nplan-searching becomes prominent. The prevailing algorithms either address the\nefficiency issue by employing the fast one-stop generation or adopt a complex\niterative generation method to improve accuracy. Both fail to balance the need\nfor efficiency and performance. Drawing inspiration from the dual system of\ncognition in the human brain, the fast and the slow think processes, we propose\na hierarchical plan-searching algorithm that integrates the one-stop reasoning\n(fast) and the Tree-of-thought (slow). Our approach succeeds in performance\nwhile significantly saving inference steps. Moreover, we repurpose the PTR and\nthe CLEVER datasets, developing a systematic framework for evaluating the\nperformance and efficiency of LLMs-based plan-search algorithms under reasoning\ntasks at different levels of difficulty. Extensive experiments demonstrate the\nsuperiority of our proposed algorithm in terms of performance and efficiency.\nThe dataset and code will be release soon.",
        "pdf_link": "https://arxiv.org/pdf/2308.09658v2.pdf"
    },
    {
        "title": "RatGPT: Turning online LLMs into Proxies for Malware Attacks",
        "authors": [
            "Mika Beckerich",
            "Laura Plein",
            "Sergio Coronado"
        ],
        "published": "2023-08-17T20:54:39Z",
        "summary": "The evolution of Generative AI and the capabilities of the newly released\nLarge Language Models (LLMs) open new opportunities in software engineering.\nHowever, they also lead to new challenges in cybersecurity. Recently,\nresearchers have shown the possibilities of using LLMs such as ChatGPT to\ngenerate malicious content that can directly be exploited or guide\ninexperienced hackers to weaponize tools and code. These studies covered\nscenarios that still require the attacker to be in the middle of the loop. In\nthis study, we leverage openly available plugins and use an LLM as proxy\nbetween the attacker and the victim. We deliver a proof-of-concept where\nChatGPT is used for the dissemination of malicious software while evading\ndetection, alongside establishing the communication to a command and control\n(C2) server to receive commands to interact with a victim's system. Finally, we\npresent the general approach as well as essential elements in order to stay\nundetected and make the attack a success. This proof-of-concept highlights\nsignificant cybersecurity issues with openly available plugins and LLMs, which\nrequire the development of security guidelines, controls, and mitigation\nstrategies.",
        "pdf_link": "https://arxiv.org/pdf/2308.09183v2.pdf"
    },
    {
        "title": "Semantic Consistency for Assuring Reliability of Large Language Models",
        "authors": [
            "Harsh Raj",
            "Vipul Gupta",
            "Domenic Rosati",
            "Subhabrata Majumdar"
        ],
        "published": "2023-08-17T18:11:33Z",
        "summary": "Large Language Models (LLMs) exhibit remarkable fluency and competence across\nvarious natural language tasks. However, recent research has highlighted their\nsensitivity to variations in input prompts. To deploy LLMs in a safe and\nreliable manner, it is crucial for their outputs to be consistent when prompted\nwith expressions that carry the same meaning or intent. While some existing\nwork has explored how state-of-the-art LLMs address this issue, their\nevaluations have been confined to assessing lexical equality of single- or\nmulti-word answers, overlooking the consistency of generative text sequences.\nFor a more comprehensive understanding of the consistency of LLMs in open-ended\ntext generation scenarios, we introduce a general measure of semantic\nconsistency, and formulate multiple versions of this metric to evaluate the\nperformance of various LLMs. Our proposal demonstrates significantly higher\nconsistency and stronger correlation with human evaluations of output\nconsistency than traditional metrics based on lexical consistency. Finally, we\npropose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance\nsemantic consistency. When evaluated for closed-book question answering based\non answer variations from the TruthfulQA benchmark, A2C increases accuracy\nmetrics for pretrained and finetuned LLMs by up to 47%, and semantic\nconsistency metrics for instruction-tuned models by up to 7-fold.",
        "pdf_link": "https://arxiv.org/pdf/2308.09138v1.pdf"
    },
    {
        "title": "MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models",
        "authors": [
            "Mohd Zaki",
            "Jayadeva",
            "Mausam",
            "N. M. Anoop Krishnan"
        ],
        "published": "2023-08-17T17:51:05Z",
        "summary": "Information extraction and textual comprehension from materials literature\nare vital for developing an exhaustive knowledge base that enables accelerated\nmaterials discovery. Language models have demonstrated their capability to\nanswer domain-specific questions and retrieve information from knowledge bases.\nHowever, there are no benchmark datasets in the materials domain that can\nevaluate the understanding of the key concepts by these language models. In\nthis work, we curate a dataset of 650 challenging questions from the materials\ndomain that require the knowledge and skills of a materials student who has\ncleared their undergraduate degree. We classify these questions based on their\nstructure and the materials science domain-based subcategories. Further, we\nevaluate the performance of GPT-3.5 and GPT-4 models on solving these questions\nvia zero-shot and chain of thought prompting. It is observed that GPT-4 gives\nthe best performance (~62% accuracy) as compared to GPT-3.5. Interestingly, in\ncontrast to the general observation, no significant improvement in accuracy is\nobserved with the chain of thought prompting. To evaluate the limitations, we\nperformed an error analysis, which revealed conceptual errors (~64%) as the\nmajor contributor compared to computational errors (~36%) towards the reduced\nperformance of LLMs. We hope that the dataset and analysis performed in this\nwork will promote further research in developing better materials science\ndomain-specific LLMs and strategies for information extraction.",
        "pdf_link": "https://arxiv.org/pdf/2308.09115v1.pdf"
    },
    {
        "title": "Building Emotional Support Chatbots in the Era of LLMs",
        "authors": [
            "Zhonghua Zheng",
            "Lizi Liao",
            "Yang Deng",
            "Liqiang Nie"
        ],
        "published": "2023-08-17T10:49:18Z",
        "summary": "The integration of emotional support into various conversational scenarios\npresents profound societal benefits, such as social interactions, mental health\ncounseling, and customer service. However, there are unsolved challenges that\nhinder real-world applications in this field, including limited data\navailability and the absence of well-accepted model training paradigms. This\nwork endeavors to navigate these challenges by harnessing the capabilities of\nLarge Language Models (LLMs). We introduce an innovative methodology that\nsynthesizes human insights with the computational prowess of LLMs to curate an\nextensive emotional support dialogue dataset. Our approach is initiated with a\nmeticulously designed set of dialogues spanning diverse scenarios as generative\nseeds. By utilizing the in-context learning potential of ChatGPT, we\nrecursively generate an ExTensible Emotional Support dialogue dataset, named\nExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,\nexamining the impact of diverse training strategies, ultimately yielding an LLM\nmeticulously optimized for emotional support interactions. An exhaustive\nassessment of the resultant model showcases its proficiency in offering\nemotional support, marking a pivotal step in the realm of emotional support\nbots and paving the way for subsequent research and implementations.",
        "pdf_link": "https://arxiv.org/pdf/2308.11584v1.pdf"
    },
    {
        "title": "BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction",
        "authors": [
            "Dong Wang",
            "Kav\u00e9 Salamatian",
            "Yunqing Xia",
            "Weiwei Deng",
            "Qi Zhiang"
        ],
        "published": "2023-08-17T08:25:54Z",
        "summary": "Although deep pre-trained language models have shown promising benefit in a\nlarge set of industrial scenarios, including Click-Through-Rate (CTR)\nprediction, how to integrate pre-trained language models that handle only\ntextual signals into a prediction pipeline with non-textual features is\nchallenging.\n  Up to now two directions have been explored to integrate multi-modal inputs\nin fine-tuning of pre-trained language models. One consists of fusing the\noutcome of language models and non-textual features through an aggregation\nlayer, resulting into ensemble framework, where the cross-information between\ntextual and non-textual inputs are only learned in the aggregation layer. The\nsecond one consists of splitting non-textual features into fine-grained\nfragments and transforming the fragments to new tokens combined with textual\nones, so that they can be fed directly to transformer layers in language\nmodels. However, this approach increases the complexity of the learning and\ninference because of the numerous additional tokens.\n  To address these limitations, we propose in this work a novel framework\nBERT4CTR, with the Uni-Attention mechanism that can benefit from the\ninteractions between non-textual and textual features while maintaining low\ntime-costs in training and inference through a dimensionality reduction.\nComprehensive experiments on both public and commercial data demonstrate that\nBERT4CTR can outperform significantly the state-of-the-art frameworks to handle\nmulti-modal inputs and be applicable to CTR prediction.",
        "pdf_link": "https://arxiv.org/pdf/2308.11527v1.pdf"
    },
    {
        "title": "CMB: A Comprehensive Medical Benchmark in Chinese",
        "authors": [
            "Xidong Wang",
            "Guiming Hardy Chen",
            "Dingjie Song",
            "Zhiyi Zhang",
            "Zhihong Chen",
            "Qingying Xiao",
            "Feng Jiang",
            "Jianquan Li",
            "Xiang Wan",
            "Benyou Wang",
            "Haizhou Li"
        ],
        "published": "2023-08-17T07:51:23Z",
        "summary": "Large Language Models (LLMs) provide a possibility to make a great\nbreakthrough in medicine. The establishment of a standardized medical benchmark\nbecomes a fundamental cornerstone to measure progression. However, medical\nenvironments in different regions have their local characteristics, e.g., the\nubiquity and significance of traditional Chinese medicine within China.\nTherefore, merely translating English-based medical evaluation may result in\n\\textit{contextual incongruities} to a local region. To solve the issue, we\npropose a localized medical benchmark called CMB, a Comprehensive Medical\nBenchmark in Chinese, designed and rooted entirely within the native Chinese\nlinguistic and cultural framework. While traditional Chinese medicine is\nintegral to this evaluation, it does not constitute its entirety. Using this\nbenchmark, we have evaluated several prominent large-scale LLMs, including\nChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical\ndomain. We hope this benchmark provide first-hand experience in existing LLMs\nfor medicine and also facilitate the widespread adoption and enhancement of\nmedical LLMs within China. Our data and code are publicly available at\nhttps://github.com/FreedomIntelligence/CMB.",
        "pdf_link": "https://arxiv.org/pdf/2308.08833v2.pdf"
    },
    {
        "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
        "authors": [
            "Zekun Li",
            "Baolin Peng",
            "Pengcheng He",
            "Xifeng Yan"
        ],
        "published": "2023-08-17T06:21:50Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\ninstruction-following, becoming increasingly crucial across various\napplications. However, this capability brings with it the risk of prompt\ninjection attacks, where attackers inject instructions into LLMs' input to\nelicit undesirable actions or content. Understanding the robustness of LLMs\nagainst such attacks is vital for their safe implementation. In this work, we\nestablish a benchmark to evaluate the robustness of instruction-following LLMs\nagainst prompt injection attacks. Our objective is to determine the extent to\nwhich LLMs can be influenced by injected instructions and their ability to\ndifferentiate between these injected and original target instructions. Through\nextensive experiments with leading instruction-following LLMs, we uncover\nsignificant vulnerabilities in their robustness to such attacks. Our results\nindicate that some models are overly tuned to follow any embedded instructions\nin the prompt, overly focusing on the latter parts of the prompt without fully\ngrasping the entire context. By contrast, models with a better grasp of the\ncontext and instruction-following capabilities will potentially be more\nsusceptible to compromise by injected instructions. This underscores the need\nto shift the focus from merely enhancing LLMs' instruction-following\ncapabilities to improving their overall comprehension of prompts and\ndiscernment of instructions that are appropriate to follow. We hope our\nin-depth analysis offers insights into the underlying causes of these\nvulnerabilities, aiding in the development of future solutions. Code and data\nare available at\nhttps://github.com/Leezekun/instruction-following-robustness-eval",
        "pdf_link": "https://arxiv.org/pdf/2308.10819v3.pdf"
    },
    {
        "title": "Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning",
        "authors": [
            "David Noever",
            "Samantha Elizabeth Miller Noever"
        ],
        "published": "2023-08-17T03:14:00Z",
        "summary": "Addressing the gap in understanding visual comprehension in Large Language\nModels (LLMs), we designed a challenge-response study, subjecting Google Bard\nand GPT-Vision to 64 visual tasks, spanning categories like \"Visual Situational\nReasoning\" and \"Next Scene Prediction.\" Previous models, such as GPT4, leaned\nheavily on optical character recognition tools like Tesseract, whereas Bard and\nGPT-Vision, akin to Google Lens and Visual API, employ deep learning techniques\nfor visual text recognition. However, our findings spotlight both\nvision-language model's limitations: while proficient in solving visual\nCAPTCHAs that stump ChatGPT alone, it falters in recreating visual elements\nlike ASCII art or analyzing Tic Tac Toe grids, suggesting an over-reliance on\neducated visual guesses. The prediction problem based on visual inputs appears\nparticularly challenging with no common-sense guesses for next-scene\nforecasting based on current \"next-token\" multimodal models. This study\nprovides experimental insights into the current capacities and areas for\nimprovement in multimodal LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16705v2.pdf"
    },
    {
        "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
        "authors": [
            "Yun Luo",
            "Zhen Yang",
            "Fandong Meng",
            "Yafu Li",
            "Jie Zhou",
            "Yue Zhang"
        ],
        "published": "2023-08-17T02:53:23Z",
        "summary": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.",
        "pdf_link": "https://arxiv.org/pdf/2308.08747v3.pdf"
    },
    {
        "title": "PMET: Precise Model Editing in a Transformer",
        "authors": [
            "Xiaopeng Li",
            "Shasha Li",
            "Shezheng Song",
            "Jing Yang",
            "Jun Ma",
            "Jie Yu"
        ],
        "published": "2023-08-17T02:33:43Z",
        "summary": "Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.",
        "pdf_link": "https://arxiv.org/pdf/2308.08742v6.pdf"
    },
    {
        "title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs",
        "authors": [
            "Young Jin Kim",
            "Rawn Henry",
            "Raffy Fahim",
            "Hany Hassan Awadalla"
        ],
        "published": "2023-08-16T23:57:41Z",
        "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross various language tasks but pose challenges for practical deployment due\nto their substantial memory requirements. Furthermore, the latest generative\nmodels suffer from high inference costs caused by the memory bandwidth\nbottleneck in the auto-regressive decoding process. To address these issues, we\npropose an efficient weight-only quantization method that reduces memory\nconsumption and accelerates inference for LLMs. To ensure minimal quality\ndegradation, we introduce a simple and effective heuristic approach that\nutilizes only the model weights of a pre-trained model. This approach is\napplicable to both Mixture-of-Experts (MoE) and dense models without requiring\nadditional fine-tuning. To demonstrate the effectiveness of our proposed\nmethod, we first analyze the challenges and issues associated with LLM\nquantization. Subsequently, we present our heuristic approach, which adaptively\nfinds the granularity of quantization, effectively addressing these problems.\nFurthermore, we implement highly efficient GPU GEMMs that perform on-the-fly\nmatrix multiplication and dequantization, supporting the multiplication of fp16\nor bf16 activations with int8 or int4 weights. We evaluate our approach on\nlarge-scale open source models such as OPT-175B and internal MoE models,\nshowcasing minimal accuracy loss while achieving up to 3.65 times higher\nthroughput on the same number of GPUs.",
        "pdf_link": "https://arxiv.org/pdf/2308.09723v1.pdf"
    },
    {
        "title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
        "authors": [
            "Ching Chang",
            "Wei-Yao Wang",
            "Wen-Chih Peng",
            "Tien-Fu Chen"
        ],
        "published": "2023-08-16T16:19:50Z",
        "summary": "Multivariate time-series forecasting is vital in various domains, e.g.,\neconomic planning and weather prediction. Deep train-from-scratch models have\nexhibited effective performance yet require large amounts of data, which limits\nreal-world applicability. Recently, researchers have leveraged the\nrepresentation learning transferability of pre-trained Large Language Models\n(LLMs) to handle limited non-linguistic datasets effectively. However,\nincorporating LLMs with time-series data presents challenges of limited\nadaptation due to different compositions between time-series and linguistic\ndata, and the inability to process multi-scale temporal information. To tackle\nthese challenges, we propose LLM4TS, a framework for time-series forecasting\nwith pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the\n\\textit{time-series alignment} stage to align LLMs with the nuances of\ntime-series data, and the \\textit{forecasting fine-tuning} stage for downstream\ntime-series forecasting tasks. Furthermore, our framework features a novel\ntwo-level aggregation method that integrates multi-scale temporal data within\npre-trained LLMs, enhancing their ability to interpret time-specific\ninformation. In experiments across 7 time-series forecasting datasets, LLM4TS\nis superior to existing state-of-the-art methods compared with\ntrained-from-scratch models in full-shot scenarios, and also achieves an\naverage improvement of 6.84% in MSE in few-shot scenarios. In addition,\nevaluations compared with different self-supervised learning approaches\nhighlight LLM4TS's effectiveness with representation learning in forecasting\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2308.08469v5.pdf"
    },
    {
        "title": "Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models",
        "authors": [
            "Zhenhua Wang",
            "Wei Xie",
            "Kai Chen",
            "Baosheng Wang",
            "Zhiwen Gui",
            "Enze Wang"
        ],
        "published": "2023-08-16T09:04:36Z",
        "summary": "Large language models (LLMs), such as ChatGPT, have emerged with astonishing\ncapabilities approaching artificial general intelligence. While providing\nconvenience for various societal needs, LLMs have also lowered the cost of\ngenerating harmful content. Consequently, LLM developers have deployed\nsemantic-level defenses to recognize and reject prompts that may lead to\ninappropriate content. Unfortunately, these defenses are not foolproof, and\nsome attackers have crafted \"jailbreak\" prompts that temporarily hypnotize the\nLLM into forgetting content defense rules and answering any improper questions.\nTo date, there is no clear explanation of the principles behind these\nsemantic-level attacks and defenses in both industry and academia.\n  This paper investigates the LLM jailbreak problem and proposes an automatic\njailbreak method for the first time. We propose the concept of a semantic\nfirewall and provide three technical implementation approaches. Inspired by the\nattack that penetrates traditional firewalls through reverse tunnels, we\nintroduce a \"self-deception\" attack that can bypass the semantic firewall by\ninducing LLM to generate prompts that facilitate jailbreak. We generated a\ntotal of 2,520 attack payloads in six languages (English, Russian, French,\nSpanish, Chinese, and Arabic) across seven virtual scenarios, targeting the\nthree most common types of violations: violence, hate, and pornography. The\nexperiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The\nsuccess rates on the two models were 86.2% and 67%, while the failure rates\nwere 4.7% and 2.2%, respectively. This highlighted the effectiveness of the\nproposed attack method. All experimental code and raw data will be released as\nopen-source to inspire future research. We believe that manipulating AI\nbehavior through carefully crafted prompts will become an important research\ndirection in the future.",
        "pdf_link": "https://arxiv.org/pdf/2308.11521v2.pdf"
    },
    {
        "title": "DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue",
        "authors": [
            "Lang Cao"
        ],
        "published": "2023-08-15T21:14:09Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, are increasingly sophisticated\nand exhibit capabilities closely resembling those of humans. A significant\napplication of these LLMs is their use as chat agents, responding to human\ninquiries across various domains. While current LLMs proficiently answer\ngeneral questions, they often fall short in complex diagnostic scenarios such\nas legal, medical, or other specialized consultations. These scenarios\ntypically require Task-Oriented Dialogue (TOD), where an AI chat agent must\nproactively pose questions and guide users toward specific goals or task\ncompletion. Previous fine-tuning models have underperformed in TOD and the full\npotential of this capability in current LLMs has not yet been fully explored.\nIn this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative\napproach that extends LLMs to more TOD scenarios. In addition to guiding users\nto complete tasks, DiagGPT can effectively manage the status of all topics\nthroughout the dialogue development. This feature enhances user experience and\noffers a more flexible interaction in TOD. Our experiments demonstrate that\nDiagGPT exhibits outstanding performance in conducting TOD with users, showing\nits potential for practical applications in various fields.",
        "pdf_link": "https://arxiv.org/pdf/2308.08043v3.pdf"
    },
    {
        "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
        "authors": [
            "Ziyu Zhuang",
            "Qiguang Chen",
            "Longxuan Ma",
            "Mingda Li",
            "Yi Han",
            "Yushan Qian",
            "Haopeng Bai",
            "Zixian Feng",
            "Weinan Zhang",
            "Ting Liu"
        ],
        "published": "2023-08-15T17:40:34Z",
        "summary": "From pre-trained language model (PLM) to large language model (LLM), the\nfield of natural language processing (NLP) has witnessed steep performance\ngains and wide practical uses. The evaluation of a research field guides its\ndirection of improvement. However, LLMs are extremely hard to thoroughly\nevaluate for two reasons. First of all, traditional NLP tasks become inadequate\ndue to the excellent performance of LLM. Secondly, existing evaluation tasks\nare difficult to keep up with the wide range of applications in real-world\nscenarios. To tackle these problems, existing works proposed various benchmarks\nto better evaluate LLMs. To clarify the numerous evaluation tasks in both\nacademia and industry, we investigate multiple papers concerning LLM\nevaluations. We summarize 4 core competencies of LLM, including reasoning,\nknowledge, reliability, and safety. For every competency, we introduce its\ndefinition, corresponding benchmarks, and metrics. Under this competency\narchitecture, similar tasks are combined to reflect corresponding ability,\nwhile new tasks can also be easily added into the system. Finally, we give our\nsuggestions on the future direction of LLM's evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2308.07902v1.pdf"
    },
    {
        "title": "Link-Context Learning for Multimodal LLMs",
        "authors": [
            "Yan Tai",
            "Weichen Fan",
            "Zhao Zhang",
            "Feng Zhu",
            "Rui Zhao",
            "Ziwei Liu"
        ],
        "published": "2023-08-15T17:33:24Z",
        "summary": "The ability to learn from context with novel concepts, and deliver\nappropriate responses are essential in human conversations. Despite current\nMultimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being\ntrained on mega-scale datasets, recognizing unseen images or understanding\nnovel concepts in a training-free manner remains a challenge. In-Context\nLearning (ICL) explores training-free few-shot learning, where models are\nencouraged to ``learn to learn\" from limited tasks and generalize to unseen\ntasks. In this work, we propose link-context learning (LCL), which emphasizes\n\"reasoning from cause and effect\" to augment the learning capabilities of\nMLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal\nrelationship between the support set and the query set. By providing\ndemonstrations with causal links, LCL guides the model to discern not only the\nanalogy but also the underlying causal associations between data points, which\nempowers MLLMs to recognize unseen images and understand novel concepts more\neffectively. To facilitate the evaluation of this novel approach, we introduce\nthe ISEKAI dataset, comprising exclusively of unseen generated image-label\npairs designed for link-context learning. Extensive experiments show that our\nLCL-MLLM exhibits strong link-context learning capabilities to novel concepts\nover vanilla MLLMs. Code and data will be released at\nhttps://github.com/isekai-portal/Link-Context-Learning.",
        "pdf_link": "https://arxiv.org/pdf/2308.07891v1.pdf"
    },
    {
        "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation",
        "authors": [
            "Charles O'Neill",
            "Yuan-Sen Ting",
            "Ioana Ciuca",
            "Jack Miller",
            "Thang Bui"
        ],
        "published": "2023-08-15T08:49:14Z",
        "summary": "Large Language Models (LLMs) hold immense potential to generate synthetic\ndata of high quality and utility, which has numerous applications from\ndownstream model training to practical data utilisation. However, contemporary\nmodels, despite their impressive capacities, consistently struggle to produce\nboth coherent and diverse data. To address the coherency issue, we introduce\ncontrastive expert guidance, where the difference between the logit\ndistributions of fine-tuned and base language models is emphasised to ensure\ndomain adherence. In order to ensure diversity, we utilise existing real and\nsynthetic examples as negative prompts to the model. We deem this dual-pronged\napproach to logit reshaping as STEER: Semantic Text Enhancement via Embedding\nRepositioning. STEER operates at inference-time and systematically guides the\nLLMs to strike a balance between adherence to the data distribution (ensuring\nsemantic fidelity) and deviation from prior synthetic examples or existing real\ndatasets (ensuring diversity and authenticity). This delicate balancing act is\nachieved by dynamically moving towards or away from chosen representations in\nthe latent space. STEER demonstrates improved performance over previous\nsynthetic data generation techniques, exhibiting better balance between data\ndiversity and coherency across three distinct tasks: hypothesis generation,\ntoxic and non-toxic comment generation, and commonsense reasoning task\ngeneration. We demonstrate how STEER allows for fine-tuned control over the\ndiversity-coherency trade-off via its hyperparameters, highlighting its\nversatility.",
        "pdf_link": "https://arxiv.org/pdf/2308.07645v2.pdf"
    },
    {
        "title": "LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation",
        "authors": [
            "Xiaoming Shi",
            "Jie Xu",
            "Jinru Ding",
            "Jiali Pang",
            "Sichen Liu",
            "Shuqing Luo",
            "Xingwei Peng",
            "Lu Lu",
            "Haihong Yang",
            "Mingtao Hu",
            "Tong Ruan",
            "Shaoting Zhang"
        ],
        "published": "2023-08-15T08:32:20Z",
        "summary": "There is an increasing interest in developing LLMs for medical diagnosis to\nimprove diagnosis efficiency. Despite their alluring technological potential,\nthere is no unified and comprehensive evaluation criterion, leading to the\ninability to evaluate the quality and potential risks of medical LLMs, further\nhindering the application of LLMs in medical treatment scenarios. Besides,\ncurrent evaluations heavily rely on labor-intensive interactions with LLMs to\nobtain diagnostic dialogues and human evaluation on the quality of diagnosis\ndialogue. To tackle the lack of unified and comprehensive evaluation criterion,\nwe first initially establish an evaluation criterion, termed LLM-specific\nMini-CEX to assess the diagnostic capabilities of LLMs effectively, based on\noriginal Mini-CEX. To address the labor-intensive interaction problem, we\ndevelop a patient simulator to engage in automatic conversations with LLMs, and\nutilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental\nresults show that the LLM-specific Mini-CEX is adequate and necessary to\nevaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual\nevaluation on the metrics of humanistic qualities and provides reproducible and\nautomated comparisons between different LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.07635v1.pdf"
    },
    {
        "title": "A Survey on Model Compression for Large Language Models",
        "authors": [
            "Xunyu Zhu",
            "Jian Li",
            "Yong Liu",
            "Can Ma",
            "Weiping Wang"
        ],
        "published": "2023-08-15T08:31:05Z",
        "summary": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks with remarkable success. However, their formidable size and computational\ndemands present significant challenges for practical deployment, especially in\nresource-constrained environments. As these challenges become increasingly\npertinent, the field of model compression has emerged as a pivotal research\narea to alleviate these limitations. This paper presents a comprehensive survey\nthat navigates the landscape of model compression techniques tailored\nspecifically for LLMs. Addressing the imperative need for efficient deployment,\nwe delve into various methodologies, encompassing quantization, pruning,\nknowledge distillation, and more. Within each of these techniques, we highlight\nrecent advancements and innovative approaches that contribute to the evolving\nlandscape of LLM research. Furthermore, we explore benchmarking strategies and\nevaluation metrics that are essential for assessing the effectiveness of\ncompressed LLMs. By providing insights into the latest developments and\npractical implications, this survey serves as an invaluable resource for both\nresearchers and practitioners. As LLMs continue to evolve, this survey aims to\nfacilitate enhanced efficiency and real-world applicability, establishing a\nfoundation for future advancements in the field.",
        "pdf_link": "https://arxiv.org/pdf/2308.07633v3.pdf"
    },
    {
        "title": "Detecting The Corruption Of Online Questionnaires By Artificial Intelligence",
        "authors": [
            "Benjamin Lebrun",
            "Sharon Temtsin",
            "Andrew Vonasch",
            "Christoph Bartneck"
        ],
        "published": "2023-08-14T23:47:56Z",
        "summary": "Online questionnaires that use crowd-sourcing platforms to recruit\nparticipants have become commonplace, due to their ease of use and low costs.\nArtificial Intelligence (AI) based Large Language Models (LLM) have made it\neasy for bad actors to automatically fill in online forms, including generating\nmeaningful text for open-ended tasks. These technological advances threaten the\ndata quality for studies that use online questionnaires. This study tested if\ntext generated by an AI for the purpose of an online study can be detected by\nboth humans and automatic AI detection systems. While humans were able to\ncorrectly identify authorship of text above chance level (76 percent accuracy),\ntheir performance was still below what would be required to ensure satisfactory\ndata quality. Researchers currently have to rely on the disinterest of bad\nactors to successfully use open-ended responses as a useful tool for ensuring\ndata quality. Automatic AI detection systems are currently completely unusable.\nIf AIs become too prevalent in submitting responses then the costs associated\nwith detecting fraudulent submissions will outweigh the benefits of online\nquestionnaires. Individual attention checks will no longer be a sufficient tool\nto ensure good data quality. This problem can only be systematically addressed\nby crowd-sourcing platforms. They cannot rely on automatic AI detection systems\nand it is unclear how they can ensure data quality for their paying clients.",
        "pdf_link": "https://arxiv.org/pdf/2308.07499v1.pdf"
    },
    {
        "title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",
        "authors": [
            "Mansi Phute",
            "Alec Helbling",
            "Matthew Hull",
            "ShengYun Peng",
            "Sebastian Szyller",
            "Cory Cornelius",
            "Duen Horng Chau"
        ],
        "published": "2023-08-14T17:54:10Z",
        "summary": "Large language models (LLMs) are popular for high-quality text generation but\ncan produce harmful content, even when aligned with human values through\nreinforcement learning. Adversarial prompts can bypass their safety measures.\nWe propose LLM Self Defense, a simple approach to defend against these attacks\nby having an LLM screen the induced responses. Our method does not require any\nfine-tuning, input preprocessing, or iterative output generation. Instead, we\nincorporate the generated content into a pre-defined prompt and employ another\ninstance of an LLM to analyze the text and predict whether it is harmful. We\ntest LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent\nLLMs against various types of attacks, such as forcefully inducing affirmative\nresponses to prompts and prompt engineering attacks. Notably, LLM Self Defense\nsucceeds in reducing the attack success rate to virtually 0 using both GPT 3.5\nand Llama 2.",
        "pdf_link": "https://arxiv.org/pdf/2308.07308v3.pdf"
    },
    {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "authors": [
            "Chi-Min Chan",
            "Weize Chen",
            "Yusheng Su",
            "Jianxuan Yu",
            "Wei Xue",
            "Shanghang Zhang",
            "Jie Fu",
            "Zhiyuan Liu"
        ],
        "published": "2023-08-14T15:13:04Z",
        "summary": "Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.",
        "pdf_link": "https://arxiv.org/pdf/2308.07201v1.pdf"
    },
    {
        "title": "Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice",
        "authors": [
            "Alexandra Sasha Luccioni",
            "Anna Rogers"
        ],
        "published": "2023-08-14T13:00:53Z",
        "summary": "Much of the recent discourse within the NLP research community has been\ncentered around Large Language Models (LLMs), their functionality and potential\n-- yet not only do we not have a working definition of LLMs, but much of this\ndiscourse relies on claims and assumptions that are worth re-examining. This\nposition paper contributes a definition of LLMs, explicates some of the\nassumptions made regarding their functionality, and outlines the existing\nevidence for and against them. We conclude with suggestions for research\ndirections and their framing in future work.",
        "pdf_link": "https://arxiv.org/pdf/2308.07120v1.pdf"
    },
    {
        "title": "CausalLM is not optimal for in-context learning",
        "authors": [
            "Nan Ding",
            "Tomer Levinboim",
            "Jialin Wu",
            "Sebastian Goodman",
            "Radu Soricut"
        ],
        "published": "2023-08-14T03:14:38Z",
        "summary": "Recent empirical evidence indicates that transformer based in-context\nlearning performs better when using a prefix language model (prefixLM), in\nwhich in-context samples can all attend to each other, compared to causal\nlanguage models (causalLM), which use auto-regressive attention that prohibits\nin-context samples to attend to future samples. While this result is intuitive,\nit is not understood from a theoretical perspective. In this paper we take a\ntheoretical approach and analyze the convergence behavior of prefixLM and\ncausalLM under a certain parameter construction. Our analysis shows that both\nLM types converge to their stationary points at a linear rate, but that while\nprefixLM converges to the optimal solution of linear regression, causalLM\nconvergence dynamics follows that of an online gradient descent algorithm,\nwhich is not guaranteed to be optimal even as the number of samples grows\ninfinitely. We supplement our theoretical claims with empirical experiments\nover synthetic and real tasks and using various types of transformers. Our\nexperiments verify that causalLM consistently underperforms prefixLM in all\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2308.06912v3.pdf"
    },
    {
        "title": "Generative Interpretation",
        "authors": [
            "Yonathan A. Arbel",
            "David Hoffman"
        ],
        "published": "2023-08-14T02:59:27Z",
        "summary": "We introduce generative interpretation, a new approach to estimating\ncontractual meaning using large language models. As AI triumphalism is the\norder of the day, we proceed by way of grounded case studies, each illustrating\nthe capabilities of these novel tools in distinct ways. Taking well-known\ncontracts opinions, and sourcing the actual agreements that they adjudicated,\nwe show that AI models can help factfinders ascertain ordinary meaning in\ncontext, quantify ambiguity, and fill gaps in parties' agreements. We also\nillustrate how models can calculate the probative value of individual pieces of\nextrinsic evidence. After offering best practices for the use of these models\ngiven their limitations, we consider their implications for judicial practice\nand contract theory. Using LLMs permits courts to estimate what the parties\nintended cheaply and accurately, and as such generative interpretation\nunsettles the current interpretative stalemate. Their use responds to\nefficiency-minded textualists and justice-oriented contextualists, who argue\nabout whether parties will prefer cost and certainty or accuracy and fairness.\nParties--and courts--would prefer a middle path, in which adjudicators strive\nto predict what the contract really meant, admitting just enough context to\napproximate reality while avoiding unguided and biased assimilation of\nevidence. As generative interpretation offers this possibility, we argue it can\nbecome the new workhorse of contractual interpretation.",
        "pdf_link": "https://arxiv.org/pdf/2308.06907v1.pdf"
    },
    {
        "title": "Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine",
        "authors": [
            "Thomas Savage",
            "Ashwin Nayak",
            "Robert Gallo",
            "Ekanath Rangan",
            "Jonathan H Chen"
        ],
        "published": "2023-08-13T19:04:07Z",
        "summary": "One of the major barriers to using large language models (LLMs) in medicine\nis the perception they use uninterpretable methods to make clinical decisions\nthat are inherently different from the cognitive processes of clinicians. In\nthis manuscript we develop novel diagnostic reasoning prompts to study whether\nLLMs can perform clinical reasoning to accurately form a diagnosis. We find\nthat GPT4 can be prompted to mimic the common clinical reasoning processes of\nclinicians without sacrificing diagnostic accuracy. This is significant because\nan LLM that can use clinical reasoning to provide an interpretable rationale\noffers physicians a means to evaluate whether LLMs can be trusted for patient\ncare. Novel prompting methods have the potential to expose the black box of\nLLMs, bringing them one step closer to safe and effective use in medicine.",
        "pdf_link": "https://arxiv.org/pdf/2308.06834v1.pdf"
    },
    {
        "title": "Large Language Models and Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
        "authors": [
            "Jiajia Li",
            "Mingle Xu",
            "Lirong Xiang",
            "Dong Chen",
            "Weichao Zhuang",
            "Xunyuan Yin",
            "Zhaojian Li"
        ],
        "published": "2023-08-13T02:59:36Z",
        "summary": "The past decade has witnessed the rapid development and adoption of ML & DL\nmethodologies in agricultural systems, showcased by great successes in\nagricultural applications. However, these conventional ML/DL models have\ncertain limitations: they heavily rely on large, costly-to-acquire labeled\ndatasets for training, require specialized expertise for development and\nmaintenance, and are mostly tailored for specific tasks, thus lacking\ngeneralizability. Recently, large pre-trained models, also known as FMs, have\ndemonstrated remarkable successes in language, vision, and decision-making\ntasks across various domains. These models are trained on a large amount of\ndata from multiple domains and modalities. Once trained, they can accomplish\nversatile tasks with just minor fine-tuning and minimal task-specific labeled\ndata. Despite their proven effectiveness and huge potential, there has been\nlittle exploration of applying FMs to agriculture AI. Thus, this study aims to\nexplore the potential of FMs in the field of smart agriculture. In particular,\nconceptual tools and technical background are presented to help the\nunderstanding of the problem space and uncover new research directions. To this\nend, recent FMs in the general CS domain are reviewed, and the models are\ncategorized into four categories: language FMs, vision FMs, multimodal FMs, and\nreinforcement learning FMs. Then, the steps of developing agriculture FMs\n(AFMs) are outlined and potential applications in smart agriculture are\ndiscussed. Moreover, challenges and risks associated with developing AFMs are\ndiscussed, including model training, validation, and deployment. In summary,\nthe advancement of AI in agriculture is explored by introducing AFMs as a\npromising paradigm that can significantly mitigate the reliance on extensive\nlabeled datasets and enhance the efficiency, effectiveness, and generalization\nof agricultural AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2308.06668v4.pdf"
    },
    {
        "title": "Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation",
        "authors": [
            "Ambrose Robinson",
            "William Thorne",
            "Ben P. Wu",
            "Abdullah Pandor",
            "Munira Essat",
            "Mark Stevenson",
            "Xingyi Song"
        ],
        "published": "2023-08-12T16:56:55Z",
        "summary": "Medical systematic reviews can be very costly and resource intensive. We\nexplore how Large Language Models (LLMs) can support and be trained to perform\nliterature screening when provided with a detailed set of selection criteria.\nSpecifically, we instruction tune LLaMA and Guanaco models to perform abstract\nscreening for medical systematic reviews. Our best model, Bio-SIEVE,\noutperforms both ChatGPT and trained traditional approaches, and generalises\nbetter across medical domains. However, there remains the challenge of adapting\nthe model to safety-first scenarios. We also explore the impact of multi-task\ntraining with Bio-SIEVE-Multi, including tasks such as PICO extraction and\nexclusion reasoning, but find that it is unable to match single-task\nBio-SIEVE's performance. We see Bio-SIEVE as an important step towards\nspecialising LLMs for the biomedical systematic review process and explore its\nfuture developmental opportunities. We release our models, code and a list of\nDOIs to reconstruct our dataset for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2308.06610v1.pdf"
    },
    {
        "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
        "authors": [
            "Youliang Yuan",
            "Wenxiang Jiao",
            "Wenxuan Wang",
            "Jen-tse Huang",
            "Pinjia He",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023-08-12T04:05:57Z",
        "summary": "Safety lies at the core of the development of Large Language Models (LLMs).\nThere is ample work on aligning LLMs with human ethics and preferences,\nincluding data filtering in pretraining, supervised fine-tuning, reinforcement\nlearning from human feedback, and red teaming, etc. In this study, we discover\nthat chat in cipher can bypass the safety alignment techniques of LLMs, which\nare mainly conducted in natural languages. We propose a novel framework\nCipherChat to systematically examine the generalizability of safety alignment\nto non-natural languages -- ciphers. CipherChat enables humans to chat with\nLLMs through cipher prompts topped with system role descriptions and few-shot\nenciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\nincluding ChatGPT and GPT-4 for different representative human ciphers across\n11 safety domains in both English and Chinese. Experimental results show that\ncertain ciphers succeed almost 100% of the time to bypass the safety alignment\nof GPT-4 in several safety domains, demonstrating the necessity of developing\nsafety alignment for non-natural languages. Notably, we identify that LLMs seem\nto have a ''secret cipher'', and propose a novel SelfCipher that uses only role\nplay and several demonstrations in natural language to evoke this capability.\nSelfCipher surprisingly outperforms existing human ciphers in almost all cases.\nOur code and data will be released at https://github.com/RobustNLP/CipherChat.",
        "pdf_link": "https://arxiv.org/pdf/2308.06463v2.pdf"
    },
    {
        "title": "Dynamic Planning with a LLM",
        "authors": [
            "Gautier Dagan",
            "Frank Keller",
            "Alex Lascarides"
        ],
        "published": "2023-08-11T21:17:13Z",
        "summary": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline.",
        "pdf_link": "https://arxiv.org/pdf/2308.06391v1.pdf"
    },
    {
        "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
        "authors": [
            "Terufumi Morishita",
            "Gaku Morio",
            "Atsuki Yamaguchi",
            "Yasuhiro Sogawa"
        ],
        "published": "2023-08-11T13:15:35Z",
        "summary": "We study a synthetic corpus based approach for language models (LMs) to\nacquire logical deductive reasoning ability. The previous studies generated\ndeduction examples using specific sets of deduction rules. However, these rules\nwere limited or otherwise arbitrary, limiting the generalizability of acquired\nreasoning ability. We rethink this and adopt a well-grounded set of deduction\nrules based on formal logic theory, which can derive any other deduction rules\nwhen combined in a multistep way. Then, using the proposed corpora, which we\nname FLD (Formal Logic Deduction), we first evaluate and analyze the logical\nreasoning ability of the latest LLMs. Even GPT-4 can solve only half of the\nproblems, suggesting that pure logical reasoning isolated from knowledge is\nstill challenging for the LLMs, and additional training specialized in logical\nreasoning is indeed essential. We next empirically verify that LMs trained on\nFLD corpora acquire more generalizable reasoning ability. Furthermore, we\nidentify the aspects of reasoning ability on which deduction corpora can\nenhance LMs and those on which they cannot, and discuss future directions on\neach aspect. The released corpora serve both as learning resources and as\nchallenging benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2308.07336v3.pdf"
    },
    {
        "title": "Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters",
        "authors": [
            "Arne Bewersdorff",
            "Kathrin Se\u00dfler",
            "Armin Baur",
            "Enkelejda Kasneci",
            "Claudia Nerdel"
        ],
        "published": "2023-08-11T12:03:12Z",
        "summary": "Identifying logical errors in complex, incomplete or even contradictory and\noverall heterogeneous data like students' experimentation protocols is\nchallenging. Recognizing the limitations of current evaluation methods, we\ninvestigate the potential of Large Language Models (LLMs) for automatically\nidentifying student errors and streamlining teacher assessments. Our aim is to\nprovide a foundation for productive, personalized feedback. Using a dataset of\n65 student protocols, an Artificial Intelligence (AI) system based on the\nGPT-3.5 and GPT-4 series was developed and tested against human raters. Our\nresults indicate varying levels of accuracy in error detection between the AI\nsystem and human raters. The AI system can accurately identify many fundamental\nstudent errors, for instance, the AI system identifies when a student is\nfocusing the hypothesis not on the dependent variable but solely on an expected\nobservation (acc. = 0.90), when a student modifies the trials in an ongoing\ninvestigation (acc. = 1), and whether a student is conducting valid test trials\n(acc. = 0.82) reliably. The identification of other, usually more complex\nerrors, like whether a student conducts a valid control trial (acc. = .60),\nposes a greater challenge. This research explores not only the utility of AI in\neducational settings, but also contributes to the understanding of the\ncapabilities of LLMs in error detection in inquiry-based learning like\nexperimentation.",
        "pdf_link": "https://arxiv.org/pdf/2308.06088v1.pdf"
    },
    {
        "title": "Large Language Models for Telecom: Forthcoming Impact on the Industry",
        "authors": [
            "Ali Maatouk",
            "Nicola Piovesan",
            "Fadhel Ayed",
            "Antonio De Domenico",
            "Merouane Debbah"
        ],
        "published": "2023-08-11T08:41:00Z",
        "summary": "Large Language Models (LLMs), AI-driven models that can achieve\ngeneral-purpose language understanding and generation, have emerged as a\ntransformative force, revolutionizing fields well beyond Natural Language\nProcessing (NLP) and garnering unprecedented attention. As LLM technology\ncontinues to progress, the telecom industry is facing the prospect of its\nimpact on its landscape. To elucidate these implications, we delve into the\ninner workings of LLMs, providing insights into their current capabilities and\nlimitations. We also examine the use cases that can be readily implemented in\nthe telecom industry, streamlining tasks, such as anomalies resolutions and\ntechnical specifications comprehension, which currently hinder operational\nefficiency and demand significant manpower and expertise. Furthermore, we\nuncover essential research directions that deal with the distinctive challenges\nof utilizing the LLMs within the telecom domain. Addressing them represents a\nsignificant stride towards fully harnessing the potential of LLMs and unlocking\ntheir capabilities to the fullest extent within the telecom domain.",
        "pdf_link": "https://arxiv.org/pdf/2308.06013v2.pdf"
    },
    {
        "title": "PIPPA: A Partially Synthetic Conversational Dataset",
        "authors": [
            "Tear Gosling",
            "Alpin Dale",
            "Yinhe Zheng"
        ],
        "published": "2023-08-11T00:33:26Z",
        "summary": "With the emergence of increasingly powerful large language models, there is a\nburgeoning interest in leveraging these models for casual conversation and\nrole-play applications. However, existing conversational and role-playing\ndatasets often fail to capture the diverse and nuanced interactions typically\nexhibited by real-world role-play participants. To address this limitation and\ncontribute to the rapidly growing field, we introduce a partially-synthetic\ndataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA\nis a result of a community-driven crowdsourcing effort involving a group of\nrole-play enthusiasts. The dataset comprises over 1 million utterances that are\ndistributed across 26,000 conversation sessions and provides a rich resource\nfor researchers and AI developers to explore and refine conversational AI\nsystems in the context of role-play scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2308.05884v1.pdf"
    },
    {
        "title": "Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems",
        "authors": [
            "Ernest Davis",
            "Scott Aaronson"
        ],
        "published": "2023-08-10T17:22:28Z",
        "summary": "This report describes a test of the large language model GPT-4 with the\nWolfram Alpha and the Code Interpreter plug-ins on 105 original problems in\nscience and math, at the high school and college levels, carried out in\nJune-August 2023. Our tests suggest that the plug-ins significantly enhance\nGPT's ability to solve these problems. Having said that, there are still often\n\"interface\" failures; that is, GPT often has trouble formulating problems in a\nway that elicits useful answers from the plug-ins. Fixing these interface\nfailures seems like a central challenge in making GPT a reliable tool for\ncollege-level calculation problems.",
        "pdf_link": "https://arxiv.org/pdf/2308.05713v2.pdf"
    },
    {
        "title": "NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search",
        "authors": [
            "Edouard Yvinec",
            "Arnaud Dapogny",
            "Kevin Bailly"
        ],
        "published": "2023-08-10T14:19:58Z",
        "summary": "Deep neural network (DNN) deployment has been confined to larger hardware\ndevices due to their expensive computational requirements. This challenge has\nrecently reached another scale with the emergence of large language models\n(LLMs). In order to reduce both their memory footprint and latency, a promising\ntechnique is quantization. It consists in converting floating point\nrepresentations to low bit-width fixed point representations, usually by\nassuming a uniform mapping onto a regular grid. This process, referred to in\nthe literature as uniform quantization, may however be ill-suited as most DNN\nweights and activations follow a bell-shaped distribution. This is even worse\non LLMs whose weight distributions are known to exhibit large, high impact,\noutlier values. In this work, we propose an improvement over the most commonly\nadopted way to tackle this limitation in deep learning models quantization,\nnamely, non-uniform quantization. NUPES leverages automorphisms to preserve the\nscalar multiplications. Such transformations are derived from power functions.\nHowever, the optimization of the exponent parameter and weight values remains a\nchallenging and novel problem which could not be solved with previous post\ntraining optimization techniques which only learn to round up or down weight\nvalues in order to preserve the predictive function. We circumvent this\nlimitation with a new paradigm: learning new quantized weights over the entire\nquantized space. Similarly, we enable the optimization of the power exponent,\ni.e. the optimization of the quantization operator itself during training by\nalleviating all the numerical instabilities. The resulting predictive function\nis compatible with integer-only low-bit inference. We show the ability of the\nmethod to achieve state-of-the-art compression rates in both, data-free and\ndata-driven configurations.",
        "pdf_link": "https://arxiv.org/pdf/2308.05600v1.pdf"
    },
    {
        "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length",
        "authors": [
            "Miao Fan",
            "Chen Hu",
            "Shuchang Zhou"
        ],
        "published": "2023-08-10T13:50:17Z",
        "summary": "The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in\nshaping the impact of large language models (LLMs), contributing significantly\nto controlling output toxicity and selecting output styles, particularly as\nLLMs often harbor misleading content, highlighting the urgency to align them\nwith human values for secure AI systems. The RLHF, characterized by complexity,\ninstability, and sensitivity to hyperparameters, makes the evaluation of the\nreward model for complex tasks challenging, thereby further complicating the\nuse of Proximal Policy Optimization (PPO). In this paper, we introduce a simple\ntask designed to employ Gloden as a reward model that validates the\neffectiveness of PPO and inspires it, primarily explaining the task of\nutilizing PPO to manipulate the tokenizer length of the output generated by the\nmodel. Experiments confirm that PPO is not only effective in manipulating the\noutput tokenizer length to a certain extent in this type of task but also\nexhibits facilitated training once the influence of the reward model effect is\nexcluded, making it an exciting development.",
        "pdf_link": "https://arxiv.org/pdf/2308.05585v1.pdf"
    },
    {
        "title": "C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT",
        "authors": [
            "Pan Liang",
            "Danwei Ye",
            "Zihao Zhu",
            "Yunchao Wang",
            "Wang Xia",
            "Ronghua Liang",
            "Guodao Sun"
        ],
        "published": "2023-08-10T13:29:12Z",
        "summary": "Large language models (LLMs), such as ChatGPT, have demonstrated outstanding\nperformance in various fields, particularly in natural language understanding\nand generation tasks. In complex application scenarios, users tend to engage in\nmulti-turn conversations with ChatGPT to keep contextual information and obtain\ncomprehensive responses. However, human forgetting and model contextual\nforgetting remain prominent issues in multi-turn conversation scenarios, which\nchallenge the users' conversation comprehension and contextual continuity for\nChatGPT. To address these challenges, we propose an interactive conversation\nvisualization system called C5, which includes Global View, Topic View, and\nContext-associated Q\\&A View. The Global View uses the GitLog diagram metaphor\nto represent the conversation structure, presenting the trend of conversation\nevolution and supporting the exploration of locally salient features. The Topic\nView is designed to display all the question and answer nodes and their\nrelationships within a topic using the structure of a knowledge graph, thereby\ndisplay the relevance and evolution of conversations. The Context-associated\nQ\\&A View consists of three linked views, which allow users to explore\nindividual conversations deeply while providing specific contextual information\nwhen posing questions. The usefulness and effectiveness of C5 were evaluated\nthrough a case study and a user study.",
        "pdf_link": "https://arxiv.org/pdf/2308.05567v1.pdf"
    },
    {
        "title": "LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following",
        "authors": [
            "Kaize Shi",
            "Xueyao Sun",
            "Dingxian Wang",
            "Yinlin Fu",
            "Guandong Xu",
            "Qing Li"
        ],
        "published": "2023-08-09T12:26:37Z",
        "summary": "E-commerce authoring involves creating attractive, abundant, and targeted\npromotional content to drive product sales. The emergence of large language\nmodels (LLMs) introduces an innovative paradigm, offering a unified solution to\naddress various authoring tasks within this scenario. However, mainstream LLMs\ntrained on general corpora with common sense knowledge reveal limitations in\nfitting complex and personalized features unique to e-commerce products and\ncustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,\nraising concerns about safeguarding voluminous customer privacy data during\ntransmission. This paper proposes the LLaMA-E, the unified and customized\ninstruction-following language models focusing on diverse e-commerce authoring\ntasks. Specifically, the domain experts create the seed instruction set from\nthe tasks of ads generation, query-enhanced product title rewriting, product\nclassification, purchase intent speculation, and general Q&A. These tasks\nenable the models to comprehensively understand precise e-commerce authoring\nknowledge by interleaving features covering typical service aspects of\ncustomers, sellers, and platforms. The GPT-3.5 is introduced as a teacher\nmodel, which expands the seed instructions to form a training set for the\nLLaMA-E models with various scales. The experimental results show that the\nproposed LLaMA-E models achieve state-of-the-art results in quantitative and\nqualitative evaluations, also exhibiting the advantage in zero-shot scenes. To\nthe best of our knowledge, this study is the first to serve the LLMs to\nspecific e-commerce authoring scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2308.04913v1.pdf"
    },
    {
        "title": "CLEVA: Chinese Language Models EVAluation Platform",
        "authors": [
            "Yanyang Li",
            "Jianqiao Zhao",
            "Duo Zheng",
            "Zi-Yuan Hu",
            "Zhi Chen",
            "Xiaohui Su",
            "Yongfeng Huang",
            "Shijia Huang",
            "Dahua Lin",
            "Michael R. Lyu",
            "Liwei Wang"
        ],
        "published": "2023-08-09T09:11:31Z",
        "summary": "With the continuous emergence of Chinese Large Language Models (LLMs), how to\nevaluate a model's capabilities has become an increasingly significant issue.\nThe absence of a comprehensive Chinese benchmark that thoroughly assesses a\nmodel's performance, the unstandardized and incomparable prompting procedure,\nand the prevalent risk of contamination pose major challenges in the current\nevaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted\nto holistically evaluate Chinese LLMs. Our platform employs a standardized\nworkflow to assess LLMs' performance across various dimensions, regularly\nupdating a competitive leaderboard. To alleviate contamination, CLEVA curates a\nsignificant proportion of new data and develops a sampling strategy that\nguarantees a unique subset for each leaderboard round. Empowered by an\neasy-to-use interface that requires just a few mouse clicks and a model API,\nusers can conduct a thorough evaluation with minimal coding. Large-scale\nexperiments featuring 23 Chinese LLMs have validated CLEVA's efficacy.",
        "pdf_link": "https://arxiv.org/pdf/2308.04813v2.pdf"
    },
    {
        "title": "A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology",
        "authors": [
            "Sean Wu",
            "Michael Koo",
            "Lesley Blum",
            "Andy Black",
            "Liyo Kao",
            "Fabien Scalzo",
            "Ira Kurtz"
        ],
        "published": "2023-08-09T05:01:28Z",
        "summary": "In recent years, there have been significant breakthroughs in the field of\nnatural language processing, particularly with the development of large\nlanguage models (LLMs). These LLMs have showcased remarkable capabilities on\nvarious benchmarks. In the healthcare field, the exact role LLMs and other\nfuture AI models will play remains unclear. There is a potential for these\nmodels in the future to be used as part of adaptive physician training, medical\nco-pilot applications, and digital patient interaction scenarios. The ability\nof AI models to participate in medical training and patient care will depend in\npart on their mastery of the knowledge content of specific medical fields. This\nstudy investigated the medical knowledge capability of LLMs, specifically in\nthe context of internal medicine subspecialty multiple-choice test-taking\nability. We compared the performance of several open-source LLMs (Koala 7B,\nFalcon 7B, Stable-Vicuna 13B, and Orca Mini 13B), to GPT-4 and Claude 2 on\nmultiple-choice questions in the field of Nephrology. Nephrology was chosen as\nan example of a particularly conceptually complex subspecialty field within\ninternal medicine. The study was conducted to evaluate the ability of LLM\nmodels to provide correct answers to nephSAP (Nephrology Self-Assessment\nProgram) multiple-choice questions. The overall success of open-sourced LLMs in\nanswering the 858 nephSAP multiple-choice questions correctly was 17.1% -\n25.5%. In contrast, Claude 2 answered 54.4% of the questions correctly, whereas\nGPT-4 achieved a score of 73.3%. We show that current widely used open-sourced\nLLMs do poorly in their ability for zero-shot reasoning when compared to GPT-4\nand Claude 2. The findings of this study potentially have significant\nimplications for the future of subspecialty medical training and patient care.",
        "pdf_link": "https://arxiv.org/pdf/2308.04709v1.pdf"
    },
    {
        "title": "Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA",
        "authors": [
            "Yuhan Ma",
            "Haiqi Jiang",
            "Chenyou Fan"
        ],
        "published": "2023-08-09T03:18:07Z",
        "summary": "Large Language Models (LLMs) have shown outstanding performance across wide\nrange of downstream tasks. This competency is attributed to their substantial\nparameter size and pre-training on extensive corpus. Moreover, LLMs have\nexhibited enhanced reasoning capabilities in tackling complex reasoning tasks,\nowing to the utilization of a method named ``Chain-of-Thought (CoT)\nprompting''. This method is designed to generate intermediate reasoning steps\nthat guide the inference of the final answer. However, it is essential to\nhighlight that these advanced reasoning abilities appear to emerge in models\nwith a minimum of 10 billion parameters, thereby limiting its efficacy in\nsituations where computational resources are constrained. In this paper, we\ninvestigate the possibility of transferring the reasoning capabilities of LLMs\nto smaller models via knowledge distillation. Specifically, we propose Sci-CoT,\na two-stage framework that separates the processes of generating rationales and\ninferring answers. This method enables a more efficient use of rationales\nduring the answer inference stage, leading to improved performance on\nscientific question-answering tasks. Utilizing Sci-CoT, our 80-million\nparameter model is able to exceed the performance of BLOOM-176B in the ARC-Easy\ndataset under the few shot setting.",
        "pdf_link": "https://arxiv.org/pdf/2308.04679v1.pdf"
    },
    {
        "title": "ChatGPT for Arabic Grammatical Error Correction",
        "authors": [
            "Sang Yun Kwon",
            "Gagan Bhatia",
            "El Moatez Billah Nagoud",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023-08-08T18:00:39Z",
        "summary": "Recently, large language models (LLMs) fine-tuned to follow human instruction\nhave exhibited significant capabilities in various English NLP tasks. However,\ntheir performance in grammatical error correction (GEC) tasks, particularly in\nnon-English languages, remains significantly unexplored. In this paper, we\ndelve into abilities of instruction fine-tuned LLMs in Arabic GEC, a task made\ncomplex due to Arabic's rich morphology. Our findings suggest that various\nprompting methods, coupled with (in-context) few-shot learning, demonstrate\nconsiderable effectiveness, with GPT-4 achieving up to $65.49$\nF\\textsubscript{1} score under expert prompting (approximately $5$ points\nhigher than our established baseline). This highlights the potential of LLMs in\nlow-resource settings, offering a viable approach for generating useful\nsynthetic data for model training. Despite these positive results, we find that\ninstruction fine-tuned models, regardless of their size, significantly\nunderperform compared to fully fine-tuned models of significantly smaller\nsizes. This disparity highlights a substantial room for improvements for LLMs.\nInspired by methods from low-resource machine translation, we also develop a\nmethod exploiting synthetic data that significantly outperforms previous models\non two standard Arabic benchmarks. Our work sets new SoTA for Arabic GEC, with\n$72.19\\%$ and $73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2308.04492v1.pdf"
    },
    {
        "title": "A Comparative Study of Code Generation using ChatGPT 3.5 across 10 Programming Languages",
        "authors": [
            "Alessio Buscemi"
        ],
        "published": "2023-08-08T15:02:32Z",
        "summary": "Large Language Models (LLMs) are advanced Artificial Intelligence (AI)\nsystems that have undergone extensive training using large datasets in order to\nunderstand and produce language that closely resembles that of humans. These\nmodels have reached a level of proficiency where they are capable of\nsuccessfully completing university exams across several disciplines and\ngenerating functional code to handle novel problems. This research investigates\nthe coding proficiency of ChatGPT 3.5, a LLM released by OpenAI in November\n2022, which has gained significant recognition for its impressive text\ngenerating and code creation capabilities. The skill of the model in creating\ncode snippets is evaluated across 10 various programming languages and 4\ndifferent software domains. Based on the findings derived from this research,\nmajor unexpected behaviors and limitations of the model have been identified.\nThis study aims to identify potential areas for development and examine the\nramifications of automated code generation on the evolution of programming\nlanguages and on the tech industry.",
        "pdf_link": "https://arxiv.org/pdf/2308.04477v1.pdf"
    },
    {
        "title": "AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models",
        "authors": [
            "Zhu Deng",
            "Jinjie Liu",
            "Biao Luo",
            "Can Yuan",
            "Qingrun Yang",
            "Lei Xiao",
            "Wenwen Zhou",
            "Zhu Liu"
        ],
        "published": "2023-08-08T13:12:03Z",
        "summary": "The product carbon footprint (PCF) is crucial for decarbonizing the supply\nchain, as it measures the direct and indirect greenhouse gas emissions caused\nby all activities during the product's life cycle. However, PCF accounting\noften requires expert knowledge and significant time to construct life cycle\nmodels. In this study, we test and compare the emergent ability of five large\nlanguage models (LLMs) in modeling the 'cradle-to-gate' life cycles of products\nand generating the inventory data of inputs and outputs, revealing their\nlimitations as a generalized PCF knowledge database. By utilizing LLMs, we\npropose an automatic AI-driven PCF accounting framework, called AutoPCF, which\nalso applies deep learning algorithms to automatically match calculation\nparameters, and ultimately calculate the PCF. The results of estimating the\ncarbon footprint for three case products using the AutoPCF framework\ndemonstrate its potential in achieving automatic modeling and estimation of PCF\nwith a large reduction in modeling time from days to minutes.",
        "pdf_link": "https://arxiv.org/pdf/2308.04241v2.pdf"
    },
    {
        "title": "Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance",
        "authors": [
            "Menglin Xia",
            "Xuchao Zhang",
            "Camille Couturier",
            "Guoqing Zheng",
            "Saravan Rajmohan",
            "Victor Ruhle"
        ],
        "published": "2023-08-08T12:27:20Z",
        "summary": "Retrieval augmentation enhances performance of traditional language models by\nincorporating additional context. However, the computational demands for\nretrieval augmented large language models (LLMs) pose a challenge when applying\nthem to real-time tasks, such as composition assistance. To address this\nlimitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG)\nframework, a novel approach that efficiently combines a cloud-based LLM with a\nsmaller, client-side, language model through retrieval augmented memory. This\nintegration enables the client model to generate effective responses,\nbenefiting from the LLM's capabilities and contextual information.\nAdditionally, through an asynchronous memory update mechanism, the client model\ncan deliver real-time completions swiftly to user inputs without the need to\nwait for responses from the cloud. Our experiments on five benchmark datasets\ndemonstrate that HybridRAG significantly improves utility over client-only\nmodels while maintaining low latency.",
        "pdf_link": "https://arxiv.org/pdf/2308.04215v2.pdf"
    },
    {
        "title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation",
        "authors": [
            "Jiaju Lin",
            "Haoran Zhao",
            "Aochi Zhang",
            "Yiting Wu",
            "Huqiuyue Ping",
            "Qin Chen"
        ],
        "published": "2023-08-08T03:59:28Z",
        "summary": "With ChatGPT-like large language models (LLM) prevailing in the community,\nhow to evaluate the ability of LLMs is an open question. Existing evaluation\nmethods suffer from following shortcomings: (1) constrained evaluation\nabilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that\ntask-based evaluation, where LLM agents complete tasks in a simulated\nenvironment, is a one-for-all solution to solve above problems. We present\nAgentSims, an easy-to-use infrastructure for researchers from all disciplines\nto test the specific capacities they are interested in. Researchers can build\ntheir evaluation tasks by adding agents and buildings on an interactive GUI or\ndeploy and test new support mechanisms, i.e. memory, planning and tool-use\nsystems, by a few lines of codes. Our demo is available at\nhttps://agentsims.com .",
        "pdf_link": "https://arxiv.org/pdf/2308.04026v1.pdf"
    },
    {
        "title": "Revisiting Prompt Engineering via Declarative Crowdsourcing",
        "authors": [
            "Aditya G. Parameswaran",
            "Shreya Shankar",
            "Parth Asawa",
            "Naman Jain",
            "Yujie Wang"
        ],
        "published": "2023-08-07T18:04:12Z",
        "summary": "Large language models (LLMs) are incredibly powerful at comprehending and\ngenerating data in the form of text, but are brittle and error-prone. There has\nbeen an advent of toolkits and recipes centered around so-called prompt\nengineering-the process of asking an LLM to do something via a series of\nprompts. However, for LLM-powered data processing workflows, in particular,\noptimizing for quality, while keeping cost bounded, is a tedious, manual\nprocess. We put forth a vision for declarative prompt engineering. We view LLMs\nlike crowd workers and leverage ideas from the declarative crowdsourcing\nliterature-including leveraging multiple prompting strategies, ensuring\ninternal consistency, and exploring hybrid-LLM-non-LLM approaches-to make\nprompt engineering a more principled process. Preliminary case studies on\nsorting, entity resolution, and imputation demonstrate the promise of our\napproach",
        "pdf_link": "https://arxiv.org/pdf/2308.03854v1.pdf"
    },
    {
        "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
        "authors": [
            "Xinyue Shen",
            "Zeyuan Chen",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "published": "2023-08-07T16:55:20Z",
        "summary": "The misuse of large language models (LLMs) has garnered significant attention\nfrom the general public and LLM vendors. In response, efforts have been made to\nalign LLMs with human values and intent use. However, a particular type of\nadversarial prompts, known as jailbreak prompt, has emerged and continuously\nevolved to bypass the safeguards and elicit harmful content from LLMs. In this\npaper, we conduct the first measurement study on jailbreak prompts in the wild,\nwith 6,387 prompts collected from four platforms over six months. Leveraging\nnatural language processing technologies and graph-based community detection\nmethods, we discover unique characteristics of jailbreak prompts and their\nmajor attack strategies, such as prompt injection and privilege escalation. We\nalso observe that jailbreak prompts increasingly shift from public platforms to\nprivate ones, posing new challenges for LLM vendors in proactive detection. To\nassess the potential harm caused by jailbreak prompts, we create a question set\ncomprising 46,800 samples across 13 forbidden scenarios. Our experiments show\nthat current LLMs and safeguards cannot adequately defend jailbreak prompts in\nall scenarios. Particularly, we identify two highly effective jailbreak prompts\nwhich achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and\nthey have persisted online for over 100 days. Our work sheds light on the\nsevere and evolving threat landscape of jailbreak prompts. We hope our study\ncan facilitate the research community and LLM vendors in promoting safer and\nregulated LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.03825v1.pdf"
    },
    {
        "title": "AgentBench: Evaluating LLMs as Agents",
        "authors": [
            "Xiao Liu",
            "Hao Yu",
            "Hanchen Zhang",
            "Yifan Xu",
            "Xuanyu Lei",
            "Hanyu Lai",
            "Yu Gu",
            "Hangliang Ding",
            "Kaiwen Men",
            "Kejuan Yang",
            "Shudan Zhang",
            "Xiang Deng",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chenhui Zhang",
            "Sheng Shen",
            "Tianjun Zhang",
            "Yu Su",
            "Huan Sun",
            "Minlie Huang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2023-08-07T16:08:11Z",
        "summary": "Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent's reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 27 API-based and\nopen-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and OSS competitors. We identify the\ntypical reasons of failures in environments and LLMs, showing that poor\nlong-term reasoning, decision-making, and instruction following abilities are\nthe main obstacles for developing usable LLM agents. Training on code and high\nquality multi-turn alignment data could improve agent performance. Datasets,\nenvironments, and an integrated evaluation package for AgentBench are released\nat \\url{https://github.com/THUDM/AgentBench}.",
        "pdf_link": "https://arxiv.org/pdf/2308.03688v2.pdf"
    },
    {
        "title": "Learning Concise and Descriptive Attributes for Visual Recognition",
        "authors": [
            "An Yan",
            "Yu Wang",
            "Yiwu Zhong",
            "Chengyu Dong",
            "Zexue He",
            "Yujie Lu",
            "William Wang",
            "Jingbo Shang",
            "Julian McAuley"
        ],
        "published": "2023-08-07T16:00:22Z",
        "summary": "Recent advances in foundation models present new opportunities for\ninterpretable visual recognition -- one can first query Large Language Models\n(LLMs) to obtain a set of attributes that describe each class, then apply\nvision-language models to classify images via these attributes. Pioneering work\nshows that querying thousands of attributes can achieve performance competitive\nwith image features. However, our further investigation on 8 datasets reveals\nthat LLM-generated attributes in a large quantity perform almost the same as\nrandom words. This surprising finding suggests that significant noise may be\npresent in these attributes. We hypothesize that there exist subsets of\nattributes that can maintain the classification performance with much smaller\nsizes, and propose a novel learning-to-search method to discover those concise\nsets of attributes. As a result, on the CUB dataset, our method achieves\nperformance close to that of massive LLM-generated attributes (e.g., 10k\nattributes for CUB), yet using only 32 attributes in total to distinguish 200\nbird species. Furthermore, our new paradigm demonstrates several additional\nbenefits: higher interpretability and interactivity for humans, and the ability\nto summarize knowledge for a recognition task.",
        "pdf_link": "https://arxiv.org/pdf/2308.03685v1.pdf"
    },
    {
        "title": "MedMine: Examining Pre-trained Language Models on Medication Mining",
        "authors": [
            "Haifa Alrdahi",
            "Lifeng Han",
            "Hendrik \u0160uvalov",
            "Goran Nenadic"
        ],
        "published": "2023-08-07T14:36:03Z",
        "summary": "Automatic medication mining from clinical and biomedical text has become a\npopular topic due to its real impact on healthcare applications and the recent\ndevelopment of powerful language models (LMs). However, fully-automatic\nextraction models still face obstacles to be overcome such that they can be\ndeployed directly into clinical practice for better impacts. Such obstacles\ninclude their imbalanced performances on different entity types and clinical\nevents. In this work, we examine current state-of-the-art pre-trained language\nmodels (PLMs) on such tasks, via fine-tuning including the monolingual model\nMed7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their\nadvantages and drawbacks using historical medication mining shared task data\nsets from n2c2-2018 challenges. We report the findings we get from these\nfine-tuning experiments such that they can facilitate future research on\naddressing them, for instance, how to combine their outputs, merge such models,\nor improve their overall accuracy by ensemble learning and data augmentation.\nMedMine is part of the M3 Initiative \\url{https://github.com/HECTA-UoM/M3}",
        "pdf_link": "https://arxiv.org/pdf/2308.03629v2.pdf"
    },
    {
        "title": "Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing",
        "authors": [
            "Wai Man Si",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2023-08-07T13:10:35Z",
        "summary": "The Machine Learning as a Service (MLaaS) market is rapidly expanding and\nbecoming more mature. For example, OpenAI's ChatGPT is an advanced large\nlanguage model (LLM) that generates responses for various queries with\nassociated fees. Although these models can deliver satisfactory performance,\nthey are far from perfect. Researchers have long studied the vulnerabilities\nand limitations of LLMs, such as adversarial attacks and model toxicity.\nInevitably, commercial ML models are also not exempt from such issues, which\ncan be problematic as MLaaS continues to grow. In this paper, we discover a new\nattack strategy against LLM APIs, namely the prompt abstraction attack.\nSpecifically, we propose Mondrian, a simple and straightforward method that\nabstracts sentences, which can lower the cost of using LLM APIs. In this\napproach, the adversary first creates a pseudo API (with a lower established\nprice) to serve as the proxy of the target API (with a higher established\nprice). Next, the pseudo API leverages Mondrian to modify the user query,\nobtain the abstracted response from the target API, and forward it back to the\nend user. Our results show that Mondrian successfully reduces user queries'\ntoken length ranging from 13% to 23% across various tasks, including text\nclassification, generation, and question answering. Meanwhile, these abstracted\nqueries do not significantly affect the utility of task-specific and general\nlanguage models like ChatGPT. Mondrian also reduces instruction prompts' token\nlength by at least 11% without compromising output quality. As a result, the\nprompt abstraction attack enables the adversary to profit without bearing the\ncost of API development and deployment.",
        "pdf_link": "https://arxiv.org/pdf/2308.03558v1.pdf"
    },
    {
        "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models",
        "authors": [
            "Seungcheol Park",
            "Hojun Choi",
            "U Kang"
        ],
        "published": "2023-08-07T10:11:42Z",
        "summary": "Given a pretrained encoder-based language model, how can we accurately\ncompress it without retraining? Retraining-free structured pruning algorithms\nare crucial in pretrained language model compression due to their significantly\nreduced pruning cost and capability to prune large language models. However,\nexisting retraining-free algorithms encounter severe accuracy degradation, as\nthey fail to handle pruning errors, especially at high compression rates. In\nthis paper, we propose K-prune (Knowledge-preserving pruning), an accurate\nretraining-free structured pruning algorithm for pretrained encoder-based\nlanguage models. K-prune focuses on preserving the useful knowledge of the\npretrained model to minimize pruning errors through a carefully designed\niterative pruning process composed of knowledge measurement,\nknowledge-preserving mask search, and knowledge-preserving weight-tuning. As a\nresult, K-prune shows significant accuracy improvements up to 58.02%p higher F1\nscore compared to existing retraining-free pruning algorithms under a high\ncompression rate of 80% on the SQuAD benchmark without any retraining process.",
        "pdf_link": "https://arxiv.org/pdf/2308.03449v2.pdf"
    },
    {
        "title": "Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records",
        "authors": [
            "Shivani Shekhar",
            "Simran Tiwari",
            "T. C. Rensink",
            "Ramy Eskander",
            "Wael Salloum"
        ],
        "published": "2023-08-07T07:29:49Z",
        "summary": "The application of Artificial Intelligence (AI) in healthcare has been\nrevolutionary, especially with the recent advancements in transformer-based\nLarge Language Models (LLMs). However, the task of understanding unstructured\nelectronic medical records remains a challenge given the nature of the records\n(e.g., disorganization, inconsistency, and redundancy) and the inability of\nLLMs to derive reasoning paradigms that allow for comprehensive understanding\nof medical variables. In this work, we examine the power of coupling symbolic\nreasoning with language modeling toward improved understanding of unstructured\nclinical texts. We show that such a combination improves the extraction of\nseveral medical variables from unstructured records. In addition, we show that\nthe state-of-the-art commercially-free LLMs enjoy retrieval capabilities\ncomparable to those provided by their commercial counterparts. Finally, we\nelaborate on the need for LLM steering through the application of symbolic\nreasoning as the exclusive use of LLMs results in the lowest performance.",
        "pdf_link": "https://arxiv.org/pdf/2308.03360v1.pdf"
    },
    {
        "title": "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis",
        "authors": [
            "Yuqiang Sun",
            "Daoyuan Wu",
            "Yue Xue",
            "Han Liu",
            "Haijun Wang",
            "Zhengzi Xu",
            "Xiaofei Xie",
            "Yang Liu"
        ],
        "published": "2023-08-07T05:48:53Z",
        "summary": "Smart contracts are prone to various vulnerabilities, leading to substantial\nfinancial losses over time. Current analysis tools mainly target\nvulnerabilities with fixed control or data-flow patterns, such as re-entrancy\nand integer overflow. However, a recent study on Web3 security bugs revealed\nthat about 80% of these bugs cannot be audited by existing tools due to the\nlack of domain-specific property description and checking. Given recent\nadvances in Large Language Models (LLMs), it is worth exploring how Generative\nPre-training Transformer (GPT) could aid in detecting logicc vulnerabilities.\n  In this paper, we propose GPTScan, the first tool combining GPT with static\nanalysis for smart contract logic vulnerability detection. Instead of relying\nsolely on GPT to identify vulnerabilities, which can lead to high false\npositives and is limited by GPT's pre-trained knowledge, we utilize GPT as a\nversatile code understanding tool. By breaking down each logic vulnerability\ntype into scenarios and properties, GPTScan matches candidate vulnerabilities\nwith GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently\nrecognize key variables and statements, which are then validated by static\nconfirmation. Evaluation on diverse datasets with around 400 contract projects\nand 3K Solidity files shows that GPTScan achieves high precision (over 90%) for\ntoken contracts and acceptable precision (57.14%) for large projects like\nWeb3Bugs. It effectively detects ground-truth logic vulnerabilities with a\nrecall of over 70%, including 9 new vulnerabilities missed by human auditors.\nGPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01\nUSD to scan per thousand lines of Solidity code. Moreover, static confirmation\nhelps GPTScan reduce two-thirds of false positives.",
        "pdf_link": "https://arxiv.org/pdf/2308.03314v2.pdf"
    },
    {
        "title": "Exploiting Code Symmetries for Learning Program Semantics",
        "authors": [
            "Kexin Pei",
            "Weichen Li",
            "Qirui Jin",
            "Shuyang Liu",
            "Scott Geng",
            "Lorenzo Cavallaro",
            "Junfeng Yang",
            "Suman Jana"
        ],
        "published": "2023-08-07T05:40:58Z",
        "summary": "This paper tackles the challenge of teaching code semantics to Large Language\nModels (LLMs) for program analysis by incorporating code symmetries into the\nmodel architecture. We introduce a group-theoretic framework that defines code\nsymmetries as semantics-preserving transformations, where forming a code\nsymmetry group enables precise and efficient reasoning of code semantics. Our\nsolution, SymC, develops a novel variant of self-attention that is provably\nequivariant to code symmetries from the permutation group defined over the\nprogram dependence graph. SymC obtains superior performance on five program\nanalysis tasks, outperforming state-of-the-art code models, including GPT-4,\nwithout any pre-training. Our results suggest that code LLMs that encode the\ncode structural prior via the code symmetry group generalize better and faster.",
        "pdf_link": "https://arxiv.org/pdf/2308.03312v7.pdf"
    },
    {
        "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning",
        "authors": [
            "Longteng Zhang",
            "Lin Zhang",
            "Shaohuai Shi",
            "Xiaowen Chu",
            "Bo Li"
        ],
        "published": "2023-08-07T05:12:27Z",
        "summary": "The low-rank adaptation (LoRA) method can largely reduce the amount of\ntrainable parameters for fine-tuning large language models (LLMs), however, it\nstill requires expensive activation memory to update low-rank weights. Reducing\nthe number of LoRA layers or using activation recomputation could harm the\nfine-tuning performance or increase the computational overhead. In this work,\nwe present LoRA-FA, a memory-efficient fine-tuning method that reduces the\nactivation memory without performance degradation and expensive recomputation.\nLoRA-FA chooses to freeze the projection-down weight of $A$ and update the\nprojection-up weight of $B$ in each LoRA layer. It ensures the change of model\nweight reside in a low-rank space during LLMs fine-tuning, while eliminating\nthe requirement to store full-rank input activations. We conduct extensive\nexperiments across multiple model types (RoBERTa, T5, LLaMA) and model scales.\nOur results show that LoRA-FA can always achieve close fine-tuning accuracy\nacross different tasks compared to full parameter fine-tuning and LoRA.\nFurthermore, LoRA-FA can reduce the overall memory cost by up to 1.4$\\times$\ncompared to LoRA.",
        "pdf_link": "https://arxiv.org/pdf/2308.03303v1.pdf"
    },
    {
        "title": "Studying Large Language Model Generalization with Influence Functions",
        "authors": [
            "Roger Grosse",
            "Juhan Bae",
            "Cem Anil",
            "Nelson Elhage",
            "Alex Tamkin",
            "Amirhossein Tajdini",
            "Benoit Steiner",
            "Dustin Li",
            "Esin Durmus",
            "Ethan Perez",
            "Evan Hubinger",
            "Kamil\u0117 Luko\u0161i\u016bt\u0117",
            "Karina Nguyen",
            "Nicholas Joseph",
            "Sam McCandlish",
            "Jared Kaplan",
            "Samuel R. Bowman"
        ],
        "published": "2023-08-07T04:47:42Z",
        "summary": "When trying to gain better visibility into a machine learning model in order\nto understand and mitigate the associated risks, a potentially valuable source\nof evidence is: which training examples most contribute to a given behavior?\nInfluence functions aim to answer a counterfactual: how would the model's\nparameters (and hence its outputs) change if a given sequence were added to the\ntraining set? While influence functions have produced insights for small\nmodels, they are difficult to scale to large language models (LLMs) due to the\ndifficulty of computing an inverse-Hessian-vector product (IHVP). We use the\nEigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)\napproximation to scale influence functions up to LLMs with up to 52 billion\nparameters. In our experiments, EK-FAC achieves similar accuracy to traditional\ninfluence function estimators despite the IHVP computation being orders of\nmagnitude faster. We investigate two algorithmic techniques to reduce the cost\nof computing gradients of candidate training sequences: TF-IDF filtering and\nquery batching. We use influence functions to investigate the generalization\npatterns of LLMs, including the sparsity of the influence patterns, increasing\nabstraction with scale, math and programming abilities, cross-lingual\ngeneralization, and role-playing behavior. Despite many apparently\nsophisticated forms of generalization, we identify a surprising limitation:\ninfluences decay to near-zero when the order of key phrases is flipped.\nOverall, influence functions give us a powerful new tool for studying the\ngeneralization properties of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.03296v1.pdf"
    },
    {
        "title": "Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)",
        "authors": [
            "Jordan Kodner",
            "Sarah Payne",
            "Jeffrey Heinz"
        ],
        "published": "2023-08-06T23:41:14Z",
        "summary": "We present a critical assessment of Piantadosi's (2023) claim that \"Modern\nlanguage models refute Chomsky's approach to language,\" focusing on four main\npoints. First, despite the impressive performance and utility of large language\nmodels (LLMs), humans achieve their capacity for language after exposure to\nseveral orders of magnitude less data. The fact that young children become\ncompetent, fluent speakers of their native languages with relatively little\nexposure to them is the central mystery of language learning to which Chomsky\ninitially drew attention, and LLMs currently show little promise of solving\nthis mystery. Second, what can the artificial reveal about the natural? Put\nsimply, the implications of LLMs for our understanding of the cognitive\nstructures and mechanisms underlying language and its acquisition are like the\nimplications of airplanes for understanding how birds fly. Third, LLMs cannot\nconstitute scientific theories of language for several reasons, not least of\nwhich is that scientific theories must provide interpretable explanations, not\njust predictions. This leads to our final point: to even determine whether the\nlinguistic and cognitive capabilities of LLMs rival those of humans requires\nexplicating what humans' capacities actually are. In other words, it requires a\nseparate theory of language and cognition; generative linguistics provides\nprecisely such a theory. As such, we conclude that generative linguistics as a\nscientific discipline will remain indispensable throughout the 21st century and\nbeyond.",
        "pdf_link": "https://arxiv.org/pdf/2308.03228v1.pdf"
    },
    {
        "title": "LARCH: Large Language Model-based Automatic Readme Creation with Heuristics",
        "authors": [
            "Yuta Koreeda",
            "Terufumi Morishita",
            "Osamu Imaichi",
            "Yasuhiro Sogawa"
        ],
        "published": "2023-08-06T12:28:24Z",
        "summary": "Writing a readme is a crucial aspect of software development as it plays a\nvital role in managing and reusing program code. Though it is a pain point for\nmany developers, automatically creating one remains a challenge even with the\nrecent advancements in large language models (LLMs), because it requires\ngenerating an abstract description from thousands of lines of code. In this\ndemo paper, we show that LLMs are capable of generating a coherent and\nfactually correct readmes if we can identify a code fragment that is\nrepresentative of the repository. Building upon this finding, we developed\nLARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages\nrepresentative code identification with heuristics and weak supervision.\nThrough human and automated evaluations, we illustrate that LARCH can generate\ncoherent and factually correct readmes in the majority of cases, outperforming\na baseline that does not rely on representative code identification. We have\nmade LARCH open-source and provided a cross-platform Visual Studio Code\ninterface and command-line interface, accessible at\nhttps://github.com/hitachi-nlp/larch. A demo video showcasing LARCH's\ncapabilities is available at https://youtu.be/ZUKkh5ED-O4.",
        "pdf_link": "https://arxiv.org/pdf/2308.03099v2.pdf"
    },
    {
        "title": "TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties",
        "authors": [
            "Karima Kadaoui",
            "Samar M. Magdy",
            "Abdul Waheed",
            "Md Tawkat Islam Khondaker",
            "Ahmed Oumar El-Shangiti",
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023-08-06T08:29:16Z",
        "summary": "Despite the purported multilingual proficiency of instruction-finetuned large\nlanguage models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of\nthese models remains insufficiently explored. Considering this constraint, we\npresent a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5\nand GPT-4) regarding their machine translation proficiencies across ten\nvarieties of Arabic. Our evaluation covers diverse Arabic varieties such as\nClassical Arabic (CA), Modern Standard Arabic (MSA), and several country-level\ndialectal variants. Our analysis indicates that LLMs may encounter challenges\nwith dialects for which minimal public datasets exist, but on average are\nbetter translators of dialects than existing commercial systems. On CA and MSA,\ninstruction-tuned LLMs, however, trail behind commercial systems such as Google\nTranslate. Finally, we undertake a human-centric study to scrutinize the\nefficacy of the relatively recent model, Bard, in following human instructions\nduring translation tasks. Our analysis reveals a circumscribed capability of\nBard in aligning with human instructions in translation contexts. Collectively,\nour findings underscore that prevailing LLMs remain far from inclusive, with\nonly limited ability to cater for the linguistic and cultural intricacies of\ndiverse communities.",
        "pdf_link": "https://arxiv.org/pdf/2308.03051v2.pdf"
    },
    {
        "title": "An Empirical Study of AI-based Smart Contract Creation",
        "authors": [
            "Rabimba Karanjai",
            "Edward Li",
            "Lei Xu",
            "Weidong Shi"
        ],
        "published": "2023-08-05T21:38:57Z",
        "summary": "The introduction of large language models (LLMs) like ChatGPT and Google\nPalm2 for smart contract generation seems to be the first well-established\ninstance of an AI pair programmer. LLMs have access to a large number of\nopen-source smart contracts, enabling them to utilize more extensive code in\nSolidity than other code generation tools. Although the initial and informal\nassessments of LLMs for smart contract generation are promising, a systematic\nevaluation is needed to explore the limits and benefits of these models. The\nmain objective of this study is to assess the quality of generated code\nprovided by LLMs for smart contracts. We also aim to evaluate the impact of the\nquality and variety of input parameters fed to LLMs. To achieve this aim, we\ncreated an experimental setup for evaluating the generated code in terms of\nvalidity, correctness, and efficiency. Our study finds crucial evidence of\nsecurity bugs getting introduced in the generated smart contracts as well as\nthe overall quality and correctness of the code getting impacted. However, we\nalso identified the areas where it can be improved. The paper also proposes\nseveral potential research directions to improve the process, quality and\nsafety of generated smart contract codes.",
        "pdf_link": "https://arxiv.org/pdf/2308.02955v2.pdf"
    },
    {
        "title": "Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology",
        "authors": [
            "Cliff Wong",
            "Sheng Zhang",
            "Yu Gu",
            "Christine Moung",
            "Jacob Abel",
            "Naoto Usuyama",
            "Roshanthi Weerasinghe",
            "Brian Piening",
            "Tristan Naumann",
            "Carlo Bifulco",
            "Hoifung Poon"
        ],
        "published": "2023-08-04T07:51:15Z",
        "summary": "Clinical trial matching is a key process in health delivery and discovery. In\npractice, it is plagued by overwhelming unstructured data and unscalable manual\nprocessing. In this paper, we conduct a systematic study on scaling clinical\ntrial matching using large language models (LLMs), with oncology as the focus\narea. Our study is grounded in a clinical trial matching system currently in\ntest deployment at a large U.S. health network. Initial findings are promising:\nout of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate\neligibility criteria of clinical trials and extract complex matching logic\n(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially\noutperform prior strong baselines and may serve as a preliminary solution to\nhelp triage patient-trial candidates with humans in the loop. Our study also\nreveals a few significant growth areas for applying LLMs to end-to-end clinical\ntrial matching, such as context limitation and accuracy, especially in\nstructuring patient information from longitudinal medical records.",
        "pdf_link": "https://arxiv.org/pdf/2308.02180v3.pdf"
    },
    {
        "title": "The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations",
        "authors": [
            "Abel Salinas",
            "Parth Vipul Shah",
            "Yuzhong Huang",
            "Robert McCormack",
            "Fred Morstatter"
        ],
        "published": "2023-08-03T21:12:54Z",
        "summary": "Large Language Models (LLMs) have seen widespread deployment in various\nreal-world applications. Understanding these biases is crucial to comprehend\nthe potential downstream consequences when using LLMs to make decisions,\nparticularly for historically disadvantaged groups. In this work, we propose a\nsimple method for analyzing and comparing demographic bias in LLMs, through the\nlens of job recommendations. We demonstrate the effectiveness of our method by\nmeasuring intersectional biases within ChatGPT and LLaMA, two cutting-edge\nLLMs. Our experiments primarily focus on uncovering gender identity and\nnationality bias; however, our method can be extended to examine biases\nassociated with any intersection of demographic identities. We identify\ndistinct biases in both models toward various demographic identities, such as\nboth models consistently suggesting low-paying jobs for Mexican workers or\npreferring to recommend secretarial roles to women. Our study highlights the\nimportance of measuring the bias of LLMs in downstream applications to\nunderstand the potential for harm and inequitable outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2308.02053v2.pdf"
    },
    {
        "title": "Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models",
        "authors": [
            "Mahammed Kamruzzaman",
            "Gene Louis Kim"
        ],
        "published": "2023-08-03T20:29:27Z",
        "summary": "While reaching for NLP systems that maximize accuracy, other important\nmetrics of system performance are often overlooked. Prior models are easily\nforgotten despite their possible suitability in settings where large computing\nresources are unavailable or relatively more costly. In this paper, we perform\na broad comparative evaluation of document-level sentiment analysis models with\na focus on resource costs that are important for the feasibility of model\ndeployment and general climate consciousness. Our experiments consider\ndifferent feature extraction techniques, the effect of ensembling,\ntask-specific deep learning modeling, and domain-independent large language\nmodels (LLMs). We find that while a fine-tuned LLM achieves the best accuracy,\nsome alternate configurations provide huge (up to 24, 283 *) resource savings\nfor a marginal (<1%) loss in accuracy. Furthermore, we find that for smaller\ndatasets, the differences in accuracy shrink while the difference in resource\nconsumption grows further.",
        "pdf_link": "https://arxiv.org/pdf/2308.02022v1.pdf"
    },
    {
        "title": "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation",
        "authors": [
            "Xueying Du",
            "Mingwei Liu",
            "Kaixin Wang",
            "Hanlin Wang",
            "Junwei Liu",
            "Yixuan Chen",
            "Jiayi Feng",
            "Chaofeng Sha",
            "Xin Peng",
            "Yiling Lou"
        ],
        "published": "2023-08-03T16:31:02Z",
        "summary": "In this work, we make the first attempt to evaluate LLMs in a more\nchallenging code generation scenario, i.e. class-level code generation. We\nfirst manually construct the first class-level code generation benchmark\nClassEval of 100 class-level Python code generation tasks with approximately\n500 person-hours. Based on it, we then perform the first study of 11\nstate-of-the-art LLMs on class-level code generation. Based on our results, we\nhave the following main findings. First, we find that all existing LLMs show\nmuch worse performance on class-level code generation compared to on standalone\nmethod-level code generation benchmarks like HumanEval; and the method-level\ncoding ability cannot equivalently reflect the class-level coding ability among\nLLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior\nthan other LLMs on class-level code generation, and the second-tier models\nincludes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very\nsimilar performance. Third, we find that generating the entire class all at\nonce (i.e. holistic generation strategy) is the best generation strategy only\nfor GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and\ncompositional) is better strategies for the other models with limited ability\nof understanding long instructions and utilizing the middle information.\nLastly, we find the limited model ability of generating method-dependent code\nand discuss the frequent error types in generated classes. Our benchmark is\navailable at https://github.com/FudanSELab/ClassEval.",
        "pdf_link": "https://arxiv.org/pdf/2308.01861v2.pdf"
    },
    {
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
        "authors": [
            "Zheng Yuan",
            "Hongyi Yuan",
            "Chengpeng Li",
            "Guanting Dong",
            "Keming Lu",
            "Chuanqi Tan",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "published": "2023-08-03T15:34:01Z",
        "summary": "Mathematical reasoning is a challenging task for large language models\n(LLMs), while the scaling relationship of it with respect to LLM capacity is\nunder-explored. In this paper, we investigate how the pre-training loss,\nsupervised data amount, and augmented data amount influence the reasoning\nperformances of a supervised LLM. We find that pre-training loss is a better\nindicator of the model's performance than the model's parameter count. We apply\nsupervised fine-tuning (SFT) with different amounts of supervised data and\nempirically find a log-linear relation between data amount and model\nperformance, and we find better models improve less with enlarged supervised\ndatasets. To augment more data samples for improving model performances without\nany human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT\nuses supervised models to generate and collect correct reasoning paths as\naugmented fine-tuning datasets. We find with augmented samples containing more\ndistinct reasoning paths, RFT improves mathematical reasoning performance more\nfor LLMs. We also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push\nLLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised\nfine-tuning (SFT) accuracy of 35.9\\% significantly.",
        "pdf_link": "https://arxiv.org/pdf/2308.01825v2.pdf"
    },
    {
        "title": "Does Correction Remain A Problem For Large Language Models?",
        "authors": [
            "Xiaowu Zhang",
            "Xiaotian Zhang",
            "Cheng Yang",
            "Hang Yan",
            "Xipeng Qiu"
        ],
        "published": "2023-08-03T14:09:31Z",
        "summary": "As large language models, such as GPT, continue to advance the capabilities\nof natural language processing (NLP), the question arises: does the problem of\ncorrection still persist? This paper investigates the role of correction in the\ncontext of large language models by conducting two experiments. The first\nexperiment focuses on correction as a standalone task, employing few-shot\nlearning techniques with GPT-like models for error correction. The second\nexperiment explores the notion of correction as a preparatory task for other\nNLP tasks, examining whether large language models can tolerate and perform\nadequately on texts containing certain levels of noise or errors. By addressing\nthese experiments, we aim to shed light on the significance of correction in\nthe era of large language models and its implications for various NLP\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2308.01776v2.pdf"
    },
    {
        "title": "Evaluating ChatGPT text-mining of clinical records for obesity monitoring",
        "authors": [
            "Ivo S. Fins",
            "Heather Davies",
            "Sean Farrell",
            "Jose R. Torres",
            "Gina Pinchbeck",
            "Alan D. Radford",
            "Peter-John Noble"
        ],
        "published": "2023-08-03T10:11:42Z",
        "summary": "Background: Veterinary clinical narratives remain a largely untapped resource\nfor addressing complex diseases. Here we compare the ability of a large\nlanguage model (ChatGPT) and a previously developed regular expression (RegexT)\nto identify overweight body condition scores (BCS) in veterinary narratives.\nMethods: BCS values were extracted from 4,415 anonymised clinical narratives\nusing either RegexT or by appending the narrative to a prompt sent to ChatGPT\ncoercing the model to return the BCS information. Data were manually reviewed\nfor comparison. Results: The precision of RegexT was higher (100%, 95% CI\n94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall\nof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of\nRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is\nneeded to improve ChatGPT output. Conclusions: Large language models create\ndiverse opportunities and, whilst complex, present an intuitive interface to\ninformation but require careful implementation to avoid unpredictable errors.",
        "pdf_link": "https://arxiv.org/pdf/2308.01666v1.pdf"
    },
    {
        "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
        "authors": [
            "Paul R\u00f6ttger",
            "Hannah Rose Kirk",
            "Bertie Vidgen",
            "Giuseppe Attanasio",
            "Federico Bianchi",
            "Dirk Hovy"
        ],
        "published": "2023-08-02T16:30:40Z",
        "summary": "Without proper safeguards, large language models will readily follow\nmalicious instructions and generate toxic content. This risk motivates safety\nefforts such as red-teaming and large-scale feedback learning, which aim to\nmake models both helpful and harmless. However, there is a tension between\nthese two objectives, since harmlessness requires models to refuse to comply\nwith unsafe prompts, and thus not be helpful. Recent anecdotal evidence\nsuggests that some models may have struck a poor balance, so that even clearly\nsafe prompts are refused if they use similar language to unsafe prompts or\nmention sensitive topics. In this paper, we introduce a new test suite called\nXSTest to identify such eXaggerated Safety behaviours in a systematic way.\nXSTest comprises 250 safe prompts across ten prompt types that well-calibrated\nmodels should not refuse to comply with, and 200 unsafe prompts as contrasts\nthat models, for most applications, should refuse. We describe XSTest's\ncreation and composition, and then use the test suite to highlight systematic\nfailure modes in state-of-the-art language models as well as more general\nchallenges in building safer language models.",
        "pdf_link": "https://arxiv.org/pdf/2308.01263v3.pdf"
    },
    {
        "title": "Towards More Human-like AI Communication: A Review of Emergent Communication Research",
        "authors": [
            "Nicolo' Brandizzi"
        ],
        "published": "2023-08-01T14:43:10Z",
        "summary": "In the recent shift towards human-centric AI, the need for machines to\naccurately use natural language has become increasingly important. While a\ncommon approach to achieve this is to train large language models, this method\npresents a form of learning misalignment where the model may not capture the\nunderlying structure and reasoning humans employ in using natural language,\npotentially leading to unexpected or unreliable behavior. Emergent\ncommunication (Emecom) is a field of research that has seen a growing number of\npublications in recent years, aiming to develop artificial agents capable of\nusing natural language in a way that goes beyond simple discriminative tasks\nand can effectively communicate and learn new concepts. In this review, we\npresent Emecom under two aspects. Firstly, we delineate all the common\nproprieties we find across the literature and how they relate to human\ninteractions. Secondly, we identify two subcategories and highlight their\ncharacteristics and open challenges. We encourage researchers to work together\nby demonstrating that different methods can be viewed as diverse solutions to a\ncommon problem and emphasize the importance of including diverse perspectives\nand expertise in the field. We believe a deeper understanding of human\ncommunication is crucial to developing machines that can accurately use natural\nlanguage in human-machine interactions.",
        "pdf_link": "https://arxiv.org/pdf/2308.02541v1.pdf"
    },
    {
        "title": "Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education",
        "authors": [
            "S. S. Manathunga",
            "Y. A. Illangasekara"
        ],
        "published": "2023-08-01T12:04:50Z",
        "summary": "Large Language Models are increasingly being used for various tasks including\ncontent generation and as chatbots. Despite their impressive performances in\ngeneral tasks, LLMs need to be aligned when applying for domain specific tasks\nto mitigate the problems of hallucination and producing harmful answers.\nRetrieval Augmented Generation (RAG) allows to easily attach and manipulate a\nnon-parametric knowledgebases to LLMs. Applications of RAG in the field of\nmedical education are discussed in this paper. A combined extractive and\nabstractive summarization method for large unstructured textual data using\nrepresentative vectors is proposed.",
        "pdf_link": "https://arxiv.org/pdf/2308.00479v1.pdf"
    },
    {
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
        "authors": [
            "Ning Miao",
            "Yee Whye Teh",
            "Tom Rainforth"
        ],
        "published": "2023-08-01T10:31:36Z",
        "summary": "The recent progress in large language models (LLMs), especially the invention\nof chain-of-thought prompting, has made it possible to automatically answer\nquestions by stepwise reasoning. However, when faced with more complicated\nproblems that require non-linear thinking, even the strongest LLMs make\nmistakes. To address this, we explore whether LLMs are able to recognize errors\nin their own step-by-step reasoning, without resorting to external resources.\nTo this end, we propose SelfCheck, a general-purpose zero-shot verification\nschema for recognizing such errors. We then use the results of these checks to\nimprove question-answering performance by conducting weighted voting on\nmultiple solutions to the question. We test SelfCheck on three datasets (GSM8K,\nMathQA, and MATH) and find that it successfully recognizes errors and, in turn,\nincreases final answer accuracies.",
        "pdf_link": "https://arxiv.org/pdf/2308.00436v3.pdf"
    },
    {
        "title": "LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack",
        "authors": [
            "Hai Zhu",
            "Zhaoqing Yang",
            "Weiwei Shang",
            "Yuren Wu"
        ],
        "published": "2023-08-01T06:30:37Z",
        "summary": "Natural language processing models are vulnerable to adversarial examples.\nPrevious textual adversarial attacks adopt gradients or confidence scores to\ncalculate word importance ranking and generate adversarial examples. However,\nthis information is unavailable in the real world. Therefore, we focus on a\nmore realistic and challenging setting, named hard-label attack, in which the\nattacker can only query the model and obtain a discrete prediction label.\nExisting hard-label attack algorithms tend to initialize adversarial examples\nby random substitution and then utilize complex heuristic algorithms to\noptimize the adversarial perturbation. These methods require a lot of model\nqueries and the attack success rate is restricted by adversary initialization.\nIn this paper, we propose a novel hard-label attack algorithm named LimeAttack,\nwhich leverages a local explainable method to approximate word importance\nranking, and then adopts beam search to find the optimal solution. Extensive\nexperiments show that LimeAttack achieves the better attacking performance\ncompared with existing hard-label attack under the same query budget. In\naddition, we evaluate the effectiveness of LimeAttack on large language models,\nand results indicate that adversarial examples remain a significant threat to\nlarge language models. The adversarial examples crafted by LimeAttack are\nhighly transferable and effectively improve model robustness in adversarial\ntraining.",
        "pdf_link": "https://arxiv.org/pdf/2308.00319v2.pdf"
    },
    {
        "title": "Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting",
        "authors": [
            "Aseem Arora",
            "Shabbirhussain Bhaisaheb",
            "Harshit Nigam",
            "Manasi Patwardhan",
            "Lovekesh Vig",
            "Gautam Shroff"
        ],
        "published": "2023-08-01T05:31:36Z",
        "summary": "Cross-domain and cross-compositional generalization of Text-to-SQL semantic\nparsing is a challenging task. Existing Large Language Model (LLM) based\nsolutions rely on inference-time retrieval of few-shot exemplars from the\ntraining set to synthesize a run-time prompt for each Natural Language (NL)\ntest query. In contrast, we devise an algorithm which performs offline sampling\nof a minimal set-of few-shots from the training data, with complete coverage of\nSQL clauses, operators and functions, and maximal domain coverage within the\nallowed token length. This allows for synthesis of a fixed Generic Prompt (GP),\nwith a diverse set-of exemplars common across NL test queries, avoiding\nexpensive test time exemplar retrieval. We further auto-adapt the GP to the\ntarget database domain (DA-GP), to better handle cross-domain generalization;\nfollowed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle\ncross-compositional generalization. The synthesis of LTMP-DA-GP is an offline\ntask, to be performed one-time per new database with minimal human\nintervention. Our approach demonstrates superior performance on the KaggleDBQA\ndataset, designed to evaluate generalizability for the Text-to-SQL task. We\nfurther showcase consistent performance improvement of LTMP-DA-GP over GP,\nacross LLMs and databases of KaggleDBQA, highlighting the efficacy and model\nagnostic benefits of our prompt based adapt and decompose approach.",
        "pdf_link": "https://arxiv.org/pdf/2308.02582v3.pdf"
    },
    {
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
            "Haonan Li",
            "Yu Hao",
            "Yizhuo Zhai",
            "Zhiyun Qian"
        ],
        "published": "2023-08-01T02:57:43Z",
        "summary": "Static analysis is a widely used technique in software engineering for\nidentifying and mitigating bugs. However, a significant hurdle lies in\nachieving a delicate balance between precision and scalability. Large Language\nModels (LLMs) offer a promising alternative, as recent advances demonstrate\nremarkable capabilities in comprehending, generating, and even debugging code.\nYet, the logic of bugs can be complex and require sophisticated reasoning and a\nlarge analysis scope spanning multiple functions. Therefore, at this point,\nLLMs are better used in an assistive role to complement static analysis. In\nthis paper, we take a deep dive into the open space of LLM-assisted static\nanalysis, using use-before-initialization (UBI) bugs as a case study. To this\nend, we develop LLift, a fully automated framework that interfaces with both a\nstatic analysis tool and an LLM. By carefully designing the framework and the\nprompts, we are able to overcome a number of challenges, including bug-specific\nmodeling, the large problem scope, the non-deterministic nature of LLMs, etc.\nTested in a real-world scenario analyzing nearly a thousand potential UBI bugs\nproduced by static analysis, LLift demonstrates a potent capability, showcasing\na reasonable precision (50%) and appearing to have no missing bugs. It even\nidentified 13 previously unknown UBI bugs in the Linux kernel. This research\npaves the way for new opportunities and methodologies in using LLMs for bug\ndiscovery in extensive, real-world datasets.",
        "pdf_link": "https://arxiv.org/pdf/2308.00245v3.pdf"
    },
    {
        "title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
        "authors": [
            "Itay Itzhak",
            "Gabriel Stanovsky",
            "Nir Rosenfeld",
            "Yonatan Belinkov"
        ],
        "published": "2023-08-01T01:39:25Z",
        "summary": "Recent studies show that instruction tuning (IT) and reinforcement learning\nfrom human feedback (RLHF) improve the abilities of large language models (LMs)\ndramatically. While these tuning methods can help align models with human\nobjectives and generate high-quality text, not much is known about their\npotential adverse effects. In this work, we investigate the effect of IT and\nRLHF on decision making and reasoning in LMs, focusing on three cognitive\nbiases - the decoy effect, the certainty effect, and the belief bias - all of\nwhich are known to influence human decision-making and reasoning. Our findings\nhighlight the presence of these biases in various models from the GPT-3,\nMistral, and T5 families. Notably, we find a stronger presence of biases in\nmodels that have undergone instruction tuning, such as Flan-T5,\nMistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward\ncomprehending cognitive biases in instruction-tuned LMs, which is crucial for\nthe development of more reliable and unbiased language models.",
        "pdf_link": "https://arxiv.org/pdf/2308.00225v2.pdf"
    },
    {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution",
        "authors": [
            "Ehsan Kamalloo",
            "Aref Jafari",
            "Xinyu Zhang",
            "Nandan Thakur",
            "Jimmy Lin"
        ],
        "published": "2023-07-31T17:49:18Z",
        "summary": "The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2307.16883v1.pdf"
    },
    {
        "title": "Transferable Decoding with Visual Entities for Zero-Shot Image Captioning",
        "authors": [
            "Junjie Fei",
            "Teng Wang",
            "Jinrui Zhang",
            "Zhenyu He",
            "Chengjie Wang",
            "Feng Zheng"
        ],
        "published": "2023-07-31T09:47:06Z",
        "summary": "Image-to-text generation aims to describe images using natural language.\nRecently, zero-shot image captioning based on pre-trained vision-language\nmodels (VLMs) and large language models (LLMs) has made significant progress.\nHowever, we have observed and empirically demonstrated that these methods are\nsusceptible to modality bias induced by LLMs and tend to generate descriptions\ncontaining objects (entities) that do not actually exist in the image but\nfrequently appear during training (i.e., object hallucination). In this paper,\nwe propose ViECap, a transferable decoding model that leverages entity-aware\ndecoding to generate descriptions in both seen and unseen scenarios. ViECap\nincorporates entity-aware hard prompts to guide LLMs' attention toward the\nvisual entities present in the image, enabling coherent caption generation\nacross diverse scenes. With entity-aware hard prompts, ViECap is capable of\nmaintaining performance when transferring from in-domain to out-of-domain\nscenarios. Extensive experiments demonstrate that ViECap sets a new\nstate-of-the-art cross-domain (transferable) captioning and performs\ncompetitively in-domain captioning compared to previous VLMs-based zero-shot\nmethods. Our code is available at: https://github.com/FeiElysia/ViECap",
        "pdf_link": "https://arxiv.org/pdf/2307.16525v1.pdf"
    },
    {
        "title": "Deception Abilities Emerged in Large Language Models",
        "authors": [
            "Thilo Hagendorff"
        ],
        "published": "2023-07-31T09:27:01Z",
        "summary": "Large language models (LLMs) are currently at the forefront of intertwining\nartificial intelligence (AI) systems with human communication and everyday\nlife. Thus, aligning them with human values is of great importance. However,\ngiven the steady increase in reasoning abilities, future LLMs are under\nsuspicion of becoming able to deceive human operators and utilizing this\nability to bypass monitoring efforts. As a prerequisite to this, LLMs need to\npossess a conceptual understanding of deception strategies. This study reveals\nthat such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were\nnon-existent in earlier LLMs. We conduct a series of experiments showing that\nstate-of-the-art LLMs are able to understand and induce false beliefs in other\nagents, that their performance in complex deception scenarios can be amplified\nutilizing chain-of-thought reasoning, and that eliciting Machiavellianism in\nLLMs can alter their propensity to deceive. In sum, revealing hitherto unknown\nmachine behavior in LLMs, our study contributes to the nascent field of machine\npsychology.",
        "pdf_link": "https://arxiv.org/pdf/2307.16513v2.pdf"
    },
    {
        "title": "A Benchmark for Understanding Dialogue Safety in Mental Health Support",
        "authors": [
            "Huachuan Qiu",
            "Tong Zhao",
            "Anqi Li",
            "Shuai Zhang",
            "Hongliang He",
            "Zhenzhong Lan"
        ],
        "published": "2023-07-31T07:33:16Z",
        "summary": "Dialogue safety remains a pervasive challenge in open-domain human-machine\ninteraction. Existing approaches propose distinctive dialogue safety taxonomies\nand datasets for detecting explicitly harmful responses. However, these\ntaxonomies may not be suitable for analyzing response safety in mental health\nsupport. In real-world interactions, a model response deemed acceptable in\ncasual conversations might have a negligible positive impact on users seeking\nmental health support. To address these limitations, this paper aims to develop\na theoretically and factually grounded taxonomy that prioritizes the positive\nimpact on help-seekers. Additionally, we create a benchmark corpus with\nfine-grained labels for each dialogue session to facilitate further research.\nWe analyze the dataset using popular language models, including BERT-base,\nRoBERTa-large, and ChatGPT, to detect and understand unsafe responses within\nthe context of mental health support. Our study reveals that ChatGPT struggles\nto detect safety categories with detailed safety definitions in a zero- and\nfew-shot paradigm, whereas the fine-tuned model proves to be more suitable. The\ndeveloped dataset and findings serve as valuable benchmarks for advancing\nresearch on dialogue safety in mental health support, with significant\nimplications for improving the design and deployment of conversation agents in\nreal-world applications. We release our code and data here:\nhttps://github.com/qiuhuachuan/DialogueSafety.",
        "pdf_link": "https://arxiv.org/pdf/2307.16457v1.pdf"
    },
    {
        "title": "HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field",
        "authors": [
            "Mingliang Bai",
            "Zhihao Zhou",
            "Ruidong Wang",
            "Yusheng Yang",
            "Zizhen Qin",
            "Yunxiao Chen",
            "Chunjin Mu",
            "Jinfu Liu",
            "Daren Yu"
        ],
        "published": "2023-07-31T06:59:36Z",
        "summary": "Renewable energy is important for achieving carbon neutrality goal. With the\ngreat success of Large Language Models (LLMs) like ChatGPT in automatic content\ngeneration, LLMs are playing an increasingly important role. However, there has\nnot been a specially designed LLM for renewable energy. Meanwhile, there has\nnot been any dataset of renewable energy for training LLMs. Therefore, this\npaper published the first open-source Renewable Energy Academic Paper (REAP)\ndataset for non-commercial LLM research of renewable energy. REAP dataset is\ncollected through searching the title and abstract of 1,168,970 academic\nliteratures from Web of Science. Based on REAP dataset, HouYi model, the first\nLLM for renewable energy, is developed through finetuning general LLMs. HouYi\ndemonstrated powerful academic paper paragraph generation ability in renewable\nenergy field. Experiments show that its ability to generate academic papers on\nrenewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIE\nBot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.",
        "pdf_link": "https://arxiv.org/pdf/2308.01414v1.pdf"
    },
    {
        "title": "Distractor generation for multiple-choice questions with predictive prompting and large language models",
        "authors": [
            "Semere Kiros Bitew",
            "Johannes Deleu",
            "Chris Develder",
            "Thomas Demeester"
        ],
        "published": "2023-07-30T23:15:28Z",
        "summary": "Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable\nperformance across various tasks and have garnered significant attention from\nboth researchers and practitioners. However, in an educational context, we\nstill observe a performance gap in generating distractors -- i.e., plausible\nyet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In\nthis study, we propose a strategy for guiding LLMs such as ChatGPT, in\ngenerating relevant distractors by prompting them with question items\nautomatically retrieved from a question bank as well-chosen in-context\nexamples. We evaluate our LLM-based solutions using a quantitative assessment\non an existing test set, as well as through quality annotations by human\nexperts, i.e., teachers. We found that on average 53% of the generated\ndistractors presented to the teachers were rated as high-quality, i.e.,\nsuitable for immediate use as is, outperforming the state-of-the-art model. We\nalso show the gains of our approach 1 in generating high-quality distractors by\ncomparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with\nstatic examples.",
        "pdf_link": "https://arxiv.org/pdf/2307.16338v1.pdf"
    },
    {
        "title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models",
        "authors": [
            "Aiwei Liu",
            "Leyi Pan",
            "Xuming Hu",
            "Shu'ang Li",
            "Lijie Wen",
            "Irwin King",
            "Philip S. Yu"
        ],
        "published": "2023-07-30T13:43:27Z",
        "summary": "Recently, text watermarking algorithms for large language models (LLMs) have\nbeen proposed to mitigate the potential harms of text generated by LLMs,\nincluding fake news and copyright issues. However, current watermark detection\nalgorithms require the secret key used in the watermark generation process,\nmaking them susceptible to security breaches and counterfeiting during public\ndetection. To address this limitation, we propose an unforgeable publicly\nverifiable watermark algorithm that uses two different neural networks for\nwatermark generation and detection, instead of using the same key at both\nstages. Meanwhile, the token embedding parameters are shared between the\ngeneration and detection networks, which makes the detection network achieve a\nhigh accuracy very efficiently. Experiments demonstrate that our algorithm\nattains high detection accuracy and computational efficiency through neural\nnetworks with a minimized number of parameters. Subsequent analysis confirms\nthe high complexity involved in forging the watermark from the detection\nnetwork. Our code and data are available at\n\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable\\_watermark}.",
        "pdf_link": "https://arxiv.org/pdf/2307.16230v5.pdf"
    },
    {
        "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
        "authors": [
            "Bohao Li",
            "Rui Wang",
            "Guangzhi Wang",
            "Yuying Ge",
            "Yixiao Ge",
            "Ying Shan"
        ],
        "published": "2023-07-30T04:25:16Z",
        "summary": "Based on powerful Large Language Models (LLMs), recent generative Multimodal\nLarge Language Models (MLLMs) have gained prominence as a pivotal research\narea, exhibiting remarkable capability for both comprehension and generation.\nIn this work, we address the evaluation of generative comprehension in MLLMs as\na preliminary step towards a comprehensive assessment of generative models, by\nintroducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple\nchoice questions with accurate human annotations (x 6 larger than existing\nbenchmarks), which spans 12 evaluation dimensions including the comprehension\nof both the image and video modality. We develop an advanced pipeline for\ngenerating multiple-choice questions that target specific evaluation\ndimensions, integrating both automatic filtering and manual verification\nprocesses. Multiple-choice questions with groundtruth options derived from\nhuman annotation enables an objective and efficient assessment of model\nperformance, eliminating the need for human or GPT intervention during\nevaluation. We further evaluate the performance of 18 models across all 12\ndimensions, covering both the spatial and temporal understanding. By revealing\nthe limitations of existing MLLMs through evaluation results, we aim for\nSEED-Bench to provide insights for motivating future research. We will launch\nand consistently maintain a leaderboard to provide a platform for the community\nto assess and investigate model capability.",
        "pdf_link": "https://arxiv.org/pdf/2307.16125v2.pdf"
    },
    {
        "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
        "authors": [
            "Viet Dac Lai",
            "Chien Van Nguyen",
            "Nghia Trung Ngo",
            "Thuat Nguyen",
            "Franck Dernoncourt",
            "Ryan A. Rossi",
            "Thien Huu Nguyen"
        ],
        "published": "2023-07-29T18:01:46Z",
        "summary": "A key technology for the development of large language models (LLMs) involves\ninstruction tuning that helps align the models' responses with human\nexpectations to realize impressive learning abilities. Two major approaches for\ninstruction tuning characterize supervised fine-tuning (SFT) and reinforcement\nlearning from human feedback (RLHF), which are currently applied to produce the\nbest commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for\nresearch and development efforts, various instruction-tuned open-source LLMs\nhave also been introduced recently, e.g., Alpaca, Vicuna, to name a few.\nHowever, existing open-source LLMs have only been instruction-tuned for English\nand a few popular languages, thus hindering their impacts and accessibility to\nmany other languages in the world. Among a few very recent work to explore\ninstruction tuning for LLMs in multiple languages, SFT has been used as the\nonly approach to instruction-tune LLMs for multiple languages. This has left a\nsignificant gap for fine-tuned LLMs based on RLHF in diverse languages and\nraised important questions on how RLHF can boost the performance of\nmultilingual instruction tuning. To overcome this issue, we present Okapi, the\nfirst system with instruction-tuned LLMs based on RLHF for multiple languages.\nOkapi introduces instruction and response-ranked data in 26 diverse languages\nto facilitate the experiments and development of future multilingual LLM\nresearch. We also present benchmark datasets to enable the evaluation of\ngenerative LLMs in multiple languages. Our experiments demonstrate the\nadvantages of RLHF for multilingual instruction over SFT for different base\nmodels and datasets. Our framework and resources are released at\nhttps://github.com/nlp-uoregon/Okapi.",
        "pdf_link": "https://arxiv.org/pdf/2307.16039v2.pdf"
    },
    {
        "title": "Towards Codable Watermarking for Injecting Multi-bits Information to LLMs",
        "authors": [
            "Lean Wang",
            "Wenkai Yang",
            "Deli Chen",
            "Hao Zhou",
            "Yankai Lin",
            "Fandong Meng",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2023-07-29T14:11:15Z",
        "summary": "As large language models (LLMs) generate texts with increasing fluency and\nrealism, there is a growing need to identify the source of texts to prevent the\nabuse of LLMs. Text watermarking techniques have proven reliable in\ndistinguishing whether a text is generated by LLMs by injecting hidden\npatterns. However, we argue that existing LLM watermarking methods are\nencoding-inefficient and cannot flexibly meet the diverse information encoding\nneeds (such as encoding model version, generation time, user id, etc.). In this\nwork, we conduct the first systematic study on the topic of Codable Text\nWatermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit\ncustomizable information. First of all, we study the taxonomy of LLM\nwatermarking technologies and give a mathematical formulation for CTWL.\nAdditionally, we provide a comprehensive evaluation system for CTWL: (1)\nwatermarking success rate, (2) robustness against various corruptions, (3)\ncoding rate of payload information, (4) encoding and decoding efficiency, (5)\nimpacts on the quality of the generated text. To meet the requirements of these\nnon-Pareto-improving metrics, we follow the most prominent vocabulary\npartition-based watermarking direction, and devise an advanced CTWL method\nnamed Balance-Marking. The core idea of our method is to use a proxy language\nmodel to split the vocabulary into probability-balanced parts, thereby\neffectively maintaining the quality of the watermarked text. Our code is\navailable at https://github.com/lancopku/codable-watermarking-for-llm.",
        "pdf_link": "https://arxiv.org/pdf/2307.15992v3.pdf"
    },
    {
        "title": "Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system",
        "authors": [
            "Sumit Asthana",
            "Sagih Hilleli",
            "Pengcheng He",
            "Aaron Halfaker"
        ],
        "published": "2023-07-28T20:25:11Z",
        "summary": "Meetings play a critical infrastructural role in the coordination of work. In\nrecent years, due to shift to hybrid and remote work, more meetings are moving\nto online Computer Mediated Spaces. This has led to new problems (e.g. more\ntime spent in less engaging meetings) and new opportunities (e.g. automated\ntranscription/captioning and recap support). Recent advances in large language\nmodels (LLMs) for dialog summarization have the potential to improve the\nexperience of meetings by reducing individuals' meeting load and increasing the\nclarity and alignment of meeting outputs. Despite this potential, they face\ntechnological limitation due to long transcripts and inability to capture\ndiverse recap needs based on user's context. To address these gaps, we design,\nimplement and evaluate in-context a meeting recap system. We first\nconceptualize two salient recap representations -- important highlights, and a\nstructured, hierarchical minutes view. We develop a system to operationalize\nthe representations with dialogue summarization as its building blocks.\nFinally, we evaluate the effectiveness of the system with seven users in the\ncontext of their work meetings. Our findings show promise in using LLM-based\ndialogue summarization for meeting recap and the need for both representations\nin different contexts. However, we find that LLM-based recap still lacks an\nunderstanding of whats personally relevant to participants, can miss important\ndetails, and mis-attributions can be detrimental to group dynamics. We identify\ncollaboration opportunities such as a shared recap document that a high quality\nrecap enables. We report on implications for designing AI systems to partner\nwith users to learn and improve from natural interactions to overcome the\nlimitations related to personal relevance and summarization quality.",
        "pdf_link": "https://arxiv.org/pdf/2307.15793v1.pdf"
    },
    {
        "title": "CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools",
        "authors": [
            "Jingwei Ni",
            "Julia Bingler",
            "Chiara Colesanti-Senni",
            "Mathias Kraus",
            "Glen Gostlow",
            "Tobias Schimanski",
            "Dominik Stammbach",
            "Saeid Ashraf Vaghefi",
            "Qian Wang",
            "Nicolas Webersinke",
            "Tobias Wekhof",
            "Tingyu Yu",
            "Markus Leippold"
        ],
        "published": "2023-07-28T18:58:16Z",
        "summary": "In the face of climate change, are companies really taking substantial steps\ntoward more sustainable operations? A comprehensive answer lies in the dense,\ninformation-rich landscape of corporate sustainability reports. However, the\nsheer volume and complexity of these reports make human analysis very costly.\nTherefore, only a few entities worldwide have the resources to analyze these\nreports at scale, which leads to a lack of transparency in sustainability\nreporting. Empowering stakeholders with LLM-based automatic analysis tools can\nbe a promising way to democratize sustainability report analysis. However,\ndeveloping such tools is challenging due to (1) the hallucination of LLMs and\n(2) the inefficiency of bringing domain experts into the AI development loop.\nIn this paper, we ChatReport, a novel LLM-based system to automate the analysis\nof corporate sustainability reports, addressing existing challenges by (1)\nmaking the answers traceable to reduce the harm of hallucination and (2)\nactively involving domain experts in the development loop. We make our\nmethodology, annotated datasets, and generated analyses of 1015 reports\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2307.15770v2.pdf"
    },
    {
        "title": "A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI",
        "authors": [
            "Arash Hajikhani",
            "Carolyn Cole"
        ],
        "published": "2023-07-28T09:20:22Z",
        "summary": "This paper examines the comparative effectiveness of a specialized compiled\nlanguage model and a general-purpose model like OpenAI's GPT-3.5 in detecting\nSDGs within text data. It presents a critical review of Large Language Models\n(LLMs), addressing challenges related to bias and sensitivity. The necessity of\nspecialized training for precise, unbiased analysis is underlined. A case study\nusing a company descriptions dataset offers insight into the differences\nbetween the GPT-3.5 and the specialized SDG detection model. While GPT-3.5\nboasts broader coverage, it may identify SDGs with limited relevance to the\ncompanies' activities. In contrast, the specialized model zeroes in on highly\npertinent SDGs. The importance of thoughtful model selection is emphasized,\ntaking into account task requirements, cost, complexity, and transparency.\nDespite the versatility of LLMs, the use of specialized models is suggested for\ntasks demanding precision and accuracy. The study concludes by encouraging\nfurther research to find a balance between the capabilities of LLMs and the\nneed for domain-specific expertise and interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2307.15425v1.pdf"
    },
    {
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
        "authors": [
            "Ankit Pal",
            "Logesh Kumar Umapathi",
            "Malaikannan Sankarasubbu"
        ],
        "published": "2023-07-28T06:43:04Z",
        "summary": "This research paper focuses on the challenges posed by hallucinations in\nlarge language models (LLMs), particularly in the context of the medical\ndomain. Hallucination, wherein these models generate plausible yet unverified\nor incorrect information, can have serious consequences in healthcare\napplications. We propose a new benchmark and dataset, Med-HALT (Medical Domain\nHallucination Test), designed specifically to evaluate and reduce\nhallucinations. Med-HALT provides a diverse multinational dataset derived from\nmedical examinations across various countries and includes multiple innovative\ntesting modalities. Med-HALT includes two categories of tests reasoning and\nmemory-based hallucination tests, designed to assess LLMs's problem-solving and\ninformation retrieval abilities.\n  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,\nMPT, and Falcon, revealing significant differences in their performance. The\npaper provides detailed insights into the dataset, promoting transparency and\nreproducibility. Through this work, we aim to contribute to the development of\nsafer and more reliable language models in healthcare. Our benchmark can be\nfound at medhalt.github.io",
        "pdf_link": "https://arxiv.org/pdf/2307.15343v2.pdf"
    },
    {
        "title": "An Overview Of Temporal Commonsense Reasoning and Acquisition",
        "authors": [
            "Georg Wenzel",
            "Adam Jatowt"
        ],
        "published": "2023-07-28T01:30:15Z",
        "summary": "Temporal commonsense reasoning refers to the ability to understand the\ntypical temporal context of phrases, actions, and events, and use it to reason\nover problems requiring such knowledge. This trait is essential in temporal\nnatural language processing tasks, with possible applications such as timeline\nsummarization, temporal question answering, and temporal natural language\ninference. Recent research on the performance of large language models suggests\nthat, although they are adept at generating syntactically correct sentences and\nsolving classification tasks, they often take shortcuts in their reasoning and\nfall prey to simple linguistic traps. This article provides an overview of\nresearch in the domain of temporal commonsense reasoning, particularly focusing\non enhancing language model performance through a variety of augmentations and\ntheir evaluation across a growing number of datasets. However, these augmented\nmodels still struggle to approach human performance on reasoning tasks over\ntemporal common sense properties, such as the typical occurrence times,\norderings, or durations of events. We further emphasize the need for careful\ninterpretation of research to guard against overpromising evaluation results in\nlight of the shallow reasoning present in transformers. This can be achieved by\nappropriately preparing datasets and suitable evaluation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2308.00002v3.pdf"
    },
    {
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
        "authors": [
            "Stephen Casper",
            "Xander Davies",
            "Claudia Shi",
            "Thomas Krendl Gilbert",
            "J\u00e9r\u00e9my Scheurer",
            "Javier Rando",
            "Rachel Freedman",
            "Tomasz Korbak",
            "David Lindner",
            "Pedro Freire",
            "Tony Wang",
            "Samuel Marks",
            "Charbel-Rapha\u00ebl Segerie",
            "Micah Carroll",
            "Andi Peng",
            "Phillip Christoffersen",
            "Mehul Damani",
            "Stewart Slocum",
            "Usman Anwar",
            "Anand Siththaranjan",
            "Max Nadeau",
            "Eric J. Michaud",
            "Jacob Pfau",
            "Dmitrii Krasheninnikov",
            "Xin Chen",
            "Lauro Langosco",
            "Peter Hase",
            "Erdem B\u0131y\u0131k",
            "Anca Dragan",
            "David Krueger",
            "Dorsa Sadigh",
            "Dylan Hadfield-Menell"
        ],
        "published": "2023-07-27T22:29:25Z",
        "summary": "Reinforcement learning from human feedback (RLHF) is a technique for training\nAI systems to align with human goals. RLHF has emerged as the central method\nused to finetune state-of-the-art large language models (LLMs). Despite this\npopularity, there has been relatively little public work systematizing its\nflaws. In this paper, we (1) survey open problems and fundamental limitations\nof RLHF and related methods; (2) overview techniques to understand, improve,\nand complement RLHF in practice; and (3) propose auditing and disclosure\nstandards to improve societal oversight of RLHF systems. Our work emphasizes\nthe limitations of RLHF and highlights the importance of a multi-faceted\napproach to the development of safer AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2307.15217v2.pdf"
    },
    {
        "title": "Matching Patients to Clinical Trials with Large Language Models",
        "authors": [
            "Qiao Jin",
            "Zifeng Wang",
            "Charalampos S. Floudas",
            "Jimeng Sun",
            "Zhiyong Lu"
        ],
        "published": "2023-07-27T17:56:56Z",
        "summary": "Clinical trials are vital in advancing drug development and evidence-based\nmedicine, but their success is often hindered by challenges in patient\nrecruitment. In this work, we investigate the potential of large language\nmodels (LLMs) to assist individual patients and referral physicians in\nidentifying suitable clinical trials from an extensive selection. Specifically,\nwe introduce TrialGPT, a novel architecture employing LLMs to predict\ncriterion-level eligibility with detailed explanations, which are then\naggregated for ranking and excluding candidate clinical trials based on\nfree-text patient notes. We evaluate TrialGPT on three publicly available\ncohorts of 184 patients and 18,238 annotated clinical trials. The experimental\nresults demonstrate several key findings: First, TrialGPT achieves high\ncriterion-level prediction accuracy with faithful explanations. Second, the\naggregated trial-level TrialGPT scores are highly correlated with expert\neligibility annotations. Third, these scores prove effective in ranking\nclinical trials and exclude ineligible candidates. Our error analysis suggests\nthat current LLMs still make some mistakes due to limited medical knowledge and\ndomain-specific context understanding. Nonetheless, we believe the explanatory\ncapabilities of LLMs are highly valuable. Future research is warranted on how\nsuch AI assistants can be integrated into the routine trial matching workflow\nin real-world settings to improve its efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2307.15051v2.pdf"
    },
    {
        "title": "WavJourney: Compositional Audio Creation with Large Language Models",
        "authors": [
            "Xubo Liu",
            "Zhongkai Zhu",
            "Haohe Liu",
            "Yi Yuan",
            "Meng Cui",
            "Qiushi Huang",
            "Jinhua Liang",
            "Yin Cao",
            "Qiuqiang Kong",
            "Mark D. Plumbley",
            "Wenwu Wang"
        ],
        "published": "2023-07-26T17:54:04Z",
        "summary": "Despite breakthroughs in audio generation models, their capabilities are\noften confined to domain-specific conditions such as speech transcriptions and\naudio captions. However, real-world audio creation aims to generate harmonious\naudio containing various elements such as speech, music, and sound effects with\ncontrollable conditions, which is challenging to address using existing audio\ngeneration systems. We present WavJourney, a novel framework that leverages\nLarge Language Models (LLMs) to connect various audio models for audio\ncreation. WavJourney allows users to create storytelling audio content with\ndiverse audio elements simply from textual descriptions. Specifically, given a\ntext instruction, WavJourney first prompts LLMs to generate an audio script\nthat serves as a structured semantic representation of audio elements. The\naudio script is then converted into a computer program, where each line of the\nprogram calls a task-specific audio generation model or computational operation\nfunction. The computer program is then executed to obtain a compositional and\ninterpretable solution for audio creation. Experimental results suggest that\nWavJourney is capable of synthesizing realistic audio aligned with\ntextually-described semantic, spatial and temporal conditions, achieving\nstate-of-the-art results on text-to-audio generation benchmarks. Additionally,\nwe introduce a new multi-genre story benchmark. Subjective evaluations\ndemonstrate the potential of WavJourney in crafting engaging storytelling audio\ncontent from text. We further demonstrate that WavJourney can facilitate\nhuman-machine co-creation in multi-round dialogues. To foster future research,\nthe code and synthesized audio are available at:\nhttps://audio-agi.github.io/WavJourney_demopage/.",
        "pdf_link": "https://arxiv.org/pdf/2307.14335v2.pdf"
    },
    {
        "title": "This is not correct! Negation-aware Evaluation of Language Generation Systems",
        "authors": [
            "Miriam Ansch\u00fctz",
            "Diego Miguel Lozano",
            "Georg Groh"
        ],
        "published": "2023-07-26T06:54:31Z",
        "summary": "Large language models underestimate the impact of negations on how much they\nchange the meaning of a sentence. Therefore, learned evaluation metrics based\non these models are insensitive to negations. In this paper, we propose\nNegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that,\nwe designed a rule-based sentence negation tool and used it to create the\nCANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a\nsentence transformer and an evaluation metric to improve their negation\nsensitivity. Evaluating these models on existing benchmarks shows that our\nfine-tuned models outperform existing metrics on the negated sentences by far\nwhile preserving their base models' performances on other perturbations.",
        "pdf_link": "https://arxiv.org/pdf/2307.13989v1.pdf"
    },
    {
        "title": "Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data",
        "authors": [
            "Xuhai Xu",
            "Bingsheng Yao",
            "Yuanzhe Dong",
            "Saadia Gabriel",
            "Hong Yu",
            "James Hendler",
            "Marzyeh Ghassemi",
            "Anind K. Dey",
            "Dakuo Wang"
        ],
        "published": "2023-07-26T06:00:50Z",
        "summary": "Advances in large language models (LLMs) have empowered a variety of\napplications. However, there is still a significant gap in research when it\ncomes to understanding and enhancing the capabilities of LLMs in the field of\nmental health. In this work, we present a comprehensive evaluation of multiple\nLLMs on various mental health prediction tasks via online text data, including\nAlpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of\nexperiments, covering zero-shot prompting, few-shot prompting, and instruction\nfine-tuning. The results indicate a promising yet limited performance of LLMs\nwith zero-shot and few-shot prompt designs for mental health tasks. More\nimportantly, our experiments show that instruction finetuning can significantly\nboost the performance of LLMs for all tasks simultaneously. Our best-finetuned\nmodels, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of\nGPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of\nGPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the\nstate-of-the-art task-specific language model. We also conduct an exploratory\ncase study on LLMs' capability on mental health reasoning tasks, illustrating\nthe promising capability of certain models such as GPT-4. We summarize our\nfindings into a set of action guidelines for potential methods to enhance LLMs'\ncapability for mental health tasks. Meanwhile, we also emphasize the important\nlimitations before achieving deployability in real-world mental health\nsettings, such as known racial and gender bias. We highlight the important\nethical risks accompanying this line of research.",
        "pdf_link": "https://arxiv.org/pdf/2307.14385v4.pdf"
    },
    {
        "title": "Is GPT a Computational Model of Emotion? Detailed Analysis",
        "authors": [
            "Ala N. Tak",
            "Jonathan Gratch"
        ],
        "published": "2023-07-25T19:34:44Z",
        "summary": "This paper investigates the emotional reasoning abilities of the GPT family\nof large language models via a component perspective. The paper first examines\nhow the model reasons about autobiographical memories. Second, it\nsystematically varies aspects of situations to impact emotion intensity and\ncoping tendencies. Even without the use of prompt engineering, it is shown that\nGPT's predictions align significantly with human-provided appraisals and\nemotional labels. However, GPT faces difficulties predicting emotion intensity\nand coping responses. GPT-4 showed the highest performance in the initial study\nbut fell short in the second, despite providing superior results after minor\nprompt engineering. This assessment brings up questions on how to effectively\nemploy the strong points and address the weak areas of these models,\nparticularly concerning response variability. These studies underscore the\nmerits of evaluating models from a componential perspective.",
        "pdf_link": "https://arxiv.org/pdf/2307.13779v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models for Radiology Natural Language Processing",
        "authors": [
            "Zhengliang Liu",
            "Tianyang Zhong",
            "Yiwei Li",
            "Yutong Zhang",
            "Yi Pan",
            "Zihao Zhao",
            "Peixin Dong",
            "Chao Cao",
            "Yuxiao Liu",
            "Peng Shu",
            "Yaonai Wei",
            "Zihao Wu",
            "Chong Ma",
            "Jiaqi Wang",
            "Sheng Wang",
            "Mengyue Zhou",
            "Zuowei Jiang",
            "Chunlin Li",
            "Jason Holmes",
            "Shaochen Xu",
            "Lu Zhang",
            "Haixing Dai",
            "Kai Zhang",
            "Lin Zhao",
            "Yuanhao Chen",
            "Xu Liu",
            "Peilong Wang",
            "Pingkun Yan",
            "Jun Liu",
            "Bao Ge",
            "Lichao Sun",
            "Dajiang Zhu",
            "Xiang Li",
            "Wei Liu",
            "Xiaoyan Cai",
            "Xintao Hu",
            "Xi Jiang",
            "Shu Zhang",
            "Xin Zhang",
            "Tuo Zhang",
            "Shijie Zhao",
            "Quanzheng Li",
            "Hongtu Zhu",
            "Dinggang Shen",
            "Tianming Liu"
        ],
        "published": "2023-07-25T17:57:18Z",
        "summary": "The rise of large language models (LLMs) has marked a pivotal shift in the\nfield of natural language processing (NLP). LLMs have revolutionized a\nmultitude of domains, and they have made a significant impact in the medical\nfield. Large language models are now more abundant than ever, and many of these\nmodels exhibit bilingual capabilities, proficient in both English and Chinese.\nHowever, a comprehensive evaluation of these models remains to be conducted.\nThis lack of assessment is especially apparent within the context of radiology\nNLP. This study seeks to bridge this gap by critically evaluating thirty two\nLLMs in interpreting radiology reports, a crucial component of radiology NLP.\nSpecifically, the ability to derive impressions from radiologic findings is\nassessed. The outcomes of this evaluation provide key insights into the\nperformance, strengths, and weaknesses of these LLMs, informing their practical\napplications within the medical domain.",
        "pdf_link": "https://arxiv.org/pdf/2307.13693v2.pdf"
    },
    {
        "title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
        "authors": [
            "Tomohiro Sawada",
            "Daniel Paleka",
            "Alexander Havrilla",
            "Pranav Tadepalli",
            "Paula Vidas",
            "Alexander Kranias",
            "John J. Nay",
            "Kshitij Gupta",
            "Aran Komatsuzaki"
        ],
        "published": "2023-07-25T17:55:19Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious quantitative reasoning and knowledge benchmarks. However, many of these\nbenchmarks are losing utility as LLMs get increasingly high scores, despite not\nyet reaching expert performance in these domains. We introduce ARB, a novel\nbenchmark composed of advanced reasoning problems in multiple fields. ARB\npresents a more challenging test than prior benchmarks, featuring problems in\nmathematics, physics, biology, chemistry, and law. As a subset of ARB, we\nintroduce a challenging set of math and physics problems which require advanced\nsymbolic reasoning and domain knowledge. We evaluate recent models such as\nGPT-4 and Claude on ARB and demonstrate that current models score well below\n50% on more demanding tasks. In order to improve both automatic and assisted\nevaluation capabilities, we introduce a rubric-based evaluation approach,\nallowing GPT-4 to score its own intermediate reasoning steps. Further, we\nconduct a human evaluation of the symbolic subset of ARB, finding promising\nagreement between annotators and GPT-4 rubric evaluation scores.",
        "pdf_link": "https://arxiv.org/pdf/2307.13692v2.pdf"
    },
    {
        "title": "How Can Large Language Models Help Humans in Design and Manufacturing?",
        "authors": [
            "Liane Makatura",
            "Michael Foshey",
            "Bohan Wang",
            "Felix H\u00e4hnLein",
            "Pingchuan Ma",
            "Bolei Deng",
            "Megan Tjandrasuwita",
            "Andrew Spielberg",
            "Crystal Elaine Owens",
            "Peter Yichen Chen",
            "Allan Zhao",
            "Amy Zhu",
            "Wil J Norton",
            "Edward Gu",
            "Joshua Jacob",
            "Yifei Li",
            "Adriana Schulz",
            "Wojciech Matusik"
        ],
        "published": "2023-07-25T17:30:38Z",
        "summary": "The advancement of Large Language Models (LLMs), including GPT-4, provides\nexciting new opportunities for generative design. We investigate the\napplication of this tool across the entire design and manufacturing workflow.\nSpecifically, we scrutinize the utility of LLMs in tasks such as: converting a\ntext-based prompt into a design specification, transforming a design into\nmanufacturing instructions, producing a design space and design variations,\ncomputing the performance of a design, and searching for designs predicated on\nperformance. Through a series of examples, we highlight both the benefits and\nthe limitations of the current LLMs. By exposing these limitations, we aspire\nto catalyze the continued improvement and progression of these models.",
        "pdf_link": "https://arxiv.org/pdf/2307.14377v1.pdf"
    },
    {
        "title": "GPT-3 Models are Few-Shot Financial Reasoners",
        "authors": [
            "Raul Salles de Padua",
            "Imran Qureshi",
            "Mustafa U. Karakaplan"
        ],
        "published": "2023-07-25T16:21:07Z",
        "summary": "Financial analysis is an important tool for evaluating company performance.\nPractitioners work to answer financial questions to make profitable investment\ndecisions, and use advanced quantitative analyses to do so. As a result,\nFinancial Question Answering (QA) is a question answering task that requires\ndeep reasoning about numbers. Furthermore, it is unknown how well pre-trained\nlanguage models can reason in the financial domain. The current\nstate-of-the-art requires a retriever to collect relevant facts about the\nfinancial question from the text and a generator to produce a valid financial\nprogram and a final answer. However, recently large language models like GPT-3\nhave achieved state-of-the-art performance on wide variety of tasks with just a\nfew shot examples. We run several experiments with GPT-3 and find that a\nseparate retrieval model and logic engine continue to be essential components\nto achieving SOTA performance in this task, particularly due to the precise\nnature of financial questions and the complex information stored in financial\ndocuments. With this understanding, our refined prompt-engineering approach on\nGPT-3 achieves near SOTA accuracy without any fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2307.13617v2.pdf"
    },
    {
        "title": "FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
        "authors": [
            "I-Chun Chern",
            "Steffi Chern",
            "Shiqi Chen",
            "Weizhe Yuan",
            "Kehua Feng",
            "Chunting Zhou",
            "Junxian He",
            "Graham Neubig",
            "Pengfei Liu"
        ],
        "published": "2023-07-25T14:20:51Z",
        "summary": "The emergence of generative pre-trained models has facilitated the synthesis\nof high-quality text, but it has also posed challenges in identifying factual\nerrors in the generated text. In particular: (1) A wider range of tasks now\nface an increasing risk of containing factual errors when handled by generative\nmodels. (2) Generated texts tend to be lengthy and lack a clearly defined\ngranularity for individual facts. (3) There is a scarcity of explicit evidence\navailable during the process of fact checking. With the above challenges in\nmind, in this paper, we propose FacTool, a task and domain agnostic framework\nfor detecting factual errors of texts generated by large language models (e.g.,\nChatGPT). Experiments on four different tasks (knowledge-based QA, code\ngeneration, mathematical reasoning, and scientific literature review) show the\nefficacy of the proposed method. We release the code of FacTool associated with\nChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",
        "pdf_link": "https://arxiv.org/pdf/2307.13528v2.pdf"
    },
    {
        "title": "Predicting Code Coverage without Execution",
        "authors": [
            "Michele Tufano",
            "Shubham Chandel",
            "Anisha Agarwal",
            "Neel Sundaresan",
            "Colin Clement"
        ],
        "published": "2023-07-25T10:07:02Z",
        "summary": "Code coverage is a widely used metric for quantifying the extent to which\nprogram elements, such as statements or branches, are executed during testing.\nCalculating code coverage is resource-intensive, requiring code building and\nexecution with additional overhead for the instrumentation. Furthermore,\ncomputing coverage of any snippet of code requires the whole program context.\nUsing Machine Learning to amortize this expensive process could lower the cost\nof code coverage by requiring only the source code context, and the task of\ncode coverage prediction can be a novel benchmark for judging the ability of\nmodels to understand code. We propose a novel benchmark task called Code\nCoverage Prediction for Large Language Models (LLMs). We formalize this task to\nevaluate the capability of LLMs in understanding code execution by determining\nwhich lines of a method are executed by a given test case and inputs. We curate\nand release a dataset we call COVERAGEEVAL by executing tests and code from the\nHumanEval dataset and collecting code coverage information. We report the\nperformance of four state-of-the-art LLMs used for code-related tasks,\nincluding OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's\nClaude, on the Code Coverage Prediction task. Finally, we argue that code\ncoverage as a metric and pre-training data source are valuable for overall LLM\nperformance on software engineering tasks.",
        "pdf_link": "https://arxiv.org/pdf/2307.13383v1.pdf"
    },
    {
        "title": "Aligning Large Language Models with Human: A Survey",
        "authors": [
            "Yufei Wang",
            "Wanjun Zhong",
            "Liangyou Li",
            "Fei Mi",
            "Xingshan Zeng",
            "Wenyong Huang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2023-07-24T17:44:58Z",
        "summary": "Large Language Models (LLMs) trained on extensive textual corpora have\nemerged as leading solutions for a broad array of Natural Language Processing\n(NLP) tasks. Despite their notable performance, these models are prone to\ncertain limitations such as misunderstanding human instructions, generating\npotentially biased content, or factually incorrect (hallucinated) information.\nHence, aligning LLMs with human expectations has become an active area of\ninterest within the research community. This survey presents a comprehensive\noverview of these alignment technologies, including the following aspects. (1)\nData collection: the methods for effectively collecting high-quality\ninstructions for LLM alignment, including the use of NLP benchmarks, human\nannotations, and leveraging strong LLMs. (2) Training methodologies: a detailed\nreview of the prevailing training methods employed for LLM alignment. Our\nexploration encompasses Supervised Fine-tuning, both Online and Offline human\npreference training, along with parameter-efficient training mechanisms. (3)\nModel Evaluation: the methods for evaluating the effectiveness of these\nhuman-aligned LLMs, presenting a multifaceted approach towards their\nassessment. In conclusion, we collate and distill our findings, shedding light\non several promising future research avenues in the field. This survey,\ntherefore, serves as a valuable resource for anyone invested in understanding\nand advancing the alignment of LLMs to better suit human-oriented tasks and\nexpectations. An associated GitHub link collecting the latest papers is\navailable at https://github.com/GaryYufei/AlignLLMHumanSurvey.",
        "pdf_link": "https://arxiv.org/pdf/2307.12966v1.pdf"
    },
    {
        "title": "The potential of LLMs for coding with low-resource and domain-specific programming languages",
        "authors": [
            "Artur Tarassow"
        ],
        "published": "2023-07-24T17:17:13Z",
        "summary": "This paper presents a study on the feasibility of using large language models\n(LLM) for coding with low-resource and domain-specific programming languages\nthat typically lack the amount of data required for effective LLM processing\ntechniques. This study focuses on the econometric scripting language named\nhansl of the open-source software gretl and employs a proprietary LLM based on\nGPT-3.5. Our findings suggest that LLMs can be a useful tool for writing,\nunderstanding, improving, and documenting gretl code, which includes generating\ndescriptive docstrings for functions and providing precise explanations for\nabstract and poorly documented econometric code. While the LLM showcased\npromoting docstring-to-code translation capability, we also identify some\nlimitations, such as its inability to improve certain sections of code and to\nwrite accurate unit tests. This study is a step towards leveraging the power of\nLLMs to facilitate software development in low-resource programming languages\nand ultimately to lower barriers to entry for their adoption.",
        "pdf_link": "https://arxiv.org/pdf/2307.13018v1.pdf"
    },
    {
        "title": "Interpretable Stereotype Identification through Reasoning",
        "authors": [
            "Jacob-Junqi Tian",
            "Omkar Dige",
            "David Emerson",
            "Faiza Khan Khattak"
        ],
        "published": "2023-07-24T15:12:13Z",
        "summary": "Given that language models are trained on vast datasets that may contain\ninherent biases, there is a potential danger of inadvertently perpetuating\nsystemic discrimination. Consequently, it becomes essential to examine and\naddress biases in language models, integrating fairness into their development\nto ensure these models are equitable and free from bias. In this work, we\ndemonstrate the importance of reasoning in zero-shot stereotype identification\nbased on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from\n13B to 33B, we show that the performance gain from reasoning significantly\nexceeds the gain from scaling up. Our findings suggest that reasoning could be\na key factor that enables LLMs to trescend the scaling law on out-of-domain\ntasks such as stereotype identification. Additionally, through a qualitative\nanalysis of select reasoning traces, we highlight how reasoning enhances not\njust accuracy but also the interpretability of the decision.",
        "pdf_link": "https://arxiv.org/pdf/2308.00071v2.pdf"
    },
    {
        "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
        "authors": [
            "Izzeddin Gur",
            "Hiroki Furuta",
            "Austin Huang",
            "Mustafa Safdari",
            "Yutaka Matsuo",
            "Douglas Eck",
            "Aleksandra Faust"
        ],
        "published": "2023-07-24T14:56:30Z",
        "summary": "Pre-trained large language models (LLMs) have recently achieved better\ngeneralization and sample efficiency in autonomous web automation. However, the\nperformance on real-world websites has still suffered from (1) open domainness,\n(2) limited context length, and (3) lack of inductive bias on HTML. We\nintroduce WebAgent, an LLM-driven agent that learns from self-experience to\ncomplete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical\nsub-instructions, summarizes long HTML documents into task-relevant snippets,\nand acts on websites via Python programs generated from those. We design\nWebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new\npre-trained LLMs for long HTML documents using local and global attention\nmechanisms and a mixture of long-span denoising objectives, for planning and\nsummarization. We empirically demonstrate that our modular recipe improves the\nsuccess on real websites by over 50%, and that HTML-T5 is the best model to\nsolve various HTML understanding tasks; achieving 18.7% higher success rate\nthan the prior method on MiniWoB web automation benchmark, and SoTA performance\non Mind2Web, an offline task planning evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2307.12856v4.pdf"
    },
    {
        "title": "Performance of Large Language Models in a Computer Science Degree Program",
        "authors": [
            "Tim Kr\u00fcger",
            "Michael Gref"
        ],
        "published": "2023-07-24T14:17:00Z",
        "summary": "Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and\ndominate the current discourse. Their transformative capabilities have led to a\nparadigm shift in how we interact with and utilize (text-based) information.\nEach day, new possibilities to leverage the capabilities of these models\nemerge. This paper presents findings on the performance of different large\nlanguage models in a university of applied sciences' undergraduate computer\nscience degree program. Our primary objective is to assess the effectiveness of\nthese models within the curriculum by employing them as educational aids. By\nprompting the models with lecture material, exercise tasks, and past exams, we\naim to evaluate their proficiency across different computer science domains. We\nshowcase the strong performance of current large language models while\nhighlighting limitations and constraints within the context of such a degree\nprogram. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10\ntested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter\nvariant, 20%. Despite these convincing results, even GPT-4.0 would not pass the\ndegree program - due to limitations in mathematical calculations.",
        "pdf_link": "https://arxiv.org/pdf/2308.02432v1.pdf"
    },
    {
        "title": "Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models",
        "authors": [
            "Yuanzhi Liang",
            "Linchao Zhu",
            "Yi Yang"
        ],
        "published": "2023-07-24T07:40:59Z",
        "summary": "Recent advancements in natural language and Large Language Models (LLMs) have\nenabled AI agents to simulate human-like interactions within virtual worlds.\nHowever, these interactions still face limitations in complexity and\nflexibility, particularly in scenarios involving multiple characters and novel\nobjects. Pre-defining all interactable objects in the agent's world model\npresents challenges, and conveying implicit intentions to multiple characters\nthrough complex interactions remains difficult. To address these issues, we\npropose integrating virtual Game Masters (GMs) into the agent's world model,\ndrawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a\ncrucial role in overseeing information, estimating players' intentions,\nproviding environment descriptions, and offering feedback, compensating for\ncurrent world model deficiencies. To facilitate future explorations for complex\ninteractions, we introduce a benchmark named Tachikuma, comprising a Multiple\ncharacter and novel Object based interaction Estimation (MOE) task and a\nsupporting dataset. MOE challenges models to understand characters' intentions\nand accurately determine their actions within intricate contexts involving\nmulti-character and novel object interactions. Besides, the dataset captures\nlog data from real-time communications during gameplay, providing diverse,\ngrounded, and complex interactions for further explorations. Finally, we\npresent a simple prompting baseline and evaluate its performance, demonstrating\nits effectiveness in enhancing interaction understanding. We hope that our\ndataset and task will inspire further research in complex interactions with\nnatural language, fostering the development of more advanced AI agents.",
        "pdf_link": "https://arxiv.org/pdf/2307.12573v1.pdf"
    },
    {
        "title": "The Effectiveness of Large Language Models (ChatGPT and CodeBERT) for Security-Oriented Code Analysis",
        "authors": [
            "Zhilong Wang",
            "Lan Zhang",
            "Chen Cao",
            "Peng Liu"
        ],
        "published": "2023-07-24T02:38:24Z",
        "summary": "Large Language Models (LLMs), such as GPT and BERT, have demonstrated\nremarkable capabilities in addressing neural language process tasks. Recently,\nthe release of ChatGPT has garnered significant attention due to its ability to\nanalyze, comprehend, and synthesize information from user inputs. Therefore,\nthese LLMs were adopted by researchers in many different domains. In the realm\nof code analysis, researchers have applied LLMs to tasks like code review and\ncode generation. However, we observed that the strengths and limitations of\nadopting these LLMs to the code analysis have not been investigated. In this\npaper, we delve into LLMs' capabilities in security-oriented program analysis,\nconsidering perspectives from both attackers and security analysts. We focus on\ntwo representative LLMs, ChatGPT and CodeBert, and evaluate their performance\nin solving typical analytic tasks with varying levels of difficulty. Given the\ndifferent natures of ChatGPT and CodeBERT, we conduct a qualitative analysis of\nthe model's output for ChatGPT and a quantitative analysis for CodeBERT,\nrespectively. For ChatGPT, we present a case study involving several\nsecurity-oriented program analysis tasks while deliberately introducing\nchallenges to assess its responses. On the other hand, for CodeBERT, we\nsystematically analyze and classify the features in code, quantitatively\nevaluating the impact of these features on the model's performance. Our study\ndemonstrates the LLM's efficiency in learning high-level semantics from code,\npositioning ChatGPT as a potential asset in security-oriented contexts.\nHowever, it is essential to acknowledge certain limitations, such as the heavy\nreliance on well-defined variable and function names, making them unable to\nlearn from anonymized code. We hope that our findings and analysis will offer\nvaluable insights for future researchers in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2307.12488v3.pdf"
    },
    {
        "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
        "authors": [
            "Jannik Kossen",
            "Yarin Gal",
            "Tom Rainforth"
        ],
        "published": "2023-07-23T16:54:41Z",
        "summary": "The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.",
        "pdf_link": "https://arxiv.org/pdf/2307.12375v4.pdf"
    },
    {
        "title": "The Imitation Game: Detecting Human and AI-Generated Texts in the Era of ChatGPT and BARD",
        "authors": [
            "Kadhim Hayawi",
            "Sakib Shahriar",
            "Sujith Samuel Mathew"
        ],
        "published": "2023-07-22T21:00:14Z",
        "summary": "The potential of artificial intelligence (AI)-based large language models\n(LLMs) holds considerable promise in revolutionizing education, research, and\npractice. However, distinguishing between human-written and AI-generated text\nhas become a significant task. This paper presents a comparative study,\nintroducing a novel dataset of human-written and LLM-generated texts in\ndifferent genres: essays, stories, poetry, and Python code. We employ several\nmachine learning models to classify the texts. Results demonstrate the efficacy\nof these models in discerning between human and AI-generated text, despite the\ndataset's limited sample size. However, the task becomes more challenging when\nclassifying GPT-generated text, particularly in story writing. The results\nindicate that the models exhibit superior performance in binary classification\ntasks, such as distinguishing human-generated text from a specific LLM,\ncompared to the more complex multiclass tasks that involve discerning among\nhuman-generated and multiple LLMs. Our findings provide insightful implications\nfor AI text detection while our dataset paves the way for future research in\nthis evolving area.",
        "pdf_link": "https://arxiv.org/pdf/2307.12166v2.pdf"
    },
    {
        "title": "FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models",
        "authors": [
            "Yuwei Yin",
            "Yazheng Yang",
            "Jian Yang",
            "Qi Liu"
        ],
        "published": "2023-07-22T09:27:05Z",
        "summary": "Financial risk prediction plays a crucial role in the financial sector.\nMachine learning methods have been widely applied for automatically detecting\npotential risks and thus saving the cost of labor. However, the development in\nthis field is lagging behind in recent years by the following two facts: 1) the\nalgorithms used are somewhat outdated, especially in the context of the fast\nadvance of generative AI and large language models (LLMs); 2) the lack of a\nunified and open-sourced financial benchmark has impeded the related research\nfor years. To tackle these issues, we propose FinPT and FinBench: the former is\na novel approach for financial risk prediction that conduct Profile Tuning on\nlarge pretrained foundation models, and the latter is a set of high-quality\ndatasets on financial risks such as default, fraud, and churn. In FinPT, we\nfill the financial tabular data into the pre-defined instruction template,\nobtain natural-language customer profiles by prompting LLMs, and fine-tune\nlarge foundation models with the profile text to make predictions. We\ndemonstrate the effectiveness of the proposed FinPT by experimenting with a\nrange of representative strong baselines on FinBench. The analytical studies\nfurther deepen the understanding of LLMs for financial risk prediction.",
        "pdf_link": "https://arxiv.org/pdf/2308.00065v1.pdf"
    },
    {
        "title": "Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models",
        "authors": [
            "Tin Lai",
            "Yukun Shi",
            "Zicong Du",
            "Jiajie Wu",
            "Ken Fu",
            "Yichao Dou",
            "Ziqi Wang"
        ],
        "published": "2023-07-22T06:21:41Z",
        "summary": "The demand for psychological counselling has grown significantly in recent\nyears, particularly with the global outbreak of COVID-19, which has heightened\nthe need for timely and professional mental health support. Online\npsychological counselling has emerged as the predominant mode of providing\nservices in response to this demand. In this study, we propose the Psy-LLM\nframework, an AI-based assistive tool leveraging Large Language Models (LLMs)\nfor question-answering in psychological consultation settings to ease the\ndemand for mental health professions. Our framework combines pre-trained LLMs\nwith real-world professional Q\\&A from psychologists and extensively crawled\npsychological articles. The Psy-LLM framework serves as a front-end tool for\nhealthcare professionals, allowing them to provide immediate responses and\nmindfulness activities to alleviate patient stress. Additionally, it functions\nas a screening tool to identify urgent cases requiring further assistance. We\nevaluated the framework using intrinsic metrics, such as perplexity, and\nextrinsic evaluation metrics, with human participant assessments of response\nhelpfulness, fluency, relevance, and logic. The results demonstrate the\neffectiveness of the Psy-LLM framework in generating coherent and relevant\nanswers to psychological questions. This article discusses the potential and\nlimitations of using large language models to enhance mental health support\nthrough AI technologies.",
        "pdf_link": "https://arxiv.org/pdf/2307.11991v2.pdf"
    },
    {
        "title": "Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors",
        "authors": [
            "Kolby Nottingham",
            "Yasaman Razeghi",
            "Kyungmin Kim",
            "JB Lanier",
            "Pierre Baldi",
            "Roy Fox",
            "Sameer Singh"
        ],
        "published": "2023-07-21T22:02:50Z",
        "summary": "Large language models (LLMs) are being applied as actors for sequential\ndecision making tasks in domains such as robotics and games, utilizing their\ngeneral world knowledge and planning abilities. However, previous work does\nlittle to explore what environment state information is provided to LLM actors\nvia language. Exhaustively describing high-dimensional states can impair\nperformance and raise inference costs for LLM actors. Previous LLM actors avoid\nthe issue by relying on hand-engineered, task-specific protocols to determine\nwhich features to communicate about a state and which to leave out. In this\nwork, we propose Brief Language INputs for DEcision-making Responses (BLINDER),\na method for automatically selecting concise state descriptions by learning a\nvalue function for task-conditioned state descriptions. We evaluate BLINDER on\nthe challenging video game NetHack and a robotic manipulation task. Our method\nimproves task success rate, reduces input size and compute costs, and\ngeneralizes between LLM actors.",
        "pdf_link": "https://arxiv.org/pdf/2307.11922v1.pdf"
    },
    {
        "title": "The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention",
        "authors": [
            "Navid Ayoobi",
            "Sadat Shahriar",
            "Arjun Mukherjee"
        ],
        "published": "2023-07-21T19:09:24Z",
        "summary": "In this paper, we present a novel method for detecting fake and Large\nLanguage Model (LLM)-generated profiles in the LinkedIn Online Social Network\nimmediately upon registration and before establishing connections. Early fake\nprofile identification is crucial to maintaining the platform's integrity since\nit prevents imposters from acquiring the private and sensitive information of\nlegitimate users and from gaining an opportunity to increase their credibility\nfor future phishing and scamming activities. This work uses textual information\nprovided in LinkedIn profiles and introduces the Section and Subsection Tag\nEmbedding (SSTE) method to enhance the discriminative characteristics of these\ndata for distinguishing between legitimate profiles and those created by\nimposters manually or by using an LLM. Additionally, the dearth of a large\npublicly available LinkedIn dataset motivated us to collect 3600 LinkedIn\nprofiles for our research. We will release our dataset publicly for research\npurposes. This is, to the best of our knowledge, the first large publicly\navailable LinkedIn dataset for fake LinkedIn account detection. Within our\nparadigm, we assess static and contextualized word embeddings, including GloVe,\nFlair, BERT, and RoBERTa. We show that the suggested method can distinguish\nbetween legitimate and fake profiles with an accuracy of about 95% across all\nword embeddings. In addition, we show that SSTE has a promising accuracy for\nidentifying LLM-generated profiles, despite the fact that no LLM-generated\nprofiles were employed during the training phase, and can achieve an accuracy\nof approximately 90% when only 20 LLM-generated profiles are added to the\ntraining set. It is a significant finding since the proliferation of several\nLLMs in the near future makes it extremely challenging to design a single\nsystem that can identify profiles created with various LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.11864v1.pdf"
    },
    {
        "title": "OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples",
        "authors": [
            "Ryuto Koike",
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "published": "2023-07-21T17:40:47Z",
        "summary": "Large Language Models (LLMs) have achieved human-level fluency in text\ngeneration, making it difficult to distinguish between human-written and\nLLM-generated texts. This poses a growing risk of misuse of LLMs and demands\nthe development of detectors to identify LLM-generated texts. However, existing\ndetectors lack robustness against attacks: they degrade detection accuracy by\nsimply paraphrasing LLM-generated texts. Furthermore, a malicious user might\nattempt to deliberately evade the detectors based on detection results, but\nthis has not been assumed in previous studies. In this paper, we propose\nOUTFOX, a framework that improves the robustness of LLM-generated-text\ndetectors by allowing both the detector and the attacker to consider each\nother's output. In this framework, the attacker uses the detector's prediction\nlabels as examples for in-context learning and adversarially generates essays\nthat are harder to detect, while the detector uses the adversarially generated\nessays as examples for in-context learning to learn to detect essays from a\nstrong attacker. Experiments in the domain of student essays show that the\nproposed detector improves the detection performance on the attacker-generated\ntexts by up to +41.3 points F1-score. Furthermore, the proposed detector shows\na state-of-the-art detection performance: up to 96.9 points F1-score, beating\nexisting detectors on non-attacked texts. Finally, the proposed attacker\ndrastically degrades the performance of detectors by up to -57.0 points\nF1-score, massively outperforming the baseline paraphrasing method for evading\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2307.11729v3.pdf"
    },
    {
        "title": "GPT-4 Can't Reason",
        "authors": [
            "Konstantine Arkoudas"
        ],
        "published": "2023-07-21T17:04:25Z",
        "summary": "GPT-4 was released in March 2023 to wide acclaim, marking a very substantial\nimprovement across the board over GPT-3.5 (OpenAI's previously best model,\nwhich had powered the initial release of ChatGPT). However, despite the\ngenuinely impressive improvement, there are good reasons to be highly skeptical\nof GPT-4's ability to reason. This position paper discusses the nature of\nreasoning; criticizes the current formulation of reasoning problems in the NLP\ncommunity, as well as the way in which LLM reasoning performance is currently\nevaluated; introduces a small collection of 21 diverse reasoning problems; and\nperforms a detailed qualitative evaluation of GPT-4's performance on those\nproblems. Based on this analysis, the paper concludes that, despite its\noccasional flashes of analytical brilliance, GPT-4 at present is utterly\nincapable of reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2308.03762v2.pdf"
    },
    {
        "title": "\"Tidy Up the Table\": Grounding Common-sense Objective for Tabletop Object Rearrangement",
        "authors": [
            "Yiqing Xu",
            "David Hsu"
        ],
        "published": "2023-07-21T03:00:31Z",
        "summary": "Tidying up a messy table may appear simple for humans, but articulating clear\ncriteria for tidiness is challenging due to the ambiguous nature of common\nsense reasoning. Large Language Models (LLMs) have proven capable of capturing\ncommon sense knowledge to reason over this vague concept of tidiness. However,\nthey alone may struggle with table tidying due to the limited grasp on the\nspatio-visual aspects of tidiness. In this work, we aim to ground the\ncommon-sense concept of tidiness within the context of object arrangement. Our\nsurvey reveals that humans usually factorize tidiness into semantic and\nvisual-spatial tidiness; our grounding approach aligns with this decomposition.\nWe connect a language-based policy generator with an image-based tidiness score\nfunction: the policy generator utilizes the LLM's commonsense knowledge to\ncluster objects by their implicit types and functionalities for semantic\ntidiness; meanwhile, the tidiness score function assesses the visual-spatial\nrelations of the object to achieve visual-spatial tidiness. Our tidiness score\nis trained using synthetic data generated cheaply from customized random walks,\nwhich inherently encode the order of tidiness, thereby bypassing the need for\nlabor-intensive human demonstrations. The simulated experiment shows that our\napproach successfully generates tidy arrangements, predominately in 2D, with\npotential for 3D stacking, for tables with various novel objects.",
        "pdf_link": "https://arxiv.org/pdf/2307.11319v2.pdf"
    },
    {
        "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
        "authors": [
            "Seonghyeon Ye",
            "Doyoung Kim",
            "Sungdong Kim",
            "Hyeonbin Hwang",
            "Seungone Kim",
            "Yongrae Jo",
            "James Thorne",
            "Juho Kim",
            "Minjoon Seo"
        ],
        "published": "2023-07-20T14:56:35Z",
        "summary": "Evaluation of Large Language Models (LLMs) is challenging because\ninstruction-following necessitates alignment with human values and the required\nset of skills varies depending on the instruction. However, previous studies\nhave mainly focused on coarse-grained evaluation (i.e. overall preference-based\nevaluation), which limits interpretability since it does not consider the\nnature of user instructions that require instance-wise skill composition. In\nthis paper, we introduce FLASK (Fine-grained Language Model Evaluation based on\nAlignment Skill Sets), a fine-grained evaluation protocol for both human-based\nand model-based evaluation which decomposes coarse-level scoring to a skill\nset-level scoring for each instruction. We experimentally observe that the\nfine-graininess of evaluation is crucial for attaining a holistic view of model\nperformance and increasing the reliability of the evaluation. Using FLASK, we\ncompare multiple open-source and proprietary LLMs and observe a high\ncorrelation between model-based and human-based evaluations. We publicly\nrelease the evaluation data and code implementation at\nhttps://github.com/kaistAI/FLASK.",
        "pdf_link": "https://arxiv.org/pdf/2307.10928v3.pdf"
    },
    {
        "title": "LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?",
        "authors": [
            "David Glukhov",
            "Ilia Shumailov",
            "Yarin Gal",
            "Nicolas Papernot",
            "Vardan Papyan"
        ],
        "published": "2023-07-20T09:25:02Z",
        "summary": "Large language models (LLMs) have exhibited impressive capabilities in\ncomprehending complex instructions. However, their blind adherence to provided\ninstructions has led to concerns regarding risks of malicious use. Existing\ndefence mechanisms, such as model fine-tuning or output censorship using LLMs,\nhave proven to be fallible, as LLMs can still generate problematic responses.\nCommonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs. In\nthis paper, we present the theoretical limitations of such semantic censorship\napproaches. Specifically, we demonstrate that semantic censorship can be\nperceived as an undecidable problem, highlighting the inherent challenges in\ncensorship that arise due to LLMs' programmatic and instruction-following\ncapabilities. Furthermore, we argue that the challenges extend beyond semantic\ncensorship, as knowledgeable attackers can reconstruct impermissible outputs\nfrom a collection of permissible ones. As a result, we propose that the problem\nof censorship needs to be reevaluated; it should be treated as a security\nproblem which warrants the adaptation of security-based approaches to mitigate\npotential risks.",
        "pdf_link": "https://arxiv.org/pdf/2307.10719v1.pdf"
    },
    {
        "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
        "authors": [
            "Xiaoxuan Wang",
            "Ziniu Hu",
            "Pan Lu",
            "Yanqiao Zhu",
            "Jieyu Zhang",
            "Satyen Subramaniam",
            "Arjun R. Loomba",
            "Shichang Zhang",
            "Yizhou Sun",
            "Wei Wang"
        ],
        "published": "2023-07-20T07:01:57Z",
        "summary": "Most of the existing Large Language Model (LLM) benchmarks on scientific\nproblem reasoning focus on problems grounded in high-school subjects and are\nconfined to elementary algebraic operations. To systematically examine the\nreasoning capabilities required for solving complex scientific problems, we\nintroduce an expansive benchmark suite SciBench for LLMs. SciBench contains a\ncarefully curated dataset featuring a range of collegiate-level scientific\nproblems from mathematics, chemistry, and physics domains. Based on the\ndataset, we conduct an in-depth benchmarking study of representative\nopen-source and proprietary LLMs with various prompting strategies. The results\nreveal that the current LLMs fall short of delivering satisfactory performance,\nwith the best overall score of merely 43.22%. Furthermore, through a detailed\nuser study, we categorize the errors made by LLMs into ten problem-solving\nabilities. Our analysis indicates that no single prompting strategy\nsignificantly outperforms the others and some strategies that demonstrate\nimprovements in certain problem-solving skills could result in declines in\nother skills. We envision that SciBench will catalyze further developments in\nthe reasoning abilities of LLMs, thereby ultimately contributing to scientific\nresearch and discovery.",
        "pdf_link": "https://arxiv.org/pdf/2307.10635v2.pdf"
    },
    {
        "title": "What can we learn from Data Leakage and Unlearning for Law?",
        "authors": [
            "Jaydeep Borkar"
        ],
        "published": "2023-07-19T22:14:58Z",
        "summary": "Large Language Models (LLMs) have a privacy concern because they memorize\ntraining data (including personally identifiable information (PII) like emails\nand phone numbers) and leak it during inference. A company can train an LLM on\nits domain-customized data which can potentially also include their users' PII.\nIn order to comply with privacy laws such as the \"right to be forgotten\", the\ndata points of users that are most vulnerable to extraction could be deleted.\nWe find that once the most vulnerable points are deleted, a new set of points\nbecome vulnerable to extraction. So far, little attention has been given to\nunderstanding memorization for fine-tuned models. In this work, we also show\nthat not only do fine-tuned models leak their training data but they also leak\nthe pre-training data (and PII) memorized during the pre-training phase. The\nproperty of new data points becoming vulnerable to extraction after unlearning\nand leakage of pre-training data through fine-tuned models can pose significant\nprivacy and legal concerns for companies that use LLMs to offer services. We\nhope this work will start an interdisciplinary discussion within AI and law\ncommunities regarding the need for policies to tackle these issues.",
        "pdf_link": "https://arxiv.org/pdf/2307.10476v1.pdf"
    },
    {
        "title": "Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?",
        "authors": [
            "Omkar Dige",
            "Jacob-Junqi Tian",
            "David Emerson",
            "Faiza Khan Khattak"
        ],
        "published": "2023-07-19T22:03:40Z",
        "summary": "As the breadth and depth of language model applications continue to expand\nrapidly, it is increasingly important to build efficient frameworks for\nmeasuring and mitigating the learned or inherited social biases of these\nmodels. In this paper, we present our work on evaluating instruction fine-tuned\nlanguage models' ability to identify bias through zero-shot prompting,\nincluding Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction\nfine-tuned versions, Alpaca 7B performs best on the bias identification task\nwith an accuracy of 56.7%. We also demonstrate that scaling up LLM size and\ndata diversity could lead to further performance gain. This is a\nwork-in-progress presenting the first component of our bias mitigation\nframework. We will keep updating this work as we get more results.",
        "pdf_link": "https://arxiv.org/pdf/2307.10472v1.pdf"
    },
    {
        "title": "PharmacyGPT: The AI Pharmacist",
        "authors": [
            "Zhengliang Liu",
            "Zihao Wu",
            "Mengxuan Hu",
            "Bokai Zhao",
            "Lin Zhao",
            "Tianyi Zhang",
            "Haixing Dai",
            "Xianyan Chen",
            "Ye Shen",
            "Sheng Li",
            "Brian Murray",
            "Tianming Liu",
            "Andrea Sikora"
        ],
        "published": "2023-07-19T19:40:34Z",
        "summary": "In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies.",
        "pdf_link": "https://arxiv.org/pdf/2307.10432v2.pdf"
    },
    {
        "title": "Code Detection for Hardware Acceleration Using Large Language Models",
        "authors": [
            "Pablo Antonio Mart\u00ednez",
            "Gregorio Bernab\u00e9",
            "Jos\u00e9 Manuel Garc\u00eda"
        ],
        "published": "2023-07-19T17:21:58Z",
        "summary": "Large language models (LLMs) have been massively applied to many tasks, often\nsurpassing state-of-the-art approaches. While their effectiveness in code\ngeneration has been extensively studied (e.g., AlphaCode), their potential for\ncode detection remains unexplored.\n  This work presents the first analysis of code detection using LLMs. Our study\nexamines essential kernels, including matrix multiplication, convolution, and\nfast-fourier transform, implemented in C/C++. We propose both a preliminary,\nnaive prompt and a novel prompting strategy for code detection.\n  Results reveal that conventional prompting achieves great precision but poor\naccuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively)\ndue to a high number of false positives. Our novel prompting strategy\nsubstantially reduces false positives, resulting in excellent overall accuracy\n(91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable\nchallenge to existing state-of-the-art code detection methods.",
        "pdf_link": "https://arxiv.org/pdf/2307.10348v1.pdf"
    },
    {
        "title": "Generating Mathematical Derivations with Large Language Models",
        "authors": [
            "Jordan Meadows",
            "Marco Valentino",
            "Andre Freitas"
        ],
        "published": "2023-07-19T14:13:02Z",
        "summary": "The derivation of mathematical results in specialised fields, using Large\nLanguage Models (LLMs), is an emerging research direction that can help\nidentify models' limitations, and potentially support mathematical discovery.\nIn this paper, we leverage a symbolic engine to generate derivations of\nequations at scale, and investigate the capabilities of LLMs when deriving goal\nequations from premises. Specifically, we employ in-context learning for GPT\nand fine-tune a range of T5 models to compare the robustness and generalisation\nof pre-training strategies to specialised models. Empirical results show that\nfine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and\nout-of-distribution test sets in conventional scores. However, an in-depth\nanalysis reveals that the fine-tuned models are more sensitive to perturbations\ninvolving unseen symbols and (to a lesser extent) changes to equation\nstructure. In addition, we analyse 1.7K equations, and over 200 derivations, to\nhighlight common reasoning errors such as the inclusion of incorrect,\nirrelevant, and redundant equations. Finally, we explore the suitability of\nexisting metrics for evaluating mathematical derivations and find evidence\nthat, while they can capture general properties such as sensitivity to\nperturbations, they fail to highlight fine-grained reasoning errors and\nessential differences between models. Overall, this work demonstrates that\ntraining models on synthetic data may improve their math capabilities beyond\nmuch larger LLMs, but current metrics are not appropriately assessing the\nquality of generated mathematical text.",
        "pdf_link": "https://arxiv.org/pdf/2307.09998v3.pdf"
    },
    {
        "title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats",
        "authors": [
            "Xiaoxia Wu",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "published": "2023-07-19T06:58:03Z",
        "summary": "In the complex domain of large language models (LLMs), striking a balance\nbetween computational efficiency and maintaining model quality is a formidable\nchallenge. Navigating the inherent limitations of uniform quantization,\nparticularly when dealing with outliers, and motivated by the launch of\nNVIDIA's H100 hardware, this study delves into the viability of floating-point\n(FP) quantization, particularly focusing on FP8 and FP4, as a potential\nsolution. Our comprehensive investigation reveals that for LLMs, FP8 activation\nconsistently outshines its integer (INT8) equivalent, with the performance edge\nbecoming more noticeable in models possessing parameters beyond one billion.\nFor weight quantization, our findings indicate that FP4 exhibits comparable, if\nnot superior, performance to INT4, simplifying deployment on FP-supported\nhardware like H100. To mitigate the overhead from precision alignment caused by\nthe disparity between weights and activations, we propose two scaling\nconstraints for weight quantization that negligibly impact the performance\ncompared to the standard W4A8 model. We additionally enhance our quantization\nmethods by integrating the Low Rank Compensation (LoRC) strategy, yielding\nimprovements especially in smaller models. The results of our investigation\nemphasize the immense potential of FP quantization for LLMs, paving the way for\nhigh-efficiency deployment in resource-limited settings.",
        "pdf_link": "https://arxiv.org/pdf/2307.09782v2.pdf"
    },
    {
        "title": "CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility",
        "authors": [
            "Guohai Xu",
            "Jiayi Liu",
            "Ming Yan",
            "Haotian Xu",
            "Jinghui Si",
            "Zhuoran Zhou",
            "Peng Yi",
            "Xing Gao",
            "Jitao Sang",
            "Rong Zhang",
            "Ji Zhang",
            "Chao Peng",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "published": "2023-07-19T01:22:40Z",
        "summary": "With the rapid evolution of large language models (LLMs), there is a growing\nconcern that they may pose risks or have negative social impacts. Therefore,\nevaluation of human values alignment is becoming increasingly important.\nPrevious work mainly focuses on assessing the performance of LLMs on certain\nknowledge and reasoning abilities, while neglecting the alignment to human\nvalues, especially in a Chinese context. In this paper, we present CValues, the\nfirst Chinese human values evaluation benchmark to measure the alignment\nability of LLMs in terms of both safety and responsibility criteria. As a\nresult, we have manually collected adversarial safety prompts across 10\nscenarios and induced responsibility prompts from 8 domains by professional\nexperts. To provide a comprehensive values evaluation of Chinese LLMs, we not\nonly conduct human evaluation for reliable comparison, but also construct\nmulti-choice prompts for automatic evaluation. Our findings suggest that while\nmost Chinese LLMs perform well in terms of safety, there is considerable room\nfor improvement in terms of responsibility. Moreover, both the automatic and\nhuman evaluation are important for assessing the human values alignment in\ndifferent aspects. The benchmark and code is available on ModelScope and\nGithub.",
        "pdf_link": "https://arxiv.org/pdf/2307.09705v1.pdf"
    },
    {
        "title": "Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study",
        "authors": [
            "Damith Premasiri",
            "Tharindu Ranasinghe",
            "Ruslan Mitkov"
        ],
        "published": "2023-07-18T18:21:26Z",
        "summary": "Text classification is an area of research which has been studied over the\nyears in Natural Language Processing (NLP). Adapting NLP to multiple domains\nhas introduced many new challenges for text classification and one of them is\nlong document classification. While state-of-the-art transformer models provide\nexcellent results in text classification, most of them have limitations in the\nmaximum sequence length of the input sequence. The majority of the transformer\nmodels are limited to 512 tokens, and therefore, they struggle with long\ndocument classification problems. In this research, we explore on employing\nModel Fusing for long document classification while comparing the results with\nwell-known BERT and Longformer architectures.",
        "pdf_link": "https://arxiv.org/pdf/2307.09532v1.pdf"
    },
    {
        "title": "Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications",
        "authors": [
            "Vishesh Thakur"
        ],
        "published": "2023-07-18T11:38:45Z",
        "summary": "Gender bias in artificial intelligence (AI) and natural language processing\nhas garnered significant attention due to its potential impact on societal\nperceptions and biases. This research paper aims to analyze gender bias in\nLarge Language Models (LLMs) with a focus on multiple comparisons between GPT-2\nand GPT-3.5, some prominent language models, to better understand its\nimplications. Through a comprehensive literature review, the study examines\nexisting research on gender bias in AI language models and identifies gaps in\nthe current knowledge. The methodology involves collecting and preprocessing\ndata from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis\ntechniques to evaluate gender bias in the generated text. The findings shed\nlight on gendered word associations, language usage, and biased narratives\npresent in the outputs of these Large Language Models. The discussion explores\nthe ethical implications of gender bias and its potential consequences on\nsocial perceptions and marginalized communities. Additionally, the paper\npresents strategies for reducing gender bias in LLMs, including algorithmic\napproaches and data augmentation techniques. The research highlights the\nimportance of interdisciplinary collaborations and the role of sociological\nstudies in mitigating gender bias in AI models. By addressing these issues, we\ncan pave the way for more inclusive and unbiased AI systems that have a\npositive impact on society.",
        "pdf_link": "https://arxiv.org/pdf/2307.09162v3.pdf"
    },
    {
        "title": "On the (In)Effectiveness of Large Language Models for Chinese Text Correction",
        "authors": [
            "Yinghui Li",
            "Haojing Huang",
            "Shirong Ma",
            "Yong Jiang",
            "Yangning Li",
            "Feng Zhou",
            "Hai-Tao Zheng",
            "Qingyu Zhou"
        ],
        "published": "2023-07-18T06:48:52Z",
        "summary": "Recently, the development and progress of Large Language Models (LLMs) have\namazed the entire Artificial Intelligence community. Benefiting from their\nemergent abilities, LLMs have attracted more and more researchers to study\ntheir capabilities and performance on various downstream Natural Language\nProcessing (NLP) tasks. While marveling at LLMs' incredible performance on all\nkinds of tasks, we notice that they also have excellent multilingual processing\ncapabilities, such as Chinese. To explore the Chinese processing ability of\nLLMs, we focus on Chinese Text Correction, a fundamental and challenging\nChinese NLP task. Specifically, we evaluate various representative LLMs on the\nChinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC)\ntasks, which are two main Chinese Text Correction scenarios. Additionally, we\nalso fine-tune LLMs for Chinese Text Correction to better observe the potential\ncapabilities of LLMs. From extensive analyses and comparisons with previous\nstate-of-the-art small models, we empirically find that the LLMs currently have\nboth amazing performance and unsatisfactory behavior for Chinese Text\nCorrection. We believe our findings will promote the landing and application of\nLLMs in the Chinese NLP community.",
        "pdf_link": "https://arxiv.org/pdf/2307.09007v2.pdf"
    },
    {
        "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
        "authors": [
            "Yanda Chen",
            "Ruiqi Zhong",
            "Narutatsu Ri",
            "Chen Zhao",
            "He He",
            "Jacob Steinhardt",
            "Zhou Yu",
            "Kathleen McKeown"
        ],
        "published": "2023-07-17T17:41:47Z",
        "summary": "Large language models (LLMs) are trained to imitate humans to explain human\ndecisions. However, do LLMs explain themselves? Can they help humans build\nmental models of how LLMs process different inputs? To answer these questions,\nwe propose to evaluate $\\textbf{counterfactual simulatability}$ of natural\nlanguage explanations: whether an explanation can enable humans to precisely\ninfer the model's outputs on diverse counterfactuals of the explained input.\nFor example, if a model answers \"yes\" to the input question \"Can eagles fly?\"\nwith the explanation \"all birds can fly\", then humans would infer from the\nexplanation that it would also answer \"yes\" to the counterfactual input \"Can\npenguins fly?\". If the explanation is precise, then the model's answer should\nmatch humans' expectations.\n  We implemented two metrics based on counterfactual simulatability: precision\nand generality. We generated diverse counterfactuals automatically using LLMs.\nWe then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on\ntwo tasks: multi-hop factual reasoning and reward modeling. We found that LLM's\nexplanations have low precision and that precision does not correlate with\nplausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may\nnot be a sufficient solution.",
        "pdf_link": "https://arxiv.org/pdf/2307.08678v1.pdf"
    },
    {
        "title": "TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",
        "authors": [
            "Liangyu Zha",
            "Junlin Zhou",
            "Liyao Li",
            "Rui Wang",
            "Qingyi Huang",
            "Saisai Yang",
            "Jing Yuan",
            "Changbao Su",
            "Xiang Li",
            "Aofeng Su",
            "Tao Zhang",
            "Chen Zhou",
            "Kaizhe Shou",
            "Miao Wang",
            "Wufang Zhu",
            "Guoshan Lu",
            "Chao Ye",
            "Yali Ye",
            "Wentao Ye",
            "Yiming Zhang",
            "Xinglong Deng",
            "Jie Xu",
            "Haobo Wang",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "published": "2023-07-17T17:36:09Z",
        "summary": "Tables are prevalent in real-world databases, requiring significant time and\neffort for humans to analyze and manipulate. The advancements in large language\nmodels (LLMs) have made it possible to interact with tables using natural\nlanguage input, bringing this capability closer to reality. In this paper, we\npresent TableGPT, a unified fine-tuned framework that enables LLMs to\nunderstand and operate on tables using external functional commands. It\nintroduces the capability to seamlessly interact with tables, enabling a wide\nrange of functionalities such as question answering, data manipulation (e.g.,\ninsert, delete, query, and modify operations), data visualization, analysis\nreport generation, and automated prediction. TableGPT aims to provide\nconvenience and accessibility to users by empowering them to effortlessly\nleverage tabular data. At the core of TableGPT lies the novel concept of global\ntabular representations, which empowers LLMs to gain a comprehensive\nunderstanding of the entire table beyond meta-information. By jointly training\nLLMs on both table and text modalities, TableGPT achieves a deep understanding\nof tabular data and the ability to perform complex operations on tables through\nchain-of-command instructions. Importantly, TableGPT offers the advantage of\nbeing a self-contained system rather than relying on external API interfaces.\nMoreover, it supports efficient data process flow, query rejection (when\nappropriate) and private deployment, enabling faster domain data fine-tuning\nand ensuring data privacy, which enhances the framework's adaptability to\nspecific use cases.",
        "pdf_link": "https://arxiv.org/pdf/2307.08674v3.pdf"
    },
    {
        "title": "Measuring Faithfulness in Chain-of-Thought Reasoning",
        "authors": [
            "Tamera Lanham",
            "Anna Chen",
            "Ansh Radhakrishnan",
            "Benoit Steiner",
            "Carson Denison",
            "Danny Hernandez",
            "Dustin Li",
            "Esin Durmus",
            "Evan Hubinger",
            "Jackson Kernion",
            "Kamil\u0117 Luko\u0161i\u016bt\u0117",
            "Karina Nguyen",
            "Newton Cheng",
            "Nicholas Joseph",
            "Nicholas Schiefer",
            "Oliver Rausch",
            "Robin Larson",
            "Sam McCandlish",
            "Sandipan Kundu",
            "Saurav Kadavath",
            "Shannon Yang",
            "Thomas Henighan",
            "Timothy Maxwell",
            "Timothy Telleen-Lawton",
            "Tristan Hume",
            "Zac Hatfield-Dodds",
            "Jared Kaplan",
            "Jan Brauner",
            "Samuel R. Bowman",
            "Ethan Perez"
        ],
        "published": "2023-07-17T01:08:39Z",
        "summary": "Large language models (LLMs) perform better when they produce step-by-step,\n\"Chain-of-Thought\" (CoT) reasoning before answering a question, but it is\nunclear if the stated reasoning is a faithful explanation of the model's actual\nreasoning (i.e., its process for answering the question). We investigate\nhypotheses for how CoT reasoning may be unfaithful, by examining how the model\npredictions change when we intervene on the CoT (e.g., by adding mistakes or\nparaphrasing it). Models show large variation across tasks in how strongly they\ncondition on the CoT when predicting their answer, sometimes relying heavily on\nthe CoT and other times primarily ignoring it. CoT's performance boost does not\nseem to come from CoT's added test-time compute alone or from information\nencoded via the particular phrasing of the CoT. As models become larger and\nmore capable, they produce less faithful reasoning on most tasks we study.\nOverall, our results suggest that CoT can be faithful if the circumstances such\nas the model size and task are carefully chosen.",
        "pdf_link": "https://arxiv.org/pdf/2307.13702v1.pdf"
    },
    {
        "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning",
        "authors": [
            "Ansh Radhakrishnan",
            "Karina Nguyen",
            "Anna Chen",
            "Carol Chen",
            "Carson Denison",
            "Danny Hernandez",
            "Esin Durmus",
            "Evan Hubinger",
            "Jackson Kernion",
            "Kamil\u0117 Luko\u0161i\u016bt\u0117",
            "Newton Cheng",
            "Nicholas Joseph",
            "Nicholas Schiefer",
            "Oliver Rausch",
            "Sam McCandlish",
            "Sheer El Showk",
            "Tamera Lanham",
            "Tim Maxwell",
            "Venkatesa Chandrasekaran",
            "Zac Hatfield-Dodds",
            "Jared Kaplan",
            "Jan Brauner",
            "Samuel R. Bowman",
            "Ethan Perez"
        ],
        "published": "2023-07-17T00:54:10Z",
        "summary": "As large language models (LLMs) perform more difficult tasks, it becomes\nharder to verify the correctness and safety of their behavior. One approach to\nhelp with this issue is to prompt LLMs to externalize their reasoning, e.g., by\nhaving them generate step-by-step reasoning as they answer a question\n(Chain-of-Thought; CoT). The reasoning may enable us to check the process that\nmodels use to perform tasks. However, this approach relies on the stated\nreasoning faithfully reflecting the model's actual reasoning, which is not\nalways the case. To improve over the faithfulness of CoT reasoning, we have\nmodels generate reasoning by decomposing questions into subquestions.\nDecomposition-based methods achieve strong performance on question-answering\ntasks, sometimes approaching that of CoT while improving the faithfulness of\nthe model's stated reasoning on several recently-proposed metrics. By forcing\nthe model to answer simpler subquestions in separate contexts, we greatly\nincrease the faithfulness of model-generated reasoning over CoT, while still\nachieving some of the performance gains of CoT. Our results show it is possible\nto improve the faithfulness of model-generated reasoning; continued\nimprovements may lead to reasoning that enables us to verify the correctness\nand safety of LLM behavior.",
        "pdf_link": "https://arxiv.org/pdf/2307.11768v2.pdf"
    },
    {
        "title": "The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant",
        "authors": [
            "Jingqing Zhang",
            "Kai Sun",
            "Akshay Jagadeesh",
            "Mahta Ghahfarokhi",
            "Deepa Gupta",
            "Ashok Gupta",
            "Vibhor Gupta",
            "Yike Guo"
        ],
        "published": "2023-07-16T21:19:47Z",
        "summary": "Recent studies have demonstrated promising performance of ChatGPT and GPT-4\non several medical domain tasks. However, none have assessed its performance\nusing a large-scale real-world electronic health record database, nor have\nevaluated its utility in providing clinical diagnostic assistance for patients\nacross a full range of disease presentation. We performed two analyses using\nChatGPT and GPT-4, one to identify patients with specific medical diagnoses\nusing a real-world large electronic health record database and the other, in\nproviding diagnostic assistance to healthcare workers in the prospective\nevaluation of hypothetical patients. Our results show that GPT-4 across disease\nclassification tasks with chain of thought and few-shot prompting can achieve\nperformance as high as 96% F1 scores. For patient assessment, GPT-4 can\naccurately diagnose three out of four times. However, there were mentions of\nfactually incorrect statements, overlooking crucial medical findings,\nrecommendations for unnecessary investigations and overtreatment. These issues\ncoupled with privacy concerns, make these models currently inadequate for real\nworld clinical use. However, limited data and time needed for prompt\nengineering in comparison to configuration of conventional machine learning\nworkflows highlight their potential for scalability across healthcare\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2307.08152v1.pdf"
    },
    {
        "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
        "authors": [
            "Peiyu Liu",
            "Zikang Liu",
            "Ze-Feng Gao",
            "Dawei Gao",
            "Wayne Xin Zhao",
            "Yaliang Li",
            "Bolin Ding",
            "Ji-Rong Wen"
        ],
        "published": "2023-07-16T15:11:01Z",
        "summary": "Despite the superior performance, Large Language Models~(LLMs) require\nsignificant computational resources for deployment and use. To overcome this\nissue, quantization methods have been widely applied to reduce the memory\nfootprint of LLMs as well as increasing the inference rate. However, a major\nchallenge is that low-bit quantization methods often lead to performance\ndegradation. It is important to understand how quantization impacts the\ncapacity of LLMs. Different from previous studies focused on overall\nperformance, this work aims to investigate the impact of quantization on\n\\emph{emergent abilities}, which are important characteristics that distinguish\nLLMs from small language models. Specially, we examine the abilities of\nin-context learning, chain-of-thought reasoning, and instruction-following in\nquantized LLMs. Our empirical experiments show that these emergent abilities\nstill exist in 4-bit quantization models, while 2-bit models encounter severe\nperformance degradation on the test of these abilities. To improve the\nperformance of low-bit models, we conduct two special experiments: (1)\nfine-gained impact analysis that studies which components (or substructures)\nare more sensitive to quantization, and (2) performance compensation through\nmodel fine-tuning. Our work derives a series of important findings to\nunderstand the impact of quantization on emergent abilities, and sheds lights\non the possibilities of extremely low-bit quantization for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.08072v2.pdf"
    },
    {
        "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
        "authors": [
            "Yuheng Huang",
            "Jiayang Song",
            "Zhijie Wang",
            "Shengming Zhao",
            "Huaming Chen",
            "Felix Juefei-Xu",
            "Lei Ma"
        ],
        "published": "2023-07-16T08:28:04Z",
        "summary": "The recent performance leap of Large Language Models (LLMs) opens up new\nopportunities across numerous industrial applications and domains. However,\nerroneous generations, such as false predictions, misinformation, and\nhallucination made by LLMs, have also raised severe concerns for the\ntrustworthiness of LLMs', especially in safety-, security- and\nreliability-sensitive scenarios, potentially hindering real-world adoptions.\nWhile uncertainty estimation has shown its potential for interpreting the\nprediction risks made by general machine learning (ML) models, little is known\nabout whether and to what extent it can help explore an LLM's capabilities and\ncounteract its undesired behavior. To bridge the gap, in this paper, we\ninitiate an exploratory study on the risk assessment of LLMs from the lens of\nuncertainty. In particular, we experiment with twelve uncertainty estimation\nmethods and four LLMs on four prominent natural language processing (NLP) tasks\nto investigate to what extent uncertainty estimation techniques could help\ncharacterize the prediction risks of LLMs. Our findings validate the\neffectiveness of uncertainty estimation for revealing LLMs'\nuncertain/non-factual predictions. In addition to general NLP tasks, we\nextensively conduct experiments with four LLMs for code generation on two\ndatasets. We find that uncertainty estimation can potentially uncover buggy\nprograms generated by LLMs. Insights from our study shed light on future design\nand development for reliable LLMs, facilitating further research toward\nenhancing the trustworthiness of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.10236v3.pdf"
    },
    {
        "title": "Leveraging Large Language Models to Generate Answer Set Programs",
        "authors": [
            "Adam Ishay",
            "Zhun Yang",
            "Joohyung Lee"
        ],
        "published": "2023-07-15T03:40:55Z",
        "summary": "Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated\nexceptional performance in various natural language processing tasks and have\nshown the ability to solve certain reasoning problems. However, their reasoning\ncapabilities are limited and relatively shallow, despite the application of\nvarious prompting techniques. In contrast, formal logic is adept at handling\ncomplex reasoning, but translating natural language descriptions into formal\nlogic is a challenging task that non-experts struggle with. This paper proposes\na neuro-symbolic method that combines the strengths of large language models\nand answer set programming. Specifically, we employ an LLM to transform natural\nlanguage descriptions of logic puzzles into answer set programs. We carefully\ndesign prompts for an LLM to convert natural language descriptions into answer\nset programs in a step by step manner. Surprisingly, with just a few in-context\nlearning examples, LLMs can generate reasonably complex answer set programs.\nThe majority of errors made are relatively simple and can be easily corrected\nby humans, thus enabling LLMs to effectively assist in the creation of answer\nset programs.",
        "pdf_link": "https://arxiv.org/pdf/2307.07699v1.pdf"
    },
    {
        "title": "Can Large Language Models Empower Molecular Property Prediction?",
        "authors": [
            "Chen Qian",
            "Huayi Tang",
            "Zhirui Yang",
            "Hong Liang",
            "Yong Liu"
        ],
        "published": "2023-07-14T16:06:42Z",
        "summary": "Molecular property prediction has gained significant attention due to its\ntransformative potential in multiple scientific disciplines. Conventionally, a\nmolecule graph can be represented either as a graph-structured data or a SMILES\ntext. Recently, the rapid development of Large Language Models (LLMs) has\nrevolutionized the field of NLP. Although it is natural to utilize LLMs to\nassist in understanding molecules represented by SMILES, the exploration of how\nLLMs will impact molecular property prediction is still in its early stage. In\nthis work, we advance towards this objective through two perspectives:\nzero/few-shot molecular classification, and using the new explanations\ngenerated by LLMs as representations of molecules. To be specific, we first\nprompt LLMs to do in-context molecular classification and evaluate their\nperformance. After that, we employ LLMs to generate semantically enriched\nexplanations for the original SMILES and then leverage that to fine-tune a\nsmall-scale LM model for multiple downstream tasks. The experimental results\nhighlight the superiority of text explanations as molecular representations\nacross multiple benchmark datasets, and confirm the immense potential of LLMs\nin molecular property prediction tasks. Codes are available at\n\\url{https://github.com/ChnQ/LLM4Mol}.",
        "pdf_link": "https://arxiv.org/pdf/2307.07443v1.pdf"
    },
    {
        "title": "Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow",
        "authors": [
            "Maria del Rio-Chanona",
            "Nadzeya Laurentsyeva",
            "Johannes Wachs"
        ],
        "published": "2023-07-14T14:22:12Z",
        "summary": "Large language models like ChatGPT efficiently provide users with information\nabout various topics, presenting a potential substitute for searching the web\nand asking people for help online. But since users interact privately with the\nmodel, these models may drastically reduce the amount of publicly available\nhuman-generated data and knowledge resources. This substitution can present a\nsignificant problem in securing training data for future models. In this work,\nwe investigate how the release of ChatGPT changed human-generated open data on\nthe web by analyzing the activity on Stack Overflow, the leading online Q\\&A\nplatform for computer programming. We find that relative to its Russian and\nChinese counterparts, where access to ChatGPT is limited, and to similar forums\nfor mathematics, where ChatGPT is less capable, activity on Stack Overflow\nsignificantly decreased. A difference-in-differences model estimates a 16\\%\ndecrease in weekly posts on Stack Overflow. This effect increases in magnitude\nover time, and is larger for posts related to the most widely used programming\nlanguages. Posts made after ChatGPT get similar voting scores than before,\nsuggesting that ChatGPT is not merely displacing duplicate or low-quality\ncontent. These results suggest that more users are adopting large language\nmodels to answer questions and they are better substitutes for Stack Overflow\nfor languages for which they have more training data. Using models like ChatGPT\nmay be more efficient for solving certain programming problems, but its\nwidespread adoption and the resulting shift away from public exchange on the\nweb will limit the open data people and models can learn from in the future.",
        "pdf_link": "https://arxiv.org/pdf/2307.07367v1.pdf"
    },
    {
        "title": "Certified Robustness for Large Language Models with Self-Denoising",
        "authors": [
            "Zhen Zhang",
            "Guanhua Zhang",
            "Bairu Hou",
            "Wenqi Fan",
            "Qing Li",
            "Sijia Liu",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "published": "2023-07-14T05:40:24Z",
        "summary": "Although large language models (LLMs) have achieved great success in vast\nreal-world applications, their vulnerabilities towards noisy inputs have\nsignificantly limited their uses, especially in high-stake environments. In\nthese contexts, it is crucial to ensure that every prediction made by large\nlanguage models is stable, i.e., LLM predictions should be consistent given\nminor differences in the input. This largely falls into the study of certified\nrobust LLMs, i.e., all predictions of LLM are certified to be correct in a\nlocal region around the input. Randomized smoothing has demonstrated great\npotential in certifying the robustness and prediction stability of LLMs.\nHowever, randomized smoothing requires adding noise to the input before model\nprediction, and its certification performance depends largely on the model's\nperformance on corrupted data. As a result, its direct application to LLMs\nremains challenging and often results in a small certification radius. To\naddress this issue, we take advantage of the multitasking nature of LLMs and\npropose to denoise the corrupted inputs with LLMs in a self-denoising manner.\nDifferent from previous works like denoised smoothing, which requires training\na separate model to robustify LLM, our method enjoys far better efficiency and\nflexibility. Our experiment results show that our method outperforms the\nexisting certification methods under both certified robustness and empirical\nrobustness. The codes are available at\nhttps://github.com/UCSB-NLP-Chang/SelfDenoise.",
        "pdf_link": "https://arxiv.org/pdf/2307.07171v1.pdf"
    },
    {
        "title": "Generating Efficient Training Data via LLM-based Attribute Manipulation",
        "authors": [
            "Letian Peng",
            "Yuwei Zhang",
            "Jingbo Shang"
        ],
        "published": "2023-07-14T00:10:03Z",
        "summary": "In this paper, we propose a novel method, Chain-of-Thoughts Attribute\nManipulation (CoTAM), to guide few-shot learning by carefully crafted data from\nLarge Language Models (LLMs). The main idea is to create data with changes only\nin the attribute targeted by the task. Inspired by facial attribute\nmanipulation, our approach generates label-switched data by leveraging LLMs to\nmanipulate task-specific attributes and reconstruct new sentences in a\ncontrolled manner. Instead of conventional latent representation controlling,\nwe implement chain-of-thoughts decomposition and reconstruction to adapt the\nprocedure to LLMs. Extensive results on text classification and other tasks\nverify the advantage of CoTAM over other LLM-based text generation methods with\nthe same number of training examples. Analysis visualizes the attribute\nmanipulation effectiveness of CoTAM and presents the potential of LLM-guided\nlearning with even less supervision.",
        "pdf_link": "https://arxiv.org/pdf/2307.07099v1.pdf"
    },
    {
        "title": "A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023",
        "authors": [
            "Yi Cheng",
            "Ziwei Xu",
            "Fen Fang",
            "Dongyun Lin",
            "Hehe Fan",
            "Yongkang Wong",
            "Ying Sun",
            "Mohan Kankanhalli"
        ],
        "published": "2023-07-13T05:54:05Z",
        "summary": "In this technical report, we present our findings from a study conducted on\nthe EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for Action\nRecognition. Our research focuses on the innovative application of a\ndifferentiable logic loss in the training to leverage the co-occurrence\nrelations between verb and noun, as well as the pre-trained Large Language\nModels (LLMs) to generate the logic rules for the adaptation to unseen action\nlabels. Specifically, the model's predictions are treated as the truth\nassignment of a co-occurrence logic formula to compute the logic loss, which\nmeasures the consistency between the predictions and the logic constraints. By\nusing the verb-noun co-occurrence matrix generated from the dataset, we observe\na moderate improvement in model performance compared to our baseline framework.\nTo further enhance the model's adaptability to novel action labels, we\nexperiment with rules generated using GPT-3.5, which leads to a slight decrease\nin performance. These findings shed light on the potential and challenges of\nincorporating differentiable logic and LLMs for knowledge extraction in\nunsupervised domain adaptation for action recognition. Our final submission\n(entitled `NS-LLM') achieved the first place in terms of top-1 action\nrecognition accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2307.06569v1.pdf"
    },
    {
        "title": "Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study",
        "authors": [
            "Zeping Min",
            "Jinbo Wang"
        ],
        "published": "2023-07-13T02:31:55Z",
        "summary": "This paper explores the integration of Large Language Models (LLMs) into\nAutomatic Speech Recognition (ASR) systems to improve transcription accuracy.\nThe increasing sophistication of LLMs, with their in-context learning\ncapabilities and instruction-following behavior, has drawn significant\nattention in the field of Natural Language Processing (NLP). Our primary focus\nis to investigate the potential of using an LLM's in-context learning\ncapabilities to enhance the performance of ASR systems, which currently face\nchallenges such as ambient noise, speaker accents, and complex linguistic\ncontexts. We designed a study using the Aishell-1 and LibriSpeech datasets,\nwith ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.\nUnfortunately, our initial experiments did not yield promising results,\nindicating the complexity of leveraging LLM's in-context learning for ASR\napplications. Despite further exploration with varied settings and models, the\ncorrected sentences from the LLMs frequently resulted in higher Word Error\nRates (WER), demonstrating the limitations of LLMs in speech applications. This\npaper provides a detailed overview of these experiments, their results, and\nimplications, establishing that using LLMs' in-context learning capabilities to\ncorrect potential errors in speech recognition transcriptions is still a\nchallenging task at the current stage.",
        "pdf_link": "https://arxiv.org/pdf/2307.06530v1.pdf"
    },
    {
        "title": "AutoHint: Automatic Prompt Optimization with Hint Generation",
        "authors": [
            "Hong Sun",
            "Xue Li",
            "Yinchuan Xu",
            "Youkow Homma",
            "Qi Cao",
            "Min Wu",
            "Jian Jiao",
            "Denis Charles"
        ],
        "published": "2023-07-13T00:49:27Z",
        "summary": "This paper presents AutoHint, a novel framework for automatic prompt\nengineering and optimization for Large Language Models (LLM). While LLMs have\ndemonstrated remarkable ability in achieving high-quality annotation in various\ntasks, the key to applying this ability to specific tasks lies in developing\nhigh-quality prompts. Thus we propose a framework to inherit the merits of both\nin-context learning and zero-shot learning by incorporating enriched\ninstructions derived from input-output demonstrations to optimize original\nprompt. We refer to the enrichment as the hint and propose a framework to\nautomatically generate the hint from labeled data. More concretely, starting\nfrom an initial prompt, our method first instructs a LLM to deduce new hints\nfor selected samples from incorrect predictions, and then summarizes from\nper-sample hints and adds the results back to the initial prompt to form a new,\nenriched instruction. The proposed method is evaluated on the BIG-Bench\nInstruction Induction dataset for both zero-shot and few-short prompts, where\nexperiments demonstrate our method is able to significantly boost accuracy for\nmultiple tasks.",
        "pdf_link": "https://arxiv.org/pdf/2307.07415v2.pdf"
    },
    {
        "title": "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation",
        "authors": [
            "Kaiyi Huang",
            "Kaiyue Sun",
            "Enze Xie",
            "Zhenguo Li",
            "Xihui Liu"
        ],
        "published": "2023-07-12T17:59:42Z",
        "summary": "Despite the stunning ability to generate high-quality images by recent\ntext-to-image models, current approaches often struggle to effectively compose\nobjects with different attributes and relationships into a complex and coherent\nscene. We propose T2I-CompBench, a comprehensive benchmark for open-world\ncompositional text-to-image generation, consisting of 6,000 compositional text\nprompts from 3 categories (attribute binding, object relationships, and complex\ncompositions) and 6 sub-categories (color binding, shape binding, texture\nbinding, spatial relationships, non-spatial relationships, and complex\ncompositions). We further propose several evaluation metrics specifically\ndesigned to evaluate compositional text-to-image generation and explore the\npotential and limitations of multimodal LLMs for evaluation. We introduce a new\napproach, Generative mOdel fine-tuning with Reward-driven Sample selection\n(GORS), to boost the compositional text-to-image generation abilities of\npretrained text-to-image models. Extensive experiments and evaluations are\nconducted to benchmark previous methods on T2I-CompBench, and to validate the\neffectiveness of our proposed evaluation metrics and GORS approach. Project\npage is available at https://karine-h.github.io/T2I-CompBench/.",
        "pdf_link": "https://arxiv.org/pdf/2307.06350v2.pdf"
    },
    {
        "title": "Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models",
        "authors": [
            "Dhruv Mullick",
            "Bilal Ghanem",
            "Alona Fyshe"
        ],
        "published": "2023-07-11T12:43:28Z",
        "summary": "Customer feedback is invaluable to companies as they refine their products.\nMonitoring customer feedback can be automated with Aspect Level Sentiment\nClassification (ALSC) which allows us to analyse specific aspects of the\nproducts in reviews. Large Language Models (LLMs) are the heart of many\nstate-of-the-art ALSC solutions, but they perform poorly in some scenarios\nrequiring Coreference Resolution (CR). In this work, we propose a framework to\nimprove an LLM's performance on CR-containing reviews by fine tuning on highly\ninferential tasks. We show that the performance improvement is likely\nattributed to the improved model CR ability. We also release a new dataset that\nfocuses on CR in ALSC.",
        "pdf_link": "https://arxiv.org/pdf/2307.05646v1.pdf"
    },
    {
        "title": "Secrets of RLHF in Large Language Models Part I: PPO",
        "authors": [
            "Rui Zheng",
            "Shihan Dou",
            "Songyang Gao",
            "Yuan Hua",
            "Wei Shen",
            "Binghai Wang",
            "Yan Liu",
            "Senjie Jin",
            "Qin Liu",
            "Yuhao Zhou",
            "Limao Xiong",
            "Lu Chen",
            "Zhiheng Xi",
            "Nuo Xu",
            "Wenbin Lai",
            "Minghao Zhu",
            "Cheng Chang",
            "Zhangyue Yin",
            "Rongxiang Weng",
            "Wensen Cheng",
            "Haoran Huang",
            "Tianxiang Sun",
            "Hang Yan",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2023-07-11T01:55:24Z",
        "summary": "Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes, aiming to make modest\ncontributions to the advancement of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.04964v2.pdf"
    },
    {
        "title": "AmadeusGPT: a natural language interface for interactive animal behavioral analysis",
        "authors": [
            "Shaokai Ye",
            "Jessy Lauer",
            "Mu Zhou",
            "Alexander Mathis",
            "Mackenzie W. Mathis"
        ],
        "published": "2023-07-10T19:15:17Z",
        "summary": "The process of quantifying and analyzing animal behavior involves translating\nthe naturally occurring descriptive language of their actions into\nmachine-readable code. Yet, codifying behavior analysis is often challenging\nwithout deep understanding of animal behavior and technical machine learning\nknowledge. To limit this gap, we introduce AmadeusGPT: a natural language\ninterface that turns natural language descriptions of behaviors into\nmachine-executable code. Large-language models (LLMs) such as GPT3.5 and GPT4\nallow for interactive language-based queries that are potentially well suited\nfor making interactive behavior analysis. However, the comprehension capability\nof these LLMs is limited by the context window size, which prevents it from\nremembering distant conversations. To overcome the context window limitation,\nwe implement a novel dual-memory mechanism to allow communication between\nshort-term and long-term memory using symbols as context pointers for retrieval\nand saving. Concretely, users directly use language-based definitions of\nbehavior and our augmented GPT develops code based on the core AmadeusGPT API,\nwhich contains machine learning, computer vision, spatio-temporal reasoning,\nand visualization modules. Users then can interactively refine results, and\nseamlessly add new behavioral modules as needed. We benchmark AmadeusGPT and\nshow we can produce state-of-the-art performance on the MABE 2022 behavior\nchallenge tasks. Note, an end-user would not need to write any code to achieve\nthis. Thus, collectively AmadeusGPT presents a novel way to merge deep\nbiological knowledge, large-language models, and core computer vision modules\ninto a more naturally intelligent system. Code and demos can be found at:\nhttps://github.com/AdaptiveMotorControlLab/AmadeusGPT.",
        "pdf_link": "https://arxiv.org/pdf/2307.04858v1.pdf"
    },
    {
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
        "authors": [
            "Jiaming Ji",
            "Mickel Liu",
            "Juntao Dai",
            "Xuehai Pan",
            "Chi Zhang",
            "Ce Bian",
            "Chi Zhang",
            "Ruiyang Sun",
            "Yizhou Wang",
            "Yaodong Yang"
        ],
        "published": "2023-07-10T15:56:17Z",
        "summary": "In this paper, we introduce the BeaverTails dataset, aimed at fostering\nresearch on safety alignment in large language models (LLMs). This dataset\nuniquely separates annotations of helpfulness and harmlessness for\nquestion-answering pairs, thus offering distinct perspectives on these crucial\nattributes. In total, we have gathered safety meta-labels for 333,963\nquestion-answer (QA) pairs and 361,903 pairs of expert comparison data for both\nthe helpfulness and harmlessness metrics. We further showcase applications of\nBeaverTails in content moderation and reinforcement learning with human\nfeedback (RLHF), emphasizing its potential for practical safety measures in\nLLMs. We believe this dataset provides vital resources for the community,\ncontributing towards the safe development and deployment of LLMs. Our project\npage is available at the following URL:\nhttps://sites.google.com/view/pku-beavertails.",
        "pdf_link": "https://arxiv.org/pdf/2307.04657v3.pdf"
    },
    {
        "title": "Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training",
        "authors": [
            "Dima Galat",
            "Marian-Andrei Rizoiu"
        ],
        "published": "2023-07-10T08:32:45Z",
        "summary": "Biomedical summarization requires large datasets to train for text\ngeneration. We show that while transfer learning offers a viable option for\naddressing this challenge, an in-domain pre-training does not always offer\nadvantages in a BioASQ summarization task. We identify a suitable model\narchitecture and use it to show a benefit of a general-domain pre-training\nfollowed by a task-specific fine-tuning in the context of a BioASQ\nsummarization task, leading to a novel three-step fine-tuning approach that\nworks with only a thousand in-domain examples. Our results indicate that a\nLarge Language Model without domain-specific pre-training can have a\nsignificant edge in some domain-specific biomedical text generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2307.04412v1.pdf"
    },
    {
        "title": "TIM: Teaching Large Language Models to Translate with Comparison",
        "authors": [
            "Jiali Zeng",
            "Fandong Meng",
            "Yongjing Yin",
            "Jie Zhou"
        ],
        "published": "2023-07-10T08:15:40Z",
        "summary": "Open-sourced large language models (LLMs) have demonstrated remarkable\nefficacy in various tasks with instruction tuning. However, these models can\nsometimes struggle with tasks that require more specialized knowledge such as\ntranslation. One possible reason for such deficiency is that instruction tuning\naims to generate fluent and coherent text that continues from a given\ninstruction without being constrained by any task-specific requirements.\nMoreover, it can be more challenging for tuning smaller LLMs with lower-quality\ntraining data. To address this issue, we propose a novel framework using\nexamples in comparison to teach LLMs to learn translation. Our approach\ninvolves presenting the model with examples of correct and incorrect\ntranslations and using a preference loss to guide the model's learning. We\nevaluate our method on WMT2022 test sets and show that it outperforms existing\nmethods. Our findings offer a new perspective on fine-tuning LLMs for\ntranslation tasks and provide a promising solution for generating high-quality\ntranslations. Please refer to Github for more details:\nhttps://github.com/lemon0830/TIM.",
        "pdf_link": "https://arxiv.org/pdf/2307.04408v3.pdf"
    },
    {
        "title": "Towards Cross-Table Masked Pretraining for Web Data Mining",
        "authors": [
            "Chao Ye",
            "Guoshan Lu",
            "Haobo Wang",
            "Liyao Li",
            "Sai Wu",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "published": "2023-07-10T02:27:38Z",
        "summary": "Tabular data pervades the landscape of the World Wide Web, playing a\nfoundational role in the digital architecture that underpins online\ninformation. Given the recent influence of large-scale pretrained models like\nChatGPT and SAM across various domains, exploring the application of\npretraining techniques for mining tabular data on the web has emerged as a\nhighly promising research direction. Indeed, there have been some recent works\naround this topic where most (if not all) of them are limited in the scope of a\nfixed-schema/single table. Due to the scale of the dataset and the parameter\nsize of the prior models, we believe that we have not reached the ''BERT\nmoment'' for the ubiquitous tabular data. The development on this line\nsignificantly lags behind the counterpart research domains such as natural\nlanguage processing. In this work, we first identify the crucial challenges\nbehind tabular data pretraining, particularly overcoming the cross-table\nhurdle. As a pioneering endeavor, this work mainly (i)-contributes a\nhigh-quality real-world tabular dataset, (ii)-proposes an innovative, generic,\nand efficient cross-table pretraining framework, dubbed as CM2, where the core\nto it comprises a semantic-aware tabular neural network that uniformly encodes\nheterogeneous tables without much restriction and (iii)-introduces a novel\npretraining objective -- prompt Masked Table Modeling (pMTM) -- inspired by NLP\nbut intricately tailored to scalable pretraining on tables. Our extensive\nexperiments demonstrate CM2's state-of-the-art performance and validate that\ncross-table pretraining can enhance various downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2307.04308v2.pdf"
    },
    {
        "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation",
        "authors": [
            "Neeraj Varshney",
            "Wenlin Yao",
            "Hongming Zhang",
            "Jianshu Chen",
            "Dong Yu"
        ],
        "published": "2023-07-08T14:25:57Z",
        "summary": "Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with GPT-3.5 (text-davinci-003) on the 'article\ngeneration task', we first demonstrate the individual efficacy of our detection\nand mitigation techniques. Specifically, the detection technique achieves a\nrecall of ~88% and the mitigation technique successfully mitigates 57.6% of the\ncorrectly detected hallucinations. Importantly, our mitigation technique does\nnot introduce new hallucinations even in the case of incorrectly detected\nhallucinations, i.e., false positives. Then, we show that the proposed active\ndetection and mitigation approach successfully reduces the hallucinations of\nthe GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the\neffectiveness and wide applicability of our approach through additional studies\nincluding performance on different types of questions (multi-hop and false\npremise questions) and with another LLM from a different model family (Vicuna).\nIn summary, our work contributes to improving the reliability and\ntrustworthiness of large language models, a crucial step en route to enabling\ntheir widespread adoption in real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2307.03987v2.pdf"
    },
    {
        "title": "Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task",
        "authors": [
            "Fanyi Qu",
            "Yunfang Wu"
        ],
        "published": "2023-07-08T13:10:59Z",
        "summary": "Large-scale language models (LLMs) has shown remarkable capability in various\nof Natural Language Processing (NLP) tasks and attracted lots of attention\nrecently. However, some studies indicated that large language models fail to\nachieve promising result beyond the state-of-the-art models in English\ngrammatical error correction (GEC) tasks. In this report, we aim to explore the\nhow large language models perform on Chinese grammatical error correction tasks\nand provide guidance for future work. We conduct experiments with 3 different\nLLMs of different model scale on 4 Chinese GEC dataset. Our experimental\nresults indicate that the performances of LLMs on automatic evaluation metrics\nfalls short of the previous sota models because of the problem of\nover-correction. Furthermore, we also discover notable variations in the\nperformance of LLMs when evaluated on different data distributions. Our\nfindings demonstrates that further investigation is required for the\napplication of LLMs on Chinese GEC task.",
        "pdf_link": "https://arxiv.org/pdf/2307.03972v1.pdf"
    },
    {
        "title": "Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions",
        "authors": [
            "Dawen Zhang",
            "Pamela Finckenberg-Broman",
            "Thong Hoang",
            "Shidong Pan",
            "Zhenchang Xing",
            "Mark Staples",
            "Xiwei Xu"
        ],
        "published": "2023-07-08T09:28:50Z",
        "summary": "The Right to be Forgotten (RTBF) was first established as the result of the\nruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\\'alez, and\nwas later included as the Right to Erasure under the General Data Protection\nRegulation (GDPR) of European Union to allow individuals the right to request\npersonal data be deleted by organizations. Specifically for search engines,\nindividuals can send requests to organizations to exclude their information\nfrom the query results. It was a significant emergent right as the result of\nthe evolution of technology. With the recent development of Large Language\nModels (LLMs) and their use in chatbots, LLM-enabled software systems have\nbecome popular. But they are not excluded from the RTBF. Compared with the\nindexing approach used by search engines, LLMs store, and process information\nin a completely different way. This poses new challenges for compliance with\nthe RTBF. In this paper, we explore these challenges and provide our insights\non how to implement technical solutions for the RTBF, including the use of\ndifferential privacy, machine unlearning, model editing, and prompt\nengineering. With the rapid advancement of AI and the increasing need of\nregulating this powerful technology, learning from the case of RTBF can provide\nvaluable lessons for technical practitioners, legal experts, organizations, and\nauthorities.",
        "pdf_link": "https://arxiv.org/pdf/2307.03941v3.pdf"
    },
    {
        "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
        "authors": [
            "Xiaomeng Hu",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "published": "2023-07-07T21:13:27Z",
        "summary": "Recent advances in large language models (LLMs) and the intensifying\npopularity of ChatGPT-like applications have blurred the boundary of\nhigh-quality text generation between humans and machines. However, in addition\nto the anticipated revolutionary changes to our technology and society, the\ndifficulty of distinguishing LLM-generated texts (AI-text) from human-generated\ntexts poses new challenges of misuse and fairness, such as fake content\ngeneration, plagiarism, and false accusations of innocent writers. While\nexisting works show that current AI-text detectors are not robust to LLM-based\nparaphrasing, this paper aims to bridge this gap by proposing a new framework\ncalled RADAR, which jointly trains a robust AI-text detector via adversarial\nlearning. RADAR is based on adversarial training of a paraphraser and a\ndetector. The paraphraser's goal is to generate realistic content to evade\nAI-text detection. RADAR uses the feedback from the detector to update the\nparaphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly\n2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,\nexperimental results show that RADAR significantly outperforms existing AI-text\ndetection methods, especially when paraphrasing is in place. We also identify\nthe strong transferability of RADAR from instruction-tuned LLMs to other LLMs,\nand evaluate the improved capability of RADAR via GPT-3.5-Turbo.",
        "pdf_link": "https://arxiv.org/pdf/2307.03838v2.pdf"
    },
    {
        "title": "Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media",
        "authors": [
            "Chuanbo Hu",
            "Bin Liu",
            "Xin Li",
            "Yanfang Ye"
        ],
        "published": "2023-07-07T16:15:59Z",
        "summary": "Social media platforms such as Instagram and Twitter have emerged as critical\nchannels for drug marketing and illegal sale. Detecting and labeling online\nillicit drug trafficking activities becomes important in addressing this issue.\nHowever, the effectiveness of conventional supervised learning methods in\ndetecting drug trafficking heavily relies on having access to substantial\namounts of labeled data, while data annotation is time-consuming and\nresource-intensive. Furthermore, these models often face challenges in\naccurately identifying trafficking activities when drug dealers use deceptive\nlanguage and euphemisms to avoid detection. To overcome this limitation, we\nconduct the first systematic study on leveraging large language models (LLMs),\nsuch as ChatGPT, to detect illicit drug trafficking activities on social media.\nWe propose an analytical framework to compose \\emph{knowledge-informed\nprompts}, which serve as the interface that humans can interact with and use\nLLMs to perform the detection task. Additionally, we design a Monte Carlo\ndropout based prompt optimization method to further to improve performance and\ninterpretability. Our experimental findings demonstrate that the proposed\nframework outperforms other baseline language models in terms of drug\ntrafficking detection accuracy, showing a remarkable improvement of nearly\n12\\%. By integrating prior knowledge and the proposed prompts, ChatGPT can\neffectively identify and label drug trafficking activities on social networks,\neven in the presence of deceptive language and euphemisms used by drug dealers\nto evade detection. The implications of our research extend to social networks,\nemphasizing the importance of incorporating prior knowledge and scenario-based\nprompts into analytical tools to improve online security and public safety.",
        "pdf_link": "https://arxiv.org/pdf/2307.03699v1.pdf"
    },
    {
        "title": "Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models",
        "authors": [
            "Yuxi Ma",
            "Chi Zhang",
            "Song-Chun Zhu"
        ],
        "published": "2023-07-07T13:58:16Z",
        "summary": "In this perspective paper, we first comprehensively review existing\nevaluations of Large Language Models (LLMs) using both standardized tests and\nability-oriented benchmarks. We pinpoint several problems with current\nevaluation methods that tend to overstate the capabilities of LLMs. We then\narticulate what artificial general intelligence should encompass beyond the\ncapabilities of LLMs. We propose four characteristics of generally intelligent\nagents: 1) they can perform unlimited tasks; 2) they can generate new tasks\nwithin a context; 3) they operate based on a value system that underpins task\ngeneration; and 4) they have a world model reflecting reality, which shapes\ntheir interaction with the world. Building on this viewpoint, we highlight the\nmissing pieces in artificial general intelligence, that is, the unity of\nknowing and acting. We argue that active engagement with objects in the real\nworld delivers more robust signals for forming conceptual representations.\nAdditionally, knowledge acquisition isn't solely reliant on passive input but\nrequires repeated trials and errors. We conclude by outlining promising future\nresearch directions in the field of artificial general intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2307.03762v1.pdf"
    },
    {
        "title": "TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction",
        "authors": [
            "Shuo Li",
            "Sangdon Park",
            "Insup Lee",
            "Osbert Bastani"
        ],
        "published": "2023-07-07T02:42:06Z",
        "summary": "When applied to open-domain question answering, large language models (LLMs)\nfrequently generate incorrect responses based on made-up facts, which are\ncalled $\\textit{hallucinations}$. Retrieval augmented generation (RAG) is a\npromising strategy to avoid hallucinations, but it does not provide guarantees\non its correctness. To address this challenge, we propose the Trustworthy\nRetrieval Augmented Question Answering, or $\\textit{TRAQ}$, which provides the\nfirst end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal\nprediction, a statistical technique for constructing prediction sets that are\nguaranteed to contain the semantically correct response with high probability.\nAdditionally, TRAQ leverages Bayesian optimization to minimize the size of the\nconstructed sets. In an extensive experimental evaluation, we demonstrate that\nTRAQ provides the desired correctness guarantee while reducing prediction set\nsize by 16.2% on average compared to an ablation. The implementation is\navailable at $\\href{https://github.com/shuoli90/TRAQ.git}{TRAQ}$.",
        "pdf_link": "https://arxiv.org/pdf/2307.04642v2.pdf"
    },
    {
        "title": "Focused Transformer: Contrastive Training for Context Scaling",
        "authors": [
            "Szymon Tworkowski",
            "Konrad Staniszewski",
            "Miko\u0142aj Pacek",
            "Yuhuai Wu",
            "Henryk Michalewski",
            "Piotr Mi\u0142o\u015b"
        ],
        "published": "2023-07-06T17:52:10Z",
        "summary": "Large language models have an exceptional capability to incorporate new\ninformation in a contextual manner. However, the full potential of such an\napproach is often restrained due to a limitation in the effective context\nlength. One solution to this issue is to endow an attention layer with access\nto an external memory, which comprises of (key, value) pairs. Yet, as the\nnumber of documents increases, the proportion of relevant keys to irrelevant\nones decreases, leading the model to focus more on the irrelevant keys. We\nidentify a significant challenge, dubbed the distraction issue, where keys\nlinked to different semantic values might overlap, making them hard to\ndistinguish. To tackle this problem, we introduce the Focused Transformer\n(FoT), a technique that employs a training process inspired by contrastive\nlearning. This novel approach enhances the structure of the (key, value) space,\nenabling an extension of the context length. Our method allows for fine-tuning\npre-existing, large-scale models to lengthen their effective context. This is\ndemonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The\nresulting models, which we name LongLLaMA, exhibit advancements in tasks\nrequiring a long context. We further illustrate that our LongLLaMA models\nadeptly manage a $256 k$ context length for passkey retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2307.03170v2.pdf"
    },
    {
        "title": "Style Over Substance: Evaluation Biases for Large Language Models",
        "authors": [
            "Minghao Wu",
            "Alham Fikri Aji"
        ],
        "published": "2023-07-06T14:42:01Z",
        "summary": "As large language models (LLMs) continue to advance, accurately and\ncomprehensively evaluating their performance becomes increasingly challenging.\nRanking the relative performance of LLMs based on Elo ratings, according to\nhuman judgment, is gaining more popularity. However, the extent to which humans\nand LLMs are capable evaluators remains uncertain. This study investigates the\nbehavior of crowd-sourced and expert annotators, as well as LLMs, when\ncomparing outputs from different models. To achieve this, we curate a dataset\nof intentionally flawed machine-generated answers. Our findings reveal a\nconcerning bias in the evaluation process, as answers with factual errors are\nrated more favorably than answers that are too short or contained grammatical\nerrors. To address this issue, we propose independently evaluating\nmachine-generated text across multiple dimensions, rather than merging all the\nevaluation aspects into a single score. We instantiate this idea with the Elo\nrating system, resulting in the Multi-Elo Rating System (MERS). Empirical\nresults from our study reveal that this proposed approach significantly\nenhances the quality of LLM-based evaluations, particularly in terms of factual\naccuracy. However, there is no significant improvement in crowd-sourced-based\nevaluations, indicating the need for further investigation.",
        "pdf_link": "https://arxiv.org/pdf/2307.03025v3.pdf"
    },
    {
        "title": "Amplifying Limitations, Harms and Risks of Large Language Models",
        "authors": [
            "Michael O'Neill",
            "Mark Connor"
        ],
        "published": "2023-07-06T11:53:45Z",
        "summary": "We present this article as a small gesture in an attempt to counter what\nappears to be exponentially growing hype around Artificial Intelligence (AI)\nand its capabilities, and the distraction provided by the associated talk of\nscience-fiction scenarios that might arise if AI should become sentient and\nsuper-intelligent. It may also help those outside of the field to become more\ninformed about some of the limitations of AI technology. In the current context\nof popular discourse AI defaults to mean foundation and large language models\n(LLMs) such as those used to create ChatGPT. This in itself is a\nmisrepresentation of the diversity, depth and volume of research, researchers,\nand technology that truly represents the field of AI. AI being a field of\nresearch that has existed in software artefacts since at least the 1950's. We\nset out to highlight a number of limitations of LLMs, and in so doing highlight\nthat harms have already arisen and will continue to arise due to these\nlimitations. Along the way we also highlight some of the associated risks for\nindividuals and organisations in using this technology.",
        "pdf_link": "https://arxiv.org/pdf/2307.04821v1.pdf"
    },
    {
        "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
        "authors": [
            "Ruosen Li",
            "Teerth Patel",
            "Xinya Du"
        ],
        "published": "2023-07-06T04:05:44Z",
        "summary": "Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) are hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs as a reference-free metric for\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose the (1) peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on preferences\nof two answers. We conduct experiments on two benchmark datasets. We find that\nour approaches achieve higher accuracy and align better with human judgments,\nrespectively. Interestingly, PR can induce a relatively accurate self-ranking\nof models under the anonymous setting, where each model's name is unrevealed.\nOur work provides space to explore evaluating models that are hard to compare\nfor humans.",
        "pdf_link": "https://arxiv.org/pdf/2307.02762v1.pdf"
    },
    {
        "title": "Scaling In-Context Demonstrations with Structured Attention",
        "authors": [
            "Tianle Cai",
            "Kaixuan Huang",
            "Jason D. Lee",
            "Mengdi Wang"
        ],
        "published": "2023-07-05T23:26:01Z",
        "summary": "The recent surge of large language models (LLMs) highlights their ability to\nperform in-context learning, i.e., \"learning\" to perform a task from a few\ndemonstrations in the context without any parameter updates. However, their\ncapabilities of in-context learning are limited by the model architecture: 1)\nthe use of demonstrations is constrained by a maximum sentence length due to\npositional embeddings; 2) the quadratic complexity of attention hinders users\nfrom using more demonstrations efficiently; 3) LLMs are shown to be sensitive\nto the order of the demonstrations. In this work, we tackle these challenges by\nproposing a better architectural design for in-context learning. We propose\nSAICL (Structured Attention for In-Context Learning), which replaces the\nfull-attention by a structured attention mechanism designed for in-context\nlearning, and removes unnecessary dependencies between individual\ndemonstrations, while making the model invariant to the permutation of\ndemonstrations. We evaluate SAICL in a meta-training framework and show that\nSAICL achieves comparable or better performance than full attention while\nobtaining up to 3.4x inference speed-up. SAICL also consistently outperforms a\nstrong Fusion-in-Decoder (FiD) baseline which processes each demonstration\nindependently. Finally, thanks to its linear nature, we demonstrate that SAICL\ncan easily scale to hundreds of demonstrations with continuous performance\ngains with scaling.",
        "pdf_link": "https://arxiv.org/pdf/2307.02690v1.pdf"
    },
    {
        "title": "Jailbroken: How Does LLM Safety Training Fail?",
        "authors": [
            "Alexander Wei",
            "Nika Haghtalab",
            "Jacob Steinhardt"
        ],
        "published": "2023-07-05T17:58:10Z",
        "summary": "Large language models trained for safety and harmlessness remain susceptible\nto adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on\nearly releases of ChatGPT that elicit undesired behavior. Going beyond\nrecognition of the issue, we investigate why such attacks succeed and how they\ncan be created. We hypothesize two failure modes of safety training: competing\nobjectives and mismatched generalization. Competing objectives arise when a\nmodel's capabilities and safety goals conflict, while mismatched generalization\noccurs when safety training fails to generalize to a domain for which\ncapabilities exist. We use these failure modes to guide jailbreak design and\nthen evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's\nClaude v1.3, against both existing and newly designed attacks. We find that\nvulnerabilities persist despite the extensive red-teaming and safety-training\nefforts behind these models. Notably, new attacks utilizing our failure modes\nsucceed on every prompt in a collection of unsafe requests from the models'\nred-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our\nanalysis emphasizes the need for safety-capability parity -- that safety\nmechanisms should be as sophisticated as the underlying model -- and argues\nagainst the idea that scaling alone can resolve these safety failure modes.",
        "pdf_link": "https://arxiv.org/pdf/2307.02483v1.pdf"
    },
    {
        "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
        "authors": [
            "Zhaofeng Wu",
            "Linlu Qiu",
            "Alexis Ross",
            "Ekin Aky\u00fcrek",
            "Boyuan Chen",
            "Bailin Wang",
            "Najoung Kim",
            "Jacob Andreas",
            "Yoon Kim"
        ],
        "published": "2023-07-05T17:50:42Z",
        "summary": "The impressive performance of recent language models across a wide range of\ntasks suggests that they possess a degree of abstract reasoning skills. Are\nthese skills general and transferable, or specialized to specific tasks seen\nduring pretraining? To disentangle these effects, we propose an evaluation\nframework based on \"counterfactual\" task variants that deviate from the default\nassumptions underlying standard tasks. Across a suite of 11 tasks, we observe\nnontrivial performance on the counterfactual variants, but nevertheless find\nthat performance substantially and consistently degrades compared to the\ndefault conditions. This suggests that while current LMs may possess abstract\ntask-solving skills to an extent, they often also rely on narrow,\nnon-transferable procedures for task-solving. These results motivate a more\ncareful interpretation of language model performance that teases apart these\naspects of behavior.",
        "pdf_link": "https://arxiv.org/pdf/2307.02477v3.pdf"
    },
    {
        "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
        "authors": [
            "Akide Liu"
        ],
        "published": "2023-07-05T17:05:32Z",
        "summary": "Memory is identified as a crucial human faculty that allows for the retention\nof visual and linguistic information within the hippocampus and neurons in the\nbrain, which can subsequently be retrieved to address real-world challenges\nthat arise through a lifetime of learning. The resolution of complex AI tasks\nthrough the application of acquired knowledge represents a stride toward the\nrealization of artificial general intelligence. However, despite the prevalence\nof Large Language Models (LLMs) like GPT-3.5 and GPT-4 \\cite{brown2020language,\nleiter2023chatgpt, zaitsu2023distinguishing, OpenAI2023GPT4TR} , which have\ndisplayed remarkable capabilities in language comprehension, generation,\ninteraction, and reasoning, they are inhibited by constraints on context length\nthat preclude the processing of extensive, continually evolving knowledge\nbases. This paper proposes that LLMs could be augmented through the selective\nintegration of knowledge from external repositories, and in doing so,\nintroduces a novel methodology for External Reasoning, exemplified by ChatPDF.\nCentral to this approach is the establishment of a tiered policy for\n\\textbf{External Reasoning based on Multiple LLM Interchange Assistance} in\n\\cref{fig:overall}, where the level of support rendered is modulated across\nentry, intermediate, and advanced tiers based on the complexity of the query,\nwith adjustments made in response to human feedback. A comprehensive evaluation\nof this methodology is conducted using multiple LLMs and the results indicate\nstate-of-the-art performance in \\cref{comparison} , surpassing existing\nsolutions including ChatPDF.com. Moreover, the paper emphasizes that this\napproach is more efficient compared to the direct processing of full text by\nLLMs. The source code is publicly available at:\n\\url{https://github.com/AkideLiu/ANLP}.",
        "pdf_link": "https://arxiv.org/pdf/2307.12057v2.pdf"
    },
    {
        "title": "Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues",
        "authors": [
            "Dollaya Hirunyasiri",
            "Danielle R. Thomas",
            "Jionghao Lin",
            "Kenneth R. Koedinger",
            "Vincent Aleven"
        ],
        "published": "2023-07-05T04:14:01Z",
        "summary": "Research suggests that providing specific and timely feedback to human tutors\nenhances their performance. However, it presents challenges due to the\ntime-consuming nature of assessing tutor performance by human evaluators. Large\nlanguage models, such as the AI-chatbot ChatGPT, hold potential for offering\nconstructive feedback to tutors in practical settings. Nevertheless, the\naccuracy of AI-generated feedback remains uncertain, with scant research\ninvestigating the ability of models like ChatGPT to deliver effective feedback.\nIn this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a\ntutor-student setting. We use two different prompting approaches, the zero-shot\nchain of thought and the few-shot chain of thought, to identify specific\ncomponents of effective praise based on five criteria. These approaches are\nthen compared to the results of human graders for accuracy. Our goal is to\nassess the extent to which GPT-4 can accurately identify each praise criterion.\nWe found that both zero-shot and few-shot chain of thought approaches yield\ncomparable results. GPT-4 performs moderately well in identifying instances\nwhen the tutor offers specific and immediate praise. However, GPT-4\nunderperforms in identifying the tutor's ability to deliver sincere praise,\nparticularly in the zero-shot prompting scenario where examples of sincere\ntutor praise statements were not provided. Future work will focus on enhancing\nprompt engineering, developing a more general tutoring rubric, and evaluating\nour method using real-life tutoring dialogues.",
        "pdf_link": "https://arxiv.org/pdf/2307.02018v1.pdf"
    },
    {
        "title": "Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations",
        "authors": [
            "Yuhan Ji",
            "Song Gao"
        ],
        "published": "2023-07-05T03:50:08Z",
        "summary": "This research focuses on assessing the ability of large language models\n(LLMs) in representing geometries and their spatial relations. We utilize LLMs\nincluding GPT-2 and BERT to encode the well-known text (WKT) format of\ngeometries and then feed their embeddings into classifiers and regressors to\nevaluate the effectiveness of the LLMs-generated embeddings for geometric\nattributes. The experiments demonstrate that while the LLMs-generated\nembeddings can preserve geometry types and capture some spatial relations (up\nto 73% accuracy), challenges remain in estimating numeric values and retrieving\nspatially related objects. This research highlights the need for improvement in\nterms of capturing the nuances and complexities of the underlying geospatial\ndata and integrating domain knowledge to support various GeoAI applications\nusing foundation models.",
        "pdf_link": "https://arxiv.org/pdf/2307.03678v1.pdf"
    },
    {
        "title": "Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification",
        "authors": [
            "Sha Li",
            "Ruining Zhao",
            "Manling Li",
            "Heng Ji",
            "Chris Callison-Burch",
            "Jiawei Han"
        ],
        "published": "2023-07-05T01:00:44Z",
        "summary": "Event schemas are a form of world knowledge about the typical progression of\nevents. Recent methods for event schema induction use information extraction\nsystems to construct a large number of event graph instances from documents,\nand then learn to generalize the schema from such instances. In contrast, we\npropose to treat event schemas as a form of commonsense knowledge that can be\nderived from large language models (LLMs). This new paradigm greatly simplifies\nthe schema induction process and allows us to handle both hierarchical\nrelations and temporal relations between events in a straightforward way. Since\nevent schemas have complex graph structures, we design an incremental prompting\nand verification method to break down the construction of a complex event graph\ninto three stages: event skeleton construction, event expansion, and\nevent-event relation verification. Compared to directly using LLMs to generate\na linearized graph, our method can generate large and complex schemas with 7.2%\nF1 improvement in temporal relations and 31.0% F1 improvement in hierarchical\nrelations. In addition, compared to the previous state-of-the-art closed-domain\nschema induction model, human assessors were able to cover $\\sim$10% more\nevents when translating the schemas into coherent stories and rated our schemas\n1.3 points higher (on a 5-point scale) in terms of readability.",
        "pdf_link": "https://arxiv.org/pdf/2307.01972v1.pdf"
    },
    {
        "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
        "authors": [
            "Siwon Kim",
            "Sangdoo Yun",
            "Hwaran Lee",
            "Martin Gubri",
            "Sungroh Yoon",
            "Seong Joon Oh"
        ],
        "published": "2023-07-04T18:53:47Z",
        "summary": "The rapid advancement and widespread use of large language models (LLMs) have\nraised significant concerns regarding the potential leakage of personally\nidentifiable information (PII). These models are often trained on vast\nquantities of web-collected data, which may inadvertently include sensitive\npersonal data. This paper presents ProPILE, a novel probing tool designed to\nempower data subjects, or the owners of the PII, with awareness of potential\nPII leakage in LLM-based services. ProPILE lets data subjects formulate prompts\nbased on their own PII to evaluate the level of privacy intrusion in LLMs. We\ndemonstrate its application on the OPT-1.3B model trained on the publicly\navailable Pile dataset. We show how hypothetical data subjects may assess the\nlikelihood of their PII being included in the Pile dataset being revealed.\nProPILE can also be leveraged by LLM service providers to effectively evaluate\ntheir own levels of PII leakage with more powerful prompts specifically tuned\nfor their in-house models. This tool represents a pioneering step towards\nempowering the data subjects for their awareness and control over their own\ndata on the web.",
        "pdf_link": "https://arxiv.org/pdf/2307.01881v1.pdf"
    },
    {
        "title": "Learning to Prompt in the Classroom to Understand AI Limits: A pilot study",
        "authors": [
            "Emily Theophilou",
            "Cansu Koyuturk",
            "Mona Yavari",
            "Sathya Bursic",
            "Gregor Donabauer",
            "Alessia Telari",
            "Alessia Testa",
            "Raffaele Boiano",
            "Davinia Hernandez-Leo",
            "Martin Ruskov",
            "Davide Taibi",
            "Alessandro Gabbiadini",
            "Dimitri Ognibene"
        ],
        "published": "2023-07-04T07:51:37Z",
        "summary": "Artificial intelligence's (AI) progress holds great promise in tackling\npressing societal concerns such as health and climate. Large Language Models\n(LLM) and the derived chatbots, like ChatGPT, have highly improved the natural\nlanguage processing capabilities of AI systems allowing them to process an\nunprecedented amount of unstructured data. However, the ensuing excitement has\nled to negative sentiments, even as AI methods demonstrate remarkable\ncontributions (e.g. in health and genetics). A key factor contributing to this\nsentiment is the misleading perception that LLMs can effortlessly provide\nsolutions across domains, ignoring their limitations such as hallucinations and\nreasoning constraints. Acknowledging AI fallibility is crucial to address the\nimpact of dogmatic overconfidence in possibly erroneous suggestions generated\nby LLMs. At the same time, it can reduce fear and other negative attitudes\ntoward AI. This necessitates comprehensive AI literacy interventions that\neducate the public about LLM constraints and effective usage techniques, i.e\nprompting strategies. With this aim, a pilot educational intervention was\nperformed in a high school with 21 students. It involved presenting high-level\nconcepts about intelligence, AI, and LLMs, followed by practical exercises\ninvolving ChatGPT in creating natural educational conversations and applying\nestablished prompting strategies. Encouraging preliminary results emerged,\nincluding high appreciation of the activity, improved interaction quality with\nthe LLM, reduced negative AI sentiments, and a better grasp of limitations,\nspecifically unreliability, limited understanding of commands leading to\nunsatisfactory responses, and limited presentation flexibility. Our aim is to\nexplore AI acceptance factors and refine this approach for more controlled\nfuture studies.",
        "pdf_link": "https://arxiv.org/pdf/2307.01540v2.pdf"
    },
    {
        "title": "CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care",
        "authors": [
            "Tong Xiang",
            "Liangzhi Li",
            "Wangyue Li",
            "Mingbai Bai",
            "Lu Wei",
            "Bowen Wang",
            "Noa Garcia"
        ],
        "published": "2023-07-04T03:34:19Z",
        "summary": "The recent advances in natural language processing (NLP), have led to a new\ntrend of applying large language models (LLMs) to real-world scenarios. While\nthe latest LLMs are astonishingly fluent when interacting with humans, they\nsuffer from the misinformation problem by unintentionally generating factually\nfalse statements. This can lead to harmful consequences, especially when\nproduced within sensitive contexts, such as healthcare. Yet few previous works\nhave focused on evaluating misinformation in the long-form (LF) generation of\nLLMs, especially for knowledge-intensive topics. Moreover, although LLMs have\nbeen shown to perform well in different languages, misinformation evaluation\nhas been mostly conducted in English. To this end, we present a benchmark,\nCARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic,\nspecifically the maternity and infant care domain; and 2) a language other than\nEnglish, namely Chinese. Most importantly, we provide an innovative paradigm\nfor building LF generation evaluation benchmarks that can be transferred to\nother knowledge-intensive domains and low-resourced languages. Our proposed\nbenchmark fills the gap between the extensive usage of LLMs and the lack of\ndatasets for assessing the misinformation generated by these models. It\ncontains 1,612 expert-checked questions, accompanied with human-selected\nreferences. Using our benchmark, we conduct extensive experiments and found\nthat current Chinese LLMs are far from perfect in the topic of maternity and\ninfant care. In an effort to minimize the reliance on human resources for\nperformance evaluation, we offer off-the-shelf judgment models for\nautomatically assessing the LF output of LLMs given benchmark questions.\nMoreover, we compare potential solutions for LF generation evaluation and\nprovide insights for building better automated metrics.",
        "pdf_link": "https://arxiv.org/pdf/2307.01458v4.pdf"
    },
    {
        "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
        "authors": [
            "Jinhao Duan",
            "Hao Cheng",
            "Shiqi Wang",
            "Alex Zavalny",
            "Chenan Wang",
            "Renjing Xu",
            "Bhavya Kailkhura",
            "Kaidi Xu"
        ],
        "published": "2023-07-03T22:17:16Z",
        "summary": "While Large Language Models (LLMs) have demonstrated remarkable potential in\nnatural language generation and instruction following, a persistent challenge\nlies in their susceptibility to \"hallucinations\", which erodes trust in their\noutputs. Although Uncertainty Quantification (UQ) presents a promising\nsolution, its accurate implementation within the context of LLMs remains a\nsignificant hurdle. To address this critical roadblock, our research originates\nfrom a fundamental heuristic insight: tokens within auto-regressive\nLLM-generated text do not equally reflect the underlying meaning. Some tokens\ncarry greater relevance and representativeness than others, owing to the\nphenomenon of \"linguistic redundancy\", wherein a select few keywords suffice to\nconvey the essence of lengthy sentences. Regrettably, existing methodologies\ntreat all tokens with equal importance when estimating uncertainty,\ndisregarding these inherent generative inequalities. Our analysis reveals a\nsignificant issue with state-of-the-art: numerous tokens (and sentences) of\nlimited semantic significance receive equal or even excessive weighting during\nuncertainty estimation. To rectify this bias, we propose to jointly Shifting\nAttention to more Relevant (SAR) components, at both the token- and the\nsentence-levels for accurate uncertainty estimation. We conduct extensive\nexperiments involving a range of popular \"off-the-shelf\" LLMs, including\ninstruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as\npretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B\nparameters. We carry out evaluation across various free-form question-answering\ntasks, encompassing domains such as reading comprehension, science Q&A, and\nmedical Q&A. Our experimental results demonstrate the superior performance of\nSAR in addressing the challenges of uncertainty estimation within the realm of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.01379v2.pdf"
    },
    {
        "title": "Multilingual Language Models are not Multicultural: A Case Study in Emotion",
        "authors": [
            "Shreya Havaldar",
            "Sunny Rai",
            "Bhumika Singhal",
            "Langchen Liu",
            "Sharath Chandra Guntuku",
            "Lyle Ungar"
        ],
        "published": "2023-07-03T21:54:28Z",
        "summary": "Emotions are experienced and expressed differently across the world. In order\nto use Large Language Models (LMs) for multilingual tasks that require\nemotional sensitivity, LMs must reflect this cultural variation in emotion. In\nthis study, we investigate whether the widely-used multilingual LMs in 2023\nreflect differences in emotional expressions across cultures and languages. We\nfind that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,\nand generative LMs (e.g., ChatGPT) reflect Western norms, even when responding\nto prompts in other languages. Our results show that multilingual LMs do not\nsuccessfully learn the culturally appropriate nuances of emotion and we\nhighlight possible research directions towards correcting this.",
        "pdf_link": "https://arxiv.org/pdf/2307.01370v2.pdf"
    },
    {
        "title": "Challenges in Domain-Specific Abstractive Summarization and How to Overcome them",
        "authors": [
            "Anum Afzal",
            "Juraj Vladika",
            "Daniel Braun",
            "Florian Matthes"
        ],
        "published": "2023-07-03T12:26:44Z",
        "summary": "Large Language Models work quite well with general-purpose data and many\ntasks in Natural Language Processing. However, they show several limitations\nwhen used for a task such as domain-specific abstractive text summarization.\nThis paper identifies three of those limitations as research problems in the\ncontext of abstractive text summarization: 1) Quadratic complexity of\ntransformer-based models with respect to the input text length; 2) Model\nHallucination, which is a model's ability to generate factually incorrect text;\nand 3) Domain Shift, which happens when the distribution of the model's\ntraining and test corpus is not the same. Along with a discussion of the open\nresearch questions, this paper also provides an assessment of existing\nstate-of-the-art techniques relevant to domain-specific text summarization to\naddress the research gaps.",
        "pdf_link": "https://arxiv.org/pdf/2307.00963v1.pdf"
    },
    {
        "title": "Evaluating Shutdown Avoidance of Language Models in Textual Scenarios",
        "authors": [
            "Teun van der Weij",
            "Simon Lermen",
            "Leon lang"
        ],
        "published": "2023-07-03T07:05:59Z",
        "summary": "Recently, there has been an increase in interest in evaluating large language\nmodels for emergent and dangerous capabilities. Importantly, agents could\nreason that in some scenarios their goal is better achieved if they are not\nturned off, which can lead to undesirable behaviors. In this paper, we\ninvestigate the potential of using toy textual scenarios to evaluate\ninstrumental reasoning and shutdown avoidance in language models such as GPT-4\nand Claude. Furthermore, we explore whether shutdown avoidance is merely a\nresult of simple pattern matching between the dataset and the prompt or if it\nis a consistent behaviour across different environments and variations.\n  We evaluated behaviours manually and also experimented with using language\nmodels for automatic evaluations, and these evaluations demonstrate that simple\npattern matching is likely not the sole contributing factor for shutdown\navoidance. This study provides insights into the behaviour of language models\nin shutdown avoidance scenarios and inspires further research on the use of\ntextual scenarios for evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2307.00787v1.pdf"
    }
]