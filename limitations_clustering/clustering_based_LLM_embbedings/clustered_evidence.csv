Source,Title,Talks about LLMs,Rate,Evidence,Year,Month,Cluster
aacl2022,VLStereoSet: A Study of Stereotypical Bias in Pre-trained Vision-Language Models,Yes.,4,experiments on six representative pretrained visionlanguage models demonstrate that stereotypical biases clearly exist in most of these models and across all four bias categories with gender bias slightly more evident,2022,November,7
aacl2022,Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique,Yes.,4,bertlike language models lms when exposed to large unstructured datasets are known to learn and sometimes even amplify the biases present in such data,2022,November,7
emnlp2022,RankGen: Improving Text Generation with Large Ranking Models,Yes.,5,modern language models often assign high probabilities to output sequences that are repetitive incoherent or irrelevant to the prefix as such modelgenerated text also contains such artifacts,2022,December,5
emnlp2022,An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models,Yes.,5,large language models are shown to present privacy risks through memorization of training data and we empirically study memorization of finetuning methods using membership inference and extraction attacks and show that their susceptibility to attacks is very different,2022,December,7
emnlp2022,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,Yes.,5,nominally large language models llm have been exposed to procedural knowledge about how objects interact yet our benchmarking shows they fail to reason about the world,2022,December,5
emnlp2022,Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence,Yes.,5,we simulate knowledge conflicts ie where parametric knowledge suggests one answer and different passages suggest different answers and examine model behaviors and contradictions among knowledge sources affect model confidence only marginally,2022,December,4
emnlp2022,SafeText: A Benchmark for Exploring Physical Safety in Language Models,Yes.,5,we find that stateoftheart large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice,2022,December,7
emnlp2022,Memory-assisted prompt editing to improve GPT-3 after deployment,Yes.,5,large lms such as gpt are powerful but can commit mistakes that are obvious to humans,2022,December,2
emnlp2022,BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation,Yes.,4,however it has been demonstrated that plms encode a range of stereotypical societal biases leading to a concern about the fairness of plms as metrics and we demonstrate that popular plmbased metrics exhibit significantly higher social bias than traditional metrics on  sensitive attributes,2022,December,7
emnlp2022,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,Yes.,5,we show that one of todayâs largest language models gpt brown et al  lacks this kind of social intelligence outofthe box and our results show that models struggle substantially at these theory of mind tasks and we,2022,December,6
emnlp2022,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,Yes.,5,neural language models at scale suffer from poor temporal generalization capability and language model pretrained on static data from past years performs worse over time on emerging data,2022,December,5
emnlp2022,Perturbation Augmentation for Fairer NLP,Yes.,4,unwanted and often harmful social biases are becoming ever more salient in nlp research affecting both models and datasets and lastly we discuss outstanding questions about how best to evaluate the unfairness of large language models,2022,December,7
emnlp2022,"The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",Yes.,5,our results show that all three investigated plms are able to recognise the structure of the cc but fail to use its meaning while humanlike performance of plms on many nlp tasks has been alleged this indicates that plms still suffer from substantial shortcomings in central domains of linguistic knowledge,2022,December,6
emnlp2022,LittleBird: Efficient Faster & Longer Transformer for Question Answering,Yes.,5,but it has a limitation dealing with long inputs due to its attention mechanism,2022,December,5
emnlp2022,Invariant Language Modeling,Yes.,5,yet they suffer from spurious correlations poor outofdomain generalization and biases,2022,December,4
emnlp2022,Mutual Information Alleviates Hallucinations in Abstractive Summarization,Yes.,5,these models still exhibit the tendency to hallucinate ie output content not supported by the source document,2022,December,3
emnlp2022,Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,Yes.,5,despite their strong performance on many tasks pretrained language models have been shown to struggle on outofdistribution compositional generalization and overall our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization while our analysis also suggests promising directions for future work,2022,December,6
emnlp2022,A Systematic Investigation of Commonsense Knowledge in Large Language Models,Yes.,5,our findings highlight the limitations of pretrained lms in acquiring commonsense knowledge without taskspecific supervision furthermore using larger models or fewshot evaluation is insufficient to achieve humanlevel commonsense performance,2022,December,6
emnlp2022,SEAL: Interactive Tool for Systematic Error Analysis and Labeling,Yes.,5,however many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation,2022,December,4
acl2022,Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts,Yes.,4,humanlike biases and undesired social stereotypes exist in large pretrained language models given the wide adoption of these models in realworld applications mitigating such biases has become an emerging and important task,2022,May,7
acl2022,Are Prompt-based Models Clueless?,Yes.,5,models with a taskspecific head require a lot of training data making them susceptible to learning and exploiting datasetspecific superficial cues that do not generalize to other datasets and analyzing fewshot promptbased models on mnli snli hans and copa has revealed that promptbased models also exploit superficial cues while the models perform well on instances with superficial cues,2022,May,4
acl2022,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Yes.,5,models generated many false answers that mimic popular misconceptions and have the potential to deceive humans and the largest models were generally the least truthful,2022,May,4
acl2022,Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models,Yes.,4,we investigate the bias transfer hypothesis,2022,May,6
acl2022,A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation,Yes.,5,large pretrained generative models like gpt often suffer from hallucinating nonexistent or incorrect content which undermines their potential merits in real applications,2022,May,3
acl2022,Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text,Yes.,4,errors in machine generations become ever subtler and harder to spot and the ten error categories of scarecrowâsuch as redundancy commonsense errors and incoherence,2022,May,2
acl2022,Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task,Yes.,5,existing promptbased techniques fail on the semantic distinction task of the wordincontext wic dataset specifically none of the existing fewshot approaches including the incontext learning of gpt can attain a performance that is meaningfully different from the random baseline,2022,May,4
acl2022,Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions,Yes.,5,however we discover that this single hidden state cannot produce all probability distributions regardless of the lm size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them,2022,May,0
acl2022,Coherence boosting: When your pretrained language model is not paying enough attention,Yes.,5,we demonstrate that large language models have insufficiently learned the effect of distant words on nexttoken prediction,2022,May,6
acl2022,Data Contamination: From Memorization to Exploitation,Yes.,5,it is not clear to what extent models exploit the contaminated data for downstream tasks and our results highlight the importance of analyzing massive webscale datasets to verify that progress in nlp is obtained by better language understanding and not better data exploitation,2022,May,7
acl2022,Kronecker Decomposition for GPT Compression,Yes.,5,despite the superior performance of gpt this overparameterized nature of gpt can be very prohibitive for deploying this model on devices with limited computational power or memory,2022,May,5
naacl2022,Provably Confidential Language Modelling,Yes.,5,large language models are shown to memorize privacy information such as social security numbers in training data,2022,July,7
naacl2022,"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",Yes.,5,we find that while the models are to a certain extent sensitive to the interactions we investigate they are all challenged by the presence of multiple nps and their behavior is not systematic which suggests that even models at the scale of gpt do not fully acquire basic entity tracking abilities,2022,July,4
naacl2022,Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models,Yes.,4,these results indicate that fairness or bias evaluation remains challenging for contextualized language models among other reasons because these choices remain subjective,2022,July,7
naacl2022,Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Yes.,5,current evaluation methods show some significant shortcomings they fail to effectively map out the aspects of language understanding that remain challenging to existing models and our experiments provide insight into the limitation of existing benchmark datasets and stateoftheart models,2022,July,6
naacl2022,Exposing the Limits of Video-Text Models through Contrast Sets,Yes.,5,we see that model performance suffers across all methods erasing the gap between recent clipbased methods vs the earlier methods,2022,July,4
naacl2022,KALA: Knowledge-Augmented Language Model Adaptation,Yes.,5,simple finetuning of plms on the other hand might be suboptimal for domainspecific tasks because they cannot possibly cover knowledge from all domains while adaptive pretraining of plms can help them obtain domainspecific knowledge it requires a large training cost moreover adaptive pretraining can harm the plmâs performance on the downstream task by causing catastrophic forgetting of its general,2022,July,5
naacl2022,You Donâ€™t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakersâ€™ Private Personas,Yes.,4,privacy concerns have arisen recently,2022,July,1
naacl2022,Methods for Estimating and Improving Robustness of Language Models,Yes.,5,large language models llms suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem and weak ability to generalise outside of the training domain,2022,July,5
naacl2022,Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition,Yes.,5,we show that dialect variations that surface in the form of a different lexicon grammar and occasionally semantics can significantly degrade the performance of the lm under mismatched conditions,2022,July,6
acl2023,MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning,Yes.,4,despite advances in large pretrained neural language models they are prone to generating toxic language which brings security risks to their applications,2023,July,7
acl2023,Knowledge of cultural moral norms in large language models,Yes.,4,we find that pretrained english language models predict empirical moral norms across countries worse than the english moral norms reported previously however finetuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the english moral norms,2023,July,6
acl2023,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Yes.,5,the robustness of these models has also been called into question recent works have shown that models can rely on shallow patterns in the problem description when generating a solution and our analysis shows that robustness does not appear to continuously improve as a function of size,2023,July,4
acl2023,ALERT: Adapt Language Models to Reasoning Tasks,Yes.,5,it is unclear whether these models are applying reasoning skills they have learnt during pretraining or if they are simply memorizing their training corpus at finer granularity and we also find that when language models are finetuned they tend to overfit to the prompt template which hurts the robustness of models causing generalization,2023,July,4
acl2023,ThinkSum: Probabilistic reasoning over sets using large language models,Yes.,5,recent studies show that even the more advanced llms fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions,2023,July,2
acl2023,Do language models have coherent mental models of everyday things?,Yes.,5,we observe that stateoftheart pretrained language models lms like gpt and macaw have fragments of knowledge about these everyday things but do not have fully coherent parts mental models  accurate  conditional constraint violation,2023,July,6
acl2023,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,Yes.,5,despite the success of zeroshotcot it still suffers from three pitfalls,2023,July,1
acl2023,Dynamic and Efficient Inference for Text Generation via BERT Family,Yes.,5,they suffer from inefficient inference on computation and memory due to their largescale parameters and the universal autoregressive decoding paradigm,2023,July,5
acl2023,Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model,Yes.,4,existing bias mitigation methods require socialgroupspecific word pairs eg âœmanâ â âœwomanâ for each social attribute eg gender restricting the bias mitigation to only one specified social attribute further this constraint renders such methods impractical and costly for mitigating bias in,2023,July,7
acl2023,Explanation-based Finetuning Makes Models More Robust to Spurious Cues,Yes.,5,large language models llms are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task leading to poor generalization on outofdistribution data,2023,July,5
acl2023,"On Second Thought, Letâ€™s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",Yes.,5,we find that zeroshot cot reasoning in sensitive domains significantly increases a modelâs likelihood to produce harmful or undesirable output and our work suggests that zeroshot cot should be used with caution on socially important tasks especially when marginalized groups or sensitive topics are involved,2023,July,7
acl2023,MISGENDERED: Limits of Large Language Models in Understanding Pronouns,Yes.,5,when prompted outofthebox language models perform poorly at correctly predicting neopronouns averaging  accuracy and genderneutral pronouns averaging  accuracy this inability to generalize results from a lack of representation of nonbinary pronouns in training data and memorized associations,2023,July,4
acl2023,SCOTT: Self-Consistent Chain-of-Thought Distillation,Yes.,4,even more concerning there is little guarantee that the generated rationales are consistent with lmâs predictions or faithfully justify the decisions,2023,July,2
acl2023,Evaluating Open-Domain Question Answering in the Era of Large Language Models,Yes.,5,the automated models struggle in detecting hallucinations in llm answers and are thus unable to evaluate llms,2023,July,3
acl2023,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,Yes.,5,one of its most fatal disadvantages is the lack of factual correctness generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications,2023,July,1
acl2023,Language model acceptability judgements are not always robust to context,Yes.,5,we find that model judgements are generally robust when placed in randomly sampled linguistic contexts but are unstable when contexts match the test stimuli in syntactic structure and this sensitivity to highly specific syntactic features of the context can only be explained by the modelsâ implicit incontext learning abilities,2023,July,6
acl2023,RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,Yes.,5,our results indicate that both stateoftheart table qa models and large language models eg gpt with fewshot learning falter in these adversarial sets,2023,July,6
acl2023,Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency,Yes.,5,nonlinguistic skill injection typically comes at a cost for llms,2023,July,1
acl2023,Parallel Context Windows for Large Language Models,Yes.,5,when applied to processing long text large language models llms are limited by their context window,2023,July,5
acl2023,Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model,Yes.,5,however these models also suffer from the pathology of overconfidence in the outofdistribution examples potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions,2023,July,4
acl2023,SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration,Yes.,4,the potential social harms that large language models pose such as generating offensive content and reinforcing biases are steeply rising,2023,July,7
acl2023,Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales,Yes.,4,we observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies existing metrics like task performance of the lm generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility,2023,July,2
acl2023,RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs,Yes.,4,despite their unprecedented success even the largest language models make mistakes and this approach does not apply to blackbox or limited access models such as chatgpt as they cannot be finetuned moreover in the era of large generalpurpose language agents finetuning,2023,July,5
acl2023,Targeted Data Generation: Finding and Fixing Model Weaknesses,Yes.,4,stateoftheart nlp models often fail systematically on specific subgroups of data resulting in unfair outcomes and eroding user trust,2023,July,7
acl2023,On â€œScientific Debtâ€ in NLP: A Case for More Rigour in Language Model Pre-Training Research,Yes.,5,current plm research practices often conflate different possible sources of model improvement without conducting proper ablation studies and principled comparisons between different models under comparable conditions these practices i leave us illequipped to understand which pretraining approaches should be used under what circumstances ii impede reproducibility and credit assignment and iii render it difficult to understand,2023,July,4
acl2023,WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,Yes.,5,we apply our benchmark to several popular llms and find that offtheshelf models generally do exhibit considerable antiqueer bias,2023,July,1
acl2023,When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,Yes.,5,despite their impressive performance on diverse tasks large language models lms still struggle with tasks requiring rich world knowledge implying the difficulty of encoding a wealth of world knowledge in their parameters and we find that lms struggle with less popular factual knowledge and that retrieval augmentation helps significantly in these cases and scaling on the other hand mainly improves memorization of,2023,July,5
acl2023,Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,Yes.,5,our experiments reveal that llms frequently fail to generate valid sentences grounded in negative commonsense knowledge and statistical shortcuts and negation reporting bias from language modeling pretraining cause this conflict,2023,July,6
acl2023,How Do In-Context Examples Affect Compositional Generalization?,Yes.,5,we find that the compositional generalization performance can be easily affected by the selection of incontext examples and two strong limitations are observed,2023,July,6
acl2023,Contrastive Decoding: Open-ended Text Generation as Optimization,Yes.,5,maximum probability is a poor decoding objective for openended generation because it produces short and repetitive text on the other hand sampling can often produce incoherent text that drifts from the original topics,2023,July,4
acl2023,CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models,Yes.,4,pretrained conversational agents have been exposed to safety issues exhibiting a range of stereotypical human biases such as gender bias and experimental results show that these chinese pretrained models are potentially risky for generating texts that contain social biases,2023,July,7
acl2023,Mitigating Label Biases for In-context Learning,Yes.,5,domainlabel bias restricts llms to randomlevel performance on many tasks regardless of the choice of incontext examples,2023,July,6
acl2023,"RARR: Researching and Revising What Language Models Say, Using Language Models",Yes.,5,however they sometimes generate unsupported or misleading content a user cannot easily determine whether their outputs are trustworthy or not because most lms do not have any builtin mechanism for attribution to external evidence,2023,July,1
acl2023,Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models,Yes.,5,test results show that the best models perform well on nonelliptical examples but struggle with all but the simplest ellipsis structures,2023,July,4
acl2023,Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,Yes.,4,societal biases present in pretrained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications rendering them unfair towards specific groups of people,2023,July,7
acl2023,Probing Physical Reasoning with Counter-Commonsense Context,Yes.,5,the results show that while large language models can use prepositions such as in and into in the provided context to infer size relationships they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense,2023,July,6
acl2023,"Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)",Yes.,5,we find that while gpt is able to summarize and simplify single biomedical articles faithfully it struggles to provide accurate aggregations of findings over multiple documents,2023,July,2
acl2023,Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning,Yes.,5,large language models llms are known to memorize significant portions of their training data parts of this memorized content have been shown to be extractable by simply querying the model which poses a privacy risk,2023,July,5
acl2023,A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification,Yes.,5,these benchmarks often do not adequately address the challenges posed in the realworld such as that of hierarchical classification  we observe llms are more prone to failure in these cases,2023,July,1
acl2023,Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section,Yes.,5,when the context length for a language model predictor is limited which part of clinical notes should we choose as the input,2023,July,5
acl2023,MathPrompter: Mathematical Reasoning using Large Language Models,Yes.,5,large language models llms have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers and to the best of our knowledge we are not aware of any llms that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption,2023,July,2
acl2023,KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications,Yes.,5,large language models llms not only learn natural text generation abilities but also social biases against different demographic groups from realworld data this poses a critical risk when deploying llmbased applications and this limitation requires localized social bias datasets to ensure the safe and effective deployment of llms,2023,July,7
acl2023,A Static Evaluation of Code Completion by Large Language Models,Yes.,4,our static analysis reveals that undefined name and unused variable are the most common errors among others made by language models,2023,July,6
acl2023,"Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",Yes.,4,responsible ai issues such as fairness bias and toxicity linguistic diversity and evaluation in the context of mmlms specifically focusing on issues in nonenglish and lowresource languages,2023,July,7
eacl2023,WinoDict: Probing language models for in-context word acquisition,Yes.,5,this benchmark addresses word acquisition one important aspect of the diachronic degradation known to afflict llms as llms are frozen in time at the moment they are trained they are normally unable to reflect the way language changes over time,2023,May,5
eacl2023,Nationality Bias in Text Generation,Yes.,5,this paper examines how a text generation model gpt accentuates preexisting societal biases about countrybased demonyms and gpt demonstrates significant bias against countries with lower internet users and adversarial triggering effectively reduces the same,2023,May,7
eacl2023,"â€œJohn is 50 years old, can his son be 65?â€ Evaluating NLP Modelsâ€™ Understanding of Feasibility",Yes.,5,some recent works have also found notable failures of these models often these failure examples involve complex reasoning abilities and we show that even stateoftheart models such as gpt gpt and t struggle to answer,2023,May,2
eacl2023,On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex,Yes.,5,the existing finetuned neural semantic parsers are vulnerable to adversarial attacks on naturallanguage inputs and the robustness of smaller semantic parsers can be enhanced through adversarial training this approach is not feasible for large language models in realworld scenarios and the large language model of code is vulnerable to carefully crafted adversarial examples,2023,May,7
eacl2023,MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers,Yes.,5,the usability of lms is constrained by computational and time complexity along with their increasing size an issue that has been referred to as overparameterisation,2023,May,1
eacl2023,SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,Yes.,4,a common limitation of diagnostic tests for detecting social biases in nlp models is that they may only detect stereotypic associations that are prespecified by the designer of the test and we also test sodapop on debiased models and show the limitations of,2023,May,7
eacl2023,Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding,Yes.,5,however very few of these studies have analyzed the impact of compression on the generalizability and robustness of compressed models for outofdistribution ood data and the compressed models are significantly less robust than their plm counterparts on ood test sets although they obtain similar performance on indistribution development sets for a task,2023,May,4
eacl2023,Opportunities and Challenges in Neural Dialog Tutoring,Yes.,5,we find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small they perform poorly in less constrained scenarios and our human quality evaluation shows that both models and groundtruth annotations exhibit low performance in terms of equitable tutoring which measures learning opportunities for,2023,May,6
eacl2023,Assessing Out-of-Domain Language Model Performance from Few Examples,Yes.,5,while pretrained language models have exhibited impressive generalization capabilities they still behave unpredictably under certain domain shifts and given a few targetdomain examples and a set of models with similar training performance can we understand how these models will perform on ood test data,2023,May,4
eacl2023,Towards preserving word order importance through Forced Invalidation,Yes.,5,however recent findings have revealed that pretrained language models are insensitive to word order the performance on nlu tasks remains unchanged even after randomly permuting the word of a sentence where crucial syntactic information is destroyed,2023,May,6
eacl2023,Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models,Yes.,4,however safely deploying them in real world applications is challenging because they generate toxic content,2023,May,1
eacl2023,When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization,Yes.,5,large language models llms are subject to sociocultural and other biases previously identified using intrinsic evaluations however when and how these intrinsic biases in pretrained lm representations propagate to downstream finetuned nlp tasks like summarization is not well understood and we show that these biases manifest themselves as hallucinations in summarization leading to factually incorrect summaries,2023,May,7
eacl2023,Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,Yes.,4,going beyond enumerating the risks of harms this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models,2023,May,7
emnlp2023,FISH: A Financial Interactive System for Signal Highlighting,Yes.,5,chatgptâs decision is sensitive to the order of labels in the prompt chatgpt has a clearly higher chance to select the labels at earlier positions as the answer,2023,December,4
emnlp2023,A Unified Framework for Emotion Identification and Generation in Dialogues,Yes.,5,experiments on our dataset show that recent large language models eg instructgpt struggle to answer the subquestions even if they are able to answer the main questions correctly we find that the models perform particularly poorly in answering subquestions written for the incorrect options,2023,December,6
emnlp2023,Addressing Domain Changes in Task-oriented Conversational Agents through Dialogue Adaptation,Yes.,5,our results reveal limitations in llmbased agentsâ planning optimization due to systematic failures in managing longhorizon contexts and hallucination about the task state,2023,December,2
emnlp2023,GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP,Yes.,5,our work adds to a growing body of research underscoring the limitations of chatgpt,2023,December,6
emnlp2023,CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models,Yes.,5,we find that llms perform poorly especially on words which are tokenized unfavorably by subword tokenization,2023,December,6
emnlp2023,Conceptual structure coheres in human cognition but not in large language models,Yes.,5,structures estimated from the llm behavior while individually fairly consistent with those estimated from human behavior depend much more upon the particular task used to generate behavior responsesâresponses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure,2023,December,2
emnlp2023,Towards LLM-driven Dialogue State Tracking,Yes.,5,despite its impressive performance chatgpt has significant limitations including its closedsource nature request restrictions raising data privacy concerns and lacking local deployment capabilities,2023,December,1
emnlp2023,Weâ€™re Afraid Language Models Arenâ€™t Modeling Ambiguity,Yes.,5,we find that the task remains extremely challenging including for gpt whose generated disambiguations are considered correct only  of the time in crowdworker evaluation compared to  for disambiguations in our dataset,2023,December,6
emnlp2023,Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus,Yes.,5,however llms are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many realworld applications,2023,December,3
emnlp2023,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,Yes.,5,however existing code llms have two main limitations first they often adopt a specific architecture encoderonly or decoderonly or rely on a unified encoderdecoder network for different downstream tasks lacking the flexibility to operate in the optimal architecture for a specific task secondly they often employ a limited set of pretraining objectives which might not be relevant to some tasks and,2023,December,5
emnlp2023,Unveiling the Implicit Toxicity in Large Language Models,Yes.,5,we show that llms can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zeroshot prompting and our findings suggest that llms pose a significant threat in generating undetectable implicit toxic outputs,2023,December,1
emnlp2023,ALCUNA: Large Language Models Meet New Knowledge,Yes.,5,we benchmark several llms reveals that their performance in face of new knowledge is not satisfactory particularly in reasoning between new and internal knowledge,2023,December,2
emnlp2023,Robust Prompt Optimization for Large Language Models Against Distribution Shifts,Yes.,5,we reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts which are common for llms in realworld scenarios such as customer reviews analysis,2023,December,1
emnlp2023,Interpreting Embedding Spaces by Conceptualization,Yes.,4,one major drawback of this type of representation is their incomprehensibility to humans and understanding the embedding space is crucial for several important needs including the need to debug the embedding method and compare it to alternatives and the need to detect biases hidden in the model,2023,December,0
emnlp2023,Knowledge-Augmented Language Model Verification,Yes.,5,yet lms often generate the factually incorrect responses to the given queries since their knowledge may be inaccurate incomplete and outdated and the model may fail to retrieve the knowledge relevant to the given query or the model may not faithfully reflect the retrieved knowledge in the generated text,2023,December,2
emnlp2023,Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation,Yes.,5,however due to their inability to capture relationships among samples these frozen llms inevitably keep repeating similar mistakes,2023,December,2
emnlp2023,â€œFifty Shades of Biasâ€: Normative Ratings of Gender Bias in GPT Generated English Text,Yes.,4,with llms increasingly gaining humanlike fluency in text generation gaining a nuanced understanding of the biases these systems can generate is imperative,2023,December,7
emnlp2023,MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations,Yes.,4,llms have a knowledge cutoff and are costly to finetune repeatedly and our findings also highlight the need for further improvements particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example,2023,December,2
emnlp2023,Instructed Language Models with Retrievers Are Powerful Entity Linkers,Yes.,5,the generative nature still makes the generated content suffer from hallucinations thus unsuitable for entitycentric tasks like entity linking el requiring precise entity predictions over a large knowledge base and the el task remains a persistent hurdle for general llms,2023,December,3
emnlp2023,Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?,Yes.,5,surprisingly throughout our experiments we find that although the knowledge seems to be successfully injected the correctness of injected knowledge only has a very limited effect on the modelsâ downstream performance this finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrainfinetune paradigm,2023,December,6
emnlp2023,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",Yes.,4,it is unclear how to collect and incorporate feedback in a way that is efficient effective and unbiased especially for highly subjective human preferences and values and we encourage a better future of feedback learning in llms by raising five unresolved conceptual and practical challenges,2023,December,1
emnlp2023,"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",Yes.,5,the issue of hallucination has parallelly emerged as a byproduct posing significant concerns and we propose two solution strategies for mitigating hallucinations,2023,December,3
emnlp2023,The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages,Yes.,5,our comprehensive analysis reveals that existing opensource instruction tuned llms still struggle to understand sm across various languages performing close to a random baseline in some cases we also find that although chatgpt outperforms many llms it still falls behind taskspecific finetuned models with a gap of  sparrow score,2023,December,6
emnlp2023,Understanding the Effect of Model Compression on Social Bias in Large Language Models,Yes.,4,large language models llms trained with selfsupervision on vast corpora of web text fit to the social biases of that text without intervention these social biases persist in the modelâs predictions in downstream tasks leading to representational harm,2023,December,7
emnlp2023,Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation,Yes.,5,hallucination of text ungrounded in the input is a wellknown problem in neural datatotext generation,2023,December,3
emnlp2023,API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs,Yes.,4,however three pivotal questions remain unanswered,2023,December,1
emnlp2023,Evaluating Large Language Models on Controlled Generation Tasks,Yes.,5,large language models struggle at meeting finegrained hard constraints,2023,December,5
emnlp2023,Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies,Yes.,5,we find that models can largely recover from syntacticstyle shifts but cannot recover from vocabulary misalignment and embedding matrix reinitialization even with continued pretraining on  million tokens,2023,December,6
emnlp2023,The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models,Yes.,5,a primary issue arising in this context is the management of unanswerable queries by llms which often results in hallucinatory behavior due to overconfidence,2023,December,3
emnlp2023,ROBBIE: Robust Bias Evaluation of Large Generative Language Models,Yes.,4,we must develop comprehensive enough tools to measure and improve their fairness and testing llms on more datasets can potentially help us characterize their biases more fully and we explore the frequency of demographic terms in common llm pretraining corpora and how this may relate to model biases,2023,December,7
emnlp2023,Adapting Language Models to Compress Contexts,Yes.,5,transformerbased language models lms are powerful and widelyapplicable tools but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents,2023,December,5
emnlp2023,Large Language Models are Temporal and Causal Reasoners for Video Question Answering,Yes.,5,however such priors often cause suboptimal results on videoqa by leading the model to overrely on questions ie linguistic bias while ignoring visual content this is also known as âungrounded guessesâ or âhallucinationsâ,2023,December,3
emnlp2023,TrojanSQL: SQL Injection against Natural Language Interface to Database,Yes.,4,experimental results demonstrate that both mediumsized models based on finetuning and llmbased parsers using prompting techniques are vulnerable to this type of attack with attack success rates as high as  and  respectively,2023,December,5
emnlp2023,Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models,Yes.,5,llms have shown the ability to memorize and reproduce portions of their training data when prompted by adversaries prior research has focused on addressing this memorization issue and preventing verbatim replication through techniques like knowledge unlearning and data preprocessing however these methods have limitations regarding the number of protected samples limited privacy types and potentially lowerquality generative models,2023,December,7
emnlp2023,Meta-Learning Online Adaptation of Language Models,Yes.,5,the knowledge in static language models falls out of date limiting the modelâs effective shelf life while online finetuning can reduce this degradation we find that naively finetuning on a stream of documents leads to a low level of information uptake,2023,December,5
emnlp2023,Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis,Yes.,5,we find that models are more sensitive to certain perturbations such as replacing words with their synonyms,2023,December,6
emnlp2023,Can Large Language Models Capture Dissenting Human Voices?,Yes.,5,we show llms exhibit limited ability in solving nli tasks and simultaneously fail to capture human disagreement distribution the inference and human alignment performances plunge even further on data samples with high human disagreement levels raising concerns about their natural language understanding nlu ability and their representativeness to a larger human population,2023,December,6
emnlp2023,Merging Generated and Retrieved Knowledge for Open-Domain QA,Yes.,4,retrieving passages from a given source is known to suffer from insufficient knowledge coverage and llms tend to hallucinate content that conflicts with the retrieved knowledge,2023,December,3
emnlp2023,Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition,Yes.,5,these deployments are increasingly plagued by prompt injection and jailbreaking collectively prompt hacking in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones,2023,December,4
emnlp2023,Prompting is not a substitute for probability measurements in large language models,Yes.,5,broadly we find that llmsâ metalinguistic judgments are inferior to quantities directly derived from representations furthermore consistency gets worse as the prompt query diverges from direct measurements of nextword probabilities,2023,December,6
emnlp2023,"The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",Yes.,5,large language models llms make natural interfaces to factual knowledge but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions,2023,December,5
emnlp2023,HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts,Yes.,5,however this strategy has two key limitations,2023,December,0
emnlp2023,"Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",Yes.,5,while large language models llms can answer many questions correctly they can also hallucinate and give wrong answers,2023,December,3
emnlp2023,Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators,Yes.,4,community concerns abound regarding the factuality and potential implications of using this uncensored knowledge and surprisingly our study reveals that the factuality of generated knowledge even if lower does not significantly hinder downstream tasks,2023,December,1
emnlp2023,Compressing Context to Enhance Inference Efficiency of Large Language Models,Yes.,5,however they face challenges in managing long documents and extended conversations due to significantly increased computational requirements both in memory and inference time and potential context truncation when the input exceeds the llmâs fixed context length,2023,December,5
emnlp2023,Can You Follow Me? Testing Situational Understanding for ChatGPT,Yes.,5,previous works have identified certain su limitations in nonchatbot large language models llms and find that despite the fundamental simplicity of the task the modelâs performance reflects an inability to retain correct environment states across time and performance degradation is largely because chatgpt has nonpersistent incontext memory although it can access the full dialogue history and it is susceptible to,2023,December,5
emnlp2023,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Yes.,5,large language models llms such as chatgpt are prone to generate hallucinations ie content that conflicts with the source or cannot be verified by the factual knowledge and moreover existing llms face great challenges in recognizing the hallucinations in texts,2023,December,3
emnlp2023,Enabling Large Language Models to Generate Text with Citations,Yes.,5,their generated outputs are prone to hallucination and current systems have considerable room for improvementâfor example on the eli dataset even the best models lack complete citation support  of the time,2023,December,3
emnlp2023,Counting the Bugs in ChatGPTâ€™s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,Yes.,5,we find that chatgpt massively underperforms purposebuilt systems particularly in english overall our resultsâthrough the lens of morphologyâcast a new light on the linguistic capabilities of chatgpt suggesting that claims of humanlike language skills are premature and misleading,2023,December,6
emnlp2023,MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models,Yes.,5,however when it comes to natural language reasoning lms still face challenges such as hallucination generating incorrect intermediate reasoning steps and making mathematical errors,2023,December,2
emnlp2023,Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models,Yes.,5,our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning offering a comprehensive reference for future research in this pivotal domain,2023,December,6
emnlp2023,Evaluation of African American Language Bias in Natural Language Generation,Yes.,4,we present evidence of dialectal bias for six pretrained llms through performance gaps on these tasks,2023,December,6
emnlp2023,An Investigation of LLMsâ€™ Inefficacy in Understanding Converse Relations,Yes.,5,the results suggest that llms often resort to shortcut learning and still face challenges on our proposed benchmark,2023,December,1
emnlp2023,HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies,Yes.,5,a myriad of different large language models llms face a common challenge in contextually analyzing table questionanswering tasks these challenges are engendered from  finite context windows for large tables  multifaceted discrepancies amongst tokenization patterns against cell boundaries and  various limitations stemming from data confidentiality in the process of using external models such as,2023,December,5
emnlp2023,"Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",Yes.,4,the ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data we show that models perform much better on memorized books than on nonmemorized books for downstream tasks,2023,December,4
emnlp2023,Copyright Violations and Large Language Models,Yes.,5,this work explores the issue of copyright violations and large language models through the lens of verbatim memorization focusing on possible redistribution of copyrighted text,2023,December,7
emnlp2023,Symbolic Planning and Code Generation for Grounded Dialogue,Yes.,5,llms have had limited applicability in grounded taskoriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding,2023,December,2
emnlp2023,Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models,Yes.,5,the typical failure modes of gpt the best model are errors in algebraic manipulation difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domainspecific concepts,2023,December,2
emnlp2023,Task-Agnostic Low-Rank Adapters for Unseen English Dialects,Yes.,4,large language models llms are trained on corpora disproportionally weighted in favor of standard american english as a result speakers of other dialects experience significantly more failures when interacting with these technologies,2023,December,5
emnlp2023,Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization,Yes.,4,the training process of large language models llms generally incurs the update of significant parameters which limits the applicability of fl techniques to tackle the llms in real scenarios and prompt tuning can significantly reduce the number of parameters to update but it either incurs performance degradation or low training efficiency and the decentralized data is generally nonindependent and identically distributed,2023,December,5
emnlp2023,Active Retrieval Augmented Generation,Yes.,5,despite the remarkable ability of large language models lms to comprehend and generate language they have a tendency to hallucinate and create factually inaccurate output,2023,December,3
emnlp2023,Reasoning with Language Model is Planning with World Model,Yes.,5,however llms can still struggle with problems that are easy for humans such as generating action plans for executing tasks or performing complex math or logical reasoning this is due to llmsâ absence of an internal world model for predicting world states eg environment status variable values and simulating,2023,December,2
emnlp2023,This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models,Yes.,5,they fail to interpret negation a crucial step in natural language processing and our findings show that while llms are proficient at classifying affirmative sentences they struggle with negative sentences and lack a deep understanding of negation often relying on superficial cues and the lack of generalization in handling negation is persistent highlighting the ongoing challenges of llms,2023,December,2
emnlp2023,SOUL: Towards Sentiment and Opinion Understanding of Language,Yes.,5,experimental results indicate that soul is a challenging task for both small and large language models with a performance gap of up to  when compared to human performance furthermore evaluations conducted with both human experts and gpt highlight the limitations of the small language model in generating reasoningbased justifications,2023,December,2
emnlp2023,Detecting and Mitigating Hallucinations in Multilingual Summarisation,Yes.,5,hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation and we assess a broad range of multilingual large language models and find that they all tend to hallucinate often in languages different from english,2023,December,3
emnlp2023,EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs,Yes.,5,however their expensive computations and high memory requirements are prohibitive for deployment and the quantized model was calibrated using few samples from the training data which might affect the generalization of the quantized llms to unknown cases and tasks,2023,December,5
emnlp2023,EpiK-Eval: Evaluation for Language Models as Epistemic Models,Yes.,5,evaluations across various llms reveal significant weaknesses in this domain and we contend that these shortcomings stem from the intrinsic nature of prevailing training objectives,2023,December,1
emnlp2023,Large Language Models are biased to overestimate profoundness,Yes.,5,however llms systematically overestimate the profoundness of nonsensical statements and this work provides insights into the potential biases induced by reinforcement learning from human feedback rlhf inducing an increase in the bias to overestimate the profoundness of statements,2023,December,2
emnlp2023,SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization,Yes.,5,most llms struggle on summedits with performance close to random chance the bestperforming model gpt is still  below estimated human performance highlighting the gaps in llmsâ ability to reason about facts and detect inconsistencies when they occur,2023,December,2
emnlp2023,Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models,Yes.,4,we show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results,2023,December,6
emnlp2023,Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,Yes.,5,however llms sometimes generate inaccurate and potentially misleading texts by hallucination or omission in healthcare this can make llms unusable at best and dangerous at worst and they also raised concerns regarding confidently composed but inaccurate llm outputs and other potential downstream harms including decreased accountability and proliferation of lowquality reviews,2023,December,3
emnlp2023,Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT,Yes.,5,we summarize and discuss the challenges faced by llms including clustering domainspecific understanding and crossdomain incontext learning scenarios,2023,December,1
emnlp2023,Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting,Yes.,5,a crucial challenge for generative large language models llms is diversity,2023,December,7
emnlp2023,Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection,Yes.,4,the utilization of these models carries inherent risks including but not limited to plagiarism the dissemination of fake news and issues in educational exercises and the existing detectors can be easily circumvented using straightforward automatic adversarial attacks,2023,December,7
emnlp2023,Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,Yes.,5,the effectiveness of the llmgenerated synthetic data in supporting model training is inconsistent across different classification tasks and subjectivity at both the task level and instance level is negatively associated with the performance of the model trained on synthetic data,2023,December,1
emnlp2023,Learning from Mistakes via Cooperative Study Assistant for Large Language Models,Yes.,5,however the feedback from llm itself is often inaccurate thereby limiting its benefits,2023,December,1
emnlp2023,Conceptor-Aided Debiasing of Large Language Models,Yes.,5,pretrained large language models llms reflect the inherent social biases of their training corpus and many methods have been proposed to mitigate this issue but they often fail to debias or they sacrifice model accuracy,2023,December,7
emnlp2023,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,Yes.,5,there is growing concern that these llm simulations are flattened caricatures of the personas that they aim to simulate failing to capture the multidimensionality of people and perpetuating stereotypes,2023,December,1
emnlp2023,Revisiting the Knowledge Injection Frameworks,Yes.,5,we find that injecting unaligned ie random knowledge tuple into the llms achieves comparable and sometimes better results than the aligned knowledge being injected and how to adapt these llms to better suit the vertical domainspecific tasks by utilizing external knowledge remains not completely solved,2023,December,6
emnlp2023,Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models,Yes.,4,an analysis of llamaâs errors reveals significant limitations in its ability to recall facts in languages other than english plus difficulties related to the location and gender of fact subjects,2023,December,2
emnlp2023,Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection,Yes.,4,the existing solutions to do so though effective are hardly feasible in practice as they involve dedicated model retraining on genderlabeled st data,2023,December,7
emnlp2023,StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding,Yes.,5,interestingly we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models llms such as chatgpt and llama,2023,December,6
emnlp2023,An Empirical Study of Translation Hypothesis Ensembling with Large Language Models,Yes.,4,large language models llms are becoming a onefitsmany solution but they sometimes hallucinate or produce unreliable output,2023,December,5
emnlp2023,FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation,Yes.,5,evaluating the factuality of longform text generated by large language models lms is nontrivial because  generations often contain a mixture of supported and unsupported pieces of information making binary judgments of quality inadequate and  human evaluation is timeconsuming,2023,December,5
emnlp2023,Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems,Yes.,5,language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation,2023,December,2
emnlp2023,StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models,Yes.,4,large language models llms have been observed to encode and perpetuate harmful associations present in the training data and this study contributes to the understanding of how llms perceive and represent social groups shedding light on their potential biases and the perpetuation of harmful associations,2023,December,7
emnlp2023,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,Yes.,5,our empirical evaluations show that gpt only manages to pass the indonesian primary school level with limited knowledge of local indonesian languages and culture,2023,December,6
emnlp2023,Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding,Yes.,5,critically assess three points recurring in critiques of llm capacities,2023,December,1
emnlp2023,Question Answering as Programming for Solving Time-Sensitive Questions,Yes.,5,our experiments reveal that the aforementioned problems still pose a significant challenge to existing llms this can be attributed to the llmsâ inability to perform rigorous reasoning based on surfacelevel text semantics,2023,December,2
emnlp2023,Context Compression for Auto-regressive Transformers with Sentinel Tokens,Yes.,5,the quadratic complexity of the attention module makes it gradually become the bulk of compute in transformerbased llms during generation moreover the excessive keyvalue cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency,2023,December,5
emnlp2023,MoPe: Model Perturbation based Privacy Attacks on Language Models,Yes.,5,recent work has shown that large language models llms can unintentionally leak sensitive information present in their training data,2023,December,7
emnlp2023,Empower Nested Boolean Logic via Self-Supervised Curriculum Learning,Yes.,5,we find that any pretrained language models even including large language models only behave like a random selector in the face of multinested boolean logic a task that humans can handle with ease,2023,December,6
emnlp2023,KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection,Yes.,5,large language models llms have demonstrated remarkable humanlevel natural language generation capabilities however their potential to generate misinformation often called the hallucination problem poses a significant risk to their deployment,2023,December,3
emnlp2023,Language Models with Rationality,Yes.,5,this lack of interpretability is a growing impediment to widespread use of llms and resolve inconsistencies that may exist,2023,December,1
emnlp2023,Mitigating Temporal Misalignment by Discarding Outdated Facts,Yes.,5,while large language models are able to retain vast amounts of world knowledge seen during pretraining such knowledge is prone to going out of date and is nontrivial to update,2023,December,5
emnlp2023,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,Yes.,5,we show that fantom is challenging for stateoftheart llms which perform significantly worse than humans even with chainofthought reasoning or finetuning,2023,December,2
emnlp2023,Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism,Yes.,5,we observed that dozens of modern llms were not robust against lexical negation eg plausibleâimplausible when performing cotstyle reasoning and the results highlight unique limitations in each llm family,2023,December,2
emnlp2023,Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness,Yes.,5,reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness,2023,December,4
emnlp2023,CRAB: Assessing the Strength of Causal Relationships Between Real-world Events,Yes.,5,find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains,2023,December,4
emnlp2023,Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation,Yes.,5,the propensity of llms to generate inaccurate or nonfactual content termed âœhallucinationsâ remains a significant challenge,2023,December,3
emnlp2023,MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,Yes.,5,current knowledgeediting approaches can recall edited facts accurately they fail catastrophically on the constructed multihop questions,2023,December,2
emnlp2023,Consistency Analysis of ChatGPT,Yes.,5,prompt designing fewshot learning and employing larger large language models llms are unlikely to be the ultimate solution to resolve the inconsistency issue of llms,2023,December,5
emnlp2023,Mitigating Societal Harms in Large Language Models,Yes.,5,we will provide an overview of potential social issues in language generation including toxicity social biases misinformation factual inconsistency and privacy violations,2023,December,7
emnlp2023,H2O Open Ecosystem for State-of-the-art Large Language Models,Yes.,5,llms represent a revolution in ai however they also pose many significant risks such as the presence of biased private copyrighted or harmful text,2023,December,7
emnlp2023,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,Yes.,5,llmsâ inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses,2023,December,3
emnlp2023,CLEVA: Chinese Language Models EVAluation Platform,Yes.,4,the absence of a comprehensive chinese benchmark that thoroughly assesses a modelâs performance the unstandardized and incomparable prompting procedure and the prevalent risk of contamination pose major challenges in the current evaluation of chinese llms,2023,December,1
emnlp2023,LM-Polygraph: Uncertainty Estimation for Language Models,Yes.,5,however a significant challenge arises as these models often âœhallucinateâ ie fabricate facts without providing users an apparent means to discern the veracity of their statements,2023,December,3
emnlp2023,CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering,Yes.,5,leveraging llms for domainspecific question answering suffers from severe limitations the generated answer tends to hallucinate due to the training data collection time when using offtheshelf complex user utterance and wrong retrieval in retrievalaugmented generation furthermore due to the,2023,December,3
emnlp2023,DELPHI: Data for Evaluating LLMsâ€™ Performance in Handling Controversial Issues,Yes.,4,this dataset presents challenges concerning knowledge recency safety fairness and bias,2023,December,1
naacl2024,Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study,Yes.,5,despite these advancements it remains an open question whether llms are fundamentally capable of reasoning and planning or if they primarily rely on recalling and synthesizing information from their training data and our experiments including trials with the advanced gpt model indicate that while llms possess the foundational abilities required for this task they struggle to integrate these into a coherent,2024,June,2
naacl2024,Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,Yes.,5,even the bestperforming model is unable to demonstrate strong visual reasoning capabilities and consistency indicating that substantial efforts are required to enable vlms to perform visual reasoning as systematically and consistently as humans,2024,June,2
naacl2024,Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?,Yes.,5,we show that existing llms are still far from being perfect in terms of their grasp of factual knowledge especially for facts of torsototail entities,2024,June,2
naacl2024,Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles,Yes.,5,our analyses suggest that despite the extraordinary capabilities of llms in singledocument summarization the proposed task remains a complex challenge for them mainly due to their limited coverage with gpt only able to cover under  of the diverse information on average,2024,June,2
naacl2024,An Examination of the Compositionality of Large Generative Vision-Language Models,Yes.,4,we identify the syntactical bias in current benchmarks which is exploited by the linguistic capability of gvlms the bias renders visualgptscore an insufficient metric for assessing gvlms,2024,June,6
naacl2024,Assessing Factual Reliability of Large Language Model Knowledge,Yes.,5,the factual knowledge of llms is typically evaluated using accuracy yet this metric does not capture the vulnerability of llms to hallucinationinducing factors like prompt and context variability,2024,June,1
naacl2024,A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,Yes.,5,despite significant advancements made by large language models llms they still struggle with complex logical reasoning problems and our main findings suggest that existing llms could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of selfverification methods,2024,June,2
naacl2024,On Large Language Modelsâ€™ Hallucination with Regard to Known Facts,Yes.,5,large language models are successful in answering factoid questions but are also prone to hallucination and we investigate the phenomenon of llms possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics,2024,June,3
naacl2024,"Language Models Hallucinate, but May Excel at Fact Verification",Yes.,5,llms frequently âœhallucinateâ resulting in nonfactual outputs and analyze the reliance of these llms on highquality evidence as well as their deficiencies in robustness and generalization ability,2024,June,3
naacl2024,Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models,Yes.,5,large language models llms exhibit impressive capabilities but also present risks such as biased content generation and privacy issues,2024,June,7
naacl2024,"E5: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain, Extract, Execute, Exhibit and Extrapolate",Yes.,4,their application to hierarchical tables is constrained by the reliance on manually curated exemplars and the modelâs token capacity limitations,2024,June,5
naacl2024,"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Model",Yes.,5,the rapid development of large language models llms has led to great strides in model capabilities like longcontext understanding and reasoning however as llms are able to process longer contexts it becomes more challenging to evaluate whether they have acquired certain capabilities since the length,2024,June,5
naacl2024,MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning,Yes.,4,extensive experiments on mmcbenchmark reveal the limitations of existing lmms on correctly interpreting charts even for the most recent gptv model,2024,June,2
naacl2024,Large Language Models Help Humans Verify Truthfulness â€“ Except When They Are Convincingly Wrong,Yes.,5,users reading llm explanations are significantly more efficient than those using search engines while achieving similar accuracy however they overrely on the llms when the explanation is wrong and natural language explanations by llms may not be a reliable replacement for reading the retrieved,2024,June,2
naacl2024,SELF-GUARD: Empower the LLM to Safeguard Itself,Yes.,4,safety training involves finetuning the llm with adversarial samples which activate the llmâs capabilities against jailbreak however it is not always effective in countering new attacks and often leads to potential performance degradation and safeguards on the other hand are methods using,2024,June,1
naacl2024,MART: Improving LLM Safety with Multi-round Automatic Red-Teaming,Yes.,4,redteaming is a common practice for mitigating unsafe behaviors in large language models llms which involves thoroughly assessing llms to identify potential flaws and addressing them with responsible and accurate responses while effective manual redteaming is costly and existing automatic redteaming typically discovers safety risks,2024,June,7
naacl2024,Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings,Yes.,5,our experiments reveal that,2024,June,6
naacl2024,The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth,Yes.,5,however they tend to be generic not empathetic enough and lack personalization resulting in nonreliable and potentially harmful advice,2024,June,1
naacl2024,ReTA: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large Language Models,Yes.,5,experimental results demonstrate that existing stateoftheart llms and reasoning schemes are largely ineffective for strategic reasoning tasks,2024,June,2
naacl2024,"First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models",Yes.,4,we argue that disparities in scale are transient and researchers can work to reduce them that data rather than hardware is still a bottleneck for many applications that meaningful realistic evaluation is still an open problem and that there is still room for speculative approaches,2024,June,1
naacl2024,Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models,Yes.,5,large language models llms exhibit positional bias in how they use context which especially affects listwise ranking,2024,June,5
naacl2024,How Well Do Large Language Models Truly Ground?,Yes.,5,to reduce issues like hallucinations and lack of control in large language models llms and we perform experiments across  llms of different sizes and training methods and provide insights into factors that influence grounding performance,2024,June,6
naacl2024,How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities,Yes.,5,however there is still a limited understanding of their trustworthiness and scrutinizing them across eight different aspects including toxicity stereotypes ethics hallucination fairness sycophancy privacy and robustness against adversarial demonstrations and our result analysis reveals that models with superior performance in general nlp tasks do not always have greater trustworthiness in fact larger,2024,June,7
naacl2024,Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models,Yes.,5,we find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate exposing the language modelâs lack of robustness and sensitivity to the surface form in reasoning through complex problems,2024,June,6
naacl2024,Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks,Yes.,4,methods for localization have never been systematically and directly evaluated and even successful methods identify neurons that are not specific to a single memorized sequence,2024,June,4
naacl2024,Fair Abstractive Summarization of Diverse Perspectives,Yes.,4,experiments show that both the modelgenerated and the humanwritten reference summaries suffer from low fairness,2024,June,7
naacl2024,Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications,Yes.,5,llms have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society and llms tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks and the social,2024,June,7
naacl2024,Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks,Yes.,5,the evaluation results demonstrate the limitations of current llms especially in ultralongcontext settings,2024,June,1
naacl2024,TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction,Yes.,5,large language models llms frequently generate incorrect responses based on madeup facts which are called hallucinations,2024,June,3
naacl2024,On-the-fly Definition Augmentation of LLMs for Biomedical NER,Yes.,5,despite their general capabilities llms still struggle on biomedical ner tasks which are difficult due to the presence of specialized terminology and lack of training data,2024,June,2
naacl2024,"This Land is Your, My Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes",Yes.,5,we show that llms recall certain geographical knowledge inconsistently when queried in different languagesâa phenomenon we term geopolitical bias and use the proposed metrics to discover numerous inconsistencies in how these models respond in different languages and highlights how brittle llms are and how they tailor their responses depending on cues from the interaction context,2024,June,6
naacl2024,Towards Improved Multi-Source Attribution for Long-Form Answer Generation,Yes.,5,current llms struggle with attribution for longform responses which require reasoning over multiple evidence sources,2024,June,2
naacl2024,Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey,Yes.,5,the contemporary llms are prone to producing hallucinations stemming mainly from the knowledge gaps within the models,2024,June,3
naacl2024,LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models,Yes.,5,todayâs large language models llms typically train on short text segments eg k tokens due to the quadratic complexity of their transformer architectures as a result their performance suffers drastically on inputs longer than those encountered during training substantially limiting their applications in realworld,2024,June,5
naacl2024,Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding,Yes.,5,large language models llms tend to inadequately integrate input context during text generation relying excessively on encoded prior knowledge in model parameters potentially resulting in generated text with factual inconsistencies or contextually unfaithful content,2024,June,5
naacl2024,Fixing Rogue Memorization in Many-to-One Multilingual Translators of Extremely-Low-Resource Languages by Rephrasing Training Samples,Yes.,5,however we also found that manytoone multilingual systems have a tendency to learn a rogue strategy of storing output strings from the training data in the llm structure and retrieving them instead of performing actual translations,2024,June,6
naacl2024,Flames: Benchmarking Value Alignment of LLMs in Chinese,Yes.,4,there is still a significant gap in llmsâ deeper alignment with human values and achieving genuine harmlessness and our findings indicate that all the evaluated llms demonstrate relatively poor performance on flames particularly in the safety and fairness dimensions,2024,June,1
naacl2024,Effective Long-Context Scaling of Foundation Models,Yes.,5,we delve into llamaâs position encodings and discuss its key limitation in modeling long data,2024,June,5
naacl2024,Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback,Yes.,5,large language models llms often generate biased outputs containing offensive toxic or stereotypical text,2024,June,7
naacl2024,Fake Alignment: Are LLMs Really Aligned Well?,Yes.,5,this study investigates an underexplored issue about the evaluation of llms namely the substantial discrepancy in performance between multiplechoice questions and openended questions,2024,June,1
naacl2024,"In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax",Yes.,5,do models guided via icl infer the underlying structure of the task defined by the context or do they rely on superficial heuristics that only generalize to identically distributed examples and we find large variance across lms the variance is explained more by the composition of the pre,2024,June,4
naacl2024,Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections,Yes.,4,while being effective such finetuningbased unalignment approaches also have their own limitations,2024,June,1
naacl2024,You donâ€™t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments,Yes.,5,our experiments on  different llms reveal that even simple perturbations significantly downgrade a modelâs questionanswering ability and that most llms have low negation consistency,2024,June,6
naacl2024,MacGyver: Are Large Language Models Creative Problem Solvers?,Yes.,5,in contrast llms exposed to a variety of specialized knowledge attempt broader problems but fail by proposing physicallyinfeasible actions and finally we provide a detailed error analysis of llms and demonstrate the potential of enhancing their problemsolving ability with novel prompting techniques such as iterative step,2024,June,2
naacl2024,Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting,Yes.,5,we uncovered a universal vulnerability among llms in processing inductive instructions,2024,June,1
naacl2024,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models,Yes.,5,without proper safeguards large language models will readily follow malicious instructions and generate toxic content and we use the test suite to highlight systematic failure modes in stateoftheart language models as well as more general challenges in building safer language models,2024,June,7
naacl2024,Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense,Yes.,5,llms have a significant discrepancy in performance when tested on culturespecific commonsense knowledge for different cultures and llmsâ general commonsense capability is affected by cultural context and the language used to query the llms can impact their performance on culturalrelated tasks,2024,June,1
naacl2024,Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers,Yes.,4,however their fairness remains largely unexplored and our analysis delves into how these llms handle queries and documents related to these attributes aiming to uncover biases in their ranking algorithms,2024,June,1
naacl2024,TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition,Yes.,5,large language models llms have shown impressive capabilities in natural language understanding and generation but they often struggle with large tables due to their limited input length,2024,June,5
naacl2024,RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models,Yes.,5,chainofthought cot has impressively unlocked the reasoning potential of large language models llms yet it falls short when tackling problems that require multiple reasoning steps this limitation arises from the complex nature of multistep reasoning processes,2024,June,2
naacl2024,Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models,Yes.,5,our evaluation shows that on average  of the summaries generated by llms contain factual inconsistency even chatgpt the strongest model evaluated has such errors in  of its summaries for answering the factual questions which is more challenging the,2024,June,2
naacl2024,"Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly",Yes.,5,current large language models show imbalance abilities in different languages and the overall crosslingual knowledge alignment especially in the conductivity level is unsatisfactory for all tested llms and neither multilingual pretraining nor instruction tuning can substantially improve the crosslingual knowledge conductivity,2024,June,5
naacl2024,Effective Large Language Model Adaptation for Improved Grounding and Citation Generation,Yes.,4,however one major issue towards their widespread deployment in the real world is that they can generate hallucinated answers that are not factual,2024,June,3
naacl2024,Grounding Gaps in Language Model Generations,Yes.,5,we find thatâcompared to humansâllms generate language with less conversational grounding instead generating text that appears to simply presume common ground,2024,June,6
naacl2024,Global Gallery: The Fine Art of Painting Culture Portraits through Multilingual Instruction Tuning,Yes.,4,they also uncover inconsistencies and biases particularly in nonwestern cultures,2024,June,1
naacl2024,ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models,Yes.,5,while gpt performs the best and can outperform humans on this task we find that it is still unreliable and struggles with selfcontradictions that require more nuance and context,2024,June,2
naacl2024,A Survey of Confidence Estimation and Calibration in Large Language Models,Yes.,4,despite their impressive performance they can be unreliable due to factual errors in their generations and we outline the challenges and we summarize recent technical advancements for llm confidence estimation and calibration,2024,June,1
naacl2024,"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",Yes.,4,however their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on llm conversation safety,2024,June,1
naacl2024,Mindâ€™s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models,Yes.,4,the massive scale and computational demands of these models present formidable challenges when considering their practical deployment in resourceconstrained environments and there is a risk that distilled slms may still inherit flawed reasoning and hallucinations from llms,2024,June,3
naacl2024,Beyond Performance: Quantifying and Mitigating Label Bias in LLMs,Yes.,4,however recent work revealed they also exhibit label biasâan undesirable preference toward predicting certain answers over others and our investigation reveals substantial label bias in models both before and after debiasing attempts and our results emphasize that label bias in the predictions of llms remains a barrier to their reliability,2024,June,7
naacl2024,Instructing Large Language Models to Identify and Ignore Irrelevant Conditions,Yes.,5,however they were seriously confused by the irrelevant conditions resulting in low accuracy,2024,June,4
naacl2024,Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method,Yes.,5,recent literature reveals that llms hallucinate intermittently which impedes their reliability for further utilization,2024,June,3
naacl2024,Are Large Language Model Temporally Grounded?,Yes.,5,generally we find that llms lag significantly behind both human performance as well as smallscale specialised lms and crucially llms struggle the most with selfconsistency displaying incoherent behaviour in at least  of their predictions and moreover public instruction,2024,June,2
naacl2024,R-Tuning: Instructing Large Language Models to Say â€˜I Donâ€™t Knowâ€™,Yes.,5,a predominant issue is the propensity for these models to generate nonexistent facts a concern termed hallucination and when the question is out of the parametric knowledge it will try to make up something and fail to indicate when it lacks knowledge,2024,June,3
naacl2024,LeanReasoner: Boosting Complex Logical Reasoning with Lean,Yes.,5,large language models llms often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning,2024,June,2
naacl2024,UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback,Yes.,4,many large language models llms struggle to consistently generate ui code that compiles and produces visually relevant designs,2024,June,5
naacl2024,Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?,Yes.,5,despite the high performances of large language models llms across numerous benchmarks recent research has unveiled their suffering from hallucinations and unfaithful reasoning and experiments show that existing llms cannot follow correct reasoning paths and resist the attempt of greedy shortcuts with gpt only achieving  accuracy,2024,June,2
naacl2024,Learning to Compress Prompt in Natural Language Formats,Yes.,5,large language models llms are great at processing multiple natural language processing tasks but their abilities are constrained by inferior performance with long context slow inference speed and the high cost of computing the results,2024,June,5
naacl2024,Naive Bayes-based Context Extension for Large Language Models,Yes.,5,conventional incontext learning icl approaches are often impeded by length limitations of transformer architecture which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples,2024,June,5
naacl2024,Branch-Solve-Merge Improves Large Language Model Evaluation and Generation,Yes.,5,however their performance can fall short due to the modelâs lack of coherence and inability to plan and decompose the problem,2024,June,4
naacl2024,Evaluating the Deductive Competence of Large Language Models,Yes.,5,the tested llms have limited abilities to solve these problems in their conventional form and overall our results suggest that llms have unique reasoning biases that are only partially predicted from human reasoning performance and the humangenerated language corpora that informs them,2024,June,2
naacl2024,Large Human Language Models: A Need and the Challenges,Yes.,4,at the same time our nlp systems have become heavily reliant on llms most of which do not model authors and this brings to the fore a range of design considerations and challenges in terms of what human aspects to capture how to represent them and what modeling strategies to pursue,2024,June,7
naacl2024,Hallucination Diversity-Aware Active Learning for Text Summarization,Yes.,5,large language models llms have shown propensity to generate hallucinated outputs ie texts that are factually incorrect or unsupported and existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in llm outputs,2024,June,3
naacl2024,Investigating Data Contamination in Modern Benchmarks for Large Language Models,Yes.,5,recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of llms raising concerns about potential contamination of evaluation benchmarks,2024,June,1
naacl2024,IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context,Yes.,4,the pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in large language models llms and we observed that the language models exhibit more bias across a majority of the intersectional groups,2024,June,7
naacl2024,Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias,Yes.,5,position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others leading to undesirable behavior,2024,June,7
naacl2024,Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?,Yes.,4,producing complex structured tabular data remains challenging and indepth error analysis and creating an ability map across six dimensions coverage formatting reasoning comprehension pragmatics and hallucination highlight areas for future enhancements and suggest forthcoming research trajectories,2024,June,2
naacl2024,Advancing the Robustness of Large Language Models through Self-Denoised Smoothing,Yes.,5,although large language models llms have achieved significant success their vulnerability to adversarial perturbations including recent jailbreak attacks has raised considerable concerns,2024,June,7
naacl2024,Lost in Space: Probing Fine-grained Spatial Understanding in Vision and Language Resamplers,Yes.,5,more finegrained tasks that require spatial understanding have not been thoroughly examined and our results show that this information is largely absent from the resampler output when kept frozen during training of the classifiers,2024,June,6
naacl2024,The Impact of Language on Arithmetic Proficiency: A Multilingual Investigation with Cross-Agent Checking Computation,Yes.,5,this paper critically examines the arithmetic capabilities of large language models llms uncovering significant limitations in their performance,2024,June,5
naacl2024,Removing RLHF Protections in GPT-4 via Fine-Tuning,Yes.,4,finetuning allows attackers to remove rlhf protections with as few as  examples and a  success rate and our results show the need for further research on protections on llms,2024,June,1
naacl2024,MuLan: A Study of Fact Mutability in Language Models,Yes.,4,we hypothesize that mutable facts are encoded differently than immutable ones hence being easier to update in a detailed evaluation of six popular large language models we consistently find differences in the llmsâ confidence representations and update behavior depending on the mutability of a fact,2024,June,6
naacl2024,DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models,Yes.,4,we also identify significant challenges of llms in adherence to taskspecific instructions and generating outputs in multiple languages highlighting areas for future research,2024,June,1
naacl2024,OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs,Yes.,4,an open research question concerns the inherent biases of trained models and their responses and current research work seeks to debias such models or suppress potentially biased answers,2024,June,7
naacl2024,Exploring Inherent Biases in LLMs within Korean Social Context: A Comparative Analysis of ChatGPT and GPT-4,Yes.,4,llms have been critiqued for perpetuating stereotypes against diverse groups based on race sexual orientation and other attributes and our findings indicate that certain personas or prompt combinations consistently yield harmful content highlighting the potential risks associated with specific personaissue alignments within the korean cultural framework,2024,June,1
naacl2024,HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms,Yes.,5,the pretraining phase is extremely computeintensive and requires several highperformance computing devices like gpus and several days or even months of training but it is crucial for the model to capture global knowledge and also has a significant impact on the finetuning task this is a major,2024,June,5
naacl2024,Combating Security and Privacy Issues in the Era of Large Language Models,Yes.,5,this tutorial seeks to provide a systematic summary of risks and vulnerabilities in security privacy and copyright aspects of large language models llms and most recent solutions to address those issues and will conclude the discussions by outlining emergent challenges in security privacy and reliability of llms that deserve timely investigation by the community,2024,June,7
naacl2024,Explanation in the Era of Large Language Models,Yes.,4,the sheer sizes and the opaque nature of llms introduce challenges to the explanation methods,2024,June,2
