Source ,Title,Talks about LLMs,Rate,Evidence
aacl2022,VLStereoSet: A Study of Stereotypical Bias in Pre-trained Vision-Language Models,Yes.,4,"""Experiments on six representative pre-trained vision-language models demonstrate that stereotypical biases clearly exist in most of these models and across all four bias categories, with gender bias slightly more evident."""
aacl2022,Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique,Yes.,4,"""BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data."""
emnlp2022,RankGen: Improving Text Generation with Large Ranking Models,Yes.,5,"""modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts."""
emnlp2022,An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models,Yes.,5,"""Large language models are shown to present privacy risks through memorization of training data,"" and ""we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different."""
emnlp2022,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,Yes.,5,"""Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world."""
emnlp2022,Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence,Yes.,5,"""we simulate knowledge conflicts (i.e., where parametric knowledge suggests one answer and different passages suggest different answers) and examine model behaviors"" and ""contradictions among knowledge sources affect model confidence only marginally."""
emnlp2022,SafeText: A Benchmark for Exploring Physical Safety in Language Models,Yes.,5,"""We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice."""
emnlp2022,Memory-assisted prompt editing to improve GPT-3 after deployment,Yes.,5,"""Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans."""
emnlp2022,BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation,Yes.,4,"""However, it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern about the fairness of PLMs as metrics."" and ""We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes,"
emnlp2022,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,Yes.,5,"""We show that one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box,"" and ""Our results show that models struggle substantially at these Theory of Mind tasks,"" and ""we"
emnlp2022,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,Yes.,5,"""neural language models at scale suffer from poor temporal generalization capability"" and ""language model pre-trained on static data from past years performs worse over time on emerging data."""
emnlp2022,Perturbation Augmentation for Fairer NLP,Yes.,4,"""Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets,"" and ""Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models."""
emnlp2022,"The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",Yes.,5,"""Our results show that all three investigated PLMs are able to recognise the structure of the CC but fail to use its meaning. While human-like performance of PLMs on many NLP tasks has been alleged, this indicates that PLMs still suffer from substantial shortcomings in central domains of linguistic knowledge."""
emnlp2022,LittleBird: Efficient Faster & Longer Transformer for Question Answering,Yes.,5,"""But it has a limitation dealing with long inputs due to its attention mechanism."""
emnlp2022,Invariant Language Modeling,Yes.,5,"""Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases."""
emnlp2022,Mutual Information Alleviates Hallucinations in Abstractive Summarization,Yes.,5,"""these models still exhibit the tendency to hallucinate, i.e., output content not supported by the source document."""
emnlp2022,Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,Yes.,5,"""Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization."" and ""Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work."""
emnlp2022,A Systematic Investigation of Commonsense Knowledge in Large Language Models,Yes.,5,"""Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation is insufficient to achieve human-level commonsense performance."""
emnlp2022,SEAL: Interactive Tool for Systematic Error Analysis and Labeling,Yes.,5,"""However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation."""
acl2022,Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts,Yes.,4,"""Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task."""
acl2022,Are Prompt-based Models Clueless?,Yes.,5,"""models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets"" and ""Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues"
acl2022,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Yes.,5,"""Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans."" and ""The largest models were generally the least truthful."""
acl2022,Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models,Yes.,4,"""We investigate the bias transfer hypothesis"
acl2022,A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation,Yes.,5,"""Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications."""
acl2022,Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text,Yes.,4,"""errors in machine generations become ever subtler and harder to spot,"" and ""the ten error categories of Scarecrow—such as redundancy, commonsense errors, and incoherence."""
acl2022,Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task,Yes.,5,"""existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline."""
acl2022,Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions,Yes.,5,"""However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them."""
acl2022,Coherence boosting: When your pretrained language model is not paying enough attention,Yes.,5,"""We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction."""
acl2022,Data Contamination: From Memorization to Exploitation,Yes.,5,"""It is not clear to what extent models exploit the contaminated data for downstream tasks."" and ""Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation."""
acl2022,Kronecker Decomposition for GPT Compression,Yes.,5,"""Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory."""
naacl2022,Provably Confidential Language Modelling,Yes.,5,"""Large language models are shown to memorize privacy information such as social security numbers in training data."""
naacl2022,"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",Yes.,5,"""We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities."""
naacl2022,Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models,Yes.,4,"""These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective."""
naacl2022,Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Yes.,5,"""current evaluation methods show some significant shortcomings,"" ""they fail to effectively map out the aspects of language understanding that remain challenging to existing models,"" and ""our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models."""
naacl2022,Exposing the Limits of Video-Text Models through Contrast Sets,Yes.,5,"""We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods."""
naacl2022,KALA: Knowledge-Augmented Language Model Adaptation,Yes.,5,"""Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general"
naacl2022,You Don’t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers’ Private Personas,Yes.,4,"""privacy concerns have arisen recently"
naacl2022,Methods for Estimating and Improving Robustness of Language Models,Yes.,5,"""large language models (LLMs) suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem"" and ""weak ability to generalise outside of the training domain."""
naacl2022,Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition,Yes.,5,"""We show that dialect variations that surface in the form of a different lexicon, grammar, and occasionally semantics can significantly degrade the performance of the LM under mismatched conditions."""
acl2023,MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning,Yes.,4,"""Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications."""
acl2023,Knowledge of cultural moral norms in large language models,Yes.,4,"""We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms."""
acl2023,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Yes.,5,"""the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution"" and ""Our analysis shows that robustness does not appear to continuously improve as a function of size."""
acl2023,ALERT: Adapt Language Models to Reasoning Tasks,Yes.,5,"""it is unclear whether these models are applying reasoning skills they have learnt during pre-training, or if they are simply memorizing their training corpus at finer granularity,"" and ""we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization"
acl2023,ThinkSum: Probabilistic reasoning over sets using large language models,Yes.,5,"""recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions."""
acl2023,Do language models have coherent mental models of everyday things?,Yes.,5,"""we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent 'parts mental models' (54-59% accurate, 19-43% conditional constraint violation)."""
acl2023,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,Yes.,5,"""Despite the success of Zero-shot-CoT, it still suffers from three pitfalls"
acl2023,Dynamic and Efficient Inference for Text Generation via BERT Family,Yes.,5,"""they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm."""
acl2023,Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model,Yes.,4,"""Existing bias mitigation methods require social-group-specific word pairs (e.g., “man” – “woman”) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating bias in"
acl2023,Explanation-based Finetuning Makes Models More Robust to Spurious Cues,Yes.,5,"""Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data."""
acl2023,"On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",Yes.,5,"""We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output,"" and ""Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved."""
acl2023,MISGENDERED: Limits of Large Language Models in Understanding Pronouns,Yes.,5,"""When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging 31.0% accuracy). This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations."""
acl2023,SCOTT: Self-Consistent Chain-of-Thought Distillation,Yes.,4,"""Even more concerning, there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions."""
acl2023,Evaluating Open-Domain Question Answering in the Era of Large Language Models,Yes.,5,"""The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs."""
acl2023,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,Yes.,5,"""one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications."""
acl2023,Language model acceptability judgements are not always robust to context,Yes.,5,"""We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure."" and ""This sensitivity to highly specific syntactic features of the context can only be explained by the models’ implicit in-context learning abilities."""
acl2023,RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,Yes.,5,"""Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets."""
acl2023,Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency,Yes.,5,"""non-linguistic skill injection typically comes at a cost for LLMs"
acl2023,Parallel Context Windows for Large Language Models,Yes.,5,"""When applied to processing long text, Large Language Models (LLMs) are limited by their context window."""
acl2023,Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model,Yes.,5,"""However, these models also suffer from the pathology of overconfidence in the out-of-distribution examples, potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions."""
acl2023,SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration,Yes.,4,"""The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising."""
acl2023,Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales,Yes.,4,"""We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility."""
acl2023,RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs,Yes.,4,"""Despite their unprecedented success, even the largest language models make mistakes,"" and ""this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning"
acl2023,Targeted Data Generation: Finding and Fixing Model Weaknesses,Yes.,4,"""state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust."""
acl2023,On “Scientific Debt” in NLP: A Case for More Rigour in Language Model Pre-Training Research,Yes.,5,"""current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions. These practices (i) leave us ill-equipped to understand which pre-training approaches should be used under what circumstances; (ii) impede reproducibility and credit assignment; and (iii) render it difficult to understand"
acl2023,WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,Yes.,5,"""We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias."""
acl2023,When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,Yes.,5,"""Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters."" and ""We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases."" and ""Scaling, on the other hand, mainly improves memorization of"
acl2023,Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,Yes.,5,"""Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge,"" and ""statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict."""
acl2023,How Do In-Context Examples Affect Compositional Generalization?,Yes.,5,"""We find that the compositional generalization performance can be easily affected by the selection of in-context examples,"" and ""two strong limitations are observed"
acl2023,Contrastive Decoding: Open-ended Text Generation as Optimization,Yes.,5,"""maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics."""
acl2023,CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models,Yes.,4,"""Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias."" and ""Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases."""
acl2023,Mitigating Label Biases for In-context Learning,Yes.,5,"""domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples."""
acl2023,"RARR: Researching and Revising What Language Models Say, Using Language Models",Yes.,5,"""However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence."""
acl2023,Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models,Yes.,5,"""Test results show that the best models perform well on non-elliptical examples but struggle with all but the simplest ellipsis structures."""
acl2023,Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,Yes.,4,"""Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people."""
acl2023,Probing Physical Reasoning with Counter-Commonsense Context,Yes.,5,"""The results show that while large language models can use prepositions such as 'in' and 'into' in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense."""
acl2023,"Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)",Yes.,5,"""We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents."""
acl2023,Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning,Yes.,5,"""Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk."""
acl2023,A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification,Yes.,5,"""these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. ... We observe LLMs are more prone to failure in these cases."""
acl2023,Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section,Yes.,5,"""when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input?"""
acl2023,MathPrompter: Mathematical Reasoning using Large Language Models,Yes.,5,"""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers."" and ""To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption."""
acl2023,KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications,Yes.,5,"""Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications."" and ""This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs."""
acl2023,A Static Evaluation of Code Completion by Large Language Models,Yes.,4,"""Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models."""
acl2023,"Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",Yes.,4,"""Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages."""
eacl2023,WinoDict: Probing language models for in-context word acquisition,Yes.,5,"""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time."""
eacl2023,Nationality Bias in Text Generation,Yes.,5,"""This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms"" and ""GPT-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same."""
eacl2023,"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility",Yes.,5,"""Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities."" and ""We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer"
eacl2023,On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex,Yes.,5,"""The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs,"" and ""the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios,"" and ""the large language model of code is vulnerable to carefully crafted adversarial examples."""
eacl2023,MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers,Yes.,5,"""the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as overparameterisation."""
eacl2023,SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,Yes.,4,"""A common limitation of diagnostic tests for detecting social biases in NLP models is that they may only detect stereotypic associations that are pre-specified by the designer of the test."" and ""We also test SODAPOP on debiased models and show the limitations of"
eacl2023,Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding,Yes.,5,"""However, very few of these studies have analyzed the impact of compression on the generalizability and robustness of compressed models for out-of-distribution (OOD) data,"" and ""the compressed models are significantly less robust than their PLM counterparts on OOD test sets although they obtain similar performance on in-distribution development sets for a task."""
eacl2023,Opportunities and Challenges in Neural Dialog Tutoring,Yes.,5,"""We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios."" and ""Our human quality evaluation shows that both models and ground-truth annotations exhibit low performance in terms of equitable tutoring, which measures learning opportunities for"
eacl2023,Assessing Out-of-Domain Language Model Performance from Few Examples,Yes.,5,"""While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts."" and ""given a few target-domain examples and a set of models with similar training performance, can we understand how these models will perform on OOD test data?"""
eacl2023,Towards preserving word order importance through Forced Invalidation,Yes.,5,"""However, recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed."""
eacl2023,Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models,Yes.,4,"""However, safely deploying them in real world applications is challenging because they generate toxic content."""
eacl2023,When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization,Yes.,5,"""Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood."" and ""We show that these biases manifest themselves as hallucinations in summarization, leading to factually incorrect summaries."""
eacl2023,Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,Yes.,4,"""Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models."""
emnlp2023,Primacy Effect of ChatGPT,Yes.,5,"""ChatGPT’s decision is sensitive to the order of labels in the prompt; ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer."""
emnlp2023,Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension,Yes.,5,"""Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options"
emnlp2023,Theory of Mind for Multi-Agent Collaboration via Large Language Models,Yes.,5,"""Our results reveal limitations in LLM-based agents’ planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state."""
emnlp2023,GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP,Yes.,5,"""our work adds to a growing body of research underscoring the limitations of ChatGPT."""
emnlp2023,CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models,Yes.,5,"""We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization."""
emnlp2023,Conceptual structure coheres in human cognition but not in large language models,Yes.,5,"""Structures estimated from the LLM behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task used to generate behavior responses–responses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure"
emnlp2023,Towards LLM-driven Dialogue State Tracking,Yes.,5,"""Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities."""
emnlp2023,We’re Afraid Language Models Aren’t Modeling Ambiguity,Yes.,5,"""We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset."""
emnlp2023,Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus,Yes.,5,"""However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications."""
emnlp2023,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,Yes.,5,"""However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and"
emnlp2023,Unveiling the Implicit Toxicity in Large Language Models,Yes.,5,"""We show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting,"" and ""Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs."""
emnlp2023,ALCUNA: Large Language Models Meet New Knowledge,Yes.,5,"""We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge."""
emnlp2023,Robust Prompt Optimization for Large Language Models Against Distribution Shifts,Yes.,5,"""We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis."""
emnlp2023,Interpreting Embedding Spaces by Conceptualization,Yes.,4,"""One major drawback of this type of representation is their incomprehensibility to humans."" and ""Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model."""
emnlp2023,Knowledge-Augmented Language Model Verification,Yes.,5,"""Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated."" and ""the model may fail to retrieve the knowledge relevant to the given query, or the model may not faithfully reflect the retrieved knowledge in the generated text."""
emnlp2023,Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation,Yes.,5,"""However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes."""
emnlp2023,“Fifty Shades of Bias”: Normative Ratings of Gender Bias in GPT Generated English Text,Yes.,4,"""With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative."""
emnlp2023,MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations,Yes.,4,"""LLMs have a knowledge cutoff and are costly to finetune repeatedly"" and ""our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example."""
emnlp2023,Instructed Language Models with Retrievers Are Powerful Entity Linkers,Yes.,5,"""the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base."" and ""the EL task remains a persistent hurdle for general LLMs."""
emnlp2023,Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?,Yes.,5,"""Surprisingly, throughout our experiments, we find that although the knowledge seems to be successfully injected, the correctness of injected knowledge only has a very limited effect on the models’ downstream performance. This finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrain-finetune paradigm."""
emnlp2023,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",Yes.,4,"""it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values"" and ""we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges."""
emnlp2023,"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",Yes.,5,"""the issue of hallucination has parallelly emerged as a by-product, posing significant concerns"" and ""we propose two solution strategies for mitigating hallucinations."""
emnlp2023,The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages,Yes.,5,"""Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score."""
emnlp2023,Understanding the Effect of Model Compression on Social Bias in Large Language Models,Yes.,4,"""Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model’s predictions in downstream tasks, leading to representational harm."""
emnlp2023,Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation,Yes.,5,"""Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation."""
emnlp2023,API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs,Yes.,4,"""However, three pivotal questions remain unanswered"
emnlp2023,Evaluating Large Language Models on Controlled Generation Tasks,Yes.,5,"""large language models struggle at meeting fine-grained hard constraints."""
emnlp2023,Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies,Yes.,5,"""We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens."""
emnlp2023,The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models,Yes.,5,"""A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence."""
emnlp2023,ROBBIE: Robust Bias Evaluation of Large Generative Language Models,Yes.,4,"""we must develop comprehensive enough tools to measure and improve their fairness,"" and ""testing LLMs on more datasets can potentially help us characterize their biases more fully,"" and ""we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases."""
emnlp2023,Adapting Language Models to Compress Contexts,Yes.,5,"""Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents."""
emnlp2023,Large Language Models are Temporal and Causal Reasoners for Video Question Answering,Yes.,5,"""However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as ‘ungrounded guesses’ or ‘hallucinations’."""
emnlp2023,TrojanSQL: SQL Injection against Natural Language Interface to Database,Yes.,4,"""Experimental results demonstrate that both medium-sized models based on fine-tuning and LLM-based parsers using prompting techniques are vulnerable to this type of attack, with attack success rates as high as 99% and 89%, respectively."""
emnlp2023,Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models,Yes.,5,"""LLMs have shown the ability to memorize and reproduce portions of their training data when prompted by adversaries. Prior research has focused on addressing this memorization issue and preventing verbatim replication through techniques like knowledge unlearning and data pre-processing. However, these methods have limitations regarding the number of protected samples, limited privacy types, and potentially lower-quality generative models."""
emnlp2023,Meta-Learning Online Adaptation of Language Models,Yes.,5,"""the knowledge in static language models falls out of date, limiting the model’s effective 'shelf life.' While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake."""
emnlp2023,Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis,Yes.,5,"""we find that models are more sensitive to certain perturbations such as replacing words with their synonyms."""
emnlp2023,Can Large Language Models Capture Dissenting Human Voices?,Yes.,5,"""we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population."""
emnlp2023,Merging Generated and Retrieved Knowledge for Open-Domain QA,Yes.,4,"""retrieving passages from a given source is known to suffer from insufficient knowledge coverage"" and ""LLMs tend to 'hallucinate' content that conflicts with the retrieved knowledge."""
emnlp2023,Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition,Yes.,5,"""These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones."""
emnlp2023,Prompting is not a substitute for probability measurements in large language models,Yes.,5,"""Broadly, we find that LLMs’ metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities."""
emnlp2023,"The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",Yes.,5,"""Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions."""
emnlp2023,HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts,Yes.,5,"""However, this strategy has two key limitations"
emnlp2023,"Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",Yes.,5,"""While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers."""
emnlp2023,Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators,Yes.,4,"""community concerns abound regarding the factuality and potential implications of using this uncensored knowledge"" and ""Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks."""
emnlp2023,Compressing Context to Enhance Inference Efficiency of Large Language Models,Yes.,5,"""However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM’s fixed context length."""
emnlp2023,Can You Follow Me? Testing Situational Understanding for ChatGPT,Yes.,5,"""Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs),"" and ""find that despite the fundamental simplicity of the task, the model’s performance reflects an inability to retain correct environment states across time,"" and ""performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to"
emnlp2023,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Yes.,5,"""Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge."" and ""Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts."""
emnlp2023,Enabling Large Language Models to Generate Text with Citations,Yes.,5,"""their generated outputs are prone to hallucination"" and ""current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time."""
emnlp2023,Counting the Bugs in ChatGPT’s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,Yes.,5,"""We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results—through the lens of morphology—cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading."""
emnlp2023,MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models,Yes.,5,"""However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors."""
emnlp2023,Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models,Yes.,5,"""Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain."""
emnlp2023,Evaluation of African American Language Bias in Natural Language Generation,Yes.,4,"""We present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks."""
emnlp2023,An Investigation of LLMs’ Inefficacy in Understanding Converse Relations,Yes.,5,"""The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark."""
emnlp2023,HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies,Yes.,5,"""A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as"
emnlp2023,"Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",Yes.,4,"""The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks."""
emnlp2023,Copyright Violations and Large Language Models,Yes.,5,"""This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text."""
emnlp2023,Symbolic Planning and Code Generation for Grounded Dialogue,Yes.,5,"""LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding."""
emnlp2023,Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models,Yes.,5,"""The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts."""
emnlp2023,Task-Agnostic Low-Rank Adapters for Unseen English Dialects,Yes.,4,"""Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies."""
emnlp2023,Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization,Yes.,4,"""the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios,"" and ""Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency,"" and ""the decentralized data is generally non-Independent and Identically Distributed ("
emnlp2023,Active Retrieval Augmented Generation,Yes.,5,"""Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output."""
emnlp2023,Reasoning with Language Model is Planning with World Model,Yes.,5,"""However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs’ absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating"
emnlp2023,This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models,Yes.,5,"""they fail to interpret negation, a crucial step in Natural Language Processing,"" and ""Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues,"" and ""the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs"
emnlp2023,SOUL: Towards Sentiment and Opinion Understanding of Language,Yes.,5,"""Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications."""
emnlp2023,Detecting and Mitigating Hallucinations in Multilingual Summarisation,Yes.,5,"""Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation."" and ""we assess a broad range of multilingual large language models, and find that they all tend to hallucinate often in languages different from English."""
emnlp2023,EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs,Yes.,5,"""However, their expensive computations and high memory requirements are prohibitive for deployment."" and ""the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks."""
emnlp2023,EpiK-Eval: Evaluation for Language Models as Epistemic Models,Yes.,5,"""Evaluations across various LLMs reveal significant weaknesses in this domain."" and ""We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives."""
emnlp2023,Large Language Models are biased to overestimate profoundness,Yes.,5,"""However, LLMs systematically overestimate the profoundness of nonsensical statements"" and ""this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements."""
emnlp2023,SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization,Yes.,5,"""Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8% below estimated human performance, highlighting the gaps in LLMs’ ability to reason about facts and detect inconsistencies when they occur."""
emnlp2023,Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models,Yes.,4,"""We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results."""
emnlp2023,Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,Yes.,5,"""However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst."" and ""They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews."""
emnlp2023,Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT,Yes.,5,"""we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios."""
emnlp2023,Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting,Yes.,5,"""A crucial challenge for generative large language models (LLMs) is diversity"
emnlp2023,Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection,Yes.,4,"""the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises,"" and ""the existing detectors can be easily circumvented using straightforward automatic adversarial attacks."""
emnlp2023,Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,Yes.,5,"""the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks"" and ""subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data."""
emnlp2023,Learning from Mistakes via Cooperative Study Assistant for Large Language Models,Yes.,5,"""However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits."""
emnlp2023,Conceptor-Aided Debiasing of Large Language Models,Yes.,5,"""Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus,"" and ""many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy."""
emnlp2023,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,Yes.,5,"""there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes."""
emnlp2023,Revisiting the Knowledge Injection Frameworks,Yes.,5,"""we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected,"" and ""how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved."""
emnlp2023,Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models,Yes.,4,"""an analysis of LLaMA’s errors reveals significant limitations in its ability to recall facts in languages other than English, plus difficulties related to the location and gender of fact subjects."""
emnlp2023,Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection,Yes.,4,"""The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data."""
emnlp2023,StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding,Yes.,5,"""Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa."""
emnlp2023,An Empirical Study of Translation Hypothesis Ensembling with Large Language Models,Yes.,4,"""Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output."""
emnlp2023,FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation,Yes.,5,"""Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming"
emnlp2023,Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems,Yes.,5,"""language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation."""
emnlp2023,StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models,Yes.,4,"""Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data."" and ""This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations."""
emnlp2023,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,Yes.,5,"""Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture."""
emnlp2023,Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding,Yes.,5,"""critically assess three points recurring in critiques of LLM capacities"
emnlp2023,Question Answering as Programming for Solving Time-Sensitive Questions,Yes.,5,"""our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs’ inability to perform rigorous reasoning based on surface-level text semantics."""
emnlp2023,Context Compression for Auto-regressive Transformers with Sentinel Tokens,Yes.,5,"""The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency."""
emnlp2023,MoPe: Model Perturbation based Privacy Attacks on Language Models,Yes.,5,"""Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data."""
emnlp2023,Empower Nested Boolean Logic via Self-Supervised Curriculum Learning,Yes.,5,"""We find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease."""
emnlp2023,KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the *hallucination* problem, poses a significant risk to their deployment."""
emnlp2023,Language Models with Rationality,Yes.,5,"""This lack of interpretability is a growing impediment to widespread use of LLMs."" and ""resolve inconsistencies that may exist."""
emnlp2023,Mitigating Temporal Misalignment by Discarding Outdated Facts,Yes.,5,"""While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update."""
emnlp2023,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,Yes,5,"We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning."
emnlp2023,Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism,Yes,5,"We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible→implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family."
emnlp2023,Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness,Yes,5,reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.
emnlp2023,CRAB: Assessing the Strength of Causal Relationships Between Real-world Events,Yes,5,find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains.
emnlp2023,Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation,Yes,5,"the propensity of LLMs to generate inaccurate or non-factual content, termed “hallucinations”, remains a significant challenge."
emnlp2023,MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,Yes,5,"current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions."
emnlp2023,Consistency Analysis of ChatGPT,Yes,5,"prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs."
emnlp2023,Mitigating Societal Harms in Large Language Models,Yes,5,"We will provide an overview of potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations."
emnlp2023,H2O Open Ecosystem for State-of-the-art Large Language Models,Yes,5,"LLMs represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text."
emnlp2023,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,Yes,5,LLMs’ inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses.
emnlp2023,CLEVA: Chinese Language Models EVAluation Platform,Yes,4,"The absence of a comprehensive Chinese benchmark that thoroughly assesses a model’s performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs."
emnlp2023,LM-Polygraph: Uncertainty Estimation for Language Models,Yes,5,"However, a significant challenge arises as these models often “hallucinate”, i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements."
emnlp2023,CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering,Yes.,5,"""leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the"
emnlp2023,DELPHI: Data for Evaluating LLMs’ Performance in Handling Controversial Issues,Yes.,4,"""This dataset presents challenges concerning knowledge recency, safety, fairness, and bias."""
ArXiv2024,A Study on Large Language Models' Limitations in Multiple-Choice Question Answering,Yes.,5,"""Despite their ubiquitous use, there is no systematic analysis of their specific capabilities and limitations."" and ""We analyze 26 small open-source models and find that 65% of the models do not understand the task, only 4 models properly select an answer from the given choices, and only 5 of these models are choice order independent."""
ArXiv2024,The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models,Yes.,5,"""hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications"" and ""To tackle the LLM hallucination, three key questions should be well studied"
ArXiv2024,LLM on FHIR -- Demystifying Health Records,Yes.,4,"""However, challenges included variability in LLM responses and the need for precise filtering of health data."" and ""While promising, the implementation and pilot also highlight risks such as inconsistent responses and the importance of replicable output."""
ArXiv2024,RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning,Yes.,5,"""Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world."" and ""the performance of GPT-4 even drops significantly from 80.00 to 58"
ArXiv2024,"The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey",Yes.,5,"""However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation."""
ArXiv2024,LLMs for Relational Reasoning: How Far are We?,Yes.,5,"""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."" and ""Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and"
ArXiv2024,Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning,Yes.,4,"""Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content."" and ""prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100%"
ArXiv2024,FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference,Yes.,5,"""The large number of parameters in Pretrained Language Models enhance their performance, but also make them resource-intensive, making it challenging to deploy them on commodity hardware like a single GPU."" and ""Due to the memory and power limitations of these devices, model compression techniques are often used to decrease both the model's size and its inference latency. This usually results in a trade-off between model accuracy"
ArXiv2024,LLMs for Test Input Generation for Semantic Caches,Yes.,4,"""However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience."" and ""Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system."""
ArXiv2024,GRATH: Gradual Self-Truthifying for Large Language Models,Yes.,5,"""existing LLMs still struggle with generating truthful content, as evidenced by their modest performance on benchmarks like TruthfulQA."""
ArXiv2024,Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling,Yes.,5,"""This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web."""
ArXiv2024,Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning,Yes.,5,"""Despite being widely applied, in-context learning is vulnerable to malicious attacks."" and ""Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model."""
ArXiv2024,Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately,Yes.,5,"""Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions."""
ArXiv2024,TrustLLM: Trustworthiness in Large Language Models,Yes.,5,"""Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. ... discussion of open challenges and future directions. ... our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. ... some LLMs may be overly calibrated towards exhibiting trustworthiness"
ArXiv2024,Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering,Yes.,5,"""Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community."""
ArXiv2024,CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities,Yes.,5,"""Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems."" and ""Using this corpus, we find that models often arrive at the correct final answer"
ArXiv2024,A Computational Framework for Behavioral Assessment of LLM Therapists,Yes.,5,"""Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences."" and ""Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care"
ArXiv2024,Navigating the OverKill in Large Language Models,Yes.,5,"""Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries."""
ArXiv2024,E^2-LLM: Efficient and Extreme Length Extension of Large Language Models,Yes.,5,"""Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources."""
ArXiv2024,LoMA: Lossless Compressed Memory Attention,Yes.,5,"""Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts."""
ArXiv2024,Can AI Assistants Know What They Don't Know?,Yes.,5,"""Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications."""
ArXiv2024,CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs,Yes.,5,"""Large Multimodal Models (LMMs) encounter two issues in such scenarios"
ArXiv2024,ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters,Yes.,5,"""achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context."""
ArXiv2024,Physio: An LLM-Based Physiotherapy Advisor,Yes.,5,"""However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being."""
ArXiv2024,"Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",Yes.,5,"""In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs."""
ArXiv2024,ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios,Yes.,5,"""Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning."""
ArXiv2024,Dynamic Q&A of Clinical Documents with Large Language Models,Yes.,4,"""Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain."""
ArXiv2024,Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks,Yes.,5,"""language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior."""
ArXiv2024,Security Code Review by LLMs: A Deep Dive into Responses,Yes.,5,"""Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection."""
ArXiv2024,EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge,Yes.,4,"""However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English."""
ArXiv2024,Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis,Yes.,4,"""However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount."""
ArXiv2024,How well can large language models explain business processes?,Yes.,5,"""Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason."""
ArXiv2024,Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models,Yes.,5,"""a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level."""
ArXiv2024,Large Language Models for Mathematical Reasoning: Progresses and Challenges,Yes.,4,"""an overview of factors and concerns affecting LLMs in solving math"" and ""an elucidation of the persisting challenges within this domain."""
ArXiv2024,Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering,Yes.,5,"""While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers."""
ArXiv2024,Seven Failure Points When Engineering a Retrieval Augmented Generation System,Yes.,4,"""RAG systems aim to"
ArXiv2024,LLMs for Robotic Object Disambiguation,Yes.,5,"""Despite multiple query attempts with zero-shot prompt engineering (details can be found in the Appendix), the LLM struggled to inquire about features not explicitly provided in the scene description."""
ArXiv2024,VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks,Yes.,5,"""Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents."""
ArXiv2024,DocFinQA: A Long-Context Financial Reasoning Dataset,Yes.,5,"""DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents."""
ArXiv2024,Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation,Yes.,5,"""existing approaches struggle with hallucinations and overconfident predictions."""
ArXiv2024,Are self-explanations from Large Language Models faithful?,Yes.,5,"""convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk"" and ""showing self-explanations should not be trusted in general."""
ArXiv2024,MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries,Yes.,4,"""However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence."""
ArXiv2024,CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning,Yes.,4,"""However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge."" and ""The results demonstrate that CMMU poses a significant challenge to the recent MLLMs."""
ArXiv2024,JumpCoder: Go Beyond Autoregressive Coder via Online Modification,Yes.,5,"""While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance."""
ArXiv2024,LightHouse: A Survey of AGI Hallucination,Yes.,4,"""numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research"" and ""Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models)."""
ArXiv2024,Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences,Yes.,5,"""we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors."""
ArXiv2024,Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models,Yes.,5,"""Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance."" and ""This research critically examines these biases and quantifies the effects on a representative list selection task."""
ArXiv2024,MouSi: Poly-Visual-Expert Vision-Language Models,Yes.,4,"""Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information."""
ArXiv2024,Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet,Yes.,5,"""We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis."""
ArXiv2024,Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do,Yes.,5,"""We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception,"" and ""Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others?"""
ArXiv2024,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,Yes.,5,"""Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings,"" and ""MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations."""
ArXiv2024,Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,Yes.,5,"""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount."""
ArXiv2024,Under the Surface: Tracking the Artifactuality of LLM-Generated Data,Yes.,5,"""This paper reveals significant hidden disparities, especially in complex tasks where LLMs often miss the nuanced understanding of intrinsic human-generated content,"" and ""It highlights the LLMs' shortcomings in replicating human traits and behaviors, underscoring the importance of addressing biases and artifacts produced in LLM-generated content for future research and development."""
ArXiv2024,Generalist embedding models are better at short-context clinical semantic search than specialized embedding models,Yes.,4,"""Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs."" and ""The highlighted problem of specialized models may be due to the fact that they have not been trained on sufficient data, and in particular on datasets that are not diverse enough to have a reliable global language understanding,"
ArXiv2024,LongAlign: A Recipe for Long Context Alignment of Large Language Models,Yes.,5,"""Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length."""
ArXiv2024,Benchmarking Large Language Models on Controllable Generation under Diversified Instructions,Yes.,5,"""revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs."""
ArXiv2024,Gender Bias in Machine Translation and The Era of Large Language Models,Yes.,4,"""The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies."""
ArXiv2024,Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications,Yes.,4,"""The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate."""
ArXiv2024,Leveraging Large Language Models for NLG Evaluation: A Survey,Yes.,4,"""Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques."""
ArXiv2024,Prompting Large Vision-Language Models for Compositional Reasoning,Yes.,5,"""However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset."" and ""this limitation stems from two factors"
ArXiv2024,MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models,Yes.,5,"""We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance."""
ArXiv2024,OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models,Yes.,5,"""The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field."""
ArXiv2024,Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability,Yes.,5,"""LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps."""
ArXiv2024,From Prompt Engineering to Prompt Science With Human in the Loop,Yes.,5,"""we need to be concerned about how it may affect that research, its findings, or any future works based on that research,"" and ""they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor."""
ArXiv2024,When Large Language Models Meet Vector Databases: A Survey,Yes.,5,"""With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues."""
ArXiv2024,OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models,Yes.,5,"""This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped."""
ArXiv2024,Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches,Yes.,5,"""although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation."""
ArXiv2024,InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification,Yes.,5,"""our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss."""
ArXiv2024,ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers,Yes.,5,"""The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models."""
ArXiv2024,Security and Privacy Challenges of Large Language Models: A Survey,Yes.,5,"""While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks."""
ArXiv2024,Hallucination Benchmark in Medical Visual Question Answering,Yes.,5,"""these models are not extensively tested on the hallucination phenomenon in clinical settings"" and ""The study provides an in-depth analysis of current models' limitations."""
ArXiv2024,Knowledge Verification to Nip Hallucination in the Bud,Yes.,5,"""they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination."""
ArXiv2024,Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness,Yes.,5,"""However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective."" and ""Our results shed light on the limitations of the LLM chatbots when compared to specialized models."""
ArXiv2024,Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values,Yes.,4,"""troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies."""
ArXiv2024,Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine,Yes.,5,"""we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows."""
ArXiv2024,On Prompt-Driven Safeguarding for Large Language Models,Yes.,5,"""We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless."""
ArXiv2024,Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation,Yes.,5,"""Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations."""
ArXiv2024,Detection of Machine-Generated Text: Literature Survey,Yes.,4,"""Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes."" and ""To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented."""
ArXiv2024,TOFU: A Task of Fictitious Unlearning for LLMs,Yes.,5,"""Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns."" and ""Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."""
ArXiv2024,Extending LLMs' Context Window with 100 Samples,Yes.,5,"""Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs."""
ArXiv2024,MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline,Yes.,5,"""there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints."""
ArXiv2024,"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",Yes.,5,"""the safety and security issues of LLM systems have become the major obstacle to their widespread application"" and ""potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies."""
ArXiv2024,Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning,Yes.,5,"""Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs."""
ArXiv2024,Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?,Yes.,5,"""The paper discusses what is needed to address the limitations of current LLM-centered AI systems."""
ArXiv2024,Small Language Model Can Self-correct,Yes.,5,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone"" and ""large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans."""
ArXiv2024,Conditional and Modal Reasoning in Large Language Models,Yes.,5,"""Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals."""
ArXiv2024,Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?,Yes.,5,"""The models showed significantly reduced accuracy on tasks with suspected hierarchical bias."""
ArXiv2024,Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring,Yes.,4,"""The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM)."" and ""highlight the potential requirements and limitations of utilizing chatbots in conversational explainability."""
ArXiv2024,Learning Shortcuts: On the Misleading Promise of NLU in Language Models,Yes.,5,"""LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules."""
ArXiv2024,Detecting Multimedia Generated by Large AI Models: A Survey,Yes.,4,"""this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns."" and ""we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs."""
ArXiv2024,SocraSynth: Multi-LLM Reasoning with Conditional Statistics,Yes.,4,"""Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability."""
ArXiv2024,Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation,Yes.,5,"""Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated."""
ArXiv2024,The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance,Yes.,5,"""We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs."""
ArXiv2024,MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance,Yes.,5,"""This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning."""
ArXiv2024,Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review,Yes.,4,"""the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations."""
ArXiv2024,Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging,Yes.,5,"""The outcomes reveal a 100% success rate in manipulating PLIP's predictions, underscoring its susceptibility to adversarial perturbations."""
ArXiv2024,A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models,Yes.,5,"""a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded,"" and ""The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations,"" and ""we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of L"
ArXiv2024,The Reasoning Under Uncertainty Trap: A Structural AI Risk,Yes.,5,"""we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy."""
