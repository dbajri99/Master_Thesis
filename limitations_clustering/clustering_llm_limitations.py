# -*- coding: utf-8 -*-
"""clustering_LLM_limitations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QdlUcjRm6bOryRBBrm_Dv3SW1LEfKVNq

## Identifying and Clustering Limitations of Large Language Models (LLMs) Using BERTopic and GPT-3.5

In this notebook, we explore the limitations of LLMs by analyzing and clustering evidence. By leveraging LLM-based clustering, we use embeddings generated from OpenAI's GPT-3.5 model, which capture the nuanced understanding of the text.
"""

from google.colab import files
uploaded = files.upload()

!pip install bertopic

!pip install openai

!pip install spacy
!python -m spacy download en_core_web_sm

import pandas as pd
import numpy as np
from bertopic import BERTopic
import matplotlib.pyplot as plt
import openai
import re

client = openai.OpenAI(api_key='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')


df = pd.read_excel('acl_only_4-5_rated_papers_final.xlsx')


def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower().strip()
    return text

df['Evidence'] = df['Evidence'].apply(preprocess_text)

def get_embeddings(texts):
    try:
        response = client.embeddings.create(
            input=texts,
            model="text-embedding-ada-002"
        )
        embeddings = [item.embedding for item in response.data]
        return np.array(embeddings)
    except Exception as e:
        print("Error fetching embeddings:", e)
        return np.array([])

embeddings = get_embeddings(df['Evidence'].tolist())

if embeddings.size > 0:
    topic_model = BERTopic(embedding_model=None)

    topics, _ = topic_model.fit_transform(df['Evidence'], embeddings)

    topic_info = topic_model.get_topic_info()
    print(topic_info)

    df['Topic'] = topics
    df.to_csv('clustered_evidence_with_bertopic.csv', index=False)

    topic_model.visualize_barchart()
    plt.show()
else:
    print("No embeddings were generated, please check the error logs.")

"""Clustering using LLMs Embeddings, and BERTopic utilizing Kmeans as the Default Algorithm"""

import pandas as pd
import numpy as np
import spacy
import re
from nltk.corpus import stopwords
import nltk
from bertopic import BERTopic
import openai


nltk.download('stopwords')
client = openai.OpenAI(api_key='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')
df = pd.read_excel('acl_only_4-5_rated_papers_final.xlsx')
nlp = spacy.load("en_core_web_sm")
nltk_stopwords = set(stopwords.words('english'))


def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower().strip()
    doc = nlp(text)
    return ' '.join(token.lemma_ for token in doc if token.text not in nltk_stopwords)

df['Evidence'] = df['Evidence'].apply(preprocess_text)
def get_embeddings(texts):
    try:
        response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
        return np.array([item.embedding for item in response.data])
    except Exception as e:
        print("Error fetching embeddings:", e)
        return np.array([])
embeddings = get_embeddings(df['Evidence'].tolist())



if embeddings.size > 0:
    topic_model = BERTopic(embedding_model=None)
    topics, _ = topic_model.fit_transform(df['Evidence'], embeddings)
    topic_info = topic_model.get_topic_info()
    print(topic_info)
    df['Topic'] = topics
    df.to_csv('clustered_evidence_with_bertopic.csv', index=False)
else:
    print("No embeddings were generated, please check the error logs.")

"""### Final Resutls of Clustering the Limitations of LLMs for ACL Academic Papers Utilizing LLM-Based-Embeddings and HDBSCAN BERTopic Model.

Final results without applying Descriptive Naming of Clusters using GPT
"""

import pandas as pd
import numpy as np
import spacy
import re
from nltk.corpus import stopwords
import nltk
from bertopic import BERTopic
import openai
import umap
from sklearn.feature_extraction.text import TfidfVectorizer
import hdbscan

nltk.download('stopwords')

openai.api_key = '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'
df = pd.read_excel('acl_only_4-5_rated_papers_final.xlsx')
nlp = spacy.load("en_core_web_sm")
nltk_stopwords = set(stopwords.words('english'))

#custom stopwords
custom_stopwords = {'model', 'language', 'large', 'task', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge'}

# combine it wíth the basic nltk stopwords
all_stopwords = nltk_stopwords.union(custom_stopwords)

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower().strip()
    doc = nlp(text)
    processed_tokens = []
    for token in doc:
        if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
            processed_tokens.append(token.lemma_)
    return ' '.join(processed_tokens)

df['Evidence'] = df['Evidence'].apply(preprocess_text)

def get_embeddings(texts):
    try:
        response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
        return np.array([data.embedding for data in response.data])
    except Exception as e:
        print("Error fetching embeddings:", e)
        return np.array([])

embeddings = get_embeddings(df['Evidence'].tolist())

if embeddings.size > 0:
    umap_model = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine', min_dist=0.0)

    #  HDBSCAN model
    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric='euclidean', cluster_selection_method='eom')
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
    topic_model = BERTopic(umap_model=umap_model,
                           hdbscan_model=hdbscan_model,
                           vectorizer_model=vectorizer,
                           min_topic_size=5)

    topics, _ = topic_model.fit_transform(df['Evidence'], embeddings)
    topic_info = topic_model.get_topic_info()
    topic_info = topic_info[topic_info.Topic != -1]

    print(topic_info)

    df['Topic'] = topics
    df = df[df.Topic != -1]
    df.to_csv('clustered_evidence_with_bertopic.csv', index=False)
else:
    print("No embeddings were generated, please check the error logs.")

"""Using GPT-3.5 Model for Postprocessing"""

import pandas as pd
import numpy as np
import spacy
import re
from nltk.corpus import stopwords
import nltk
from bertopic import BERTopic
import openai
import umap
from sklearn.feature_extraction.text import TfidfVectorizer
import hdbscan

nltk.download('stopwords')

openai.api_key = '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'
df = pd.read_excel('acl_only_4-5_rated_papers_final.xlsx')
nlp = spacy.load("en_core_web_sm")
nltk_stopwords = set(stopwords.words('english'))

#custom stopwords
custom_stopwords = {'model', 'language', 'large', 'task', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge'}

# combine it wíth the basic nltk stopwords
all_stopwords = nltk_stopwords.union(custom_stopwords)

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower().strip()
    doc = nlp(text)
    processed_tokens = []
    for token in doc:
        if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
            processed_tokens.append(token.lemma_)
    return ' '.join(processed_tokens)

df['Evidence'] = df['Evidence'].apply(preprocess_text)

def get_embeddings(texts):
    try:
        response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
        return np.array([data.embedding for data in response.data])
    except Exception as e:
        print("Error fetching embeddings:", e)
        return np.array([])

embeddings = get_embeddings(df['Evidence'].tolist())

if embeddings.size > 0:
    umap_model = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine', min_dist=0.0)

    #  HDBSCAN model
    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric='euclidean', cluster_selection_method='eom')
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
    topic_model = BERTopic(umap_model=umap_model,
                           hdbscan_model=hdbscan_model,
                           vectorizer_model=vectorizer,
                           min_topic_size=5)

    topics, _ = topic_model.fit_transform(df['Evidence'], embeddings)
    topic_info = topic_model.get_topic_info()
    topic_info = topic_info[topic_info.Topic != -1]

    def generate_cluster_name(cluster_terms):
        prompt = f"Given the following key terms: {', '.join(cluster_terms)}, generate a concise limitation with 2 words at most that summarizes the descriptive name for this cluster of topics focusing on the LIMITATIONS of large language models."

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant specialized in summarizing limitations with 2-3 words at most."},
                    {"role": "user", "content": prompt}
                ]
            )

            if response and response.choices:
                return response.choices[0].message.content.strip()
            else:
                return "No description available"
        except Exception as e:
            print(f"Failed to generate cluster name: {str(e)}")
            return "Error generating name"
    topic_info['Descriptive_Name'] = topic_info['Representation'].apply(lambda terms: generate_cluster_name(terms))

    print(topic_info[['Topic', 'Descriptive_Name', 'Count']])

    df['Topic'] = topics
    df = df[df.Topic != -1]
    df.to_csv('clustered_evidence_with_bertopic_ACL_postprocessing.csv', index=False)
else:
    print("No embeddings were generated, please check the error logs.")

from google.colab import files
uploaded = files.upload()

"""#### Time Series Analysis of LLM Limitations by Fraction of Papers Retrived from ACL
This code performs a time series analysis to visualize the trend of different LLM limitations over time.
"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_evidence_with_bertopic_ACL_postprocessing_all_papers.csv')
df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%B')
total_papers_per_date = df.groupby('Date').size()
papers_per_topic_per_date = df.groupby(['Date', 'Topic']).size()
fraction_per_topic_per_date = papers_per_topic_per_date.div(total_papers_per_date, level='Date').unstack(fill_value=0)
fraction_per_topic_per_date *= 100
max_fraction = fraction_per_topic_per_date.max().max()
cluster_names = {
    0: "Reasoning Struggle",
    1: "Hallucination",
    2: "Social Bias",
    3: "Privacy Risks",
    4: "Compute Constraints"
}
fig, axs = plt.subplots(len(fraction_per_topic_per_date.columns), 1, figsize=(10, 15), sharex=True)
colors = plt.cm.get_cmap('tab10', len(fraction_per_topic_per_date.columns))
for i, topic in enumerate(fraction_per_topic_per_date.columns):
    axs[i].scatter(fraction_per_topic_per_date.index, fraction_per_topic_per_date[topic],
                   color=colors(i), s=40, label=f'Cluster {int(topic)} - {cluster_names[int(topic)]}')
    axs[i].plot(fraction_per_topic_per_date.index, fraction_per_topic_per_date[topic],
                color=colors(i), linewidth=2, linestyle='-')
    axs[i].set_title(f'Cluster {int(topic)}: {cluster_names[int(topic)]}', fontsize=12, weight='bold')
    axs[i].grid(True, linestyle='--', alpha=0.6)
    axs[i].set_ylabel('Fraction of Papers (%)')
    axs[i].set_ylim([0, max_fraction + 1])
plt.xlabel('Date', fontsize=12)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_evidence_with_bertopic_ACL_postprocessing_all_papers.csv')
df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%B')
df['Year'] = df['Date'].dt.year
total_papers_per_year = df.groupby('Year').size()
papers_per_topic_per_year = df.groupby(['Year', 'Topic']).size()
fraction_per_topic_per_year = papers_per_topic_per_year.div(total_papers_per_year, level='Year')
fraction_per_topic_per_year = fraction_per_topic_per_year.unstack(fill_value=0)
fraction_per_topic_per_year *= 100

cluster_names = {
    0: "Reasoning struggle",
    1: "Hallucination",
    2: "Social bias",
    3: "Privacy risks",
    4: "Compute constraints"
}
fraction_per_topic_per_year.columns = [cluster_names.get(int(i), f'Cluster {int(i)}') for i in fraction_per_topic_per_year.columns]
lighter_academic_colors = [
    "#B22222",
    "#27408B",
    "#006400",
    "#FFD700",
    "#FF8C00",
]
colors = lighter_academic_colors * (len(fraction_per_topic_per_year.columns) // len(lighter_academic_colors) + 1)
plt.figure(figsize=(30, 15))
ax = fraction_per_topic_per_year.plot(
    kind='bar',
    stacked=False,
    color=colors[:len(fraction_per_topic_per_year.columns)],
    width=0.85,
    edgecolor='black'
)
ax.set_title('Fraction of ACL Papers by LLM Limitations per Year', fontsize=20, pad=20, loc='center')
ax.set_xlabel('Year', fontsize=18)
ax.set_ylabel('Fraction of Papers (%)', fontsize=18)
ax.legend(title='Clusters', fontsize=12, title_fontsize=14, bbox_to_anchor=(1.05, 1), loc='upper left')
plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.1)
ax.grid(True, linestyle='--', alpha=0.6)
ax.tick_params(axis='x', rotation=45, labelsize=14)
ax.tick_params(axis='y', labelsize=14)
ax.set_yticklabels([f'{int(tick)}' for tick in ax.get_yticks()])
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_evidence_with_bertopic_ACL_postprocessing_all_papers.csv')
df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%B')
cluster_names = {
    0: "Reasoning struggle",
    1: "Hallucination",
    2: "Social bias",
    3: "Privacy risks",
    4: "Compute constraints"
}
total_papers_per_date = df.groupby('Date').size()
papers_per_topic_per_date = df.groupby(['Date', 'Topic']).size()
fraction_per_topic_per_date = papers_per_topic_per_date.div(total_papers_per_date, level='Date')
fraction_per_topic_per_date = fraction_per_topic_per_date.unstack(fill_value=0)
plt.figure(figsize=(12, 8))
colors = plt.cm.get_cmap('tab10', len(fraction_per_topic_per_date.columns))

for i, topic in enumerate(fraction_per_topic_per_date.columns):
    line = plt.plot(fraction_per_topic_per_date.index, fraction_per_topic_per_date[topic], label=f'{cluster_names.get(topic, f"Cluster {topic}")}',
                    color=colors(i), marker='o', linewidth=2)

    for date, value in fraction_per_topic_per_date[topic].items():
        if value != 0:
            source_text = df[(df['Date'] == date) & (df['Topic'] == topic)]['Source'].unique()
            source_text = ', '.join(source_text)
            plt.annotate(source_text, (date, value), textcoords="offset points", xytext=(0,10), ha='center')
plt.title('Fraction of ACL Papers by LLM Limitations Over Time', fontsize=14, weight='bold')
plt.xlabel('Date', fontsize=12)
plt.ylabel('Fraction of Papers', fontsize=12)
plt.legend(title='Clusters', title_fontsize='13', fontsize='11', loc='upper left')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

"""Smoother Trendline of Clustering"""

import statsmodels.api as sm
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd

plt.figure(figsize=(12, 7))
colors = plt.cm.get_cmap('tab10', len(fraction_per_topic_per_date.columns))

# Loop through each topic and apply LOESS smoothing
for i, topic in enumerate(fraction_per_topic_per_date.columns):
    # Apply LOESS smoothing using the original datetime index (not Julian dates)
    loess_model = sm.nonparametric.lowess(
        fraction_per_topic_per_date[topic],
        fraction_per_topic_per_date.index.astype(np.int64) // 10**9,
        frac=0.5
    )
    x_vals, y_vals = zip(*loess_model)
    x_vals = pd.to_datetime(x_vals, unit='s')
    y_vals = np.array(y_vals) * 1
    plt.plot(x_vals, y_vals, label=f'{cluster_names.get(topic, f"Cluster {topic}")}', color=colors(i), linewidth=2.5)
plt.title('Fraction of ACL Papers by LLM Limitations per Year (LOESS)', fontsize=16, weight='bold')
plt.xlabel('Date', fontsize=14)
plt.ylabel('Fraction of Papers (%)', fontsize=14)
plt.legend(title='Clusters', title_fontsize='14', fontsize='12', loc='upper left', frameon=True, shadow=True)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
plt.tight_layout()
plt.show()

"""### Final Resutls of Clustering the Limitations of LLMs for ArXiv Academic Papers Utilizing LLM-Based-Embeddings and HDBSCAN BERTopic Model."""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import spacy
import re
from nltk.corpus import stopwords
import nltk
from bertopic import BERTopic
import openai
import umap
from sklearn.feature_extraction.text import TfidfVectorizer
import hdbscan

nltk.download('stopwords')
client = openai.OpenAI(api_key='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')
#openai.api_key = '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'
df = pd.read_excel('arxiv_2.xlsx')
nlp = spacy.load("en_core_web_sm")
nltk_stopwords = set(stopwords.words('english'))

custom_stopwords = {'model', 'language', 'large', 'task', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge', 'gpt'}
s
all_stopwords = nltk_stopwords.union(custom_stopwords)

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower().strip()
    doc = nlp(text)
    processed_tokens = []
    for token in doc:
        if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
            processed_tokens.append(token.lemma_)
    return ' '.join(processed_tokens)

df['Evidence'] = df['Evidence'].apply(preprocess_text)

def get_embeddings(texts):
    try:
        response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
        return np.array([data.embedding for data in response.data])
    except Exception as e:
        print("Error fetching embeddings:", e)
        return np.array([])

embeddings = get_embeddings(df['Evidence'].tolist())

if embeddings.size > 0:
    umap_model = umap.UMAP(n_neighbors=10, n_components=5, metric='cosine', min_dist=0.0)
    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric='euclidean', cluster_selection_method='eom')
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
    topic_model = BERTopic(umap_model=umap_model,
                           hdbscan_model=hdbscan_model,
                           vectorizer_model=vectorizer,
                           min_topic_size=5)

    topics, _ = topic_model.fit_transform(df['Evidence'], embeddings)
    topic_info = topic_model.get_topic_info()
    topic_info = topic_info[topic_info.Topic != -1]
    def generate_cluster_name(cluster_terms):
        prompt = f"Given the following key terms: {', '.join(cluster_terms)}, generate a concise limitation with 2 words at most that summarizes the descriptive name for this cluster of topics focusing on the LIMITATIONS of large language models."

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant specialized in summarizing limitations with 2 words at most."},
                    {"role": "user", "content": prompt}
                ]
            )
            if response and response.choices:
                return response.choices[0].message.content.strip()
            else:
                return "No description available"
        except Exception as e:
            print(f"Failed to generate cluster name: {str(e)}")
            return "Error generating name"
    topic_info['Descriptive_Name'] = topic_info['Representation'].apply(lambda terms: generate_cluster_name(terms))
    print(topic_info[['Topic', 'Descriptive_Name', 'Count', 'Representation']])

    df['Topic'] = topics
    df = df[df.Topic != -1]
    df.to_csv('clustered_evidence_arxiv.csv', index=False)
else:
    print("No embeddings were generated, please check the error logs.")

"""**Batch Processing of LLM Limitations with Topic Modeling and Descriptive Naming**

The notebook handles large datasets by processing entries in batches, ensuring efficient use of resources.

> Add blockquote



1. Preprocessing Text: Cleans and prepares text data by removing special characters, numbers, and common stopwords.
2. Embedding Generation: Uses OpenAI's API to convert preprocessed text into vector embeddings, enabling numerical analysis.
3. Dimensionality Reduction and Clustering: Applies UMAP for dimensionality reduction and HDBSCAN for clustering to discover inherent groupings in the data based on LLM limitations.
4. Topic Modeling: Utilizes BERTopic to identify and characterize topics within clusters, revealing prevalent themes in LLM limitations.
5. Descriptive Naming: Leverages GPT-3.5 to generate concise, descriptive names for each identified topic, enhancing interpretability of the results.
"""

import pandas as pd
import numpy as np
import spacy
import re
from nltk.corpus import stopwords
import nltk
from bertopic import BERTopic
import openai
import umap
from sklearn.feature_extraction.text import TfidfVectorizer
import hdbscan

nltk.download('stopwords')

client = openai.OpenAI(api_key='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')
df = pd.read_excel('arXiv_only_4-5_rated_papers_final.xlsx')
nlp = spacy.load("en_core_web_sm")
nltk_stopwords = set(stopwords.words('english'))

custom_stopwords = {'model', 'language', 'large', 'task', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge', 'gpt'}
all_stopwords = nltk_stopwords.union(custom_stopwords)

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower().strip()
    doc = nlp(text)
    processed_tokens = []
    for token in doc:
        if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
            processed_tokens.append(token.lemma_)
    return ' '.join(processed_tokens)

def get_embeddings(texts):
    try:
        response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
        return np.array([data.embedding for data in response.data])
    except Exception as e:
        print("Error fetching embeddings:", e)
        return np.array([])


def process_in_batches(dataframe, batch_size):
    num_batches = len(dataframe) // batch_size + (1 if len(dataframe) % batch_size != 0 else 0)
    all_embeddings = []

    for i in range(num_batches):
        batch_df = dataframe[i*batch_size:(i+1)*batch_size]
        preprocessed_texts = batch_df['Evidence'].apply(preprocess_text).tolist()
        batch_embeddings = get_embeddings(preprocessed_texts)

        if len(batch_embeddings) != len(preprocessed_texts):
            print(f"Batch {i} processing error: Mismatch in processed embeddings count.")
            continue

        all_embeddings.append(batch_embeddings)

        np.save(f'embeddings_batch_{i}.npy', batch_embeddings)

    return np.vstack(all_embeddings)
batch_size = 500
embeddings = process_in_batches(df, batch_size)

if embeddings.size > 0:
    umap_model = umap.UMAP(n_neighbors=30, n_components=5, metric='cosine', min_dist=0.0)
    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=10, metric='euclidean', cluster_selection_method='eom')
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer, min_topic_size=5)

    topics, _ = topic_model.fit_transform(df['Evidence'].apply(preprocess_text).tolist(), embeddings)
    topic_info = topic_model.get_topic_info()
    topic_info = topic_info[topic_info.Topic != -1]
    def generate_cluster_name(cluster_terms):
        prompt = f"Given the following key terms: {', '.join(cluster_terms)}, generate a concise limitation with 2 words at most that summarizes the descriptive name for this cluster of topics focusing on the LIMITATIONS of large language models."

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant specialized in summarizing limitations with 2 words at most."},
                    {"role": "user", "content": prompt}
                ]
            )
            if response and response.choices:
                return response.choices[0].message.content.strip()
            else:
                return "No description available"
        except Exception as e:
            print(f"Failed to generate cluster name: {str(e)}")
            return "Error generating name"

    topic_info['Descriptive_Name'] = topic_info['Representation'].apply(generate_cluster_name)
    print(topic_info[['Topic', 'Descriptive_Name', 'Count', 'Representation']])

    df['Topic'] = topics
    df = df[df.Topic != -1]
    df.to_csv('clustered_evidence_arxiv_5.csv', index=False)
else:
    print("No embeddings were generated, please check the error logs.")

"""running in several rounds due to high NA vlaues  """

# import pandas as pd
# import numpy as np
# import spacy
# import re
# from nltk.corpus import stopwords
# import nltk
# from bertopic import BERTopic
# import openai
# import umap
# from sklearn.feature_extraction.text import TfidfVectorizer
# import hdbscan

# nltk.download('stopwords')

# client = openai.OpenAI(api_key='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')
# df = pd.read_excel('arXiv_only_4-5_rated_papers_final.xlsx')
# nlp = spacy.load("en_core_web_sm")
# nltk_stopwords = set(stopwords.words('english'))

# custom_stopwords = {'model', 'language', 'large', 'task', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge', 'gpt'}

# all_stopwords = nltk_stopwords.union(custom_stopwords)

# def preprocess_text(text):
#     text = re.sub(r'\s+', ' ', text)
#     text = re.sub(r'\d+', '', text)
#     text = re.sub(r'[^\w\s]', '', text)
#     text = text.lower().strip()
#     doc = nlp(text)
#     processed_tokens = []
#     for token in doc:
#         if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
#             processed_tokens.append(token.lemma_)
#     return ' '.join(processed_tokens)

# def get_embeddings(texts):
#     try:
#         response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
#         return np.array([data.embedding for data in response.data])
#     except Exception as e:
#         print("Error fetching embeddings:", e)
#         return np.array([])


# def process_in_batches(dataframe, batch_size):
#     num_batches = len(dataframe) // batch_size + (1 if len(dataframe) % batch_size != 0 else 0)
#     all_embeddings = []

#     for i in range(num_batches):
#         batch_df = dataframe[i*batch_size:(i+1)*batch_size]
#         preprocessed_texts = batch_df['Evidence'].apply(preprocess_text).tolist()
#         batch_embeddings = get_embeddings(preprocessed_texts)

#         if len(batch_embeddings) != len(preprocessed_texts):
#             print(f"Batch {i} processing error: Mismatch in processed embeddings count.")
#             continue

#         all_embeddings.append(batch_embeddings)

#         np.save(f'embeddings_batch_{i}.npy', batch_embeddings)

#     return np.vstack(all_embeddings)

# batch_size = 500
# embeddings = process_in_batches(df, batch_size)

# if embeddings.size > 0:
#     umap_model = umap.UMAP(n_neighbors=25, n_components=5, metric='cosine', min_dist=0.0)
#     hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=5, metric='euclidean', cluster_selection_method='eom')
#     vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
#     topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer, min_topic_size=5)

#     topics, _ = topic_model.fit_transform(df['Evidence'].apply(preprocess_text).tolist(), embeddings)
#     topic_info = topic_model.get_topic_info()
#     topic_info = topic_info[topic_info.Topic != -1]
#     def generate_cluster_name(cluster_terms):
#         prompt = f"Given the following key terms: {', '.join(cluster_terms)}, generate a concise limitation with 2 words at most that summarizes the descriptive name for this cluster of topics focusing on the LIMITATIONS of large language models."

#         try:
#             response = client.chat.completions.create(
#                 model="gpt-3.5-turbo",
#                 messages=[
#                     {"role": "system", "content": "You are a helpful assistant specialized in summarizing limitations with 2 words at most."},
#                     {"role": "user", "content": prompt}
#                 ]
#             )
#             if response and response.choices:
#                 return response.choices[0].message.content.strip()
#             else:
#                 return "No description available"
#         except Exception as e:
#             print(f"Failed to generate cluster name: {str(e)}")
#             return "Error generating name"

#     topic_info['Descriptive_Name'] = topic_info['Representation'].apply(generate_cluster_name)
#     print(topic_info[['Topic', 'Descriptive_Name', 'Count', 'Representation']])

#     df['Topic'] = topics
#     df = df[df.Topic != -1]
#     df.to_csv('clustered_evidence_arxiv_7.csv', index=False)
# else:
#     print("No embeddings were generated, please check the error logs.")

# import pandas as pd
# import numpy as np
# import spacy
# import re
# from nltk.corpus import stopwords
# import nltk
# from bertopic import BERTopic
# import openai
# import umap
# from sklearn.feature_extraction.text import TfidfVectorizer
# import hdbscan

# nltk.download('stopwords')

# client = openai.OpenAI(api_key='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')
# df = pd.read_excel('arXiv_only_4-5_rated_papers_final_Copy_2.xlsx')
# nlp = spacy.load("en_core_web_sm")
# nltk_stopwords = set(stopwords.words('english'))

# custom_stopwords = {'model', 'language', 'large', 'task', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge', 'gpt'}
# all_stopwords = nltk_stopwords.union(custom_stopwords)

# def preprocess_text(text):
#     text = re.sub(r'\s+', ' ', text)
#     text = re.sub(r'\d+', '', text)
#     text = re.sub(r'[^\w\s]', '', text)
#     text = text.lower().strip()
#     doc = nlp(text)
#     processed_tokens = []
#     for token in doc:
#         if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
#             processed_tokens.append(token.lemma_)
#     return ' '.join(processed_tokens)

# def get_embeddings(texts):
#     try:
#         response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
#         return np.array([data.embedding for data in response.data])
#     except Exception as e:
#         print("Error fetching embeddings:", e)
#         return np.array([])


# def process_in_batches(dataframe, batch_size):
#     num_batches = len(dataframe) // batch_size + (1 if len(dataframe) % batch_size != 0 else 0)
#     all_embeddings = []

#     for i in range(num_batches):
#         batch_df = dataframe[i*batch_size:(i+1)*batch_size]
#         preprocessed_texts = batch_df['Evidence'].apply(preprocess_text).tolist()
#         batch_embeddings = get_embeddings(preprocessed_texts)

#         if len(batch_embeddings) != len(preprocessed_texts):
#             print(f"Batch {i} processing error: Mismatch in processed embeddings count.")
#             continue

#         all_embeddings.append(batch_embeddings)

#         np.save(f'embeddings_batch_{i}.npy', batch_embeddings)

#     return np.vstack(all_embeddings)
# batch_size = 500
# embeddings = process_in_batches(df, batch_size)

# if embeddings.size > 0:
#     umap_model = umap.UMAP(n_neighbors=20, n_components=5, metric='cosine', min_dist=0.1)
#     hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric='euclidean', cluster_selection_method='eom')
#     vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
#     topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer, min_topic_size=3)

#     topics, _ = topic_model.fit_transform(df['Evidence'].apply(preprocess_text).tolist(), embeddings)
#     topic_info = topic_model.get_topic_info()
#     topic_info = topic_info[topic_info.Topic != -1]
#     def generate_cluster_name(cluster_terms):
#         prompt = f"Given the following key terms: {', '.join(cluster_terms)}, generate a concise limitation with 2 words at most that summarizes the descriptive name for this cluster of topics focusing on the LIMITATIONS of large language models."

#         try:
#             response = client.chat.completions.create(
#                 model="gpt-3.5-turbo",
#                 messages=[
#                     {"role": "system", "content": "You are a helpful assistant specialized in summarizing limitations with 2 words at most."},
#                     {"role": "user", "content": prompt}
#                 ]
#             )
#             if response and response.choices:
#                 return response.choices[0].message.content.strip()
#             else:
#                 return "No description available"
#         except Exception as e:
#             print(f"Failed to generate cluster name: {str(e)}")
#             return "Error generating name"

#     topic_info['Descriptive_Name'] = topic_info['Representation'].apply(generate_cluster_name)
#     print(topic_info[['Topic', 'Descriptive_Name', 'Count', 'Representation']])

#     df['Topic'] = topics
#     df = df[df.Topic != -1]
#     df.to_csv('clustered_evidence_arxiv_5_part_4.csv', index=False)
# else:
#     print("No embeddings were generated, please check the error logs.")

# import pandas as pd
# import numpy as np
# import spacy
# import re
# from nltk.corpus import stopwords
# import nltk
# from bertopic import BERTopic
# import openai
# import umap
# from sklearn.feature_extraction.text import TfidfVectorizer
# import hdbscan

# nltk.download('stopwords')

# client = openai.OpenAI(api_key='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')
# df = pd.read_excel('arXiv_only_4-5_rated_papers_final_Copy_2.xlsx')
# nlp = spacy.load("en_core_web_sm")
# nltk_stopwords = set(stopwords.words('english'))

# custom_stopwords = {'model', 'language', 'large', 'task', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge', 'gpt'}
# all_stopwords = nltk_stopwords.union(custom_stopwords)

# def preprocess_text(text):
#     text = re.sub(r'\s+', ' ', text)
#     text = re.sub(r'\d+', '', text)
#     text = re.sub(r'[^\w\s]', '', text)
#     text = text.lower().strip()
#     doc = nlp(text)
#     processed_tokens = []
#     for token in doc:
#         if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
#             processed_tokens.append(token.lemma_)
#     return ' '.join(processed_tokens)

# def get_embeddings(texts):
#     try:
#         response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
#         return np.array([data.embedding for data in response.data])
#     except Exception as e:
#         print("Error fetching embeddings:", e)
#         return np.array([])


# def process_in_batches(dataframe, batch_size):
#     num_batches = len(dataframe) // batch_size + (1 if len(dataframe) % batch_size != 0 else 0)
#     all_embeddings = []

#     for i in range(num_batches):
#         batch_df = dataframe[i*batch_size:(i+1)*batch_size]
#         preprocessed_texts = batch_df['Evidence'].apply(preprocess_text).tolist()
#         batch_embeddings = get_embeddings(preprocessed_texts)

#         if len(batch_embeddings) != len(preprocessed_texts):
#             print(f"Batch {i} processing error: Mismatch in processed embeddings count.")
#             continue

#         all_embeddings.append(batch_embeddings)

#         np.save(f'embeddings_batch_{i}.npy', batch_embeddings)

#     return np.vstack(all_embeddings)
# batch_size = 500
# embeddings = process_in_batches(df, batch_size)

# if embeddings.size > 0:
#     umap_model = umap.UMAP(n_neighbors=20, n_components=5, metric='cosine', min_dist=0.1)
#     hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric='euclidean', cluster_selection_method='eom')
#     vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
#     topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer, min_topic_size=3)

#     topics, _ = topic_model.fit_transform(df['Evidence'].apply(preprocess_text).tolist(), embeddings)
#     topic_info = topic_model.get_topic_info()
#     topic_info = topic_info[topic_info.Topic != -1]
#     def generate_cluster_name(cluster_terms):
#         prompt = f"Given the following key terms: {', '.join(cluster_terms)}, generate a concise limitation with 2 words at most that summarizes the descriptive name for this cluster of topics focusing on the LIMITATIONS of large language models."

#         try:
#             response = client.chat.completions.create(
#                 model="gpt-3.5-turbo",
#                 messages=[
#                     {"role": "system", "content": "You are a helpful assistant specialized in summarizing limitations with 2 words at most."},
#                     {"role": "user", "content": prompt}
#                 ]
#             )
#             if response and response.choices:
#                 return response.choices[0].message.content.strip()
#             else:
#                 return "No description available"
#         except Exception as e:
#             print(f"Failed to generate cluster name: {str(e)}")
#             return "Error generating name"

#     topic_info['Descriptive_Name'] = topic_info['Representation'].apply(generate_cluster_name)
#     print(topic_info[['Topic', 'Descriptive_Name', 'Count', 'Representation']])

#     df['Topic'] = topics
#     df = df[df.Topic != -1]
#     df.to_csv('clustered_evidence_arxiv_5_part_5.csv', index=False)
# else:
#     print("No embeddings were generated, please check the error logs.")

""" Calculating the Fraction of **ArXiV** Papers within Each Limitations Cluster per Month"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_evidence_arxiv_5.csv', encoding='ISO-8859-1')
df['Date'] = pd.to_datetime(df['Date ']).dt.to_period('M')
total_papers_per_date = df.groupby('Date').size()
papers_per_topic_per_date = df.groupby(['Date', 'Topic']).size()
fraction_per_topic_per_date = papers_per_topic_per_date.div(total_papers_per_date, level='Date').unstack(fill_value=0)
window_size = 7
smoothed = fraction_per_topic_per_date.rolling(window=window_size, center=True).mean()
smoothed *= 10

cluster_names = [
    "Security risks", "Hallucinations", "Incomplete reasoning",
    "Bias limitations", "Catastrophic forgetting", "Multilingual performance constraints",
    "Compute constraints", "Domain adaptation", "Complex MLLM limitations",
    "Code generation", "Context dependency"
]

plt.figure(figsize=(14, 9))
colors = plt.cm.get_cmap('tab20', len(smoothed.columns))

for i, topic in enumerate(smoothed.columns):
    plt.plot(smoothed.index.astype(str), smoothed[topic], label=cluster_names[int(topic)], color=colors(i), linewidth=2)

plt.title('Fraction of ArXiv Papers by LLM Limitations Over Time - Monthly Based', fontsize=14, weight='bold')
plt.xlabel('Date', fontsize=12)
plt.ylabel('Fraction of Papers (%)', fontsize=12)
plt.legend(title='Clusters', title_fontsize='10', fontsize='9', loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)
plt.grid(True, linestyle='--', alpha=0.6)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('clustered_evidence_arxiv_5.csv', encoding='ISO-8859-1')
df['Year'] = pd.to_datetime(df['Date']).dt.year
total_papers_per_year = df.groupby('Year').size()
papers_per_topic_per_year = df.groupby(['Year', 'Topic']).size().unstack(fill_value=0)
fraction_per_topic_per_year = papers_per_topic_per_year.div(total_papers_per_year, axis=0) * 100  # Multiply by 100 to get percentages
cluster_names = [
    "Security risks", "Hallucinations", "Incomplete reasoning",
    "Bias limitations", "Catastrophic forgetting", "Multilingual performance constraints",
    "Compute constraints", "Domain adaptation", "Complex MLLM limitations",
    "Code generation", "Context dependency"
]
distinct_colors = [
    "#1f77b4",  # Blue
    "#ff7f0e",  # Orange
    "#2ca02c",  # Green
    "#d62728",  # Red
    "#9467bd",  # Purple
    "#8c564b",  # Brown
    "#e377c2",  # Pink
    "#7f7f7f",  # Gray
    "#bcbd22",  # Yellow-Green
    "#17becf",  # Cyan
    "#aec7e8"   # Light Blue
]
plt.figure(figsize=(30, 15))
ax = fraction_per_topic_per_year.plot(kind='bar', stacked=False, color=distinct_colors, width=0.85, edgecolor='white')
ax.set_title('Fraction of LLM Limitation Clusters Discussed per Year in ArXiV', fontsize=18, pad=20, loc='center')
ax.set_xlabel('Year', fontsize=14)
ax.set_ylabel('Fraction of Papers (%)', fontsize=16)
ax.legend(cluster_names, title='Clusters', fontsize=12, title_fontsize=14, bbox_to_anchor=(1.05, 1), loc='upper left')
plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.1)
plt.show()

"""Calculating the Fraction of the ArXiV papers within each Limitation Cluster per Quarter of the Years"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_evidence_arxiv_5.csv', encoding='ISO-8859-1')
df['Date'] = pd.to_datetime(df['Date'])
df['Quarter'] = df['Date'].dt.to_period('Q')
total_papers_per_quarter = df.groupby('Quarter').size()
papers_per_topic_per_quarter = df.groupby(['Quarter', 'Topic']).size()
fraction_per_topic_per_quarter = papers_per_topic_per_quarter.div(total_papers_per_quarter, level='Quarter').multiply(100).unstack(fill_value=0)
window_size = 2
smoothed_fractions = fraction_per_topic_per_quarter.rolling(window=window_size, min_periods=1, center=True).mean()

cluster_names = [
    "Security risks", "Hallucinations", "Incomplete reasoning",
    "Bias limitations", "Catastrophic forgetting", "Multilingual performance constraints",
    "Compute constraints", "Domain adaptation", "Complex MLLM limitations",
    "Code generation", "Context dependency"
]
plt.figure(figsize=(14, 9))
colors = plt.cm.get_cmap('tab20', len(smoothed_fractions.columns))

for i, topic in enumerate(smoothed_fractions.columns):
    if i < 3:
        linewidth = 4  # Thicker lines for the first three topics
        scatter_size = 60
    else:
        linewidth = 1.5
        scatter_size = 40
    plt.plot(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
             label=cluster_names[int(topic)], color=colors(i), linewidth=linewidth)
    plt.scatter(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
                color=colors(i), s=scatter_size, edgecolor='black', zorder=3)
plt.title('Smoothed Percentage of ArXiv Papers by LLM Limitations Over Quarters', fontsize=16, weight='bold')
plt.xlabel('Quarter', fontsize=14)
plt.ylabel('Percentage of Papers (%)', fontsize=14)
plt.legend(title='Clusters', title_fontsize='12', fontsize='10', loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)
plt.grid(True, linestyle='--', alpha=0.6)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_evidence_arxiv_5.csv', encoding='ISO-8859-1')
df['Date'] = pd.to_datetime(df['Date'])
df['Quarter'] = df['Date'].dt.to_period('Q')
total_papers_per_quarter = df.groupby('Quarter').size()
papers_per_topic_per_quarter = df.groupby(['Quarter', 'Topic']).size()
fraction_per_topic_per_quarter = papers_per_topic_per_quarter.div(total_papers_per_quarter, level='Quarter').unstack(fill_value=0)
window_size = 2 #smoothing
smoothed_fractions = fraction_per_topic_per_quarter.rolling(window=window_size, min_periods=1, center=True).mean()

smoothed_fractions *= 100

cluster_names = [
    "Security risks", "Hallucinations", "Incomplete reasoning",
    "Bias limitations", "Catastrophic forgetting", "Multilingual performance constraints",
    "Compute constraints", "Domain adaptation", "Complex MLLM limitations",
    "Code generation", "Context dependency"
]

fig, axs = plt.subplots(len(smoothed_fractions.columns), 1, figsize=(10, 20), sharex=True)
colors = plt.cm.get_cmap('tab20', len(smoothed_fractions.columns))
for i, topic in enumerate(smoothed_fractions.columns):
    axs[i].scatter(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
                   color=colors(i), s=40, label=cluster_names[int(topic)])
    axs[i].plot(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
                color=colors(i), linewidth=2, linestyle='-')
    axs[i].set_title(cluster_names[int(topic)], fontsize=16, weight='bold')
    axs[i].grid(True, linestyle='--', alpha=0.6)
    axs[i].set_ylabel('Fraction of Papers (%)')
    axs[i].set_ylim([0, 4])
plt.xlabel('Quarter', fontsize=14)
plt.tight_layout()
plt.show()
max_percentage = smoothed_fractions.max().max()
print("Maximum percentage value after adjustment:", max_percentage)

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker

df = pd.read_csv('clustered_evidence_arxiv_5.csv', encoding='ISO-8859-1')
df['Date'] = pd.to_datetime(df['Date'])
df['Half_Year'] = df['Date'].dt.year.astype(str) + ' H' + (df['Date'].dt.month > 6).astype(int).replace({0: '1', 1: '2'})
total_papers_per_half_year = df.groupby('Half_Year').size()
df = df[df['Topic'].isin(range(8))]
papers_per_topic_per_half_year = df.groupby(['Half_Year', 'Topic']).size()

fraction_per_topic_per_half_year = papers_per_topic_per_half_year.div(total_papers_per_half_year, level='Half_Year') * 100
smoothed_fractions = fraction_per_topic_per_half_year.unstack(fill_value=0).rolling(window=2, min_periods=1, center=True).mean()
fig, axs = plt.subplots(2, 4, figsize=(20, 10), sharey=True)
formatter = mticker.FormatStrFormatter('%.1f')
titles = ["Security risks", "Hallucinations", "Incomplete reasoning",
          "Bias limitations", "Catastrophic forgetting", "Multilingual performance constraints",
          "Compute constraints", "Domain adaptation"]
for i, ax in enumerate(axs.flat[:len(titles)]):
    ax.plot(smoothed_fractions.index, smoothed_fractions[i], label=titles[i], color=f'C{i}')
    ax.set_title(titles[i], fontsize=12)
    ax.yaxis.set_major_formatter(formatter)
    ax.set_xticks(smoothed_fractions.index)
    ax.set_xticklabels(smoothed_fractions.index, rotation=45)
    ax.set_ylim(0, 3)
    ax.grid(True)

for ax in axs.flat[len(titles):]:
    ax.set_visible(False)
fig.supxlabel('Half-Year', fontsize=12)
fig.supylabel('Fraction of Papers (%)', fontsize=14)
plt.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.1, wspace=0.25, hspace=0.4)
plt.show()

"""## Clustering LLMs Limitations using Few Shot Clustering by Utilizing GPT Model
- ACL Papers
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import openai
from tqdm import tqdm
import os
import time
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer


client = openai.OpenAI(api_key='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')
progress_file = 'progress.keyphrase.txt'
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])
    return text
def generate_keyphrase(evidence):
    evidence = preprocess_text(evidence)
    prompt = f"""
    I am trying to summarize limitations of Large Language Models (LLMs) mentioned in various pieces of evidence. To assist with this, for each given piece of evidence about an LLM, please generate a keyphrase that succinctly encapsulates its main limitation. Below are some examples followed by a new evidence for you to process:

    ### Example Evidences and Keyphrases:

    **Example 1:**
    Evidence: "however demonstrate plm encode range stereotypical societal bias lead concern fairness plm metric demonstrate popular plmbase metric exhibit significantly high social bias traditional metric sensitive attribute"
    Keyphrase: "Stereotypical biases"

    **Example 2:**
    Evidence: ""neural scale suffer poor temporal generalization capability pretraine static past year bad time emerge""
    Keyphrase: "Poor generalization"

    **Example 3:**
    Evidence: "still exhibit tendency hallucinate ie content support source document"
    Keyphrase: "Tendency to hallucinate"

   **Example 4:**
    Evidence: "Yet LLM often factually incorrect response give query since may inaccurate incomplete outdated may fail retrieve relevant give query may faithfully reflect retrieve text."
    Keyphrase: "Poor data generalization"

    Now, given the following evidence, please provide a corresponding keyphrase that captures its main limitation:

    Evidence: "{evidence}"

    Please provide a concise keyphrase summarizing its limitation regarding LLMs issues.
    """

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=100,
            n=1,
            stop=None,
            temperature=0.5
        )
        return response.choices[0].message.content.strip()
    except openai.APIError as e:
        print(f"Rate limit error: {e}")
        raise
    except Exception as e:
        print(f"Error during API call: {e}")
        return None

def save_progress(index):
    try:
        with open(progress_file, 'w') as file:
            file.write(str(index))
        print(f"Progress saved at index: {index}")
    except Exception as e:
        print(f"Error saving progress: {e}")

def get_last_processed_index():
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r') as file:
                return int(file.read().strip())
        except Exception as e:
            print(f"Error reading progress file: {e}")
            return -1
    return -1

csv_input_path = 'acl_evidences.xlsx'
csv_output_path = 'acl_keyphrases.csv'
df = pd.read_excel(csv_input_path)
if 'Keyphrase' not in df.columns:
    df['Keyphrase'] = None

last_index = get_last_processed_index()
print(f"Last processed index: {last_index}")
for index, row in tqdm(df.iterrows(), total=df.shape[0], initial=last_index+1, desc="Generating keyphrases"):
    if index <= last_index:
        continue

    evidence = row['Evidence']
    keyphrase = generate_keyphrase(evidence)
    if keyphrase is None:
        print(f"Skipping evidence at index {index} due to API call failure.")
        continue

    df.at[index, 'Keyphrase'] = keyphrase
    print(f"Evidence: {evidence}\nKeyphrase: {keyphrase}\n")

    save_progress(index)
    time.sleep(1)
df.to_csv(csv_output_path, index=False)
print("Keyphrase generation complete. Results have been saved.")

"""- ARXIV Papers"""

import pandas as pd
import openai
from tqdm import tqdm
import os
import time
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer


client = openai.OpenAI(api_key='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')
progress_file = 'progress.keyphrase.txt'
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()  # Lowercase the text
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    # Remove stopwords and lemmatize
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])
    return text
def generate_keyphrase(evidence):
    evidence = preprocess_text(evidence)
    prompt = f"""
    I am trying to summarize limitations of Large Language Models (LLMs) mentioned in various pieces of evidence. To assist with this, for each given piece of evidence about an LLM, please generate a keyphrase that succinctly encapsulates its main limitation. Below are some examples followed by a new evidence for you to process:

    ### Example Evidences and Keyphrases:

    **Example 1:**
    Evidence: "however demonstrate plm encode range stereotypical societal bias lead concern fairness plm metric demonstrate popular plmbase metric exhibit significantly high social bias traditional metric sensitive attribute"
    Keyphrase: "Stereotypical biases"

    **Example 2:**
    Evidence: ""neural scale suffer poor temporal generalization capability pretraine static past year bad time emerge""
    Keyphrase: "Poor generalization"

    **Example 3:**
    Evidence: "still exhibit tendency hallucinate ie content support source document"
    Keyphrase: "Tendency to hallucinate"

   **Example 4:**
    Evidence: "Yet LLM often factually incorrect response give query since may inaccurate incomplete outdated may fail retrieve relevant give query may faithfully reflect retrieve text."
    Keyphrase: "Poor data generalization"

    Now, given the following evidence, please provide a corresponding keyphrase that captures its main limitation:

    Evidence: "{evidence}"

    Please provide a concise keyphrase summarizing its limitation regarding LLMs issues.
    """

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=100,
            n=1,
            stop=None,
            temperature=0.5
        )
        return response.choices[0].message.content.strip()
    except openai.APIError as e:
        print(f"Rate limit error: {e}")
        raise
    except Exception as e:
        print(f"Error during API call: {e}")
        return None

def save_progress(index):
    try:
        with open(progress_file, 'w') as file:
            file.write(str(index))
        print(f"Progress saved at index: {index}")
    except Exception as e:
        print(f"Error saving progress: {e}")

def get_last_processed_index():
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r') as file:
                return int(file.read().strip())
        except Exception as e:
            print(f"Error reading progress file: {e}")
            return -1
    return -1

csv_input_path = 'arxiv_evidences_part_5.xlsx'
csv_output_path = 'arxiv_keyphrases_part_5.csv'
df = pd.read_excel(csv_input_path)
if 'Keyphrase' not in df.columns:
    df['Keyphrase'] = None

last_index = get_last_processed_index()
print(f"Last processed index: {last_index}")
for index, row in tqdm(df.iterrows(), total=df.shape[0], initial=last_index+1, desc="Generating keyphrases"):
    if index <= last_index:
        continue

    evidence = row['Evidence']
    keyphrase = generate_keyphrase(evidence)
    if keyphrase is None:
        print(f"Skipping evidence at index {index} due to API call failure.")
        continue

    df.at[index, 'Keyphrase'] = keyphrase
    print(f"Evidence: {evidence}\nKeyphrase: {keyphrase}\n")

    save_progress(index)
    time.sleep(1)
df.to_csv(csv_output_path, index=False)
print("Keyphrase generation complete. Results have been saved.")



"""### Clustering the Keyphrase and Evidence Sets

- ACL Anthology Papers
"""

import pandas as pd

df = pd.read_csv('acl_keyphrases_final.csv')
if 'Evidence' not in df.columns or 'Keyphrase' not in df.columns:
    raise ValueError("The DataFrame must contain 'Evidence' and 'Keyphrase' columns.")
df['Evidence'] = df['Evidence'].astype(str) + " " + df['Keyphrase'].astype(str)
df.to_csv('acl_combined_final.csv', index=False)

print("The combined DataFrame has been saved to 'acl_combined_final.csv'.")

"""FINAL - ADDING CONSTRAINTS USING GPT 3.5 Turbo"""

import pandas as pd
import numpy as np
import spacy
import re
from nltk.corpus import stopwords
import nltk
from bertopic import BERTopic
import openai
import umap
from sklearn.feature_extraction.text import TfidfVectorizer
import hdbscan

nltk.download('stopwords')

client = openai.OpenAI(api_key='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')
df = pd.read_csv('acl_combined_final.csv')
nlp = spacy.load("en_core_web_sm")
nltk_stopwords = set(stopwords.words('english'))

# Custom stopwords
custom_stopwords = {'model', 'language', 'large', 'task', 'keyphrase', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge', 'gpt'}

# Combine it with the basic nltk stopwords
all_stopwords = nltk_stopwords.union(custom_stopwords)

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower().strip()
    doc = nlp(text)
    processed_tokens = []
    for token in doc:
        if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
            processed_tokens.append(token.lemma_)
    return ' '.join(processed_tokens)

def generate_pairwise_constraints(df, num_pairs=10):
    pairs = []
    for _ in range(num_pairs):
        idx1, idx2 = np.random.choice(len(df), 2, replace=False)
        text1 = df.iloc[idx1]['Evidence']
        text2 = df.iloc[idx2]['Evidence']

        prompt1 = f"""
        Consider the following two texts:
        Text 1: {text1}
        Text 2: {text2}
        I want to create some clusters of the crucial limitations regarding Large Language Models (LLMs). Should these two text evidence of limitations be placed in the same cluster of LLMs limitations? Answer 'yes' or 'no'.
        """

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt1}
                ],
                max_tokens=10,
                temperature=0.0
            )
            answer = response.choices[0].message.content.strip()
            same_cluster = True if answer == 'yes' else False
            pairs.append((idx1, idx2, same_cluster))
        except Exception as e:
            print(f"Error generating constraints: {e}")
    return pairs


df['Evidence'] = df['Evidence'].apply(preprocess_text)

def get_embeddings(texts):
    try:
        response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
        return np.array([data.embedding for data in response.data])
    except Exception as e:
        print("Error fetching embeddings:", e)
        return np.array([])

embeddings = get_embeddings(df['Evidence'].tolist())

def apply_constraints(df, clusters, constraints):
    cluster_map = {i: cluster for i, cluster in enumerate(clusters)}
    for idx1, idx2, same_cluster in constraints:
        if same_cluster:
            target_cluster = cluster_map[idx2]
            for idx in cluster_map:
                if cluster_map[idx] == cluster_map[idx1]:
                    cluster_map[idx] = target_cluster
        else:
            if cluster_map[idx1] == cluster_map[idx2]:
                cluster_map[idx1] = max(clusters) + 1
    return [cluster_map[i] for i in range(len(clusters))]

if embeddings.size > 0:
    umap_model = umap.UMAP(n_neighbors=10, n_components=5, metric='cosine', min_dist=0.0)

    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric='euclidean', cluster_selection_method='eom')
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
    topic_model = BERTopic(umap_model=umap_model,
                           hdbscan_model=hdbscan_model,
                           vectorizer_model=vectorizer,
                           min_topic_size=5)

    topics, _ = topic_model.fit_transform(df['Evidence'], embeddings)
    topic_info = topic_model.get_topic_info()
    topic_info = topic_info[topic_info.Topic != -1]
    def generate_cluster_name(cluster_terms):
        prompt = f"Given the following key terms: {', '.join(cluster_terms)}, generate a concise limitation with 2 words at most that summarizes the descriptive name for this cluster of topics focusing on the LIMITATIONS of large language models."

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant specialized in summarizing limitations with 2 words at most."},
                    {"role": "user", "content": prompt}
                ]
            )
            if response and response.choices:
                return response.choices[0].message.content.strip()
            else:
                return "No description available"
        except Exception as e:
            print(f"Failed to generate cluster name: {str(e)}")
            return "Error generating name"
    topic_info['Descriptive_Name'] = topic_info['Representation'].apply(lambda terms: generate_cluster_name(terms))
    print(topic_info[['Topic', 'Descriptive_Name', 'Count', 'Representation']])

    df['Topic'] = topics
    # apply constraints
    constraints = generate_pairwise_constraints(df, num_pairs=300)
    df['Topic'] = apply_constraints(df, df['Topic'].tolist(), constraints)

    df = df[df.Topic != -1]
    df.to_csv('clustered_few_shot_ACL_final.csv', index=False)
else:
    print("No embeddings were generated, please check the error logs.")

"""Visualizing the Clustering Timeline"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_few_shot_ACL_final_current.csv')
df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%B')
total_papers_per_date = df.groupby('Date').size()
papers_per_topic_per_date = df.groupby(['Date', 'Topic']).size()
fraction_per_topic_per_date = papers_per_topic_per_date.div(total_papers_per_date, level='Date').unstack(fill_value=0)
fraction_per_topic_per_date *= 100
max_fraction = fraction_per_topic_per_date.max().max()
cluster_names = {
    0: "Reasoning Struggle",
    1: "Bias Risk",
    2: "Hallucination tendency",
    3: "Memory Complexity",
    4: "Text Generation"
}
fig, axs = plt.subplots(len(fraction_per_topic_per_date.columns), 1, figsize=(10, 15), sharex=True)
colors = plt.cm.get_cmap('tab10', len(fraction_per_topic_per_date.columns))
for topic in fraction_per_topic_per_date.columns:
    dates = []
    fractions = []
    sources = []
    for date, group in df[df['Topic'] == topic].groupby('Date'):
        total = group.shape[0]
        dates.append(date)
        fractions.append((group.shape[0] / total_papers_per_date[date]) * 100)
        sources.append(group['Source'].iloc[0])
    ax = axs[int(topic)]
    ax.scatter(dates, fractions, color=colors(int(topic)), s=40, label=f'Cluster {int(topic)} - {cluster_names[int(topic)]}')
    ax.plot(dates, fractions, color=colors(int(topic)), linewidth=2, linestyle='-')
    for i, text in enumerate(sources):
        ax.annotate(text, (dates[i], fractions[i]), textcoords="offset points", xytext=(0,10), ha='center')
    ax.set_title(f'Cluster {int(topic)}: {cluster_names[int(topic)]}', fontsize=12, weight='bold')
    ax.grid(True, linestyle='--', alpha=0.6)
    ax.set_ylabel('Fraction of Papers (%)')
    ax.set_ylim([0, max_fraction + 0.01])
plt.xlabel('Date', fontsize=12)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('clustered_few_shot_ACL_final_current.csv')

df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%B')
df['Year'] = df['Date'].dt.year
total_papers_per_year = df.groupby('Year').size()
papers_per_topic_per_year = df.groupby(['Year', 'Topic']).size()
fraction_per_topic_per_year = papers_per_topic_per_year.div(total_papers_per_year, level='Year')
fraction_per_topic_per_year = fraction_per_topic_per_year.unstack(fill_value=0)
fraction_per_topic_per_year *= 100
cluster_names = {
    0: "Reasoning Struggle",
    1: "Bias Risk",
    2: "Hallucination tendency",
    3: "Memory Complexity",
    4: "Text Generation"
}
fraction_per_topic_per_year.columns = [cluster_names.get(int(i), f'Cluster {int(i)}') for i in fraction_per_topic_per_year.columns]
custom_colors = [
    "#003366",
    "#ff7f0e",  # Orange
    "#2ca02c",  # Green
    "#d62728",  # Red
    "#9467bd",  # Purple
    "#8c564b",  # Brown
    "#e377c2",  # Pink
]
colors = custom_colors * (len(fraction_per_topic_per_year.columns) // len(custom_colors) + 1)
plt.figure(figsize=(30, 15))
ax = fraction_per_topic_per_year.plot(kind='bar', stacked=False, color=colors[:len(fraction_per_topic_per_year.columns)], width=0.85, edgecolor='black')
ax.set_title('Fraction of ACL Papers by LLM Limitations per Year- 2', fontsize=20, pad=20, loc='center')
ax.set_xlabel('Year', fontsize=18)
ax.set_ylabel('Fraction of Papers (%)', fontsize=18)
ax.legend(title='Clusters', fontsize=12, title_fontsize=14, bbox_to_anchor=(1.05, 1), loc='upper left')
plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.1)
plt.show()

"""- ArXiV Papers"""

import pandas as pd

df = pd.read_excel('arxiv_keyphrases_final.xlsx')
if 'Evidence' not in df.columns or 'Keyphrase' not in df.columns:
    raise ValueError("The DataFrame must contain 'Evidence' and 'Keyphrase' columns.")
df['Combined_Text'] = df['Evidence'].astype(str) + " " + df['Keyphrase'].astype(str)
df.to_csv('arxiv_combined_final.csv', index=False)

print("The combined DataFrame has been saved to 'arxiv_combined_final.csv'.")

import pandas as pd
import numpy as np
import spacy
import re
from nltk.corpus import stopwords
import nltk
from bertopic import BERTopic
import openai
import umap
from sklearn.feature_extraction.text import TfidfVectorizer
import hdbscan

nltk.download('stopwords')

client = openai.OpenAI(api_key='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')
df = pd.read_csv('arxiv_combined_final.csv')
nlp = spacy.load("en_core_web_sm")
nltk_stopwords = set(stopwords.words('english'))

custom_stopwords = {'model', 'language', 'large', 'keyphrase', 'task', 'method', 'system', 'data', 'result', 'llm', 'output', 'generate', 'input', 'perform', 'answer', 'knowledge', 'gpt'}
all_stopwords = nltk_stopwords.union(custom_stopwords)

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower().strip()
    doc = nlp(text)
    processed_tokens = []
    for token in doc:
        if token.text not in all_stopwords and token.lemma_ not in all_stopwords:
            processed_tokens.append(token.lemma_)
    return ' '.join(processed_tokens)


def generate_pairwise_constraints(df, num_pairs=10):
    pairs = []
    for _ in range(num_pairs):
        idx1, idx2 = np.random.choice(len(df), 2, replace=False)
        text1 = df.iloc[idx1]['Combined_Text']
        text2 = df.iloc[idx2]['Combined_Text']

        prompt1 = f"""
        Consider the following two texts:
        Text 1: {text1}
        Text 2: {text2}
        I want to create some clusters of the crucial limitations regarding Large Language Models (LLMs). Should these two text evidence of limitations be placed in the same cluster of LLMs limitations? Answer 'yes' or 'no'.
        """

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt1}
                ],
                max_tokens=10,
                temperature=0.0
            )
            answer = response.choices[0].message.content.strip()
            same_cluster = True if answer == 'yes' else False
            pairs.append((idx1, idx2, same_cluster))
        except Exception as e:
            print(f"Error generating constraints: {e}")
    return pairs


def get_embeddings(texts):
    try:
        response = client.embeddings.create(input=texts, model="text-embedding-ada-002")
        return np.array([data.embedding for data in response.data])
    except Exception as e:
        print("Error fetching embeddings:", e)
        return np.array([])

def apply_constraints(df, clusters, constraints):
    cluster_map = {i: cluster for i, cluster in enumerate(clusters)}
    for idx1, idx2, same_cluster in constraints:
        if same_cluster:
            target_cluster = cluster_map[idx2]
            for idx in cluster_map:
                if cluster_map[idx] == cluster_map[idx1]:
                    cluster_map[idx] = target_cluster
        else:
            if cluster_map[idx1] == cluster_map[idx2]:
                cluster_map[idx1] = max(clusters) + 1
    return [cluster_map[i] for i in range(len(clusters))]

def process_in_batches(dataframe, batch_size):
    num_batches = len(dataframe) // batch_size + (1 if len(dataframe) % batch_size != 0 else 0)
    all_embeddings = []

    for i in range(num_batches):
        batch_df = dataframe[i*batch_size:(i+1)*batch_size]
        preprocessed_texts = batch_df['Combined_Text'].apply(preprocess_text).tolist()
        batch_embeddings = get_embeddings(preprocessed_texts)

        if len(batch_embeddings) != len(preprocessed_texts):
            print(f"Batch {i} processing error: Mismatch in processed embeddings count.")
            continue

        all_embeddings.append(batch_embeddings)

        np.save(f'embeddings_batch_{i}.npy', batch_embeddings)

    return np.vstack(all_embeddings)
batch_size = 500
embeddings = process_in_batches(df, batch_size)

if embeddings.size > 0:
    umap_model = umap.UMAP(n_neighbors=30, n_components=5, metric='cosine', min_dist=0.0)
    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=10, metric='euclidean', cluster_selection_method='eom')
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer, min_topic_size=5)

    topics, _ = topic_model.fit_transform(df['Combined_Text'].apply(preprocess_text).tolist(), embeddings)
    topic_info = topic_model.get_topic_info()
    topic_info = topic_info[topic_info.Topic != -1]
    def generate_cluster_name(cluster_terms):
        prompt = f"Given the following key terms: {', '.join(cluster_terms)}, generate a concise limitation with 2 words at most that summarizes the descriptive name for this cluster of topics focusing on the LIMITATIONS of large language models."

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant specialized in summarizing limitations with 2 words at most."},
                    {"role": "user", "content": prompt}
                ]
            )
            if response and response.choices:
                return response.choices[0].message.content.strip()
            else:
                return "No description available"
        except Exception as e:
            print(f"Failed to generate cluster name: {str(e)}")
            return "Error generating name"

    topic_info['Descriptive_Name'] = topic_info['Representation'].apply(generate_cluster_name)
    print(topic_info[['Topic', 'Descriptive_Name', 'Count', 'Representation']])

    df['Topic'] = topics
    # apply constraints
    constraints = generate_pairwise_constraints(df, num_pairs=3000)
    df['Topic'] = apply_constraints(df, df['Topic'].tolist(), constraints)

    df = df[df.Topic != -1]
    df.to_csv('clustered_few_shot_ARXIV_final.csv', index=False)
else:
    print("No embeddings were generated, please check the error logs.")

"""Visualizations of Few-Shot Clustering Timeline"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_few_shot_ARXIV_final_current.csv', encoding='ISO-8859-1')
df['Date'] = pd.to_datetime(df['Date '])
df['Quarter'] = df['Date'].dt.to_period('Q')
total_papers_per_quarter = df.groupby('Quarter').size()
papers_per_topic_per_quarter = df.groupby(['Quarter', 'Topic']).size()
fraction_per_topic_per_quarter = papers_per_topic_per_quarter.div(total_papers_per_quarter, level='Quarter').multiply(100).unstack(fill_value=0)
window_size = 2
smoothed_fractions = fraction_per_topic_per_quarter.rolling(window=window_size, min_periods=1, center=True).mean()
cluster_names = [
    "Hallucinations", "Reasoning limits", "Security risks",
    "Biased models", "Compute constraints", "Catastrophic forgetting",
    "Error Rate Challenges in Adaptive Contexts", "Code Performance Limit", "Privacy Risks"
]
plt.figure(figsize=(14, 9))
colors = plt.cm.get_cmap('tab20', len(smoothed_fractions.columns))

for i, topic in enumerate(smoothed_fractions.columns):
    if i < 3:
        linewidth = 4  # Thicker lines for the first three topics
        scatter_size = 60
    else:
        linewidth = 1.5
        scatter_size = 40
    plt.plot(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
             label=cluster_names[int(topic)], color=colors(i), linewidth=linewidth)
    plt.scatter(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
                color=colors(i), s=scatter_size, edgecolor='black', zorder=3)
plt.title('Smoothed Percentage of ArXiv Papers by LLM Limitations Over Quarters', fontsize=16, weight='bold')
plt.xlabel('Quarter', fontsize=14)
plt.ylabel('Percentage of Papers (%)', fontsize=14)
plt.legend(title='Clusters', title_fontsize='12', fontsize='10', loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)
plt.grid(True, linestyle='--', alpha=0.6)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('clustered_few_shot_ARXIV_final_current.csv', encoding='ISO-8859-1')
df['Date'] = pd.to_datetime(df['Date '])
df['Quarter'] = df['Date'].dt.to_period('Q')
total_papers_per_quarter = df.groupby('Quarter').size()
papers_per_topic_per_quarter = df.groupby(['Quarter', 'Topic']).size()
fraction_per_topic_per_quarter = papers_per_topic_per_quarter.div(total_papers_per_quarter, level='Quarter').unstack(fill_value=0)
window_size = 2
smoothed_fractions = fraction_per_topic_per_quarter.rolling(window=window_size, min_periods=1, center=True).mean()
smoothed_fractions *= 100

cluster_names = [
    "Hallucinations", "Reasoning limits", "Security risks",
    "Biased models", "Compute constraints", "Catastrophic forgetting",
    "Error Rate Challenges in Adaptive Contexts", "Code Performance Limit", "Privacy Risks"
]
fig, axs = plt.subplots(len(smoothed_fractions.columns), 1, figsize=(10, 20), sharex=True)
colors = plt.cm.get_cmap('tab20', len(smoothed_fractions.columns))

for i, topic in enumerate(smoothed_fractions.columns):
    axs[i].scatter(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
                   color=colors(i), s=40, label=cluster_names[int(topic)])
    axs[i].plot(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
                color=colors(i), linewidth=2, linestyle='-')
    axs[i].set_title(cluster_names[int(topic)], fontsize=12, weight='bold')
    axs[i].grid(True, linestyle='--', alpha=0.6)
    axs[i].set_ylabel('Fraction of Papers (%)')
    axs[i].set_ylim([0, 4])
plt.xlabel('Quarter', fontsize=12)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('clustered_few_shot_ARXIV_final_current.csv', encoding='ISO-8859-1')
df['Year'] = pd.to_datetime(df['Date ']).dt.year
total_papers_per_year = df.groupby('Year').size()
papers_per_topic_per_year = df.groupby(['Year', 'Topic']).size().unstack(fill_value=0)

fraction_per_topic_per_year = papers_per_topic_per_year.div(total_papers_per_year, axis=0) * 100
cluster_names = [
   "Hallucinations", "Reasoning limits", "Security risks",
    "Biased models", "Compute constraints", "Catastrophic forgetting",
    "Error Rate Challenges in Adaptive Contexts", "Code Performance Limit", "Privacy Risks"
]
distinct_colors = [
    "#1f77b4",  # Blue
    "#ff7f0e",  # Orange
    "#2ca02c",  # Green
    "#d62728",  # Red
    "#9467bd",  # Purple
    "#8c564b",  # Brown
    "#e377c2",  # Pink
    "#7f7f7f",  # Gray
    "#bcbd22",  # Yellow-Green
    "#17becf",  # Cyan
    "#aec7e8"   # Light Blue
]
plt.figure(figsize=(30, 15))
ax = fraction_per_topic_per_year.plot(kind='bar', stacked=False, color=distinct_colors[:len(cluster_names)], width=0.85, edgecolor='white')
ax.set_title('Fraction of LLM Limitation Clusters Discussed per Year in ArXiV-2', fontsize=16, pad=20, loc='center')
ax.set_xlabel('Year', fontsize=14)
ax.set_ylabel('Fraction of Papers (%)', fontsize=14)
ax.legend(cluster_names, title='Clusters', fontsize=12, title_fontsize=14, bbox_to_anchor=(1.05, 1), loc='upper left')
plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.1)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

df = pd.read_csv('clustered_few_shot_ARXIV_final_current.csv', encoding='ISO-8859-1')
df['Date'] = pd.to_datetime(df['Date '].str.strip(), errors='coerce')

df = df[df['Date'].notna() & (df['Date'].dt.year != 1970)]
df['Half_Year'] = df['Date'].dt.to_period('6M')
total_papers_per_half_year = df.groupby('Half_Year').size()
papers_per_topic_per_half_year = df.groupby(['Half_Year', 'Topic']).size()
fraction_per_topic_per_half_year = papers_per_topic_per_half_year.div(total_papers_per_half_year, level='Half_Year').unstack(fill_value=0)

window_size = 2
smoothed_fractions = fraction_per_topic_per_half_year.rolling(window=window_size, min_periods=1, center=True).mean()

cluster_names = [
    "Hallucinations", "Reasoning limits", "Security risks",
    "Biased models", "Compute constraints", "Catastrophic forgetting",
    "Error Rate Challenges in Adaptive Contexts", "Code Performance Limit", "Privacy Risks"
]

fig, axs = plt.subplots(len(smoothed_fractions.columns), 1, figsize=(10, 30), sharex=True)
colors = plt.cm.get_cmap('tab20', len(smoothed_fractions.columns))

for i, topic in enumerate(smoothed_fractions.columns):
    smoothed_fractions[topic] = smoothed_fractions[topic] * 100

    axs[i].scatter(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
                   color=colors(i), s=40, label=cluster_names[int(topic)])
    axs[i].plot(smoothed_fractions.index.astype(str), smoothed_fractions[topic],
                color=colors(i), linewidth=2, linestyle='-')
    axs[i].set_title(cluster_names[int(topic)], fontsize=12, weight='bold')
    axs[i].grid(True, linestyle='--', alpha=0.6)
    axs[i].set_ylabel('Fraction of Papers (%)')
    axs[i].set_ylim([0, 4])
axs[-1].xaxis.set_major_locator(plt.NullLocator())
axs[-1].xaxis.set_major_formatter(plt.NullFormatter())
plt.xlabel('Year: 2022-2024', fontsize=12)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker

df = pd.read_csv('clustered_few_shot_ARXIV_final_current.csv', encoding='ISO-8859-1')
df['Date'] = pd.to_datetime(df['Date '])
df['Half_Year'] = df['Date'].dt.year.astype(str) + ' H' + (df['Date'].dt.month > 6).astype(int).replace({0: '1', 1: '2'})
total_papers_per_half_year = df.groupby('Half_Year').size()
df = df[df['Topic'].isin(range(8))]
papers_per_topic_per_half_year = df.groupby(['Half_Year', 'Topic']).size()
fraction_per_topic_per_half_year = papers_per_topic_per_half_year.div(total_papers_per_half_year, level='Half_Year') * 100
smoothed_fractions = fraction_per_topic_per_half_year.unstack(fill_value=0).rolling(window=2, min_periods=1, center=True).mean()

fig, axs = plt.subplots(2, 4, figsize=(20, 10), sharey=True)
formatter = mticker.FormatStrFormatter('%.1f')
titles = ["Hallucinations", "Reasoning limits", "Security risks",
    "Biased models", "Compute constraints", "Catastrophic forgetting",
    "Error Rate Challenges in Adaptive Contexts", "Code Performance Limit"]

for i, ax in enumerate(axs.flat[:len(titles)]):
    ax.plot(smoothed_fractions.index, smoothed_fractions[i], label=titles[i], color=f'C{i}')
    ax.set_title(titles[i], fontsize=12)
    ax.yaxis.set_major_formatter(formatter)
    ax.set_xticks(smoothed_fractions.index)
    ax.set_xticklabels(smoothed_fractions.index, rotation=45)
    ax.set_ylim(0, 3)
    ax.grid(True)

for ax in axs.flat[len(titles):]:
    ax.set_visible(False)
fig.supxlabel('Half-Year', fontsize=12)
fig.supylabel('Fraction of Papers (%)', fontsize=14)
plt.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.1, wspace=0.25, hspace=0.4)
plt.show()