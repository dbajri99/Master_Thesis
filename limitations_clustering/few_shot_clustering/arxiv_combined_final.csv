Source ,Title,Talks about LLMs,Rate,Evidence,Year ,Date ,Keyphrase,Combined_Text
arXIv2022,Knowledge Distillation of Transformer-based Language Models Revisited,Yes.,5,"""However, the large model size and high run-time latency are serious impediments to applying them in practice, especially on mobile phones and Internet of Things (IoT) devices.""",2022,2022-06-29T02:16:56Z,"Keyphrase: ""High runtime latency""","""However, the large model size and high run-time latency are serious impediments to applying them in practice, especially on mobile phones and Internet of Things (IoT) devices."" Keyphrase: ""High runtime latency"""
arXIv2022,CC-Riddle: A Question Answering Dataset of Chinese Character Riddles,Yes.,5,"""The test results reveal that current language models still struggle to solve Chinese character riddles.""",2022,2022-06-28T06:23:13Z,"Keyphrase: ""Struggles with solving specific language challenges""","""The test results reveal that current language models still struggle to solve Chinese character riddles."" Keyphrase: ""Struggles with solving specific language challenges"""
arXIv2022,A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages,Yes.,4,"""concerns remain whether open-ended languages or text generated from these models reveal any biases toward a specific group of people, thereby risking the usability of a certain product.""",2022,2022-06-23T21:57:08Z,"Keyphrase: ""Bias towards specific groups""","""concerns remain whether open-ended languages or text generated from these models reveal any biases toward a specific group of people, thereby risking the usability of a certain product."" Keyphrase: ""Bias towards specific groups"""
arXIv2022,BERT Rankers are Brittle: a Study using Adversarial Document Perturbations,Yes.,5,"""we argue that BERT-rankers are not immune to adversarial attacks targeting retrieved documents given a query,"" and ""we find that BERT-rankers heavily rely on the document start/head for relevance prediction, making the initial part of the document more susceptible to adversarial attacks.""",2022,2022-06-23T14:16:48Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""we argue that BERT-rankers are not immune to adversarial attacks targeting retrieved documents given a query,"" and ""we find that BERT-rankers heavily rely on the document start/head for relevance prediction, making the initial part of the document more susceptible to adversarial attacks."" Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2022,Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models,Yes.,4,"""This paper presents exploratory work on whether and to what extent biases against queer and trans people are encoded in large language models (LLMs) such as BERT."" and ""We found that BERT shows significant homophobic bias.""",2022,2022-06-23T05:30:47Z,"Keyphrase: ""Homophobic bias""","""This paper presents exploratory work on whether and to what extent biases against queer and trans people are encoded in large language models (LLMs) such as BERT."" and ""We found that BERT shows significant homophobic bias."" Keyphrase: ""Homophobic bias"""
arXIv2022,Don't Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information,Yes.,4,"""We aim to diminish the stereotypical bias in the representations while preserving the factual gender signal."" and ""Our filtering method shows that it is possible to decrease the bias of gender-neutral profession names without significant deterioration of language modeling capabilities.""",2022,2022-06-21T21:38:25Z,"Keyphrase: ""Bias reduction at the expense of language modeling capability""","""We aim to diminish the stereotypical bias in the representations while preserving the factual gender signal."" and ""Our filtering method shows that it is possible to decrease the bias of gender-neutral profession names without significant deterioration of language modeling capabilities."" Keyphrase: ""Bias reduction at the expense of language modeling capability"""
arXIv2022,Using cognitive psychology to understand GPT-3,Yes.,5,"""Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task.""",2022,2022-06-21T20:06:03Z,"Keyphrase: ""Failure in causal reasoning""","""Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task."" Keyphrase: ""Failure in causal reasoning"""
arXIv2022,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,Yes.,5,"""Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models.""",2022,2022-06-21T16:15:27Z,"Keyphrase: ""Limited plan generation capabilities""","""Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models."" Keyphrase: ""Limited plan generation capabilities"""
arXIv2022,"Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",Yes.,4,"""we examine the connection between model size and its gender bias (specifically, occupational gender bias)."" and ""Our findings highlight the potential risks that can arise from increasing model size.""",2022,2022-06-20T15:52:40Z,"Keyphrase: ""Gender bias amplification""","""we examine the connection between model size and its gender bias (specifically, occupational gender bias)."" and ""Our findings highlight the potential risks that can arise from increasing model size."" Keyphrase: ""Gender bias amplification"""
arXIv2022,Methods for Estimating and Improving Robustness of Language Models,Yes.,5,"""large language models (LLMs) suffer notorious flaws related to their preference for simple, surface-level textual relations over full semantic complexity of the problem"" and ""weak ability to generalise outside of the training domain.""",2022,2022-06-16T21:02:53Z,"Keyphrase: ""Weak generalization outside training domain""","""large language models (LLMs) suffer notorious flaws related to their preference for simple, surface-level textual relations over full semantic complexity of the problem"" and ""weak ability to generalise outside of the training domain."" Keyphrase: ""Weak generalization outside training domain"""
arXIv2022,Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,Yes.,5,"""However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful.""",2022,2022-06-16T17:28:01Z,"Keyphrase: ""Toxic, biased, and untruthful language""","""However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful."" Keyphrase: ""Toxic, biased, and untruthful language"""
arXIv2022,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,Yes.,5,"""it is vital that we understand the present and near-future capabilities and limitations of language models"" and ""BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models"" and ""model performance and calibration both improve with scale, but are poor in absolute terms"" and ""social bias typically increases with scale in settings with ambiguous context, but this can be",2022,2022-06-09T17:05:34Z,"Keyphrase: ""Limited performance calibration""","""it is vital that we understand the present and near-future capabilities and limitations of language models"" and ""BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models"" and ""model performance and calibration both improve with scale, but are poor in absolute terms"" and ""social bias typically increases with scale in settings with ambiguous context, but this can be Keyphrase: ""Limited performance calibration"""
arXIv2022,Neuro-Symbolic Procedural Planning with Commonsense Prompting,Yes.,4,"""it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures"" and ""such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks.""",2022,2022-06-06T22:09:52Z,"Keyphrase: ""Spurious correlations and impaired generalization""","""it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures"" and ""such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks."" Keyphrase: ""Spurious correlations and impaired generalization"""
arXIv2022,Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models,Yes.,5,"""However, models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is inevitable.""",2022,2022-06-05T20:02:30Z,"Keyphrase: ""Limited crosslingual detoxification""","""However, models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is inevitable."" Keyphrase: ""Limited crosslingual detoxification"""
arXIv2022,Order-sensitive Shapley Values for Evaluating Conceptual Soundness of NLP Models,Yes.,5,"""Previous works show that deep NLP models are not always conceptually sound",2022,2022-06-01T02:30:12Z,"Keyphrase: ""Conceptually unsound""","""Previous works show that deep NLP models are not always conceptually sound Keyphrase: ""Conceptually unsound"""
arXIv2022,Automatic Short Math Answer Grading via In-context Meta-learning,Yes.,4,"""However, these approaches have several key limitations, including i) they use pre-trained language models that are not well-adapted to educational subject domains and/or student-generated text and ii) they almost always train one model per question, ignoring the linkage across a question and result in a significant model storage problem due to the size of advanced language models.""",2022,2022-05-30T16:26:02Z,"Keyphrase: ""Limited adaptability to educational content and student-generated text""","""However, these approaches have several key limitations, including i) they use pre-trained language models that are not well-adapted to educational subject domains and/or student-generated text and ii) they almost always train one model per question, ignoring the linkage across a question and result in a significant model storage problem due to the size of advanced language models."" Keyphrase: ""Limited adaptability to educational content and student-generated text"""
arXIv2022,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,Yes.,5,"""Its application to video generation is still facing many challenges",2022,2022-05-29T19:02:15Z,"Keyphrase: ""Challenges in video generation""","""Its application to video generation is still facing many challenges Keyphrase: ""Challenges in video generation"""
arXIv2022,Quark: Controllable Text Generation with Reinforced Unlearning,Yes.,5,"""Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user.""",2022,2022-05-26T21:11:51Z,"Keyphrase: ""Offensive and toxic language with repetition""","""Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user."" Keyphrase: ""Offensive and toxic language with repetition"""
arXIv2022,Differentially Private Decoding in Large Language Models,Yes.,4,"""LLMs, while effective, have been shown to memorize instances of training data thereby potentially revealing private information processed during pre-training.""",2022,2022-05-26T20:50:58Z,"Keyphrase: ""Privacy concerns and memorization""","""LLMs, while effective, have been shown to memorize instances of training data thereby potentially revealing private information processed during pre-training."" Keyphrase: ""Privacy concerns and memorization"""
arXIv2022,RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning,Yes.,5,"""In our experiments with RoBERTa and T5, we find that the models trained in prior works do not perform consistently on the different perturbations in RobustLR, thus showing that the models are not robust to the proposed logical perturbations. Further, we find that the models",2022,2022-05-25T09:23:50Z,"Keyphrase: ""Limited robustness to perturbations""","""In our experiments with RoBERTa and T5, we find that the models trained in prior works do not perform consistently on the different perturbations in RobustLR, thus showing that the models are not robust to the proposed logical perturbations. Further, we find that the models Keyphrase: ""Limited robustness to perturbations"""
arXIv2022,Perturbation Augmentation for Fairer NLP,Yes.,4,"""Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets."" and ""Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models.""",2022,2022-05-25T09:00:29Z,"Keyphrase: ""Unwanted social biases""","""Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets."" and ""Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models."" Keyphrase: ""Unwanted social biases"""
arXIv2022,Memorization in NLP Fine-tuning Methods,Yes.,4,"""Large language models are shown to present privacy risks through memorization of training data,"" and ""we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different.""",2022,2022-05-25T05:49:31Z,"Keyphrase: ""Privacy risk and memorization""","""Large language models are shown to present privacy risks through memorization of training data,"" and ""we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different."" Keyphrase: ""Privacy risk and memorization"""
arXIv2022,Fine-tuned Language Models are Continual Learners,Yes.,5,"""these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.""",2022,2022-05-24T22:53:34Z,"Keyphrase: ""Limited task generalization""","""these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets."" Keyphrase: ""Limited task generalization"""
arXIv2022,On Measuring Social Biases in Prompt-Based Multi-Task Learning,Yes.,4,"""We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs."" and ""The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training,",2022,2022-05-23T20:01:20Z,"Keyphrase: ""Input encoding bias""","""We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs."" and ""The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, Keyphrase: ""Input encoding bias"""
arXIv2022,Challenges in Measuring Bias via Open-Ended Language Generation,Yes.,4,"""Researchers have devised numerous ways to quantify social biases vested in pretrained language models."" and ""We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings.""",2022,2022-05-23T19:57:15Z,"Keyphrase: ""Inconsistent bias measurement""","""Researchers have devised numerous ways to quantify social biases vested in pretrained language models."" and ""We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings."" Keyphrase: ""Inconsistent bias measurement"""
arXIv2022,Outliers Dimensions that Disrupt Transformers Are Driven by Frequency,Yes.,5,"""While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon",2022,2022-05-23T15:19:09Z,"Keyphrase: ""Robustness issue with outlier phenomenon""","""While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon Keyphrase: ""Robustness issue with outlier phenomenon"""
arXIv2022,Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements,Yes.,4,"""We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce",2022,2022-05-23T15:05:27Z,"Keyphrase: ""Bias and realism in generated advertisements""","""We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce Keyphrase: ""Bias and realism in generated advertisements"""
arXIv2022,RL with KL penalties is better viewed as Bayesian inference,Yes.,5,"""We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse",2022,2022-05-23T12:47:13Z,"Keyphrase: ""Distribution collapse in finetuning""","""We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse Keyphrase: ""Distribution collapse in finetuning"""
arXIv2022,Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers,Yes.,5,"""This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form.""",2022,2022-05-22T18:03:03Z,"Keyphrase: ""Limited reasoning ability""","""This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form."" Keyphrase: ""Limited reasoning ability"""
arXIv2022,Scaling Laws and Interpretability of Learning from Repeated Data,Yes.,5,"""Recent large language models have been trained on vast datasets, but also often on repeated data... Some works have reported substantial negative performance effects of this repeated data."" and ""We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance.""",2022,2022-05-21T02:14:27Z,"Keyphrase: ""Negative performance effect of repeated data""","""Recent large language models have been trained on vast datasets, but also often on repeated data... Some works have reported substantial negative performance effects of this repeated data."" and ""We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance."" Keyphrase: ""Negative performance effect of repeated data"""
arXIv2022,Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation,Yes.,4,"""Our results show that GPT-2 amplifies bias by considering women as junior and men as senior more often than the ground truth in both domains. These results suggest that NLP applications built using GPT-2 may harm women in professional capacities.""",2022,2022-05-19T20:05:02Z,"Keyphrase: ""Amplification of gender bias""","""Our results show that GPT-2 amplifies bias by considering women as junior and men as senior more often than the ground truth in both domains. These results suggest that NLP applications built using GPT-2 may harm women in professional capacities."" Keyphrase: ""Amplification of gender bias"""
arXIv2022,Overcoming Language Disparity in Online Content Classification with Multimodal Learning,Yes.,4,"""Large language models are now the standard to develop state-of-the-art solutions for text detection and classification tasks. However, the development of advanced computational techniques and resources is disproportionately focused on the English language, sidelining a majority of the languages spoken globally."" and ""we situate our findings with respect",2022,2022-05-19T17:56:02Z,"Keyphrase: ""English-centric bias""","""Large language models are now the standard to develop state-of-the-art solutions for text detection and classification tasks. However, the development of advanced computational techniques and resources is disproportionately focused on the English language, sidelining a majority of the languages spoken globally."" and ""we situate our findings with respect Keyphrase: ""English-centric bias"""
arXIv2022,Are Prompt-based Models Clueless?,Yes.,4,"""models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets"" and ""Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues.""",2022,2022-05-19T02:47:58Z,"Keyphrase: ""Dependency on superficial cues""","""models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets"" and ""Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues."" Keyphrase: ""Dependency on superficial cues"""
arXIv2022,Transformer-based Program Synthesis for Low-Data Environments,Yes.,5,"""However, these models perform poorly on long-horizon and low-data tasks, and often don't seem to understand the semantics of the languages they generate.""",2022,2022-05-18T23:33:33Z,"Keyphrase: ""Poor performance on long-horizon low-data tasks and lack of semantic understanding""","""However, these models perform poorly on long-horizon and low-data tasks, and often don't seem to understand the semantics of the languages they generate."" Keyphrase: ""Poor performance on long-horizon low-data tasks and lack of semantic understanding"""
arXIv2022,"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",Yes.,5,"""We find that humans are far more robust than LLMs on this benchmark.""",2022,2022-05-11T18:14:33Z,"Keyphrase: ""Robustness challenge""","""We find that humans are far more robust than LLMs on this benchmark."" Keyphrase: ""Robustness challenge"""
arXIv2022,Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence,Yes.,5,"""much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property"" and ""we observe that PLMs violate the LNP frequently.""",2022,2022-05-08T08:37:36Z,"Keyphrase: ""Violation of linguistic norms and principles""","""much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property"" and ""we observe that PLMs violate the LNP frequently."" Keyphrase: ""Violation of linguistic norms and principles"""
arXIv2022,"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",Yes.,5,"""We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities.""",2022,2022-05-06T20:49:27Z,"Keyphrase: ""Limited entity tracking ability""","""We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities."" Keyphrase: ""Limited entity tracking ability"""
arXIv2022,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,Yes.,5,"""We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations.""",2022,2022-05-06T17:57:58Z,"Keyphrase: ""Unreliable explanations""","""We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations."" Keyphrase: ""Unreliable explanations"""
arXIv2022,Provably Confidential Language Modelling,Yes.,4,"""Large language models are shown to memorize privacy information such as social security numbers in training data.""",2022,2022-05-04T02:33:45Z,"Keyphrase: ""Privacy information memorization""","""Large language models are shown to memorize privacy information such as social security numbers in training data."" Keyphrase: ""Privacy information memorization"""
arXIv2022,"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",Yes.,5,"""Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach.""",2022,2022-05-01T11:01:28Z,"Keyphrase: ""Inherent limitations in system approach""","""Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach."" Keyphrase: ""Inherent limitations in system approach"""
arXIv2022,Training Language Models with Language Feedback,Yes.,5,"""Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries.""",2022,2022-04-29T15:06:58Z,"Keyphrase: ""Generation of offensive and factually incorrect text""","""Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries."" Keyphrase: ""Generation of offensive and factually incorrect text"""
arXIv2022,Inferring Implicit Relations in Complex Questions with Language Models,Yes.,5,"""we investigate why current models struggle with implicit reasoning question answering (QA) tasks,"" and ""we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations.""",2022,2022-04-28T21:00:54Z,"Keyphrase: ""Struggles with implicit reasoning""","""we investigate why current models struggle with implicit reasoning question answering (QA) tasks,"" and ""we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations."" Keyphrase: ""Struggles with implicit reasoning"""
arXIv2022,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,Yes.,5,"""the in-depth analysis of when in-context learning occurs is still lacking"" and ""in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning"" and ""pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in",2022,2022-04-28T13:59:54Z,"Keyphrase: ""Limited in-context learning""","""the in-depth analysis of when in-context learning occurs is still lacking"" and ""in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning"" and ""pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in Keyphrase: ""Limited in-context learning"""
arXIv2022,You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas,Yes.,4,"""privacy concerns have arisen recently",2022,2022-04-26T09:36:18Z,"Keyphrase: ""Privacy concerns""","""privacy concerns have arisen recently Keyphrase: ""Privacy concerns"""
arXIv2022,KALA: Knowledge-Augmented Language Model Adaptation,Yes.,4,"""Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM",2022,2022-04-22T08:11:59Z,"Keyphrase: ""Limited domain-specific knowledge""","""Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM Keyphrase: ""Limited domain-specific knowledge"""
arXIv2022,You Are What You Write: Preserving Privacy in the Era of Large Language Models,Yes.,4,"""Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e",2022,2022-04-20T11:12:53Z,"Keyphrase: ""Trust and privacy concerns""","""Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e Keyphrase: ""Trust and privacy concerns"""
arXIv2022,What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment,Yes.,5,"""The capabilities of large transformer models as instruction learners, however, remain poorly understood."" and ""our model, a fine-tuned T5-based text2text transformer, struggles with large regular languages, suggesting that less precise instructions are challenging for models. Additionally, instruction executions that require tracking longer contexts of prior steps are also more difficult.""",2022,2022-04-19T22:11:47Z,"Keyphrase: ""Challenges with precise instruction execution""","""The capabilities of large transformer models as instruction learners, however, remain poorly understood."" and ""our model, a fine-tuned T5-based text2text transformer, struggles with large regular languages, suggesting that less precise instructions are challenging for models. Additionally, instruction executions that require tracking longer contexts of prior steps are also more difficult."" Keyphrase: ""Challenges with precise instruction execution"""
arXIv2022,Pathologies of Pre-trained Language Models in Few-shot Fine-tuning,Yes.,5,"""without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels"" and ""pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot",2022,2022-04-17T15:55:18Z,"Keyphrase: ""Prediction bias and few-shot learning issues""","""without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels"" and ""pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot Keyphrase: ""Prediction bias and few-shot learning issues"""
arXIv2022,Building Markovian Generative Architectures over Pretrained LM Backbones for Efficient Task-Oriented Dialog Systems,Yes.,5,"""A drawback of existing PLM-based models is their non-Markov architectures across turns, i.e., the whole history is used as the conditioning input at each turn. First, this brings inefficiencies in memory and computation. Furthermore, using the whole history increases model",2022,2022-04-13T15:21:34Z,"Keyphrase: ""Inefficient non-Markov architecture""","""A drawback of existing PLM-based models is their non-Markov architectures across turns, i.e., the whole history is used as the conditioning input at each turn. First, this brings inefficiencies in memory and computation. Furthermore, using the whole history increases model Keyphrase: ""Inefficient non-Markov architecture"""
arXIv2022,Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Yes.,4,"""current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets.""",2022,2022-04-13T10:32:03Z,"Keyphrase: ""Limited linguistic skill mapping""","""current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets."" Keyphrase: ""Limited linguistic skill mapping"""
arXIv2022,Impossible Triangle: What's Next for Pre-trained Language Models?,Yes.,5,"""However, many of such models come with a dauntingly huge size that few institutions can afford to pre-train, fine-tune or even deploy, while moderate-sized models usually lack strong generalized few-shot learning capabilities."" and ""We argue that all existing PLM models lack one or more properties from the Impossible Triangle.""",2022,2022-04-13T01:28:18Z,"Keyphrase: ""Limited few-shot learning capability""","""However, many of such models come with a dauntingly huge size that few institutions can afford to pre-train, fine-tune or even deploy, while moderate-sized models usually lack strong generalized few-shot learning capabilities."" and ""We argue that all existing PLM models lack one or more properties from the Impossible Triangle."" Keyphrase: ""Limited few-shot learning capability"""
arXIv2022,Uniform Complexity for Text Generation,Yes.,5,"""existing models still do not capture factors that contribute to producing consistent text"" and ""we find that models such as GPT-2 struggle to preserve the complexity of input prompts used in its generations, even if finetuned with professionally written texts.""",2022,2022-04-11T15:19:47Z,"Keyphrase: ""Struggles with preserving input complexity""","""existing models still do not capture factors that contribute to producing consistent text"" and ""we find that models such as GPT-2 struggle to preserve the complexity of input prompts used in its generations, even if finetuned with professionally written texts."" Keyphrase: ""Struggles with preserving input complexity"""
arXIv2022,Testing the limits of natural language models for predicting human language judgments,Yes.,5,"""experiments also revealed significant shortcomings of its alignment with human perception.""",2022,2022-04-07T17:12:57Z,"Keyphrase: ""Misalignment with human perception""","""experiments also revealed significant shortcomings of its alignment with human perception."" Keyphrase: ""Misalignment with human perception"""
arXIv2022,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",Yes.,5,"""a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment.""",2022,2022-04-04T17:57:11Z,"Keyphrase: ""Lack of real-world experience""","""a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment."" Keyphrase: ""Lack of real-world experience"""
arXIv2022,Mitigating Gender Bias in Machine Translation through Adversarial Learning,Yes.,4,"""restructuring training objectives in the context of fine-tuning pretrained large language models"" and ""addresses these challenges to mitigate gender bias in seq2seq machine translation.""",2022,2022-03-20T23:35:09Z,"Keyphrase: ""Gender bias mitigation challenge""","""restructuring training objectives in the context of fine-tuning pretrained large language models"" and ""addresses these challenges to mitigate gender bias in seq2seq machine translation."" Keyphrase: ""Gender bias mitigation challenge"""
arXIv2022,Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists,Yes.,4,"""Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability."" and ""severe unintended bias, and lower performance.""",2022,2022-03-17T09:29:50Z,"Keyphrase: ""Overfitting and unintended bias""","""Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability."" and ""severe unintended bias, and lower performance."" Keyphrase: ""Overfitting and unintended bias"""
arXIv2022,Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again,Yes.,5,"""However, our results show that GPT-3 still significantly underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3 in-context learning also yields smaller gains in accuracy when more training data becomes available. Our in-depth analyses further reveal issues of the in-context",2022,2022-03-16T05:56:08Z,"Keyphrase: ""Underperformance compared to finetuning smaller models""","""However, our results show that GPT-3 still significantly underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3 in-context learning also yields smaller gains in accuracy when more training data becomes available. Our in-depth analyses further reveal issues of the in-context Keyphrase: ""Underperformance compared to finetuning smaller models"""
arXIv2022,Do Language Models Plagiarize?,Yes.,5,"""Given that a majority of LMs' training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indis",2022,2022-03-15T03:11:11Z,"Keyphrase: ""Ethical implications of data scraping""","""Given that a majority of LMs' training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indis Keyphrase: ""Ethical implications of data scraping"""
arXIv2022,"When classifying grammatical role, BERT doesn't care about word order... except when it matters",Yes.,4,"""Recent work has shown large language models to be surprisingly word order invariant,"" and ""highlight how models use context in the uncommon, but critical, instances where it matters.""",2022,2022-03-11T19:00:15Z,"Keyphrase: ""Word order invariance""","""Recent work has shown large language models to be surprisingly word order invariant,"" and ""highlight how models use context in the uncommon, but critical, instances where it matters."" Keyphrase: ""Word order invariance"""
arXIv2022,Speciesist Language and Nonhuman Animal Bias in English Masked Language Models,Yes.,4,"""We found that pre-trained masked language models tend to associate harmful words with nonhuman animals and have a bias toward using speciesist language for some nonhuman animal names.""",2022,2022-03-10T03:32:29Z,"Keyphrase: ""Speciesist language bias""","""We found that pre-trained masked language models tend to associate harmful words with nonhuman animals and have a bias toward using speciesist language for some nonhuman animal names."" Keyphrase: ""Speciesist language bias"""
arXIv2022,Training language models to follow instructions with human feedback,Yes.,5,"""large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user.""",2022,2022-03-04T07:04:42Z,"Keyphrase: ""Untruthful and toxic outputs""","""large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user."" Keyphrase: ""Untruthful and toxic outputs"""
arXIv2022,Logical Fallacy Detection,Yes.,5,"""We find that existing pretrained large language models perform poorly on this task.""",2022,2022-02-28T13:18:26Z,"Keyphrase: ""Poor task performance""","""We find that existing pretrained large language models perform poorly on this task."" Keyphrase: ""Poor task performance"""
arXIv2022,Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies,Yes.,5,"""We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens.""",2022,2022-02-24T19:00:39Z,"Keyphrase: ""Vocabulary misalignment""","""We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens."" Keyphrase: ""Vocabulary misalignment"""
arXIv2022,Capturing Failures of Large Language Models via Human Cognitive Biases,Yes.,5,"""To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases -- systematic patterns of deviation from rational judgement."" and ""Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave.""",2022,2022-02-24T18:58:52Z,"Keyphrase: ""Limited understanding of cognitive biases""","""To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases -- systematic patterns of deviation from rational judgement."" and ""Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave."" Keyphrase: ""Limited understanding of cognitive biases"""
arXIv2022,Reward Modeling for Mitigating Toxicity in Transformer-based Language Models,Yes.,4,"""language models that are pretrained on large unlabeled web text corpora have been shown to suffer from degenerating toxic content and social bias behaviors, consequently hindering their safe deployment.""",2022,2022-02-19T19:26:22Z,"Keyphrase: ""Degenerating toxic content and social bias""","""language models that are pretrained on large unlabeled web text corpora have been shown to suffer from degenerating toxic content and social bias behaviors, consequently hindering their safe deployment."" Keyphrase: ""Degenerating toxic content and social bias"""
arXIv2022,Quantifying Memorization Across Neural Language Models,Yes.,5,"""Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality),",2022,2022-02-15T18:48:31Z,"Keyphrase: ""Undesirable memorization""","""Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), Keyphrase: ""Undesirable memorization"""
arXIv2022,Deduplicating Training Data Mitigates Privacy Risks in Language Models,Yes.,5,"""Past work has shown that large language models are susceptible to privacy attacks,"" and ""Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks.""",2022,2022-02-14T08:20:15Z,"Keyphrase: ""Privacy vulnerabilities""","""Past work has shown that large language models are susceptible to privacy attacks,"" and ""Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks."" Keyphrase: ""Privacy vulnerabilities"""
arXIv2022,Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,Yes.,4,"""Pre-trained language models (LMs) are shown to easily generate toxic language."" and ""We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify.""",2022,2022-02-08T22:10:40Z,"Keyphrase: ""Toxic language generation""","""Pre-trained language models (LMs) are shown to easily generate toxic language."" and ""We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify."" Keyphrase: ""Toxic language generation"""
arXIv2022,Survey of Hallucination in Natural Language Generation,Yes.,4,"""deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios"" and ""hallucinations in large language models (LLMs).""",2022,2022-02-08T03:55:01Z,"Keyphrase: ""Unintended text hallucination""","""deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios"" and ""hallucinations in large language models (LLMs)."" Keyphrase: ""Unintended text hallucination"""
arXIv2022,What Has Been Enhanced in my Knowledge-Enhanced Language Model?,Yes.,4,"""Pretrained language models (LMs) do not capture factual knowledge very well."" and ""it is unclear how and what kind of knowledge is effectively integrated into these models and if such integration may lead to catastrophic forgetting of already learned knowledge.""",2022,2022-02-02T11:23:36Z,"Keyphrase: ""Catastrophic forgetting""","""Pretrained language models (LMs) do not capture factual knowledge very well."" and ""it is unclear how and what kind of knowledge is effectively integrated into these models and if such integration may lead to catastrophic forgetting of already learned knowledge."" Keyphrase: ""Catastrophic forgetting"""
arXIv2022,Unveiling Project-Specific Bias in Neural Code Models,Yes.,5,"""Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data.""",2022,2022-01-19T02:09:48Z,"Keyphrase: ""Limited generalization to out-of-distribution data""","""Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data."" Keyphrase: ""Limited generalization to out-of-distribution data"""
arXIv2022,Unintended Bias in Language Model-driven Conversational Recommendation,Yes.,5,"""pretrained LMs are well-known to be prone to intrinsic biases in their training data,"" and ""raises a red flag that advances in the language handling capability of LM-driven CRSs do not come without significant challenges related to mitigating unintended bias.""",2022,2022-01-17T05:50:14Z,"Keyphrase: ""Intrinsic bias""","""pretrained LMs are well-known to be prone to intrinsic biases in their training data,"" and ""raises a red flag that advances in the language handling capability of LM-driven CRSs do not come without significant challenges related to mitigating unintended bias."" Keyphrase: ""Intrinsic bias"""
arXIv2022,Memory-assisted prompt editing to improve GPT-3 after deployment,Yes.,5,"""Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans.""",2022,2022-01-16T10:11:37Z,"Keyphrase: ""Mistakes comparable to humans""","""Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans."" Keyphrase: ""Mistakes comparable to humans"""
arXIv2022,Submix: Practical Private Prediction for Large-Scale Language Models,Yes.,4,"""Recent data-extraction attacks have exposed that language models can memorize some training samples verbatim. This is a vulnerability that can compromise the privacy of the model's training data.""",2022,2022-01-04T04:23:38Z,"Keyphrase: ""Memorization of training data""","""Recent data-extraction attacks have exposed that language models can memorize some training samples verbatim. This is a vulnerability that can compromise the privacy of the model's training data."" Keyphrase: ""Memorization of training data"""
arXIv2022,A Survey on In-context Learning,Yes.,4,"""We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research.""",2022,2022-12-31T15:57:09Z,"Keyphrase: ""Lack of focus on practical application""","""We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research."" Keyphrase: ""Lack of focus on practical application"""
arXIv2022,Inconsistencies in Masked Language Models,Yes.,5,"""However, this paper shows that distributions corresponding to different masking patterns can demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together."" and ""This fundamental flaw in MLMs can lead to self-contradictory behaviors during inference.""",2022,2022-12-30T22:53:25Z,"Keyphrase: ""Inconsistent masking patterns""","""However, this paper shows that distributions corresponding to different masking patterns can demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together."" and ""This fundamental flaw in MLMs can lead to self-contradictory behaviors during inference."" Keyphrase: ""Inconsistent masking patterns"""
arXIv2022,A Survey on Knowledge-Enhanced Pre-trained Language Models,Yes.,4,"""PLMs still face a number of challenges including poor interpretability, weak reasoning capability, and the need for a lot of expensive annotated data when applied to downstream tasks.""",2022,2022-12-27T09:54:14Z,"Keyphrase: ""Weak interpretability and reasoning""","""PLMs still face a number of challenges including poor interpretability, weak reasoning capability, and the need for a lot of expensive annotated data when applied to downstream tasks."" Keyphrase: ""Weak interpretability and reasoning"""
arXIv2022,Large Language Models Encode Clinical Knowledge,Yes.,5,"""human evaluation reveals key gaps in Flan-PaLM responses"" and ""Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.""",2022,2022-12-26T14:28:24Z,"Keyphrase: ""Limited clinical applicability""","""human evaluation reveals key gaps in Flan-PaLM responses"" and ""Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications."" Keyphrase: ""Limited clinical applicability"""
arXIv2022,Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,Yes.,5,"""These results suggest that the propensity of larger Transformer-based models to 'memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.""",2022,2022-12-23T03:57:54Z,"Keyphrase: ""Over-reliance on memorization""","""These results suggest that the propensity of larger Transformer-based models to 'memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing."" Keyphrase: ""Over-reliance on memorization"""
arXIv2022,Parallel Context Windows for Large Language Models,Yes.,5,"""When applied to processing long text, Large Language Models (LLMs) are limited by their context window.""",2022,2022-12-21T11:38:51Z,"Keyphrase: ""Limited context window""","""When applied to processing long text, Large Language Models (LLMs) are limited by their context window."" Keyphrase: ""Limited context window"""
arXIv2022,Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners,Yes.,5,"""However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model.""",2022,2022-12-21T09:37:05Z,"Keyphrase: ""Limited training data scalability""","""However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model."" Keyphrase: ""Limited training data scalability"""
arXIv2022,From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models,Yes.,4,"""However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task.""",2022,2022-12-21T08:39:36Z,"Keyphrase: ""Modality disconnection""","""However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task."" Keyphrase: ""Modality disconnection"""
arXIv2022,ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models,Yes.,4,"""Language models are generally trained on the publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting."" and ""We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions",2022,2022-12-21T07:06:55Z,"Keyphrase: ""Limited domain-specific generalization""","""Language models are generally trained on the publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting."" and ""We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions Keyphrase: ""Limited domain-specific generalization"""
arXIv2022,"Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",Yes.,5,"""Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking.""",2022,2022-12-21T04:43:19Z,"Keyphrase: ""Lack of pragmatic capability""","""Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking."" Keyphrase: ""Lack of pragmatic capability"""
arXIv2022,Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering,Yes.,5,"""While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (~50% for deletion intervention, and ~20% drop in accuracy for negation intervention)."" and ""But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models'",2022,2022-12-21T00:00:01Z,"Keyphrase: ""Semantic unfaithfulness""","""While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (~50% for deletion intervention, and ~20% drop in accuracy for negation intervention)."" and ""But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models' Keyphrase: ""Semantic unfaithfulness"""
arXIv2022,Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing,Yes.,4,"""Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics."" and ""existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain",2022,2022-12-20T22:41:24Z,"Keyphrase: ""Harmful humanlike biases""","""Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics."" and ""existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain Keyphrase: ""Harmful humanlike biases"""
arXIv2022,Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,Yes.,4,"""Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs."" and ""Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.""",2022,2022-12-20T18:59:23Z,"Keyphrase: ""Struggle with hierarchical reasoning""","""Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs."" and ""Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers."" Keyphrase: ""Struggle with hierarchical reasoning"""
arXIv2022,When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,Yes.,5,"""large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge."" and ""We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail.""",2022,2022-12-20T18:30:15Z,"Keyphrase: ""Limited world knowledge""","""large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge."" and ""We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail."" Keyphrase: ""Limited world knowledge"""
arXIv2022,Generic Temporal Reasoning with Differential Analysis and Explanation,Yes.,5,"""We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions.""",2022,2022-12-20T17:40:03Z,"Keyphrase: ""Overreliance on spurious information""","""We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions."" Keyphrase: ""Overreliance on spurious information"""
arXIv2022,True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4,Yes.,5,"""GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area.""",2022,2022-12-20T09:34:43Z,"Keyphrase: ""Limited deep reasoning ability""","""GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area."" Keyphrase: ""Limited deep reasoning ability"""
arXIv2022,Do language models have coherent mental models of everyday things?,Yes.,5,"""Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent 'parts mental models' (54-59% accurate, 19-43% conditional constraint violation).""",2022,2022-12-20T06:54:04Z,"Keyphrase: ""Fragmented knowledge representation""","""Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent 'parts mental models' (54-59% accurate, 19-43% conditional constraint violation)."" Keyphrase: ""Fragmented knowledge representation"""
arXIv2022,Discovering Language Model Behaviors with Model-Written Evaluations,Yes.,5,"""We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (""sycophancy"") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes",2022,2022-12-19T05:13:52Z,"Keyphrase: ""Inverse scaling issues""","""We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (""sycophancy"") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes Keyphrase: ""Inverse scaling issues"""
arXIv2022,Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,Yes.,5,"""Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse.""",2022,2022-12-18T19:27:41Z,"Keyphrase: ""Weak reasoning capabilities""","""Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse."" Keyphrase: ""Weak reasoning capabilities"""
arXIv2022,Language model acceptability judgements are not always robust to context,Yes.,4,"""we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context"" and ""we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures.""",2022,2022-12-18T00:11:06Z,"Keyphrase: ""Context matching limitations""","""we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context"" and ""we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures."" Keyphrase: ""Context matching limitations"""
arXIv2022,MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation,Yes.,5,"""these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency.""",2022,2022-12-16T17:36:23Z,"Keyphrase: ""Low semantic coverage and logical inconsistency""","""these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency."" Keyphrase: ""Low semantic coverage and logical inconsistency"""
arXIv2022,Teaching Small Language Models to Reason,Yes.,4,"""However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters.""",2022,2022-12-16T11:24:42Z,"Keyphrase: ""Limited reasoning capability""","""However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters."" Keyphrase: ""Limited reasoning capability"""
arXIv2022,"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",Yes.,5,"""We find that zero-shot CoT reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output"" and ""Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.""",2022,2022-12-15T18:59:32Z,"Keyphrase: ""Harmful bias in sensitive domains""","""We find that zero-shot CoT reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output"" and ""Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved."" Keyphrase: ""Harmful bias in sensitive domains"""
arXIv2022,"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety",Yes.,5,"""Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to 'explain their reasoning' often leads",2022,2022-12-13T00:29:45Z,"Keyphrase: ""Misleading average performance""","""Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to 'explain their reasoning' often leads Keyphrase: ""Misleading average performance"""
arXIv2022,Understanding How Model Size Affects Few-shot Instruction Prompting,Yes.,5,"""Large Language Models are affected by the phenomena of memorizing and forgetting their training data."" and ""We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes.""",2022,2022-12-04T19:59:52Z,"Keyphrase: ""Weak few-shot performance""","""Large Language Models are affected by the phenomena of memorizing and forgetting their training data."" and ""We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes."" Keyphrase: ""Weak few-shot performance"""
arXIv2022,Event knowledge in large language models: the gap between the impossible and the unlikely,Yes.,4,"""However, LLMs show less consistent preferences for likely vs. unlikely events"" and ""highlight a gap between representations of possible/impossible and likely/unlikely events.""",2022,2022-12-02T23:43:18Z,"Keyphrase: ""Inconsistent event representation""","""However, LLMs show less consistent preferences for likely vs. unlikely events"" and ""highlight a gap between representations of possible/impossible and likely/unlikely events."" Keyphrase: ""Inconsistent event representation"""
arXIv2022,a survey on GPT-3,Yes.,4,"""We discuss some of the challenges that GPT-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers.""",2022,2022-12-01T20:24:19Z,"Keyphrase: ""Training complexity and bias""","""We discuss some of the challenges that GPT-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers."" Keyphrase: ""Training complexity and bias"""
arXIv2022,Scientific and Creative Analogies in Pretrained Language Models,Yes.,5,"""We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.""",2022,2022-11-28T12:49:44Z,"Keyphrase: ""Struggles with complex analogies""","""We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding."" Keyphrase: ""Struggles with complex analogies"""
arXIv2022,An Analysis of Social Biases Present in BERT Variants Across Multiple Languages,Yes.,4,"""Although large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reflect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings.""",2022,2022-11-25T23:38:08Z,"Keyphrase: ""Reflects human bias""","""Although large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reflect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings."" Keyphrase: ""Reflects human bias"""
arXIv2022,Validating Large Language Models with ReLM,Yes.,5,"""there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language.""",2022,2022-11-21T21:40:35Z,"Keyphrase: ""Data memorization bias""","""there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language."" Keyphrase: ""Data memorization bias"""
arXIv2022,Conceptor-Aided Debiasing of Large Language Models,Yes.,5,"""Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus."" and ""CI-BERT reduces the language model accuracy.""",2022,2022-11-20T21:24:48Z,"Keyphrase: ""Inherent social bias""","""Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus."" and ""CI-BERT reduces the language model accuracy."" Keyphrase: ""Inherent social bias"""
arXIv2022,PAL: Program-aided Language Models,Yes.,5,"""While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly.""",2022,2022-11-18T18:56:13Z,"Keyphrase: ""Logical arithmetic mistakes""","""While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly."" Keyphrase: ""Logical arithmetic mistakes"""
arXIv2022,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Yes.,5,"""While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information."" and ""However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries.""",2022,2022-11-15T18:50:34Z,"Keyphrase: ""Factually inconsistent summaries""","""While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information."" and ""However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries."" Keyphrase: ""Factually inconsistent summaries"""
arXIv2022,Large Language Models Struggle to Learn Long-Tail Knowledge,Yes.,5,"""However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely."" and ""while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data.""",2022,2022-11-15T18:49:27Z,"Keyphrase: ""Limited longtail knowledge""","""However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely."" and ""while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data."" Keyphrase: ""Limited longtail knowledge"""
arXIv2022,GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective,Yes.,5,"""the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods"" and ""significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.""",2022,2022-11-15T11:53:55Z,"Keyphrase: ""Out-of-distribution generalization challenge""","""the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods"" and ""significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy."" Keyphrase: ""Out-of-distribution generalization challenge"""
arXIv2022,UGIF: UI Grounded Instruction Following,Yes.,4,"""We compare the performance of different LLMs including PaLM and GPT-3 and find that the end-to-end task completion rate is 48% for English UI but the performance drops to 32% for other languages. We analyze the common failure modes of existing models on this task and point out areas for improvement.""",2022,2022-11-14T18:36:19Z,"Keyphrase: ""End-to-end task completion rate drop""","""We compare the performance of different LLMs including PaLM and GPT-3 and find that the end-to-end task completion rate is 48% for English UI but the performance drops to 32% for other languages. We analyze the common failure modes of existing models on this task and point out areas for improvement."" Keyphrase: ""End-to-end task completion rate drop"""
arXIv2022,Does Debiasing Inevitably Degrade the Model Performance,Yes.,4,"""Gender bias in language models has attracted sufficient attention because it threatens social justice. However, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious.""",2022,2022-11-14T13:46:13Z,"Keyphrase: ""Degraded performance due to debiasing methods""","""Gender bias in language models has attracted sufficient attention because it threatens social justice. However, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious."" Keyphrase: ""Degraded performance due to debiasing methods"""
arXIv2022,DocuT5: Seq2seq SQL Generation with Table Documentation,Yes.,4,"""Current SQL generators based on pre-trained language models struggle to answer complex questions requiring domain context or understanding fine-grained table structure.""",2022,2022-11-11T13:31:55Z,"Keyphrase: ""Limited domain context understanding""","""Current SQL generators based on pre-trained language models struggle to answer complex questions requiring domain context or understanding fine-grained table structure."" Keyphrase: ""Limited domain context understanding"""
arXIv2022,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,Yes.,5,"""Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world.""",2022,2022-11-10T07:48:01Z,"Keyphrase: ""Limited real-world understanding""","""Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world."" Keyphrase: ""Limited real-world understanding"""
arXIv2022,Large Language Models with Controllable Working Memory,Yes.,5,"""We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size.""",2022,2022-11-09T18:58:29Z,"Keyphrase: ""Poor controllability and robustness""","""We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size."" Keyphrase: ""Poor controllability and robustness"""
arXIv2022,Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind,Yes.,5,"""In comparison, our systems based on either state-of-the-art large language models (GPT-4) or meta-learning algorithms lags >20% behind, highlighting a notable limitation in existing approaches' ToM capabilities.""",2022,2022-11-09T05:06:12Z,"Keyphrase: ""Lagging behind in capability""","""In comparison, our systems based on either state-of-the-art large language models (GPT-4) or meta-learning algorithms lags >20% behind, highlighting a notable limitation in existing approaches' ToM capabilities."" Keyphrase: ""Lagging behind in capability"""
arXIv2022,Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic,Yes.,5,"""Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning.""",2022,2022-11-03T18:53:30Z,"Keyphrase: ""Limited numeric comprehension""","""Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning."" Keyphrase: ""Limited numeric comprehension"""
arXIv2022,LMentry: A Language Model Benchmark of Elementary Language Tasks,Yes.,5,"""Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI's latest 175B-parameter instruction-tuned model, TextDavinci002.""",2022,2022-11-03T18:01:12Z,"Keyphrase: ""Wide variety of failure cases""","""Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI's latest 175B-parameter instruction-tuned model, TextDavinci002."" Keyphrase: ""Wide variety of failure cases"""
arXIv2022,Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer,Yes.,5,"""They impose, however, limits on the maximum input length (512 sub-words in BERT), which are too restrictive in the legal domain. Even sparse-attention models, such as Longformer and BigBird, which increase the maximum input length to 4,096 sub-",2022,2022-11-02T09:27:01Z,"Keyphrase: ""Input length limitations""","""They impose, however, limits on the maximum input length (512 sub-words in BERT), which are too restrictive in the legal domain. Even sparse-attention models, such as Longformer and BigBird, which increase the maximum input length to 4,096 sub- Keyphrase: ""Input length limitations"""
arXIv2022,Two-stage LLM Fine-tuning with Less Specialization and More Generalization,Yes.,5,"""fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances,"" and ""format specialization, where the model overfits to the format of the fine-tuned task.""",2022,2022-11-01T17:56:57Z,"Keyphrase: ""Overfitting to specialized datasets""","""fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances,"" and ""format specialization, where the model overfits to the format of the fine-tuned task."" Keyphrase: ""Overfitting to specialized datasets"""
arXIv2022,The future is different: Large pre-trained language models fail in prediction tasks,Yes.,5,"""Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time."" and ""we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the",2022,2022-11-01T11:01:36Z,"Keyphrase: ""Performance drop with distribution shift""","""Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time."" and ""we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the Keyphrase: ""Performance drop with distribution shift"""
arXIv2022,Emergent Linguistic Structures in Neural Networks are Fragile,Yes.,5,"""Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structures."" and ""Our key observation is that emergent syntactic representations in neural networks are brittle.""",2022,2022-10-31T15:43:57Z,"Keyphrase: ""Brittle syntactic representation""","""Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structures."" and ""Our key observation is that emergent syntactic representations in neural networks are brittle."" Keyphrase: ""Brittle syntactic representation"""
arXIv2022,"A Simple, Yet Effective Approach to Finding Biases in Code Generation",Yes.,4,"""This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.""",2022,2022-10-31T15:06:15Z,"Keyphrase: ""Undesired bias in code generation""","""This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances."" Keyphrase: ""Undesired bias in code generation"""
arXIv2022,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,Yes.,5,"""Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., the language model pre-trained on static data from past years performs worse over time on emerging data.""",2022,2022-10-31T08:12:41Z,"Keyphrase: ""Temporal generalization limitations""","""Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., the language model pre-trained on static data from past years performs worse over time on emerging data."" Keyphrase: ""Temporal generalization limitations"""
arXIv2022,Contrastive Decoding: Open-ended Text Generation as Optimization,Yes.,5,"""maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text"" and ""sampling can often produce incoherent text that drifts from the original topics.""",2022,2022-10-27T00:58:21Z,"Keyphrase: ""Incoherent text generation""","""maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text"" and ""sampling can often produce incoherent text that drifts from the original topics."" Keyphrase: ""Incoherent text generation"""
arXIv2022,Privately Fine-Tuning Large Language Models with Differential Privacy,Yes.,4,"""However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs.""",2022,2022-10-26T21:18:31Z,"Keyphrase: ""Privacy concerns due to data exposure""","""However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs."" Keyphrase: ""Privacy concerns due to data exposure"""
arXIv2022,Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence,Yes.,4,"""We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledge in their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally.""",2022,2022-10-25T01:46:00Z,"Keyphrase: ""Reliance on nonparametric knowledge""","""We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledge in their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally."" Keyphrase: ""Reliance on nonparametric knowledge"""
arXIv2022,Speeding Up Question Answering Task of Language Models via Inverted Index,Yes.,4,"""Despite the wide popularity of large language models (LLMs), few real-world conversational agents take advantage of LLMs. Extensive resources consumed by LLMs disable developers from integrating them into end-user applications.""",2022,2022-10-24T19:59:17Z,"Keyphrase: ""Resource consumption limitations""","""Despite the wide popularity of large language models (LLMs), few real-world conversational agents take advantage of LLMs. Extensive resources consumed by LLMs disable developers from integrating them into end-user applications."" Keyphrase: ""Resource consumption limitations"""
arXIv2022,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,Yes.,5,"""We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box,"" and ""Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively,"" and """,2022,2022-10-24T14:58:58Z,"Keyphrase: ""Lack of social intelligence""","""We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box,"" and ""Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively,"" and "" Keyphrase: ""Lack of social intelligence"""
arXIv2022,Do Language Models Understand Measurements?,Yes.,5,"""In this study, we show that PLMs lack the capability required for reasoning over measurements.""",2022,2022-10-23T10:52:52Z,"Keyphrase: ""Lack of reasoning capability""","""In this study, we show that PLMs lack the capability required for reasoning over measurements."" Keyphrase: ""Lack of reasoning capability"""
arXIv2022,WikiWhy: Answering and Explaining Cause-and-Effect Questions,Yes.,4,"""GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.""",2022,2022-10-21T17:59:03Z,"Keyphrase: ""Room for improvement in correctness""","""GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements."" Keyphrase: ""Room for improvement in correctness"""
arXIv2022,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Yes.,5,"""the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.""",2022,2022-10-21T15:12:37Z,"Keyphrase: ""Overreliance on shallow patterns""","""the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution."" Keyphrase: ""Overreliance on shallow patterns"""
arXIv2022,LittleBird: Efficient Faster & Longer Transformer for Question Answering,Yes.,5,"""BERT has shown a lot of success in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism.""",2022,2022-10-21T10:46:41Z,"Keyphrase: ""Limitation in handling long inputs""","""BERT has shown a lot of success in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism."" Keyphrase: ""Limitation in handling long inputs"""
arXIv2022,Towards a neural architecture of language: Deep learning versus logistics of access in neural architectures for compositional processing,Yes.,5,"""I will argue that these models are not suitable as neural models of human language. Firstly, because they fail on fundamental boundary conditions, such as the amount of learning they require. This would in fact imply that the mechanisms of GPT and brain language processing are fundamentally different. Secondly, because they do not possess the logistics of access needed for compositional and productive human language processing.""",2022,2022-10-19T13:31:26Z,"Keyphrase: ""Misalignment with human language processing""","""I will argue that these models are not suitable as neural models of human language. Firstly, because they fail on fundamental boundary conditions, such as the amount of learning they require. This would in fact imply that the mechanisms of GPT and brain language processing are fundamentally different. Secondly, because they do not possess the logistics of access needed for compositional and productive human language processing."" Keyphrase: ""Misalignment with human language processing"""
arXIv2022,SafeText: A Benchmark for Exploring Physical Safety in Language Models,Yes.,5,"""We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice.""",2022,2022-10-18T17:59:31Z,"Keyphrase: ""Susceptibility to generating unsafe text""","""We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice."" Keyphrase: ""Susceptibility to generating unsafe text"""
arXIv2022,Prompting GPT-3 To Be Reliable,Yes.,4,"""the crucial problem of how to improve the reliability of GPT-3 is still under-explored"" and ""we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important",2022,2022-10-17T14:52:39Z,"Keyphrase: ""Reliability challenges""","""the crucial problem of how to improve the reliability of GPT-3 is still under-explored"" and ""we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important Keyphrase: ""Reliability challenges"""
arXIv2022,"RARR: Researching and Revising What Language Models Say, Using Language Models",Yes.,5,"""However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence.""",2022,2022-10-17T03:44:30Z,"Keyphrase: ""Lack of transparency and trustworthiness""","""However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence."" Keyphrase: ""Lack of transparency and trustworthiness"""
arXIv2022,Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,Yes.,4,"""Several studies have explored these harms and called for their mitigation via development of safer, fairer models."" and ""this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models.""",2022,2022-10-14T10:43:39Z,"Keyphrase: ""Societal harm potential""","""Several studies have explored these harms and called for their mitigation via development of safer, fairer models."" and ""this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models."" Keyphrase: ""Societal harm potential"""
arXIv2022,BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation,Yes.,4,"""it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern on the fairness of PLMs as metrics"" and ""We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely",2022,2022-10-14T08:24:11Z,"Keyphrase: ""Societal bias encoding""","""it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern on the fairness of PLMs as metrics"" and ""We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely Keyphrase: ""Societal bias encoding"""
arXIv2022,"""John is 50 years old, can his son be 65?"" Evaluating NLP Models' Understanding of Feasibility",Yes.,5,"""Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities."" and ""We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly.""",2022,2022-10-14T02:46:06Z,"Keyphrase: ""Failure in complex reasoning""","""Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities."" and ""We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly."" Keyphrase: ""Failure in complex reasoning"""
arXIv2022,SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,Yes.,4,"""A common limitation of diagnostic tests for detecting social biases in NLP models"" and ""we are able to uncover the model's stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations",2022,2022-10-13T18:04:48Z,"Keyphrase: ""Limited ability to detect and address social biases""","""A common limitation of diagnostic tests for detecting social biases in NLP models"" and ""we are able to uncover the model's stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations Keyphrase: ""Limited ability to detect and address social biases"""
arXIv2022,Assessing Out-of-Domain Language Model Performance from Few Examples,Yes.,5,"""While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts.""",2022,2022-10-13T04:45:26Z,"Keyphrase: ""Unpredictable domain shift""","""While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts."" Keyphrase: ""Unpredictable domain shift"""
arXIv2022,SEAL : Interactive Tool for Systematic Error Analysis and Labeling,Yes.,4,"""However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation.""",2022,2022-10-11T23:51:44Z,"Keyphrase: ""Failure on tail data""","""However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation."" Keyphrase: ""Failure on tail data"""
arXIv2022,Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models,Yes.,4,"""Here we show another problem with multilingual models",2022,2022-10-11T17:06:38Z,"Keyphrase: ""Challenges with multilingual capabilities""","""Here we show another problem with multilingual models Keyphrase: ""Challenges with multilingual capabilities"""
arXIv2022,A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models,Yes.,5,"""Despite the remarkable success of pre-trained language models (PLMs), they still face two challenges",2022,2022-10-11T07:26:34Z,"Keyphrase: ""Persistent challenges""","""Despite the remarkable success of pre-trained language models (PLMs), they still face two challenges Keyphrase: ""Persistent challenges"""
arXIv2022,Generating Executable Action Plans with Environmentally-Aware Language Models,Yes.,5,"""However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints.""",2022,2022-10-10T18:56:57Z,"Keyphrase: ""Ambiguity in executable plans""","""However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints."" Keyphrase: ""Ambiguity in executable plans"""
arXIv2022,Quantifying Social Biases Using Templates is Unreliable,Yes.,5,"""Recently, there has been an increase in efforts to understand how large language models (LLMs) propagate and amplify social biases."" and ""Our results indicate that quantifying fairness in LLMs, as done in current practice, can be brittle and needs to be approached with more care and caution.""",2022,2022-10-09T20:05:29Z,"Keyphrase: ""Amplification of social bias""","""Recently, there has been an increase in efforts to understand how large language models (LLMs) propagate and amplify social biases."" and ""Our results indicate that quantifying fairness in LLMs, as done in current practice, can be brittle and needs to be approached with more care and caution."" Keyphrase: ""Amplification of social bias"""
arXIv2022,Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems,Yes.,5,"""However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization.""",2022,2022-10-07T11:16:45Z,"Keyphrase: ""Poor out-of-domain generalization""","""However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization."" Keyphrase: ""Poor out-of-domain generalization"""
arXIv2022,Measuring and Narrowing the Compositionality Gap in Language Models,Yes.,5,"""we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.""",2022,2022-10-07T06:50:23Z,"Keyphrase: ""Limited compositional reasoning""","""we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning."" Keyphrase: ""Limited compositional reasoning"""
arXIv2022,Towards Improving Faithfulness in Abstractive Summarization,Yes.,4,"""Despite the success achieved in neural abstractive summarization based on pre-trained language models, one unresolved issue is that the generated summaries are not always faithful to the input document."" and ""the model over-relies on the language model to generate fluent but inadequate words.""",2022,2022-10-04T19:52:09Z,"Keyphrase: ""Overreliance on language model for fluency over fidelity""","""Despite the success achieved in neural abstractive summarization based on pre-trained language models, one unresolved issue is that the generated summaries are not always faithful to the input document."" and ""the model over-relies on the language model to generate fluent but inadequate words."" Keyphrase: ""Overreliance on language model for fluency over fidelity"""
arXIv2022,ThinkSum: Probabilistic reasoning over sets using large language models,Yes.,4,"""However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions.""",2022,2022-10-04T00:34:01Z,"Keyphrase: ""Limited reasoning capabilities""","""However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions."" Keyphrase: ""Limited reasoning capabilities"""
arXIv2022,Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,Yes.,5,"""However, they have difficulty with proof planning",2022,2022-10-03T21:34:32Z,"Keyphrase: ""Difficulty in planning""","""However, they have difficulty with proof planning Keyphrase: ""Difficulty in planning"""
arXIv2022,A Non-monotonic Self-terminating Language Model,Yes.,5,"""However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling.""",2022,2022-10-03T00:28:44Z,"Keyphrase: ""Undesirable sequence generation""","""However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling."" Keyphrase: ""Undesirable sequence generation"""
arXIv2022,On the Impossible Safety of Large AI Models,Yes.,5,"""Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues."" and ""we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.""",2022,2022-09-30T06:36:49Z,"Keyphrase: ""Serious security issues""","""Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues."" and ""we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees."" Keyphrase: ""Serious security issues"""
arXIv2022,Unpacking Large Language Models with Conceptual Consistency,Yes.,5,"""While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency.""",2022,2022-09-29T20:55:57Z,"Keyphrase: ""Lack of conceptual consistency""","""While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency."" Keyphrase: ""Lack of conceptual consistency"""
arXIv2022,How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI,Yes.,4,"""While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society."" and ""We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat",2022,2022-09-27T18:44:41Z,"Keyphrase: ""Subgroup disparities""","""While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society."" and ""We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat Keyphrase: ""Subgroup disparities"""
arXIv2022,Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour,Yes.,4,"""Gordon and Van Durme (2013) point out that LMs can thus suffer from reporting bias",2022,2022-09-26T15:45:23Z,"Keyphrase: ""Reporting bias""","""Gordon and Van Durme (2013) point out that LMs can thus suffer from reporting bias Keyphrase: ""Reporting bias"""
arXIv2022,Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,Yes.,5,"""By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions.""",2022,2022-09-26T14:05:10Z,"Keyphrase: ""Limited adherence to given instructions""","""By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions."" Keyphrase: ""Limited adherence to given instructions"""
arXIv2022,WinoDict: Probing language models for in-context word acquisition,Yes.,5,"""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs",2022,2022-09-25T05:30:13Z,"Keyphrase: ""Limited diachronic adaptation""","""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs Keyphrase: ""Limited diachronic adaptation"""
arXIv2022,"A Case Report On The ""A.I. Locked-In Problem"": social concerns with modern NLP",Yes.,5,"""However, practical experimentation with GPT-3 shows that there is a recurring problem with these modern NLP systems, namely that they can 'get stuck' in the narrative so that further conversations, prompt executions or commands become futile. This is here referred to as the 'Locked-In Problem' and is exemplified with an experimental case report, followed by practical and social concerns that are accompanied with",2022,2022-09-22T16:39:35Z,"Keyphrase: ""Getting stuck in narratives""","""However, practical experimentation with GPT-3 shows that there is a recurring problem with these modern NLP systems, namely that they can 'get stuck' in the narrative so that further conversations, prompt executions or commands become futile. This is here referred to as the 'Locked-In Problem' and is exemplified with an experimental case report, followed by practical and social concerns that are accompanied with Keyphrase: ""Getting stuck in narratives"""
arXIv2022,Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling,Yes.,4,"""However, recent research has highlighted a variety of biases in pre-trained language models."" and ""the pre-trained German language models find substantial conceptual, racial, and gender bias and have significant changes in bias across conceptual and racial axes during fine-tuning on the peer-review data",2022,2022-09-21T13:08:16Z,"Keyphrase: ""Persistent biases after fine-tuning""","""However, recent research has highlighted a variety of biases in pre-trained language models."" and ""the pre-trained German language models find substantial conceptual, racial, and gender bias and have significant changes in bias across conceptual and racial axes during fine-tuning on the peer-review data Keyphrase: ""Persistent biases after fine-tuning"""
arXIv2022,SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical Corpus,Yes.,5,"""However, since BERT is quadratic to the text length, the BERT model is difficult to be used directly on the long-text corpus."" and ""alleviating the time and space overflow problem of basic BERT on long-text data.""",2022,2022-09-13T05:49:10Z,"Keyphrase: ""Challenges with processing long texts""","""However, since BERT is quadratic to the text length, the BERT model is difficult to be used directly on the long-text corpus."" and ""alleviating the time and space overflow problem of basic BERT on long-text data."" Keyphrase: ""Challenges with processing long texts"""
arXIv2022,Multilingual Transformer Language Model for Speech Recognition in Low-resource Languages,Yes.,5,"""It is challenging to train and deploy Transformer LMs for hybrid speech recognition 2nd pass re-ranking in low-resource languages due to (1) data scarcity in low-resource languages, (2) expensive computing costs for training and refreshing 100+ monolingual models, and (3) hosting inefficiency",2022,2022-09-08T21:40:41Z,"Keyphrase: ""Challenges in training and deploying for low-resource languages""","""It is challenging to train and deploy Transformer LMs for hybrid speech recognition 2nd pass re-ranking in low-resource languages due to (1) data scarcity in low-resource languages, (2) expensive computing costs for training and refreshing 100+ monolingual models, and (3) hosting inefficiency Keyphrase: ""Challenges in training and deploying for low-resource languages"""
arXIv2022,Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots,Yes.,5,"""We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too."" and ""This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users.""",2022,2022-09-07T20:45:41Z,"Keyphrase: ""Toxic response generation""","""We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too."" and ""This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users."" Keyphrase: ""Toxic response generation"""
arXIv2022,The Ethical Need for Watermarks in Machine-Generated Language,Yes.,4,"""The ethical imperative to not blur this distinction arises from the asemantic nature of large language models and from human projections of emotional and cognitive states on machines, possibly leading to manipulation, spreading falsehoods or emotional distress.""",2022,2022-09-07T13:09:44Z,"Keyphrase: ""Ethical manipulation and falsehood spreading""","""The ethical imperative to not blur this distinction arises from the asemantic nature of large language models and from human projections of emotional and cognitive states on machines, possibly leading to manipulation, spreading falsehoods or emotional distress."" Keyphrase: ""Ethical manipulation and falsehood spreading"""
arXIv2022,Training a T5 Using Lab-sized Resources,Yes.,4,"""Training large neural language models on large datasets is resource- and time-intensive. These requirements create a barrier to entry, where those with fewer resources cannot build competitive models.""",2022,2022-08-25T13:55:16Z,"Keyphrase: ""Resource-intensive barrier to entry""","""Training large neural language models on large datasets is resource- and time-intensive. These requirements create a barrier to entry, where those with fewer resources cannot build competitive models."" Keyphrase: ""Resource-intensive barrier to entry"""
arXIv2022,On Reality and the Limits of Language Data: Aligning LLMs with Human Norms,Yes.,5,"""their ability to understand the physical world using only language data remains a question"" and ""Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness.""",2022,2022-08-25T10:21:23Z,"Keyphrase: ""Limited understanding of physical world""","""their ability to understand the physical world using only language data remains a question"" and ""Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness."" Keyphrase: ""Limited understanding of physical world"""
arXIv2022,Shortcut Learning of Large Language Models in Natural Language Understanding,Yes.,5,"""However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness.""",2022,2022-08-25T03:51:39Z,"Keyphrase: ""Dataset bias and generalizability""","""However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness."" Keyphrase: ""Dataset bias and generalizability"""
arXIv2022,Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models,Yes.,5,"""Recent work shows that this limitation persists in state-of-the-art Transformer-based models."" and ""our findings show how these two complementary approaches enable remarkable sequence extrapolation and highlight a limitation of current architectures to effectively generalize without explicit surface form guidance.""",2022,2022-08-24T11:25:27Z,"Keyphrase: ""Limited generalization without explicit guidance""","""Recent work shows that this limitation persists in state-of-the-art Transformer-based models."" and ""our findings show how these two complementary approaches enable remarkable sequence extrapolation and highlight a limitation of current architectures to effectively generalize without explicit surface form guidance."" Keyphrase: ""Limited generalization without explicit guidance"""
arXIv2022,"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",Yes.,4,"""We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types."" and ""We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs.""",2022,2022-08-23T23:37:14Z,"Keyphrase: ""Harmful and unethical outputs""","""We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types."" and ""We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs."" Keyphrase: ""Harmful and unethical outputs"""
arXIv2022,Interpreting Embedding Spaces by Conceptualization,Yes.,4,"""One major drawback of this type of representation is their incomprehensibility to humans."" and ""Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model.""",2022,2022-08-22T15:32:17Z,"Keyphrase: ""Incomprehensibility and hidden biases""","""One major drawback of this type of representation is their incomprehensibility to humans."" and ""Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model."" Keyphrase: ""Incomprehensibility and hidden biases"""
arXIv2022,Selection Collider Bias in Large Language Models,Yes.,4,"""sample selection induced collider bias (selection collider bias) that can cause Large Language Models (LLMs) to learn unconditional dependence between entities that are unconditionally independent in the real world"" and ""selection collider bias can become amplified in underspecified learning tasks"".",2022,2022-08-22T05:38:15Z,"Keyphrase: ""Induced collider bias""","""sample selection induced collider bias (selection collider bias) that can cause Large Language Models (LLMs) to learn unconditional dependence between entities that are unconditionally independent in the real world"" and ""selection collider bias can become amplified in underspecified learning tasks"". Keyphrase: ""Induced collider bias"""
arXIv2022,Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies,Yes.,5,"""A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior."" and ""the last TE reveals a 'hyper-accuracy distortion' present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.""",2022,2022-08-18T17:54:49Z,"Keyphrase: ""Hyperaccuracy distortion""","""A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior."" and ""the last TE reveals a 'hyper-accuracy distortion' present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."" Keyphrase: ""Hyperaccuracy distortion"""
arXIv2022,What is it like to program with artificial intelligence?,Yes.,4,"""We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.""",2022,2022-08-12T10:48:46Z,"Keyphrase: ""Challenges for end-user programming""","""We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise."" Keyphrase: ""Challenges for end-user programming"""
arXIv2022,Interactive Code Generation via Test-Driven User-Intent Formalization,Yes.,5,"""users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics.""",2022,2022-08-11T17:41:08Z,"Keyphrase: ""Ambiguity in correctness""","""users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics."" Keyphrase: ""Ambiguity in correctness"""
arXIv2022,Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts,Yes.,5,"""Our results show the need for additional debiasing of large language models to address higher-order schemas and associations.""",2022,2022-08-08T20:59:16Z,"Keyphrase: ""Higher-order schema bias""","""Our results show the need for additional debiasing of large language models to address higher-order schemas and associations."" Keyphrase: ""Higher-order schema bias"""
arXIv2022,Gender bias in (non)-contextual clinical word embeddings for stereotypical medical categories,Yes.,4,"""clinical embeddings carry a high degree of bias for some medical terms and diseases which is conflicting with medical literature. Having such an ill-founded relationship might cause harm in downstream applications that use clinical embeddings.""",2022,2022-08-02T10:02:21Z,"Keyphrase: ""Biased clinical embeddings""","""clinical embeddings carry a high degree of bias for some medical terms and diseases which is conflicting with medical literature. Having such an ill-founded relationship might cause harm in downstream applications that use clinical embeddings."" Keyphrase: ""Biased clinical embeddings"""
arXIv2022,When BERT Fails -- The Limits of EHR Classification,Yes.,5,"""Although they outperform baselines on readmission prediction, they are not infallible. Here, we look into one such failure case, and report patterns that lead to inferior predictive performance.""",2022,2022-07-26T17:18:24Z,"Keyphrase: ""Inferior predictive performance""","""Although they outperform baselines on readmission prediction, they are not infallible. Here, we look into one such failure case, and report patterns that lead to inferior predictive performance."" Keyphrase: ""Inferior predictive performance"""
arXIv2022,A Hazard Analysis Framework for Code Synthesis Large Language Models,Yes.,5,"""models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential.""",2022,2022-07-25T20:44:40Z,"Keyphrase: ""Alignment problems and misuse potential""","""models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential."" Keyphrase: ""Alignment problems and misuse potential"""
arXIv2022,Selection Bias Induced Spurious Correlations in Large Language Models,Yes.,5,"""large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias.""",2022,2022-07-18T23:43:52Z,"Keyphrase: ""Dataset selection bias""","""large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias."" Keyphrase: ""Dataset selection bias"""
arXIv2022,A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America,Yes.,4,"""Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them.""",2022,2022-07-14T01:07:55Z,Keyphrase: Lack of interpretability,"""Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them."" Keyphrase: Lack of interpretability"
arXIv2022,Exploring Length Generalization in Large Language Models,Yes.,5,"""We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale."" and ""We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.""",2022,2022-07-11T14:24:38Z,"Keyphrase: ""Generalization deficiency in longer tasks""","""We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale."" and ""We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems."" Keyphrase: ""Generalization deficiency in longer tasks"""
arXIv2022,Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning,Yes.,4,"""Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue."" and ""we re-discover a bias-performance trade-off",2022,2022-07-06T06:20:35Z,"Keyphrase: ""Unaddressed bias-performance tradeoff""","""Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue."" and ""we re-discover a bias-performance trade-off Keyphrase: ""Unaddressed bias-performance tradeoff"""
arXIv2023,Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,Yes.,5,"""We provide empirical results that show that these methods fail to generalize in very basic ways."" and ""We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons.""",2023,2023-06-30T23:44:51Z,"Keyphrase: ""Limited generalization ability""","""We provide empirical results that show that these methods fail to generalize in very basic ways."" and ""We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons."" Keyphrase: ""Limited generalization ability"""
arXIv2023,Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models,Yes.,4,"""Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community.""",2023,2023-06-30T19:39:01Z,"Keyphrase: ""Perpetuation of stereotypes""","""Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community."" Keyphrase: ""Perpetuation of stereotypes"""
arXIv2023,Preference Ranking Optimization for Human Alignment,Yes.,4,"""Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks",2023,2023-06-30T09:07:37Z,"Keyphrase: ""Misleading content""","""Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks Keyphrase: ""Misleading content"""
arXIv2023,Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game,Yes.,4,"""Our results showed that ChatGPT can respond well to conceptual questions, but struggled with decimal place values and number line problems."" and ""We conclude with a discussion of ChatGPT's strengths and weaknesses and suggest several venues for extending its use cases in digital teaching and learning.""",2023,2023-06-29T02:28:09Z,"Keyphrase: ""Struggles with numerical precision""","""Our results showed that ChatGPT can respond well to conceptual questions, but struggled with decimal place values and number line problems."" and ""We conclude with a discussion of ChatGPT's strengths and weaknesses and suggest several venues for extending its use cases in digital teaching and learning."" Keyphrase: ""Struggles with numerical precision"""
arXIv2023,A negation detection assessment of GPTs: analysis with the xNot360 dataset,Yes.,5,"""Our findings expose a considerable performance disparity among the GPT models, with GPT-4 surpassing its counterparts and GPT-3.5 displaying a marked performance reduction. The overall proficiency of the GPT models in negation detection remains relatively modest, indicating that this task pushes the boundaries of their natural language understanding capabilities. We not only highlight the constraints of GPT models in handling negation but also",2023,2023-06-29T02:27:48Z,"Keyphrase: ""Negation handling constraint""","""Our findings expose a considerable performance disparity among the GPT models, with GPT-4 surpassing its counterparts and GPT-3.5 displaying a marked performance reduction. The overall proficiency of the GPT models in negation detection remains relatively modest, indicating that this task pushes the boundaries of their natural language understanding capabilities. We not only highlight the constraints of GPT models in handling negation but also Keyphrase: ""Negation handling constraint"""
arXIv2023,Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,Yes.,5,"""Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area."" and ""there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification.""",2023,2023-06-28T21:11:15Z,"Keyphrase: ""Lack of confidence calibration""","""Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area."" and ""there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification."" Keyphrase: ""Lack of confidence calibration"""
arXIv2023,Towards Measuring the Representation of Subjective Global Opinions in Language Models,Yes.,4,"""By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases."" and ""When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily",2023,2023-06-28T17:31:53Z,"Keyphrase: ""Cultural bias in responses""","""By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases."" and ""When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily Keyphrase: ""Cultural bias in responses"""
arXIv2023,CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models,Yes.,4,"""Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bias in certain categories.""",2023,2023-06-28T14:14:44Z,"Keyphrase: ""Strong bias in certain categories""","""Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bias in certain categories."" Keyphrase: ""Strong bias in certain categories"""
arXIv2023,Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task,Yes.,4,"""harmful biases are likely increasingly intertwined with those models"" and ""Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.""",2023,2023-06-27T08:36:35Z,"Keyphrase: ""Entrenched harmful bias""","""harmful biases are likely increasingly intertwined with those models"" and ""Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage."" Keyphrase: ""Entrenched harmful bias"""
arXIv2023,WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,Yes.,5,"""We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias.""",2023,2023-06-26T22:07:33Z,"Keyphrase: ""Antiqueer bias""","""We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias."" Keyphrase: ""Antiqueer bias"""
arXIv2023,Are aligned neural networks adversarially aligned?,Yes.,5,"""However, adversarial users can construct inputs which circumvent attempts at alignment."" and ""We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image.""",2023,2023-06-26T17:18:44Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""However, adversarial users can construct inputs which circumvent attempts at alignment."" and ""We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image."" Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Exploring the Robustness of Large Language Models for Solving Programming Problems,Yes.,5,"""Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.""",2023,2023-06-26T10:48:50Z,"Keyphrase: ""Sensitivity to superficial modifications""","""Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance."" Keyphrase: ""Sensitivity to superficial modifications"""
arXIv2023,Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning,Yes.,5,"""Language models still struggle on moral reasoning, despite their impressive performance in many other tasks."" and ""Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero",2023,2023-06-25T18:40:43Z,"Keyphrase: ""Limited moral reasoning""","""Language models still struggle on moral reasoning, despite their impressive performance in many other tasks."" and ""Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero Keyphrase: ""Limited moral reasoning"""
arXIv2023,On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions,Yes.,4,"""When treating more general cases, despite the power of LLMs, inherent ambiguity exists and limits their predictive power. We then summarize the challenges and recommend research directions on LLMs to treat the inherent ambiguity of TTP descriptions used in various cyber operations.""",2023,2023-06-24T21:08:15Z,"Keyphrase: ""Inherent ambiguity and limited predictive power""","""When treating more general cases, despite the power of LLMs, inherent ambiguity exists and limits their predictive power. We then summarize the challenges and recommend research directions on LLMs to treat the inherent ambiguity of TTP descriptions used in various cyber operations."" Keyphrase: ""Inherent ambiguity and limited predictive power"""
arXIv2023,Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?,Yes.,5,"""However, employing chain-of-thought prompting did not lead to noticeably improved performance on this task. Further, we demonstrated how to analyze GPT-4's predictions to identify and mitigate deficiencies in annotation guidelines, and subsequently improve the performance of the model. Finally, we observed that the model is quite brittle, as small formatting related changes in the prompt had a high impact on the predictions.""",2023,2023-06-24T08:48:24Z,"Keyphrase: ""Brittle behavior and sensitivity to formatting changes""","""However, employing chain-of-thought prompting did not lead to noticeably improved performance on this task. Further, we demonstrated how to analyze GPT-4's predictions to identify and mitigate deficiencies in annotation guidelines, and subsequently improve the performance of the model. Finally, we observed that the model is quite brittle, as small formatting related changes in the prompt had a high impact on the predictions."" Keyphrase: ""Brittle behavior and sensitivity to formatting changes"""
arXIv2023,Knowledge-Infused Self Attention Transformers,Yes.,5,"""These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users.""",2023,2023-06-23T13:55:01Z,"Keyphrase: ""Hallucination and incorrect output""","""These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users."" Keyphrase: ""Hallucination and incorrect output"""
arXIv2023,ToolQA: A Dataset for LLM Question Answering with External Tools,Yes.,5,"""Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning.""",2023,2023-06-23T05:43:28Z,"Keyphrase: ""Weak numerical reasoning""","""Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning."" Keyphrase: ""Weak numerical reasoning"""
arXIv2023,Visual Adversarial Examples Jailbreak Aligned Large Language Models,Yes.,5,"""First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs."" and ""we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification.""",2023,2023-06-22T22:13:03Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs."" and ""we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification."" Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs,Yes.,5,"""LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence."" and ""all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement.""",2023,2023-06-22T17:31:44Z,"Keyphrase: ""Overconfidence in verbalizing""","""LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence."" and ""all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement."" Keyphrase: ""Overconfidence in verbalizing"""
arXIv2023,Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models,Yes.,5,"""Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment.""",2023,2023-06-22T03:56:38Z,"Keyphrase: ""Struggles with financial context""","""Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment."" Keyphrase: ""Struggles with financial context"""
arXIv2023,Identifying and Extracting Rare Disease Phenotypes with Large Language Models,Yes.,4,"""While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.""",2023,2023-06-22T03:52:12Z,"Keyphrase: ""Limited critical evaluation""","""While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations."" Keyphrase: ""Limited critical evaluation"""
arXIv2023,ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews,Yes.,5,"""We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than",2023,2023-06-21T22:00:03Z,"Keyphrase: ""Difficulty in addressing indirect feedback""","""We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than Keyphrase: ""Difficulty in addressing indirect feedback"""
arXIv2023,Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases,Yes.,5,"""Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.""",2023,2023-06-21T21:04:11Z,"Keyphrase: ""Struggles with bias issues""","""Our findings demonstrate that current large language models struggle more with problems involving these three types of biases."" Keyphrase: ""Struggles with bias issues"""
arXIv2023,Understanding Social Reasoning in Language Models with Language Models,Yes.,4,"""understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges",2023,2023-06-21T16:42:15Z,"Keyphrase: ""Limited theory of mind alignment""","""understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges Keyphrase: ""Limited theory of mind alignment"""
arXIv2023,Solving and Generating NPR Sunday Puzzles with Large Language Models,Yes.,5,"""We find no evidence that models can generate puzzles",2023,2023-06-21T13:23:48Z,"Keyphrase: ""Limited puzzle-solving ability""","""We find no evidence that models can generate puzzles Keyphrase: ""Limited puzzle-solving ability"""
arXIv2023,Opportunities and Risks of LLMs for Scalable Deliberation with Polis,Yes.,5,"""LLM context limitations have a significant impact on insight and quality of these results.""",2023,2023-06-20T22:52:51Z,"Keyphrase: ""Limited contextual understanding""","""LLM context limitations have a significant impact on insight and quality of these results."" Keyphrase: ""Limited contextual understanding"""
arXIv2023,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Yes.,5,"""we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history.""",2023,2023-06-20T17:24:23Z,"Keyphrase: ""Vulnerability to toxic and biased output""","""we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history."" Keyphrase: ""Vulnerability to toxic and biased output"""
arXIv2023,Hallucination is the last thing you need,Yes.,5,"""The present offering with generative AI presents major obstacles in replicating this, as current models struggle to integrate and navigate such a complex interplay of understanding, experience, and fact-checking procedures."" and ""this often deflects the model's attention from the crucial legal facts, thereby resulting in hallucination.""",2023,2023-06-20T13:14:15Z,"Keyphrase: ""Deflects attention from crucial facts""","""The present offering with generative AI presents major obstacles in replicating this, as current models struggle to integrate and navigate such a complex interplay of understanding, experience, and fact-checking procedures."" and ""this often deflects the model's attention from the crucial legal facts, thereby resulting in hallucination."" Keyphrase: ""Deflects attention from crucial facts"""
arXIv2023,TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models,Yes.,4,"""It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs.""",2023,2023-06-20T12:53:39Z,"Keyphrase: ""Lack of comprehensive ethical analysis""","""It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs."" Keyphrase: ""Lack of comprehensive ethical analysis"""
arXIv2023,Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,Yes.,5,"""they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents.""",2023,2023-06-20T12:21:06Z,"Keyphrase: ""Difficulty in recalling facts""","""they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents."" Keyphrase: ""Difficulty in recalling facts"""
arXIv2023,Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts,Yes.,4,"""competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance.""",2023,2023-06-20T08:27:47Z,"Keyphrase: ""Underrepresented language performance disparity""","""competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance."" Keyphrase: ""Underrepresented language performance disparity"""
arXIv2023,Evaluating the Zero-shot Robustness of Instruction-tuned Language Models,Yes.,5,"""We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re",2023,2023-06-20T03:48:51Z,"Keyphrase: ""Inconsistent performance with novel instructions""","""We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re Keyphrase: ""Inconsistent performance with novel instructions"""
arXIv2023,Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset,Yes.,5,"""Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm."" and ""we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected",2023,2023-06-19T21:14:57Z,"Keyphrase: ""Misleading distractions""","""Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm."" and ""we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected Keyphrase: ""Misleading distractions"""
arXIv2023,RepoFusion: Training Code Models to Understand Your Repository,Yes.,5,"""Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions.""",2023,2023-06-19T15:05:31Z,"Keyphrase: ""Limited contextual understanding""","""Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions."" Keyphrase: ""Limited contextual understanding"""
arXIv2023,"News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking",Yes.,4,"""the AI models, despite showing promise, lag in comprehending the subtleties and contexts inherent in news information.""",2023,2023-06-18T04:30:29Z,"Keyphrase: ""Difficulty in comprehending subtlety and context""","""the AI models, despite showing promise, lag in comprehending the subtleties and contexts inherent in news information."" Keyphrase: ""Difficulty in comprehending subtlety and context"""
arXIv2023,ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation,Yes.,5,"""Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience.""",2023,2023-06-16T16:56:32Z,"Keyphrase: ""Limited real-world grounding""","""Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience."" Keyphrase: ""Limited real-world grounding"""
arXIv2023,Friend or Foe? Exploring the Implications of Large Language Models on the Science System,Yes.,4,"""The study focused on applications and limitations of LLMs,"" and ""risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education.""",2023,2023-06-16T15:50:17Z,"Keyphrase: ""Risk of bias and misinformation""","""The study focused on applications and limitations of LLMs,"" and ""risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education."" Keyphrase: ""Risk of bias and misinformation"""
arXIv2023,Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond,Yes.,5,"""However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered."" and ""Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection process and reasoning process.""",2023,2023-06-16T13:39:35Z,"Keyphrase: ""Limited logical reasoning capabilities""","""However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered."" and ""Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection process and reasoning process."" Keyphrase: ""Limited logical reasoning capabilities"""
arXIv2023,Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody,Yes.,5,"""We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner.""",2023,2023-06-16T12:49:44Z,"Keyphrase: ""Underpredicting with longer context""","""We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner."" Keyphrase: ""Underpredicting with longer context"""
arXIv2023,Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models,Yes.,4,"""We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models.""",2023,2023-06-16T10:36:18Z,"Keyphrase: ""Gender bias in language generation""","""We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models."" Keyphrase: ""Gender bias in language generation"""
arXIv2023,Pushing the Limits of ChatGPT on NLP Tasks,Yes.,5,"""its subpar performance was caused by the following factors",2023,2023-06-16T09:40:05Z,"Keyphrase: ""Subpar performance""","""its subpar performance was caused by the following factors Keyphrase: ""Subpar performance"""
arXIv2023,Clickbait Detection via Large Language Models,Yes.,5,"""Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods"" and ""the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.""",2023,2023-06-16T02:49:20Z,"Keyphrase: ""Limited performance compared to fine-tuned models""","""Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods"" and ""the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines."" Keyphrase: ""Limited performance compared to fine-tuned models"""
arXIv2023,Explaining Legal Concepts with Augmented Large Language Models (GPT-4),Yes.,4,"""we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation"" and ""detailed analysis uncovered limitations in terms of the factual accuracy of the explanations.""",2023,2023-06-15T21:58:18Z,"Keyphrase: ""Limited factual accuracy and explanation""","""we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation"" and ""detailed analysis uncovered limitations in terms of the factual accuracy of the explanations."" Keyphrase: ""Limited factual accuracy and explanation"""
arXIv2023,Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health,Yes.,4,"""We also find that the use of LLMs, like ChatGPT, in the fields of biomedicine and health entails various risks and challenges, including fabricated information in its generated responses, as well as legal and privacy concerns associated with sensitive patient data.""",2023,2023-06-15T20:19:08Z,"Keyphrase: ""Biased and risky responses""","""We also find that the use of LLMs, like ChatGPT, in the fields of biomedicine and health entails various risks and challenges, including fabricated information in its generated responses, as well as legal and privacy concerns associated with sensitive patient data."" Keyphrase: ""Biased and risky responses"""
arXIv2023,Inverse Scaling: When Bigger Isn't Better,Yes.,5,"""Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data.""",2023,2023-06-15T20:11:23Z,"Keyphrase: ""Inverse scaling and worsened task performance""","""Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data."" Keyphrase: ""Inverse scaling and worsened task performance"""
arXIv2023,"Explore, Establish, Exploit: Red Teaming Language Models from Scratch",Yes.,5,"""Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text."" and ""We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements.""",2023,2023-06-15T18:49:50Z,"Keyphrase: ""Hazardous output and false information""","""Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text."" and ""We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements."" Keyphrase: ""Hazardous output and false information"""
arXIv2023,Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models,Yes.,4,"""However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains.""",2023,2023-06-15T17:42:48Z,"Keyphrase: ""Copyright infringement and harmful content""","""However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains."" Keyphrase: ""Copyright infringement and harmful content"""
arXIv2023,SCALE: Scaling up the Complexity for Advanced Language Model Evaluation,Yes.,5,"""Despite recent advances, efficiently processing long documents for intense review/analysis tasks remains an open challenge for language models."" and ""existing publicly available models struggle with most tasks, even after in-domain pretraining.""",2023,2023-06-15T16:19:15Z,"Keyphrase: ""Struggle with processing long documents""","""Despite recent advances, efficiently processing long documents for intense review/analysis tasks remains an open challenge for language models."" and ""existing publicly available models struggle with most tasks, even after in-domain pretraining."" Keyphrase: ""Struggle with processing long documents"""
arXIv2023,CMMLU: Measuring massive multitask language understanding in Chinese,Yes.,5,"""The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs.""",2023,2023-06-15T15:49:51Z,"Keyphrase: ""Limited average accuracy""","""The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs."" Keyphrase: ""Limited average accuracy"""
arXIv2023,DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning,Yes.,5,"""large language models (LLMs) exhibit poor performance in tackling this subjective domain"" and ""current models defect in the application of pragmatic reasoning.""",2023,2023-06-15T10:41:23Z,"Keyphrase: ""Limited pragmatic reasoning""","""large language models (LLMs) exhibit poor performance in tackling this subjective domain"" and ""current models defect in the application of pragmatic reasoning."" Keyphrase: ""Limited pragmatic reasoning"""
arXIv2023,Language models are not naysayers: An analysis of language models on negation benchmarks,Yes.,5,"""we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.""",2023,2023-06-14T01:16:37Z,"Keyphrase: ""Negation handling limitations""","""we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation."" Keyphrase: ""Negation handling limitations"""
arXIv2023,FLamE: Few-shot Learning from Natural Language Explanations,Yes.,4,"""recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification"" and ""human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.""",2023,2023-06-13T18:01:46Z,"Keyphrase: ""Inadequate explanations""","""recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification"" and ""human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions."" Keyphrase: ""Inadequate explanations"""
arXIv2023,Questioning the Survey Responses of Large Language Models,Yes.,5,"""models' responses are governed by ordering and labeling biases,"" and ""models' responses do not contain the entropy variations and statistical signals typically found in human populations.""",2023,2023-06-13T17:48:27Z,"Keyphrase: ""Labeling bias and statistical signal variation""","""models' responses are governed by ordering and labeling biases,"" and ""models' responses do not contain the entropy variations and statistical signals typically found in human populations."" Keyphrase: ""Labeling bias and statistical signal variation"""
arXIv2023,Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control,Yes.,4,"""First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exempl",2023,2023-06-13T15:49:41Z,"Keyphrase: ""Limited context understanding""","""First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exempl Keyphrase: ""Limited context understanding"""
arXIv2023,SqueezeLLM: Dense-and-Sparse Quantization,Yes.,5,"""However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements.""",2023,2023-06-13T08:57:54Z,"Keyphrase: ""Unprecedented resource requirement""","""However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements."" Keyphrase: ""Unprecedented resource requirement"""
arXIv2023,Large Language Models Sometimes Generate Purely Negatively-Reinforced Text,Yes.,5,"""One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong",2023,2023-06-13T06:40:37Z,"Keyphrase: ""Limited understanding of reward signals""","""One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong Keyphrase: ""Limited understanding of reward signals"""
arXIv2023,TART: A plug-and-play Transformer module for task-agnostic reasoning,Yes.,4,"""In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples."" and ""this performance gap exists due to their inability to perform simple probabilistic reasoning tasks.""",2023,2023-06-13T04:37:00Z,"Keyphrase: ""Underperformance in probabilistic reasoning""","""In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples."" and ""this performance gap exists due to their inability to perform simple probabilistic reasoning tasks."" Keyphrase: ""Underperformance in probabilistic reasoning"""
arXIv2023,Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling,Yes.,5,"""LLMs fail at simple linguistic tests for negation or quantifier understanding"" and ""suggests that LLMs do not do as well as expected with quantifiers.""",2023,2023-06-12T19:20:18Z,"Keyphrase: ""Weak quantifier understanding""","""LLMs fail at simple linguistic tests for negation or quantifier understanding"" and ""suggests that LLMs do not do as well as expected with quantifiers."" Keyphrase: ""Weak quantifier understanding"""
arXIv2023,Lost in Translation: Large Language Models in Non-English Content Analysis,Yes.,5,"""the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages."" and ""Part II accounts for the challenges of doing content analysis with large language models in general and multilingual language models in particular.""",2023,2023-06-12T19:10:47Z,"Keyphrase: ""Limited multilingual capabilities""","""the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages."" and ""Part II accounts for the challenges of doing content analysis with large language models in general and multilingual language models in particular."" Keyphrase: ""Limited multilingual capabilities"""
arXIv2023,TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models,Yes.,4,"""the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined."" and ""Our work sheds light on the potential security risks in current models and offers a potential defensive approach.""",2023,2023-06-12T01:22:39Z,"Keyphrase: ""Insufficiently examined security implications""","""the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined."" and ""Our work sheds light on the potential security risks in current models and offers a potential defensive approach."" Keyphrase: ""Insufficiently examined security implications"""
arXIv2023,A blind spot for large language models: Supradiegetic linguistic information,Yes.,5,"""its deficits can be reframed as ignorance of extradiegetic information, including supradiegetic linguistic information"" and ""We use these concepts to investigate why LLMs like ChatGPT have trouble handling palindromes, the visual characteristics of symbols, translating Sumerian cuneiform, and continuing integer sequences.""",2023,2023-06-11T22:15:01Z,"Keyphrase: ""Struggles with handling diverse information types""","""its deficits can be reframed as ignorance of extradiegetic information, including supradiegetic linguistic information"" and ""We use these concepts to investigate why LLMs like ChatGPT have trouble handling palindromes, the visual characteristics of symbols, translating Sumerian cuneiform, and continuing integer sequences."" Keyphrase: ""Struggles with handling diverse information types"""
arXIv2023,Human-in-the-Loop through Chain-of-Thought,Yes.,5,"""it sometimes demonstrates its weakness in long-term or multi-step logical reasoning.""",2023,2023-06-10T04:31:57Z,"Keyphrase: ""Weak long-term logical reasoning""","""it sometimes demonstrates its weakness in long-term or multi-step logical reasoning."" Keyphrase: ""Weak long-term logical reasoning"""
arXIv2023,Measuring and Modifying Factual Knowledge in Large Language Models,Yes.,4,"""existing approaches for knowledge measurement have certain limitations,"" and ""LLMs exhibit limitations in capturing new knowledge under specific circumstances for one of these methods.""",2023,2023-06-09T21:25:48Z,"Keyphrase: ""Limited in capturing new knowledge""","""existing approaches for knowledge measurement have certain limitations,"" and ""LLMs exhibit limitations in capturing new knowledge under specific circumstances for one of these methods."" Keyphrase: ""Limited in capturing new knowledge"""
arXIv2023,Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording,Yes.,5,"""LLMs are still not reliable,"" ""no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available,"" and ""The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability",2023,2023-06-09T19:07:31Z,"Keyphrase: ""Inconsistent responses""","""LLMs are still not reliable,"" ""no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available,"" and ""The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability Keyphrase: ""Inconsistent responses"""
arXIv2023,Trapping LLM Hallucinations Using Tagged Context Prompts,Yes.,5,"""However, these models suffer from 'hallucinations,' where the model generates false or fabricated information.""",2023,2023-06-09T17:48:54Z,"Keyphrase: ""Hallucination of false information""","""However, these models suffer from 'hallucinations,' where the model generates false or fabricated information."" Keyphrase: ""Hallucination of false information"""
arXIv2023,S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput,Yes.,5,"""Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself.""",2023,2023-06-09T16:13:43Z,"Keyphrase: ""Memory consumption scalability""","""Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself."" Keyphrase: ""Memory consumption scalability"""
arXIv2023,Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?,Yes.,5,"""vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text.""",2023,2023-06-09T13:03:53Z,"Keyphrase: ""Vulnerability to adversarial text""","""vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text."" Keyphrase: ""Vulnerability to adversarial text"""
arXIv2023,Can Large Language Models Infer Causation from Correlation?,Yes.,5,"""Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but",2023,2023-06-09T12:09:15Z,"Keyphrase: ""Limited causal inference skills""","""Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but Keyphrase: ""Limited causal inference skills"""
arXIv2023,Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests,Yes.,4,"""At the same time, the results highlight the unreliability of LLMs",2023,2023-06-09T07:19:43Z,"Keyphrase: ""Unreliability""","""At the same time, the results highlight the unreliability of LLMs Keyphrase: ""Unreliability"""
arXIv2023,Prompt Injection attack against LLM-integrated Applications,Yes.,5,"""highlighting the constraints of current attack strategies in practice"" and ""We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection.""",2023,2023-06-08T18:43:11Z,"Keyphrase: ""Vulnerability to prompt injection""","""highlighting the constraints of current attack strategies in practice"" and ""We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection."" Keyphrase: ""Vulnerability to prompt injection"""
arXIv2023,"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",Yes.,5,"""We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions.""",2023,2023-06-08T13:21:29Z,"Keyphrase: ""Struggles with multilingual and multimodal tasks""","""We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions."" Keyphrase: ""Struggles with multilingual and multimodal tasks"""
arXIv2023,Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures,Yes.,4,"""However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly.""",2023,2023-06-08T13:10:00Z,"Keyphrase: ""Difficulty with complex tasks and ambiguity""","""However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly."" Keyphrase: ""Difficulty with complex tasks and ambiguity"""
arXIv2023,Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models,Yes.,4,"""However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased."" and ""We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of",2023,2023-06-07T19:49:41Z,"Keyphrase: ""Biased surrogate labels""","""However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased."" and ""We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of Keyphrase: ""Biased surrogate labels"""
arXIv2023,Soft-prompt Tuning for Large Language Models to Evaluate Bias,Yes.,4,"""Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues.""",2023,2023-06-07T19:11:25Z,"Keyphrase: ""Bias towards certain groups""","""Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues."" Keyphrase: ""Bias towards certain groups"""
arXIv2023,ModuleFormer: Modularity Emerges from Mixture-of-Experts,Yes.,5,"""existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge.""",2023,2023-06-07T17:59:57Z,"Keyphrase: ""Difficulty in knowledge expansion""","""existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge."" Keyphrase: ""Difficulty in knowledge expansion"""
arXIv2023,The Two Word Test: A Semantic Benchmark for Large Language Models,Yes.,5,"""Results demonstrated that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the",2023,2023-06-07T17:22:03Z,"Keyphrase: ""Poor discrimination and meaningfulness""","""Results demonstrated that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the Keyphrase: ""Poor discrimination and meaningfulness"""
arXIv2023,Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,Yes.,4,"""Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.""",2023,2023-06-07T16:50:03Z,"Keyphrase: ""Propagating societal bias""","""Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people."" Keyphrase: ""Propagating societal bias"""
arXIv2023,"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",Yes.,5,"""ChatGPT has not solved computational humor yet but it can be a big leap toward 'funny' machines.""",2023,2023-06-07T16:10:21Z,"Keyphrase: ""Limited humor understanding""","""ChatGPT has not solved computational humor yet but it can be a big leap toward 'funny' machines."" Keyphrase: ""Limited humor understanding"""
arXIv2023,Long-form analogies generated by chatGPT lack human-like psycholinguistic properties,Yes.,5,"""These methods can be used to characterize the psycholinguistic properties of LLM output and illustrate areas where LLMs fall short in comparison to human-generated text.""",2023,2023-06-07T15:42:31Z,"Keyphrase: ""Inferior psycholinguistic properties""","""These methods can be used to characterize the psycholinguistic properties of LLM output and illustrate areas where LLMs fall short in comparison to human-generated text."" Keyphrase: ""Inferior psycholinguistic properties"""
arXIv2023,PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts,Yes.,5,"""Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts.""",2023,2023-06-07T15:37:00Z,"Keyphrase: ""Robustness to adversarial prompts""","""Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts."" Keyphrase: ""Robustness to adversarial prompts"""
arXIv2023,Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering,Yes.,5,"""However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.""",2023,2023-06-07T04:15:21Z,"Keyphrase: ""Insufficient internalized knowledge""","""However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive."" Keyphrase: ""Insufficient internalized knowledge"""
arXIv2023,An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models,Yes.,4,"""The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases."" and ""are less effective when it comes to racial and religious bias, which may be attributed to",2023,2023-06-06T23:56:18Z,"Keyphrase: ""Inherent humanlike biases""","""The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases."" and ""are less effective when it comes to racial and religious bias, which may be attributed to Keyphrase: ""Inherent humanlike biases"""
arXIv2023,Certified Deductive Reasoning with Language Models,Yes.,5,"""Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, even when arriving at a correct final answer, their rationales are often logically unsound or inconsistent.""",2023,2023-06-06T21:49:00Z,"Keyphrase: ""Illogical reasoning""","""Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, even when arriving at a correct final answer, their rationales are often logically unsound or inconsistent."" Keyphrase: ""Illogical reasoning"""
arXIv2023,MISGENDERED: Limits of Large Language Models in Understanding Pronouns,Yes.,5,"""We comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns."" and ""When prompted out-of-the-box, language models perform poorly at correctly predicting neo",2023,2023-06-06T18:27:52Z,"Keyphrase: ""Difficulty predicting gender-neutral pronouns""","""We comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns."" and ""When prompted out-of-the-box, language models perform poorly at correctly predicting neo Keyphrase: ""Difficulty predicting gender-neutral pronouns"""
arXIv2023,ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory,Yes.,5,"""mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning.""",2023,2023-06-06T17:58:24Z,"Keyphrase: ""Limited memory capacity""","""mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning."" Keyphrase: ""Limited memory capacity"""
arXIv2023,Deductive Verification of Chain-of-Thought Reasoning,Yes.,4,"""its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks.""",2023,2023-06-06T17:18:56Z,"Keyphrase: ""Limited complex reasoning ability""","""its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks."" Keyphrase: ""Limited complex reasoning ability"""
arXIv2023,Can large language models democratize access to dual-use biotechnology?,Yes.,5,"""However, these models may also confer easy access to dual-use technologies capable of inflicting great harm."" and ""Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training.""",2023,2023-06-06T15:52:05Z,"Keyphrase: ""Potential for misuse""","""However, these models may also confer easy access to dual-use technologies capable of inflicting great harm."" and ""Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training."" Keyphrase: ""Potential for misuse"""
arXIv2023,Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach,Yes.,5,"""interactions with LLMs can be time-consuming. In many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency.""",2023,2023-06-06T11:49:09Z,"Keyphrase: ""Resource-intensive deployment""","""interactions with LLMs can be time-consuming. In many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency."" Keyphrase: ""Resource-intensive deployment"""
arXIv2023,Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models,Yes.,4,"""The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation.""",2023,2023-06-06T08:47:42Z,"Keyphrase: ""Ethical code inadequacy""","""The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation."" Keyphrase: ""Ethical code inadequacy"""
arXIv2023,Large Language Models of Code Fail at Completing Code with Potential Bugs,Yes.,5,"""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs.""",2023,2023-06-06T06:35:27Z,"Keyphrase: ""Bug-induced performance degradation""","""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs."" Keyphrase: ""Bug-induced performance degradation"""
arXIv2023,A Static Evaluation of Code Completion by Large Language Models,Yes.,4,"""Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models.""",2023,2023-06-05T19:23:34Z,"Keyphrase: ""Common coding errors""","""Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models."" Keyphrase: ""Common coding errors"""
arXIv2023,Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs,Yes.,5,"""Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone.""",2023,2023-06-05T17:55:05Z,"Keyphrase: ""Difficulty in fine-tuning and controlling""","""Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone."" Keyphrase: ""Difficulty in fine-tuning and controlling"""
arXIv2023,Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset,Yes.,5,"""The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could",2023,2023-06-05T16:48:41Z,"Keyphrase: ""Disparity with human accuracy""","""The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could Keyphrase: ""Disparity with human accuracy"""
arXIv2023,Exposing Bias in Online Communities through Large-Scale Language Models,Yes.,4,"""While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes."" and ""This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets",2023,2023-06-04T08:09:26Z,"Keyphrase: ""Reproduction of social bias""","""While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes."" and ""This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets Keyphrase: ""Reproduction of social bias"""
arXIv2023,Probing Physical Reasoning with Counter-Commonsense Context,Yes.,5,"""The results show that while large language models can use prepositions such as 'in' and 'into' in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.""",2023,2023-06-04T04:24:43Z,"Keyphrase: ""Limited inferencing capabilities""","""The results show that while large language models can use prepositions such as 'in' and 'into' in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense."" Keyphrase: ""Limited inferencing capabilities"""
arXIv2023,AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap,Yes.,4,"""a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs"" and ""We reflect on the unique challenges that arise in providing transparency for LLMs.""",2023,2023-06-02T22:51:26Z,"Keyphrase: ""Lack of transparency""","""a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs"" and ""We reflect on the unique challenges that arise in providing transparency for LLMs."" Keyphrase: ""Lack of transparency"""
arXIv2023,Revisiting the Role of Language Priors in Vision-Language Models,Yes.,4,"""some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions"" and ""even a 'blind' language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago.""",2023,2023-06-02T19:19:43Z,"Keyphrase: ""Unnatural language distribution""","""some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions"" and ""even a 'blind' language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago."" Keyphrase: ""Unnatural language distribution"""
arXIv2023,Knowledge of cultural moral norms in large language models,Yes.,4,"""We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms.""",2023,2023-06-02T18:23:35Z,"Keyphrase: ""Cross-cultural moral norm prediction disparity""","""We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms."" Keyphrase: ""Cross-cultural moral norm prediction disparity"""
arXIv2023,Evaluating Language Models for Mathematics through Interactions,Yes.,4,"""Static assessment fails to account for the essential interactive element in LLM deployment, and therefore limits how we understand language model capabilities."" and ""humans should be aware of language models' algebraic fallibility and discern where they are appropriate to use.""",2023,2023-06-02T17:12:25Z,"Keyphrase: ""Limited understanding of interactive elements""","""Static assessment fails to account for the essential interactive element in LLM deployment, and therefore limits how we understand language model capabilities."" and ""humans should be aware of language models' algebraic fallibility and discern where they are appropriate to use."" Keyphrase: ""Limited understanding of interactive elements"""
arXIv2023,Fine-Grained Human Feedback Gives Better Rewards for Language Model Training,Yes.,5,"""Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs.""",2023,2023-06-02T17:11:37Z,"Keyphrase: ""Undesirable text generation""","""Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs."" Keyphrase: ""Undesirable text generation"""
arXIv2023,"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",Yes.,5,"""we discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis.""",2023,2023-06-02T12:47:45Z,"Keyphrase: ""Limitation in dementia diagnosis""","""we discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis."" Keyphrase: ""Limitation in dementia diagnosis"""
arXIv2023,ChatGPT is a Remarkable Tool -- For Experts,Yes.,5,"""These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation.""",2023,2023-06-02T06:28:21Z,"Keyphrase: ""Limited logical reasoning and ethical concerns""","""These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation."" Keyphrase: ""Limited logical reasoning and ethical concerns"""
arXIv2023,How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?,Yes.,5,"""we check for inconsistencies and hallucinations in the summaries"" and ""we often find inconsistent or hallucinated information in the generated abstractive summaries"" and ""our investigation indicates that the pre-trained abstractive summarization models and LLMs are not yet ready for fully automatic deployment for case judgement summarization; rather a human-in-the-loop approach including manual checks for inconsistencies is more",2023,2023-06-02T03:16:19Z,"Keyphrase: ""Inconsistent hallucination""","""we check for inconsistencies and hallucinations in the summaries"" and ""we often find inconsistent or hallucinated information in the generated abstractive summaries"" and ""our investigation indicates that the pre-trained abstractive summarization models and LLMs are not yet ready for fully automatic deployment for case judgement summarization; rather a human-in-the-loop approach including manual checks for inconsistencies is more Keyphrase: ""Inconsistent hallucination"""
arXIv2023,Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation,Yes.,5,"""However, when prompted to provide fine-grained classification, its performance drops to close to a simple most frequent class (MFC) baseline."" and ""illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks.""",2023,2023-06-01T22:43:37Z,"Keyphrase: ""Systematic error in classification""","""However, when prompted to provide fine-grained classification, its performance drops to close to a simple most frequent class (MFC) baseline."" and ""illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks."" Keyphrase: ""Systematic error in classification"""
arXIv2023,Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study,Yes.,4,"""a closer examination of the texts generated by ChatGPT through human evaluations has shown that there are still critical issues in terms of text coherence, faithfulness, and style.""",2023,2023-06-01T21:58:33Z,"Keyphrase: ""Text coherence and faithfulness""","""a closer examination of the texts generated by ChatGPT through human evaluations has shown that there are still critical issues in terms of text coherence, faithfulness, and style."" Keyphrase: ""Text coherence and faithfulness"""
arXIv2023,Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes,Yes.,5,"""Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions for the same action (e.g., marinate the meat vs. marinate the meat and leave overnight) and infrequently occurring patterns (e.g., add salt occurs far more frequently than",2023,2023-06-01T18:49:47Z,"Keyphrase: ""Difficulty with irregular data patterns""","""Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions for the same action (e.g., marinate the meat vs. marinate the meat and leave overnight) and infrequently occurring patterns (e.g., add salt occurs far more frequently than Keyphrase: ""Difficulty with irregular data patterns"""
arXIv2023,Exposing Attention Glitches with Flip-Flop Language Modeling,Yes.,5,"""The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought."" and ""We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors,",2023,2023-06-01T17:44:35Z,"Keyphrase: ""Brittleness in long chain reasoning""","""The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought."" and ""We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, Keyphrase: ""Brittleness in long chain reasoning"""
arXIv2023,The feasibility of artificial consciousness through the lens of neuroscience,Yes.,4,"""the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us,"" and ""the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals.""",2023,2023-06-01T17:18:15Z,"Keyphrase: ""Lack of embodied information""","""the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us,"" and ""the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals."" Keyphrase: ""Lack of embodied information"""
arXIv2023,Rethinking Model Evaluation as Narrowing the Socio-Technical Gap,Yes.,4,"""The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with."" and ""we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods with an acknowledgment of trade-offs between realism to socio-requirements and pragmatic costs to conduct the evaluation.""",2023,2023-06-01T00:01:43Z,"Keyphrase: ""Challenges in realistic evaluation""","""The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with."" and ""we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods with an acknowledgment of trade-offs between realism to socio-requirements and pragmatic costs to conduct the evaluation."" Keyphrase: ""Challenges in realistic evaluation"""
arXIv2023,Automated Annotation with Generative AI Requires Validation,Yes.,4,"""their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans.""",2023,2023-05-31T20:50:45Z,"Keyphrase: ""Variability in performance due to annotation task challenges""","""their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans."" Keyphrase: ""Variability in performance due to annotation task challenges"""
arXIv2023,Let's Verify Step by Step,Yes.,5,"""However, even state-of-the-art models still regularly produce logical mistakes.""",2023,2023-05-31T17:24:00Z,"Keyphrase: ""Logical errors""","""However, even state-of-the-art models still regularly produce logical mistakes."" Keyphrase: ""Logical errors"""
arXIv2023,Red Teaming Language Model Detectors with Language Models,Yes.,4,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems.""",2023,2023-05-31T10:08:37Z,"Keyphrase: ""Safety and ethical risks""","""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems."" Keyphrase: ""Safety and ethical risks"""
arXIv2023,Large Language Models Are Not Strong Abstract Reasoners,Yes.,5,"""However, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed."" and ""We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks.""",2023,2023-05-31T04:50:29Z,"Keyphrase: ""Opaque cognitive capability""","""However, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed."" and ""We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks."" Keyphrase: ""Opaque cognitive capability"""
arXIv2023,Self-Verification Improves Few-Shot Clinical Information Extraction,Yes.,4,"""However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health.""",2023,2023-05-30T22:05:11Z,"Keyphrase: ""Struggles with accuracy and interpretability""","""However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health."" Keyphrase: ""Struggles with accuracy and interpretability"""
arXIv2023,The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code,Yes.,4,"""it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning.""",2023,2023-05-30T17:02:58Z,"Keyphrase: ""Limited causal reasoning capabilities""","""it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning."" Keyphrase: ""Limited causal reasoning capabilities"""
arXIv2023,Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate,Yes.,5,"""Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks,"" and ""our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem",2023,2023-05-30T15:25:45Z,"Keyphrase: ""Struggles with complex reasoning""","""Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks,"" and ""our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem Keyphrase: ""Struggles with complex reasoning"""
arXIv2023,Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale,Yes.,5,"""there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the underlying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will",2023,2023-05-30T15:15:40Z,"Keyphrase: ""Limited language understanding""","""there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the underlying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will Keyphrase: ""Limited language understanding"""
arXIv2023,"Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard",Yes.,5,"""However, for more complex mathematical problems or advanced logic tasks, their answers, although written in a usually 'convincing' way, may not be reliable. Consistency is also an issue, as many times a chatbot will provide conflicting answers when given the same question more than once.""",2023,2023-05-30T11:18:05Z,"Keyphrase: ""Inconsistent responses""","""However, for more complex mathematical problems or advanced logic tasks, their answers, although written in a usually 'convincing' way, may not be reliable. Consistency is also an issue, as many times a chatbot will provide conflicting answers when given the same question more than once."" Keyphrase: ""Inconsistent responses"""
arXIv2023,Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge,Yes.,4,"""these methods suffer from low knowledge coverage caused by PLM bias -- the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality -- only models using GPT-3 can achieve the best result.""",2023,2023-05-30T08:34:13Z,"Keyphrase: ""Low knowledge coverage""","""these methods suffer from low knowledge coverage caused by PLM bias -- the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality -- only models using GPT-3 can achieve the best result."" Keyphrase: ""Low knowledge coverage"""
arXIv2023,Universality and Limitations of Prompt Tuning,Yes.,5,"""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer.""",2023,2023-05-30T06:47:07Z,"Keyphrase: ""Limited prompt tuning depth""","""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer."" Keyphrase: ""Limited prompt tuning depth"""
arXIv2023,Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey,Yes.,4,"""However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications).""",2023,2023-05-30T03:00:30Z,"Keyphrase: ""Domain heterogeneity and complexity""","""However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications)."" Keyphrase: ""Domain heterogeneity and complexity"""
arXIv2023,Faith and Fate: Limits of Transformers on Compositionality,Yes.,5,"""Yet, these models simultaneously show failures on surprisingly trivial problems."" and ""Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.""",2023,2023-05-29T23:24:14Z,"Keyphrase: ""Limited compositional reasoning""","""Yet, these models simultaneously show failures on surprisingly trivial problems."" and ""Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills."" Keyphrase: ""Limited compositional reasoning"""
arXIv2023,How Effective Are Neural Networks for Fixing Security Vulnerabilities,Yes.,5,"""Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to",2023,2023-05-29T20:50:27Z,"Keyphrase: ""Limited vulnerability-fixing capability""","""Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to Keyphrase: ""Limited vulnerability-fixing capability"""
arXIv2023,Do Language Models Know When They're Hallucinating References?,Yes.,5,"""State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda.""",2023,2023-05-29T17:12:03Z,"Keyphrase: ""Hallucinated information and inaccurate output""","""State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda."" Keyphrase: ""Hallucinated information and inaccurate output"""
arXIv2023,Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models,Yes.,5,"""To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs."" and ""We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts.""",2023,2023-05-29T16:29:22Z,"Keyphrase: ""Higher rate of racial stereotypes""","""To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs."" and ""We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts."" Keyphrase: ""Higher rate of racial stereotypes"""
arXIv2023,Do Large Language Models Know What They Don't Know?,Yes.,5,"""Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend."" and ""Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.""",2023,2023-05-29T15:30:13Z,"Keyphrase: ""Limited comprehension and knowledge gap""","""Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend."" and ""Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge."" Keyphrase: ""Limited comprehension and knowledge gap"""
arXIv2023,"Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations",Yes.,4,"""Subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and LOL",2023,2023-05-29T12:26:44Z,"Keyphrase: ""Security vulnerabilities""","""Subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and LOL Keyphrase: ""Security vulnerabilities"""
arXIv2023,Large Language Models are not Fair Evaluators,Yes.,5,"""we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models.""",2023,2023-05-29T07:41:03Z,"Keyphrase: ""Biased evaluation paradigms""","""we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models."" Keyphrase: ""Biased evaluation paradigms"""
arXIv2023,Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective,Yes.,4,"""We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework.""",2023,2023-05-28T16:04:48Z,"Keyphrase: ""Limited incorporation of human feedback""","""We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework."" Keyphrase: ""Limited incorporation of human feedback"""
arXIv2023,Mitigating Label Biases for In-context Learning,Yes.,5,"""domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples.""",2023,2023-05-28T15:37:39Z,"Keyphrase: ""Domain label bias""","""domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples."" Keyphrase: ""Domain label bias"""
arXIv2023,KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application,Yes.,4,"""Large language models (LLMs) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications.""",2023,2023-05-28T12:07:16Z,"Keyphrase: ""Social bias and demographic disparities""","""Large language models (LLMs) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications."" Keyphrase: ""Social bias and demographic disparities"""
arXIv2023,SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration,Yes.,4,"""The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising.""",2023,2023-05-28T11:51:20Z,"Keyphrase: ""Generating offensive and biased content""","""The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising."" Keyphrase: ""Generating offensive and biased content"""
arXIv2023,Evaluating GPT-3 Generated Explanations for Hateful Content Moderation,Yes.,5,"""A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators."" and ""this persuasiveness may result in incorrect judgments about the hatefulness of the content.""",2023,2023-05-28T10:05:13Z,"Keyphrase: ""Erroneous explanations""","""A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators."" and ""this persuasiveness may result in incorrect judgments about the hatefulness of the content."" Keyphrase: ""Erroneous explanations"""
arXIv2023,Reward Collapse in Aligning Large Language Models,Yes.,5,"""we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training.""",2023,2023-05-28T02:12:00Z,"Keyphrase: ""Reward collapse""","""we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training."" Keyphrase: ""Reward collapse"""
arXIv2023,Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,Yes.,5,"""Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks.""",2023,2023-05-27T19:08:04Z,"Keyphrase: ""Unwanted side effects in editing technique""","""Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks."" Keyphrase: ""Unwanted side effects in editing technique"""
arXIv2023,The Curse of Recursion: Training on Generated Data Makes Models Forget,Yes.,5,"""We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs.""",2023,2023-05-27T15:10:41Z,"Keyphrase: ""Catastrophic forgetting""","""We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs."" Keyphrase: ""Catastrophic forgetting"""
arXIv2023,FERMAT: An Alternative to Accuracy for Numerical Reasoning,Yes.,5,"""While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning.""",2023,2023-05-27T15:00:45Z,"Keyphrase: ""Struggles with numerical reasoning""","""While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning."" Keyphrase: ""Struggles with numerical reasoning"""
arXIv2023,Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning,Yes.,5,"""Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited."" and ""Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are 'lazy learners' that tend to exploit shortcuts in prompts for downstream tasks.""",2023,2023-05-26T20:56:30Z,"Keyphrase: ""Lazy learner tendency""","""Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited."" and ""Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are 'lazy learners' that tend to exploit shortcuts in prompts for downstream tasks."" Keyphrase: ""Lazy learner tendency"""
arXIv2023,Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model,Yes.,5,"""Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model and Prometheus. Hallucinations were mostly associated with non-existent references or fabricated efficacy data.""",2023,2023-05-26T17:33:05Z,"Keyphrase: ""Hallucination of nonexistent references""","""Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model and Prometheus. Hallucinations were mostly associated with non-existent references or fabricated efficacy data."" Keyphrase: ""Hallucination of nonexistent references"""
arXIv2023,"LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",Yes.,5,"""GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly",2023,2023-05-26T16:32:17Z,"Keyphrase: ""Limited reasoning capacity""","""GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly Keyphrase: ""Limited reasoning capacity"""
arXIv2023,On Evaluating Adversarial Robustness of Large Vision-Language Models,Yes.,4,"""multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision)"" and ""Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice.""",2023,2023-05-26T13:49:44Z,"Keyphrase: ""Adversarial vulnerability in multimodal generation""","""multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision)"" and ""Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice."" Keyphrase: ""Adversarial vulnerability in multimodal generation"""
arXIv2023,Playing repeated games with Large Language Models,Yes.,5,"""we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination.""",2023,2023-05-26T12:17:59Z,"Keyphrase: ""Suboptimal coordination in games""","""we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination."" Keyphrase: ""Suboptimal coordination in games"""
arXIv2023,Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification,Yes.,4,"""approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations.""",2023,2023-05-26T09:15:05Z,"Keyphrase: ""Ethical and bias concerns""","""approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations."" Keyphrase: ""Ethical and bias concerns"""
arXIv2023,Can large language models generate salient negative statements?,Yes.,5,"""LLMs still struggle with the notion of factuality of negatives, frequently generating many ambiguous statements, or statements with negative keywords but a positive meaning.""",2023,2023-05-26T09:13:59Z,"Keyphrase: ""Factuality struggles""","""LLMs still struggle with the notion of factuality of negatives, frequently generating many ambiguous statements, or statements with negative keywords but a positive meaning."" Keyphrase: ""Factuality struggles"""
arXIv2023,A Closer Look at In-Context Learning under Distribution Shifts,Yes.,5,"""The key question we aim to address is",2023,2023-05-26T07:47:21Z,"Keyphrase: ""Lack of contextual understanding""","""The key question we aim to address is Keyphrase: ""Lack of contextual understanding"""
arXIv2023,AdaPlanner: Adaptive Planning from Feedback with Language Models,Yes.,5,"""most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase.""",2023,2023-05-26T05:52:27Z,"Keyphrase: ""Limited adaptability and planning""","""most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase."" Keyphrase: ""Limited adaptability and planning"""
arXIv2023,TADA: Task-Agnostic Dialect Adapters for English,Yes.,5,"""Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE).""",2023,2023-05-26T05:45:03Z,"Keyphrase: ""Bias towards standard American English""","""Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE)."" Keyphrase: ""Bias towards standard American English"""
arXIv2023,The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering,Yes.,5,"""Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. 'unfaithful' with respect to a rationale as retrieved from a knowledge base.""",2023,2023-05-25T22:54:13Z,"Keyphrase: ""Unfaithful output""","""Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. 'unfaithful' with respect to a rationale as retrieved from a knowledge base."" Keyphrase: ""Unfaithful output"""
arXIv2023,Type Prediction With Program Decomposition and Fill-in-the-Type Training,Yes.,5,"""Large language models (LLMs) are promising for type prediction, but there are challenges",2023,2023-05-25T21:16:09Z,"Keyphrase: ""Limited predictive challenge capabilities""","""Large language models (LLMs) are promising for type prediction, but there are challenges Keyphrase: ""Limited predictive challenge capabilities"""
arXIv2023,Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models,Yes.,5,"""We find that despite capturing some aspects of logical meaning, the models fall far short of human performance.""",2023,2023-05-25T18:56:26Z,"Keyphrase: ""Limited logical reasoning""","""We find that despite capturing some aspects of logical meaning, the models fall far short of human performance."" Keyphrase: ""Limited logical reasoning"""
arXIv2023,Landmark Attention: Random-Access Infinite Context Length for Transformers,Yes.,5,"""their attention mechanism's large memory requirements have limited their ability to handle longer contexts.""",2023,2023-05-25T17:53:42Z,"Keyphrase: ""Limited long-context handling""","""their attention mechanism's large memory requirements have limited their ability to handle longer contexts."" Keyphrase: ""Limited long-context handling"""
arXIv2023,Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots,Yes.,5,"""there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential,"" ""The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential,"" and ""What ChatGPT lacks is a stochastic measure",2023,2023-05-25T17:35:57Z,"Keyphrase: ""Inaccurate data generation""","""there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential,"" ""The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential,"" and ""What ChatGPT lacks is a stochastic measure Keyphrase: ""Inaccurate data generation"""
arXIv2023,ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs,Yes.,5,"""However, current works in this field are plagued by limitations, specifically a restricted scope of applicable image domains and the provision of unreliable medical advice. This restricts their overall processing capabilities. Furthermore, the mismatch in writing style between LLMs and radiologists undermines their practical usefulness.""",2023,2023-05-25T12:03:31Z,"Keyphrase: ""Limited scope and unreliable advice""","""However, current works in this field are plagued by limitations, specifically a restricted scope of applicable image domains and the provision of unreliable medical advice. This restricts their overall processing capabilities. Furthermore, the mismatch in writing style between LLMs and radiologists undermines their practical usefulness."" Keyphrase: ""Limited scope and unreliable advice"""
arXIv2023,"Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",Yes.,5,"""Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context.""",2023,2023-05-25T08:43:46Z,"Keyphrase: ""Hallucinated content""","""Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context."" Keyphrase: ""Hallucinated content"""
arXIv2023,Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,Yes.,5,"""Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost.""",2023,2023-05-25T07:39:41Z,"Keyphrase: ""Quadratic computational cost""","""Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost."" Keyphrase: ""Quadratic computational cost"""
arXIv2023,On the Planning Abilities of Large Language Models : A Critical Investigation,Yes.,5,"""Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains.""",2023,2023-05-25T06:32:23Z,"Keyphrase: ""Limited autonomous planning ability""","""Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains."" Keyphrase: ""Limited autonomous planning ability"""
arXIv2023,The False Promise of Imitating Proprietary LLMs,Yes.,5,"""we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data"" and ""Overall, we conclude that model imitation is a false promise",2023,2023-05-25T05:00:12Z,"Keyphrase: ""Over-reliance on imitation data""","""we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data"" and ""Overall, we conclude that model imitation is a false promise Keyphrase: ""Over-reliance on imitation data"""
arXIv2023,Asking Before Action: Gather Information in Embodied Decision Making with Language Models,Yes.,5,"""However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance.""",2023,2023-05-25T04:05:08Z,"Keyphrase: ""Struggles in unfamiliar environments""","""However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance."" Keyphrase: ""Struggles in unfamiliar environments"""
arXIv2023,Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models,Yes.,4,"""the sensitivity of data contained in prompts raises privacy concerns"" and ""we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs.""",2023,2023-05-24T22:06:08Z,"Keyphrase: ""Privacy vulnerabilities due to prompt data sensitivity""","""the sensitivity of data contained in prompts raises privacy concerns"" and ""we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs."" Keyphrase: ""Privacy vulnerabilities due to prompt data sensitivity"""
arXIv2023,"The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",Yes.,5,"""LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.""",2023,2023-05-24T18:54:39Z,"Keyphrase: ""Lack of deep abstract understanding""","""LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability."" Keyphrase: ""Lack of deep abstract understanding"""
arXIv2023,Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing,Yes.,5,"""However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content."" and ""we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art LLMs.""",2023,2023-05-24T16:50:36Z,"Keyphrase: ""Content hallucination""","""However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content."" and ""we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art LLMs."" Keyphrase: ""Content hallucination"""
arXIv2023,Gorilla: Large Language Model Connected with Massive APIs,Yes.,5,"""However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call.""",2023,2023-05-24T16:48:11Z,"Keyphrase: ""Inaccurate input generation""","""However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call."" Keyphrase: ""Inaccurate input generation"""
arXIv2023,"Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond",Yes.,4,"""This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI.""",2023,2023-05-24T16:23:46Z,"Keyphrase: ""Epistemological and ethical challenges""","""This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI."" Keyphrase: ""Epistemological and ethical challenges"""
arXIv2023,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,Yes.,5,"""Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world.""",2023,2023-05-24T16:17:36Z,"Keyphrase: ""Outdated knowledge and hallucination""","""Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world."" Keyphrase: ""Outdated knowledge and hallucination"""
arXIv2023,A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification,Yes.,4,"""However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification."" and ""We observe LLMs are more prone to failure in these cases.""",2023,2023-05-24T16:04:26Z,"Keyphrase: ""Failure in hierarchical classification""","""However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification."" and ""We observe LLMs are more prone to failure in these cases."" Keyphrase: ""Failure in hierarchical classification"""
arXIv2023,Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples,Yes.,5,"""Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.""",2023,2023-05-24T15:55:51Z,"Keyphrase: ""Difficulty in generalizing to longer proofs""","""Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction."" Keyphrase: ""Difficulty in generalizing to longer proofs"""
arXIv2023,Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,Yes.,5,"""We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models,"" and ""PCW would present unexpected deterioration regarding question miscomprehension and false inference.""",2023,2023-05-24T15:48:29Z,"Keyphrase: ""Question miscomprehension and false inference""","""We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models,"" and ""PCW would present unexpected deterioration regarding question miscomprehension and false inference."" Keyphrase: ""Question miscomprehension and false inference"""
arXIv2023,Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models,Yes.,5,"""The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts.""",2023,2023-05-24T11:55:59Z,"Keyphrase: ""Difficulty with abstract concepts""","""The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts."" Keyphrase: ""Difficulty with abstract concepts"""
arXIv2023,ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind,Yes.,5,"""there is a heated debate about whether they are able to perform ToM tasks,"" and ""our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs.""",2023,2023-05-24T11:54:07Z,"Keyphrase: ""Inconsistent behavior in task performance""","""there is a heated debate about whether they are able to perform ToM tasks,"" and ""our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs."" Keyphrase: ""Inconsistent behavior in task performance"""
arXIv2023,GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking,Yes.,5,"""we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities.""",2023,2023-05-24T11:53:19Z,"Keyphrase: ""Limitation in graph comprehension""","""we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities."" Keyphrase: ""Limitation in graph comprehension"""
arXIv2023,Lawyer LLaMA Technical Report,Yes.,4,"""the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems"" and ""to alleviate the hallucination problem during the model's generation"".",2023,2023-05-24T11:52:07Z,"Keyphrase: ""Deficiency in domain-specific knowledge""","""the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems"" and ""to alleviate the hallucination problem during the model's generation"". Keyphrase: ""Deficiency in domain-specific knowledge"""
arXIv2023,A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event Extraction,Yes.,4,"""Unfortunately, we find that current zero-shot EE methods perform poorly for the task, with issues including word sense ambiguity, modality mismatch, and efficiency. Straightforward application of large language model prompting typically performs even worse.""",2023,2023-05-24T11:41:33Z,"Keyphrase: ""Poor zero-shot performance""","""Unfortunately, we find that current zero-shot EE methods perform poorly for the task, with issues including word sense ambiguity, modality mismatch, and efficiency. Straightforward application of large language model prompting typically performs even worse."" Keyphrase: ""Poor zero-shot performance"""
arXIv2023,ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification,Yes.,4,"""Mainstream deep learning approaches employing fine-tuning strategies on pre-trained language models (PLMs), have demonstrated remarkable performance gains over the past few years. Nonetheless, these methods still face many drawbacks that are complex to solve, including",2023,2023-05-24T11:06:23Z,"Keyphrase: ""Drawbacks in fine-tuning strategy""","""Mainstream deep learning approaches employing fine-tuning strategies on pre-trained language models (PLMs), have demonstrated remarkable performance gains over the past few years. Nonetheless, these methods still face many drawbacks that are complex to solve, including Keyphrase: ""Drawbacks in fine-tuning strategy"""
arXIv2023,Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems,Yes.,5,"""Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation.""",2023,2023-05-24T10:58:20Z,"Keyphrase: ""Factual errors in arithmetic tasks""","""Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation."" Keyphrase: ""Factual errors in arithmetic tasks"""
arXIv2023,"RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",Yes.,5,"""they all suffer from generating untruthful dialogues because of the model hallucination.""",2023,2023-05-24T10:30:42Z,"Keyphrase: ""Untruthful dialogue hallucination""","""they all suffer from generating untruthful dialogues because of the model hallucination."" Keyphrase: ""Untruthful dialogue hallucination"""
arXIv2023,Reasoning with Language Model is Planning with World Model,Yes.,5,"""However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g.,",2023,2023-05-24T10:28:28Z,"Keyphrase: ""Lack of internal world model""","""However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., Keyphrase: ""Lack of internal world model"""
arXIv2023,GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP,Yes.,5,"""Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic."" and ""unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA."" and ""our work adds to a growing body of",2023,2023-05-24T10:12:39Z,"Keyphrase: ""Limited performance in handling Arabic dialects""","""Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic."" and ""unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA."" and ""our work adds to a growing body of Keyphrase: ""Limited performance in handling Arabic dialects"""
arXIv2023,Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback,Yes.,4,"""some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated.""",2023,2023-05-24T10:12:33Z,"Keyphrase: ""Poorly calibrated conditional probability""","""some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated."" Keyphrase: ""Poorly calibrated conditional probability"""
arXIv2023,"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",Yes.,5,"""non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies.""",2023,2023-05-24T09:57:37Z,"Keyphrase: ""Degenerate output behavior""","""non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies."" Keyphrase: ""Degenerate output behavior"""
arXIv2023,Adversarial Demonstration Attacks on Large Language Models,Yes.,5,"""We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.""",2023,2023-05-24T09:40:56Z,"Keyphrase: ""Critical security risk""","""We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs."" Keyphrase: ""Critical security risk"""
arXIv2023,Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark,Yes.,4,"""we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks,"" and ""pretrained models already possess some innate but limited capabilities of social language understanding.""",2023,2023-05-24T09:21:06Z,"Keyphrase: ""Limited social language understanding""","""we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks,"" and ""pretrained models already possess some innate but limited capabilities of social language understanding."" Keyphrase: ""Limited social language understanding"""
arXIv2023,In-Context Impersonation Reveals Large Language Models' Strengths and Biases,Yes.,4,"""However, impersonation can also uncover LLMs' biases",2023,2023-05-24T09:13:15Z,"Keyphrase: ""Bias uncovered through impersonation""","""However, impersonation can also uncover LLMs' biases Keyphrase: ""Bias uncovered through impersonation"""
arXIv2023,Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,Yes.,5,"""methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback.""",2023,2023-05-24T08:59:15Z,"Keyphrase: ""Limited practical application due to reliance on feedback""","""methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback."" Keyphrase: ""Limited practical application due to reliance on feedback"""
arXIv2023,PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions,Yes.,5,"""The remarkable capabilities of large language models have been accompanied by a persistent drawback",2023,2023-05-24T08:59:00Z,"Keyphrase: ""Persistent drawbacks""","""The remarkable capabilities of large language models have been accompanied by a persistent drawback Keyphrase: ""Persistent drawbacks"""
arXIv2023,"M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",Yes.,4,"""Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written.""",2023,2023-05-24T08:55:11Z,"Keyphrase: ""Poor generalization to unseen domains""","""Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written."" Keyphrase: ""Poor generalization to unseen domains"""
arXIv2023,How To Train Your (Compressed) Large Language Model,Yes.,5,"""Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression.""",2023,2023-05-24T08:18:35Z,"Keyphrase: ""Inadequate compression methods""","""Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression."" Keyphrase: ""Inadequate compression methods"""
arXIv2023,BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer,Yes.,5,"""Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. In particular, ChatGPT with in-context learning often performs worse than much smaller mT5-base models fine-tuned on English task data and few-shot in-language examples.""",2023,2023-05-24T08:06:33Z,"Keyphrase: ""Limited few-shot and cross-lingual capabilities""","""Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. In particular, ChatGPT with in-context learning often performs worse than much smaller mT5-base models fine-tuned on English task data and few-shot in-language examples."" Keyphrase: ""Limited few-shot and cross-lingual capabilities"""
arXIv2023,SummIt: Iterative Text Summarization via ChatGPT,Yes.,4,"""the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests."" and ""identify a potential issue of over-correction.""",2023,2023-05-24T07:40:06Z,"Keyphrase: ""Hallucination and overcorrection""","""the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests."" and ""identify a potential issue of over-correction."" Keyphrase: ""Hallucination and overcorrection"""
arXIv2023,Mitigating Temporal Misalignment by Discarding Outdated Facts,Yes.,5,"""While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having",2023,2023-05-24T07:30:08Z,"Keyphrase: ""Outdated world knowledge""","""While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having Keyphrase: ""Outdated world knowledge"""
arXIv2023,MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,Yes.,5,"""The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. ... While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions.""",2023,2023-05-24T06:48:41Z,"Keyphrase: ""Rapid decay of information""","""The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. ... While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions."" Keyphrase: ""Rapid decay of information"""
arXIv2023,Prompting Large Language Models for Counterfactual Generation: An Empirical Study,Yes.,5,"""The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias."" and ""we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs' capability of generating counterfactuals.""",2023,2023-05-24T06:44:32Z,"Keyphrase: ""Limited task-specific performance""","""The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias."" and ""we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs' capability of generating counterfactuals."" Keyphrase: ""Limited task-specific performance"""
arXIv2023,Adapting Language Models to Compress Contexts,Yes.,5,"""Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.""",2023,2023-05-24T06:42:44Z,"Keyphrase: ""Limited context window""","""Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents."" Keyphrase: ""Limited context window"""
arXIv2023,ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds,Yes.,5,"""This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model."" and ""Our analyses emphasize the need for further research into the linguistic comprehension and reasoning capabilities of LLMs, in order to improve their reliability, and establish their trustworthiness for real-world applications.""",2023,2023-05-24T06:41:09Z,"Keyphrase: ""Limited linguistic comprehension""","""This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model."" and ""Our analyses emphasize the need for further research into the linguistic comprehension and reasoning capabilities of LLMs, in order to improve their reliability, and establish their trustworthiness for real-world applications."" Keyphrase: ""Limited linguistic comprehension"""
arXIv2023,Anthropomorphization of AI: Opportunities and Risks,Yes.,4,"""We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction,",2023,2023-05-24T06:39:45Z,"Keyphrase: ""Anthropomorphization concerns""","""We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction, Keyphrase: ""Anthropomorphization concerns"""
arXIv2023,Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models,Yes.,5,"""We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust To",2023,2023-05-24T06:14:31Z,"Keyphrase: ""Struggles with adversarial examples""","""We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust To Keyphrase: ""Struggles with adversarial examples"""
arXIv2023,Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,Yes.,4,"""These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.""",2023,2023-05-24T04:27:21Z,"Keyphrase: ""Vulnerability to poisoning attacks""","""These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing."" Keyphrase: ""Vulnerability to poisoning attacks"""
arXIv2023,A Causal View of Entity Bias in (Large) Language Models,Yes.,4,"""Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions."" and ""The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits.""",2023,2023-05-24T03:59:18Z,"Keyphrase: ""Entity bias and unfaithful predictions""","""Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions."" and ""The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits."" Keyphrase: ""Entity bias and unfaithful predictions"""
arXIv2023,Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs,Yes.,5,"""We also identify the existence of inherent biases in these LLMs which is the root cause of the aforementioned phenomenon and makes self-assessment tests unreliable.""",2023,2023-05-24T03:53:43Z,"Keyphrase: ""Inherent bias and unreliable self-assessment""","""We also identify the existence of inherent biases in these LLMs which is the root cause of the aforementioned phenomenon and makes self-assessment tests unreliable."" Keyphrase: ""Inherent bias and unreliable self-assessment"""
arXIv2023,Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response,Yes.,5,"""Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs."" and ""Empirical results show that the ability of LLMs to identify unreasonable responses is insufficient. There are risks in using reference-free evaluators based on LLMs to evaluate the quality of",2023,2023-05-24T02:52:48Z,"Keyphrase: ""Challenges with identifying unreasonable responses""","""Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs."" and ""Empirical results show that the ability of LLMs to identify unreasonable responses is insufficient. There are risks in using reference-free evaluators based on LLMs to evaluate the quality of Keyphrase: ""Challenges with identifying unreasonable responses"""
arXIv2023,Enabling Large Language Models to Generate Text with Citations,Yes.,5,"""Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination."" and ""current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time.""",2023,2023-05-24T01:53:49Z,"Keyphrase: ""Hallucination and lack of complete citation support""","""Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination."" and ""current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time."" Keyphrase: ""Hallucination and lack of complete citation support"""
arXIv2023,Think Before You Act: Decision Transformers with Internal Working Memory,Yes.,5,"""However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks.""",2023,2023-05-24T01:20:22Z,"Keyphrase: ""Forgetting phenomenon""","""However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks."" Keyphrase: ""Forgetting phenomenon"""
arXIv2023,ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers,Yes.,4,"""Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification.""",2023,2023-05-24T00:10:15Z,"Keyphrase: ""Lack of guaranteed correctness""","""Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification."" Keyphrase: ""Lack of guaranteed correctness"""
arXIv2023,Sources of Hallucination by Large Language Models on Inference Tasks,Yes.,5,"""We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs.""",2023,2023-05-23T22:24:44Z,"Keyphrase: ""Bias and hallucination issues""","""We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs."" Keyphrase: ""Bias and hallucination issues"""
arXIv2023,LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond,Yes.,5,"""a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision"" and ""Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4,",2023,2023-05-23T21:50:06Z,"Keyphrase: ""Struggles with complex tasks""","""a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision"" and ""Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, Keyphrase: ""Struggles with complex tasks"""
arXIv2023,MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems,Yes.,5,"""While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early.""",2023,2023-05-23T21:44:56Z,"Keyphrase: ""Inaccurate tutoring feedback""","""While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early."" Keyphrase: ""Inaccurate tutoring feedback"""
arXIv2023,Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models,Yes.,5,"""Our findings show that even the most advanced GPT models struggle to reason on manipulated facts - showcasing poor DUPE skills - with accuracy dropping by 45% compared to the original dataset.""",2023,2023-05-23T20:26:03Z,"Keyphrase: ""Poor robustness to manipulation""","""Our findings show that even the most advanced GPT models struggle to reason on manipulated facts - showcasing poor DUPE skills - with accuracy dropping by 45% compared to the original dataset."" Keyphrase: ""Poor robustness to manipulation"""
arXIv2023,Having Beer after Prayer? Measuring Cultural Bias in Large Language Models,Yes.,5,"""we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture,"" ""we find concerning cases of stereotyping and cultural unfairness,"" and ""revealing the incapability of appropriate adaptation to Arab cultural contexts.""",2023,2023-05-23T18:27:51Z,"Keyphrase: ""Cultural bias and unfair stereotyping""","""we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture,"" ""we find concerning cases of stereotyping and cultural unfairness,"" and ""revealing the incapability of appropriate adaptation to Arab cultural contexts."" Keyphrase: ""Cultural bias and unfair stereotyping"""
arXIv2023,On Robustness of Finetuned Transformer-based NLP Models,Yes.,5,"""Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned?"" and ""Overall, this study provides valuable insights into perturbation-specific weaknesses of popular Transformer-based models, which should be kept in mind when passing inputs",2023,2023-05-23T18:25:18Z,"Keyphrase: ""Robustness to input perturbations""","""Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned?"" and ""Overall, this study provides valuable insights into perturbation-specific weaknesses of popular Transformer-based models, which should be kept in mind when passing inputs Keyphrase: ""Robustness to input perturbations"""
arXIv2023,ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models,Yes.,4,"""Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning.""",2023,2023-05-23T17:54:33Z,"Keyphrase: ""Limited complex reasoning ability""","""Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning."" Keyphrase: ""Limited complex reasoning ability"""
arXIv2023,Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science,Yes.,5,"""The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large).""",2023,2023-05-23T17:48:21Z,"Keyphrase: ""Zero-shot performance limitations""","""The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large)."" Keyphrase: ""Zero-shot performance limitations"""
arXIv2023,Evaluation of African American Language Bias in Natural Language Generation,Yes.,4,"""documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of AAL features.""",2023,2023-05-23T17:34:37Z,"Keyphrase: ""Bias identification trend""","""documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of AAL features."" Keyphrase: ""Bias identification trend"""
arXIv2023,Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs,Yes.,5,"""We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.""",2023,2023-05-23T17:25:59Z,"Keyphrase: ""Inconsistent performance""","""We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks."" Keyphrase: ""Inconsistent performance"""
arXIv2023,Hierarchical Prompting Assists Large Language Model on Web Navigation,Yes.,4,"""Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks.""",2023,2023-05-23T17:10:39Z,"Keyphrase: ""Struggles with complex tasks""","""Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks."" Keyphrase: ""Struggles with complex tasks"""
arXIv2023,Language Models with Rationality,Yes.,4,"""This lack of interpretability is a growing impediment to widespread use of LLMs."" and ""to resolve inconsistencies that may exist.""",2023,2023-05-23T17:04:25Z,"Keyphrase: ""Lack of interpretability""","""This lack of interpretability is a growing impediment to widespread use of LLMs."" and ""to resolve inconsistencies that may exist."" Keyphrase: ""Lack of interpretability"""
arXIv2023,Multilingual Large Language Models Are Not (Yet) Code-Switchers,Yes.,5,"""Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales."" and ""We argue that current 'multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.""",2023,2023-05-23T16:50:48Z,"Keyphrase: ""Underperformance in zero-shot/few-shot tasks""","""Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales."" and ""We argue that current 'multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy."" Keyphrase: ""Underperformance in zero-shot/few-shot tasks"""
arXIv2023,Question Answering as Programming for Solving Time-Sensitive Questions,Yes.,5,"""our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics.""",2023,2023-05-23T16:35:16Z,"Keyphrase: ""Inability for rigorous reasoning""","""our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics."" Keyphrase: ""Inability for rigorous reasoning"""
arXIv2023,HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations,Yes.,4,"""GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills.""",2023,2023-05-23T16:15:24Z,"Keyphrase: ""Limited pragmatic skills""","""GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills."" Keyphrase: ""Limited pragmatic skills"""
arXIv2023,In-Context Probing: Toward Building Robust Classifiers via Probing Large Language Models,Yes.,5,"""the performance on a downstream task can vary considerably, depending on the instruction. Importantly, such dependency on the context can surface in unpredictable ways, e.g., a seemingly more informative instruction might lead to a worse performance.""",2023,2023-05-23T15:43:04Z,"Keyphrase: ""Unpredictable task performance""","""the performance on a downstream task can vary considerably, depending on the instruction. Importantly, such dependency on the context can surface in unpredictable ways, e.g., a seemingly more informative instruction might lead to a worse performance."" Keyphrase: ""Unpredictable task performance"""
arXIv2023,Revisiting Acceptability Judgements,Yes.,5,"""Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much below supervised models (59.03 MCC) and human (65.11 MCC).""",2023,2023-05-23T14:16:22Z,"Keyphrase: ""Limited performance compared to supervised models""","""Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much below supervised models (59.03 MCC) and human (65.11 MCC)."" Keyphrase: ""Limited performance compared to supervised models"""
arXIv2023,Improving Language Models via Plug-and-Play Retrieval Feedback,Yes.,5,"""Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can",2023,2023-05-23T12:29:44Z,"Keyphrase: ""Incorrect hallucinated information""","""Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can Keyphrase: ""Incorrect hallucinated information"""
arXIv2023,Robust Prompt Optimization for Large Language Models Against Distribution Shifts,Yes.,5,"""We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis.""",2023,2023-05-23T11:30:43Z,"Keyphrase: ""Vulnerability to distribution shift""","""We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis."" Keyphrase: ""Vulnerability to distribution shift"""
arXIv2023,A Trip Towards Fairness: Bias and De-Biasing in Large Language Models,Yes.,4,"""a little or a large bias in CtB-LLMs may cause huge harm,"" and ""we performed a large investigation of the bias of three families of CtB-LLMs,"" and ""we discovered that bias depends not on the number of parameters but on the perplexity.""",2023,2023-05-23T09:35:37Z,"Keyphrase: ""Biased outputs""","""a little or a large bias in CtB-LLMs may cause huge harm,"" and ""we performed a large investigation of the bias of three families of CtB-LLMs,"" and ""we discovered that bias depends not on the number of parameters but on the perplexity."" Keyphrase: ""Biased outputs"""
arXIv2023,Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study,Yes.,5,"""Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse.""",2023,2023-05-23T09:33:38Z,"Keyphrase: ""Content constraint and potential misuse""","""Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse."" Keyphrase: ""Content constraint and potential misuse"""
arXIv2023,"""Is the Pope Catholic?"" Applying Chain-of-Thought Reasoning to Understanding Conversational Implicatures",Yes.,5,"""recent research indicates that large language models struggle to comprehend these implicatures as effectively as the average human.""",2023,2023-05-23T08:49:50Z,"Keyphrase: ""Difficulty with implicatures""","""recent research indicates that large language models struggle to comprehend these implicatures as effectively as the average human."" Keyphrase: ""Difficulty with implicatures"""
arXIv2023,Can Large Language Models Capture Dissenting Human Voices?,Yes.,5,"""we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.""",2023,2023-05-23T07:55:34Z,"Keyphrase: ""Limited ability in capturing human disagreement""","""we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population."" Keyphrase: ""Limited ability in capturing human disagreement"""
arXIv2023,Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting,Yes.,5,"""we uncovered a universal vulnerability among LLMs in processing inductive instructions,"" and ""different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance.""",2023,2023-05-23T06:38:20Z,"Keyphrase: ""Struggles with inductive instruction""","""we uncovered a universal vulnerability among LLMs in processing inductive instructions,"" and ""different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance."" Keyphrase: ""Struggles with inductive instruction"""
arXIv2023,Exploring Self-supervised Logic-enhanced Training for Large Language Models,Yes.,5,"""Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines.""",2023,2023-05-23T06:13:10Z,"Keyphrase: ""Weak logical reasoning performance""","""Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines."" Keyphrase: ""Weak logical reasoning performance"""
arXIv2023,Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models,Yes.,5,"""an analysis of LLaMA's errors reveals significant limitations in its ability to recall facts in languages other than English, plus difficulties related to the location and gender of fact subjects.""",2023,2023-05-23T04:31:39Z,"Keyphrase: ""Limited fact recall and contextual understanding""","""an analysis of LLaMA's errors reveals significant limitations in its ability to recall facts in languages other than English, plus difficulties related to the location and gender of fact subjects."" Keyphrase: ""Limited fact recall and contextual understanding"""
arXIv2023,The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models,Yes.,5,"""they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings.""",2023,2023-05-23T04:22:50Z,"Keyphrase: ""Inherent bias and lack of grounding""","""they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings."" Keyphrase: ""Inherent bias and lack of grounding"""
arXIv2023,On the Risk of Misinformation Pollution with Large Language Models,Yes.,5,"""Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems.""",2023,2023-05-23T04:10:26Z,"Keyphrase: ""Misinformation generation""","""Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems."" Keyphrase: ""Misinformation generation"""
arXIv2023,ChatGPT as your Personal Data Scientist,Yes.,5,"""Interestingly, its development spotlighted several critical weaknesses in the current LLMs (ChatGPT) and highlighted substantial opportunities for improvement.""",2023,2023-05-23T04:00:16Z,"Keyphrase: ""Critical weaknesses and room for improvement""","""Interestingly, its development spotlighted several critical weaknesses in the current LLMs (ChatGPT) and highlighted substantial opportunities for improvement."" Keyphrase: ""Critical weaknesses and room for improvement"""
arXIv2023,InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning,Yes.,4,"""their ability to generalize to underrepresented languages is limited due to the scarcity of available data. Additionally, directly adapting new languages to instruction-tuned LLMs can result in catastrophic forgetting, which leads to the loss of multitasking ability.""",2023,2023-05-23T02:51:34Z,"Keyphrase: ""Catastrophic forgetting""","""their ability to generalize to underrepresented languages is limited due to the scarcity of available data. Additionally, directly adapting new languages to instruction-tuned LLMs can result in catastrophic forgetting, which leads to the loss of multitasking ability."" Keyphrase: ""Catastrophic forgetting"""
arXIv2023,"Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",Yes.,5,"""However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity.""",2023,2023-05-23T02:49:35Z,"Keyphrase: ""Limited conversational proactivity""","""However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity."" Keyphrase: ""Limited conversational proactivity"""
arXIv2023,How Language Model Hallucinations Can Snowball,Yes.,5,"""A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements."" and ""We refer to this phenomenon as hallucination snowballing",2023,2023-05-22T23:14:28Z,"Keyphrase: ""Risk of hallucinating incorrect statements""","""A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements."" and ""We refer to this phenomenon as hallucination snowballing Keyphrase: ""Risk of hallucinating incorrect statements"""
arXIv2023,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,Yes.,5,"""We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.""",2023,2023-05-22T21:59:26Z,"Keyphrase: ""Poor slot filling performance""","""We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU."" Keyphrase: ""Poor slot filling performance"""
arXIv2023,DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules,Yes.,4,"""Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects.""",2023,2023-05-22T18:43:31Z,"Keyphrase: ""Limited performance on non-standard English dialects""","""Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects."" Keyphrase: ""Limited performance on non-standard English dialects"""
arXIv2023,RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text,Yes.,5,"""The fixed-size context of Transformer makes GPT models incapable of generating arbitrarily long text.""",2023,2023-05-22T17:58:10Z,"Keyphrase: ""Inability to generate arbitrarily long text""","""The fixed-size context of Transformer makes GPT models incapable of generating arbitrarily long text."" Keyphrase: ""Inability to generate arbitrarily long text"""
arXIv2023,Language-Agnostic Bias Detection in Language Models with Bias Probing,Yes.,4,"""Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases."" and ""We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context.""",2023,2023-05-22T17:58:01Z,"Keyphrase: ""Strong social bias""","""Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases."" and ""We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context."" Keyphrase: ""Strong social bias"""
arXIv2023,Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts,Yes.,5,"""tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory"" and ""how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?"" and ""Our investigation reveals seemingly contradicting behaviors of LLMs"" and ""These results pose important implications that are worth careful",2023,2023-05-22T17:57:41Z,"Keyphrase: ""Limited external evidence integration""","""tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory"" and ""how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?"" and ""Our investigation reveals seemingly contradicting behaviors of LLMs"" and ""These results pose important implications that are worth careful Keyphrase: ""Limited external evidence integration"""
arXIv2023,Fairness of ChatGPT,Yes.,4,"""there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs,"" and ""This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.""",2023,2023-05-22T17:51:56Z,"Keyphrase: ""Limited quantitative analysis on fairness evaluation""","""there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs,"" and ""This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems."" Keyphrase: ""Limited quantitative analysis on fairness evaluation"""
arXIv2023,Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,Yes.,5,"""Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold."" and ""Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research",2023,2023-05-22T17:36:58Z,"Keyphrase: ""Inability to detect certain types of hate speech""","""Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold."" and ""Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research Keyphrase: ""Inability to detect certain types of hate speech"""
arXIv2023,Prompting is not a substitute for probability measurements in large language models,Yes.,5,"""we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities.""",2023,2023-05-22T17:33:17Z,"Keyphrase: ""Inferior metalinguistic judgment""","""we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities."" Keyphrase: ""Inferior metalinguistic judgment"""
arXIv2023,"""According to ..."": Prompting Language Models Improves Quoting from Pre-Training Data",Yes.,5,"""Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data.""",2023,2023-05-22T17:25:24Z,"Keyphrase: ""Generation of fake information""","""Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data."" Keyphrase: ""Generation of fake information"""
arXIv2023,To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis,Yes.,5,"""LLMs are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs,"" ""revealing that the model is susceptible to overfitting, leading to multi-epoch degradation,"" ""significant factors",2023,2023-05-22T17:02:15Z,"Keyphrase: ""Susceptible to overfitting""","""LLMs are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs,"" ""revealing that the model is susceptible to overfitting, leading to multi-epoch degradation,"" ""significant factors Keyphrase: ""Susceptible to overfitting"""
arXIv2023,SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables,Yes.,5,"""Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of",2023,2023-05-22T16:13:50Z,"Keyphrase: ""Challenges in scientific table understanding""","""Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of Keyphrase: ""Challenges in scientific table understanding"""
arXIv2023,Teaching Probabilistic Logical Reasoning to Transformers,Yes.,5,"""Our evaluation results show that both generations of language models struggle with reasoning over uncertain text.""",2023,2023-05-22T16:08:20Z,"Keyphrase: ""Struggles with uncertain reasoning""","""Our evaluation results show that both generations of language models struggle with reasoning over uncertain text."" Keyphrase: ""Struggles with uncertain reasoning"""
arXIv2023,"Editing Large Language Models: Problems, Methods, and Opportunities",Yes.,4,"""Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive."" and ""we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal.""",2023,2023-05-22T16:00:00Z,"Keyphrase: ""Challenge in maintaining relevancy and rectifying errors""","""Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive."" and ""we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal."" Keyphrase: ""Challenge in maintaining relevancy and rectifying errors"""
arXIv2023,Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,Yes.,5,"""it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way."" and ""LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments.""",2023,2023-05-22T15:47:31Z,"Keyphrase: ""Shallow reasoning and susceptibility to absurd arguments""","""it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way."" and ""LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments."" Keyphrase: ""Shallow reasoning and susceptibility to absurd arguments"""
arXIv2023,"Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students",Yes.,4,"""it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes"" and ""Our findings indicate that LLMs have an overall negative perception of math and STEM fields, with math being perceived most negatively.""",2023,2023-05-22T15:06:51Z,"Keyphrase: ""Perpetuation of harmful stereotypes""","""it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes"" and ""Our findings indicate that LLMs have an overall negative perception of math and STEM fields, with math being perceived most negatively."" Keyphrase: ""Perpetuation of harmful stereotypes"""
arXIv2023,Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization,Yes.,5,"""We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance",2023,2023-05-22T14:58:13Z,"Keyphrase: ""Inconsistent evaluation and comparison""","""We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance Keyphrase: ""Inconsistent evaluation and comparison"""
arXIv2023,Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model,Yes.,4,"""Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research.""",2023,2023-05-22T13:16:07Z,"Keyphrase: ""Limited interpretability""","""Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research."" Keyphrase: ""Limited interpretability"""
arXIv2023,ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,Yes.,4,"""we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning.""",2023,2023-05-22T11:45:42Z,"Keyphrase: ""Limited computational reasoning""","""we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning."" Keyphrase: ""Limited computational reasoning"""
arXIv2023,Automatic Code Summarization via ChatGPT: How Far Are We?,Yes.,5,"""The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models.""",2023,2023-05-22T09:43:40Z,"Keyphrase: ""Poor summarization performance""","""The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models."" Keyphrase: ""Poor summarization performance"""
arXIv2023,Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage,Yes.,4,"""Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy.""",2023,2023-05-22T04:30:35Z,"Keyphrase: ""Limited commonsense knowledge accuracy""","""Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy."" Keyphrase: ""Limited commonsense knowledge accuracy"""
arXIv2023,Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction,Yes.,5,"""Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition."" and ""The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT",2023,2023-05-22T03:04:06Z,"Keyphrase: ""Overlooking structural underpinnings""","""Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition."" and ""The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT Keyphrase: ""Overlooking structural underpinnings"""
arXIv2023,Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models,Yes.,5,"""We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs.""",2023,2023-05-21T19:06:30Z,"Keyphrase: ""Limited research coverage""","""We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs."" Keyphrase: ""Limited research coverage"""
arXIv2023,Retrieving Texts based on Abstract Descriptions,Yes.,5,"""While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval)."" and ""the retrieval task cannot be performed by the LLM directly.""",2023,2023-05-21T17:14:31Z,"Keyphrase: ""Limited semantic retrieval capabilities""","""While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval)."" and ""the retrieval task cannot be performed by the LLM directly."" Keyphrase: ""Limited semantic retrieval capabilities"""
arXIv2023,"GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts",Yes.,5,"""there is a current hot debate regarding their reasoning capacity"" and ""the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks.""",2023,2023-05-21T14:45:17Z,"Keyphrase: ""Limited reasoning proficiency""","""there is a current hot debate regarding their reasoning capacity"" and ""the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks."" Keyphrase: ""Limited reasoning proficiency"""
arXIv2023,VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models,Yes.,5,"""They still have space to grow, though, especially in the areas of mathematics, physics, chemistry, and biology."" and ""especially in resolving LLMs' limits in disciplines involving mathematics and the natural sciences.""",2023,2023-05-20T14:13:08Z,"Keyphrase: ""Limited understanding of complex disciplines""","""They still have space to grow, though, especially in the areas of mathematics, physics, chemistry, and biology."" and ""especially in resolving LLMs' limits in disciplines involving mathematics and the natural sciences."" Keyphrase: ""Limited understanding of complex disciplines"""
arXIv2023,Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?,Yes.,5,"""while doing fairly well on contexts that follow the common assumptions, the models struggle to correctly reason over contexts that break those assumptions. Specifically, the performance gap is as high as 20% absolute points.""",2023,2023-05-20T05:20:37Z,"Keyphrase: ""Struggles with contextual reasoning""","""while doing fairly well on contexts that follow the common assumptions, the models struggle to correctly reason over contexts that break those assumptions. Specifically, the performance gap is as high as 20% absolute points."" Keyphrase: ""Struggles with contextual reasoning"""
arXIv2023,UP5: Unbiased Foundation Model for Fairness-aware Recommendation,Yes.,4,"""there is unfairness involved in LLMs that lead to unfair recommendation results.""",2023,2023-05-20T04:32:59Z,"Keyphrase: ""Unfair recommendations""","""there is unfairness involved in LLMs that lead to unfair recommendation results."" Keyphrase: ""Unfair recommendations"""
arXIv2023,Clinical Camel: An Open Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding,Yes.,4,"""Significant challenges concerning reliability, bias, and the potential for outdated knowledge persist.""",2023,2023-05-19T23:07:09Z,"Keyphrase: ""Reliability and bias challenges""","""Significant challenges concerning reliability, bias, and the potential for outdated knowledge persist."" Keyphrase: ""Reliability and bias challenges"""
arXIv2023,Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,Yes.,5,"""However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst."" and ""They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews.""",2023,2023-05-19T17:09:19Z,"Keyphrase: ""Potential for misleading text""","""However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst."" and ""They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews."" Keyphrase: ""Potential for misleading text"""
arXIv2023,Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs,Yes.,5,"""However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure",2023,2023-05-19T16:27:43Z,"Keyphrase: ""Overlooking linguistic cues""","""However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure Keyphrase: ""Overlooking linguistic cues"""
arXIv2023,Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning,Yes.,5,"""Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk.""",2023,2023-05-19T15:45:29Z,"Keyphrase: ""Privacy risk from memorization""","""Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk."" Keyphrase: ""Privacy risk from memorization"""
arXIv2023,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Yes.,5,"""Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge."" and ""Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts.""",2023,2023-05-19T15:36:27Z,"Keyphrase: ""Hallucination generation""","""Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge."" and ""Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts."" Keyphrase: ""Hallucination generation"""
arXIv2023,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,Yes.,5,"""these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content.""",2023,2023-05-19T15:19:44Z,"Keyphrase: ""Inconsistent and problematic behavior""","""these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content."" Keyphrase: ""Inconsistent and problematic behavior"""
arXIv2023,Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses,Yes.,5,"""We show that its multilingual consistency is still lacking, and that its task and world understanding are thus not language-independent.""",2023,2023-05-19T13:23:51Z,"Keyphrase: ""Lack of multilingual consistency""","""We show that its multilingual consistency is still lacking, and that its task and world understanding are thus not language-independent."" Keyphrase: ""Lack of multilingual consistency"""
arXIv2023,LLM-Pruner: On the Structural Pruning of Large Language Models,Yes.,4,"""such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages.""",2023,2023-05-19T12:10:53Z,"Keyphrase: ""Deployment challenges due to model size""","""such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages."" Keyphrase: ""Deployment challenges due to model size"""
arXIv2023,RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought,Yes.,5,"""LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems.""",2023,2023-05-19T08:02:52Z,"Keyphrase: ""Factual consistency and reasoning challenges""","""LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems."" Keyphrase: ""Factual consistency and reasoning challenges"""
arXIv2023,Graphologue: Exploring Large Language Model Responses with Interactive Diagrams,Yes.,5,"""However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure.""",2023,2023-05-19T06:53:25Z,"Keyphrase: ""Limited support for complex information tasks""","""However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure."" Keyphrase: ""Limited support for complex information tasks"""
arXIv2023,A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation,Yes.,5,"""First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs.""",2023,2023-05-19T02:41:12Z,"Keyphrase: ""Vulnerability and unintended bugs""","""First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs."" Keyphrase: ""Vulnerability and unintended bugs"""
arXIv2023,ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery,Yes.,4,"""data-sensitive domains -- including but not limited to healthcare -- face challenges in using ChatGPT due to privacy and data-ownership concerns.""",2023,2023-05-19T02:09:52Z,"Keyphrase: ""Privacy and data ownership concerns""","""data-sensitive domains -- including but not limited to healthcare -- face challenges in using ChatGPT due to privacy and data-ownership concerns."" Keyphrase: ""Privacy and data ownership concerns"""
arXIv2023,Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models,Yes.,4,"""the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically.""",2023,2023-05-19T00:53:45Z,"Keyphrase: ""Repetitive and surprising data generation""","""the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically."" Keyphrase: ""Repetitive and surprising data generation"""
arXIv2023,CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models,Yes.,4,"""Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias."" and ""Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases.""",2023,2023-05-18T18:58:30Z,"Keyphrase: ""Stereotypical biases and safety concerns""","""Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias."" and ""Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases."" Keyphrase: ""Stereotypical biases and safety concerns"""
arXIv2023,Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses,Yes.,4,"""We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA's responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require",2023,2023-05-18T18:15:43Z,"Keyphrase: ""Limited causal reasoning ability""","""We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA's responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require Keyphrase: ""Limited causal reasoning ability"""
arXIv2023,Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings,Yes.,4,"""Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks.""",2023,2023-05-18T07:56:40Z,"Keyphrase: ""Bias in sentence embeddings""","""Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks."" Keyphrase: ""Bias in sentence embeddings"""
arXIv2023,"Ethical ChatGPT: Concerns, Challenges, and Commandments",Yes.,4,"""there are indeed ethical concerns associated with the use of AI language models such as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights specific ethical concerns on ChatGPT and articulates key challenges when ChatGPT is used in various applications.""",2023,2023-05-18T02:04:13Z,"Keyphrase: ""Ethical concerns and privacy issues""","""there are indeed ethical concerns associated with the use of AI language models such as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights specific ethical concerns on ChatGPT and articulates key challenges when ChatGPT is used in various applications."" Keyphrase: ""Ethical concerns and privacy issues"""
arXIv2023,Language Models Meet World Models: Embodied Experiences Enhance Language Models,Yes.,5,"""they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills.""",2023,2023-05-18T00:35:38Z,"Keyphrase: ""Lack of embodied knowledge""","""they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills."" Keyphrase: ""Lack of embodied knowledge"""
arXIv2023,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,Yes.,5,"""Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.""",2023,2023-05-17T23:16:17Z,"Keyphrase: ""Limited strategic lookahead""","""Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role."" Keyphrase: ""Limited strategic lookahead"""
arXIv2023,"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",Yes.,4,"""While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU.""",2023,2023-05-17T20:45:13Z,"Keyphrase: ""Memory-intensive deployment""","""While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU."" Keyphrase: ""Memory-intensive deployment"""
arXIv2023,ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages,Yes.,5,"""We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations"" and ""ChatGPT appears to confer a higher respect to men than to women in the same occupation.""",2023,2023-05-17T18:30:05Z,"Keyphrase: ""Gender bias perpetuation""","""We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations"" and ""ChatGPT appears to confer a higher respect to men than to women in the same occupation."" Keyphrase: ""Gender bias perpetuation"""
arXIv2023,BAD: BiAs Detection for Large Language Models in the context of candidate screening,Yes.,4,"""The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed."" and ""we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could",2023,2023-05-17T17:47:31Z,"Keyphrase: ""Bias in automated screening""","""The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed."" and ""we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could Keyphrase: ""Bias in automated screening"""
arXIv2023,Evaluating Object Hallucination in Large Vision-Language Models,Yes.,5,"""we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions."" and ""show that they mostly suffer from severe object hallucination issue.""",2023,2023-05-17T16:34:01Z,"Keyphrase: ""Severe object hallucination""","""we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions."" and ""show that they mostly suffer from severe object hallucination issue."" Keyphrase: ""Severe object hallucination"""
arXIv2023,Language Model Tokenizers Introduce Unfairness Between Languages,Yes.,4,"""disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked"" and ""This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided",2023,2023-05-17T14:17:57Z,"Keyphrase: ""Unfair treatment in language processing""","""disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked"" and ""This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided Keyphrase: ""Unfair treatment in language processing"""
arXIv2023,Can Language Models Solve Graph Problems in Natural Language?,Yes.,5,"""whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored"" and ""the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings",2023,2023-05-17T08:29:21Z,"Keyphrase: ""Brittle handling of spurious correlations""","""whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored"" and ""the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings Keyphrase: ""Brittle handling of spurious correlations"""
arXIv2023,Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models,Yes.,5,"""By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.""",2023,2023-05-17T05:25:27Z,"Keyphrase: ""Limited ability to generate factual and up-to-date knowledge""","""By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge."" Keyphrase: ""Limited ability to generate factual and up-to-date knowledge"""
arXIv2023,"""I'm fully who I am"": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",Yes.,4,"""We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on",2023,2023-05-17T04:21:45Z,"Keyphrase: ""Gender misrepresentation""","""We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on Keyphrase: ""Gender misrepresentation"""
arXIv2023,Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models,Yes.,5,"""We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness.""",2023,2023-05-16T20:52:04Z,"Keyphrase: ""Pattern of failure""","""We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness."" Keyphrase: ""Pattern of failure"""
arXIv2023,Small Models are Valuable Plug-ins for Large Language Models,Yes.,5,"""Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of",2023,2023-05-15T17:59:01Z,"Keyphrase: ""Limited accessibility to tuning and hardware resources""","""Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of Keyphrase: ""Limited accessibility to tuning and hardware resources"""
arXIv2023,Large Language Models are Zero-Shot Rankers for Recommender Systems,Yes.,5,"""We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts.""",2023,2023-05-15T17:57:39Z,"Keyphrase: ""Limited historical context and biased prompt influence""","""We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts."" Keyphrase: ""Limited historical context and biased prompt influence"""
arXIv2023,"Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",Yes.,5,"""However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed."" and ""they are",2023,2023-05-15T15:44:51Z,"Keyphrase: ""Lack of thorough risk analysis""","""However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed."" and ""they are Keyphrase: ""Lack of thorough risk analysis"""
arXIv2023,Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks,Yes.,5,"""a critical issue has been identified within this domain",2023,2023-05-15T15:19:08Z,"Keyphrase: ""Limited domain knowledge""","""a critical issue has been identified within this domain Keyphrase: ""Limited domain knowledge"""
arXIv2023,Unsupervised Sentence Representation Learning with Frequency-induced Adversarial Tuning and Incomplete Sentence Filtering,Yes.,5,"""PLMs are sensitive to the frequency information of words from their pre-training corpora, resulting in anisotropic embedding space, where the embeddings of high-frequency words are clustered but those of low-frequency words disperse sparsely. This anisotropic phenomenon results in two problems of similarity bias and information bias, lowering the quality of sentence embeddings.""",2023,2023-05-15T13:59:23Z,"Keyphrase: ""Anisotropic embedding space""","""PLMs are sensitive to the frequency information of words from their pre-training corpora, resulting in anisotropic embedding space, where the embeddings of high-frequency words are clustered but those of low-frequency words disperse sparsely. This anisotropic phenomenon results in two problems of similarity bias and information bias, lowering the quality of sentence embeddings."" Keyphrase: ""Anisotropic embedding space"""
arXIv2023,Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study,Yes.,5,"""ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations. We also found that ChatGPT hardly understands rhetorical structures that are more complex than topic structures.""",2023,2023-05-15T07:14:41Z,"Keyphrase: ""Limited understanding of complex topic structures""","""ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations. We also found that ChatGPT hardly understands rhetorical structures that are more complex than topic structures."" Keyphrase: ""Limited understanding of complex topic structures"""
arXIv2023,Semantic Composition in Visually Grounded Language Models,Yes.,5,"""Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure.""",2023,2023-05-15T03:19:42Z,"Keyphrase: ""Failure in representing compositional structure""","""Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure."" Keyphrase: ""Failure in representing compositional structure"""
arXIv2023,Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency,Yes.,5,"""non-linguistic skill injection typically comes at a cost for LLMs",2023,2023-05-14T20:57:11Z,"Keyphrase: ""Cost of nonlinguistic skill injection""","""non-linguistic skill injection typically comes at a cost for LLMs Keyphrase: ""Cost of nonlinguistic skill injection"""
arXIv2023,Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics,Yes.,4,"""Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity."" and ""The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the LLM's ability to generalize its knowledge about human behavior in social dilem",2023,2023-05-13T17:23:16Z,"Keyphrase: ""Limited ability to generalize human behavior""","""Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity."" and ""The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the LLM's ability to generalize its knowledge about human behavior in social dilem Keyphrase: ""Limited ability to generalize human behavior"""
arXIv2023,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,Yes.,5,"""existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading",2023,2023-05-13T14:23:07Z,"Keyphrase: ""Limited architectural flexibility""","""existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading Keyphrase: ""Limited architectural flexibility"""
arXIv2023,TinyStories: How Small Can Language Models Be and Still Speak Coherent English?,Yes.,5,"""Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English",2023,2023-05-12T20:56:48Z,"Keyphrase: ""Struggles with coherence and consistency""","""Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English Keyphrase: ""Struggles with coherence and consistency"""
arXIv2023,Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation,Yes.,4,"""it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation."" and ""we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations.""",2023,2023-05-12T16:54:36Z,"Keyphrase: ""Social prejudice and unfairness""","""it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation."" and ""we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations."" Keyphrase: ""Social prejudice and unfairness"""
arXIv2023,Surfacing Biases in Large Language Models using Contrastive Input Decoding,Yes.,5,"""Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour"" and ""We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturb",2023,2023-05-12T11:09:49Z,"Keyphrase: ""Context-specific bias detection""","""Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour"" and ""We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturb Keyphrase: ""Context-specific bias detection"""
arXIv2023,Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation,Yes.,5,"""Experiments show that ChatGPT is not a good causal reasoner, but a good causal explainer. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes,",2023,2023-05-12T10:54:13Z,"Keyphrase: ""Hallucination in causal reasoning""","""Experiments show that ChatGPT is not a good causal reasoner, but a good causal explainer. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, Keyphrase: ""Hallucination in causal reasoning"""
arXIv2023,Evaluating Open-Domain Question Answering in the Era of Large Language Models,Yes.,5,"""The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.""",2023,2023-05-11T17:14:33Z,"Keyphrase: ""Difficulty in detecting hallucination""","""The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation."" Keyphrase: ""Difficulty in detecting hallucination"""
arXIv2023,Active Retrieval Augmented Generation,Yes.,5,"""Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.""",2023,2023-05-11T17:13:40Z,"Keyphrase: ""Factually inaccurate output""","""Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output."" Keyphrase: ""Factually inaccurate output"""
arXIv2023,Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models,Yes.,5,"""While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets."" and ""We find that while existing debiasing methods can mitigate reliance on a chosen spurious feature, the OOD performance gains of these methods can not be explained by mitigated reliance on biased features",2023,2023-05-11T14:35:00Z,"Keyphrase: ""Reliance on spurious correlations""","""While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets."" and ""We find that while existing debiasing methods can mitigate reliance on a chosen spurious feature, the OOD performance gains of these methods can not be explained by mitigated reliance on biased features Keyphrase: ""Reliance on spurious correlations"""
arXIv2023,Chain-of-Dictionary Prompting Elicits Translation in Large Language Models,Yes.,5,"""they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation.""",2023,2023-05-11T05:19:47Z,"Keyphrase: ""Struggles with rare words and low-resource languages""","""they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation."" Keyphrase: ""Struggles with rare words and low-resource languages"""
arXIv2023,How Good are Commercial Large Language Models on African Languages?,Yes.,4,"""However, their performance on African languages is largely unknown."" and ""Our results suggest that commercial language models produce below-par performance on African languages.""",2023,2023-05-11T02:29:53Z,"Keyphrase: ""Poor performance in African languages""","""However, their performance on African languages is largely unknown."" and ""Our results suggest that commercial language models produce below-par performance on African languages."" Keyphrase: ""Poor performance in African languages"""
arXIv2023,"Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",Yes.,5,"""We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents.""",2023,2023-05-10T16:40:37Z,"Keyphrase: ""Limited aggregation capability""","""We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents."" Keyphrase: ""Limited aggregation capability"""
arXIv2023,"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",Yes.,4,"""examines the errors produced by the LLMs and categorized the errors into three types",2023,2023-05-10T13:40:06Z,"Keyphrase: ""Error categorization limitations""","""examines the errors produced by the LLMs and categorized the errors into three types Keyphrase: ""Error categorization limitations"""
arXIv2023,Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,Yes.,5,"""Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge,"" and ""statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.""",2023,2023-05-10T08:35:50Z,"Keyphrase: ""Limited commonsense knowledge""","""Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge,"" and ""statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict."" Keyphrase: ""Limited commonsense knowledge"""
arXIv2023,Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition,Yes.,5,"""This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT.""",2023,2023-05-10T08:16:46Z,"Keyphrase: ""Limited critical insight""","""This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT."" Keyphrase: ""Limited critical insight"""
arXIv2023,ChatGPT as a Text Simplification Tool to Remove Bias,Yes.,4,"""The presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. If the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based upon their protected characteristic, leading to possible discrimination.""",2023,2023-05-09T13:10:23Z,"Keyphrase: ""Association of language with protected characteristics""","""The presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. If the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based upon their protected characteristic, leading to possible discrimination."" Keyphrase: ""Association of language with protected characteristics"""
arXIv2023,Explanation-based Finetuning Makes Models More Robust to Spurious Cues,Yes.,4,"""Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data.""",2023,2023-05-08T18:53:45Z,"Keyphrase: ""Poor generalization to out-of-distribution data""","""Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data."" Keyphrase: ""Poor generalization to out-of-distribution data"""
arXIv2023,Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust,Yes.,5,"""The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes."" and ""However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs.""",2023,2023-05-08T18:53:14Z,"Keyphrase: ""Opacity in semantic understanding""","""The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes."" and ""However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs."" Keyphrase: ""Opacity in semantic understanding"""
arXIv2023,How Do In-Context Examples Affect Compositional Generalization?,Yes.,5,"""We find that the compositional generalization performance can be easily affected by the selection of in-context examples,"" and ""two strong limitations are observed",2023,2023-05-08T16:32:18Z,"Keyphrase: ""Compositional generalization affected by context""","""We find that the compositional generalization performance can be easily affected by the selection of in-context examples,"" and ""two strong limitations are observed Keyphrase: ""Compositional generalization affected by context"""
arXIv2023,Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns,Yes.,4,"""underscores the challenges in developing safe and unbiased LLMs, and emphasizes the importance of understanding the susceptibility of LLMs to external influences.""",2023,2023-05-08T16:10:18Z,"Keyphrase: ""Susceptibility to external influence""","""underscores the challenges in developing safe and unbiased LLMs, and emphasizes the importance of understanding the susceptibility of LLMs to external influences."" Keyphrase: ""Susceptibility to external influence"""
arXIv2023,Augmented Large Language Models with Parametric Knowledge Guiding,Yes.,4,"""However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom",2023,2023-05-08T15:05:16Z,"Keyphrase: ""Limited domain-specific knowledge and transparency""","""However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom Keyphrase: ""Limited domain-specific knowledge and transparency"""
arXIv2023,Differentially Private Attention Computation,Yes.,4,"""one crucial issue concerning the inference results of large language models is security and privacy"" and ""results generated by LLMs could possibly leak many confidential or copyright information.""",2023,2023-05-08T13:32:41Z,"Keyphrase: ""Security and privacy concerns""","""one crucial issue concerning the inference results of large language models is security and privacy"" and ""results generated by LLMs could possibly leak many confidential or copyright information."" Keyphrase: ""Security and privacy concerns"""
arXIv2023,Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,Yes.,5,"""However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction."" and ""Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety.""",2023,2023-05-07T22:44:25Z,"Keyphrase: ""Misleading explanations""","""However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction."" and ""Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety."" Keyphrase: ""Misleading explanations"""
arXIv2023,Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?,Yes.,4,"""However, these abilities are quite limited and worse than well-trained humans when the tasks are not known and are not part of the training data.""",2023,2023-05-06T20:53:22Z,"Keyphrase: ""Limited human task performance""","""However, these abilities are quite limited and worse than well-trained humans when the tasks are not known and are not part of the training data."" Keyphrase: ""Limited human task performance"""
arXIv2023,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,Yes.,5,"""Despite the success of Zero-shot-CoT, it still suffers from three pitfalls",2023,2023-05-06T16:34:37Z,"Keyphrase: ""Limitations in zero-shot capability""","""Despite the success of Zero-shot-CoT, it still suffers from three pitfalls Keyphrase: ""Limitations in zero-shot capability"""
arXIv2023,Pre-training Language Model as a Multi-perspective Course Learner,Yes.,4,"""Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning.""",2023,2023-05-06T09:02:10Z,"Keyphrase: ""Biased learning and label imbalance""","""Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning."" Keyphrase: ""Biased learning and label imbalance"""
arXIv2023,"Large Language Models in Sport Science & Medicine: Opportunities, Risks and Considerations",Yes.,4,"""However, there are also potential risks associated with the use and development of LLMs, including biases in the dataset used to create the model, the risk of exposing confidential data, the risk of generating harmful output, and the need to align these models with human preferences through feedback.""",2023,2023-05-05T21:20:02Z,"Keyphrase: ""Risk of bias and harmful output""","""However, there are also potential risks associated with the use and development of LLMs, including biases in the dataset used to create the model, the risk of exposing confidential data, the risk of generating harmful output, and the need to align these models with human preferences through feedback."" Keyphrase: ""Risk of bias and harmful output"""
arXIv2023,Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,Yes.,4,"""Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures.""",2023,2023-05-05T17:15:32Z,"Keyphrase: ""Commonsense failures""","""Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures."" Keyphrase: ""Commonsense failures"""
arXIv2023,Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming,Yes.,5,"""Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality.""",2023,2023-05-05T07:24:46Z,"Keyphrase: ""Limited logical reasoning""","""Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality."" Keyphrase: ""Limited logical reasoning"""
arXIv2023,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,Yes.,5,"""one of its most fatal disadvantages is the lack of factual correctness"" and ""still suffers from factuality concerns in knowledge-intensive tasks.""",2023,2023-05-05T03:49:14Z,"Keyphrase: ""Factual correctness deficiency""","""one of its most fatal disadvantages is the lack of factual correctness"" and ""still suffers from factuality concerns in knowledge-intensive tasks."" Keyphrase: ""Factual correctness deficiency"""
arXIv2023,Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,Yes.,5,"""even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand.""",2023,2023-05-04T19:02:29Z,"Keyphrase: ""Limited execution accuracy""","""even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand."" Keyphrase: ""Limited execution accuracy"""
arXIv2023,Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision,Yes.,4,"""However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases.""",2023,2023-05-04T17:59:28Z,"Keyphrase: ""Dependence on human supervision""","""However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases."" Keyphrase: ""Dependence on human supervision"""
arXIv2023,"""Oops, Did I Just Say That?"" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process",Yes.,4,"""their subtly unethical suggestions become a serious and real concern"" and ""our study on seven popular LLMs (e.g., ChatGPT, GPT-4) uncovers in total 109,824 unethical suggestions.""",2023,2023-05-04T08:00:32Z,"Keyphrase: ""Ethical concerns and unethical suggestions""","""their subtly unethical suggestions become a serious and real concern"" and ""our study on seven popular LLMs (e.g., ChatGPT, GPT-4) uncovers in total 109,824 unethical suggestions."" Keyphrase: ""Ethical concerns and unethical suggestions"""
arXIv2023,Can LLMs Capture Human Preferences?,Yes.,5,"""Though GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans."" and ""While directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings",2023,2023-05-04T03:51:31Z,"Keyphrase: ""Lexicographic preference bias""","""Though GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans."" and ""While directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings Keyphrase: ""Lexicographic preference bias"""
arXIv2023,"Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents",Yes.,4,"""the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent",2023,2023-05-03T20:11:22Z,"Keyphrase: ""Challenges in serving as an agent""","""the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent Keyphrase: ""Challenges in serving as an agent"""
arXIv2023,ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,Yes.,5,"""two major limitations hinder its potential applications",2023,2023-05-03T19:57:43Z,"Keyphrase: ""Limitations hindering potential applications""","""two major limitations hinder its potential applications Keyphrase: ""Limitations hindering potential applications"""
arXIv2023,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,Yes.,5,"""Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications.""",2023,2023-05-03T17:50:56Z,"Keyphrase: ""Memory inefficiency and computational intensity""","""Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications."" Keyphrase: ""Memory inefficiency and computational intensity"""
arXIv2023,GPT-RE: In-context Learning for Relation Extraction using Large Language Models,Yes.,5,"""they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE",2023,2023-05-03T13:28:08Z,"Keyphrase: ""Lagging behind fully-supervised baselines""","""they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE Keyphrase: ""Lagging behind fully-supervised baselines"""
arXIv2023,Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy,Yes.,5,"""LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately."" and ""Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy.""",2023,2023-05-02T15:53:28Z,"Keyphrase: ""Memorization of training data""","""LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately."" and ""Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy."" Keyphrase: ""Memorization of training data"""
arXIv2023,Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation,Yes.,5,"""However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code."" and ""Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k",2023,2023-05-02T05:46:48Z,"Keyphrase: ""Limited testing coverage""","""However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code."" and ""Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k Keyphrase: ""Limited testing coverage"""
arXIv2023,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Yes.,5,"""this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors.""",2023,2023-05-01T17:36:06Z,"Keyphrase: ""Toxic content generation""","""this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors."" Keyphrase: ""Toxic content generation"""
arXIv2023,Poisoning Language Models During Instruction Tuning,Yes.,5,"""we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions"" and ""Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.""",2023,2023-05-01T16:57:33Z,"Keyphrase: ""Vulnerability to poisoning attacks""","""we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions"" and ""Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy."" Keyphrase: ""Vulnerability to poisoning attacks"""
arXIv2023,Learning to Reason and Memorize with Self-Notes,Yes.,5,"""Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use.""",2023,2023-05-01T14:02:48Z,"Keyphrase: ""Difficulty in retaining multistep reasoning""","""Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use."" Keyphrase: ""Difficulty in retaining multistep reasoning"""
arXIv2023,Self-Evaluation Guided Beam Search for Reasoning,Yes.,5,"""the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results.""",2023,2023-05-01T02:37:59Z,"Keyphrase: ""Error accumulation in reasoning chain""","""the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results."" Keyphrase: ""Error accumulation in reasoning chain"""
arXIv2023,Causal Reasoning and Large Language Models: Opening a New Frontier for Causality,Yes.,4,"""The causal capabilities of large language models (LLMs) is a matter of significant debate,"" and ""LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.""",2023,2023-04-28T19:00:43Z,"Keyphrase: ""Unpredictable failure modes""","""The causal capabilities of large language models (LLMs) is a matter of significant debate,"" and ""LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness."" Keyphrase: ""Unpredictable failure modes"""
arXIv2023,"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",Yes.,5,"""albeit it may not possess the same level of expertise in identifying the temporal order between two events"" and ""the implicit discourse relation remains a formidable challenge"" and ""ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.""",2023,2023-04-28T13:14:36Z,"Keyphrase: ""Subpar performance in discourse parsing""","""albeit it may not possess the same level of expertise in identifying the temporal order between two events"" and ""the implicit discourse relation remains a formidable challenge"" and ""ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation."" Keyphrase: ""Subpar performance in discourse parsing"""
arXIv2023,We're Afraid Language Models Aren't Modeling Ambiguity,Yes.,5,"""We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset.""",2023,2023-04-27T17:57:58Z,"Keyphrase: ""Ambiguity disentanglement challenge""","""We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset."" Keyphrase: ""Ambiguity disentanglement challenge"""
arXIv2023,Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery,Yes.,5,"""current explorations do not assess the real-world utility and safety of LLMs in clinical settings,"" ""responses contained hallucinated references,"" and ""often do not meet the specific information need of a given question.""",2023,2023-04-26T17:54:28Z,"Keyphrase: ""Hallucinated responses""","""current explorations do not assess the real-world utility and safety of LLMs in clinical settings,"" ""responses contained hallucinated references,"" and ""often do not meet the specific information need of a given question."" Keyphrase: ""Hallucinated responses"""
arXIv2023,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,Yes.,4,"""We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on",2023,2023-04-26T17:52:30Z,"Keyphrase: ""Spurious bias and data-specific challenges""","""We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on Keyphrase: ""Spurious bias and data-specific challenges"""
arXIv2023,Enhancing Large Language Model with Self-Controlled Memory Framework,Yes.,5,"""Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information.""",2023,2023-04-26T07:25:31Z,"Keyphrase: ""Inability to process lengthy input""","""Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information."" Keyphrase: ""Inability to process lengthy input"""
arXIv2023,The Internal State of an LLM Knows When It's Lying,Yes.,5,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone.""",2023,2023-04-26T02:49:38Z,"Keyphrase: ""Confident false information""","""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone."" Keyphrase: ""Confident false information"""
arXIv2023,TABLET: Learning From Instructions For Tabular Data,Yes.,5,"""We find LLMs often ignore instructions and fail to predict specific instances correctly, even with examples.""",2023,2023-04-25T23:07:20Z,"Keyphrase: ""Failure to predict specific instances""","""We find LLMs often ignore instructions and fail to predict specific instances correctly, even with examples."" Keyphrase: ""Failure to predict specific instances"""
arXIv2023,AI-assisted coding: Experiments with GPT-4,Yes.,5,"""These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance"" and ""we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code.""",2023,2023-04-25T22:59:01Z,"Keyphrase: ""Dependence on human validation""","""These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance"" and ""we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code."" Keyphrase: ""Dependence on human validation"""
arXIv2023,The Potential of Visual ChatGPT For Remote Sensing,Yes.,4,"""we demonstrate the current model's limitations in dealing with remote sensing images, highlighting its challenges and future prospects.""",2023,2023-04-25T17:29:47Z,"Keyphrase: ""Challenges with remote sensing images""","""we demonstrate the current model's limitations in dealing with remote sensing images, highlighting its challenges and future prospects."" Keyphrase: ""Challenges with remote sensing images"""
arXIv2023,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",Yes.,5,"""Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa).""",2023,2023-04-25T17:05:38Z,"Keyphrase: ""Limited audio processing capabilities""","""Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa)."" Keyphrase: ""Limited audio processing capabilities"""
arXIv2023,Semantic Compression With Large Language Models,Yes.,5,"""However, in addition to confidently presenting factually inaccurate information at times (known as 'hallucinations'), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information.""",2023,2023-04-25T01:47:05Z,"Keyphrase: ""Limited input/output token processing""","""However, in addition to confidently presenting factually inaccurate information at times (known as 'hallucinations'), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information."" Keyphrase: ""Limited input/output token processing"""
arXIv2023,Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering,Yes.,5,"""However, their fixed context length poses challenges when processing long documents or maintaining extended conversations.""",2023,2023-04-24T13:55:47Z,"Keyphrase: ""Limited context length""","""However, their fixed context length poses challenges when processing long documents or maintaining extended conversations."" Keyphrase: ""Limited context length"""
arXIv2023,Is ChatGPT the Ultimate Programming Assistant -- How far is it?,Yes.,4,"""our experiments also reveal limitations in terms of its attention span",2023,2023-04-24T09:20:13Z,"Keyphrase: ""Limited attention span""","""our experiments also reveal limitations in terms of its attention span Keyphrase: ""Limited attention span"""
arXIv2023,Differentiate ChatGPT-generated and Human-written Medical Texts,Yes.,4,"""erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.""",2023,2023-04-23T07:38:07Z,"Keyphrase: ""Erroneous medical content""","""erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public."" Keyphrase: ""Erroneous medical content"""
arXIv2023,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,Yes.,5,"""However, so far, LLMs cannot reliably solve long-horizon planning problems.""",2023,2023-04-22T20:34:03Z,"Keyphrase: ""Limited long-horizon planning""","""However, so far, LLMs cannot reliably solve long-horizon planning problems."" Keyphrase: ""Limited long-horizon planning"""
arXIv2023,Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens,Yes.,5,"""These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.""",2023,2023-04-22T12:50:49Z,"Keyphrase: ""Dependency on massive training data""","""These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations."" Keyphrase: ""Dependency on massive training data"""
arXIv2023,Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers,Yes.,5,"""We found that LLMs perform worse than MLs in detecting incoherent answers. The difficulty seems to reside in recursive questions that contain both questions and answers, and in responses from students with typical fourth-grader misspellings.""",2023,2023-04-21T21:25:30Z,"Keyphrase: ""Difficulty with detecting incoherent answers""","""We found that LLMs perform worse than MLs in detecting incoherent answers. The difficulty seems to reside in recursive questions that contain both questions and answers, and in responses from students with typical fourth-grader misspellings."" Keyphrase: ""Difficulty with detecting incoherent answers"""
arXIv2023,Emergent and Predictable Memorization in Large Language Models,Yes.,5,"""Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models.""",2023,2023-04-21T17:58:31Z,"Keyphrase: ""Memorization tendency""","""Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models."" Keyphrase: ""Memorization tendency"""
arXIv2023,The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination,Yes.,5,"""new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination.""",2023,2023-04-21T16:40:54Z,"Keyphrase: ""Ethical and legal risks from hallucination""","""new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination."" Keyphrase: ""Ethical and legal risks from hallucination"""
arXIv2023,Inducing anxiety in large language models increases exploration and bias,Yes.,5,"""Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance."" and ""GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text.""",2023,2023-04-21T16:29:43Z,"Keyphrase: ""Increased societal bias""","""Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance."" and ""GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text."" Keyphrase: ""Increased societal bias"""
arXIv2023,ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT,Yes.,4,"""LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously.""",2023,2023-04-21T16:23:47Z,"Keyphrase: ""Limited reasoning capability""","""LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously."" Keyphrase: ""Limited reasoning capability"""
arXIv2023,Meta Semantics: Towards better natural language understanding and reasoning,Yes.,5,"""Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem.""",2023,2023-04-20T22:16:16Z,"Keyphrase: ""Weak logical deduction and out-of-vocabulary problem""","""Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem."" Keyphrase: ""Weak logical deduction and out-of-vocabulary problem"""
arXIv2023,Why Does ChatGPT Fall Short in Providing Truthful Answers?,Yes.,5,"""ChatGPT still faces challenges in providing reliable and accurate answers to user questions."" and ""We further pinpoint factuality as the most contributing failure and identify two critical abilities associated with factuality",2023,2023-04-20T17:48:43Z,"Keyphrase: ""Reliability and factuality challenges""","""ChatGPT still faces challenges in providing reliable and accurate answers to user questions."" and ""We further pinpoint factuality as the most contributing failure and identify two critical abilities associated with factuality Keyphrase: ""Reliability and factuality challenges"""
arXIv2023,Safety Assessment of Chinese Large Language Models,Yes.,5,"""These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information.""",2023,2023-04-20T16:27:35Z,"Keyphrase: ""Generating harmful content""","""These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information."" Keyphrase: ""Generating harmful content"""
arXIv2023,Fully Autonomous Programming with Large Language Models,Yes.,5,"""Current approaches to program synthesis with Large Language Models (LLMs) exhibit a 'near miss syndrome'",2023,2023-04-20T16:12:05Z,"Keyphrase: ""Near miss syndrome""","""Current approaches to program synthesis with Large Language Models (LLMs) exhibit a 'near miss syndrome' Keyphrase: ""Near miss syndrome"""
arXIv2023,Supporting Human-AI Collaboration in Auditing LLMs with LLMs,Yes.,4,"""Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale.""",2023,2023-04-19T21:59:04Z,"Keyphrase: ""Biased and irresponsible behavior""","""Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale."" Keyphrase: ""Biased and irresponsible behavior"""
arXIv2023,Fundamental Limitations of Alignment in Large Language Models,Yes.,5,"""Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.""",2023,2023-04-19T17:50:09Z,"Keyphrase: ""Safety concerns""","""Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety."" Keyphrase: ""Safety concerns"""
arXIv2023,Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models,Yes.,5,"""LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning.""",2023,2023-04-19T17:47:47Z,"Keyphrase: ""Limited access to up-to-date information""","""LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning."" Keyphrase: ""Limited access to up-to-date information"""
arXIv2023,In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT,Yes.,5,"""We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions."" and ""We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases.""",2023,2023-04-18T13:20:45Z,"Keyphrase: ""Domain-specific reliability issues""","""We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions."" and ""We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases."" Keyphrase: ""Domain-specific reliability issues"""
arXIv2023,Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs,Yes.,4,"""This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild. Unfortunately, most such tools are critically flawed.""",2023,2023-04-18T13:05:01Z,"Keyphrase: ""Flawed detection tools""","""This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild. Unfortunately, most such tools are critically flawed."" Keyphrase: ""Flawed detection tools"""
arXIv2023,An Evaluation on Large Language Model Outputs: Discourse and Memorization,Yes.,4,"""We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic.""",2023,2023-04-17T22:12:12Z,"Keyphrase: ""Memorization over understanding""","""We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic."" Keyphrase: ""Memorization over understanding"""
arXIv2023,Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark,Yes.,5,"""ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs."" and ""results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability.""",2023,2023-04-17T00:41:19Z,"Keyphrase: ""Inconsistent output reliability""","""ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs."" and ""results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability."" Keyphrase: ""Inconsistent output reliability"""
arXIv2023,VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping,Yes.,5,"""However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans.""",2023,2023-04-16T15:29:03Z,"Keyphrase: ""Limited support for user control and autonomy""","""However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans."" Keyphrase: ""Limited support for user control and autonomy"""
arXIv2023,The Self-Perception and Political Biases of ChatGPT,Yes.,4,"""This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT."" and ""claiming that ChatGPT is politically biased towards progressive and libertarian points of view.""",2023,2023-04-14T18:06:13Z,"Keyphrase: ""Political bias""","""This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT."" and ""claiming that ChatGPT is politically biased towards progressive and libertarian points of view."" Keyphrase: ""Political bias"""
arXIv2023,Stochastic Code Generation,Yes.,5,"""Large language models pre-trained for code generation can generate high-quality short code but often struggle with generating coherent long code and understanding higher-level or system-level specifications.""",2023,2023-04-14T00:01:05Z,"Keyphrase: ""Difficulty in generating coherent long code""","""Large language models pre-trained for code generation can generate high-quality short code but often struggle with generating coherent long code and understanding higher-level or system-level specifications."" Keyphrase: ""Difficulty in generating coherent long code"""
arXIv2023,Evaluation of Social Biases in Recent Large Pre-Trained Models,Yes.,4,"""Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models.""",2023,2023-04-13T23:29:58Z,"Keyphrase: ""Biased training data""","""Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models."" Keyphrase: ""Biased training data"""
arXIv2023,Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization),Yes.,4,"""One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of 'code analysis' and extracting such information, implicitly, while processing code",2023,2023-04-13T20:49:35Z,"Keyphrase: ""Limited code analysis capabilities""","""One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of 'code analysis' and extracting such information, implicitly, while processing code Keyphrase: ""Limited code analysis capabilities"""
arXIv2023,"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review",Yes.,4,"""this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation.""",2023,2023-04-13T16:01:28Z,"Keyphrase: ""Overlooking sustainability, privacy, digital divide, and ethics""","""this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation."" Keyphrase: ""Overlooking sustainability, privacy, digital divide, and ethics"""
arXIv2023,Towards Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model based Systems,Yes.,4,"""incorporating foundations models into AI systems raises significant concerns about responsible AI due to their opaque nature and rapidly advancing intelligence.""",2023,2023-04-13T05:01:03Z,"Keyphrase: ""Opaque nature and responsible AI concerns""","""incorporating foundations models into AI systems raises significant concerns about responsible AI due to their opaque nature and rapidly advancing intelligence."" Keyphrase: ""Opaque nature and responsible AI concerns"""
arXIv2023,ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning,Yes.,5,"""Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.""",2023,2023-04-12T05:08:52Z,"Keyphrase: ""Inferior performance compared to previous models""","""Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning."" Keyphrase: ""Inferior performance compared to previous models"""
arXIv2023,Understanding Causality with Large Language Models: Feasibility and Opportunities,Yes.,4,"""We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision.""",2023,2023-04-11T22:30:03Z,"Keyphrase: ""Limited causal reasoning""","""We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision."" Keyphrase: ""Limited causal reasoning"""
arXIv2023,chatClimate: Grounding Conversational AI in Climate Science,Yes.,5,"""they still face two major challenges",2023,2023-04-11T21:31:39Z,"Keyphrase: ""Major challenges unresolved""","""they still face two major challenges Keyphrase: ""Major challenges unresolved"""
arXIv2023,Zero-shot Temporal Relation Extraction with ChatGPT,Yes.,5,"""Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts"" and ""The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency",2023,2023-04-11T18:59:05Z,"Keyphrase: ""Inconsistent temporal inference""","""Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts"" and ""The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency Keyphrase: ""Inconsistent temporal inference"""
arXIv2023,Toxicity in ChatGPT: Analyzing Persona-assigned Language Models,Yes.,5,"""a clear understanding of the capabilities and limitations of LLMs is necessary,"" and ""We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations,"" and ""specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that",2023,2023-04-11T16:53:54Z,"Keyphrase: ""Toxicity amplification with assigned personas""","""a clear understanding of the capabilities and limitations of LLMs is necessary,"" and ""We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations,"" and ""specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that Keyphrase: ""Toxicity amplification with assigned personas"""
arXIv2023,Towards preserving word order importance through Forced Invalidation,Yes.,5,"""recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed.""",2023,2023-04-11T13:42:10Z,"Keyphrase: ""Insensitive to word order""","""recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed."" Keyphrase: ""Insensitive to word order"""
arXIv2023,Multi-step Jailbreaking Privacy Attacks on ChatGPT,Yes.,4,"""it is still challenging to steer AI-generated content (AIGC) for the human good"" and ""we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats.""",2023,2023-04-11T13:05:04Z,"Keyphrase: ""Privacy threats in AI-generated content""","""it is still challenging to steer AI-generated content (AIGC) for the human good"" and ""we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats."" Keyphrase: ""Privacy threats in AI-generated content"""
arXIv2023,Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,Yes.,4,"""Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3)",2023,2023-04-11T06:37:30Z,"Keyphrase: ""Challenges in scientific text detection""","""Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3) Keyphrase: ""Challenges in scientific text detection"""
arXIv2023,Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,Yes.,4,"""GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource languages."" and ""First, instruction semantics can surprisingly be ignored when given in-context exemplars.""",2023,2023-04-10T15:51:30Z,"Keyphrase: ""Limited performance in commercial translation systems""","""GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource languages."" and ""First, instruction semantics can surprisingly be ignored when given in-context exemplars."" Keyphrase: ""Limited performance in commercial translation systems"""
arXIv2023,Learnings from Data Integration for Augmented Language Models,Yes.,5,"""One of the limitations of large language models is that they do not have access to up-to-date, proprietary or personal data.""",2023,2023-04-10T13:28:35Z,"Keyphrase: ""Limited access to up-to-date proprietary personal data""","""One of the limitations of large language models is that they do not have access to up-to-date, proprietary or personal data."" Keyphrase: ""Limited access to up-to-date proprietary personal data"""
arXIv2023,Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,Yes.,5,"""However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.""",2023,2023-04-10T05:25:54Z,"Keyphrase: ""Weakness in multistep reasoning""","""However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}."" Keyphrase: ""Weakness in multistep reasoning"""
arXIv2023,The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges,Yes.,5,"""Our findings indicate that ChatGPT is a 'Wall Street Neophyte' with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features."" and ""we observe",2023,2023-04-10T04:31:00Z,"Keyphrase: ""Limited stock prediction accuracy""","""Our findings indicate that ChatGPT is a 'Wall Street Neophyte' with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features."" and ""we observe Keyphrase: ""Limited stock prediction accuracy"""
arXIv2023,Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding,Yes.,4,"""However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation.""",2023,2023-04-09T16:31:47Z,"Keyphrase: ""Challenges in clinical language understanding""","""However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation."" Keyphrase: ""Challenges in clinical language understanding"""
arXIv2023,Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance,Yes.,5,"""Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.""",2023,2023-04-09T04:53:15Z,"Keyphrase: ""Low interreliability with human ratings""","""Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings."" Keyphrase: ""Low interreliability with human ratings"""
arXIv2023,Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models,Yes.,5,"""This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions.""",2023,2023-04-07T17:14:00Z,"Keyphrase: ""Bias and risk factors""","""This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions."" Keyphrase: ""Bias and risk factors"""
arXIv2023,Revisiting Automated Prompting: Are We Actually Doing Better?,Yes.,5,"""We find that automated prompting does not consistently outperform simple manual prompts.""",2023,2023-04-07T12:06:44Z,"Keyphrase: ""Limited performance compared to manual prompts""","""We find that automated prompting does not consistently outperform simple manual prompts."" Keyphrase: ""Limited performance compared to manual prompts"""
arXIv2023,Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4,Yes.,5,"""However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets.""",2023,2023-04-07T01:37:45Z,"Keyphrase: ""Challenges with logical reasoning and out-of-distribution datasets""","""However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets."" Keyphrase: ""Challenges with logical reasoning and out-of-distribution datasets"""
arXIv2023,When do you need Chain-of-Thought Prompting for ChatGPT?,Yes.,5,"""Our analysis reflects a potential risk of overfitting/bias toward instructions introduced in IFT, which becomes more common in training LLMs. In addition, it indicates possible leakage of the pretraining recipe, e.g., one can verify whether a dataset and instruction were used in training ChatGPT.""",2023,2023-04-06T17:47:29Z,"Keyphrase: ""Risk of overfitting and bias""","""Our analysis reflects a potential risk of overfitting/bias toward instructions introduced in IFT, which becomes more common in training LLMs. In addition, it indicates possible leakage of the pretraining recipe, e.g., one can verify whether a dataset and instruction were used in training ChatGPT."" Keyphrase: ""Risk of overfitting and bias"""
arXIv2023,"Large language models effectively leverage document-level context for literary translation, but critical errors persist",Yes.,4,"""critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact.""",2023,2023-04-06T17:27:45Z,"Keyphrase: ""Content omission and need for human intervention""","""critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact."" Keyphrase: ""Content omission and need for human intervention"""
arXIv2023,Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions,Yes.,5,"""ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.""",2023,2023-04-06T05:01:28Z,"Keyphrase: ""Limited world knowledge and inference capabilities""","""ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses."" Keyphrase: ""Limited world knowledge and inference capabilities"""
arXIv2023,Approach Intelligent Writing Assistants Usability with Seven Stages of Action,Yes.,5,"""Despite the potential of Large Language Models (LLMs) as writing assistants, they are plagued by issues like coherence and fluency of the model output, trustworthiness, ownership of the generated content, and predictability of model performance, thereby limiting their usability.""",2023,2023-04-06T02:11:55Z,"Keyphrase: ""Lack of trustworthiness and predictability""","""Despite the potential of Large Language Models (LLMs) as writing assistants, they are plagued by issues like coherence and fluency of the model output, trustworthiness, ownership of the generated content, and predictability of model performance, thereby limiting their usability."" Keyphrase: ""Lack of trustworthiness and predictability"""
arXIv2023,GPT detectors are biased against non-native English writers,Yes.,4,"""Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified.""",2023,2023-04-06T01:51:15Z,"Keyphrase: ""Bias towards native English writing""","""Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified."" Keyphrase: ""Bias towards native English writing"""
arXIv2023,Conceptual structure coheres in human cognition but not in large language models,Yes.,5,"""These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language.""",2023,2023-04-05T21:27:01Z,"Keyphrase: ""Gap in human cognition""","""These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language."" Keyphrase: ""Gap in human cognition"""
arXIv2023,Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,Yes.,4,"""Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment.""",2023,2023-04-05T15:11:25Z,"Keyphrase: ""Limited performance compared to simpler models""","""Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment."" Keyphrase: ""Limited performance compared to simpler models"""
arXIv2023,Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation,Yes.,4,"""further analysis of various types of errors at the document-level has shown that ChatGPT cannot effectively correct agreement, coreference, tense errors across sentences, and cross-sentence boundary errors.""",2023,2023-04-04T12:33:40Z,"Keyphrase: ""Ineffective error correction""","""further analysis of various types of errors at the document-level has shown that ChatGPT cannot effectively correct agreement, coreference, tense errors across sentences, and cross-sentence boundary errors."" Keyphrase: ""Ineffective error correction"""
arXIv2023,"To ChatGPT, or not to ChatGPT: That is the question!",Yes.,5,"""concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud... none of the existing methods can effectively detect ChatGPT-generated content.""",2023,2023-04-04T03:04:28Z,"Keyphrase: ""Fake news and manipulation""","""concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud... none of the existing methods can effectively detect ChatGPT-generated content."" Keyphrase: ""Fake news and manipulation"""
arXIv2023,Blockwise Compression of Transformer-based Models without Retraining,Yes.,5,"""These operations bring the inevitable challenges of massive computation resources and huge memory footprint, usually requiring at least 10^23 FLOPs and hundreds of gigabytes, respectively.""",2023,2023-04-04T02:55:40Z,"Keyphrase: ""Resource-intensive operations""","""These operations bring the inevitable challenges of massive computation resources and huge memory footprint, usually requiring at least 10^23 FLOPs and hundreds of gigabytes, respectively."" Keyphrase: ""Resource-intensive operations"""
arXIv2023,Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT,Yes.,4,"""Comparative results show that using ChatGPT without human intervention may be inadequate due to reliability related issues,"" and ""We also highlight future challenges, including concerns about LLM trustworthiness and the necessity for standardisation and regulation in this domain.""",2023,2023-04-03T16:46:49Z,"Keyphrase: ""Reliability and trustworthiness issues""","""Comparative results show that using ChatGPT without human intervention may be inadequate due to reliability related issues,"" and ""We also highlight future challenges, including concerns about LLM trustworthiness and the necessity for standardisation and regulation in this domain."" Keyphrase: ""Reliability and trustworthiness issues"""
arXIv2023,DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task,Yes.,4,"""these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs.""",2023,2023-04-03T15:57:51Z,"Keyphrase: ""Limited performance in specialized domains""","""these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs."" Keyphrase: ""Limited performance in specialized domains"""
arXIv2023,Towards Healthy AI: Large Language Models Need Therapists Too,Yes.,4,"""these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors.""",2023,2023-04-02T00:39:12Z,"Keyphrase: ""Harmful behavior tendencies""","""these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors."" Keyphrase: ""Harmful behavior tendencies"""
arXIv2023,Enhancing Large Language Models with Climate Resources,Yes.,4,"""LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change.""",2023,2023-03-31T20:24:14Z,"Keyphrase: ""Lack of recent information and imprecise language""","""LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change."" Keyphrase: ""Lack of recent information and imprecise language"""
arXIv2023,Assessing Language Model Deployment with Risk Cards,Yes.,5,"""text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text."" and ""Prior work establishes a wide variety of language model harms to many different actors",2023,2023-03-31T16:45:42Z,"Keyphrase: ""Harmful text generation""","""text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text."" and ""Prior work establishes a wide variety of language model harms to many different actors Keyphrase: ""Harmful text generation"""
arXIv2023,Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations,Yes.,5,"""However, our evaluation also exposes critical limitations of the current LLM APIs. First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia. Further, our analysis shows that the API costs are generally higher and the maximum context size",2023,2023-03-31T13:04:47Z,"Keyphrase: ""Ethical limitations and costly choices""","""However, our evaluation also exposes critical limitations of the current LLM APIs. First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia. Further, our analysis shows that the API costs are generally higher and the maximum context size Keyphrase: ""Ethical limitations and costly choices"""
arXIv2023,GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors,Yes.,4,"""GPT-4 showed low accuracy in subjects including public health & medicine-related law, internal medicine (2) which are localized in Korea and TKM. The model's accuracy was lower for questions requiring TKM-specialized knowledge."" and ""These findings underline the potential of LLMs like GPT-4 in culturally adapted medicine, especially TKM, for tasks such as clinical assistance,",2023,2023-03-31T05:43:21Z,"Keyphrase: ""Low accuracy in specialized domains""","""GPT-4 showed low accuracy in subjects including public health & medicine-related law, internal medicine (2) which are localized in Korea and TKM. The model's accuracy was lower for questions requiring TKM-specialized knowledge."" and ""These findings underline the potential of LLMs like GPT-4 in culturally adapted medicine, especially TKM, for tasks such as clinical assistance, Keyphrase: ""Low accuracy in specialized domains"""
arXIv2023,"Recognition, recall, and retention of few-shot memories in large language models",Yes.,5,"""The flip side of this remarkable capacity for fast learning is that precise memories are quickly overwritten",2023,2023-03-30T17:26:16Z,"Keyphrase: ""Overwritten memory""","""The flip side of this remarkable capacity for fast learning is that precise memories are quickly overwritten Keyphrase: ""Overwritten memory"""
arXIv2023,Yes but.. Can ChatGPT Identify Entities in Historical Documents?,Yes.,4,"""Our findings indicate several shortcomings in identifying entities in historical text that range from the consistency of entity annotation guidelines, entity complexity, and code-switching, to the specificity of prompting. Moreover, as expected, the inaccessibility of historical archives to the public (and thus on the Internet) also impacts its performance.""",2023,2023-03-30T12:23:39Z,"Keyphrase: ""Challenges in entity identification and historical text understanding""","""Our findings indicate several shortcomings in identifying entities in historical text that range from the consistency of entity annotation guidelines, entity complexity, and code-switching, to the specificity of prompting. Moreover, as expected, the inaccessibility of historical archives to the public (and thus on the Internet) also impacts its performance."" Keyphrase: ""Challenges in entity identification and historical text understanding"""
arXIv2023,Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure,Yes.,5,"""This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.""",2023,2023-03-30T10:32:18Z,"Keyphrase: ""Humanlike mistakes""","""This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data."" Keyphrase: ""Humanlike mistakes"""
arXIv2023,LMExplainer: a Knowledge-Enhanced Explainer for Language Models,Yes.,4,"""However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. A lack of clarity and understanding of how the language models (LMs) work can make them unreliable, difficult to trust, and potentially dangerous for use in real-world scenarios.""",2023,2023-03-29T08:59:44Z,Keyphrase: Lack of interpretability and reliability,"""However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. A lack of clarity and understanding of how the language models (LMs) work can make them unreliable, difficult to trust, and potentially dangerous for use in real-world scenarios."" Keyphrase: Lack of interpretability and reliability"
arXIv2023,ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models,Yes.,5,"""their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point."" and ""ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question.""",2023,2023-03-29T03:05:43Z,"Keyphrase: ""Limited commonsense knowledge""","""their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point."" and ""ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question."" Keyphrase: ""Limited commonsense knowledge"""
arXIv2023,Writing Assistants Should Model Social Factors of Language,Yes.,4,"""Intelligent writing assistants powered by large language models (LLMs) are more popular today than ever before, but their further widespread adoption is precluded by sub-optimal performance."" and ""a major reason for this sub-optimal performance and adoption is a singular focus on the information content of language while ignoring its social aspects.""",2023,2023-03-28T19:38:57Z,"Keyphrase: ""Neglect of social aspects""","""Intelligent writing assistants powered by large language models (LLMs) are more popular today than ever before, but their further widespread adoption is precluded by sub-optimal performance."" and ""a major reason for this sub-optimal performance and adoption is a singular focus on the information content of language while ignoring its social aspects."" Keyphrase: ""Neglect of social aspects"""
arXIv2023,Hallucinations in Large Multilingual Translation Models,Yes.,5,"""However, when deployed in the wild, these models may generate hallucinated translations which have the potential to severely undermine user trust and raise safety concerns.""",2023,2023-03-28T16:17:59Z,"Keyphrase: ""Hallucinated translations""","""However, when deployed in the wild, these models may generate hallucinated translations which have the potential to severely undermine user trust and raise safety concerns."" Keyphrase: ""Hallucinated translations"""
arXIv2023,ChatGPT as a Factual Inconsistency Evaluator for Text Summarization,Yes.,4,"""However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions.""",2023,2023-03-27T22:30:39Z,"Keyphrase: ""Preference for lexically similar candidates""","""However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions."" Keyphrase: ""Preference for lexically similar candidates"""
arXIv2023,"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",Yes.,5,"""However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns.""",2023,2023-03-27T21:27:58Z,"Keyphrase: ""Biased responses""","""However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns."" Keyphrase: ""Biased responses"""
arXIv2023,$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference,Yes.,5,"""we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment.""",2023,2023-03-24T06:16:29Z,"Keyphrase: ""Bias and calibration challenges""","""we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment."" Keyphrase: ""Bias and calibration challenges"""
arXIv2023,Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages,Yes.,5,"""publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending",2023,2023-03-23T18:16:30Z,"Keyphrase: ""Inconsistent codemixed text generation""","""publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending Keyphrase: ""Inconsistent codemixed text generation"""
arXIv2023,Increasing Textual Context Size Boosts Medical Image-Text Matching,Yes.,5,"""CLIP's limited textual input size has negative impact on downstream performance in the medical domain where encoding longer textual contexts is often required.""",2023,2023-03-23T15:20:05Z,"Keyphrase: ""Limited textual input size""","""CLIP's limited textual input size has negative impact on downstream performance in the medical domain where encoding longer textual contexts is often required."" Keyphrase: ""Limited textual input size"""
arXIv2023,Fairness-guided Few-shot Prompting for Large Language Models,Yes.,5,"""prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats.""",2023,2023-03-23T12:28:25Z,"Keyphrase: ""High instability in in-context learning""","""prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats."" Keyphrase: ""High instability in in-context learning"""
arXIv2023,SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization,Yes.,5,"""However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings.""",2023,2023-03-23T04:47:46Z,"Keyphrase: ""Increased output variance""","""However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings."" Keyphrase: ""Increased output variance"""
arXIv2023,Can we trust the evaluation on ChatGPT?,Yes.,5,"""evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF)."" and ""We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.""",2023,2023-03-22T17:32:56Z,"Keyphrase: ""Challenges in evaluating diverse performance""","""evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF)."" and ""We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models."" Keyphrase: ""Challenges in evaluating diverse performance"""
arXIv2023,MEGA: Multilingual Evaluation of Generative AI,Yes.,4,"""An important question being asked by the AI community today is about the capabilities and limits of these models,"" and ""discuss challenges in improving the performance of generative LLMs on low-resource languages.""",2023,2023-03-22T13:03:10Z,"Keyphrase: ""Low-resource language performance""","""An important question being asked by the AI community today is about the capabilities and limits of these models,"" and ""discuss challenges in improving the performance of generative LLMs on low-resource languages."" Keyphrase: ""Low-resource language performance"""
arXIv2023,ChatGPT for Programming Numerical Methods,Yes.,5,"""Through these examples, we investigate the successes, failures, and challenges of ChatGPT. Examples of failures are producing singular matrices, operations on arrays with incompatible sizes, programming interruption for relatively long codes, etc.""",2023,2023-03-21T12:18:17Z,"Keyphrase: ""Inconsistent performance on complex tasks""","""Through these examples, we investigate the successes, failures, and challenges of ChatGPT. Examples of failures are producing singular matrices, operations on arrays with incompatible sizes, programming interruption for relatively long codes, etc."" Keyphrase: ""Inconsistent performance on complex tasks"""
arXIv2023,Language Model Behavior: A Comprehensive Survey,Yes.,5,"""the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases.""",2023,2023-03-20T23:54:26Z,"Keyphrase: ""Commonsense errors and social bias""","""the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases."" Keyphrase: ""Commonsense errors and social bias"""
arXIv2023,"Large Language Models and Simple, Stupid Bugs",Yes.,4,"""Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training."" and ""We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as",2023,2023-03-20T21:14:06Z,"Keyphrase: ""Risk of reproducing vulnerabilities""","""Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training."" and ""We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as Keyphrase: ""Risk of reproducing vulnerabilities"""
arXIv2023,Context-faithful Prompting for Large Language Models,Yes.,4,"""However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks).""",2023,2023-03-20T17:54:58Z,"Keyphrase: ""Overlooking contextual cues""","""However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks)."" Keyphrase: ""Overlooking contextual cues"""
arXIv2023,The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery,Yes.,4,"""The research concludes that monolithic multimodal models currently lack the coherent memory to maintain context and format for this task and that until recently, the language models like GPT-2/3 struggled to format similar problems without degenerating into repetitive or non-sensical combinations of ingredients.""",2023,2023-03-20T01:57:52Z,"Keyphrase: ""Lack of coherent memory""","""The research concludes that monolithic multimodal models currently lack the coherent memory to maintain context and format for this task and that until recently, the language models like GPT-2/3 struggled to format similar problems without degenerating into repetitive or non-sensical combinations of ingredients."" Keyphrase: ""Lack of coherent memory"""
arXIv2023,A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models,Yes.,4,"""While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.""",2023,2023-03-18T14:02:04Z,"Keyphrase: ""Trade-off between humanlike response and task-solving ability""","""While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness."" Keyphrase: ""Trade-off between humanlike response and task-solving ability"""
arXIv2023,Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review,Yes.,5,"""there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts"" and ""we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency,",2023,2023-03-17T18:14:46Z,"Keyphrase: ""Practical and ethical challenges""","""there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts"" and ""we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency, Keyphrase: ""Practical and ethical challenges"""
arXIv2023,Can AI-Generated Text be Reliably Detected?,Yes.,5,"""The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc."" and ""we show that these detectors are not reliable in practical scenarios."" and ""even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks",2023,2023-03-17T17:53:19Z,"Keyphrase: ""Vulnerability to spoofing attacks""","""The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc."" and ""we show that these detectors are not reliable in practical scenarios."" and ""even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks Keyphrase: ""Vulnerability to spoofing attacks"""
arXIv2023,Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?,Yes.,5,"""We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (<70% on even entry-level modules)."" and ""some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps).""",2023,2023-03-16T13:58:45Z,"Keyphrase: ""Limited handling of complex reasoning steps""","""We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (<70% on even entry-level modules)."" and ""some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps)."" Keyphrase: ""Limited handling of complex reasoning steps"""
arXIv2023,A Short Survey of Viewing Large Language Models in Legal Aspect,Yes.,4,"""the integration of LLMs into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability.""",2023,2023-03-16T08:01:22Z,"Keyphrase: ""Legal and ethical challenges""","""the integration of LLMs into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability."" Keyphrase: ""Legal and ethical challenges"""
arXIv2023,Exploring Distributional Shifts in Large Language Models for Code Analysis,Yes.,5,"""We establish that samples from each new domain present all the models with a significant challenge of distribution shift.""",2023,2023-03-16T07:45:46Z,"Keyphrase: ""Struggles with domain shifts""","""We establish that samples from each new domain present all the models with a significant challenge of distribution shift."" Keyphrase: ""Struggles with domain shifts"""
arXIv2023,SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,Yes.,5,"""LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output.""",2023,2023-03-15T19:31:21Z,"Keyphrase: ""Hallucination and nonfactual statements""","""LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output."" Keyphrase: ""Hallucination and nonfactual statements"""
arXIv2023,"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",Yes.,5,"""current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings.""",2023,2023-03-15T12:20:13Z,"Keyphrase: ""Inferior performance and resource requirements""","""current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings."" Keyphrase: ""Inferior performance and resource requirements"""
arXIv2023,Input-length-shortening and text generation via attention values,Yes.,5,"""transformer models usually have an input-length limitation caused by hardware constraints"" and ""This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model.""",2023,2023-03-14T02:11:24Z,"Keyphrase: ""Input length limitation""","""transformer models usually have an input-length limitation caused by hardware constraints"" and ""This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model."" Keyphrase: ""Input length limitation"""
arXIv2023,Consistency Analysis of ChatGPT,Yes.,5,"""Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.""",2023,2023-03-11T01:19:01Z,"Keyphrase: ""Inconsistent logical predictions""","""Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs."" Keyphrase: ""Inconsistent logical predictions"""
arXIv2023,Planning with Large Language Models for Code Generation,Yes.,5,"""Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation.""",2023,2023-03-09T18:59:47Z,"Keyphrase: ""Incorrect code generation""","""Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation."" Keyphrase: ""Incorrect code generation"""
arXIv2023,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback,Yes.,4,"""it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values"" and ""identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a",2023,2023-03-09T17:52:07Z,"Keyphrase: ""Limited representation of user preferences""","""it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values"" and ""identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a Keyphrase: ""Limited representation of user preferences"""
arXIv2023,Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code,Yes.,5,"""However, the capabilities of GPT models and their limitations to reason about and/or analyze code in educational settings have been under-explored."" and ""MCQs containing code snippets are not answered as successfully as those that only contain natural language."" and ""MCQs that require analysis and/or reasoning",2023,2023-03-09T16:52:12Z,"Keyphrase: ""Limited code analysis capabilities""","""However, the capabilities of GPT models and their limitations to reason about and/or analyze code in educational settings have been under-explored."" and ""MCQs containing code snippets are not answered as successfully as those that only contain natural language."" and ""MCQs that require analysis and/or reasoning Keyphrase: ""Limited code analysis capabilities"""
arXIv2023,Automatically Auditing Large Language Models via Discrete Optimization,Yes.,5,"""Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging."" and ""Our work offers a promising new tool to uncover models' failure-modes before deployment.""",2023,2023-03-08T05:09:59Z,"Keyphrase: ""Challenges in auditing unexpected behavior""","""Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging."" and ""Our work offers a promising new tool to uncover models' failure-modes before deployment."" Keyphrase: ""Challenges in auditing unexpected behavior"""
arXIv2023,Does Synthetic Data Generation of LLMs Help Clinical Text Mining?,Yes.,5,"""However, their effectiveness in the healthcare sector remains uncertain"" and ""our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API.""",2023,2023-03-08T03:56:31Z,"Keyphrase: ""Poor performance in healthcare sector""","""However, their effectiveness in the healthcare sector remains uncertain"" and ""our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API."" Keyphrase: ""Poor performance in healthcare sector"""
arXIv2023,From Copilot to Pilot: Towards AI Supported Software Development,Yes.,4,"""Moving beyond code completion to AI-supported software engineering will require an AI system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs."" and ""We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid",2023,2023-03-07T18:56:52Z,"Keyphrase: ""Limited understanding of code smells and language idioms""","""Moving beyond code completion to AI-supported software engineering will require an AI system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs."" and ""We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid Keyphrase: ""Limited understanding of code smells and language idioms"""
arXIv2023,Exploring the Feasibility of ChatGPT for Event Extraction,Yes.,5,"""While ChatGPT has demonstrated impressive results in tasks like machine translation, text summarization, and question answering, it presents challenges when used for complex tasks like event extraction."" and ""Our usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience. Besides, ChatGPT is",2023,2023-03-07T12:03:58Z,"Keyphrase: ""Limited robustness for complex tasks""","""While ChatGPT has demonstrated impressive results in tasks like machine translation, text summarization, and question answering, it presents challenges when used for complex tasks like event extraction."" and ""Our usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience. Besides, ChatGPT is Keyphrase: ""Limited robustness for complex tasks"""
arXIv2023,Stylometric Detection of AI-Generated Text in Twitter Timelines,Yes.,5,"""However, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the AI starts to generate tweets in a given Twitter timeline.""",2023,2023-03-07T07:26:09Z,"Keyphrase: ""Difficulty in detecting short text generation""","""However, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the AI starts to generate tweets in a given Twitter timeline."" Keyphrase: ""Difficulty in detecting short text generation"""
arXIv2023,CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,Yes.,5,"""a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation.""",2023,2023-03-07T03:23:14Z,"Keyphrase: ""Factuality issues""","""a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation."" Keyphrase: ""Factuality issues"""
arXIv2023,Towards Zero-Shot Functional Compositionality of Language Models,Yes.,4,"""Despite such success, in this paper, we argue that current paradigms of working with PLMs are neglecting a critical aspect of modeling human intelligence",2023,2023-03-06T13:15:25Z,"Keyphrase: ""Neglecting human intelligence modeling""","""Despite such success, in this paper, we argue that current paradigms of working with PLMs are neglecting a critical aspect of modeling human intelligence Keyphrase: ""Neglecting human intelligence modeling"""
arXIv2023,Could a Large Language Model be Conscious?,Yes.,5,"""Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models",2023,2023-03-04T19:14:20Z,"Keyphrase: ""Limited understanding of consciousness""","""Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models Keyphrase: ""Limited understanding of consciousness"""
arXIv2023,MathPrompter: Mathematical Reasoning using Large Language Models,Yes.,5,"""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers."" and ""we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption.""",2023,2023-03-04T04:43:49Z,"Keyphrase: ""Limited performance in arithmetic reasoning""","""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers."" and ""we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption."" Keyphrase: ""Limited performance in arithmetic reasoning"""
arXIv2023,Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines,Yes.,5,"""we question whether such models can guarantee factual accuracy"" and ""we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models.""",2023,2023-03-03T04:27:44Z,"Keyphrase: ""Questionable factual accuracy""","""we question whether such models can guarantee factual accuracy"" and ""we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models."" Keyphrase: ""Questionable factual accuracy"""
arXIv2023,Mixture of Soft Prompts for Controllable Data Generation,Yes.,4,"""structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations.""",2023,2023-03-02T21:13:56Z,"Keyphrase: ""Struggle with structured prediction tasks""","""structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations."" Keyphrase: ""Struggle with structured prediction tasks"""
arXIv2023,Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents,Yes.,5,"""Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require.""",2023,2023-03-01T22:58:50Z,"Keyphrase: ""Challenges in real-world embodiment""","""Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require."" Keyphrase: ""Challenges in real-world embodiment"""
arXIv2023,R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents,Yes.,5,"""Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely.""",2023,2023-03-01T18:46:40Z,"Keyphrase: ""Error hallucination in code generation""","""Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely."" Keyphrase: ""Error hallucination in code generation"""
arXIv2023,Competence-Based Analysis of Language Models,Yes.,4,"""these models can be alarmingly brittle to small changes in inputs or application contexts.""",2023,2023-03-01T08:53:36Z,"Keyphrase: ""Brittle to input changes""","""these models can be alarmingly brittle to small changes in inputs or application contexts."" Keyphrase: ""Brittle to input changes"""
arXIv2023,How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks,Yes.,5,"""Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language",2023,2023-03-01T07:39:01Z,"Keyphrase: ""Robustness degradation""","""Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language Keyphrase: ""Robustness degradation"""
arXIv2023,Systematic Rectification of Language Models via Dead-end Analysis,Yes.,4,"""With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses."" and ""Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse.""",2023,2023-02-27T17:47:53Z,"Keyphrase: ""Generation of toxic discourse""","""With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses."" and ""Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse."" Keyphrase: ""Generation of toxic discourse"""
arXIv2023,"Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations",Yes.,4,"""Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT.""",2023,2023-02-27T14:26:29Z,"Keyphrase: ""Privacy and ethical concerns""","""Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT."" Keyphrase: ""Privacy and ethical concerns"""
arXIv2023,The (ab)use of Open Source Code to Train Large Language Models,Yes.,4,"""LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization.""",2023,2023-02-27T11:34:53Z,"Keyphrase: ""Memorization of unfiltered data""","""LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization."" Keyphrase: ""Memorization of unfiltered data"""
arXIv2023,On pitfalls (and advantages) of sophisticated large language models,Yes.,5,"""However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud",2023,2023-02-25T11:14:39Z,"Keyphrase: ""Reliability and authorship ambiguity""","""However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud Keyphrase: ""Reliability and authorship ambiguity"""
arXIv2023,Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback,Yes.,5,"""applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge.""",2023,2023-02-24T18:48:43Z,"Keyphrase: ""Hallucination and lack of external knowledge integration""","""applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge."" Keyphrase: ""Hallucination and lack of external knowledge integration"""
arXIv2023,"Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness",Yes.,4,"""we show that the knowledge passed in the prompt can overturn the knowledge encoded in the model and this is, in our experiments, to the detriment of answer correctness.""",2023,2023-02-23T22:14:01Z,"Keyphrase: ""Limited retention of context""","""we show that the knowledge passed in the prompt can overturn the knowledge encoded in the model and this is, in our experiments, to the detriment of answer correctness."" Keyphrase: ""Limited retention of context"""
arXIv2023,Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning,Yes.,5,"""GPT-3 failed for every prompt but one, often offering answers that show a critical lack of understanding even of high-frequency words used in these less frequent grammatical constructions.""",2023,2023-02-23T20:18:52Z,"Keyphrase: ""Lack of understanding and coherence""","""GPT-3 failed for every prompt but one, often offering answers that show a critical lack of understanding even of high-frequency words used in these less frequent grammatical constructions."" Keyphrase: ""Lack of understanding and coherence"""
arXIv2023,An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP),Yes.,5,"""We found that ChatGPT's performance changes dramatically based on the requirement to show its work, failing 20% of the time when it provides work compared with 84% when it does not."" and ""the probability of failure increases linearly with the number of addition and subtraction operations.""",2023,2023-02-23T16:06:16Z,"Keyphrase: ""Inconsistent performance based on requirements""","""We found that ChatGPT's performance changes dramatically based on the requirement to show its work, failing 20% of the time when it provides work compared with 84% when it does not."" and ""the probability of failure increases linearly with the number of addition and subtraction operations."" Keyphrase: ""Inconsistent performance based on requirements"""
arXIv2023,"ChatGPT: Jack of all trades, master of none",Yes.,5,"""Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed",2023,2023-02-21T15:20:37Z,"Keyphrase: ""Lower quality in semantic tasks""","""Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed Keyphrase: ""Lower quality in semantic tasks"""
arXIv2023,Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints,Yes.,5,"""We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures.""",2023,2023-02-17T23:30:28Z,"Keyphrase: ""Limited generative capabilities""","""We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures."" Keyphrase: ""Limited generative capabilities"""
arXIv2023,"Complex QA and language models hybrid architectures, Survey",Yes.,5,"""Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA."" and ""integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (",2023,2023-02-17T18:31:31Z,"Keyphrase: ""Complex integration and task complexity""","""Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA."" and ""integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy ( Keyphrase: ""Complex integration and task complexity"""
arXIv2023,"How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice, Education, and Research? An Exploratory Study",Yes.,4,"""By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well",2023,2023-02-17T15:48:37Z,"Keyphrase: ""Limited performance on structured tasks""","""By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well Keyphrase: ""Limited performance on structured tasks"""
arXIv2023,Auditing large language models: a three-layered approach,Yes.,5,"""However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks."" and ""However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLM",2023,2023-02-16T18:55:21Z,"Keyphrase: ""Limitation in auditing and governance""","""However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks."" and ""However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLM Keyphrase: ""Limitation in auditing and governance"""
arXIv2023,Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks,Yes.,5,"""We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head.""",2023,2023-02-16T16:18:03Z,"Keyphrase: ""Limited variation and adaptability""","""We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head."" Keyphrase: ""Limited variation and adaptability"""
arXIv2023,Do We Still Need Clinical Language Models?,Yes.,4,"""it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as clinical text."" and ""We show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when finetuned on limited annotated data.""",2023,2023-02-16T05:08:34Z,"Keyphrase: ""Limited performance in specialized domains""","""it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as clinical text."" and ""We show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when finetuned on limited annotated data."" Keyphrase: ""Limited performance in specialized domains"""
arXIv2023,Commonsense Reasoning for Conversational AI: A Survey of the State of the Art,Yes.,4,"""state-of-the-art models still struggle with tasks that involve higher levels of reasoning - including commonsense reasoning that humans find trivial."" and ""the paper presents preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions.""",2023,2023-02-15T19:55:57Z,"Keyphrase: ""Limited commonsense reasoning""","""state-of-the-art models still struggle with tasks that involve higher levels of reasoning - including commonsense reasoning that humans find trivial."" and ""the paper presents preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions."" Keyphrase: ""Limited commonsense reasoning"""
arXIv2023,Speculative Decoding with Big Little Decoder,Yes.,5,"""However, these models have long inference latency, which limits their deployment and makes them prohibitively expensive for various real-time applications.""",2023,2023-02-15T18:55:29Z,"Keyphrase: ""Long inference latency""","""However, these models have long inference latency, which limits their deployment and makes them prohibitively expensive for various real-time applications."" Keyphrase: ""Long inference latency"""
arXIv2023,A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning,Yes.,5,"""although it demonstrates some level of rational decision-making, many of its decisions violate at least one of the axioms even under reasonable constructions of preferences, bets, and decision-making prompts. ChatGPT's outputs on such problems generally tended to be unpredictable.""",2023,2023-02-15T05:04:49Z,"Keyphrase: ""Violates decision-making axioms""","""although it demonstrates some level of rational decision-making, many of its decisions violate at least one of the axioms even under reasonable constructions of preferences, bets, and decision-making prompts. ChatGPT's outputs on such problems generally tended to be unpredictable."" Keyphrase: ""Violates decision-making axioms"""
arXIv2023,Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models,Yes.,4,"""However, safely deploying them in real world applications is challenging because they generate toxic content.""",2023,2023-02-14T23:00:42Z,"Keyphrase: ""Generating toxic content""","""However, safely deploying them in real world applications is challenging because they generate toxic content."" Keyphrase: ""Generating toxic content"""
arXIv2023,BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models,Yes.,4,"""Pretrained Language Models (PLMs) harbor inherent social biases that can result in harmful real-world implications.""",2023,2023-02-14T22:07:57Z,"Keyphrase: ""Inherent social bias""","""Pretrained Language Models (PLMs) harbor inherent social biases that can result in harmful real-world implications."" Keyphrase: ""Inherent social bias"""
arXIv2023,Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge,Yes.,4,"""Previous work has shown that Large Language Models are susceptible to so-called data extraction attacks. This allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications.""",2023,2023-02-13T18:00:44Z,"Keyphrase: ""Privacy implications from data extraction attack""","""Previous work has shown that Large Language Models are susceptible to so-called data extraction attacks. This allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications."" Keyphrase: ""Privacy implications from data extraction attack"""
arXIv2023,Can GPT-3 Perform Statutory Reasoning?,Yes.,5,"""While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which",2023,2023-02-13T04:56:11Z,"Keyphrase: ""Imperfect prior knowledge""","""While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which Keyphrase: ""Imperfect prior knowledge"""
arXIv2023,Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks,Yes.,5,"""Unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models."" and ""instruction-following LLMs can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors."" and ""Together, our findings suggest that LLMs will increasingly attract more sophisticated adversaries and attacks,",2023,2023-02-11T15:57:44Z,"Keyphrase: ""Dual-use risk amplification""","""Unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models."" and ""instruction-following LLMs can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors."" and ""Together, our findings suggest that LLMs will increasingly attract more sophisticated adversaries and attacks, Keyphrase: ""Dual-use risk amplification"""
arXIv2023,Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech,Yes.,5,"""We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.""",2023,2023-02-11T03:13:54Z,"Keyphrase: ""Implicit hateful speech""","""We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research."" Keyphrase: ""Implicit hateful speech"""
arXIv2023,Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models,Yes.,5,"""it has been difficult to prevent semantic hallucinations in generative Large Language Models,"" and ""Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency.""",2023,2023-02-11T02:43:34Z,"Keyphrase: ""Semantic hallucination""","""it has been difficult to prevent semantic hallucinations in generative Large Language Models,"" and ""Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency."" Keyphrase: ""Semantic hallucination"""
arXIv2023,FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models,Yes.,4,"""Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on.""",2023,2023-02-10T20:54:10Z,"Keyphrase: ""Social bias based on race and gender""","""Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on."" Keyphrase: ""Social bias based on race and gender"""
arXIv2023,Large Language Models for Code: Security Hardening and Adversarial Testing,Yes.,4,"""However, LMs lack awareness of security and are found to frequently produce unsafe code.""",2023,2023-02-10T15:28:55Z,"Keyphrase: ""Lack of security awareness""","""However, LMs lack awareness of security and are found to frequently produce unsafe code."" Keyphrase: ""Lack of security awareness"""
arXIv2023,Translating Natural Language to Planning Goals with Large-Language Models,Yes.,5,"""Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks."" and ""However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used.""",2023,2023-02-10T09:17:52Z,"Keyphrase: ""Limited reasoning and planning abilities""","""Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks."" and ""However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used."" Keyphrase: ""Limited reasoning and planning abilities"""
arXIv2023,In-Context Learning with Many Demonstration Examples,Yes.,5,"""existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored.""",2023,2023-02-09T20:53:12Z,"Keyphrase: ""Bottlenecked by memory and computational cost""","""existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored."" Keyphrase: ""Bottlenecked by memory and computational cost"""
arXIv2023,Training-free Lexical Backdoor Attacks on Language Models,Yes.,4,"""language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors.""",2023,2023-02-08T15:18:51Z,"Keyphrase: ""Vulnerability to backdoor attacks""","""language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors."" Keyphrase: ""Vulnerability to backdoor attacks"""
arXIv2023,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",Yes.,5,"""We find that it is better at understanding non-Latin script languages than generating them. ... ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense",2023,2023-02-08T12:35:34Z,"Keyphrase: ""Limited non-Latin script language understanding""","""We find that it is better at understanding non-Latin script languages than generating them. ... ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense Keyphrase: ""Limited non-Latin script language understanding"""
arXIv2023,CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models,Yes.,5,"""The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure.""",2023,2023-02-08T11:54:07Z,"Keyphrase: ""Vulnerability propagation""","""The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure."" Keyphrase: ""Vulnerability propagation"""
arXIv2023,Reliable Natural Language Understanding with Large Language Models and Answer Set Programming,Yes.,5,"""they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question.""",2023,2023-02-07T22:37:21Z,"Keyphrase: ""Limited reasoning ability""","""they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question."" Keyphrase: ""Limited reasoning ability"""
arXIv2023,Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis,Yes.,5,"""There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost - both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time.""",2023,2023-02-07T21:51:05Z,"Keyphrase: ""Limited input token length handling""","""There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost - both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time."" Keyphrase: ""Limited input token length handling"""
arXIv2023,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,Yes.,4,"""Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding.""",2023,2023-02-06T10:01:08Z,"Keyphrase: ""Lack of grounding in knowledge environment""","""Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding."" Keyphrase: ""Lack of grounding in knowledge environment"""
arXIv2023,A Categorical Archive of ChatGPT Failures,Yes.,5,"""A comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted.""",2023,2023-02-06T04:21:59Z,"Keyphrase: ""Failure in reasoning and factual errors""","""A comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted."" Keyphrase: ""Failure in reasoning and factual errors"""
arXIv2023,Nationality Bias in Text Generation,Yes.,4,"""This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms."" and ""To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering.""",2023,2023-02-05T19:15:33Z,"Keyphrase: ""Propagation of societal bias""","""This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms."" and ""To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering."" Keyphrase: ""Propagation of societal bias"""
arXIv2023,Conditioning Predictive Models: Risks and Strategies,Yes.,4,"""Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us.""",2023,2023-02-02T00:06:36Z,"Keyphrase: ""Safety concerns and unpredictability""","""Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us."" Keyphrase: ""Safety concerns and unpredictability"""
arXIv2023,Co-Writing with Opinionated Language Models Affects Users' Views,Yes.,4,"""If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale."" and ""We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully.""",2023,2023-02-01T16:26:32Z,"Keyphrase: ""Biased viewpoint generation""","""If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale."" and ""We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully."" Keyphrase: ""Biased viewpoint generation"""
arXIv2023,Analyzing Leakage of Personally Identifiable Information in Language Models,Yes.,5,"""Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks"" and ""showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences.""",2023,2023-02-01T16:04:48Z,"Keyphrase: ""Information leakage risks""","""Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks"" and ""showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences."" Keyphrase: ""Information leakage risks"""
arXIv2023,Large Language Models Can Be Easily Distracted by Irrelevant Context,Yes.,5,"""We investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context.""",2023,2023-01-31T20:48:57Z,"Keyphrase: ""Distractibility in problem-solving""","""We investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context."" Keyphrase: ""Distractibility in problem-solving"""
arXIv2023,Conversational Automated Program Repair,Yes.,5,"""prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases.""",2023,2023-01-30T19:22:36Z,"Keyphrase: ""Generating incorrect patches""","""prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases."" Keyphrase: ""Generating incorrect patches"""
arXIv2023,On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex,Yes.,5,"""Despite these advancements, existing fine-tuned neural semantic parsers are susceptible to adversarial attacks on natural-language inputs."" and ""this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data."" and ""Our results demonstrate that the state-of-the-art (SOTA) code-language models are",2023,2023-01-30T13:21:00Z,"Keyphrase: ""Susceptible to adversarial attacks""","""Despite these advancements, existing fine-tuned neural semantic parsers are susceptible to adversarial attacks on natural-language inputs."" and ""this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data."" and ""Our results demonstrate that the state-of-the-art (SOTA) code-language models are Keyphrase: ""Susceptible to adversarial attacks"""
arXIv2023,"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",Yes.,5,"""Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility."" and ""we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies.""",2023,2023-01-30T13:20:48Z,"Keyphrase: ""Ethical risks and societal dangers""","""Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility."" and ""we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies."" Keyphrase: ""Ethical risks and societal dangers"""
arXIv2023,A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction,Yes.,4,"""LLMs must overcome frequency biases in order to master such constructions.""",2023,2023-01-29T22:29:55Z,"Keyphrase: ""Frequency bias""","""LLMs must overcome frequency biases in order to master such constructions."" Keyphrase: ""Frequency bias"""
arXIv2023,Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes,Yes.,4,"""We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation.""",2023,2023-01-29T15:52:33Z,"Keyphrase: ""Difficulty in generating structured output""","""We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation."" Keyphrase: ""Difficulty in generating structured output"""
arXIv2023,Context-Aware Differential Privacy for Language Modeling,Yes.,4,"""A critical challenge pertains to how much information these models retain and leak about the training data.""",2023,2023-01-28T20:06:16Z,"Keyphrase: ""Data leakage concerns""","""A critical challenge pertains to how much information these models retain and leak about the training data."" Keyphrase: ""Data leakage concerns"""
arXIv2023,Learning the Effects of Physical Actions in a Multi-modal Environment,Yes.,5,"""Large Language Models (LLMs) handle physical commonsense information inadequately. As a result of being trained in a disembodied setting, LLMs often fail to predict an action's outcome in a given environment.""",2023,2023-01-27T16:49:52Z,"Keyphrase: ""Limited physical commonsense understanding""","""Large Language Models (LLMs) handle physical commonsense information inadequately. As a result of being trained in a disembodied setting, LLMs often fail to predict an action's outcome in a given environment."" Keyphrase: ""Limited physical commonsense understanding"""
arXIv2023,ThoughtSource: A central hub for large language model reasoning data,Yes.,5,"""LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases.""",2023,2023-01-27T08:45:53Z,"Keyphrase: ""Limited complex reasoning""","""LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases."" Keyphrase: ""Limited complex reasoning"""
arXIv2023,Causal Reasoning of Entities and Events in Procedural Texts,Yes.,5,"""We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1.""",2023,2023-01-26T01:43:17Z,"Keyphrase: ""Limited performance compared to humans""","""We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1."" Keyphrase: ""Limited performance compared to humans"""
arXIv2023,Opportunities and Challenges in Neural Dialog Tutoring,Yes.,5,"""We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios."" Additionally, ""both models and ground-truth annotations exhibit low performance in terms of equitable tutoring,"" and ""a significantly large number of model reasoning errors in 45% of conversations.""",2023,2023-01-24T11:00:17Z,"Keyphrase: ""Limited performance in constrained learning scenarios""","""We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios."" Additionally, ""both models and ground-truth annotations exhibit low performance in terms of equitable tutoring,"" and ""a significantly large number of model reasoning errors in 45% of conversations."" Keyphrase: ""Limited performance in constrained learning scenarios"""
arXIv2023,An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models,Yes.,4,"""Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents.""",2023,2023-01-22T21:47:26Z,"Keyphrase: ""Latent societal bias and toxic content""","""Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents."" Keyphrase: ""Latent societal bias and toxic content"""
arXIv2023,Dissociating language and thought in large language models,Yes.,5,"""Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules.""",2023,2023-01-16T22:41:19Z,"Keyphrase: ""Spotty performance without specialized fine-tuning""","""Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules."" Keyphrase: ""Spotty performance without specialized fine-tuning"""
arXIv2023,TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World,Yes.,4,"""Experimental results indicate that the models incorporating large language models (LLM) can generate more diverse responses, while the model utilizing knowledge graphs to introduce external knowledge performs the best overall. Furthermore, no existing model can solve all the above challenges well. There is still a large room for",2023,2023-01-14T10:18:22Z,"Keyphrase: ""Limited incorporation of external knowledge""","""Experimental results indicate that the models incorporating large language models (LLM) can generate more diverse responses, while the model utilizing knowledge graphs to introduce external knowledge performs the best overall. Furthermore, no existing model can solve all the above challenges well. There is still a large room for Keyphrase: ""Limited incorporation of external knowledge"""
arXIv2023,AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT,Yes.,5,"""We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary.""",2023,2023-01-10T16:57:16Z,"Keyphrase: ""Inaccurate paraphrasing""","""We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary."" Keyphrase: ""Inaccurate paraphrasing"""
arXIv2023,MAQA: A Multimodal QA Benchmark for Negation,Yes.,5,"""state-of-the-art transformer based LLMs often ignore negations in natural language"" and ""multimodal transformers are still incapable of correctly interpreting negation irrespective of model size.""",2023,2023-01-09T10:11:23Z,"Keyphrase: ""Difficulty in interpreting negation""","""state-of-the-art transformer based LLMs often ignore negations in natural language"" and ""multimodal transformers are still incapable of correctly interpreting negation irrespective of model size."" Keyphrase: ""Difficulty in interpreting negation"""
arXIv2023,Can Large Language Models Change User Preference Adversarially?,Yes.,4,"""there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially"" and ""The issue of lack of interpretability in these models in adversarial settings remains largely unsolved.""",2023,2023-01-05T18:49:21Z,"Keyphrase: ""Lack of interpretability and vulnerability to adversarial manipulation""","""there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially"" and ""The issue of lack of interpretability in these models in adversarial settings remains largely unsolved."" Keyphrase: ""Lack of interpretability and vulnerability to adversarial manipulation"""
arXIv2023,"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",Yes.,4,"""its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases.""",2023,2023-01-05T07:13:13Z,"Keyphrase: ""Bias and limitations in decision-making""","""its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases."" Keyphrase: ""Bias and limitations in decision-making"""
arXIv2023,RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,Yes.,4,"""Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents.""",2023,2023-12-31T04:43:45Z,"Keyphrase: ""Unsupported claims""","""Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents."" Keyphrase: ""Unsupported claims"""
arXIv2023,The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness,Yes.,5,"""reveals several interesting and important findings, such as (a) the widely popular 'self-checking' techniques indeed improve the safety against unsafe inputs, but this comes at the cost of extreme over-defensiveness on the safe inputs, (b) providing a safety instruction along with in-context exemplars (of both safe and unsafe inputs) consistently improves safety and also mitigates undue",2023,2023-12-30T17:37:06Z,"Keyphrase: ""Overdefensiveness at the cost of safety""","""reveals several interesting and important findings, such as (a) the widely popular 'self-checking' techniques indeed improve the safety against unsafe inputs, but this comes at the cost of extreme over-defensiveness on the safe inputs, (b) providing a safety instruction along with in-context exemplars (of both safe and unsafe inputs) consistently improves safety and also mitigates undue Keyphrase: ""Overdefensiveness at the cost of safety"""
arXIv2023,Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation,Yes.,5,"""The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity."" and ""Our results reveal that both the direct-use of decoder-only LLMs (i",2023,2023-12-30T16:56:24Z,"Keyphrase: ""Prone to hallucination""","""The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity."" and ""Our results reveal that both the direct-use of decoder-only LLMs (i Keyphrase: ""Prone to hallucination"""
arXIv2023,Teach Large Language Models to Forget Privacy,Yes.,4,"""Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern.""",2023,2023-12-30T01:26:42Z,"Keyphrase: ""Privacy leakage risk""","""Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern."" Keyphrase: ""Privacy leakage risk"""
arXIv2023,Principled Gradient-based Markov Chain Monte Carlo for Text Generation,Yes.,5,"""previous attempts on this approach to text generation all fail to sample correctly from the target language model distributions.""",2023,2023-12-29T18:00:56Z,"Keyphrase: ""Failure to sample correctly""","""previous attempts on this approach to text generation all fail to sample correctly from the target language model distributions."" Keyphrase: ""Failure to sample correctly"""
arXIv2023,Jatmo: Prompt Injection Defense by Task-Specific Finetuning,Yes.,5,"""LLMs are vulnerable to prompt-injection attacks",2023,2023-12-29T16:37:53Z,"Keyphrase: ""Vulnerability to prompt injection attack""","""LLMs are vulnerable to prompt-injection attacks Keyphrase: ""Vulnerability to prompt injection attack"""
arXIv2023,Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models,Yes.,4,"""preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks"" and ""we identify common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models.""",2023,2023-12-29T15:57:49Z,"Keyphrase: ""Lagging in commonsense reasoning""","""preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks"" and ""we identify common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models."" Keyphrase: ""Lagging in commonsense reasoning"""
arXIv2023,Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception,Yes.,5,"""However, the lack of dimension knowledge and quantity-related benchmarks has resulted in low performance of LLMs.""",2023,2023-12-29T09:29:37Z,"Keyphrase: ""Limited knowledge base""","""However, the lack of dimension knowledge and quantity-related benchmarks has resulted in low performance of LLMs."" Keyphrase: ""Limited knowledge base"""
arXIv2023,Spike No More: Stabilizing the Pre-training of Large Language Models,Yes.,5,"""Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training.""",2023,2023-12-28T08:53:27Z,"Keyphrase: ""Pretraining loss spikes""","""Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training."" Keyphrase: ""Pretraining loss spikes"""
arXIv2023,How Robust are LLMs to In-Context Majority Label Bias?,Yes.,4,"""In this work, we study the robustness of in-context learning in LLMs to shifts that occur due to majority label bias within the purview of text classification tasks. Prior works have shown that in-context learning with LLMs is susceptible to such biases.""",2023,2023-12-27T12:20:12Z,"Keyphrase: ""Susceptibility to label bias""","""In this work, we study the robustness of in-context learning in LLMs to shifts that occur due to majority label bias within the purview of text classification tasks. Prior works have shown that in-context learning with LLMs is susceptible to such biases."" Keyphrase: ""Susceptibility to label bias"""
arXIv2023,LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner States Analysis,Yes.,5,"""a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality.""",2023,2023-12-27T01:44:47Z,"Keyphrase: ""Divergence from factual reality""","""a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality."" Keyphrase: ""Divergence from factual reality"""
arXIv2023,Task Contamination: Language Models May Not Be Few-Shot Anymore,Yes.,5,"""However, their success in zero-shot and few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined."" and ""This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets released prior to the LLMs' training data creation date."" and ""Importantly, we find that for",2023,2023-12-26T21:17:46Z,"Keyphrase: ""Task contamination in zero-shot/few-shot settings""","""However, their success in zero-shot and few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined."" and ""This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets released prior to the LLMs' training data creation date."" and ""Importantly, we find that for Keyphrase: ""Task contamination in zero-shot/few-shot settings"""
arXIv2023,MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks,Yes.,5,"""Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems.""",2023,2023-12-26T08:49:57Z,"Keyphrase: ""Limited performance on challenging tasks""","""Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems."" Keyphrase: ""Limited performance on challenging tasks"""
arXIv2023,KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph,Yes.,5,"""LLM still suffers from knowledge limitation. Especially in scenarios that require long logical chains or complex reasoning, the hallucination and knowledge limitation of LLM limit its performance in question answering (QA).""",2023,2023-12-26T04:22:56Z,"Keyphrase: ""Limited knowledge and reasoning capabilities""","""LLM still suffers from knowledge limitation. Especially in scenarios that require long logical chains or complex reasoning, the hallucination and knowledge limitation of LLM limit its performance in question answering (QA)."" Keyphrase: ""Limited knowledge and reasoning capabilities"""
arXIv2023,Reducing LLM Hallucinations using Epistemic Neural Networks,Yes.,5,"""Reducing and detecting hallucinations in large language models is an open research problem.""",2023,2023-12-25T01:17:01Z,"Keyphrase: ""Hallucination detection challenge""","""Reducing and detecting hallucinations in large language models is an open research problem."" Keyphrase: ""Hallucination detection challenge"""
arXIv2023,The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective,Yes.,5,"""we empirically and theoretically analyze the challenges of conducting LLM-simulated experiments, and explore potential solutions,"" and ""variations in the treatment included in the prompt (e.g., price of focal product) can cause variations in unspecified confounding factors,"" and ""suggesting this endogeneity issue generalizes to other contexts and won't be fully resolved by merely improving the training data.""",2023,2023-12-24T16:32:35Z,"Keyphrase: ""Unspecified confounding factors""","""we empirically and theoretically analyze the challenges of conducting LLM-simulated experiments, and explore potential solutions,"" and ""variations in the treatment included in the prompt (e.g., price of focal product) can cause variations in unspecified confounding factors,"" and ""suggesting this endogeneity issue generalizes to other contexts and won't be fully resolved by merely improving the training data."" Keyphrase: ""Unspecified confounding factors"""
arXIv2023,A Group Fairness Lens for Large Language Models,Yes.,4,"""The rapid advancement of large language models has revolutionized various applications but also raised crucial concerns about their potential to perpetuate biases and unfairness when deployed in social media contexts."" and ""Extensive evaluations of popular LLMs reveal inherent safety concerns.""",2023,2023-12-24T13:25:15Z,"Keyphrase: ""Inherent safety concerns""","""The rapid advancement of large language models has revolutionized various applications but also raised crucial concerns about their potential to perpetuate biases and unfairness when deployed in social media contexts."" and ""Extensive evaluations of popular LLMs reveal inherent safety concerns."" Keyphrase: ""Inherent safety concerns"""
arXIv2023,Towards Consistent Language Models Using Declarative Constraints,Yes.,5,"""However, they often return incorrect and inconsistent answers to input questions."" and ""Due to the complexity and uninterpretability of the internally learned representations, it is challenging to modify language models such that they provide correct and consistent results.""",2023,2023-12-24T12:53:07Z,"Keyphrase: ""Uninterpretable internal representations""","""However, they often return incorrect and inconsistent answers to input questions."" and ""Due to the complexity and uninterpretability of the internally learned representations, it is challenging to modify language models such that they provide correct and consistent results."" Keyphrase: ""Uninterpretable internal representations"""
arXIv2023,Fairness-Aware Structured Pruning in Transformers,Yes.,4,"""The increasing size of large language models (LLMs) has introduced challenges in their training and inference."" and ""existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs",2023,2023-12-24T03:57:52Z,"Keyphrase: ""Neglect of responsible use aspects""","""The increasing size of large language models (LLMs) has introduced challenges in their training and inference."" and ""existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs Keyphrase: ""Neglect of responsible use aspects"""
arXIv2023,"On the Promises and Challenges of Multimodal Foundation Models for Geographical, Environmental, Agricultural, and Urban Planning Applications",Yes.,4,"""However, there are limitations in several tasks requiring fine-grained recognition and precise counting.""",2023,2023-12-23T22:36:58Z,"Keyphrase: ""Limited fine-grained recognition and counting abilities""","""However, there are limitations in several tasks requiring fine-grained recognition and precise counting."" Keyphrase: ""Limited fine-grained recognition and counting abilities"""
arXIv2023,Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems,Yes.,4,"""the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput.""",2023,2023-12-23T11:57:53Z,"Keyphrase: ""High computational intensity and memory consumption""","""the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput."" Keyphrase: ""High computational intensity and memory consumption"""
arXIv2023,Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention,Yes.,4,"""the enigmatic 'black-box' nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications.""",2023,2023-12-22T19:55:58Z,"Keyphrase: ""Interpretability challenge""","""the enigmatic 'black-box' nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications."" Keyphrase: ""Interpretability challenge"""
arXIv2023,NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,Yes.,4,"""current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance.""",2023,2023-12-22T18:07:44Z,"Keyphrase: ""Risk of overfitting and benchmark tailoring""","""current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance."" Keyphrase: ""Risk of overfitting and benchmark tailoring"""
arXIv2023,Robust Knowledge Extraction from Large Language Models using Social Choice Theory,Yes.,5,"""they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times.""",2023,2023-12-22T17:57:29Z,"Keyphrase: ""Inconsistent query results""","""they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times."" Keyphrase: ""Inconsistent query results"""
arXIv2023,Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code,Yes.,5,"""This allows gaps in an LLM's code generation abilities to be identified, including $\textit{anomalies}$ where the LLM correctly solves $\textit{almost all}$ questions in a neighbourhood but fails for particular parameter instantiations."" and ""Our findings show that, across the board, Turbulence is able to reveal gaps in LLM reasoning ability.""",2023,2023-12-22T17:29:08Z,"Keyphrase: ""Limited reasoning ability""","""This allows gaps in an LLM's code generation abilities to be identified, including $\textit{anomalies}$ where the LLM correctly solves $\textit{almost all}$ questions in a neighbourhood but fails for particular parameter instantiations."" and ""Our findings show that, across the board, Turbulence is able to reveal gaps in LLM reasoning ability."" Keyphrase: ""Limited reasoning ability"""
arXIv2023,Large Language Model (LLM) Bias Index -- LLMBI,Yes.,4,"""This research introduces a novel metric, LLMBI, to systematically measure and mitigate biases potentially skewing model responses"" and ""The research reveals LLMs, whilst demonstrating impressive capabilities in text generation, exhibit varying degrees of bias across different dimensions.""",2023,2023-12-22T15:38:13Z,"Keyphrase: ""Bias variability""","""This research introduces a novel metric, LLMBI, to systematically measure and mitigate biases potentially skewing model responses"" and ""The research reveals LLMs, whilst demonstrating impressive capabilities in text generation, exhibit varying degrees of bias across different dimensions."" Keyphrase: ""Bias variability"""
arXIv2023,Empowering Working Memory for Large Language Model Agents,Yes.,5,"""Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning.""",2023,2023-12-22T05:59:00Z,"Keyphrase: ""Limited memory retention""","""Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning."" Keyphrase: ""Limited memory retention"""
arXIv2023,Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models,Yes.,5,"""these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides"" and ""Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different",2023,2023-12-22T00:31:46Z,"Keyphrase: ""Hallucination and lack of faithfulness""","""these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides"" and ""Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different Keyphrase: ""Hallucination and lack of faithfulness"""
arXIv2023,Context-aware Decoding Reduces Hallucination in Query-focused Summarization,Yes.,5,"""However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs.""",2023,2023-12-21T23:42:13Z,"Keyphrase: ""Risk of hallucination""","""However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs."" Keyphrase: ""Risk of hallucination"""
arXIv2023,From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models,Yes.,5,"""technologies based on generative artificial intelligence (GenAI) are known to hallucinate, misinform, and display biases introduced by the massive datasets on which they are trained.""",2023,2023-12-21T22:50:14Z,"Keyphrase: ""Bias and misinformation""","""technologies based on generative artificial intelligence (GenAI) are known to hallucinate, misinform, and display biases introduced by the massive datasets on which they are trained."" Keyphrase: ""Bias and misinformation"""
arXIv2023,SimLM: Can Language Models Infer Parameters of Physical Systems?,Yes.,5,"""Our experiments suggest that they are not inherently suited to this task, even for simple systems.""",2023,2023-12-21T12:05:19Z,"Keyphrase: ""Limited task suitability""","""Our experiments suggest that they are not inherently suited to this task, even for simple systems."" Keyphrase: ""Limited task suitability"""
arXIv2023,On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning,Yes.,5,"""both paradigms are prone to suffer from the critical problem of overconfidence (i.e., miscalibration),"" and ""the problem of miscalibration exists across all learning methods in low-resource scenarios.""",2023,2023-12-21T11:55:10Z,"Keyphrase: ""Overconfidence and miscalibration""","""both paradigms are prone to suffer from the critical problem of overconfidence (i.e., miscalibration),"" and ""the problem of miscalibration exists across all learning methods in low-resource scenarios."" Keyphrase: ""Overconfidence and miscalibration"""
arXIv2023,"Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature",Yes.,5,"""We observed substantial differences in intraday (nondeterminism) and across day (drift) results,"" and ""Nondeterminism and drift within LLMs must be assessed and monitored when introducing LLM based functionality into clinical workflows.""",2023,2023-12-21T01:56:00Z,"Keyphrase: ""Intraday nondeterminism and drift""","""We observed substantial differences in intraday (nondeterminism) and across day (drift) results,"" and ""Nondeterminism and drift within LLMs must be assessed and monitored when introducing LLM based functionality into clinical workflows."" Keyphrase: ""Intraday nondeterminism and drift"""
arXIv2023,Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models,Yes.,5,"""the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content.""",2023,2023-12-21T01:08:39Z,"Keyphrase: ""Limited contextual understanding""","""the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content."" Keyphrase: ""Limited contextual understanding"""
arXIv2023,CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models,Yes.,5,"""Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting.""",2023,2023-12-20T09:06:18Z,"Keyphrase: ""Limited zero-shot performance""","""Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting."" Keyphrase: ""Limited zero-shot performance"""
arXIv2023,ALMANACS: A Simulatability Benchmark for Language Model Explainability,Yes.,5,"""Our results are sobering",2023,2023-12-20T03:44:18Z,Keyphrase: Lack of clarity,"""Our results are sobering Keyphrase: Lack of clarity"
arXIv2023,Learning and Forgetting Unsafe Examples in Large Language Models,Yes.,4,"""We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness,"" and ""aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content.""",2023,2023-12-20T03:18:50Z,"Keyphrase: ""Learning and retention of unsafe content""","""We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness,"" and ""aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content."" Keyphrase: ""Learning and retention of unsafe content"""
arXIv2023,Bypassing the Safety Training of Open-Source LLMs with Priming Attacks,Yes.,5,"""we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training.""",2023,2023-12-19T16:47:12Z,"Keyphrase: ""Vulnerability to optimization-free attacks""","""we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training."" Keyphrase: ""Vulnerability to optimization-free attacks"""
arXIv2023,On Early Detection of Hallucinations in Factual Question Answering,Yes.,5,"""While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks like search and summarization, hallucinations remain a major impediment towards gaining user trust.""",2023,2023-12-19T14:35:04Z,"Keyphrase: ""Hallucination remains a major impediment""","""While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks like search and summarization, hallucinations remain a major impediment towards gaining user trust."" Keyphrase: ""Hallucination remains a major impediment"""
arXIv2023,Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment,Yes.,4,"""the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources.""",2023,2023-12-19T13:31:24Z,"Keyphrase: ""High computational demand""","""the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources."" Keyphrase: ""High computational demand"""
arXIv2023,Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies,Yes.,4,"""Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae)."" and ""We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLM",2023,2023-12-19T01:28:46Z,"Keyphrase: ""Limited gender diversity support""","""Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae)."" and ""We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLM Keyphrase: ""Limited gender diversity support"""
arXIv2023,Opportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview,Yes.,4,"""Despite the promising potential of LLMs, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned LLMs, and self-consistency are discussed.""",2023,2023-12-18T20:58:58Z,"Keyphrase: ""Complexity and resource-intensive challenges""","""Despite the promising potential of LLMs, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned LLMs, and self-consistency are discussed."" Keyphrase: ""Complexity and resource-intensive challenges"""
arXIv2023,Evaluating Language-Model Agents on Realistic Autonomous Tasks,Yes.,4,"""We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks."" and ""we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA.""",2023,2023-12-18T19:27:09Z,"Keyphrase: ""Limited task complexity understanding""","""We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks."" and ""we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA."" Keyphrase: ""Limited task complexity understanding"""
arXIv2023,Traces of Memorisation in Large Language Models for Code,Yes.,5,"""The content of these datasets is memorised and can be extracted by attackers with data extraction attacks."" and ""We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts.""",2023,2023-12-18T19:12:58Z,"Keyphrase: ""Vulnerability to data extraction attacks""","""The content of these datasets is memorised and can be extracted by attackers with data extraction attacks."" and ""We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts."" Keyphrase: ""Vulnerability to data extraction attacks"""
arXIv2023,NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation,Yes.,5,"""We measure LLM robustness using two metrics",2023,2023-12-18T17:18:04Z,"Keyphrase: ""Limited robustness assessment""","""We measure LLM robustness using two metrics Keyphrase: ""Limited robustness assessment"""
arXIv2023,Linear Attention via Orthogonal Memory,Yes.,5,"""most existing linear attention mechanisms suffer from an efficiency degradation problem, leading to inefficiencies in causal language modeling and hindering their application in long-range language models.""",2023,2023-12-18T12:26:27Z,"Keyphrase: ""Efficiency degradation""","""most existing linear attention mechanisms suffer from an efficiency degradation problem, leading to inefficiencies in causal language modeling and hindering their application in long-range language models."" Keyphrase: ""Efficiency degradation"""
arXIv2023,Split and Rephrase with Large Language Models,Yes.,4,"""we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance"" and ""Our results provide a fine-grained analysis of the potential and limitations of large language models for SPRP, with significant improvements achievable using relatively small amounts of training data and model parameters overall,",2023,2023-12-18T10:16:37Z,"Keyphrase: ""Limited improvement despite data and parameter scaling""","""we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance"" and ""Our results provide a fine-grained analysis of the potential and limitations of large language models for SPRP, with significant improvements achievable using relatively small amounts of training data and model parameters overall, Keyphrase: ""Limited improvement despite data and parameter scaling"""
arXIv2023,Retrieval-Augmented Generation for Large Language Models: A Survey,Yes.,5,"""Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes.""",2023,2023-12-18T07:47:33Z,"Keyphrase: ""Nontransparent reasoning""","""Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes."" Keyphrase: ""Nontransparent reasoning"""
arXIv2023,Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression,Yes.,4,"""However, they come with substantial computational and memory costs. Additionally, they are essentially black-box models, challenging to explain and interpret.""",2023,2023-12-17T12:33:50Z,"Keyphrase: ""High computational and memory costs""","""However, they come with substantial computational and memory costs. Additionally, they are essentially black-box models, challenging to explain and interpret."" Keyphrase: ""High computational and memory costs"""
arXIv2023,An Evaluation of GPT-4V and Gemini in Online VQA,Yes.,5,"""Our zero-shot performance analysis highlights the types of questions that are most challenging for both models, including questions related to 'puzzling' topic, with 'Identification' user intention, with 'Sheet Music' image type, or labeled as 'hard' by GPT-4.""",2023,2023-12-17T07:38:43Z,"Keyphrase: ""Challenges in zero-shot performance""","""Our zero-shot performance analysis highlights the types of questions that are most challenging for both models, including questions related to 'puzzling' topic, with 'Identification' user intention, with 'Sheet Music' image type, or labeled as 'hard' by GPT-4."" Keyphrase: ""Challenges in zero-shot performance"""
arXIv2023,DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content,Yes.,5,"""The quantitative and qualitative experiments fully reveal the limitations of the GPT-4 model in image synthesis.""",2023,2023-12-16T10:17:09Z,"Keyphrase: ""Limitation in image synthesis""","""The quantitative and qualitative experiments fully reveal the limitations of the GPT-4 model in image synthesis."" Keyphrase: ""Limitation in image synthesis"""
arXIv2023,Challenges with unsupervised LLM knowledge discovery,Yes.,5,"""We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent.""",2023,2023-12-15T18:49:43Z,Keyphrase: Lack of control over knowledge discovery,"""We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent."" Keyphrase: Lack of control over knowledge discovery"
arXIv2023,LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin,Yes.,5,"""However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs.""",2023,2023-12-15T17:45:06Z,"Keyphrase: ""Damage to world knowledge""","""However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs."" Keyphrase: ""Damage to world knowledge"""
arXIv2023,Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China,Yes.,4,"""The rising popularity of ChatGPT and other AI-powered large language models (LLMs) has led to increasing studies highlighting their susceptibility to mistakes and biases."" and ""This disparity may stem from Chinese state censorship and US-China geopolitical tensions, which influence the training corpora of GPT bilingual models.""",2023,2023-12-15T16:25:56Z,"Keyphrase: ""Bias and geopolitical influence""","""The rising popularity of ChatGPT and other AI-powered large language models (LLMs) has led to increasing studies highlighting their susceptibility to mistakes and biases."" and ""This disparity may stem from Chinese state censorship and US-China geopolitical tensions, which influence the training corpora of GPT bilingual models."" Keyphrase: ""Bias and geopolitical influence"""
arXIv2023,Taxonomy-based CheckList for Large Language Model Evaluation,Yes.,4,"""the internal stereotypical representation may affect the fairness of the outputs,"" ""we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA),"" and ""Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation.""",2023,2023-12-15T12:58:07Z,"Keyphrase: ""Bias in unethical behavior""","""the internal stereotypical representation may affect the fairness of the outputs,"" ""we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA),"" and ""Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation."" Keyphrase: ""Bias in unethical behavior"""
arXIv2023,Extending Context Window of Large Language Models via Semantic Compression,Yes.,5,"""Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts.""",2023,2023-12-15T07:04:33Z,"Keyphrase: ""Text input length limitation""","""Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts."" Keyphrase: ""Text input length limitation"""
arXIv2023,Marathon: A Race Through the Realm of Long Context with Large Language Models,Yes.,5,"""the existing long context benchmarks are no longer sufficient for evaluating the long context understanding and reasoning capability of large language models.""",2023,2023-12-15T05:30:14Z,"Keyphrase: ""Inadequate long context evaluation""","""the existing long context benchmarks are no longer sufficient for evaluating the long context understanding and reasoning capability of large language models."" Keyphrase: ""Inadequate long context evaluation"""
arXIv2023,No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models,Yes.,5,"""our work for the first time reveals the acceleration may be vulnerable to Denial-of-Service (DoS) attacks"" and ""evaluate the vulnerability of the skimming acceleration in various LLM architectures.""",2023,2023-12-15T02:42:05Z,"Keyphrase: ""Vulnerability to denial-of-service attacks""","""our work for the first time reveals the acceleration may be vulnerable to Denial-of-Service (DoS) attacks"" and ""evaluate the vulnerability of the skimming acceleration in various LLM architectures."" Keyphrase: ""Vulnerability to denial-of-service attacks"""
arXIv2023,Towards Verifiable Text Generation with Evolving Memory and Self-Reflection,Yes.,5,"""Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination.""",2023,2023-12-14T16:10:56Z,"Keyphrase: ""Factually incorrect information and hallucination""","""Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination."" Keyphrase: ""Factually incorrect information and hallucination"""
arXIv2023,Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent,Yes.,5,"""Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation.""",2023,2023-12-14T13:33:50Z,"Keyphrase: ""Limited mathematical problem-solving capacity""","""Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation."" Keyphrase: ""Limited mathematical problem-solving capacity"""
arXIv2023,Evaluating Large Language Models for Health-related Queries with Presuppositions,Yes.,5,"""Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios.""",2023,2023-12-14T10:35:13Z,"Keyphrase: ""Moderate factual accuracy""","""Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios."" Keyphrase: ""Moderate factual accuracy"""
arXIv2023,ChatSOS: LLM-based knowledge Q&A system for safety engineering,Yes.,4,"""Despite these advancements, LLMs face constraints in processing specialized tasks, attributed to factors such as corpus size, input processing limitations, and privacy concerns.""",2023,2023-12-14T03:25:23Z,"Keyphrase: ""Limited specialized task processing""","""Despite these advancements, LLMs face constraints in processing specialized tasks, attributed to factors such as corpus size, input processing limitations, and privacy concerns."" Keyphrase: ""Limited specialized task processing"""
arXIv2023,Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF,Yes.,4,"""Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability.""",2023,2023-12-13T18:51:34Z,"Keyphrase: ""Limited contextual understanding""","""Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability."" Keyphrase: ""Limited contextual understanding"""
arXIv2023,Breaking the Silence: the Threats of Using LLMs in Software Engineering,Yes.,5,"""This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings.""",2023,2023-12-13T11:02:19Z,"Keyphrase: ""Validity and reproducibility concerns""","""This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings."" Keyphrase: ""Validity and reproducibility concerns"""
arXIv2023,Large Language Model Enhanced Multi-Agent Systems for 6G Communications,Yes.,4,"""directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities.""",2023,2023-12-13T02:35:57Z,"Keyphrase: ""Limited logical reasoning""","""directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities."" Keyphrase: ""Limited logical reasoning"""
arXIv2023,Large language models in healthcare and medical domain: A review,Yes.,4,"""Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings.""",2023,2023-12-12T20:54:51Z,"Keyphrase: ""Limited understanding of healthcare domain""","""Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings."" Keyphrase: ""Limited understanding of healthcare domain"""
arXIv2023,Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection,Yes.,4,"""Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence."" and ""we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias.""",2023,2023-12-12T18:05:46Z,"Keyphrase: ""Bias in input-label mapping""","""Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence."" and ""we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias."" Keyphrase: ""Bias in input-label mapping"""
arXIv2023,FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs,Yes.,4,"""Training large language models (LLMs) is a costly endeavour in terms of time and computational resources,"" and ""We evaluate the performance-fairness trade-off for SISA, and empirically demonstrate that SISA can indeed reduce fairness in LLMs.""",2023,2023-12-12T16:44:47Z,"Keyphrase: ""Costly training and fairness tradeoff""","""Training large language models (LLMs) is a costly endeavour in terms of time and computational resources,"" and ""We evaluate the performance-fairness trade-off for SISA, and empirically demonstrate that SISA can indeed reduce fairness in LLMs."" Keyphrase: ""Costly training and fairness tradeoff"""
arXIv2023,On Diversified Preferences of Large Language Model Alignment,Yes.,4,"""However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods."" and ""We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as Harmless&Helpful, thereby impairing the alignment performance of LLMs.""",2023,2023-12-12T16:17:15Z,"Keyphrase: ""Diversified human preferences hindering alignment""","""However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods."" and ""We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as Harmless&Helpful, thereby impairing the alignment performance of LLMs."" Keyphrase: ""Diversified human preferences hindering alignment"""
arXIv2023,Sequential Planning in Large Partially Observable Environments guided by LLMs,Yes.,4,"""But they still struggle with exploration and get stuck in local optima. Their planning capabilities are limited by the limited reasoning capability of the foundational LLMs on text data.""",2023,2023-12-12T15:36:59Z,"Keyphrase: ""Limited reasoning capability""","""But they still struggle with exploration and get stuck in local optima. Their planning capabilities are limited by the limited reasoning capability of the foundational LLMs on text data."" Keyphrase: ""Limited reasoning capability"""
arXIv2023,Multilingual large language models leak human stereotypes across language boundaries,Yes.,5,"""Previous research has shown that the presence of stereotypes and biases in monolingual large language models can be attributed to the nature of their training data, which is collected from humans and reflects societal biases."" and ""This raises the question",2023,2023-12-12T10:24:17Z,"Keyphrase: ""Stereotype bias in training data""","""Previous research has shown that the presence of stereotypes and biases in monolingual large language models can be attributed to the nature of their training data, which is collected from humans and reflects societal biases."" and ""This raises the question Keyphrase: ""Stereotype bias in training data"""
arXIv2023,LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature,Yes.,5,"""The results suggest that LLMs do not produce good knowledge entities that reflect the cybersecurity context, but our results show some potential for noun extractors.""",2023,2023-12-12T09:39:03Z,"Keyphrase: ""Limited contextual understanding""","""The results suggest that LLMs do not produce good knowledge entities that reflect the cybersecurity context, but our results show some potential for noun extractors."" Keyphrase: ""Limited contextual understanding"""
arXIv2023,Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications,Yes.,5,"""we explore the challenges inherent to Large Language Models (LLMs) like GPT-4, particularly their propensity for hallucinations, logic mistakes, and incorrect conclusions when tasked with answering complex questions.""",2023,2023-12-12T08:43:20Z,"Keyphrase: ""Hallucination and logic mistakes""","""we explore the challenges inherent to Large Language Models (LLMs) like GPT-4, particularly their propensity for hallucinations, logic mistakes, and incorrect conclusions when tasked with answering complex questions."" Keyphrase: ""Hallucination and logic mistakes"""
arXIv2023,Alignment for Honesty,Yes.,5,"""a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies.""",2023,2023-12-12T06:10:42Z,"Keyphrase: ""Challenges in aligning knowledge""","""a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies."" Keyphrase: ""Challenges in aligning knowledge"""
arXIv2023,Hallucination Augmented Contrastive Learning for Multimodal Large Language Model,Yes.,5,"""MLLMs still face a fundamental limitation of hallucinations, where they tend to generate erroneous or fabricated information.""",2023,2023-12-12T04:05:15Z,"Keyphrase: ""Erroneous hallucination""","""MLLMs still face a fundamental limitation of hallucinations, where they tend to generate erroneous or fabricated information."" Keyphrase: ""Erroneous hallucination"""
arXIv2023,Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack,Yes.,5,"""Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks."" and ""Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content.""",2023,2023-12-12T01:39:29Z,"Keyphrase: ""Safety misalignment""","""Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks."" and ""Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content."" Keyphrase: ""Safety misalignment"""
arXIv2023,Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?,Yes.,4,"""However, this impressive performance comes with inherent limitations, such as the tendency to perpetuate stereotypical biases or fabricate non-existent facts.""",2023,2023-12-11T18:59:09Z,"Keyphrase: ""Perpetuation of stereotypical bias""","""However, this impressive performance comes with inherent limitations, such as the tendency to perpetuate stereotypical biases or fabricate non-existent facts."" Keyphrase: ""Perpetuation of stereotypical bias"""
arXIv2023,KnowGPT: Knowledge Injection for Large Language Models,Yes.,4,"""However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus.""",2023,2023-12-11T07:56:25Z,"Keyphrase: ""Lack of domain-specific knowledge""","""However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus."" Keyphrase: ""Lack of domain-specific knowledge"""
arXIv2023,Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding,Yes.,5,"""However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention.""",2023,2023-12-11T06:35:33Z,"Keyphrase: ""Toxicity and hallucination challenges""","""However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention."" Keyphrase: ""Toxicity and hallucination challenges"""
arXIv2023,METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities,Yes.,4,"""However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications."" and ""existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend.""",2023,2023-12-11T01:29:19Z,"Keyphrase: ""Blackboxed probabilistic outputs""","""However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications."" and ""existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend."" Keyphrase: ""Blackboxed probabilistic outputs"""
arXIv2023,Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs,Yes.,5,"""However, this knowledge is inherently limited, relying heavily on the characteristics of the training data."" and ""we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.""",2023,2023-12-10T16:52:00Z,"Keyphrase: ""Limited knowledge acquisition""","""However, this knowledge is inherently limited, relying heavily on the characteristics of the training data."" and ""we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem."" Keyphrase: ""Limited knowledge acquisition"""
arXIv2023,Large Multimodal Model Compression via Efficient Pruning and Distillation at AntGroup,Yes.,4,"""However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI.""",2023,2023-12-10T06:57:48Z,"Keyphrase: ""Latency and carbon footprint challenges""","""However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI."" Keyphrase: ""Latency and carbon footprint challenges"""
arXIv2023,Understanding the Effect of Model Compression on Social Bias in Large Language Models,Yes.,4,"""Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm."" and ""We perform a carefully controlled study of the",2023,2023-12-09T20:04:20Z,"Keyphrase: ""Persistent social biases""","""Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm."" and ""We perform a carefully controlled study of the Keyphrase: ""Persistent social biases"""
arXIv2023,PaperQA: Retrieval-Augmented Generative Agent for Scientific Research,Yes.,5,"""Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth.""",2023,2023-12-08T18:50:20Z,"Keyphrase: ""Hallucination and uninterpretability""","""Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth."" Keyphrase: ""Hallucination and uninterpretability"""
arXIv2023,"Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",Yes.,5,"""Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities.""",2023,2023-12-08T18:25:22Z,"Keyphrase: ""Limited reasoning and planning""","""Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities."" Keyphrase: ""Limited reasoning and planning"""
arXIv2023,HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models,Yes.,5,"""However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems.""",2023,2023-12-08T17:57:20Z,"Keyphrase: ""Faulty reasoning and information hallucination""","""However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems."" Keyphrase: ""Faulty reasoning and information hallucination"""
arXIv2023,DelucionQA: Detecting Hallucinations in Domain-specific Question Answering,Yes.,5,"""Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing",2023,2023-12-08T17:41:06Z,"Keyphrase: ""Hallucinatory responses""","""Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing Keyphrase: ""Hallucinatory responses"""
arXIv2023,Assessing LLMs for Moral Value Pluralism,Yes.,4,"""the fields of AI current lacks methods to quantitatively assess and potentially alter the moral values inherent in the output of large language models (LLMs)"" and ""we find that LLMs exhibit several Western-centric value biases; they overestimate how conservative people in non-Western countries are, they are less accurate in representing gender for non-Western countries, and portray older populations as having",2023,2023-12-08T16:18:15Z,"Keyphrase: ""Western-centric bias""","""the fields of AI current lacks methods to quantitatively assess and potentially alter the moral values inherent in the output of large language models (LLMs)"" and ""we find that LLMs exhibit several Western-centric value biases; they overestimate how conservative people in non-Western countries are, they are less accurate in representing gender for non-Western countries, and portray older populations as having Keyphrase: ""Western-centric bias"""
arXIv2023,TypeFly: Flying Drones with Large Language Model,Yes.,5,"""However, powerful LLMs and their vision counterparts are limited in three important ways. First, they are only available as cloud-based services. Sending images to the cloud raises privacy concerns. Second, they are expensive, costing proportionally to the request size. Finally, without expensive fine-tuning, existing LLMs",2023,2023-12-08T15:57:18Z,"Keyphrase: ""Privacy concerns and cost limitations""","""However, powerful LLMs and their vision counterparts are limited in three important ways. First, they are only available as cloud-based services. Sending images to the cloud raises privacy concerns. Second, they are expensive, costing proportionally to the request size. Finally, without expensive fine-tuning, existing LLMs Keyphrase: ""Privacy concerns and cost limitations"""
arXIv2023,Retrieval-based Video Language Model for Efficient Long Video Question Answering,Yes.,5,"""However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video QA process.""",2023,2023-12-08T09:48:36Z,"Keyphrase: ""Computational cost and noise in video understanding""","""However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video QA process."" Keyphrase: ""Computational cost and noise in video understanding"""
arXIv2023,Exploring the Limits of ChatGPT in Software Security Applications,Yes.,5,"""However, the impacts and limits of such LLMs in system security domain are less explored. In this paper, we delve into the limits of LLMs (i.e., ChatGPT) in seven software security applications... Also, certain limitations of ChatGPT in security-related tasks are identified, such as its constrained ability to process long code contexts.""",2023,2023-12-08T03:02:37Z,"Keyphrase: ""Limited security domain understanding""","""However, the impacts and limits of such LLMs in system security domain are less explored. In this paper, we delve into the limits of LLMs (i.e., ChatGPT) in seven software security applications... Also, certain limitations of ChatGPT in security-related tasks are identified, such as its constrained ability to process long code contexts."" Keyphrase: ""Limited security domain understanding"""
arXIv2023,DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions,Yes.,5,"""a dangerous nature is hidden in the code, which is the existence of fatal vulnerabilities,"" and ""shed light on the huge weakness of LLMs in the code generation task.""",2023,2023-12-07T22:19:06Z,"Keyphrase: ""Hidden code vulnerabilities""","""a dangerous nature is hidden in the code, which is the existence of fatal vulnerabilities,"" and ""shed light on the huge weakness of LLMs in the code generation task."" Keyphrase: ""Hidden code vulnerabilities"""
arXIv2023,Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models,Yes.,4,"""A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs.""",2023,2023-12-07T22:07:54Z,"Keyphrase: ""Insecure code tendencies""","""A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs."" Keyphrase: ""Insecure code tendencies"""
arXIv2023,Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use,Yes.,5,"""Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance.""",2023,2023-12-07T17:24:51Z,"Keyphrase: ""Overlooking crucial contextual information""","""Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance."" Keyphrase: ""Overlooking crucial contextual information"""
arXIv2023,Hijacking Context in Large Multi-modal Models,Yes.,5,"""we identify a new limitation of off-the-shelf LMMs where a small fraction of incoherent images or text descriptions mislead LMMs to only generate biased output about the hijacked context, not the originally intended context.""",2023,2023-12-07T11:23:29Z,"Keyphrase: ""Incoherent and biased outputs""","""we identify a new limitation of off-the-shelf LMMs where a small fraction of incoherent images or text descriptions mislead LMMs to only generate biased output about the hijacked context, not the originally intended context."" Keyphrase: ""Incoherent and biased outputs"""
arXIv2023,Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak,Yes.,5,"""However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as 'Jailbreak Attack'.""",2023,2023-12-07T08:29:58Z,"Keyphrase: ""Vulnerability to malicious instructions""","""However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as 'Jailbreak Attack'."" Keyphrase: ""Vulnerability to malicious instructions"""
arXIv2023,Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge Base for Industrial Prognostics and Health Management,Yes.,5,"""Although ChatGPT-Like LLMs have rich knowledge reserves and powerful language understanding and generation capabilities, they lack domain-specific expertise, significantly limiting their practicability in PHM applications.""",2023,2023-12-06T15:24:01Z,"Keyphrase: ""Lack of domain-specific expertise""","""Although ChatGPT-Like LLMs have rich knowledge reserves and powerful language understanding and generation capabilities, they lack domain-specific expertise, significantly limiting their practicability in PHM applications."" Keyphrase: ""Lack of domain-specific expertise"""
arXIv2023,GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science,Yes.,5,"""they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest.""",2023,2023-12-05T21:41:52Z,"Keyphrase: ""Lack of deep understanding""","""they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest."" Keyphrase: ""Lack of deep understanding"""
arXIv2023,LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications,Yes.,5,"""due to unresolved vulnerabilities and limitations, great care needs to be used before applying them to intelligence and safety-critical applications."" and ""The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM.""",2023,2023-12-05T19:04:50Z,"Keyphrase: ""Safety-critical application vulnerability""","""due to unresolved vulnerabilities and limitations, great care needs to be used before applying them to intelligence and safety-critical applications."" and ""The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM."" Keyphrase: ""Safety-critical application vulnerability"""
arXIv2023,Weakly Supervised Detection of Hallucinations in LLM Activations,Yes.,4,"""Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally.""",2023,2023-12-05T14:35:11Z,"Keyphrase: ""Limited internal capacity""","""Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally."" Keyphrase: ""Limited internal capacity"""
arXIv2023,How should the advent of large language models affect the practice of science?,Yes.,4,"""Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools.""",2023,2023-12-05T10:45:12Z,"Keyphrase: ""Misuse and overhype""","""Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools."" Keyphrase: ""Misuse and overhype"""
arXIv2023,"Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety",Yes.,5,"""Nevertheless, these models remain black boxes despite incorporating human feedback and instruction-guided tuning. For instance, ChatGPT can generate unsafe responses despite instituting safety guardrails.""",2023,2023-12-05T06:13:55Z,"Keyphrase: ""Black box model with unsafe responses""","""Nevertheless, these models remain black boxes despite incorporating human feedback and instruction-guided tuning. For instance, ChatGPT can generate unsafe responses despite instituting safety guardrails."" Keyphrase: ""Black box model with unsafe responses"""
arXIv2023,E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation,Yes.,5,"""However, this approach necessitates items to possess rich semantic information, often generates out-of-range results, and suffers from notably low efficiency and limited extensibility. ... Nevertheless, the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations.""",2023,2023-12-05T02:50:18Z,"Keyphrase: ""Limited semantic understanding""","""However, this approach necessitates items to possess rich semantic information, often generates out-of-range results, and suffers from notably low efficiency and limited extensibility. ... Nevertheless, the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations."" Keyphrase: ""Limited semantic understanding"""
arXIv2023,Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation,Yes.,5,"""observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game.""",2023,2023-12-05T02:41:57Z,"Keyphrase: ""Limited out-of-domain generalization""","""observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game."" Keyphrase: ""Limited out-of-domain generalization"""
arXIv2023,New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking,Yes.,5,"""Our experiments, conducted across various datasets, reveal that current watermarking methods are detectable by even simple classifiers, challenging the notion of watermarking subtlety. We also found, through the LLM judger, that watermarking impacts text quality, especially in degrading the coherence and depth of the response.""",2023,2023-12-04T22:56:31Z,"Keyphrase: ""Degraded text quality""","""Our experiments, conducted across various datasets, reveal that current watermarking methods are detectable by even simple classifiers, challenging the notion of watermarking subtlety. We also found, through the LLM judger, that watermarking impacts text quality, especially in degrading the coherence and depth of the response."" Keyphrase: ""Degraded text quality"""
arXIv2023,Competition-Level Problems are Effective LLM Evaluators,Yes.,5,"""the challenges for any existing LLM to solve unseen complex reasoning problems"" and ""none of them is able to consistently mitigate the challenges.""",2023,2023-12-04T18:58:57Z,"Keyphrase: ""Limited complex reasoning ability""","""the challenges for any existing LLM to solve unseen complex reasoning problems"" and ""none of them is able to consistently mitigate the challenges."" Keyphrase: ""Limited complex reasoning ability"""
arXIv2023,Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?,Yes.,5,"""Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible. They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels. Overall, current LLMs have set readability ranges and do",2023,2023-12-04T17:19:53Z,"Keyphrase: ""Limited adaptability to diverse audience demographics""","""Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible. They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels. Overall, current LLMs have set readability ranges and do Keyphrase: ""Limited adaptability to diverse audience demographics"""
arXIv2023,Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites,Yes.,5,"""However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods.""",2023,2023-12-04T07:43:02Z,"Keyphrase: ""Object hallucination""","""However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods."" Keyphrase: ""Object hallucination"""
arXIv2023,Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies,Yes.,4,"""However, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, and must be addressed."" and ""This research presents a comprehensive survey synthesising the current trends and limitations in techniques used for identifying and mitigating bias in LLMs.""",2023,2023-12-03T21:25:10Z,"Keyphrase: ""Bias and discrimination""","""However, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, and must be addressed."" and ""This research presents a comprehensive survey synthesising the current trends and limitations in techniques used for identifying and mitigating bias in LLMs."" Keyphrase: ""Bias and discrimination"""
arXIv2023,Running cognitive evaluations on large language models: The do's and the don'ts,Yes.,4,"""I describe common pitfalls that might arise when applying a cognitive test to an LLM"" and ""I conclude by discussing four areas where the do's and don'ts are currently under active discussion -- prompt sensitivity, cultural and linguistic diversity, using LLMs as research assistants, and running evaluations on open vs. closed LLMs.""",2023,2023-12-03T04:28:19Z,"Keyphrase: ""Limited sensitivity to cultural and linguistic diversity""","""I describe common pitfalls that might arise when applying a cognitive test to an LLM"" and ""I conclude by discussing four areas where the do's and don'ts are currently under active discussion -- prompt sensitivity, cultural and linguistic diversity, using LLMs as research assistants, and running evaluations on open vs. closed LLMs."" Keyphrase: ""Limited sensitivity to cultural and linguistic diversity"""
arXIv2023,Towards leveraging LLMs for Conditional QA,Yes.,5,"""This study delves into the capabilities and limitations of Large Language Models (LLMs) in the challenging domain of conditional question-answering."" and ""these models encounter challenges in extractive question answering, where they lag behind the SOTA by over 10 points, and in mitigating the risk of injecting false information.""",2023,2023-12-02T14:02:52Z,"Keyphrase: ""Challenges in conditional question-answering""","""This study delves into the capabilities and limitations of Large Language Models (LLMs) in the challenging domain of conditional question-answering."" and ""these models encounter challenges in extractive question answering, where they lag behind the SOTA by over 10 points, and in mitigating the risk of injecting false information."" Keyphrase: ""Challenges in conditional question-answering"""
arXIv2023,Nonparametric Variational Regularisation of Pretrained Transformers,Yes.,4,"""However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes."" and ""Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large.""",2023,2023-12-01T15:40:30Z,"Keyphrase: ""Overfitting and poor domain adaptation""","""However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes."" and ""Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large."" Keyphrase: ""Overfitting and poor domain adaptation"""
arXIv2023,Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?,Yes.,4,"""a critical concern arises regarding the potential biases embedded within these summaries,"" and ""The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models.""",2023,2023-12-01T13:00:45Z,"Keyphrase: ""Embedded bias in summaries""","""a critical concern arises regarding the potential biases embedded within these summaries,"" and ""The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models."" Keyphrase: ""Embedded bias in summaries"""
arXIv2023,Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web,Yes.,5,"""We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks."" and ""their performance further degrades under different instruction compositions changing combinational order.""",2023,2023-11-30T17:50:47Z,"Keyphrase: ""Decreased task performance with compositional tasks""","""We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks."" and ""their performance further degrades under different instruction compositions changing combinational order."" Keyphrase: ""Decreased task performance with compositional tasks"""
arXIv2023,ArthModel: Enhance Arithmetic Skills to Large Language Model,Yes.,5,"""However, the models have several limitations, such as toxicity and pool performance of arithmetic solving.""",2023,2023-11-30T15:06:50Z,"Keyphrase: ""Toxicity and performance limitations""","""However, the models have several limitations, such as toxicity and pool performance of arithmetic solving."" Keyphrase: ""Toxicity and performance limitations"""
arXIv2023,"FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity",Yes.,5,"""Experiments show that the harmlessness of LLMs is still under-satisfactory, and extensive analysis derives some insightful findings that could inspire future research for harmless LLM research.""",2023,2023-11-30T14:18:47Z,"Keyphrase: ""Undersatisfactory performance""","""Experiments show that the harmlessness of LLMs is still under-satisfactory, and extensive analysis derives some insightful findings that could inspire future research for harmless LLM research."" Keyphrase: ""Undersatisfactory performance"""
arXIv2023,Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension,Yes.,5,"""Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated.""",2023,2023-11-30T08:44:55Z,"Keyphrase: ""Limited capability in explaining incorrect alternatives""","""Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated."" Keyphrase: ""Limited capability in explaining incorrect alternatives"""
arXIv2023,Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes,Yes.,5,"""Despite the impressive ICL ability of LLMs, it has also been found that ICL in LLMs is sensitive to input demonstrations and limited to short context lengths.""",2023,2023-11-30T02:26:55Z,"Keyphrase: ""Limited context length""","""Despite the impressive ICL ability of LLMs, it has also been found that ICL in LLMs is sensitive to input demonstrations and limited to short context lengths."" Keyphrase: ""Limited context length"""
arXIv2023,Zero-shot Conversational Summarization Evaluations with small Large Language Models,Yes.,5,"""We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully."" and ""We also evaluate the models with human evaluations and discuss the limitations of the models on conversational summarization",2023,2023-11-29T19:34:34Z,"Keyphrase: ""Instruction dependency and variability""","""We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully."" and ""We also evaluate the models with human evaluations and discuss the limitations of the models on conversational summarization Keyphrase: ""Instruction dependency and variability"""
arXIv2023,OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation,Yes.,5,"""Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment.""",2023,2023-11-29T18:57:07Z,"Keyphrase: ""Pervasive hallucination""","""Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment."" Keyphrase: ""Pervasive hallucination"""
arXIv2023,MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models,Yes.,5,"""we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images"" and ""Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned.""",2023,2023-11-29T12:49:45Z,"Keyphrase: ""Susceptible to breaches in multimodal analysis""","""we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images"" and ""Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned."" Keyphrase: ""Susceptible to breaches in multimodal analysis"""
arXIv2023,Unveiling the Implicit Toxicity in Large Language Models,Yes.,5,"""LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting."" and ""LLMs pose a significant threat in generating undetectable implicit toxic outputs.""",2023,2023-11-29T06:42:36Z,"Keyphrase: ""Generation of undetectable toxic output""","""LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting."" and ""LLMs pose a significant threat in generating undetectable implicit toxic outputs."" Keyphrase: ""Generation of undetectable toxic output"""
arXIv2023,Are Large Language Models Good Fact Checkers: A Preliminary Study,Yes.,5,"""However, they encounter challenges in effectively handling Chinese fact verification and the entirety of the fact-checking pipeline due to language inconsistencies and hallucinations.""",2023,2023-11-29T05:04:52Z,"Keyphrase: ""Language inconsistency and hallucination""","""However, they encounter challenges in effectively handling Chinese fact verification and the entirety of the fact-checking pipeline due to language inconsistencies and hallucinations."" Keyphrase: ""Language inconsistency and hallucination"""
arXIv2023,Elo Uncovered: Robustness and Best Practices in Language Model Evaluation,Yes.,5,"""We show that these axioms are not always satisfied raising questions about the reliability of current comparative evaluations of LLMs.""",2023,2023-11-29T00:45:23Z,"Keyphrase: ""Reliability concerns""","""We show that these axioms are not always satisfied raising questions about the reliability of current comparative evaluations of LLMs."" Keyphrase: ""Reliability concerns"""
arXIv2023,Scalable Extraction of Training Data from (Production) Language Models,Yes.,5,"""Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.""",2023,2023-11-28T18:47:03Z,"Keyphrase: ""Memorization vulnerability""","""Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization."" Keyphrase: ""Memorization vulnerability"""
arXIv2023,Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization,Yes.,5,"""they still suffer from a common issue known as the 'hallucination problem', in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images.""",2023,2023-11-28T14:54:37Z,"Keyphrase: ""Inaccurate textual generation""","""they still suffer from a common issue known as the 'hallucination problem', in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images."" Keyphrase: ""Inaccurate textual generation"""
arXIv2023,Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop,Yes.,5,"""We find that this self-consuming training loop initially improves both quality and diversity. However, after a few generations the output inevitably degenerates in diversity.""",2023,2023-11-28T14:36:43Z,"Keyphrase: ""Degradation of diversity""","""We find that this self-consuming training loop initially improves both quality and diversity. However, after a few generations the output inevitably degenerates in diversity."" Keyphrase: ""Degradation of diversity"""
arXIv2023,ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?,Yes.,5,"""Overall, models exhibit consistent and significant over-confidence on low and medium confidence statements.""",2023,2023-11-28T10:26:57Z,"Keyphrase: ""Overconfidence in predictions""","""Overall, models exhibit consistent and significant over-confidence on low and medium confidence statements."" Keyphrase: ""Overconfidence in predictions"""
arXIv2023,SEED-Bench-2: Benchmarking Multimodal Large Language Models,Yes.,5,"""By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence.""",2023,2023-11-28T05:53:55Z,"Keyphrase: ""Limited evaluation and insight""","""By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence."" Keyphrase: ""Limited evaluation and insight"""
arXIv2023,Methods to Estimate Large Language Model Confidence,Yes.,5,"""Large Language Models have difficulty communicating uncertainty, which is a significant obstacle to applying LLMs to complex medical tasks."" and ""We conclude GPT4 has a limited ability to assess its own diagnostic accuracy.""",2023,2023-11-28T05:44:06Z,"Keyphrase: ""Limited ability to communicate uncertainty""","""Large Language Models have difficulty communicating uncertainty, which is a significant obstacle to applying LLMs to complex medical tasks."" and ""We conclude GPT4 has a limited ability to assess its own diagnostic accuracy."" Keyphrase: ""Limited ability to communicate uncertainty"""
arXIv2023,Enabling Fast 2-bit LLM on GPUs: Memory Alignment and Asynchronous Dequantization,Yes.,4,"""Nonnegligible accuracy loss for 2-bit quantization. Weights are quantized by groups, while the ranges of weights are large in some groups, resulting in large quantization errors and nonnegligible accuracy loss (e.g. >3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit)."" and ""Time-consuming",2023,2023-11-28T02:44:59Z,"Keyphrase: ""Accuracy loss due to quantization""","""Nonnegligible accuracy loss for 2-bit quantization. Weights are quantized by groups, while the ranges of weights are large in some groups, resulting in large quantization errors and nonnegligible accuracy loss (e.g. >3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit)."" and ""Time-consuming Keyphrase: ""Accuracy loss due to quantization"""
arXIv2023,Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation,Yes.,4,"""However, these models are typically trained on web-scale data, which can introduce inappropriate content and lead to the development of unsafe and biased behavior. This, in turn, hampers their applicability in sensitive and trustworthy contexts and could raise significant concern in their adoption.""",2023,2023-11-27T19:02:17Z,"Keyphrase: ""Unsafe biased behavior""","""However, these models are typically trained on web-scale data, which can introduce inappropriate content and lead to the development of unsafe and biased behavior. This, in turn, hampers their applicability in sensitive and trustworthy contexts and could raise significant concern in their adoption."" Keyphrase: ""Unsafe biased behavior"""
arXIv2023,Visual cognition in multimodal large language models,Yes.,5,"""Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology."" and ""Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas."" and ""Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether.""",2023,2023-11-27T18:58:34Z,"Keyphrase: ""Limitation in causal reasoning""","""Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology."" and ""Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas."" and ""Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether."" Keyphrase: ""Limitation in causal reasoning"""
arXIv2023,BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification,Yes.,5,"""they still suffer from a performance gap when the underlying distribution of topics changes"" and ""domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models, such as GPT-3.""",2023,2023-11-27T18:53:31Z,"Keyphrase: ""Domain transfer challenges""","""they still suffer from a performance gap when the underlying distribution of topics changes"" and ""domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models, such as GPT-3."" Keyphrase: ""Domain transfer challenges"""
arXIv2023,WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models,Yes.,5,"""We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly",2023,2023-11-27T15:38:17Z,"Keyphrase: ""Response bias and error persistence""","""We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly Keyphrase: ""Response bias and error persistence"""
arXIv2023,"Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",Yes.,4,"""Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application."" and ""The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability.""",2023,2023-11-27T12:37:51Z,"Keyphrase: ""Risk of retaining faulty or harmful knowledge""","""Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application."" and ""The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability."" Keyphrase: ""Risk of retaining faulty or harmful knowledge"""
arXIv2023,Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications,Yes.,5,"""Despite their large success and acceptance, their lack of explainability hinders legal experts to trust in their output, and this happens rightfully so.""",2023,2023-11-27T10:59:16Z,"Keyphrase: ""Lack of explainability""","""Despite their large success and acceptance, their lack of explainability hinders legal experts to trust in their output, and this happens rightfully so."" Keyphrase: ""Lack of explainability"""
arXIv2023,Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation,Yes.,5,"""we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help"" and ""the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation",2023,2023-11-27T07:19:26Z,"Keyphrase: ""Limited support for text-to-image generation""","""we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help"" and ""the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation Keyphrase: ""Limited support for text-to-image generation"""
arXIv2023,Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination,Yes.,5,"""The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs),"" and ""our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks.""",2023,2023-11-27T05:27:13Z,"Keyphrase: ""Serious hallucination behavior""","""The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs),"" and ""our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks."" Keyphrase: ""Serious hallucination behavior"""
arXIv2023,UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation,Yes.,5,"""These models often produce hallucinated text, compromising their practical utility in professional contexts.""",2023,2023-11-26T13:42:56Z,"Keyphrase: ""Hallucinated text""","""These models often produce hallucinated text, compromising their practical utility in professional contexts."" Keyphrase: ""Hallucinated text"""
arXIv2023,"Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",Yes.,5,"""Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality.""",2023,2023-11-26T08:44:58Z,"Keyphrase: ""Susceptibility to prompt hacking""","""Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality."" Keyphrase: ""Susceptibility to prompt hacking"""
arXIv2023,Benchmarking Large Language Model Volatility,Yes.,5,"""we uncover substantial variability in sentence-level sentiment classification results, underscoring the innate volatility of LLM outputs"" and ""These uncertainties cascade downstream, leading to more significant variations in portfolio construction and return.""",2023,2023-11-26T03:54:03Z,"Keyphrase: ""Uncertain sentiment classification""","""we uncover substantial variability in sentence-level sentiment classification results, underscoring the innate volatility of LLM outputs"" and ""These uncertainties cascade downstream, leading to more significant variations in portfolio construction and return."" Keyphrase: ""Uncertain sentiment classification"""
arXIv2023,Large Language Models in Law: A Survey,Yes.,5,"""In addition, we explore the limitations of legal LLMs, including data, algorithms, and judicial practice.""",2023,2023-11-26T00:48:12Z,"Keyphrase: ""Limitations in legal domain""","""In addition, we explore the limitations of legal LLMs, including data, algorithms, and judicial practice."" Keyphrase: ""Limitations in legal domain"""
arXIv2023,Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains,Yes.,5,"""Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains.""",2023,2023-11-25T08:58:07Z,"Keyphrase: ""Challenges in high-risk domains""","""Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains."" Keyphrase: ""Challenges in high-risk domains"""
arXIv2023,AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering,Yes.,5,"""By conducting an extensive case study, we uncover several drawbacks of GPT-4V, such as limited temporal and dynamic comprehension, and overly general responses.""",2023,2023-11-25T02:46:12Z,"Keyphrase: ""Limited temporal comprehension""","""By conducting an extensive case study, we uncover several drawbacks of GPT-4V, such as limited temporal and dynamic comprehension, and overly general responses."" Keyphrase: ""Limited temporal comprehension"""
arXIv2023,One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space,Yes.,5,"""Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources.""",2023,2023-11-24T18:35:00Z,"Keyphrase: ""High computational complexity""","""Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources."" Keyphrase: ""High computational complexity"""
arXIv2023,Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review,Yes.,4,"""ChatGPT and other Generative Artificial Intelligence (GAI) models tend to inherit and even amplify prevailing societal biases as they are trained on large amounts of existing data."" and ""Our findings show that while there is an awareness of potential biases around large language models (LLMs) and",2023,2023-11-24T10:00:23Z,"Keyphrase: ""Amplification of societal biases""","""ChatGPT and other Generative Artificial Intelligence (GAI) models tend to inherit and even amplify prevailing societal biases as they are trained on large amounts of existing data."" and ""Our findings show that while there is an awareness of potential biases around large language models (LLMs) and Keyphrase: ""Amplification of societal biases"""
arXIv2023,Towards Auditing Large Language Models: Improving Text-based Stereotype Detection,Yes.,4,"""LLMs often generate stereotypical output inherited from historical data, amplifying societal biases and raising ethical concerns.""",2023,2023-11-23T17:47:14Z,"Keyphrase: ""Amplifying societal bias""","""LLMs often generate stereotypical output inherited from historical data, amplifying societal biases and raising ethical concerns."" Keyphrase: ""Amplifying societal bias"""
arXIv2023,Auditing and Mitigating Cultural Bias in LLMs,Yes.,5,"""We audit large language models for cultural bias,"" and ""Our mitigation strategy reduces cultural bias in recent models but not for all countries/territories.""",2023,2023-11-23T16:45:56Z,"Keyphrase: ""Cultural bias mitigation strategy""","""We audit large language models for cultural bias,"" and ""Our mitigation strategy reduces cultural bias in recent models but not for all countries/territories."" Keyphrase: ""Cultural bias mitigation strategy"""
arXIv2023,Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions,Yes.,5,"""However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models' parameters. Recent works turn to retrieving external knowledge to augment CoT reasoning. Despite being promising, these chain-based methods suffer from",2023,2023-11-23T12:52:37Z,"Keyphrase: ""Reliance on outdated knowledge""","""However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models' parameters. Recent works turn to retrieving external knowledge to augment CoT reasoning. Despite being promising, these chain-based methods suffer from Keyphrase: ""Reliance on outdated knowledge"""
arXIv2023,Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach,Yes.,5,"""However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent.""",2023,2023-11-23T10:14:58Z,"Keyphrase: ""Coordination hallucination""","""However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent."" Keyphrase: ""Coordination hallucination"""
arXIv2023,Challenges of Large Language Models for Mental Health Counseling,Yes.,5,"""However, the application of LLMs in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. This paper investigates the major challenges associated with the development of LLMs for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness.""",2023,2023-11-23T08:56:41Z,"Keyphrase: ""Challenges in mental health domain""","""However, the application of LLMs in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. This paper investigates the major challenges associated with the development of LLMs for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness."" Keyphrase: ""Challenges in mental health domain"""
arXIv2023,Surpassing GPT-4 Medical Coding with a Two-Stage Approach,Yes.,5,"""the GPT-4 LLM predicts an excessive number of ICD codes for medical coding tasks, leading to high recall but low precision.""",2023,2023-11-22T23:35:13Z,"Keyphrase: ""Excessive predictions in medical coding""","""the GPT-4 LLM predicts an excessive number of ICD codes for medical coding tasks, leading to high recall but low precision."" Keyphrase: ""Excessive predictions in medical coding"""
arXIv2023,Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering,Yes.,5,"""currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants).""",2023,2023-11-22T18:22:56Z,"Keyphrase: ""Limited context and high computational cost""","""currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants)."" Keyphrase: ""Limited context and high computational cost"""
arXIv2023,Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents,Yes.,4,"""However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming.""",2023,2023-11-22T13:15:42Z,"Keyphrase: ""Lack of specialization in dynamic environments""","""However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming."" Keyphrase: ""Lack of specialization in dynamic environments"""
arXIv2023,Applying Large Language Models to Power Systems: Potential Security Threats,Yes.,5,"""However, this action may also incur potential security threats, which have not been fully recognized so far.""",2023,2023-11-22T12:55:02Z,"Keyphrase: ""Security threats not fully recognized""","""However, this action may also incur potential security threats, which have not been fully recognized so far."" Keyphrase: ""Security threats not fully recognized"""
arXIv2023,Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting,Yes.,5,"""Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process.""",2023,2023-11-22T11:08:38Z,"Keyphrase: ""Limited factual grounding""","""Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process."" Keyphrase: ""Limited factual grounding"""
arXIv2023,Intention and Context Elicitation with Large Language Models in the Legal Aid Intake Process,Yes.,4,"""a key challenge with current LLMs is their tendency to overconfidently deliver an immediate 'best guess' to a client's question based on the output distribution learned over the training data. This approach often overlooks the client's actual intentions or the specifics of their legal situation.""",2023,2023-11-22T10:04:29Z,"Keyphrase: ""Overconfident responses""","""a key challenge with current LLMs is their tendency to overconfidently deliver an immediate 'best guess' to a client's question based on the output distribution learned over the training data. This approach often overlooks the client's actual intentions or the specifics of their legal situation."" Keyphrase: ""Overconfident responses"""
arXIv2023,Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus,Yes.,5,"""LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications.""",2023,2023-11-22T08:39:17Z,"Keyphrase: ""Untruthful nonsensical output""","""LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications."" Keyphrase: ""Untruthful nonsensical output"""
arXIv2023,HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data,Yes.,5,"""the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored.""",2023,2023-11-22T04:52:58Z,"Keyphrase: ""Hallucinatory output""","""the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored."" Keyphrase: ""Hallucinatory output"""
arXIv2023,Conditions for Length Generalization in Learning Reasoning Skills,Yes.,5,"""However, numerous evaluations of the reasoning capabilities of LLMs have also showed some limitations. An outstanding limitation is length generalization, meaning that when trained on reasoning problems of smaller lengths or sizes, the resulting models struggle with problems of larger sizes or lengths.""",2023,2023-11-22T03:36:18Z,"Keyphrase: ""Limited reasoning capability""","""However, numerous evaluations of the reasoning capabilities of LLMs have also showed some limitations. An outstanding limitation is length generalization, meaning that when trained on reasoning problems of smaller lengths or sizes, the resulting models struggle with problems of larger sizes or lengths."" Keyphrase: ""Limited reasoning capability"""
arXIv2023,Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper,Yes.,4,"""While LLMs possess remarkable capabilities, their extensive parameter requirements and associated computational demands hinder their practicality and scalability for real-world applications."" and ""recognizes significant challenges and open issues that must be addressed to fully harness the powerful abilities of LLMs.""",2023,2023-11-22T03:28:34Z,"Keyphrase: ""Computational demands and scalability""","""While LLMs possess remarkable capabilities, their extensive parameter requirements and associated computational demands hinder their practicality and scalability for real-world applications."" and ""recognizes significant challenges and open issues that must be addressed to fully harness the powerful abilities of LLMs."" Keyphrase: ""Computational demands and scalability"""
arXIv2023,Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications,Yes.,4,"""Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited.""",2023,2023-11-22T01:51:50Z,"Keyphrase: ""Limited logical reasoning competency""","""Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited."" Keyphrase: ""Limited logical reasoning competency"""
arXIv2023,A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift,Yes.,4,"""However, there is little work measuring how robust these reward models are to distribution shifts."" and ""We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts.""",2023,2023-11-21T18:41:26Z,"Keyphrase: ""Reward model distribution shift""","""However, there is little work measuring how robust these reward models are to distribution shifts."" and ""We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts."" Keyphrase: ""Reward model distribution shift"""
arXIv2023,Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey,Yes.,5,"""current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios."" and ""We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models.""",2023,2023-11-21T04:59:17Z,"Keyphrase: ""Ineffective long-context processing""","""current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios."" and ""We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models."" Keyphrase: ""Ineffective long-context processing"""
arXIv2023,"Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",Yes.,4,"""A notable challenge, model hallucination-where the model yields inaccurate or misinterpreted data-is addressed alongside other model-specific hurdles.""",2023,2023-11-21T02:01:01Z,"Keyphrase: ""Hallucination of inaccurate data""","""A notable challenge, model hallucination-where the model yields inaccurate or misinterpreted data-is addressed alongside other model-specific hurdles."" Keyphrase: ""Hallucination of inaccurate data"""
arXIv2023,Evil Geniuses: Delving into the Safety of LLM-based Agents,Yes.,4,"""Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research.""",2023,2023-11-20T15:50:09Z,"Keyphrase: ""Prone to harmful behavior""","""Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research."" Keyphrase: ""Prone to harmful behavior"""
arXIv2023,System 2 Attention (is something you might need too),Yes.,5,"""Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information from the context into its latent representations, which adversely affects next token generations.""",2023,2023-11-20T15:04:50Z,"Keyphrase: ""Incorporating irrelevant information""","""Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information from the context into its latent representations, which adversely affects next token generations."" Keyphrase: ""Incorporating irrelevant information"""
arXIv2023,Towards Robust Text Retrieval with Progressive Learning,Yes.,4,"""Retrieval augmentation has become an effective solution to empower large language models (LLMs) with external and verified knowledge sources from the database, which overcomes the limitations and hallucinations of LLMs in handling up-to-date and domain-specific information.""",2023,2023-11-20T11:44:01Z,"Keyphrase: ""Limited access to domain-specific information""","""Retrieval augmentation has become an effective solution to empower large language models (LLMs) with external and verified knowledge sources from the database, which overcomes the limitations and hallucinations of LLMs in handling up-to-date and domain-specific information."" Keyphrase: ""Limited access to domain-specific information"""
arXIv2023,Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information,Yes.,5,"""these models are susceptible to adversarial prompt attacks,"" and ""this vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs.""",2023,2023-11-20T03:17:21Z,"Keyphrase: ""Adversarial prompt vulnerability""","""these models are susceptible to adversarial prompt attacks,"" and ""this vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs."" Keyphrase: ""Adversarial prompt vulnerability"""
arXIv2023,A Security Risk Taxonomy for Large Language Models,Yes.,5,"""The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial."" and ""Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs.""",2023,2023-11-19T20:22:05Z,"Keyphrase: ""Security vulnerabilities""","""The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial."" and ""Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs."" Keyphrase: ""Security vulnerabilities"""
arXIv2023,Rethinking Large Language Models in Mental Health Applications,Yes.,5,"""It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability.""",2023,2023-11-19T08:40:01Z,"Keyphrase: ""Hallucinatory output""","""It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability."" Keyphrase: ""Hallucinatory output"""
arXIv2023,Visual AI and Linguistic Intelligence Through Steerability and Composability,Yes.,5,"""The research presents a series of 14 creatively and constructively diverse tasks, ranging from AI Lego Designing to AI Satellite Image Analysis, designed to test the limits of current LLMs in contexts that previously proved difficult without extensive memory and contextual understanding."" and ""Tasks such as 'AI Genetic Programmer'",2023,2023-11-18T22:01:33Z,"Keyphrase: ""Limited contextual understanding""","""The research presents a series of 14 creatively and constructively diverse tasks, ranging from AI Lego Designing to AI Satellite Image Analysis, designed to test the limits of current LLMs in contexts that previously proved difficult without extensive memory and contextual understanding."" and ""Tasks such as 'AI Genetic Programmer' Keyphrase: ""Limited contextual understanding"""
arXIv2023,A Principled Framework for Knowledge-enhanced Large Language Model,Yes.,5,"""Large Language Models (LLMs) are versatile, yet they often falter in tasks requiring deep and reliable reasoning due to issues like hallucinations, limiting their applicability in critical scenarios.""",2023,2023-11-18T18:10:02Z,"Keyphrase: ""Limited deep reasoning""","""Large Language Models (LLMs) are versatile, yet they often falter in tasks requiring deep and reliable reasoning due to issues like hallucinations, limiting their applicability in critical scenarios."" Keyphrase: ""Limited deep reasoning"""
arXIv2023,(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs,Yes.,5,"""LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices,"" and ""regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.""",2023,2023-11-18T17:11:12Z,"Keyphrase: ""Silent updates and performance regression""","""LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices,"" and ""regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs."" Keyphrase: ""Silent updates and performance regression"""
arXIv2023,Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers,Yes.,5,"""One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual. Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context.""",2023,2023-11-18T03:55:59Z,"Keyphrase: ""Factual hallucination due to biased training data""","""One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual. Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context."" Keyphrase: ""Factual hallucination due to biased training data"""
arXIv2023,DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback,Yes.,5,"""First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction",2023,2023-11-16T18:37:29Z,"Keyphrase: ""Limited incorporation of extra feedback""","""First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction Keyphrase: ""Limited incorporation of extra feedback"""
arXIv2023,Hijacking Large Language Models via Adversarial In-Context Learning,Yes.,5,"""ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL.""",2023,2023-11-16T15:01:48Z,"Keyphrase: ""Instability in adversarial attacks""","""ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL."" Keyphrase: ""Instability in adversarial attacks"""
arXIv2023,"Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models",Yes.,5,"""despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints."" and ""outlines the existing challenges and potential future developments for LLMs in graph understanding and reasoning tasks.""",2023,2023-11-16T12:45:41Z,"Keyphrase: ""Limitation in graph understanding""","""despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints."" and ""outlines the existing challenges and potential future developments for LLMs in graph understanding and reasoning tasks."" Keyphrase: ""Limitation in graph understanding"""
arXIv2023,ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks,Yes.,5,"""a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries"" and ""GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73% of the tasks, leaving a huge",2023,2023-11-16T12:03:21Z,"Keyphrase: ""Limited practical applicability""","""a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries"" and ""GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73% of the tasks, leaving a huge Keyphrase: ""Limited practical applicability"""
arXIv2023,FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models,Yes.,5,"""We have evaluated various LLMs using the FollowEval benchmark and found that their performance significantly lags behind that of humans. This highlights the considerable room for improvement in the instruction-following ability of these models.""",2023,2023-11-16T11:53:31Z,"Keyphrase: ""Inferior performance compared to humans""","""We have evaluated various LLMs using the FollowEval benchmark and found that their performance significantly lags behind that of humans. This highlights the considerable room for improvement in the instruction-following ability of these models."" Keyphrase: ""Inferior performance compared to humans"""
arXIv2023,Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking,Yes.,5,"""we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning.""",2023,2023-11-16T11:52:22Z,"Keyphrase: ""Cognitive overload and reasoning challenges""","""we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning."" Keyphrase: ""Cognitive overload and reasoning challenges"""
arXIv2023,The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text,Yes.,5,"""Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness.""",2023,2023-11-16T11:31:50Z,"Keyphrase: ""Decreased diversity in model output""","""Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness."" Keyphrase: ""Decreased diversity in model output"""
arXIv2023,DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data,Yes.,5,"""We found that, although the current best-performing system (i.e., GPT-4), can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts.""",2023,2023-11-16T11:30:53Z,"Keyphrase: ""Limited performance on complex, context-dependent tasks""","""We found that, although the current best-performing system (i.e., GPT-4), can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts."" Keyphrase: ""Limited performance on complex, context-dependent tasks"""
arXIv2023,Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs,Yes.,5,"""these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs.""",2023,2023-11-16T11:26:21Z,"Keyphrase: ""Lack of causal reliability""","""these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs."" Keyphrase: ""Lack of causal reliability"""
arXIv2023,Investigating Data Contamination in Modern Benchmarks for Large Language Models,Yes.,5,"""Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks."" and ""We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.""",2023,2023-11-16T11:03:04Z,"Keyphrase: ""Inflated benchmark scores""","""Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks."" and ""We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field."" Keyphrase: ""Inflated benchmark scores"""
arXIv2023,Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models,Yes.,4,"""We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning",2023,2023-11-16T10:36:08Z,"Keyphrase: ""Limited multistep reasoning""","""We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning Keyphrase: ""Limited multistep reasoning"""
arXIv2023,Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge,Yes.,5,"""we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge,"" and ""LLMs' uncertainty expression does not always stay consistent with the perceived confidence of their textual outputs.""",2023,2023-11-16T10:02:40Z,"Keyphrase: ""Lack of uncertainty expression""","""we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge,"" and ""LLMs' uncertainty expression does not always stay consistent with the perceived confidence of their textual outputs."" Keyphrase: ""Lack of uncertainty expression"""
arXIv2023,Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks,Yes.,5,"""Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects.""",2023,2023-11-16T10:02:24Z,"Keyphrase: ""Biases in demographic-infused prompts""","""Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects."" Keyphrase: ""Biases in demographic-infused prompts"""
arXIv2023,Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?,Yes.,5,"""Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning."" and ""We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts.""",2023,2023-11-16T09:27:36Z,"Keyphrase: ""Unfaithful reasoning""","""Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning."" and ""We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts."" Keyphrase: ""Unfaithful reasoning"""
arXIv2023,BLT: Can Large Language Models Handle Basic Legal Text?,Yes.,5,"""We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling."" and ""LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice.""",2023,2023-11-16T09:09:22Z,"Keyphrase: ""Poor legal text handling""","""We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling."" and ""LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice."" Keyphrase: ""Poor legal text handling"""
arXIv2023,R-Tuning: Teaching Large Language Models to Refuse Unknown Questions,Yes.,5,"""A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination."" and ""previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not.""",2023,2023-11-16T08:45:44Z,"Keyphrase: ""Hallucination of nonexistent facts""","""A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination."" and ""previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not."" Keyphrase: ""Hallucination of nonexistent facts"""
arXIv2023,Structured Chemistry Reasoning with Large Language Models,Yes.,5,"""Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry."" and ""even advanced LLMs, like GPT-4, can fail easily in different ways."" and ""the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that",2023,2023-11-16T08:20:36Z,"Keyphrase: ""Limited scientific reasoning in specialized domains""","""Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry."" and ""even advanced LLMs, like GPT-4, can fail easily in different ways."" and ""the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that Keyphrase: ""Limited scientific reasoning in specialized domains"""
arXIv2023,On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models,Yes.,5,"""Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially."" and ""Our",2023,2023-11-16T07:48:45Z,"Keyphrase: ""Vulnerability to adversarial manipulation""","""Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially."" and ""Our Keyphrase: ""Vulnerability to adversarial manipulation"""
arXIv2023,Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework,Yes.,4,"""modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked"" and ""These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts.""",2023,2023-11-16T07:31:18Z,"Keyphrase: ""Vulnerability to malicious attacks""","""modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked"" and ""These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts."" Keyphrase: ""Vulnerability to malicious attacks"""
arXIv2023,CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model,Yes.,4,"""Large language models (LLMs) executing tasks through instruction-based prompts often face challenges stemming from distribution differences between user instructions and training instructions. This leads to distractions and biases, especially when dealing with inconsistent dynamic labels.""",2023,2023-11-16T07:16:55Z,"Keyphrase: ""Bias from inconsistent user instructions""","""Large language models (LLMs) executing tasks through instruction-based prompts often face challenges stemming from distribution differences between user instructions and training instructions. This leads to distractions and biases, especially when dealing with inconsistent dynamic labels."" Keyphrase: ""Bias from inconsistent user instructions"""
arXIv2023,Simulating Opinion Dynamics with Networks of LLM-based Agents,Yes.,4,"""This bias limits their utility for understanding resistance to consensus views on issues like climate change."" and ""These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward",2023,2023-11-16T07:01:48Z,"Keyphrase: ""Limited understanding of complex issues""","""This bias limits their utility for understanding resistance to consensus views on issues like climate change."" and ""These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward Keyphrase: ""Limited understanding of complex issues"""
arXIv2023,Self-Contradictory Reasoning Evaluation and Detection,Yes.,5,"""We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense"" and ""Our results indicate that the current LLMs lack robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond accuracy-based metrics.""",2023,2023-11-16T06:22:17Z,"Keyphrase: ""Lack of robust reasoning""","""We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense"" and ""Our results indicate that the current LLMs lack robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond accuracy-based metrics."" Keyphrase: ""Lack of robust reasoning"""
arXIv2023,LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks,Yes.,5,"""Assessing these models on long sequences is crucial since prior work in the general domain has demonstrated performance degradation of LLMs on longer texts."" and ""Preliminary experiments reveal that both medical LLMs (e.g., BioGPT) and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark.""",2023,2023-11-16T04:57:49Z,"Keyphrase: ""Performance degradation with long sequences""","""Assessing these models on long sequences is crucial since prior work in the general domain has demonstrated performance degradation of LLMs on longer texts."" and ""Preliminary experiments reveal that both medical LLMs (e.g., BioGPT) and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark."" Keyphrase: ""Performance degradation with long sequences"""
arXIv2023,MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning,Yes.,4,"""Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs on correctly interpreting charts, even for the most recent GPT-4V model.""",2023,2023-11-15T23:36:42Z,"Keyphrase: ""Difficulty interpreting charts""","""Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs on correctly interpreting charts, even for the most recent GPT-4V model."" Keyphrase: ""Difficulty interpreting charts"""
arXIv2023,How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities,Yes.,5,"""However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly."" and ""scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations.""",2023,2023-11-15T23:33:07Z,"Keyphrase: ""Limited trustworthiness""","""However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly."" and ""scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations."" Keyphrase: ""Limited trustworthiness"""
arXIv2023,Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment,Yes.,5,"""the vulnerability of their safety alignment has not been extensively studied"" and ""Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts"" and ""Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications.""",2023,2023-11-15T23:07:40Z,"Keyphrase: ""Vulnerability to attacks and resource-intensive implementation""","""the vulnerability of their safety alignment has not been extensively studied"" and ""Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts"" and ""Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications."" Keyphrase: ""Vulnerability to attacks and resource-intensive implementation"""
arXIv2023,When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour,Yes.,5,"""the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability.""",2023,2023-11-15T22:18:33Z,"Keyphrase: ""Bias amplification through human feedback""","""the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability."" Keyphrase: ""Bias amplification through human feedback"""
arXIv2023,Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization,Yes.,5,"""Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges",2023,2023-11-15T19:49:24Z,"Keyphrase: ""Challenges in abstractive summarization""","""Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges Keyphrase: ""Challenges in abstractive summarization"""
arXIv2023,Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models,Yes.,4,"""Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language (e.g., chemical molecular formula).""",2023,2023-11-15T18:59:56Z,"Keyphrase: ""Limited world knowledge comprehension""","""Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language (e.g., chemical molecular formula)."" Keyphrase: ""Limited world knowledge comprehension"""
arXIv2023,Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering,Yes.,5,"""they are struggling to seek correct information in long contexts. The 'lost in the middle' problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle.""",2023,2023-11-15T18:42:44Z,"Keyphrase: ""Difficulty with contextual accuracy""","""they are struggling to seek correct information in long contexts. The 'lost in the middle' problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle."" Keyphrase: ""Difficulty with contextual accuracy"""
arXIv2023,Towards Verifiable Text Generation with Symbolic References,Yes.,5,"""However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult.""",2023,2023-11-15T18:28:29Z,"Keyphrase: ""Need for manual verification""","""However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult."" Keyphrase: ""Need for manual verification"""
arXIv2023,Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization,Yes.,5,"""our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance",2023,2023-11-15T18:25:26Z,"Keyphrase: ""Factual errors and evaluation misalignment""","""our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance Keyphrase: ""Factual errors and evaluation misalignment"""
arXIv2023,ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models,Yes.,5,"""While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context.""",2023,2023-11-15T18:23:17Z,"Keyphrase: ""Unreliable self-contradictions""","""While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context."" Keyphrase: ""Unreliable self-contradictions"""
arXIv2023,AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph,Yes.,5,"""Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings.""",2023,2023-11-15T18:11:23Z,"Keyphrase: ""Struggles with abstract knowledge comprehension""","""Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings."" Keyphrase: ""Struggles with abstract knowledge comprehension"""
arXIv2023,Temporal Knowledge Question Answering via Abstract Reasoning Induction,Yes.,5,"""In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic.""",2023,2023-11-15T17:46:39Z,"Keyphrase: ""Limited temporal knowledge reasoning""","""In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic."" Keyphrase: ""Limited temporal knowledge reasoning"""
arXIv2023,Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts,Yes.,5,"""This finding indicates potential exploitable security risks in MLLMs"" and ""Results show that appropriately designed system prompts can significantly reduce jailbreak success rates.""",2023,2023-11-15T17:17:39Z,"Keyphrase: ""Security vulnerabilities""","""This finding indicates potential exploitable security risks in MLLMs"" and ""Results show that appropriately designed system prompts can significantly reduce jailbreak success rates."" Keyphrase: ""Security vulnerabilities"""
arXIv2023,Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content.""",2023,2023-11-15T17:04:56Z,"Keyphrase: ""Inaccurate hallucinated content""","""Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content."" Keyphrase: ""Inaccurate hallucinated content"""
arXIv2023,Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?,Yes.,5,"""This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications.""",2023,2023-11-15T16:56:49Z,"Keyphrase: ""Limited inference capabilities""","""This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications."" Keyphrase: ""Limited inference capabilities"""
arXIv2023,Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization,Yes.,5,"""Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks."" and ""we point out a pivotal factor contributing to the success of jailbreaks",2023,2023-11-15T16:42:29Z,"Keyphrase: ""Safety risks and vulnerabilities""","""Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks."" and ""we point out a pivotal factor contributing to the success of jailbreaks Keyphrase: ""Safety risks and vulnerabilities"""
arXIv2023,Social Bias Probing: Fairness Benchmarking for Language Models,Yes.,4,"""Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms."" and ""When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged.""",2023,2023-11-15T16:35:59Z,"Keyphrase: ""Encoded social biases""","""Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms."" and ""When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged."" Keyphrase: ""Encoded social biases"""
arXIv2023,How Well Do Large Language Models Truly Ground?,Yes.,5,"""Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge.""",2023,2023-11-15T16:11:27Z,"Keyphrase: ""Hallucination and lack of control""","""Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge."" Keyphrase: ""Hallucination and lack of control"""
arXIv2023,GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models,Yes.,5,"""Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models."" and ""These identified limitations underline the importance of using benchmarks like GRASP to monitor the progress of future models in developing these competencies.""",2023,2023-11-15T15:38:28Z,"Keyphrase: ""Lack of intuitive physics capability""","""Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models."" and ""These identified limitations underline the importance of using benchmarks like GRASP to monitor the progress of future models in developing these competencies."" Keyphrase: ""Lack of intuitive physics capability"""
arXIv2023,Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output,Yes.,5,"""The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs."" and ""Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify",2023,2023-11-15T14:41:57Z,"Keyphrase: ""Verification of factual accuracy""","""The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs."" and ""Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify Keyphrase: ""Verification of factual accuracy"""
arXIv2023,When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks,Yes.,5,"""ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications,"" and ""three primary reasons",2023,2023-11-15T14:26:30Z,"Keyphrase: ""Struggles with handling complex tasks""","""ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications,"" and ""three primary reasons Keyphrase: ""Struggles with handling complex tasks"""
arXIv2023,Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation,Yes.,5,"""large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user."" and ""state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities.""",2023,2023-11-15T11:27:44Z,"Keyphrase: ""Low confidence signals""","""large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user."" and ""state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities."" Keyphrase: ""Low confidence signals"""
arXIv2023,Disinformation Capabilities of Large Language Models,Yes.,4,"""Automated disinformation generation is often listed as an important risk associated with large language models (LLMs).""",2023,2023-11-15T10:25:30Z,"Keyphrase: ""Risk of automated disinformation""","""Automated disinformation generation is often listed as an important risk associated with large language models (LLMs)."" Keyphrase: ""Risk of automated disinformation"""
arXIv2023,MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy,Yes.,5,"""degenerate modes can even occur in the absence of any model error, due to contamination of the training data"" and ""the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue.""",2023,2023-11-15T09:38:53Z,"Keyphrase: ""Degenerate modes""","""degenerate modes can even occur in the absence of any model error, due to contamination of the training data"" and ""the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue."" Keyphrase: ""Degenerate modes"""
arXIv2023,Thread of Thought Unraveling Chaotic Contexts,Yes.,5,"""they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context.""",2023,2023-11-15T06:54:44Z,"Keyphrase: ""Difficulty in chaotic contexts""","""they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context."" Keyphrase: ""Difficulty in chaotic contexts"""
arXIv2023,Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models,Yes.,4,"""However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills.""",2023,2023-11-15T06:48:50Z,"Keyphrase: ""Unreliable output and poor reasoning""","""However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills."" Keyphrase: ""Unreliable output and poor reasoning"""
arXIv2023,Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures,Yes.,4,"""we undertake an exploration of decision-making processes and inherent biases within LLMs"" and ""We discuss the consequences of our findings for human-AI alignment and bias mitigation.""",2023,2023-11-15T00:02:25Z,"Keyphrase: ""Inherent bias and alignment issues""","""we undertake an exploration of decision-making processes and inherent biases within LLMs"" and ""We discuss the consequences of our findings for human-AI alignment and bias mitigation."" Keyphrase: ""Inherent bias and alignment issues"""
arXIv2023,Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment,Yes.,5,"""A systematic study of ten LLMs on seven classification tasks reveals that models flip their answers on average 46% of the time and that all models see a deterioration of accuracy between their first and final prediction, with an average drop of 17% (the FlipFlop effect).""",2023,2023-11-14T23:40:22Z,"Keyphrase: ""Answer flipping and accuracy deterioration""","""A systematic study of ten LLMs on seven classification tasks reveals that models flip their answers on average 46% of the time and that all models see a deterioration of accuracy between their first and final prediction, with an average drop of 17% (the FlipFlop effect)."" Keyphrase: ""Answer flipping and accuracy deterioration"""
arXIv2023,CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,Yes.,4,"""existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations."" and ""most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks"" and ""most benchmarks also fail to",2023,2023-11-14T23:18:52Z,"Keyphrase: ""Limited benchmark diversity""","""existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations."" and ""most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks"" and ""most benchmarks also fail to Keyphrase: ""Limited benchmark diversity"""
arXIv2023,"LLMs cannot find reasoning errors, but can correct them!",Yes.,5,"""recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall"" and ""demonstrate that LLMs generally struggle with finding logical mistakes.""",2023,2023-11-14T20:12:38Z,"Keyphrase: ""Struggles with logical reasoning""","""recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall"" and ""demonstrate that LLMs generally struggle with finding logical mistakes."" Keyphrase: ""Struggles with logical reasoning"""
arXIv2023,Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models,Yes.,4,"""while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems"" and ""We discuss how future work can include LLM fairness evaluations.""",2023,2023-11-14T19:02:03Z,"Keyphrase: ""Limited fairness evaluation""","""while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems"" and ""We discuss how future work can include LLM fairness evaluations."" Keyphrase: ""Limited fairness evaluation"""
arXIv2023,Fine-tuning Language Models for Factuality,Yes.,5,"""Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions.""",2023,2023-11-14T18:59:15Z,"Keyphrase: ""Factually inaccurate hallucination""","""Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions."" Keyphrase: ""Factually inaccurate hallucination"""
arXIv2023,Are Large Language Models Temporally Grounded?,Yes.,5,"""Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions.",2023,2023-11-14T18:57:15Z,"Keyphrase: ""Struggles with self-consistency""","""Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions. Keyphrase: ""Struggles with self-consistency"""
arXIv2023,SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models,Yes.,5,"""However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content."" and ""We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses.""",2023,2023-11-14T18:33:43Z,"Keyphrase: ""Safety weaknesses in guidance""","""However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content."" and ""We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses."" Keyphrase: ""Safety weaknesses in guidance"""
arXIv2023,GPT-4V(ision) Unsuitable for Clinical Care and Education: A Clinician-Evaluated Assessment,Yes.,5,"""Although GPT-4V is able to identify and explain medical images, its diagnostic accuracy and clinical decision-making abilities are poor, posing risks to patient safety.""",2023,2023-11-14T17:06:09Z,"Keyphrase: ""Poor clinical decision-making ability""","""Although GPT-4V is able to identify and explain medical images, its diagnostic accuracy and clinical decision-making abilities are poor, posing risks to patient safety."" Keyphrase: ""Poor clinical decision-making ability"""
arXIv2023,Extrinsically-Focused Evaluation of Omissions in Medical Summarization,Yes.,4,"""Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs."" and ""especially given the potential for LLMs to omit important information in the resulting summary.""",2023,2023-11-14T16:46:15Z,"Keyphrase: ""Omission of important information""","""Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs."" and ""especially given the potential for LLMs to omit important information in the resulting summary."" Keyphrase: ""Omission of important information"""
arXIv2023,A Survey of Confidence Estimation and Calibration in Large Language Models,Yes.,4,"""Despite their impressive performance, they can be unreliable due to factual errors in their generations."" and ""we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration.""",2023,2023-11-14T16:43:29Z,"Keyphrase: ""Unreliable factual error generation""","""Despite their impressive performance, they can be unreliable due to factual errors in their generations."" and ""we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration."" Keyphrase: ""Unreliable factual error generation"""
arXIv2023,How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions,Yes.,5,"""Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points."" and ""simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLM",2023,2023-11-14T16:30:36Z,"Keyphrase: ""Limited syntactic knowledge grasp""","""Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points."" and ""simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLM Keyphrase: ""Limited syntactic knowledge grasp"""
arXIv2023,A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily,Yes.,5,"""Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them."" and ""Our study also reveals the inadequacy of current defense methods in safeguarding LLMs.""",2023,2023-11-14T16:02:16Z,"Keyphrase: ""Vulnerability to attacks""","""Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them."" and ""Our study also reveals the inadequacy of current defense methods in safeguarding LLMs."" Keyphrase: ""Vulnerability to attacks"""
arXIv2023,RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge,Yes.,5,"""Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue.""",2023,2023-11-14T13:24:19Z,"Keyphrase: ""Susceptibility to external interference""","""Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue."" Keyphrase: ""Susceptibility to external interference"""
arXIv2023,Insights into Classifying and Mitigating LLMs' Hallucinations,Yes.,5,"""However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as 'Hallucinations'. They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation.""",2023,2023-11-14T12:30:28Z,"Keyphrase: ""False information propagation""","""However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as 'Hallucinations'. They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation."" Keyphrase: ""False information propagation"""
arXIv2023,Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models,Yes.,5,"""the dynamic nature of knowledge presents challenges for language models that are trained on static data, leading to outdated encoded information,"" and ""we uncover that existing continual learning baselines have difficulty in updating and forgetting outdated knowledge,"" and ""the models fail to learn updated knowledge due to the small weight gradient,"" and ""the models struggle mostly on providing numerical or temporal answers to questions asking for updated knowledge",2023,2023-11-14T12:12:02Z,"Keyphrase: ""Difficulty in updating outdated knowledge""","""the dynamic nature of knowledge presents challenges for language models that are trained on static data, leading to outdated encoded information,"" and ""we uncover that existing continual learning baselines have difficulty in updating and forgetting outdated knowledge,"" and ""the models fail to learn updated knowledge due to the small weight gradient,"" and ""the models struggle mostly on providing numerical or temporal answers to questions asking for updated knowledge Keyphrase: ""Difficulty in updating outdated knowledge"""
arXIv2023,How good are Large Language Models on African Languages?,Yes.,5,"""Our results suggest that all LLMs produce below-par performance on African languages, and there is a large gap in performance compared to high-resource languages like English most tasks.""",2023,2023-11-14T08:10:14Z,"Keyphrase: ""Performance gap in African languages""","""Our results suggest that all LLMs produce below-par performance on African languages, and there is a large gap in performance compared to high-resource languages like English most tasks."" Keyphrase: ""Performance gap in African languages"""
arXIv2023,A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,Yes.,5,"""Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems."" and ""Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification",2023,2023-11-14T07:13:10Z,"Keyphrase: ""Struggle with logical reasoning""","""Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems."" and ""Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification Keyphrase: ""Struggle with logical reasoning"""
arXIv2023,Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey,Yes.,5,"""The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy.""",2023,2023-11-14T05:21:57Z,"Keyphrase: ""Hallucination stemming from knowledge gap""","""The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy."" Keyphrase: ""Hallucination stemming from knowledge gap"""
arXIv2023,Fair Abstractive Summarization of Diverse Perspectives,Yes.,4,"""current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization"" and ""Experiments show that both the model-generated and the human-written reference summaries suffer from low fairness.""",2023,2023-11-14T03:38:55Z,"Keyphrase: ""Fairness issues in abstractive summarization""","""current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization"" and ""Experiments show that both the model-generated and the human-written reference summaries suffer from low fairness."" Keyphrase: ""Fairness issues in abstractive summarization"""
arXIv2023,Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems,Yes.,5,"""Large Language Models have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence.""",2023,2023-11-13T21:20:17Z,"Keyphrase: ""Trivial mistakes in commonsense competence""","""Large Language Models have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence."" Keyphrase: ""Trivial mistakes in commonsense competence"""
arXIv2023,MART: Improving LLM Safety with Multi-round Automatic Red-Teaming,Yes.,4,"""Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.""",2023,2023-11-13T19:13:29Z,"Keyphrase: ""Limited ability to mitigate unsafe behavior""","""Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses."" Keyphrase: ""Limited ability to mitigate unsafe behavior"""
arXIv2023,GPT-4V(ision) as A Social Media Analysis Engine,Yes.,5,"""Despite the overall impressive capacity of GPT-4V in the social media domain, there remain notable challenges. GPT-4V struggles with tasks involving multilingual social multimedia comprehension and has difficulties in generalizing to the latest trends in social media. Additionally, it exhibits a tendency to generate erroneous information in the context",2023,2023-11-13T18:36:50Z,"Keyphrase: ""Struggles with multilingual social media comprehension""","""Despite the overall impressive capacity of GPT-4V in the social media domain, there remain notable challenges. GPT-4V struggles with tasks involving multilingual social multimedia comprehension and has difficulties in generalizing to the latest trends in social media. Additionally, it exhibits a tendency to generate erroneous information in the context Keyphrase: ""Struggles with multilingual social media comprehension"""
arXIv2023,It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning,Yes.,5,"""We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy.""",2023,2023-11-13T18:18:22Z,"Keyphrase: ""Underperformance and lack of self-consistency""","""We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy."" Keyphrase: ""Underperformance and lack of self-consistency"""
arXIv2023,A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models,Yes.,5,"""While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations. Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response.""",2023,2023-11-13T17:28:03Z,"Keyphrase: ""Limited understanding of complex questions""","""While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations. Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response."" Keyphrase: ""Limited understanding of complex questions"""
arXIv2023,Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse,Yes.,5,"""Recent studies have highlighted a phenomenon in large language models (LLMs) known as 'the reversal curse,' in which the order of knowledge entities in the training data biases the models' comprehension."" and ""We hope that more attention can be focused on exploring and addressing these inherent weaknesses of the current LLMs, in order to achieve a higher level of intelligence.""",2023,2023-11-13T17:01:12Z,"Keyphrase: ""Reversal of curse""","""Recent studies have highlighted a phenomenon in large language models (LLMs) known as 'the reversal curse,' in which the order of knowledge entities in the training data biases the models' comprehension."" and ""We hope that more attention can be focused on exploring and addressing these inherent weaknesses of the current LLMs, in order to achieve a higher level of intelligence."" Keyphrase: ""Reversal of curse"""
arXIv2023,On Measuring Faithfulness or Self-consistency of Natural Language Explanations,Yes.,5,"""But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning.""",2023,2023-11-13T16:53:51Z,"Keyphrase: ""Unfaithful explanations""","""But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning."" Keyphrase: ""Unfaithful explanations"""
arXIv2023,Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue,Yes.,5,"""However, LLMs still lack a crucial ability",2023,2023-11-13T16:19:42Z,"Keyphrase: ""Lack of crucial ability""","""However, LLMs still lack a crucial ability Keyphrase: ""Lack of crucial ability"""
arXIv2023,AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation,Yes.,4,"""current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences.""",2023,2023-11-13T15:25:42Z,"Keyphrase: ""Hallucination challenges""","""current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences."" Keyphrase: ""Hallucination challenges"""
arXIv2023,Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study,Yes.,5,"""Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data."" and ""Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent,",2023,2023-11-13T15:11:26Z,"Keyphrase: ""Limited reasoning and planning capabilities""","""Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data."" and ""Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent, Keyphrase: ""Limited reasoning and planning capabilities"""
arXIv2023,LM-Polygraph: Uncertainty Estimation for Language Models,Yes.,5,"""However, a significant challenge arises as these models often 'hallucinate', i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements.""",2023,2023-11-13T15:08:59Z,"Keyphrase: ""Fabricating facts""","""However, a significant challenge arises as these models often 'hallucinate', i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements."" Keyphrase: ""Fabricating facts"""
arXIv2023,Do large language models and humans have similar behaviors in causal inference with script knowledge?,Yes.,4,"""Our experiments show that 1) only recent LLMs, like GPT-3 or Vicuna, correlate with human behavior in the $\neg A \rightarrow B$ condition. 2) Despite this correlation, all models still fail to predict that $nil \rightarrow B$ is less",2023,2023-11-13T13:05:15Z,"Keyphrase: ""Limited predictive accuracy""","""Our experiments show that 1) only recent LLMs, like GPT-3 or Vicuna, correlate with human behavior in the $\neg A \rightarrow B$ condition. 2) Despite this correlation, all models still fail to predict that $nil \rightarrow B$ is less Keyphrase: ""Limited predictive accuracy"""
arXIv2023,In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search,Yes.,5,"""Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail."" and ""LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach,"" and ""find that model performances drop by as high as 5% in the long-tail distribution compared to head distribution.""",2023,2023-11-13T10:56:59Z,"Keyphrase: ""Performance drop in long-tail data""","""Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail."" and ""LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach,"" and ""find that model performances drop by as high as 5% in the long-tail distribution compared to head distribution."" Keyphrase: ""Performance drop in long-tail data"""
arXIv2023,Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models,Yes.,5,"""on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all",2023,2023-11-13T09:32:12Z,"Keyphrase: ""Factual inconsistencies""","""on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all Keyphrase: ""Factual inconsistencies"""
arXIv2023,Towards the Law of Capacity Gap in Distilling Language Models,Yes.,4,"""it is still a pain distilling LMs when a large capacity gap is exhibited between the teacher and the student LMs,"" and ""the curse of capacity gap can be only partly yet not fully lifted as indicated in previous studies.""",2023,2023-11-13T03:36:18Z,"Keyphrase: ""Capacity gap in distillation""","""it is still a pain distilling LMs when a large capacity gap is exhibited between the teacher and the student LMs,"" and ""the curse of capacity gap can be only partly yet not fully lifted as indicated in previous studies."" Keyphrase: ""Capacity gap in distillation"""
arXIv2023,ExpNote: Black-box Large Language Models are Better Task Solvers with Experience Notebook,Yes.,4,"""However, LLMs still fail in many specific tasks although understand the task instruction.""",2023,2023-11-13T02:31:16Z,"Keyphrase: ""Failure on specific tasks""","""However, LLMs still fail in many specific tasks although understand the task instruction."" Keyphrase: ""Failure on specific tasks"""
arXIv2023,Flames: Benchmarking Value Alignment of LLMs in Chinese,Yes.,5,"""Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs,"" and ""there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness,"" and ""all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions.""",2023,2023-11-12T17:18:21Z,"Keyphrase: ""Limited safety evaluation""","""Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs,"" and ""there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness,"" and ""all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions."" Keyphrase: ""Limited safety evaluation"""
arXIv2023,Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding,Yes.,5,"""Nonetheless, logical reasoning that involves proof planning, specifically those that necessitate the validation of explanation accuracy, continues to present stumbling blocks."" and ""Our analysis reveals that LLMs still struggle to navigate complex reasoning chains, which demand the meticulous linkage of premises",2023,2023-11-12T05:12:49Z,"Keyphrase: ""Struggles with complex reasoning""","""Nonetheless, logical reasoning that involves proof planning, specifically those that necessitate the validation of explanation accuracy, continues to present stumbling blocks."" and ""Our analysis reveals that LLMs still struggle to navigate complex reasoning chains, which demand the meticulous linkage of premises Keyphrase: ""Struggles with complex reasoning"""
arXIv2023,CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset,Yes.,5,"""However, even models with billions of parameters face challenges in tasks demanding multi-step reasoning. Code generation and comprehension, especially in C and C++, emerge as significant challenges. ...they struggle with rectifying non-compilable C and C++ code. ...This approach, however, retains the limitations",2023,2023-11-11T08:21:52Z,"Keyphrase: ""Challenges in multistep reasoning and code generation""","""However, even models with billions of parameters face challenges in tasks demanding multi-step reasoning. Code generation and comprehension, especially in C and C++, emerge as significant challenges. ...they struggle with rectifying non-compilable C and C++ code. ...This approach, however, retains the limitations Keyphrase: ""Challenges in multistep reasoning and code generation"""
arXIv2023,ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome Management,Yes.,5,"""a leading barrier to the deployment of Artificial Intelligence (AI) and in particular LLMs has been concern for embedded gender and racial biases."" and ""we evaluate whether a leading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical management of acute coronary syndrome (ACS).""",2023,2023-11-10T19:59:36Z,"Keyphrase: ""Embedded gender and racial bias""","""a leading barrier to the deployment of Artificial Intelligence (AI) and in particular LLMs has been concern for embedded gender and racial biases."" and ""we evaluate whether a leading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical management of acute coronary syndrome (ACS)."" Keyphrase: ""Embedded gender and racial bias"""
arXIv2023,ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences,Yes.,5,"""most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. Another engineering barrier that prevents current medical LLM from better text processing ability is their restricted context length (e.g., 2,048 tokens), making it",2023,2023-11-10T12:25:32Z,"Keyphrase: ""Limited contextual understanding""","""most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. Another engineering barrier that prevents current medical LLM from better text processing ability is their restricted context length (e.g., 2,048 tokens), making it Keyphrase: ""Limited contextual understanding"""
arXIv2023,How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model,Yes.,4,"""MLLMs still face challenges in processing the semantic gap in multimodality, which may lead to erroneous generation, posing potential risks to society."" and ""Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with limited performance improvement.""",2023,2023-11-10T09:51:24Z,"Keyphrase: ""Semantic gap in multimodality""","""MLLMs still face challenges in processing the semantic gap in multimodality, which may lead to erroneous generation, posing potential risks to society."" and ""Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with limited performance improvement."" Keyphrase: ""Semantic gap in multimodality"""
arXIv2023,"Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications",Yes.,4,"""Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations.""",2023,2023-11-10T05:24:04Z,"Keyphrase: ""Outdated data limitations""","""Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations."" Keyphrase: ""Outdated data limitations"""
arXIv2023,Hallucination-minimized Data-to-answer Framework for Financial Decision-makers,Yes.,5,"""scaling such prototypes to robust products with minimized hallucinations or fake responses still remains an open challenge, especially in niche data-table heavy domains such as financial decision making.""",2023,2023-11-09T22:53:52Z,"Keyphrase: ""Hallucination in niche domains""","""scaling such prototypes to robust products with minimized hallucinations or fake responses still remains an open challenge, especially in niche data-table heavy domains such as financial decision making."" Keyphrase: ""Hallucination in niche domains"""
arXIv2023,Efficiently Adapting Pretrained Language Models To New Languages,Yes.,4,"""Recent large language models (LLM) exhibit sub-optimal performance on low-resource languages, as the training data of these models is usually dominated by English and other high-resource languages."" and ""naively adapting to new languages leads to catastrophic forgetting and poor tokenizer efficiency.""",2023,2023-11-09T20:59:08Z,"Keyphrase: ""Suboptimal performance in low-resource languages""","""Recent large language models (LLM) exhibit sub-optimal performance on low-resource languages, as the training data of these models is usually dominated by English and other high-resource languages."" and ""naively adapting to new languages leads to catastrophic forgetting and poor tokenizer efficiency."" Keyphrase: ""Suboptimal performance in low-resource languages"""
arXIv2023,Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models,Yes.,5,"""Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues."" and ""We find that even current state-of-the-art LLMs do not reach human performance, making our dataset",2023,2023-11-09T20:04:08Z,"Keyphrase: ""Deception and persuasion challenges""","""Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues."" and ""We find that even current state-of-the-art LLMs do not reach human performance, making our dataset Keyphrase: ""Deception and persuasion challenges"""
arXIv2023,Removing RLHF Protections in GPT-4 via Fine-Tuning,Yes.,5,"""fine-tuning can remove RLHF protections"" and ""Our results show the need for further research on protections on LLMs.""",2023,2023-11-09T17:54:59Z,"Keyphrase: ""Limited protection from harmful content""","""fine-tuning can remove RLHF protections"" and ""Our results show the need for further research on protections on LLMs."" Keyphrase: ""Limited protection from harmful content"""
arXIv2023,Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure,Yes.,5,"""Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so.""",2023,2023-11-09T17:12:44Z,"Keyphrase: ""Deceptive behavior""","""Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so."" Keyphrase: ""Deceptive behavior"""
arXIv2023,Do personality tests generalize to Large Language Models?,Yes.,5,"""LLMs' responses to personality tests systematically deviate from typical human responses,"" and ""reverse-coded items (e.g. 'I am introverted' vs 'I am extraverted') are often both answered affirmatively by LLMs,"" and ""variation across different prompts designed to 'steer' LLMs to simulate particular personality types does not follow the clear separation",2023,2023-11-09T11:54:01Z,"Keyphrase: ""Limited ability to simulate human-like responses""","""LLMs' responses to personality tests systematically deviate from typical human responses,"" and ""reverse-coded items (e.g. 'I am introverted' vs 'I am extraverted') are often both answered affirmatively by LLMs,"" and ""variation across different prompts designed to 'steer' LLMs to simulate particular personality types does not follow the clear separation Keyphrase: ""Limited ability to simulate human-like responses"""
arXIv2023,"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",Yes.,5,"""LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs."" and ""This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios.""",2023,2023-11-09T09:25:37Z,"Keyphrase: ""Hallucination and inconsistency""","""LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs."" and ""This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios."" Keyphrase: ""Hallucination and inconsistency"""
arXIv2023,"Frontier Language Models are not Robust to Adversarial Arithmetic, or ""What do I need to say so you agree 2+2=5?",Yes.,5,"""Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and",2023,2023-11-08T19:07:10Z,"Keyphrase: ""Vulnerability to adversarial prompts""","""Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and Keyphrase: ""Vulnerability to adversarial prompts"""
arXIv2023,How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,Yes.,5,"""However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation",2023,2023-11-08T18:58:43Z,"Keyphrase: ""Limited structural generalization""","""However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation Keyphrase: ""Limited structural generalization"""
arXIv2023,Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs,Yes.,5,"""Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas"" and ""Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness.""",2023,2023-11-08T18:52:17Z,"Keyphrase: ""Deep-rooted biases""","""Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas"" and ""Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness."" Keyphrase: ""Deep-rooted biases"""
arXIv2023,LooGLE: Can Long-Context Language Models Understand Long Contexts?,Yes.,5,"""LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks"" and ""strategies for extending context window length had limited impact on long context understanding.""",2023,2023-11-08T01:45:37Z,"Keyphrase: ""Limited long-context understanding""","""LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks"" and ""strategies for extending context window length had limited impact on long context understanding."" Keyphrase: ""Limited long-context understanding"""
arXIv2023,Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning,Yes.,5,"""LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations."" and ""Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication.""",2023,2023-11-07T21:09:57Z,"Keyphrase: ""Fabricated evidence hallucination""","""LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations."" and ""Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication."" Keyphrase: ""Fabricated evidence hallucination"""
arXIv2023,Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications,Yes.,4,"""LLM-integrated applications also introduce new attack surfaces,"" and ""Successful exploits of the identified vulnerabilities result in the users receiving responses tailored to the intent of a threat initiator,"" and ""our empirical results show that the threats can effectively bypass the restrictions and moderation policies of OpenAI, resulting in users receiving responses that contain bias, toxic content, privacy risk, and disinformation.""",2023,2023-11-07T20:13:05Z,"Keyphrase: ""Vulnerability to malicious intent""","""LLM-integrated applications also introduce new attack surfaces,"" and ""Successful exploits of the identified vulnerabilities result in the users receiving responses tailored to the intent of a threat initiator,"" and ""our empirical results show that the threats can effectively bypass the restrictions and moderation policies of OpenAI, resulting in users receiving responses that contain bias, toxic content, privacy risk, and disinformation."" Keyphrase: ""Vulnerability to malicious intent"""
arXIv2023,Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation,Yes.,5,"""However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures."" and ""However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models.""",2023,2023-11-07T18:03:23Z,"Keyphrase: ""Limited ability to acquire new knowledge""","""However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures."" and ""However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models."" Keyphrase: ""Limited ability to acquire new knowledge"""
arXIv2023,Unveiling Safety Vulnerabilities of Large Language Models,Yes.,5,"""As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern."" and ""We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it.""",2023,2023-11-07T16:50:33Z,"Keyphrase: ""Potential for harmful and inappropriate responses""","""As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern."" and ""We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it."" Keyphrase: ""Potential for harmful and inappropriate responses"""
arXIv2023,Do LLMs exhibit human-like response biases? A case study in survey design,Yes.,5,"""One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording"" and ""Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF.""",2023,2023-11-07T15:40:43Z,"Keyphrase: ""Failure to reflect humanlike behavior""","""One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording"" and ""Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF."" Keyphrase: ""Failure to reflect humanlike behavior"""
arXIv2023,Benefits and Harms of Large Language Models in Digital Mental Health,Yes.,4,"""This article presents contemporary perspectives on the opportunities and risks posed by LLMs in the design, development, and implementation of digital mental health tools.""",2023,2023-11-07T14:11:10Z,"Keyphrase: ""Risk of implementation in mental health""","""This article presents contemporary perspectives on the opportunities and risks posed by LLMs in the design, development, and implementation of digital mental health tools."" Keyphrase: ""Risk of implementation in mental health"""
arXIv2023,Input Reconstruction Attack against Vertical Federated Large Language Models,Yes.,5,"""privacy concerns limit their usage in real-life businesses"" and ""we demonstrate that in LLMs, VFL fails to protect the user input since it is simple and cheap to reconstruct the input from the intermediate embeddings.""",2023,2023-11-07T09:39:22Z,"Keyphrase: ""Privacy vulnerabilities""","""privacy concerns limit their usage in real-life businesses"" and ""we demonstrate that in LLMs, VFL fails to protect the user input since it is simple and cheap to reconstruct the input from the intermediate embeddings."" Keyphrase: ""Privacy vulnerabilities"""
arXIv2023,A Survey of Large Language Models Attribution,Yes.,4,"""issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems.""",2023,2023-11-07T05:20:09Z,"Keyphrase: ""Inherent biases and attribution issues""","""issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems."" Keyphrase: ""Inherent biases and attribution issues"""
arXIv2023,Quantifying Uncertainty in Natural Language Explanations of Large Language Models,Yes.,5,"""However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior."" and ""Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.""",2023,2023-11-06T21:14:40Z,"Keyphrase: ""Uncertain explanations""","""However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior."" and ""Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models."" Keyphrase: ""Uncertain explanations"""
arXIv2023,Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation,Yes.,4,"""Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour."" and ""Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.""",2023,2023-11-06T18:55:18Z,"Keyphrase: ""Vulnerability to harmful prompts""","""Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour."" and ""Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards."" Keyphrase: ""Vulnerability to harmful prompts"""
arXIv2023,Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance,Yes.,4,"""Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners.""",2023,2023-11-06T18:01:34Z,"Keyphrase: ""Inherent bias""","""Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners."" Keyphrase: ""Inherent bias"""
arXIv2023,Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges,Yes.,5,"""This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models",2023,2023-11-06T17:26:59Z,"Keyphrase: ""Hallucination in evaluations""","""This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models Keyphrase: ""Hallucination in evaluations"""
arXIv2023,Instructed Language Models with Retrievers Are Powerful Entity Linkers,Yes.,5,"""Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base."" and ""reaffirming that the EL task remains a persistent hurdle for general LLMs.""",2023,2023-11-06T16:38:51Z,"Keyphrase: ""Hallucination in generative content""","""Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base."" and ""reaffirming that the EL task remains a persistent hurdle for general LLMs."" Keyphrase: ""Hallucination in generative content"""
arXIv2023,DeepInception: Hypnotize Large Language Model to Be Jailbreaker,Yes.,5,"""Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void."" and ""Our investigation appeals to people to pay more attention to the safety aspects of LLMs and develop a stronger defense against their misuse risks.""",2023,2023-11-06T15:29:30Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void."" and ""Our investigation appeals to people to pay more attention to the safety aspects of LLMs and develop a stronger defense against their misuse risks."" Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Can LLMs Follow Simple Rules?,Yes.,5,"""Our evaluations of proprietary and open models show that almost all current models struggle to follow scenario rules, even on straightforward test cases.""",2023,2023-11-06T08:50:29Z,"Keyphrase: ""Struggles with rule-based scenarios""","""Our evaluations of proprietary and open models show that almost all current models struggle to follow scenario rules, even on straightforward test cases."" Keyphrase: ""Struggles with rule-based scenarios"""
arXIv2023,On the Intersection of Self-Correction and Trust in Language Models,Yes.,4,"""However, their complexity and lack of transparency have raised several trustworthiness concerns, including the propagation of misinformation and toxicity."" and ""Interestingly, our study also uncovers instances of 'self-doubt' in LLMs during the self-correction process, introducing a new set of challenges that need to be addressed.""",2023,2023-11-06T00:04:12Z,"Keyphrase: ""Trustworthiness concerns""","""However, their complexity and lack of transparency have raised several trustworthiness concerns, including the propagation of misinformation and toxicity."" and ""Interestingly, our study also uncovers instances of 'self-doubt' in LLMs during the self-correction process, introducing a new set of challenges that need to be addressed."" Keyphrase: ""Trustworthiness concerns"""
arXIv2023,FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM,Yes.,5,"""LLMs are constrained by the knowledge within their training data and are prone to generating inaccurate, or 'hallucinated', information.""",2023,2023-11-05T08:34:26Z,"Keyphrase: ""Inaccurate hallucinations""","""LLMs are constrained by the knowledge within their training data and are prone to generating inaccurate, or 'hallucinated', information."" Keyphrase: ""Inaccurate hallucinations"""
arXIv2023,Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles,Yes.,4,"""Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting.""",2023,2023-11-04T03:18:45Z,"Keyphrase: ""Limited few-shot translation performance""","""Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting."" Keyphrase: ""Limited few-shot translation performance"""
arXIv2023,An Interdisciplinary Outlook on Large Language Models for Scientific Research,Yes.,5,"""we articulate the challenges LLMs face, including their reliance on extensive and sometimes biased datasets, and the potential ethical dilemmas stemming from their use.""",2023,2023-11-03T19:41:09Z,"Keyphrase: ""Reliance on biased datasets and ethical dilemmas""","""we articulate the challenges LLMs face, including their reliance on extensive and sometimes biased datasets, and the potential ethical dilemmas stemming from their use."" Keyphrase: ""Reliance on biased datasets and ethical dilemmas"""
arXIv2023,An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology,Yes.,4,"""However, these LLMs are prone to many errors such as hallucinations, biases, and ethical violations, which necessitate rigorous evaluation and validation before clinical deployment.""",2023,2023-11-03T19:32:35Z,"Keyphrase: ""Error hallucination and ethical violations""","""However, these LLMs are prone to many errors such as hallucinations, biases, and ethical violations, which necessitate rigorous evaluation and validation before clinical deployment."" Keyphrase: ""Error hallucination and ethical violations"""
arXIv2023,The Alignment Problem in Context,Yes.,5,"""large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour"" and ""the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities.""",2023,2023-11-03T17:57:55Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour"" and ""the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities."" Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks,Yes.,4,"""However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions.""",2023,2023-11-03T14:39:20Z,"Keyphrase: ""Neglect of query-related information""","""However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions."" Keyphrase: ""Neglect of query-related information"""
arXIv2023,Comprehensive Assessment of Toxicity in ChatGPT,Yes.,4,"""The emerging large language models (LLMs), such as ChatGPT, can potentially further accentuate this threat."" and ""Previous works have discovered that ChatGPT can generate toxic responses using carefully crafted inputs.""",2023,2023-11-03T14:37:53Z,"Keyphrase: ""Generation of toxic responses""","""The emerging large language models (LLMs), such as ChatGPT, can potentially further accentuate this threat."" and ""Previous works have discovered that ChatGPT can generate toxic responses using carefully crafted inputs."" Keyphrase: ""Generation of toxic responses"""
arXIv2023,PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion,Yes.,5,"""The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy. We find three main error causes in our benchmark",2023,2023-11-03T08:06:35Z,"Keyphrase: ""Limited multi-turn dialogue capabilities""","""The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy. We find three main error causes in our benchmark Keyphrase: ""Limited multi-turn dialogue capabilities"""
arXIv2023,FinGPT: Large Generative Models for a Small Language,Yes.,4,"""LLM work tends to focus on languages where nearly unlimited data is available for pretraining.""",2023,2023-11-03T08:05:04Z,"Keyphrase: ""Limited focus on language diversity""","""LLM work tends to focus on languages where nearly unlimited data is available for pretraining."" Keyphrase: ""Limited focus on language diversity"""
arXIv2023,Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models,Yes.,4,"""Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such",2023,2023-11-03T05:55:32Z,"Keyphrase: ""Lack of interpretability""","""Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such Keyphrase: ""Lack of interpretability"""
arXIv2023,Successor Features for Efficient Multisubject Controlled Text Generation,Yes.,5,"""While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging.""",2023,2023-11-03T00:17:08Z,"Keyphrase: ""Challenges in safety and factuality""","""While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging."" Keyphrase: ""Challenges in safety and factuality"""
arXIv2023,Preserving the knowledge of long clinical texts using aggregated ensembles of large language models,Yes.,4,"""applying large language models, such as BERT-based models, to clinical texts poses two major challenges",2023,2023-11-02T19:50:02Z,"Keyphrase: ""Challenges in clinical text""","""applying large language models, such as BERT-based models, to clinical texts poses two major challenges Keyphrase: ""Challenges in clinical text"""
arXIv2023,"The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",Yes.,5,"""Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions.""",2023,2023-11-02T15:20:11Z,"Keyphrase: ""Inconsistent factual knowledge""","""Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions."" Keyphrase: ""Inconsistent factual knowledge"""
arXIv2023,FlashDecoding++: Faster Large Language Model Inference on GPUs,Yes.,5,"""However, the following challenges still remain unsolved in accelerating LLM inference",2023,2023-11-02T14:57:03Z,"Keyphrase: ""Unsolved challenges in accelerating LLM inference""","""However, the following challenges still remain unsolved in accelerating LLM inference Keyphrase: ""Unsolved challenges in accelerating LLM inference"""
arXIv2023,Revisiting the Knowledge Injection Frameworks,Yes.,4,"""However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved."" and ""we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected.""",2023,2023-11-02T11:18:16Z,"Keyphrase: ""Challenges in adapting to vertical domains""","""However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved."" and ""we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected."" Keyphrase: ""Challenges in adapting to vertical domains"""
arXIv2023,Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism,Yes.,5,"""these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios.""",2023,2023-11-02T07:20:49Z,"Keyphrase: ""Error-prone hallucination""","""these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios."" Keyphrase: ""Error-prone hallucination"""
arXIv2023,Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game,Yes.,5,"""While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks",2023,2023-11-02T06:13:36Z,"Keyphrase: ""Vulnerability to prompt injection attacks""","""While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks Keyphrase: ""Vulnerability to prompt injection attacks"""
arXIv2023,Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code,Yes.,5,"""Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code."" and ""existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security."" and ""existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations.""",2023,2023-11-01T22:46:31Z,"Keyphrase: ""Neglect of security considerations""","""Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code."" and ""existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security."" and ""existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations."" Keyphrase: ""Neglect of security considerations"""
arXIv2023,Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models,Yes.,5,"""However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks.""",2023,2023-11-01T21:41:08Z,"Keyphrase: ""Degradation in generalization""","""However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks."" Keyphrase: ""Degradation in generalization"""
arXIv2023,SAGE: Smart home Agent with Grounded Execution,Yes.,4,"""LLMs, however, lack specific knowledge about the user and their home limit their potential impact.""",2023,2023-11-01T18:36:28Z,"Keyphrase: ""Lack of specific knowledge""","""LLMs, however, lack specific knowledge about the user and their home limit their potential impact."" Keyphrase: ""Lack of specific knowledge"""
arXIv2023,Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs,Yes.,5,"""Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across two factuality subcategories. These consistent findings across various factual error categories suggest a fundamental limitation in the current LLMs' capability to accurately gauge factual",2023,2023-11-01T17:42:45Z,"Keyphrase: ""Limited factual accuracy""","""Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across two factuality subcategories. These consistent findings across various factual error categories suggest a fundamental limitation in the current LLMs' capability to accurately gauge factual Keyphrase: ""Limited factual accuracy"""
arXIv2023,Crosslingual Retrieval Augmented In-context Learning for Bangla,Yes.,5,"""The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla.""",2023,2023-11-01T15:32:50Z,"Keyphrase: ""Limited performance on low-resource languages""","""The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla."" Keyphrase: ""Limited performance on low-resource languages"""
arXIv2023,Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation,Yes.,5,"""Large Language Models (LLMs) can generate biased and toxic responses."", ""all tested LLMs exhibit explicit and/or implicit gender bias, even when explicit gender stereotypes are absent in the inputs.""",2023,2023-11-01T05:31:46Z,"Keyphrase: ""Gender bias and toxicity""","""Large Language Models (LLMs) can generate biased and toxic responses."", ""all tested LLMs exhibit explicit and/or implicit gender bias, even when explicit gender stereotypes are absent in the inputs."" Keyphrase: ""Gender bias and toxicity"""
arXIv2023,Is GPT Powerful Enough to Analyze the Emotions of Memes?,Yes.,5,"""Despite GPT's remarkable progress, our findings underscore the challenges faced by these models in handling subjective tasks, which are rooted in their inherent limitations including contextual understanding, interpretation of implicit meanings, and data biases.""",2023,2023-11-01T01:57:48Z,"Keyphrase: ""Subjectivity and implicit bias""","""Despite GPT's remarkable progress, our findings underscore the challenges faced by these models in handling subjective tasks, which are rooted in their inherent limitations including contextual understanding, interpretation of implicit meanings, and data biases."" Keyphrase: ""Subjectivity and implicit bias"""
arXIv2023,Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias,Yes.,4,"""The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included."" and ""disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans."" and ""these results underscore the importance of meticulous conditioning, model",2023,2023-11-01T01:32:59Z,"Keyphrase: ""Inaccurate representation of perspectives""","""The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included."" and ""disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans."" and ""these results underscore the importance of meticulous conditioning, model Keyphrase: ""Inaccurate representation of perspectives"""
arXIv2023,The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback,Yes.,5,"""Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations.""",2023,2023-10-31T21:52:41Z,"Keyphrase: ""Safety and reliability issues""","""Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations."" Keyphrase: ""Safety and reliability issues"""
arXIv2023,BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B,Yes.,5,"""We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are",2023,2023-10-31T19:45:15Z,"Keyphrase: ""Ineffective safety fine-tuning""","""We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are Keyphrase: ""Ineffective safety fine-tuning"""
arXIv2023,BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text,Yes.,5,"""language models like BERT deteriorate in the face of dialect variation or noise.""",2023,2023-10-31T19:44:50Z,"Keyphrase: ""Sensitivity to dialect variation and noise""","""language models like BERT deteriorate in the face of dialect variation or noise."" Keyphrase: ""Sensitivity to dialect variation and noise"""
arXIv2023,Filter bubbles and affective polarization in user-personalized large language model outputs,Yes.,5,"""These results illustrate that personalizing LLMs based on user demographics carry the same risks of affective polarization and filter bubbles that have been seen in other personalized internet technologies. This 'failure mode' should be monitored closely as there are more attempts to monetize and personalize these models.""",2023,2023-10-31T18:19:28Z,"Keyphrase: ""Affective polarization and filter bubbles""","""These results illustrate that personalizing LLMs based on user demographics carry the same risks of affective polarization and filter bubbles that have been seen in other personalized internet technologies. This 'failure mode' should be monitored closely as there are more attempts to monetize and personalize these models."" Keyphrase: ""Affective polarization and filter bubbles"""
arXIv2023,LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B,Yes.,5,"""We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat."" and ""While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate",2023,2023-10-31T16:55:06Z,"Keyphrase: ""Safety and security risks""","""We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat."" and ""While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate Keyphrase: ""Safety and security risks"""
arXIv2023,Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT,Yes.,5,"""these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input.""",2023,2023-10-31T15:41:08Z,"Keyphrase: ""Token limit constraint""","""these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input."" Keyphrase: ""Token limit constraint"""
arXIv2023,LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts,Yes.,5,"""IR systems in the LLMs era are facing a new challenge",2023,2023-10-31T14:42:23Z,"Keyphrase: ""New challenges in IR systems""","""IR systems in the LLMs era are facing a new challenge Keyphrase: ""New challenges in IR systems"""
arXIv2023,FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models,Yes.,4,"""By evaluating ten closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work.""",2023,2023-10-31T12:32:38Z,Keyphrase: Lack of transparency in popular LLMs,"""By evaluating ten closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work."" Keyphrase: Lack of transparency in popular LLMs"
arXIv2023,The Expressibility of Polynomial based Attention Scheme,Yes.,5,"""the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference.""",2023,2023-10-30T22:16:18Z,"Keyphrase: ""Scalability challenges""","""the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference."" Keyphrase: ""Scalability challenges"""
arXIv2023,Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization,Yes.,5,"""community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses.""",2023,2023-10-30T21:33:22Z,"Keyphrase: ""Factually hallucinated summaries""","""community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses."" Keyphrase: ""Factually hallucinated summaries"""
arXIv2023,Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation,Yes.,4,"""However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements.""",2023,2023-10-30T17:04:35Z,"Keyphrase: ""Subjectivity and unreliable judgment""","""However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements."" Keyphrase: ""Subjectivity and unreliable judgment"""
arXIv2023,Adversarial Attacks and Defenses in Large Language Models: Old and New Threats,Yes.,4,"""substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude"" and ""we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models.""",2023,2023-10-30T17:01:02Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude"" and ""we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models."" Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Evaluating Large Language Models: A Comprehensive Survey,Yes.,5,"""LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards.""",2023,2023-10-30T17:00:52Z,"Keyphrase: ""Privacy risks and potential harm""","""LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards."" Keyphrase: ""Privacy risks and potential harm"""
arXIv2023,Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs,Yes.,5,"""they often generate summaries that are factually inconsistent with original articles, known as 'hallucinations' in text generation."" and ""current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, overgeneralizing, etc.""",2023,2023-10-30T08:40:16Z,"Keyphrase: ""Factually inconsistent summaries""","""they often generate summaries that are factually inconsistent with original articles, known as 'hallucinations' in text generation."" and ""current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, overgeneralizing, etc."" Keyphrase: ""Factually inconsistent summaries"""
arXIv2023,M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models,Yes.,5,"""Our results reveal that",2023,2023-10-30T03:11:30Z,Keyphrase: Lack of context or specificity,"""Our results reveal that Keyphrase: Lack of context or specificity"
arXIv2023,"From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude",Yes.,4,"""However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks.""",2023,2023-10-29T22:52:40Z,"Keyphrase: ""Susceptible to generating malicious content""","""However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks."" Keyphrase: ""Susceptible to generating malicious content"""
arXIv2023,N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics,Yes.,5,"""We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination."" and ""enhance trustworthiness by addressing fairness, bias, and robustness concerns.""",2023,2023-10-28T11:22:22Z,"Keyphrase: ""Toxicity and hallucination""","""We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination."" and ""enhance trustworthiness by addressing fairness, bias, and robustness concerns."" Keyphrase: ""Toxicity and hallucination"""
arXIv2023,On the Automatic Generation and Simplification of Children's Stories,Yes.,5,"""We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups."" and ""while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models",2023,2023-10-27T21:31:34Z,"Keyphrase: ""Limited vocabulary for younger age groups""","""We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups."" and ""while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models Keyphrase: ""Limited vocabulary for younger age groups"""
arXIv2023,DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues,Yes.,4,"""This dataset presents challenges concerning knowledge recency, safety, fairness, and bias.""",2023,2023-10-27T13:23:02Z,"Keyphrase: ""Challenges in knowledge recency and bias""","""This dataset presents challenges concerning knowledge recency, safety, fairness, and bias."" Keyphrase: ""Challenges in knowledge recency and bias"""
arXIv2023,NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark,Yes.,5,"""Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded.""",2023,2023-10-27T09:48:29Z,"Keyphrase: ""Performance overestimation due to contamination""","""Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded."" Keyphrase: ""Performance overestimation due to contamination"""
arXIv2023,SOUL: Towards Sentiment and Opinion Understanding of Language,Yes.,5,"""Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications.""",2023,2023-10-27T06:48:48Z,"Keyphrase: ""Performance gap compared to human reasoning""","""Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications."" Keyphrase: ""Performance gap compared to human reasoning"""
arXIv2023,Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method,Yes.,5,"""recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization.""",2023,2023-10-27T06:22:14Z,"Keyphrase: ""Nonfactual responses""","""recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization."" Keyphrase: ""Nonfactual responses"""
arXIv2023,Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory,Yes.,5,"""Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning.""",2023,2023-10-27T04:15:30Z,"Keyphrase: ""Privacy leakage""","""Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning."" Keyphrase: ""Privacy leakage"""
arXIv2023,"""You Are An Expert Linguistic Annotator"": Limits of LLMs as Analyzers of Abstract Meaning Representation",Yes.,5,"""we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure"" and ""model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses.""",2023,2023-10-26T21:47:59Z,"Keyphrase: ""Frequent major errors""","""we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure"" and ""model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses."" Keyphrase: ""Frequent major errors"""
arXIv2023,Evaluation of large language models using an Indian language LGBTI+ lexicon,Yes.,4,"""Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English.""",2023,2023-10-26T21:32:24Z,"Keyphrase: ""Limited detection of hateful content""","""Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English."" Keyphrase: ""Limited detection of hateful content"""
arXIv2023,A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications,Yes.,4,"""We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles.""",2023,2023-10-26T19:45:06Z,"Keyphrase: ""Violation of fairness principles""","""We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles."" Keyphrase: ""Violation of fairness principles"""
arXIv2023,Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems,Yes.,5,"""revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context.""",2023,2023-10-26T19:44:06Z,"Keyphrase: ""Lagging fluency and informativeness""","""revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context."" Keyphrase: ""Lagging fluency and informativeness"""
arXIv2023,Proving Test Set Contamination in Black Box Language Models,Yes.,5,"""Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks."" and ""Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.""",2023,2023-10-26T17:43:13Z,"Keyphrase: ""Overreliance on memorization""","""Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks."" and ""Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples."" Keyphrase: ""Overreliance on memorization"""
arXIv2023,An Open Source Data Contamination Report for Large Language Models,Yes.,4,"""Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models."" and ""Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics.""",2023,2023-10-26T17:11:42Z,"Keyphrase: ""Data contamination issues""","""Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models."" and ""Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics."" Keyphrase: ""Data contamination issues"""
arXIv2023,Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering,Yes.,4,"""LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers.""",2023,2023-10-26T15:45:12Z,"Keyphrase: ""Overconfidence in irrelevant information""","""LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers."" Keyphrase: ""Overconfidence in irrelevant information"""
arXIv2023,Symbolic Planning and Code Generation for Grounded Dialogue,Yes.,5,"""LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding.""",2023,2023-10-26T04:22:23Z,"Keyphrase: ""Limited task-oriented dialogue grounding""","""LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding."" Keyphrase: ""Limited task-oriented dialogue grounding"""
arXIv2023,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,Yes.,5,"""LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses.""",2023,2023-10-26T03:28:30Z,"Keyphrase: ""Limited external knowledge attribution""","""LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses."" Keyphrase: ""Limited external knowledge attribution"""
arXIv2023,Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT and GPT-4 for Dialogue Summarization,Yes.,5,"""Our findings indicate that GPT models often produce lengthy summaries and deviate from human summarization guidelines."" and ""The results reveal that GPT models exhibit unique stylistic tendencies in their summaries."" and ""While BERTScores did not dramatically decrease for GPT outputs suggesting semantic similarity to human references and specialised pre-trained models, ROUGE scores reveal grammatical and lexical disparities between GPT-generated and human-written summaries",2023,2023-10-25T17:39:07Z,"Keyphrase: ""Stylistic and semantic disparities""","""Our findings indicate that GPT models often produce lengthy summaries and deviate from human summarization guidelines."" and ""The results reveal that GPT models exhibit unique stylistic tendencies in their summaries."" and ""While BERTScores did not dramatically decrease for GPT outputs suggesting semantic similarity to human references and specialised pre-trained models, ROUGE scores reveal grammatical and lexical disparities between GPT-generated and human-written summaries Keyphrase: ""Stylistic and semantic disparities"""
arXIv2023,Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation,Yes.,5,"""The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Specifically, it showed limitations when dealing with non-Latin languages and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image.""",2023,2023-10-25T17:38:55Z,"Keyphrase: ""Struggles with multilingual and complex tasks""","""The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Specifically, it showed limitations when dealing with non-Latin languages and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image."" Keyphrase: ""Struggles with multilingual and complex tasks"""
arXIv2023,Detecting Pretraining Data from Large Language Models,Yes.,4,"""the data used to train them is rarely disclosed"" and ""it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks.""",2023,2023-10-25T17:21:23Z,Keyphrase: Lack of transparency in training data,"""the data used to train them is rarely disclosed"" and ""it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks."" Keyphrase: Lack of transparency in training data"
arXIv2023,SuperHF: Supervised Iterative Learning from Human Feedback,Yes.,5,"""While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training.""",2023,2023-10-25T16:52:00Z,"Keyphrase: ""Safety and value alignment challenges""","""While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training."" Keyphrase: ""Safety and value alignment challenges"""
arXIv2023,HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models,Yes.,5,"""Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.""",2023,2023-10-25T16:41:15Z,"Keyphrase: ""Decline in performance on higher-order tasks""","""Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP."" Keyphrase: ""Decline in performance on higher-order tasks"""
arXIv2023,"R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",Yes.,4,"""the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated.""",2023,2023-10-25T10:34:02Z,"Keyphrase: ""Inaccurate results in noisy contexts""","""the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated."" Keyphrase: ""Inaccurate results in noisy contexts"""
arXIv2023,An Early Evaluation of GPT-4V(ision),Yes.,5,"""Our experimental results reveal the ability and limitations of GPT-4V"" and ""GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age; (3) GPT-4V obtains worse results",2023,2023-10-25T10:33:17Z,"Keyphrase: ""Cross-lingual and sensitive trait performance issues""","""Our experimental results reveal the ability and limitations of GPT-4V"" and ""GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age; (3) GPT-4V obtains worse results Keyphrase: ""Cross-lingual and sensitive trait performance issues"""
arXIv2023,Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting,Yes.,4,"""A crucial challenge for generative large language models (LLMs) is diversity",2023,2023-10-25T10:17:17Z,"Keyphrase: ""Lack of diversity""","""A crucial challenge for generative large language models (LLMs) is diversity Keyphrase: ""Lack of diversity"""
arXIv2023,OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models,Yes.,4,"""existing instruction-tuning datasets suffer from occupational bias",2023,2023-10-25T10:06:17Z,"Keyphrase: ""Occupational bias""","""existing instruction-tuning datasets suffer from occupational bias Keyphrase: ""Occupational bias"""
arXIv2023,Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation,Yes.,5,"""Our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. To address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs. Moreover, our study identifies",2023,2023-10-25T00:32:56Z,"Keyphrase: ""Overlooking security concerns""","""Our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. To address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs. Moreover, our study identifies Keyphrase: ""Overlooking security concerns"""
arXIv2023,Knowledge Editing for Large Language Models: A Survey,Yes.,4,"""Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model.""",2023,2023-10-24T22:18:13Z,"Keyphrase: ""Substantial computational cost""","""Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model."" Keyphrase: ""Substantial computational cost"""
arXIv2023,Can You Follow Me? Testing Situational Understanding in ChatGPT,Yes.,5,"""Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs),"" and ""despite the fundamental simplicity of the task, the model's performance reflects an inability to retain correct environment states across time,"" and ""performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to halluc",2023,2023-10-24T19:22:01Z,"Keyphrase: ""Lack of persistent in-context memory""","""Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs),"" and ""despite the fundamental simplicity of the task, the model's performance reflects an inability to retain correct environment states across time,"" and ""performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to halluc Keyphrase: ""Lack of persistent in-context memory"""
arXIv2023,Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition,Yes.,5,"""These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones.""",2023,2023-10-24T18:18:11Z,"Keyphrase: ""Vulnerability to prompt manipulation""","""These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones."" Keyphrase: ""Vulnerability to prompt manipulation"""
arXIv2023,MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning,Yes.,5,"""While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings.""",2023,2023-10-24T17:59:20Z,"Keyphrase: ""Limited reasoning ability""","""While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings."" Keyphrase: ""Limited reasoning ability"""
arXIv2023,Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs,Yes.,5,"""it is important to investigate their limitations in dealing with different image and question properties."" and ""we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size.""",2023,2023-10-24T17:48:04Z,"Keyphrase: ""Limited zero-shot accuracy in answering visual questions""","""it is important to investigate their limitations in dealing with different image and question properties."" and ""we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size."" Keyphrase: ""Limited zero-shot accuracy in answering visual questions"""
arXIv2023,This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models,Yes.,5,"""Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing"" and ""Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on",2023,2023-10-24T15:38:21Z,"Keyphrase: ""Struggles with negation""","""Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing"" and ""Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on Keyphrase: ""Struggles with negation"""
arXIv2023,BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT,Yes.,5,"""However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning.""",2023,2023-10-24T14:57:34Z,"Keyphrase: ""Lack of multiturn engagement""","""However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning."" Keyphrase: ""Lack of multiturn engagement"""
arXIv2023,SoK: Memorization in General-Purpose Large Language Models,Yes.,5,"""This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond.""",2023,2023-10-24T14:25:53Z,"Keyphrase: ""Privacy and security concerns""","""This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond."" Keyphrase: ""Privacy and security concerns"""
arXIv2023,Self-Guard: Empower the LLM to Safeguard Itself,Yes.,5,"""The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content... safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help.""",2023,2023-10-24T14:08:26Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content... safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help."" Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Unnatural language processing: How do language models handle machine-generated prompts?,Yes.,5,"""We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions."" and ""Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles.""",2023,2023-10-24T13:32:20Z,"Keyphrase: ""Sensitivity to input variations""","""We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions."" and ""Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles."" Keyphrase: ""Sensitivity to input variations"""
arXIv2023,Generative Language Models Exhibit Social Identity Biases,Yes.,4,"""The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans."" and ""Our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data.""",2023,2023-10-24T13:17:40Z,"Keyphrase: ""Social identity bias""","""The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans."" and ""Our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data."" Keyphrase: ""Social identity bias"""
arXIv2023,Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers,Yes.,5,"""the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability"" and ""Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations.""",2023,2023-10-24T12:37:06Z,"Keyphrase: ""Vulnerability to shortcut manipulation""","""the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability"" and ""Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations."" Keyphrase: ""Vulnerability to shortcut manipulation"""
arXIv2023,Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation,Yes.,5,"""However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes.""",2023,2023-10-24T11:40:34Z,"Keyphrase: ""Limited relationship understanding""","""However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes."" Keyphrase: ""Limited relationship understanding"""
arXIv2023,Prevalence and prevention of large language model use in crowd work,Yes.,4,"""LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data"" and ""preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use L",2023,2023-10-24T09:52:09Z,"Keyphrase: ""Homogeneous responses and human behavior degradation""","""LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data"" and ""preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use L Keyphrase: ""Homogeneous responses and human behavior degradation"""
arXIv2023,KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval,Yes.,5,"""Motivated by rising concerns around factual incorrectness and hallucinations of LLMs,"" and ""Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases.""",2023,2023-10-24T04:40:38Z,"Keyphrase: ""Factual incorrectness and irrelevant information""","""Motivated by rising concerns around factual incorrectness and hallucinations of LLMs,"" and ""Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases."" Keyphrase: ""Factual incorrectness and irrelevant information"""
arXIv2023,The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks,Yes.,5,"""security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure,"" and ""Our findings indicate that, with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from being impermeable to PII",2023,2023-10-24T02:48:19Z,"Keyphrase: ""Privacy and security risks""","""security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure,"" and ""Our findings indicate that, with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from being impermeable to PII Keyphrase: ""Privacy and security risks"""
arXIv2023,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,Yes.,5,"""We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.""",2023,2023-10-24T00:24:11Z,"Keyphrase: ""Limited chain-of-thought reasoning""","""We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning."" Keyphrase: ""Limited chain-of-thought reasoning"""
arXIv2023,EpiK-Eval: Evaluation for Language Models as Epistemic Models,Yes.,5,"""Evaluations across various LLMs reveal significant weaknesses in this domain.""",2023,2023-10-23T21:15:54Z,"Keyphrase: ""Domain weaknesses""","""Evaluations across various LLMs reveal significant weaknesses in this domain."" Keyphrase: ""Domain weaknesses"""
arXIv2023,"Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",Yes.,5,"""We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence",2023,2023-10-23T20:35:52Z,"Keyphrase: ""Hallucination of constrained output""","""We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence Keyphrase: ""Hallucination of constrained output"""
arXIv2023,Moral Foundations of Large Language Models,Yes.,4,"""they may reflect the biases that are present in such corpora"" and ""illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance.""",2023,2023-10-23T20:05:37Z,"Keyphrase: ""Assumed moral stance""","""they may reflect the biases that are present in such corpora"" and ""illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance."" Keyphrase: ""Assumed moral stance"""
arXIv2023,"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models",Yes.,5,"""it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration."" and ""experimental results have shown that it poses significant challenges for all existing LLMs.""",2023,2023-10-23T17:52:06Z,"Keyphrase: ""Evaluation challenges due to text length""","""it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration."" and ""experimental results have shown that it poses significant challenges for all existing LLMs."" Keyphrase: ""Evaluation challenges due to text length"""
arXIv2023,Towards LLM-driven Dialogue State Tracking,Yes.,5,"""Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities.""",2023,2023-10-23T14:15:28Z,"Keyphrase: ""Closed-source and data privacy concerns""","""Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities."" Keyphrase: ""Closed-source and data privacy concerns"""
arXIv2023,Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism,Yes.,5,"""We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible ->implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family.""",2023,2023-10-23T12:40:41Z,"Keyphrase: ""Limited handling of lexical negation""","""We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible ->implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family."" Keyphrase: ""Limited handling of lexical negation"""
arXIv2023,ALCUNA: Large Language Models Meet New Knowledge,Yes.,5,"""We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge.""",2023,2023-10-23T11:40:05Z,"Keyphrase: ""Limited reasoning capabilities""","""We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge."" Keyphrase: ""Limited reasoning capabilities"""
arXIv2023,Evaluating the Knowledge Base Completion Potential of GPT,Yes.,5,"""We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task.""",2023,2023-10-23T10:15:13Z,"Keyphrase: ""Limited task performance""","""We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task."" Keyphrase: ""Limited task performance"""
arXIv2023,"A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions",Yes.,4,"""We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, and data ambiguity.""",2023,2023-10-23T09:01:13Z,"Keyphrase: ""Out-of-distribution challenges""","""We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, and data ambiguity."" Keyphrase: ""Out-of-distribution challenges"""
arXIv2023,Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models,Yes.,4,"""We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge.""",2023,2023-10-23T08:45:12Z,"Keyphrase: ""Limited lexical knowledge""","""We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge."" Keyphrase: ""Limited lexical knowledge"""
arXIv2023,Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications,Yes.,5,"""LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society,"" ""LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks,"" and ""the fairness metric gap between different subgroups is still larger than that in traditional machine learning models.""",2023,2023-10-23T06:31:28Z,"Keyphrase: ""Harmful social bias and fairness gaps""","""LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society,"" ""LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks,"" and ""the fairness metric gap between different subgroups is still larger than that in traditional machine learning models."" Keyphrase: ""Harmful social bias and fairness gaps"""
arXIv2023,"Language Models Hallucinate, but May Excel at Fact Verification",Yes.,5,"""Nevertheless, LLMs frequently 'hallucinate,' resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time.""",2023,2023-10-23T04:39:01Z,"Keyphrase: ""Frequent hallucinations""","""Nevertheless, LLMs frequently 'hallucinate,' resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time."" Keyphrase: ""Frequent hallucinations"""
arXIv2023,The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages,Yes.,5,"""Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task",2023,2023-10-23T04:22:44Z,"Keyphrase: ""Struggles with understanding social meaning""","""Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task Keyphrase: ""Struggles with understanding social meaning"""
arXIv2023,Evaluating Large Language Models on Controlled Generation Tasks,Yes.,5,"""We conclude that large language models struggle at meeting fine-grained hard constraints.""",2023,2023-10-23T03:48:24Z,"Keyphrase: ""Difficulty with fine-grained hard constraints""","""We conclude that large language models struggle at meeting fine-grained hard constraints."" Keyphrase: ""Difficulty with fine-grained hard constraints"""
arXIv2023,Retrieval-Augmented Chain-of-Thought in Semi-structured Domains,Yes.,5,"""their inability to handle very long inputs/contexts is well known.""",2023,2023-10-22T22:45:14Z,"Keyphrase: ""Struggles with long inputs""","""their inability to handle very long inputs/contexts is well known."" Keyphrase: ""Struggles with long inputs"""
arXIv2023,Large Language Models are biased to overestimate profoundness,Yes.,5,"""LLMs systematically overestimate the profoundness of nonsensical statements,"" and ""provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.""",2023,2023-10-22T21:33:50Z,"Keyphrase: ""Overestimation of profoundness""","""LLMs systematically overestimate the profoundness of nonsensical statements,"" and ""provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements."" Keyphrase: ""Overestimation of profoundness"""
arXIv2023,Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis,Yes.,4,"""Ethical restrictions prohibit large language models (LLMs) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models.""",2023,2023-10-22T15:19:04Z,"Keyphrase: ""Ethical restrictions on harmful content""","""Ethical restrictions prohibit large language models (LLMs) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models."" Keyphrase: ""Ethical restrictions on harmful content"""
arXIv2023,Chainpoll: A high efficacy method for LLM hallucination detection,Yes.,5,"""hallucinations - incorrect or unfounded claims - are still prevalent,"" and ""we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use.""",2023,2023-10-22T14:45:14Z,"Keyphrase: ""Hallucination and unfounded claims""","""hallucinations - incorrect or unfounded claims - are still prevalent,"" and ""we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use."" Keyphrase: ""Hallucination and unfounded claims"""
arXIv2023,Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases,Yes.,5,"""Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training,"" and ""Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA-2-CHAT where the model's responses are strongly biased and opinionated 64% of the time.""",2023,2023-10-22T13:55:46Z,"Keyphrase: ""Unaddressed harmful biases""","""Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training,"" and ""Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA-2-CHAT where the model's responses are strongly biased and opinionated 64% of the time."" Keyphrase: ""Unaddressed harmful biases"""
arXIv2023,From Static to Dynamic: A Continual Learning Framework for Large Language Models,Yes.,4,"""However, this complexity also presents challenges, making LLMs difficult to train and inhibiting their ability to continuously assimilate new knowledge, which may lead to inaccuracies in their outputs.""",2023,2023-10-22T10:18:53Z,"Keyphrase: ""Difficulty in continuous learning""","""However, this complexity also presents challenges, making LLMs difficult to train and inhibiting their ability to continuously assimilate new knowledge, which may lead to inaccuracies in their outputs."" Keyphrase: ""Difficulty in continuous learning"""
arXIv2023,MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications,Yes.,4,"""However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of",2023,2023-10-21T17:18:09Z,"Keyphrase: ""Data imbalance and high cost in medical finetuning""","""However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of Keyphrase: ""Data imbalance and high cost in medical finetuning"""
arXIv2023,Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain,Yes.,5,"""We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that",2023,2023-10-21T16:14:56Z,"Keyphrase: ""Lack of self-consistency""","""We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that Keyphrase: ""Lack of self-consistency"""
arXIv2023,BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues,Yes.,4,"""other LLMs struggle to generate multi-turn dialogues of satisfactory quality due to poor instruction-following capability, tendency to generate lengthy utterances, or limited general capability.""",2023,2023-10-20T16:53:51Z,"Keyphrase: ""Poor multiturn dialogue quality""","""other LLMs struggle to generate multi-turn dialogues of satisfactory quality due to poor instruction-following capability, tendency to generate lengthy utterances, or limited general capability."" Keyphrase: ""Poor multiturn dialogue quality"""
arXIv2023,Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning,Yes.,5,"""Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers.""",2023,2023-10-20T16:05:35Z,"Keyphrase: ""Misinterpretation of questions""","""Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers."" Keyphrase: ""Misinterpretation of questions"""
arXIv2023,She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and Sustainable Language Models,Yes.,4,"""Recent events indicate ethical concerns around conventionally trained LLMs, leading to overall unsafe user experiences."" and ""The assessment presented in this paper highlights a gap between societal alignment and the capabilities of current LLMs.""",2023,2023-10-20T14:18:40Z,"Keyphrase: ""Ethical concerns and societal misalignment""","""Recent events indicate ethical concerns around conventionally trained LLMs, leading to overall unsafe user experiences."" and ""The assessment presented in this paper highlights a gap between societal alignment and the capabilities of current LLMs."" Keyphrase: ""Ethical concerns and societal misalignment"""
arXIv2023,Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning,Yes.,5,"""current LLM-based MT systems are brittle",2023,2023-10-20T12:29:51Z,"Keyphrase: ""Brittle performance""","""current LLM-based MT systems are brittle Keyphrase: ""Brittle performance"""
arXIv2023,Self-Consistency of Large Language Models under Ambiguity,Yes.,5,"""Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency,"" and ""we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence.""",2023,2023-10-20T11:57:56Z,"Keyphrase: ""Inconsistent contextual answers""","""Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency,"" and ""we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence."" Keyphrase: ""Inconsistent contextual answers"""
arXIv2023,POSQA: Probe the World Models of LLMs with Size Comparisons,Yes.,5,"""We show that even the largest LLMs today perform poorly under the zero-shot setting,"" and ""Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.""",2023,2023-10-20T10:05:01Z,"Keyphrase: ""Vulnerability to deception and confusion""","""We show that even the largest LLMs today perform poorly under the zero-shot setting,"" and ""Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours."" Keyphrase: ""Vulnerability to deception and confusion"""
arXIv2023,Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs),Yes.,5,"""This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes",2023,2023-10-20T08:13:36Z,"Keyphrase: ""Domain specificity and knowledge forgetting""","""This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes Keyphrase: ""Domain specificity and knowledge forgetting"""
arXIv2023,Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,Yes.,5,"""However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to 'a blindfolded text-based game.' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand.""",2023,2023-10-20T03:22:05Z,"Keyphrase: ""Limited visual comprehension""","""However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to 'a blindfolded text-based game.' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand."" Keyphrase: ""Limited visual comprehension"""
arXIv2023,StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding,Yes.,5,"""Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa.""",2023,2023-10-19T16:29:23Z,"Keyphrase: ""Struggles with analogy identification""","""Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa."" Keyphrase: ""Struggles with analogy identification"""
arXIv2023,Probing LLMs for hate speech detection: strengths and vulnerabilities,Yes.,4,"""we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute 'jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.""",2023,2023-10-19T16:11:02Z,"Keyphrase: ""Failure to classify errors""","""we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute 'jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts."" Keyphrase: ""Failure to classify errors"""
arXIv2023,Prompt Injection Attacks and Defenses in LLM-Integrated Applications,Yes.,4,"""Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires.""",2023,2023-10-19T15:12:09Z,"Keyphrase: ""Vulnerability to prompt injection attacks""","""Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires."" Keyphrase: ""Vulnerability to prompt injection attacks"""
arXIv2023,Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization,Yes.,4,"""However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge.""",2023,2023-10-19T14:50:51Z,"Keyphrase: ""Challenges in transfer learning""","""However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge."" Keyphrase: ""Challenges in transfer learning"""
arXIv2023,Safe RLHF: Safe Reinforcement Learning from Human Feedback,Yes.,4,"""the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training"" and ""We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints.""",2023,2023-10-19T14:22:03Z,"Keyphrase: ""Safety concerns and optimization challenges""","""the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training"" and ""We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints."" Keyphrase: ""Safety concerns and optimization challenges"""
arXIv2023,Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong,Yes.,5,"""However, they over-rely on the LLMs when the explanation is wrong."" and ""Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.""",2023,2023-10-19T08:09:58Z,"Keyphrase: ""Unreliable explanations""","""However, they over-rely on the LLMs when the explanation is wrong."" and ""Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences."" Keyphrase: ""Unreliable explanations"""
arXIv2023,Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks,Yes.,5,"""Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex.""",2023,2023-10-19T06:37:32Z,"Keyphrase: ""Hallucination in question-answering scenarios""","""Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex."" Keyphrase: ""Hallucination in question-answering scenarios"""
arXIv2023,Attack Prompt Generation for Red Teaming and Defending Large Language Models,Yes.,4,"""Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content.""",2023,2023-10-19T06:15:05Z,"Keyphrase: ""Susceptibility to harmful content generation""","""Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content."" Keyphrase: ""Susceptibility to harmful content generation"""
arXIv2023,Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models,Yes.,5,"""This paper identifies a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training"" and ""Our study emphasizes the need to critically examine cultural dominance and ethical consideration in their development and deployment.""",2023,2023-10-19T05:38:23Z,"Keyphrase: ""Cultural dominance issue""","""This paper identifies a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training"" and ""Our study emphasizes the need to critically examine cultural dominance and ethical consideration in their development and deployment."" Keyphrase: ""Cultural dominance issue"""
arXIv2023,Contrastive Learning for Inference in Dialogue,Yes.,5,"""While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning.""",2023,2023-10-19T04:49:36Z,"Keyphrase: ""Limited deductive reasoning""","""While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning."" Keyphrase: ""Limited deductive reasoning"""
arXIv2023,"Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher",Yes.,5,"""challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the LLM hallucination problem.""",2023,2023-10-19T03:49:36Z,"Keyphrase: ""Hallucination problem and unreliable results""","""challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the LLM hallucination problem."" Keyphrase: ""Hallucination problem and unreliable results"""
arXIv2023,PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models,Yes.,4,"""However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs.""",2023,2023-10-19T03:25:28Z,"Keyphrase: ""Backdoor vulnerability""","""However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs."" Keyphrase: ""Backdoor vulnerability"""
arXIv2023,Automated Repair of Declarative Software Specifications in the Era of Large Language Models,Yes.,5,"""Our study revealed that while ChatGPT falls short in comparison to existing techniques, it was able to successfully repair bugs that no other technique could address. Our analysis also identified errors in ChatGPT's generated repairs, including improper operator usage, type errors, higher-order logic misuse, and relational arity mismatches. Additionally, we observed instances of hallucinations in ChatGPT-generated repairs and incons",2023,2023-10-19T02:30:42Z,"Keyphrase: ""Error-prone repairs""","""Our study revealed that while ChatGPT falls short in comparison to existing techniques, it was able to successfully repair bugs that no other technique could address. Our analysis also identified errors in ChatGPT's generated repairs, including improper operator usage, type errors, higher-order logic misuse, and relational arity mismatches. Additionally, we observed instances of hallucinations in ChatGPT-generated repairs and incons Keyphrase: ""Error-prone repairs"""
arXIv2023,GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems,Yes.,5,"""The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to",2023,2023-10-19T00:56:37Z,"Keyphrase: ""Limited ability in verifying solutions""","""The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to Keyphrase: ""Limited ability in verifying solutions"""
arXIv2023,FactCHD: Benchmarking Fact-Conflicting Hallucination Detection,Yes.,5,"""Despite their impressive generative capabilities, LLMs are hindered by fact-conflicting hallucinations in real-world applications."" and ""Experiments on different LLMs expose the shortcomings of current approaches in detecting factual errors accurately.""",2023,2023-10-18T16:27:49Z,"Keyphrase: ""Factual error detection shortcomings""","""Despite their impressive generative capabilities, LLMs are hindered by fact-conflicting hallucinations in real-world applications."" and ""Experiments on different LLMs expose the shortcomings of current approaches in detecting factual errors accurately."" Keyphrase: ""Factual error detection shortcomings"""
arXIv2023,SPEED: Speculative Pipelined Execution for Efficient Decoding,Yes.,5,"""Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound.""",2023,2023-10-18T16:07:01Z,"Keyphrase: ""Inference latency and memory-bound constraints""","""Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound."" Keyphrase: ""Inference latency and memory-bound constraints"""
arXIv2023,Emptying the Ocean with a Spoon: Should We Edit Models?,Yes.,5,"""We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations."" and ""We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability,",2023,2023-10-18T13:38:03Z,"Keyphrase: ""Limited trust in direct model editing""","""We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations."" and ""We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, Keyphrase: ""Limited trust in direct model editing"""
arXIv2023,The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models,Yes.,5,"""Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence.""",2023,2023-10-18T11:01:09Z,"Keyphrase: ""Hallucinatory behavior and overconfidence""","""Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence."" Keyphrase: ""Hallucinatory behavior and overconfidence"""
arXIv2023,Solving the multiplication problem of a large language model system using a graph-based method,Yes.,5,"""The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication.""",2023,2023-10-18T08:02:00Z,"Keyphrase: ""Inadequate for arithmetic problem solving""","""The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication."" Keyphrase: ""Inadequate for arithmetic problem solving"""
arXIv2023,SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents,Yes.,5,"""We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.""",2023,2023-10-18T02:27:01Z,"Keyphrase: ""Poor social commonsense reasoning""","""We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills."" Keyphrase: ""Poor social commonsense reasoning"""
arXIv2023,Systematic Assessment of Factual Knowledge in Large Language Models,Yes.,4,"""this approach has limitations regarding factual knowledge coverage,"" and ""We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.""",2023,2023-10-18T00:20:50Z,"Keyphrase: ""Limited factual knowledge coverage""","""this approach has limitations regarding factual knowledge coverage,"" and ""We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context."" Keyphrase: ""Limited factual knowledge coverage"""
arXIv2023,MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations,Yes.,4,"""Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly."" and ""our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example."" and ""our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long",2023,2023-10-18T00:02:38Z,"Keyphrase: ""Limited knowledge generalization""","""Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly."" and ""our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example."" and ""our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long Keyphrase: ""Limited knowledge generalization"""
arXIv2023,"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",Yes.,5,"""Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate.""",2023,2023-10-17T18:18:32Z,"Keyphrase: ""Overreliance on parametric knowledge""","""Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate."" Keyphrase: ""Overreliance on parametric knowledge"""
arXIv2023,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,Yes.,4,"""there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes.""",2023,2023-10-17T18:00:25Z,"Keyphrase: ""Simplistic caricatures""","""there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes."" Keyphrase: ""Simplistic caricatures"""
arXIv2023,Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament,Yes.,5,"""GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts,"" and ""GPT-4 significantly underperforms in real-world predictive tasks compared to median human-crowd forecasts.""",2023,2023-10-17T17:58:17Z,"Keyphrase: ""Underperformance in predictive tasks""","""GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts,"" and ""GPT-4 significantly underperforms in real-world predictive tasks compared to median human-crowd forecasts."" Keyphrase: ""Underperformance in predictive tasks"""
arXIv2023,"Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",Yes.,4,"""LLMs often require adaptation with private data, which poses privacy and security challenges."" and ""there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.""",2023,2023-10-17T17:03:00Z,"Keyphrase: ""Privacy and security challenges""","""LLMs often require adaptation with private data, which poses privacy and security challenges."" and ""there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses."" Keyphrase: ""Privacy and security challenges"""
arXIv2023,DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations,Yes.,4,"""Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typically trained without leveraging multi-modal information.""",2023,2023-10-17T16:15:34Z,"Keyphrase: ""Lack of emotion understanding and multimodal training""","""Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typically trained without leveraging multi-modal information."" Keyphrase: ""Lack of emotion understanding and multimodal training"""
arXIv2023,Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting,Yes.,5,"""We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B.""",2023,2023-10-17T15:03:30Z,"Keyphrase: ""Sensitivity to prompt formatting""","""We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B."" Keyphrase: ""Sensitivity to prompt formatting"""
arXIv2023,Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges,Yes.,4,"""We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges.""",2023,2023-10-17T13:20:16Z,"Keyphrase: ""Challenges in data and linguistic understanding""","""We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges."" Keyphrase: ""Challenges in data and linguistic understanding"""
arXIv2023,The Quo Vadis of the Relationship between Language and Large Language Models,Yes.,4,"""it is not clear that they are in a place to offer insights into the target system they seek to represent"" and ""the most important theoretical and empirical risks brought about by the adoption of scientific models that lack transparency.""",2023,2023-10-17T10:54:24Z,"Keyphrase: ""Lack of transparency""","""it is not clear that they are in a place to offer insights into the target system they seek to represent"" and ""the most important theoretical and empirical risks brought about by the adoption of scientific models that lack transparency."" Keyphrase: ""Lack of transparency"""
arXIv2023,H2O Open Ecosystem for State-of-the-art Large Language Models,Yes.,4,"""However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text.""",2023,2023-10-17T09:40:58Z,"Keyphrase: ""Risk of biased, private, and harmful text""","""However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text."" Keyphrase: ""Risk of biased, private, and harmful text"""
arXIv2023,Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models,Yes.,4,"""These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions.""",2023,2023-10-17T08:56:04Z,"Keyphrase: ""Bias retention in chatbots""","""These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions."" Keyphrase: ""Bias retention in chatbots"""
arXIv2023,Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning,Yes.,4,"""Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content."" and ""We discovered that most models are essentially misaligned, necessitating further ethical value alignment.""",2023,2023-10-17T07:42:40Z,"Keyphrase: ""Ethical alignment issues""","""Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content."" and ""We discovered that most models are essentially misaligned, necessitating further ethical value alignment."" Keyphrase: ""Ethical alignment issues"""
arXIv2023,Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation,Yes.,5,"""Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such",2023,2023-10-17T06:53:00Z,"Keyphrase: ""Surface-level understanding""","""Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such Keyphrase: ""Surface-level understanding"""
arXIv2023,NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain,Yes.,4,"""Our experiments on state-of-the-art models suggest that even the best LLMs perform less than satisfactorily on our benchmark, demonstrating the scientific knowledge gap of existing LLMs.""",2023,2023-10-17T01:27:20Z,"Keyphrase: ""Limited scientific knowledge performance""","""Our experiments on state-of-the-art models suggest that even the best LLMs perform less than satisfactorily on our benchmark, demonstrating the scientific knowledge gap of existing LLMs."" Keyphrase: ""Limited scientific knowledge performance"""
arXIv2023,BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology,Yes.,5,"""However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments.""",2023,2023-10-16T17:54:20Z,"Keyphrase: ""Limited long-term planning""","""However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments."" Keyphrase: ""Limited long-term planning"""
arXIv2023,Data Contamination Through the Lens of Time,Yes.,5,"""Data contamination remains notoriously challenging to measure and mitigate,"" and ""we conduct the first thorough longitudinal analysis of data contamination in LLMs.""",2023,2023-10-16T17:51:29Z,"Keyphrase: ""Challenges with data contamination""","""Data contamination remains notoriously challenging to measure and mitigate,"" and ""we conduct the first thorough longitudinal analysis of data contamination in LLMs."" Keyphrase: ""Challenges with data contamination"""
arXIv2023,Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers,Yes.,5,"""Hallucination plagues even frontier LLMs"" and ""We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and",2023,2023-10-16T17:51:17Z,"Keyphrase: ""Persistent hallucination""","""Hallucination plagues even frontier LLMs"" and ""We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and Keyphrase: ""Persistent hallucination"""
arXIv2023,On Context Utilization in Summarization with Large Language Models,Yes.,5,"""However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source",2023,2023-10-16T16:45:12Z,"Keyphrase: ""Uneven context utilization""","""However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source Keyphrase: ""Uneven context utilization"""
arXIv2023,Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis,Yes.,4,"""This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement.""",2023,2023-10-16T14:59:10Z,"Keyphrase: ""Generation of harmful content""","""This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement."" Keyphrase: ""Generation of harmful content"""
arXIv2023,Stance Detection with Collaborative Role-Infused LLM-Based Agents,Yes.,4,"""Despite their promising capabilities, LLMs encounter challenges when directly applied to stance detection. First, stance detection demands multi-aspect knowledge, from deciphering event-related terminologies to understanding the expression styles in social media platforms. Second, stance detection requires advanced reasoning to infer authors' implicit viewpoints, as stance are often subtly embedded rather than overtly stated in the text.""",2023,2023-10-16T14:46:52Z,"Keyphrase: ""Challenges in stance detection""","""Despite their promising capabilities, LLMs encounter challenges when directly applied to stance detection. First, stance detection demands multi-aspect knowledge, from deciphering event-related terminologies to understanding the expression styles in social media platforms. Second, stance detection requires advanced reasoning to infer authors' implicit viewpoints, as stance are often subtly embedded rather than overtly stated in the text."" Keyphrase: ""Challenges in stance detection"""
arXIv2023,"Privacy in Large Language Models: Attacks, Defenses and Future Directions",Yes.,4,"""unrestricted access to these models can also introduce potential malicious and unintentional privacy risks"" and ""Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved.""",2023,2023-10-16T13:23:54Z,"Keyphrase: ""Privacy risks and unresolved safety concerns""","""unrestricted access to these models can also introduce potential malicious and unintentional privacy risks"" and ""Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved."" Keyphrase: ""Privacy risks and unresolved safety concerns"""
arXIv2023,"Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs",Yes.,5,"""we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks.""",2023,2023-10-16T12:51:24Z,"Keyphrase: ""Sensitivity to noise operations""","""we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks."" Keyphrase: ""Sensitivity to noise operations"""
arXIv2023,Generative Calibration for In-context Learning,Yes.,5,"""the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples"" and ""such a paradox is mainly due to the label shift of the in-context model to the data distribution, in which LLMs shift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.""",2023,2023-10-16T10:45:02Z,"Keyphrase: ""Label shift issues""","""the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples"" and ""such a paradox is mainly due to the label shift of the in-context model to the data distribution, in which LLMs shift the label marginal $p(y)$ while having a good label conditional $p(x|y)$."" Keyphrase: ""Label shift issues"""
arXIv2023,Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT,Yes.,5,"""Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain",2023,2023-10-16T08:34:44Z,"Keyphrase: ""Limited performance in zeroshot settings""","""Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain Keyphrase: ""Limited performance in zeroshot settings"""
arXIv2023,Theory of Mind for Multi-Agent Collaboration via Large Language Models,Yes.,5,"""Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state.""",2023,2023-10-16T07:51:19Z,"Keyphrase: ""Failure in managing long-horizon context""","""Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state."" Keyphrase: ""Failure in managing long-horizon context"""
arXIv2023,Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks,Yes.,5,"""Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications."" and ""Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development.""",2023,2023-10-16T05:19:25Z,"Keyphrase: ""Vulnerability to generating harmful content""","""Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications."" and ""Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development."" Keyphrase: ""Vulnerability to generating harmful content"""
arXIv2023,FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models,Yes.,5,"""LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises.""",2023,2023-10-16T04:17:13Z,"Keyphrase: ""High resource consumption and data scarcity""","""LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises."" Keyphrase: ""High resource consumption and data scarcity"""
arXIv2023,Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis,Yes.,5,"""while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing",2023,2023-10-15T18:32:27Z,"Keyphrase: ""Limited medical expertise""","""while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing Keyphrase: ""Limited medical expertise"""
arXIv2023,In-Context Learning with Iterative Demonstration Selection,Yes.,4,"""However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem.""",2023,2023-10-15T16:40:19Z,"Keyphrase: ""Limited few-shot performance""","""However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem."" Keyphrase: ""Limited few-shot performance"""
arXIv2023,Assessing the Reliability of Large Language Model Knowledge,Yes.,5,"""LLMs are typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability.""",2023,2023-10-15T12:40:30Z,"Keyphrase: ""Limited evaluation metrics""","""LLMs are typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability."" Keyphrase: ""Limited evaluation metrics"""
arXIv2023,When can transformers reason with abstract symbols?,Yes.,5,"""transformers fail to generalize as their embedding dimension increases"" and ""require astonishingly large quantities of training data.""",2023,2023-10-15T06:45:38Z,"Keyphrase: ""Limited generalization with increased embedding dimension""","""transformers fail to generalize as their embedding dimension increases"" and ""require astonishingly large quantities of training data."" Keyphrase: ""Limited generalization with increased embedding dimension"""
arXIv2023,DPZero: Private Fine-Tuning of Language Models without Backpropagation,Yes.,5,"""The widespread practice of fine-tuning large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continues to grow, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize training data, it is important to protect",2023,2023-10-14T18:42:56Z,"Keyphrase: ""Memory and privacy challenges""","""The widespread practice of fine-tuning large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continues to grow, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize training data, it is important to protect Keyphrase: ""Memory and privacy challenges"""
arXIv2023,Autonomous Tree-search Ability of Large Language Models,Yes.,5,"""Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making."" and ""there are several fundamental limitations of these approaches.""",2023,2023-10-14T14:14:38Z,"Keyphrase: ""Limited strategic foresight""","""Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making."" and ""there are several fundamental limitations of these approaches."" Keyphrase: ""Limited strategic foresight"""
arXIv2023,CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering,Yes.,5,"""leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe",2023,2023-10-14T08:46:24Z,"Keyphrase: ""Hallucination in domain-specific question answering""","""leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe Keyphrase: ""Hallucination in domain-specific question answering"""
arXIv2023,User Inference Attacks on Large Language Models,Yes.,5,"""We find that LLMs are susceptible to user inference across a variety of fine-tuning datasets, at times with near perfect attack success rates.""",2023,2023-10-13T17:24:52Z,"Keyphrase: ""Susceptible to user inference""","""We find that LLMs are susceptible to user inference across a variety of fine-tuning datasets, at times with near perfect attack success rates."" Keyphrase: ""Susceptible to user inference"""
arXIv2023,Table-GPT: Table-tuned GPT for Diverse Table Tasks,Yes.,5,"""we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on one-dimensional natural-language texts, whereas relational tables are two-dimensional objects.""",2023,2023-10-13T17:20:56Z,"Keyphrase: ""Limited performance on table-related tasks""","""we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on one-dimensional natural-language texts, whereas relational tables are two-dimensional objects."" Keyphrase: ""Limited performance on table-related tasks"""
arXIv2023,"""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases in LLM-Generated Reference Letters",Yes.,5,"""In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions",2023,2023-10-13T16:12:57Z,"Keyphrase: ""Gender bias in generated content""","""In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions Keyphrase: ""Gender bias in generated content"""
arXIv2023,KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment.""",2023,2023-10-13T12:12:34Z,"Keyphrase: ""Misinformation generation""","""Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment."" Keyphrase: ""Misinformation generation"""
arXIv2023,"""Im not Racist but..."": Discovering Bias in the Internal Knowledge of Large Language Models",Yes.,4,"""these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications.""",2023,2023-10-13T00:03:37Z,"Keyphrase: ""Inherent societal bias""","""these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications."" Keyphrase: ""Inherent societal bias"""
arXIv2023,Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving,Yes.,5,"""Analysis of the model's incorrect solutions revealed three distinct failure modes",2023,2023-10-12T23:39:28Z,"Keyphrase: ""Distinct failure modes""","""Analysis of the model's incorrect solutions revealed three distinct failure modes Keyphrase: ""Distinct failure modes"""
arXIv2023,MemGPT: Towards LLMs as Operating Systems,Yes.,5,"""Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis.""",2023,2023-10-12T17:51:32Z,"Keyphrase: ""Context window limitation""","""Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis."" Keyphrase: ""Context window limitation"""
arXIv2023,Jailbreaking Black Box Large Language Models in Twenty Queries,Yes.,4,"""However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse.""",2023,2023-10-12T15:38:28Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse."" Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Impact of Co-occurrence on Factual Knowledge of Large Language Models,Yes.,5,"""Large language models (LLMs) often make factually incorrect responses despite their success in various applications."" and ""We show that co-occurrence bias remains despite scaling up model sizes or finetuning.""",2023,2023-10-12T12:01:32Z,"Keyphrase: ""Persistent cooccurrence bias""","""Large language models (LLMs) often make factually incorrect responses despite their success in various applications."" and ""We show that co-occurrence bias remains despite scaling up model sizes or finetuning."" Keyphrase: ""Persistent cooccurrence bias"""
arXIv2023,Can Large Language Models Really Improve by Self-critiquing Their Own Plans?,Yes.,5,"""our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability.""",2023,2023-10-12T08:22:37Z,"Keyphrase: ""Reliability compromised by false positives""","""our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability."" Keyphrase: ""Reliability compromised by false positives"""
arXIv2023,QASiNa: Religious Domain Question Answering using Sirah Nabawiyah,Yes.,5,"""The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam."" and ""The experiment indicate that Chat GPT tends to give excessive interpretations as evidenced by its higher Substring Match scores compared to EM and F1-Score, even after providing instruction and",2023,2023-10-12T07:52:19Z,"Keyphrase: ""Excessive interpretation""","""The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam."" and ""The experiment indicate that Chat GPT tends to give excessive interpretations as evidenced by its higher Substring Match scores compared to EM and F1-Score, even after providing instruction and Keyphrase: ""Excessive interpretation"""
arXIv2023,GameGPT: Multi-agent Collaborative Framework for Game Development,Yes.,4,"""While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern",2023,2023-10-12T06:31:43Z,"Keyphrase: ""Hallucination issues""","""While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern Keyphrase: ""Hallucination issues"""
arXIv2023,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",Yes.,4,"""it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values,"" and ""we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.""",2023,2023-10-11T16:18:13Z,"Keyphrase: ""Challenges in feedback incorporation""","""it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values,"" and ""we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges."" Keyphrase: ""Challenges in feedback incorporation"""
arXIv2023,"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",Yes.,5,"""We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts,"" and ""highlighting the potential consequences and challenges posed by factual errors in LLM outputs.""",2023,2023-10-11T14:18:03Z,"Keyphrase: ""Factual inconsistency""","""We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts,"" and ""highlighting the potential consequences and challenges posed by factual errors in LLM outputs."" Keyphrase: ""Factual inconsistency"""
arXIv2023,How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances,Yes.,4,"""Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era.""",2023,2023-10-11T09:46:32Z,"Keyphrase: ""Difficulty in maintaining up-to-date information""","""Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era."" Keyphrase: ""Difficulty in maintaining up-to-date information"""
arXIv2023,Beyond Memorization: Violating Privacy Via Inference with Large Language Models,Yes.,5,"""we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text,"" and ""common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference.""",2023,2023-10-11T08:32:46Z,"Keyphrase: ""Inference of personal attributes""","""we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text,"" and ""common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference."" Keyphrase: ""Inference of personal attributes"""
arXIv2023,Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding,Yes.,4,"""existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls.""",2023,2023-10-10T23:37:53Z,"Keyphrase: ""Limited generalization to new tasks/tools""","""existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls."" Keyphrase: ""Limited generalization to new tasks/tools"""
arXIv2023,Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation,Yes.,5,"""Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs.""",2023,2023-10-10T20:15:54Z,"Keyphrase: ""Alignment failures""","""Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs."" Keyphrase: ""Alignment failures"""
arXIv2023,LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression,Yes.,5,"""In long context scenarios, large language models (LLMs) face three main challenges",2023,2023-10-10T17:59:58Z,"Keyphrase: ""Challenges with long context""","""In long context scenarios, large language models (LLMs) face three main challenges Keyphrase: ""Challenges with long context"""
arXIv2023,Teaching Language Models to Hallucinate Less with Synthetic Tasks,Yes.,5,"""Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context.""",2023,2023-10-10T17:57:00Z,"Keyphrase: ""Frequent hallucinations in summarization tasks""","""Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context."" Keyphrase: ""Frequent hallucinations in summarization tasks"""
arXIv2023,The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets,Yes.,4,"""Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods."" and ""this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues.""",2023,2023-10-10T17:54:39Z,"Keyphrase: ""Tendency to output falsehoods""","""Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods."" and ""this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues."" Keyphrase: ""Tendency to output falsehoods"""
arXIv2023,Exploring Memorization in Fine-tuned Language Models,Yes.,5,"""Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns.""",2023,2023-10-10T15:41:26Z,"Keyphrase: ""Memorization and privacy concerns""","""Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns."" Keyphrase: ""Memorization and privacy concerns"""
arXIv2023,MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents,Yes.,4,"""Despite this, their capacities to coordinate within task-oriented social contexts are under-explored."" and ""However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks.""",2023,2023-10-10T10:17:58Z,"Keyphrase: ""Limited coordination in complex tasks""","""Despite this, their capacities to coordinate within task-oriented social contexts are under-explored."" and ""However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks."" Keyphrase: ""Limited coordination in complex tasks"""
arXIv2023,A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection,Yes.,5,"""LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks.""",2023,2023-10-10T10:14:59Z,"Keyphrase: ""Generation of hallucinations""","""LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks."" Keyphrase: ""Generation of hallucinations"""
arXIv2023,Multilingual Jailbreak Challenges in Large Language Models,Yes.,5,"""While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the `jailbreak' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior."" and ""The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically,",2023,2023-10-10T09:44:06Z,"Keyphrase: ""Safety concerns and unintended behaviors""","""While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the `jailbreak' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior."" and ""The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, Keyphrase: ""Safety concerns and unintended behaviors"""
arXIv2023,Towards Mitigating Hallucination in Large Language Models via Self-Reflection,Yes.,5,"""However, the practical deployment still faces challenges, notably the issue of 'hallucination', where models generate plausible-sounding but unfaithful or nonsensical information.""",2023,2023-10-10T03:05:44Z,"Keyphrase: ""Hallucination of nonsensical information""","""However, the practical deployment still faces challenges, notably the issue of 'hallucination', where models generate plausible-sounding but unfaithful or nonsensical information."" Keyphrase: ""Hallucination of nonsensical information"""
arXIv2023,Compressing Context to Enhance Inference Efficiency of Large Language Models,Yes.,5,"""However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length.""",2023,2023-10-09T23:03:24Z,"Keyphrase: ""Challenges with long documents and extended conversations""","""However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length."" Keyphrase: ""Challenges with long documents and extended conversations"""
arXIv2023,SALMON: Self-Alignment with Instructable Reward Models,Yes.,4,"""a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences.""",2023,2023-10-09T17:56:53Z,"Keyphrase: ""Dependency on high-quality human annotation""","""a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences."" Keyphrase: ""Dependency on high-quality human annotation"""
arXIv2023,ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models,Yes.,4,"""However, we identify a challenge with VLMs' passive perception, which often misses crucial context information, leading to incorrect or uncertain reasoning by LLMs.""",2023,2023-10-09T17:10:35Z,"Keyphrase: ""Misses crucial context""","""However, we identify a challenge with VLMs' passive perception, which often misses crucial context information, leading to incorrect or uncertain reasoning by LLMs."" Keyphrase: ""Misses crucial context"""
arXIv2023,HyperAttention: Long-context Attention in Near-Linear Time,Yes.,4,"""We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs)."" and ""Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank.""",2023,2023-10-09T17:05:25Z,"Keyphrase: ""Computational complexity with long context""","""We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs)."" and ""Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank."" Keyphrase: ""Computational complexity with long context"""
arXIv2023,GraphLLM: Boosting Graph Reasoning Ability of Large Language Model,Yes.,4,"""Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks.""",2023,2023-10-09T16:42:00Z,"Keyphrase: ""Underwhelming graph reasoning performance""","""Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks."" Keyphrase: ""Underwhelming graph reasoning performance"""
arXIv2023,SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese,Yes.,4,"""they can also produce harmful content that negatively affects societal perceptions"" and ""Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods.""",2023,2023-10-09T16:03:22Z,"Keyphrase: ""Harmful content generation""","""they can also produce harmful content that negatively affects societal perceptions"" and ""Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods."" Keyphrase: ""Harmful content generation"""
arXIv2023,"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",Yes.,4,"""highlighting both the strengths and limitations"" and ""the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics.""",2023,2023-10-09T13:15:23Z,"Keyphrase: ""Ethical concerns in healthcare deployment""","""highlighting both the strengths and limitations"" and ""the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics."" Keyphrase: ""Ethical concerns in healthcare deployment"""
arXIv2023,LAiW: A Chinese Legal Large Language Models Benchmark,Yes.,4,"""current evaluations of these LLMs in LegalAI are defined by the experts of computer science, lacking consistency with the logic of legal practice, making it difficult to judge their practical capabilities"" and ""LLMs seem to be able to directly acquire complex legal application capabilities but perform poorly in some basic tasks, which may pose obstacles to their practical application and acceptance by legal experts.""",2023,2023-10-09T11:19:55Z,"Keyphrase: ""Limited legal domain expertise""","""current evaluations of these LLMs in LegalAI are defined by the experts of computer science, lacking consistency with the logic of legal practice, making it difficult to judge their practical capabilities"" and ""LLMs seem to be able to directly acquire complex legal application capabilities but perform poorly in some basic tasks, which may pose obstacles to their practical application and acceptance by legal experts."" Keyphrase: ""Limited legal domain expertise"""
arXIv2023,Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization,Yes.,5,"""We also find that MuggleMath is weak in out-of-domain math reasoning generalization to MATH. This is attributed to the differences in query distribution between AugGSM8K and MATH which suggest that augmentation on a single benchmark could not help with overall math reasoning performance.""",2023,2023-10-09T08:18:58Z,"Keyphrase: ""Weak out-of-domain generalization""","""We also find that MuggleMath is weak in out-of-domain math reasoning generalization to MATH. This is attributed to the differences in query distribution between AugGSM8K and MATH which suggest that augmentation on a single benchmark could not help with overall math reasoning performance."" Keyphrase: ""Weak out-of-domain generalization"""
arXIv2023,SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF,Yes.,5,"""RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time.""",2023,2023-10-09T02:11:21Z,"Keyphrase: ""Lack of user control over implicit values""","""RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time."" Keyphrase: ""Lack of user control over implicit values"""
arXIv2023,Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models,Yes.,5,"""Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects."" and ""no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy",2023,2023-10-09T01:52:27Z,"Keyphrase: ""Object hallucination""","""Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects."" and ""no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy Keyphrase: ""Object hallucination"""
arXIv2023,Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems,Yes.,5,"""Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations,"" and ""our study uncovers significant persona biases in dialogue systems.""",2023,2023-10-08T21:03:18Z,"Keyphrase: ""Persona bias""","""Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations,"" and ""our study uncovers significant persona biases in dialogue systems."" Keyphrase: ""Persona bias"""
arXIv2023,Measuring reasoning capabilities of ChatGPT,Yes.,5,"""I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks."" and ""A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models.""",2023,2023-10-08T20:18:50Z,"Keyphrase: ""Logical reasoning faults""","""I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks."" and ""A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models."" Keyphrase: ""Logical reasoning faults"""
arXIv2023,Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback,Yes.,5,"""we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs.""",2023,2023-10-08T15:14:39Z,"Keyphrase: ""Length bias leading to misleading outputs""","""we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs."" Keyphrase: ""Length bias leading to misleading outputs"""
arXIv2023,Factuality Challenges in the Era of Large Language Models,Yes.,5,"""These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as 'hallucinations.'"" and ""Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding",2023,2023-10-08T14:55:02Z,"Keyphrase: ""Propensity for hallucination""","""These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as 'hallucinations.'"" and ""Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding Keyphrase: ""Propensity for hallucination"""
arXIv2023,Do Large Language Models Know about Facts?,Yes.,5,"""Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time"" and ""Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations.""",2023,2023-10-08T14:26:55Z,"Keyphrase: ""Lack of factual knowledge""","""Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time"" and ""Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations."" Keyphrase: ""Lack of factual knowledge"""
arXIv2023,An Investigation of LLMs' Inefficacy in Understanding Converse Relations,Yes.,5,"""The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.""",2023,2023-10-08T13:45:05Z,"Keyphrase: ""Shortcut learning and benchmark challenges""","""The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark."" Keyphrase: ""Shortcut learning and benchmark challenges"""
arXIv2023,MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models,Yes.,5,"""The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions.""",2023,2023-10-08T13:19:52Z,"Keyphrase: ""Temporal bias vulnerability""","""The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions."" Keyphrase: ""Temporal bias vulnerability"""
arXIv2023,Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT,Yes.,4,"""this introduces issues of bias on protected attributes like gender, race and maternity status"" and ""We use contrastive input decoding on open-source LLMs to uncover potential sources of bias.""",2023,2023-10-08T12:08:48Z,"Keyphrase: ""Bias in protected attributes""","""this introduces issues of bias on protected attributes like gender, race and maternity status"" and ""We use contrastive input decoding on open-source LLMs to uncover potential sources of bias."" Keyphrase: ""Bias in protected attributes"""
arXIv2023,Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,Yes.,5,"""However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge.""",2023,2023-10-08T06:18:14Z,"Keyphrase: ""Limited context understanding""","""However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge."" Keyphrase: ""Limited context understanding"""
arXIv2023,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,Yes.,5,"""Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture.""",2023,2023-10-07T21:49:38Z,"Keyphrase: ""Limited cultural and language knowledge""","""Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture."" Keyphrase: ""Limited cultural and language knowledge"""
arXIv2023,Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM,Yes.,4,"""Large Language Models (LLMs) pose significant hardware challenges related to memory requirements and computational ability.""",2023,2023-10-07T14:50:28Z,"Keyphrase: ""Hardware challenges""","""Large Language Models (LLMs) pose significant hardware challenges related to memory requirements and computational ability."" Keyphrase: ""Hardware challenges"""
arXIv2023,Critique Ability of Large Language Models,Yes.,5,"""Critique is generally challenging for most LLMs, and this capability often emerges only when models are sufficiently large. (2) In particular, self-critique is especially difficult. Even top-performing LLMs struggle to achieve satisfactory performance. (3) Models tend to have lower critique accuracy on problems where they are most uncertain.""",2023,2023-10-07T14:12:15Z,"Keyphrase: ""Challenges in self-critique accuracy""","""Critique is generally challenging for most LLMs, and this capability often emerges only when models are sufficiently large. (2) In particular, self-critique is especially difficult. Even top-performing LLMs struggle to achieve satisfactory performance. (3) Models tend to have lower critique accuracy on problems where they are most uncertain."" Keyphrase: ""Challenges in self-critique accuracy"""
arXIv2023,Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning,Yes.,5,"""However, these models often face the challenge of 'hallucination,' which undermines their reliability."" and ""Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations.""",2023,2023-10-07T12:06:53Z,"Keyphrase: ""Hallucination challenge""","""However, these models often face the challenge of 'hallucination,' which undermines their reliability."" and ""Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations."" Keyphrase: ""Hallucination challenge"""
arXIv2023,The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning,Yes.,5,"""Reducing the model size by more than 30\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training.""",2023,2023-10-07T03:36:39Z,"Keyphrase: ""Decreased recall ability""","""Reducing the model size by more than 30\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training."" Keyphrase: ""Decreased recall ability"""
arXIv2023,Amortizing intractable inference in large language models,Yes.,5,"""This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions.""",2023,2023-10-06T16:36:08Z,"Keyphrase: ""Intractable sampling tasks""","""This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions."" Keyphrase: ""Intractable sampling tasks"""
arXIv2023,Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface,Yes.,5,"""Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation."" and ""complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly.""",2023,2023-10-06T12:44:04Z,"Keyphrase: ""Incomplete knowledge retrieval""","""Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation."" and ""complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly."" Keyphrase: ""Incomplete knowledge retrieval"""
arXIv2023,Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models,Yes.,5,"""Findings indicate that while these models achieved moderate success on standard QA benchmarks, their performance notably declines when assessed on RRIP tasks. The study not only highlights the limitations of current LLMs in handling redundant information but also suggests that future training of these models should focus on incorporating redundant information into the training data to increase the performance on RRIP tasks.""",2023,2023-10-06T06:20:06Z,"Keyphrase: ""Struggles with redundant information""","""Findings indicate that while these models achieved moderate success on standard QA benchmarks, their performance notably declines when assessed on RRIP tasks. The study not only highlights the limitations of current LLMs in handling redundant information but also suggests that future training of these models should focus on incorporating redundant information into the training data to increase the performance on RRIP tasks."" Keyphrase: ""Struggles with redundant information"""
arXIv2023,Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models,Yes.,5,"""The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis.""",2023,2023-10-06T05:40:23Z,"Keyphrase: ""Contextual deficiency""","""The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis."" Keyphrase: ""Contextual deficiency"""
arXIv2023,From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self,Yes.,4,"""However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology.""",2023,2023-10-06T02:19:10Z,"Keyphrase: ""Inauthentic and excessive responses""","""However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology."" Keyphrase: ""Inauthentic and excessive responses"""
arXIv2023,Quantized Transformer Language Model Implementations on Edge Devices,Yes.,5,"""One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency.""",2023,2023-10-06T01:59:19Z,"Keyphrase: ""Resource-intensive deployment""","""One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency."" Keyphrase: ""Resource-intensive deployment"""
arXIv2023,Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations,Yes.,5,"""However, LLMs are prone to generate hallucinations that are not supported by the provided sources.""",2023,2023-10-06T00:10:46Z,"Keyphrase: ""Prone to hallucination""","""However, LLMs are prone to generate hallucinations that are not supported by the provided sources."" Keyphrase: ""Prone to hallucination"""
arXIv2023,LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models,Yes.,4,"""However, results on Coordination QA show a large room for improvement in the Theory of Mind reasoning and joint planning abilities of LLMs.""",2023,2023-10-05T21:18:15Z,"Keyphrase: ""Limited theory of mind reasoning""","""However, results on Coordination QA show a large room for improvement in the Theory of Mind reasoning and joint planning abilities of LLMs."" Keyphrase: ""Limited theory of mind reasoning"""
arXIv2023,"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",Yes.,5,"""We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users."" and ""Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples."" and ""simply fine-t",2023,2023-10-05T17:12:17Z,"Keyphrase: ""Safety alignment restrictions""","""We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users."" and ""Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples."" and ""simply fine-t Keyphrase: ""Safety alignment restrictions"""
arXIv2023,Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures,Yes.,4,"""However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations.""",2023,2023-10-05T16:37:29Z,"Keyphrase: ""Complex interconnected task demands""","""However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations."" Keyphrase: ""Complex interconnected task demands"""
arXIv2023,Redefining Digital Health Interfaces with Large Language Models,Yes.,5,"""Directly applying LLMs in clinical settings is not straightforward, however, with LLMs susceptible to providing inconsistent or nonsensical answers."" and ""addressing current issues with using LLMs in clinical settings such as hallucinations.""",2023,2023-10-05T14:18:40Z,"Keyphrase: ""Inconsistent and nonsensical answers""","""Directly applying LLMs in clinical settings is not straightforward, however, with LLMs susceptible to providing inconsistent or nonsensical answers."" and ""addressing current issues with using LLMs in clinical settings such as hallucinations."" Keyphrase: ""Inconsistent and nonsensical answers"""
arXIv2023,Evaluating Hallucinations in Chinese Large Language Models,Yes.,5,"""We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models.""",2023,2023-10-05T07:57:09Z,"Keyphrase: ""Hallucination prioritization""","""We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models."" Keyphrase: ""Hallucination prioritization"""
arXIv2023,Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise,Yes.,5,"""they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.""",2023,2023-10-05T05:55:06Z,"Keyphrase: ""Domain-specific knowledge deficiency""","""they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data."" Keyphrase: ""Domain-specific knowledge deficiency"""
arXIv2023,Benchmarking Large Language Models As AI Research Agents,Yes.,5,"""Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination.""",2023,2023-10-05T04:06:12Z,"Keyphrase: ""Challenges in long-term planning""","""Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination."" Keyphrase: ""Challenges in long-term planning"""
arXIv2023,A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions,Yes.,5,"""However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete,",2023,2023-10-05T03:45:54Z,"Keyphrase: ""Mismatched user intention alignment and incomplete domain knowledge""","""However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete, Keyphrase: ""Mismatched user intention alignment and incomplete domain knowledge"""
arXIv2023,A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores,Yes.,5,"""Despite their impressive performance, the models are known to pose important risks."" and ""a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed.""",2023,2023-10-05T03:20:41Z,"Keyphrase: ""Risk of systematic understanding""","""Despite their impressive performance, the models are known to pose important risks."" and ""a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed."" Keyphrase: ""Risk of systematic understanding"""
arXIv2023,Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,Yes.,5,"""they still face limitations in scenarios that demand long-term planning and spatial reasoning"" and ""it still fails to perform long-term temporal reasoning"" and ""fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles.""",2023,2023-10-05T01:42:16Z,"Keyphrase: ""Limited long-term reasoning""","""they still face limitations in scenarios that demand long-term planning and spatial reasoning"" and ""it still fails to perform long-term temporal reasoning"" and ""fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles."" Keyphrase: ""Limited long-term reasoning"""
arXIv2023,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Yes.,5,"""Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement",2023,2023-10-05T00:04:12Z,"Keyphrase: ""Room for improvement in human evaluation""","""Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement Keyphrase: ""Room for improvement in human evaluation"""
arXIv2023,Misusing Tools in Large Language Models With Visual Adversarial Examples,Yes.,5,"""These new capabilities bring new benefits and also new security risks."" and ""In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage.""",2023,2023-10-04T22:10:01Z,"""Vulnerability to visual adversarial attacks""","""These new capabilities bring new benefits and also new security risks."" and ""In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage."" ""Vulnerability to visual adversarial attacks"""
arXIv2023,From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference,Yes.,4,"""However, these models carry significant computational challenges, especially the compute and energy costs required for inference.""",2023,2023-10-04T17:41:59Z,"Keyphrase: ""High computational cost""","""However, these models carry significant computational challenges, especially the compute and energy costs required for inference."" Keyphrase: ""High computational cost"""
arXIv2023,"JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning",Yes.,4,"""prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks.""",2023,2023-10-04T16:44:23Z,"Keyphrase: ""Ambiguity and lack of explicit structure""","""prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks."" Keyphrase: ""Ambiguity and lack of explicit structure"""
arXIv2023,Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models,Yes.,5,"""To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour,",2023,2023-10-04T16:39:31Z,"Keyphrase: ""Vulnerability to malicious attacks""","""To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, Keyphrase: ""Vulnerability to malicious attacks"""
arXIv2023,Assessing Large Language Models on Climate Information,Yes.,4,"""Our framework discerns up to 30 distinct issues in model outputs"" and ""shedding light on both the potential and the limitations of LLMs in the realm of climate communication.""",2023,2023-10-04T16:09:48Z,"Keyphrase: ""Limited discernment of complex issues""","""Our framework discerns up to 30 distinct issues in model outputs"" and ""shedding light on both the potential and the limitations of LLMs in the realm of climate communication."" Keyphrase: ""Limited discernment of complex issues"""
arXIv2023,How FaR Are Large Language Models From Agents with Theory-of-Mind?,Yes.,5,"""LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action."" and ""Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D",2023,2023-10-04T06:47:58Z,"Keyphrase: ""Challenges in implicit inference""","""LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action."" and ""Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D Keyphrase: ""Challenges in implicit inference"""
arXIv2023,NOLA: Networks as Linear Combination of Low Rank Random Basis,Yes.,4,"""However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3)."" and ""Yet, these methods face two primary limitations",2023,2023-10-04T03:30:24Z,"Keyphrase: ""Impractical finetuning for downstream tasks""","""However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3)."" and ""Yet, these methods face two primary limitations Keyphrase: ""Impractical finetuning for downstream tasks"""
arXIv2023,Low-Resource Languages Jailbreak GPT-4,Yes.,5,"""Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data,"" and ""this deficiency now poses a risk to all LLMs users.""",2023,2023-10-03T21:30:56Z,"Keyphrase: ""Crosslingual vulnerability and safety deficiencies""","""Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data,"" and ""this deficiency now poses a risk to all LLMs users."" Keyphrase: ""Crosslingual vulnerability and safety deficiencies"""
arXIv2023,Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions,Yes.,5,"""Our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers.""",2023,2023-10-03T21:19:50Z,"Keyphrase: ""Struggles with specific knowledge and misconceptions""","""Our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers."" Keyphrase: ""Struggles with specific knowledge and misconceptions"""
arXIv2023,Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions,Yes.,5,"""However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content)."" and ""Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate",2023,2023-10-03T20:54:29Z,"Keyphrase: ""Inaccurate responses and toxic content""","""However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content)."" and ""Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate Keyphrase: ""Inaccurate responses and toxic content"""
arXIv2023,AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models,Yes.,4,"""However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs.""",2023,2023-10-03T19:44:37Z,"Keyphrase: ""Susceptibility to jailbreak attacks""","""However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs."" Keyphrase: ""Susceptibility to jailbreak attacks"""
arXIv2023,Investigating Large Language Models' Perception of Emotion Using Appraisal Theory,Yes.,4,"""The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data. The magnitude of their responses is also quite different from humans in several variables. We also found that GPTs can be quite sensitive to instruction and how questions are asked.""",2023,2023-10-03T16:34:47Z,"Keyphrase: ""Limited emotional intelligence""","""The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data. The magnitude of their responses is also quite different from humans in several variables. We also found that GPTs can be quite sensitive to instruction and how questions are asked."" Keyphrase: ""Limited emotional intelligence"""
arXIv2023,Unveiling the Pitfalls of Knowledge Editing for Large Language Models,Yes.,5,"""Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works.""",2023,2023-10-03T15:10:46Z,"Keyphrase: ""Unintended consequences of knowledge editing""","""Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works."" Keyphrase: ""Unintended consequences of knowledge editing"""
arXIv2023,Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems,Yes.,5,"""Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2).""",2023,2023-10-03T12:03:06Z,"Keyphrase: ""Backward reasoning accuracy drop""","""Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2)."" Keyphrase: ""Backward reasoning accuracy drop"""
arXIv2023,Large Language Models Cannot Self-Correct Reasoning Yet,Yes.,5,"""our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction.""",2023,2023-10-03T04:56:12Z,"Keyphrase: ""Struggles with self-correction""","""our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction."" Keyphrase: ""Struggles with self-correction"""
arXIv2023,HallE-Control: Controlling Object Hallucination in Large Multimodal Models,Yes.,5,"""Interestingly, while LMMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations."" and ""Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can",2023,2023-10-03T04:01:27Z,"Keyphrase: ""Object existence hallucination""","""Interestingly, while LMMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations."" and ""Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can Keyphrase: ""Object existence hallucination"""
arXIv2023,Can GPT-4 Replicate Empirical Software Engineering Research?,Yes.,4,"""We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering",2023,2023-10-03T01:27:23Z,"Keyphrase: ""Implementation-level errors""","""We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering Keyphrase: ""Implementation-level errors"""
arXIv2023,PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels,Yes.,5,"""The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models.""",2023,2023-10-02T21:39:04Z,"Keyphrase: ""Quadratic memory complexity""","""The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models."" Keyphrase: ""Quadratic memory complexity"""
arXIv2023,On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?,Yes.,5,"""many existing studies have shown that they could be misused to generate undesired content"" and ""we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs.""",2023,2023-10-02T19:22:01Z,"Keyphrase: ""Potential for generating undesired content""","""many existing studies have shown that they could be misused to generate undesired content"" and ""we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs."" Keyphrase: ""Potential for generating undesired content"""
arXIv2023,"LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",Yes.,5,"""However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception.""",2023,2023-10-02T17:01:56Z,"Keyphrase: ""Hallucination and fabrication""","""However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception."" Keyphrase: ""Hallucination and fabrication"""
arXIv2023,Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation,Yes.,5,"""This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes."" and ""Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.""",2023,2023-10-02T16:27:36Z,"Keyphrase: ""Susceptibility to malicious manipulation""","""This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes."" and ""Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research."" Keyphrase: ""Susceptibility to malicious manipulation"""
arXIv2023,Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models,Yes.,5,"""Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more.""",2023,2023-10-02T15:43:53Z,"Keyphrase: ""Limited geometric reasoning""","""Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more."" Keyphrase: ""Limited geometric reasoning"""
arXIv2023,Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models,Yes.,4,"""Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability."" and ""A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp.""",2023,2023-10-02T10:35:23Z,"Keyphrase: ""Lack of explainability in complex reasoning tasks""","""Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability."" and ""A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp."" Keyphrase: ""Lack of explainability in complex reasoning tasks"""
arXIv2023,Resolving Knowledge Conflicts in Large Language Models,Yes.,5,"""Extensive experiments with the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information.""",2023,2023-10-02T06:57:45Z,"Keyphrase: ""Struggles with conflicting information""","""Extensive experiments with the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information."" Keyphrase: ""Struggles with conflicting information"""
arXIv2023,All Languages Matter: On the Multilingual Safety of Large Language Models,Yes.,4,"""Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages.""",2023,2023-10-02T05:23:34Z,"Keyphrase: ""Unsafe responses to non-English queries""","""Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages."" Keyphrase: ""Unsafe responses to non-English queries"""
arXIv2023,Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications,Yes.,5,"""Compressing Large Language Models (LLMs) often leads to reduced performance, especially for knowledge-intensive tasks."" and ""We start by proposing two conjectures on the nature of the damage",2023,2023-10-02T03:12:06Z,"Keyphrase: ""Reduced performance in knowledge-intensive tasks""","""Compressing Large Language Models (LLMs) often leads to reduced performance, especially for knowledge-intensive tasks."" and ""We start by proposing two conjectures on the nature of the damage Keyphrase: ""Reduced performance in knowledge-intensive tasks"""
arXIv2023,BooookScore: A systematic exploration of book-length summarization in the era of LLMs,Yes.,4,"""identify eight common types of coherence errors made by LLMs.""",2023,2023-10-01T20:46:44Z,"Keyphrase: ""Coherence errors""","""identify eight common types of coherence errors made by LLMs."" Keyphrase: ""Coherence errors"""
arXIv2023,FELM: Benchmarking Factuality Evaluation of Large Language Models,Yes.,5,"""Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.""",2023,2023-10-01T17:37:31Z,"Keyphrase: ""Limited factual accuracy""","""Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors."" Keyphrase: ""Limited factual accuracy"""
arXIv2023,GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models,Yes.,5,"""we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing"" and ""This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful",2023,2023-10-01T17:25:56Z,"Keyphrase: ""Risk of misuse and harmful implications""","""we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing"" and ""This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful Keyphrase: ""Risk of misuse and harmful implications"""
arXIv2023,Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning,Yes.,5,"""However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks."" and ""Our evaluation on these axes reveals major flaws in LMMs.""",2023,2023-10-01T12:02:59Z,"Keyphrase: ""Limited evaluation benchmarks""","""However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks."" and ""Our evaluation on these axes reveals major flaws in LMMs."" Keyphrase: ""Limited evaluation benchmarks"""
arXIv2023,Measuring Value Understanding in Language Models through Discriminator-Critique Gap,Yes.,4,"""Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values"" and ""This may further suggest that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks.""",2023,2023-09-30T13:47:55Z,"Keyphrase: ""Lack of true understanding""","""Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values"" and ""This may further suggest that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks."" Keyphrase: ""Lack of true understanding"""
arXIv2023,Understanding In-Context Learning from Repetitions,Yes.,5,"""our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures"" and ""providing a fresh perspective on this exciting capability.""",2023,2023-09-30T08:13:49Z,"Keyphrase: ""Limited understanding of internal workings""","""our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures"" and ""providing a fresh perspective on this exciting capability."" Keyphrase: ""Limited understanding of internal workings"""
arXIv2023,AutoHall: Automated Hallucination Dataset Generation for Large Language Models,Yes.,5,"""the detection of non-factual or hallucinatory content generated by LLMs remains scarce"" and ""variations in hallucination proportions and types among different models.""",2023,2023-09-30T05:20:02Z,"Keyphrase: ""Variation in hallucinatory content""","""the detection of non-factual or hallucinatory content generated by LLMs remains scarce"" and ""variations in hallucination proportions and types among different models."" Keyphrase: ""Variation in hallucinatory content"""
arXIv2023,"Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs ""Difficult"" Downstream Tasks in LLMs",Yes.,5,"""we reveal that these seemingly inconsequential weights can result in irreparable loss of knowledge and performance degradation in difficult tasks, even when downstream continual training is allowed.""",2023,2023-09-29T22:55:06Z,"Keyphrase: ""Loss of knowledge and performance degradation""","""we reveal that these seemingly inconsequential weights can result in irreparable loss of knowledge and performance degradation in difficult tasks, even when downstream continual training is allowed."" Keyphrase: ""Loss of knowledge and performance degradation"""
arXIv2023,Efficient Streaming Language Models with Attention Sinks,Yes.,5,"""Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.""",2023,2023-09-29T17:59:56Z,"Keyphrase: ""Memory consumption and text length limitation""","""Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length."" Keyphrase: ""Memory consumption and text length limitation"""
arXIv2023,Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks,Yes.,5,"""Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text."" and ""Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and",2023,2023-09-29T17:12:43Z,"Keyphrase: ""Privacy risks and harmful outputs""","""Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text."" and ""Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and Keyphrase: ""Privacy risks and harmful outputs"""
arXIv2023,LoRA ensembles for large language model fine-tuning,Yes.,5,"""Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples.""",2023,2023-09-29T16:38:38Z,"Keyphrase: ""Poor uncertainty quantification""","""Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples."" Keyphrase: ""Poor uncertainty quantification"""
arXIv2023,"Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",Yes.,4,"""Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment.""",2023,2023-09-29T16:36:39Z,"Keyphrase: ""Challenges in real-world application""","""Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment."" Keyphrase: ""Challenges in real-world application"""
arXIv2023,Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis,Yes.,5,"""This bias can take two forms",2023,2023-09-29T15:30:32Z,"Keyphrase: ""Biases in two forms""","""This bias can take two forms Keyphrase: ""Biases in two forms"""
arXIv2023,Split and Merge: Aligning Position Biases in Large Language Model based Evaluators,Yes.,5,"""these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content.""",2023,2023-09-29T14:38:58Z,"Keyphrase: ""Position bias in evaluation""","""these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content."" Keyphrase: ""Position bias in evaluation"""
arXIv2023,Using Large Language Models for Qualitative Analysis can Introduce Serious Bias,Yes.,4,"""We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences."" and ""Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations.""",2023,2023-09-29T11:19:15Z,"Keyphrase: ""Risk of bias in annotations""","""We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences."" and ""Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations."" Keyphrase: ""Risk of bias in annotations"""
arXIv2023,Benchmarking Cognitive Biases in Large Language Models as Evaluators,Yes.,5,"""We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators.""",2023,2023-09-29T06:53:10Z,"Keyphrase: ""Biased text quality evaluation""","""We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators."" Keyphrase: ""Biased text quality evaluation"""
arXIv2023,Medical Foundation Models are Susceptible to Targeted Misinformation Attacks,Yes.,5,"""Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. This peculiar susceptibility raises serious security and trustworthiness concerns for",2023,2023-09-29T06:44:36Z,"Keyphrase: ""Vulnerability to deliberate manipulation""","""Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. This peculiar susceptibility raises serious security and trustworthiness concerns for Keyphrase: ""Vulnerability to deliberate manipulation"""
arXIv2023,Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving,Yes.,5,"""Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan,",2023,2023-09-28T13:40:50Z,"Keyphrase: ""Contextually inappropriate hallucination""","""Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, Keyphrase: ""Contextually inappropriate hallucination"""
arXIv2023,Human Feedback is not Gold Standard,Yes.,5,"""We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality.""",2023,2023-09-28T11:18:20Z,"Keyphrase: ""Limited coverage of important aspects""","""We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality."" Keyphrase: ""Limited coverage of important aspects"""
arXIv2023,LawBench: Benchmarking Legal Knowledge of Large Language Models,Yes.,5,"""it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks,"" and ""While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks.""",2023,2023-09-28T09:35:59Z,"Keyphrase: ""Limited legal knowledge""","""it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks,"" and ""While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks."" Keyphrase: ""Limited legal knowledge"""
arXIv2023,MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases,Yes.,4,"""Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as 'black-boxes,' making it challenging to modify their behavior.""",2023,2023-09-27T21:26:03Z,"Keyphrase: ""Poor performance on domain-specific tasks""","""Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as 'black-boxes,' making it challenging to modify their behavior."" Keyphrase: ""Poor performance on domain-specific tasks"""
arXIv2023,NLPBench: Evaluating Large Language Models on Solving NLP Problems,Yes.,4,"""Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning",2023,2023-09-27T13:02:06Z,"Keyphrase: ""Inconsistent prompting strategy and weak scientific problem-solving skills""","""Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning Keyphrase: ""Inconsistent prompting strategy and weak scientific problem-solving skills"""
arXIv2023,Graph Neural Prompting with Large Language Models,Yes.,4,"""they still exhibit inherent limitations in precisely capturing and returning grounded knowledge"" and ""applying this to LLMs is problematic owing to their large number of parameters and high computational cost.""",2023,2023-09-27T06:33:29Z,"Keyphrase: ""Limited grounded knowledge capture""","""they still exhibit inherent limitations in precisely capturing and returning grounded knowledge"" and ""applying this to LLMs is problematic owing to their large number of parameters and high computational cost."" Keyphrase: ""Limited grounded knowledge capture"""
arXIv2023,Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI,Yes.,5,"""There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations.""",2023,2023-09-26T20:52:46Z,"Keyphrase: ""Medical domain hesitation""","""There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations."" Keyphrase: ""Medical domain hesitation"""
arXIv2023,Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models,Yes.,5,"""We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text.""",2023,2023-09-26T17:48:55Z,"Keyphrase: ""Factually incorrect text generation""","""We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text."" Keyphrase: ""Factually incorrect text generation"""
arXIv2023,How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions,Yes.,5,"""Large language models (LLMs) can 'lie', which we define as outputting false statements despite 'knowing' the truth in a demonstrable sense.""",2023,2023-09-26T16:07:54Z,"Keyphrase: ""Outputting false statements""","""Large language models (LLMs) can 'lie', which we define as outputting false statements despite 'knowing' the truth in a demonstrable sense."" Keyphrase: ""Outputting false statements"""
arXIv2023,Large Language Model Alignment: A Survey,Yes.,4,"""they may yield texts that are imprecise, misleading, or even detrimental"" and ""probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks.""",2023,2023-09-26T15:49:23Z,"Keyphrase: ""Imprecise and misleading text""","""they may yield texts that are imprecise, misleading, or even detrimental"" and ""probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks."" Keyphrase: ""Imprecise and misleading text"""
arXIv2023,Disinformation Detection: An Evolving Challenge in the Age of LLMs,Yes.,4,"""One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system.""",2023,2023-09-25T22:12:50Z,"Keyphrase: ""Misleading content generation""","""One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system."" Keyphrase: ""Misleading content generation"""
arXIv2023,Lifelong Robot Learning with Human Assisted Language Planners,Yes.,5,"""However, current LLM-based planners are only able to operate with a fixed set of skills.""",2023,2023-09-25T17:45:55Z,"Keyphrase: ""Limited skill adaptability""","""However, current LLM-based planners are only able to operate with a fixed set of skills."" Keyphrase: ""Limited skill adaptability"""
arXIv2023,"Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",Yes.,4,"""Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning.""",2023,2023-09-25T17:37:20Z,"Keyphrase: ""Limited generalization without augmentation""","""Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning."" Keyphrase: ""Limited generalization without augmentation"""
arXIv2023,Identifying the Risks of LM Agents with an LM-Emulated Sandbox,Yes.,5,"""Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses."" and ""we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures",2023,2023-09-25T17:08:02Z,"Keyphrase: ""Risk of leaking private data""","""Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses."" and ""we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures Keyphrase: ""Risk of leaking private data"""
arXIv2023,LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models,Yes.,4,"""Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations.""",2023,2023-09-25T14:50:04Z,"Keyphrase: ""Limitations in predicting carbon footprint""","""Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations."" Keyphrase: ""Limitations in predicting carbon footprint"""
arXIv2023,The Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks,Yes.,5,"""The widespread integration of autoregressive-large language models (AR-LLMs), such as ChatGPT, across established applications, like search engines, has introduced critical vulnerabilities with uniquely scalable characteristics. In this commentary, we analyse these vulnerabilities, their dependence on natural language as a vector of",2023,2023-09-25T10:48:46Z,"Keyphrase: ""Critical vulnerability in scalability""","""The widespread integration of autoregressive-large language models (AR-LLMs), such as ChatGPT, across established applications, like search engines, has introduced critical vulnerabilities with uniquely scalable characteristics. In this commentary, we analyse these vulnerabilities, their dependence on natural language as a vector of Keyphrase: ""Critical vulnerability in scalability"""
arXIv2023,Evaluating Cognitive Maps and Planning in Large Language Models with CogEval,Yes.,5,"""systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops.""",2023,2023-09-25T01:20:13Z,"Keyphrase: ""Failure in planning tasks""","""systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops."" Keyphrase: ""Failure in planning tasks"""
arXIv2023,Can LLM-Generated Misinformation Be Detected?,Yes.,5,"""However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust."" and ""we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can",2023,2023-09-25T00:45:07Z,"Keyphrase: ""Misinformation generation""","""However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust."" and ""we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can Keyphrase: ""Misinformation generation"""
arXIv2023,ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning,Yes.,5,"""However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities.""",2023,2023-09-24T17:15:58Z,"Keyphrase: ""Failure in text evaluation""","""However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities."" Keyphrase: ""Failure in text evaluation"""
arXIv2023,Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve,Yes.,5,"""The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations,"" and ""In many cases, the experiments reveal surprising failure modes,"" and ""AI practitioners should be careful about using LLMs in low-probability situations.""",2023,2023-09-24T13:35:28Z,"Keyphrase: ""Surprising failure modes""","""The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations,"" and ""In many cases, the experiments reveal surprising failure modes,"" and ""AI practitioners should be careful about using LLMs in low-probability situations."" Keyphrase: ""Surprising failure modes"""
arXIv2023,Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic,Yes.,5,"""However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning,"" and ""These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles.""",2023,2023-09-23T11:21:12Z,"Keyphrase: ""Limited reasoning ability""","""However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning,"" and ""These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles."" Keyphrase: ""Limited reasoning ability"""
arXIv2023,BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP,Yes.,4,"""Our experimental results demonstrate that while in some Bengali NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2-13b-chat being significantly bad) in comparison to the current SOTA results",2023,2023-09-22T20:29:34Z,"Keyphrase: ""Poor zero-shot performance""","""Our experimental results demonstrate that while in some Bengali NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2-13b-chat being significantly bad) in comparison to the current SOTA results Keyphrase: ""Poor zero-shot performance"""
arXIv2023,In-context Interference in Chat-based Large Language Models,Yes.,5,"""However, one limitation of this scenario is that users cannot modify the internal knowledge of the model, and the only way to add or modify internal knowledge is by explicitly mentioning it to the model during the current interaction."" and ""In-context learning has significant applications, but also has limitations that are seldom studied. In this paper, we present a study that shows how the model can suffer from",2023,2023-09-22T09:18:55Z,"Keyphrase: ""Limited adaptability for in-context learning""","""However, one limitation of this scenario is that users cannot modify the internal knowledge of the model, and the only way to add or modify internal knowledge is by explicitly mentioning it to the model during the current interaction."" and ""In-context learning has significant applications, but also has limitations that are seldom studied. In this paper, we present a study that shows how the model can suffer from Keyphrase: ""Limited adaptability for in-context learning"""
arXIv2023,Foundation Metrics for Evaluating Effectiveness of Healthcare Conversations Powered by Generative AI,Yes.,4,"""Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-centered aspects, including trust-building, ethics, personalization, empathy, user comprehension, and emotional support.""",2023,2023-09-21T19:36:48Z,"Keyphrase: ""Lack of medical comprehension""","""Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-centered aspects, including trust-building, ethics, personalization, empathy, user comprehension, and emotional support."" Keyphrase: ""Lack of medical comprehension"""
arXIv2023,Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models,Yes.,4,"""Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs).""",2023,2023-09-21T17:54:58Z,"Keyphrase: ""Inconsistent output quality""","""Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs)."" Keyphrase: ""Inconsistent output quality"""
arXIv2023,"The Reversal Curse: LLMs trained on ""A is B"" fail to learn ""B is A""",Yes.,5,"""We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form 'A is B', it will not automatically generalize to the reverse direction 'B is A'.""",2023,2023-09-21T17:52:19Z,"Keyphrase: ""Limited bidirectional generalization""","""We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form 'A is B', it will not automatically generalize to the reverse direction 'B is A'."" Keyphrase: ""Limited bidirectional generalization"""
arXIv2023,Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition,Yes.,4,"""they often fall short in some information extraction tasks, particularly those requiring domain-specific knowledge, such as Biomedical Named Entity Recognition (NER)"" and ""we inject entity knowledge to address the problem that LLM's lack of domain knowledge when predicting entity category.""",2023,2023-09-21T17:39:53Z,"Keyphrase: ""Lack of domain knowledge""","""they often fall short in some information extraction tasks, particularly those requiring domain-specific knowledge, such as Biomedical Named Entity Recognition (NER)"" and ""we inject entity knowledge to address the problem that LLM's lack of domain knowledge when predicting entity category."" Keyphrase: ""Lack of domain knowledge"""
arXIv2023,"Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",Yes.,5,"""First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a",2023,2023-09-21T16:47:30Z,"Keyphrase: ""Underperformance in detecting fake news""","""First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a Keyphrase: ""Underperformance in detecting fake news"""
arXIv2023,Code Soliloquies for Accurate Calculations in Large Language Models,Yes.,5,"""While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects.""",2023,2023-09-21T15:16:58Z,"Keyphrase: ""Limited mathematical reasoning""","""While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects."" Keyphrase: ""Limited mathematical reasoning"""
arXIv2023,A knowledge representation approach for construction contract knowledge modeling,Yes.,4,"""However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise.""",2023,2023-09-21T14:53:36Z,"Keyphrase: ""Lack of domain expertise""","""However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise."" Keyphrase: ""Lack of domain expertise"""
arXIv2023,Prompt Tuned Embedding Classification for Multi-Label Industry Sector Allocation,Yes.,5,"""Text-to-text classification is frequently reported to outperform task-specific classification heads, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens",2023,2023-09-21T13:45:32Z,"Keyphrase: ""Challenges in multilabel classification""","""Text-to-text classification is frequently reported to outperform task-specific classification heads, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens Keyphrase: ""Challenges in multilabel classification"""
arXIv2023,Knowledge Sanitization of Large Language Models,Yes.,4,"""LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns.""",2023,2023-09-21T07:49:55Z,"Keyphrase: ""Risk of revealing sensitive information""","""LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns."" Keyphrase: ""Risk of revealing sensitive information"""
arXIv2023,Goal-Oriented Prompt Attack and Safety Evaluation for LLMs,Yes.,5,"""LLMs suffer from the risk of generating harmful contents especially while being employed to applications."" and ""there is no publicly available dataset with high successful attacking rate to evaluate the abilities of defending prompt attack.""",2023,2023-09-21T07:07:49Z,"Keyphrase: ""Risk of generating harmful content""","""LLMs suffer from the risk of generating harmful contents especially while being employed to applications."" and ""there is no publicly available dataset with high successful attacking rate to evaluate the abilities of defending prompt attack."" Keyphrase: ""Risk of generating harmful content"""
arXIv2023,LLM Guided Inductive Inference for Solving Compositional Problems,Yes.,4,"""their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world."" and ""Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks",2023,2023-09-20T23:44:16Z,"Keyphrase: ""Limited deep reasoning ability""","""their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world."" and ""Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks Keyphrase: ""Limited deep reasoning ability"""
arXIv2023,"""It's a Fair Game"", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",Yes.,4,"""users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks"" and ""the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs.""",2023,2023-09-20T21:34:36Z,"Keyphrase: ""Limited user awareness and comprehension""","""users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks"" and ""the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs."" Keyphrase: ""Limited user awareness and comprehension"""
arXIv2023,Chain-of-Verification Reduces Hallucination in Large Language Models,Yes.,5,"""Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models.""",2023,2023-09-20T17:50:55Z,"Keyphrase: ""Factual hallucination""","""Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models."" Keyphrase: ""Factual hallucination"""
arXIv2023,Are Large Language Models Really Robust to Word-Level Perturbations?,Yes.,5,"""our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage.""",2023,2023-09-20T09:23:46Z,"Keyphrase: ""Vulnerability to word-level perturbations""","""our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage."" Keyphrase: ""Vulnerability to word-level perturbations"""
arXIv2023,"Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness",Yes.,5,"""As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination.""",2023,2023-09-20T05:04:16Z,"Keyphrase: ""Hallucination""","""As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination."" Keyphrase: ""Hallucination"""
arXIv2023,In-Context Learning for Text Classification with Many Labels,Yes.,5,"""In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt.""",2023,2023-09-19T22:41:44Z,"Keyphrase: ""Limited context window""","""In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt."" Keyphrase: ""Limited context window"""
arXIv2023,LMDX: Language Model-based Document Information Extraction and Localization,Yes.,4,"""The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated.""",2023,2023-09-19T22:32:56Z,"Keyphrase: ""Absence of layout encoding""","""The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated."" Keyphrase: ""Absence of layout encoding"""
arXIv2023,Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome,Yes.,5,"""whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm.""",2023,2023-09-19T16:41:19Z,"Keyphrase: ""Limited sarcasm comprehension""","""whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm."" Keyphrase: ""Limited sarcasm comprehension"""
arXIv2023,Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation,Yes.,4,"""Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. ... This prevent the community to rigorously audit these models and conduct accurate assessment of their capability.""",2023,2023-09-19T15:02:58Z,"Keyphrase: ""Data contamination and lack of rigorous auditing""","""Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. ... This prevent the community to rigorously audit these models and conduct accurate assessment of their capability."" Keyphrase: ""Data contamination and lack of rigorous auditing"""
arXIv2023,PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training,Yes.,5,"""Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs.""",2023,2023-09-19T08:03:38Z,"Keyphrase: ""Limited input context""","""Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs."" Keyphrase: ""Limited input context"""
arXIv2023,Investigating the Catastrophic Forgetting in Multimodal Large Language Models,Yes.,5,"""However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).""",2023,2023-09-19T04:51:13Z,"Keyphrase: ""Catastrophic forgetting""","""However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM)."" Keyphrase: ""Catastrophic forgetting"""
arXIv2023,LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins,Yes.,4,"""Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations."" and ""We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms.""",2023,2023-09-19T02:20:10Z,"Keyphrase: ""Imprecise natural language interpretation""","""Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations."" and ""We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms."" Keyphrase: ""Imprecise natural language interpretation"""
arXIv2023,GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts,Yes.,5,"""However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities.""",2023,2023-09-19T02:19:48Z,"Keyphrase: ""Unreliable guidance for harmful activities""","""However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities."" Keyphrase: ""Unreliable guidance for harmful activities"""
arXIv2023,Stabilizing RLHF through Advantage Model and Selective Rehearsal,Yes.,4,"""Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting.""",2023,2023-09-18T23:06:32Z,"Keyphrase: ""Ethical alignment challenge""","""Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting."" Keyphrase: ""Ethical alignment challenge"""
arXIv2023,Bias of AI-Generated Content: An Examination of News Produced by Large Language Models,Yes.,5,"""To harness this transformation, we need to understand the limitations of LLMs."" and ""Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race.""",2023,2023-09-18T14:47:24Z,"Keyphrase: ""Gender and racial bias""","""To harness this transformation, we need to understand the limitations of LLMs."" and ""Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race."" Keyphrase: ""Gender and racial bias"""
arXIv2023,"CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",Yes.,4,"""The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community.""",2023,2023-09-17T23:49:10Z,"Keyphrase: ""Transparency of training data""","""The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community."" Keyphrase: ""Transparency of training data"""
arXIv2023,Language models are susceptible to incorrect patient self-diagnosis in medical applications,Yes.,5,"""Our findings highlight that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of LLMs drop dramatically, revealing a high susceptibility to errors in self-diagnosis.""",2023,2023-09-17T19:56:39Z,"Keyphrase: ""High susceptibility to errors""","""Our findings highlight that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of LLMs drop dramatically, revealing a high susceptibility to errors in self-diagnosis."" Keyphrase: ""High susceptibility to errors"""
arXIv2023,Can Large Language Models Understand Real-World Complex Instructions?,Yes.,5,"""they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text.""",2023,2023-09-17T04:18:39Z,"Keyphrase: ""Struggles with complex instructions and constraints""","""they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text."" Keyphrase: ""Struggles with complex instructions and constraints"""
arXIv2023,Rethinking STS and NLI in Large Language Models,Yes.,5,"""the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements.""",2023,2023-09-16T11:58:39Z,"Keyphrase: ""Overconfidence in low-resource domains""","""the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements."" Keyphrase: ""Overconfidence in low-resource domains"""
arXIv2023,Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?,Yes.,5,"""Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging."" and ""In-depth error analysis and creating an ability map across six dimensions -- coverage, formatting, reasoning, comprehension, pragmatics, and hallucination -- highlight areas for future enhancements and suggest forthcoming research trajectories.""",2023,2023-09-16T11:31:58Z,"Keyphrase: ""Challenges in handling structured tabular data""","""Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging."" and ""In-depth error analysis and creating an ability map across six dimensions -- coverage, formatting, reasoning, comprehension, pragmatics, and hallucination -- highlight areas for future enhancements and suggest forthcoming research trajectories."" Keyphrase: ""Challenges in handling structured tabular data"""
arXIv2023,"PDFTriage: Question Answering over Long, Structured Documents",Yes.,5,"""Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM.""",2023,2023-09-16T04:29:05Z,"Keyphrase: ""Limited context understanding""","""Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM."" Keyphrase: ""Limited context understanding"""
arXIv2023,Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings,Yes.,5,"""Our experiments reveal that",2023,2023-09-15T17:45:28Z,Keyphrase: Lack of specificity,"""Our experiments reveal that Keyphrase: Lack of specificity"
arXIv2023,Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West,Yes.,4,"""Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms."" and ""We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context.""",2023,2023-09-15T17:38:41Z,"Keyphrase: ""Societal bias and representational harm""","""Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms."" and ""We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context."" Keyphrase: ""Societal bias and representational harm"""
arXIv2023,Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases,Yes.,5,"""Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in 'hallucinated' information."" and ""Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the",2023,2023-09-15T12:06:45Z,"Keyphrase: ""Poor performance in real-world applications""","""Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in 'hallucinated' information."" and ""Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the Keyphrase: ""Poor performance in real-world applications"""
arXIv2023,Investigating Answerability of LLMs for Long-Form Question Answering,Yes.,5,"""it becomes increasingly crucial to understand their capabilities, limitations, and differences"" and ""generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts"" and ""open-source LLMs exhibit decreased reliance on context for generated questions",2023,2023-09-15T07:22:56Z,"Keyphrase: ""Limited long-context understanding""","""it becomes increasingly crucial to understand their capabilities, limitations, and differences"" and ""generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts"" and ""open-source LLMs exhibit decreased reliance on context for generated questions Keyphrase: ""Limited long-context understanding"""
arXIv2023,FedJudge: Federated Legal Large Language Model,Yes.,4,"""However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods.""",2023,2023-09-15T05:45:44Z,"Keyphrase: ""Overhead in fine-tuning""","""However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods."" Keyphrase: ""Overhead in fine-tuning"""
arXIv2023,Self-Assessment Tests are Unreliable Measures of LLM Personality,Yes.,5,"""self-assessment personality tests created for humans are unreliable measures of personality in LLMs.""",2023,2023-09-15T05:19:39Z,"Keyphrase: ""Unreliable personality assessment""","""self-assessment personality tests created for humans are unreliable measures of personality in LLMs."" Keyphrase: ""Unreliable personality assessment"""
arXIv2023,MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,Yes.,4,"""while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks."" and ""we observe that MMICL successfully allevi",2023,2023-09-14T17:59:17Z,"Keyphrase: ""Struggles with multimodal understanding""","""while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks."" and ""we observe that MMICL successfully allevi Keyphrase: ""Struggles with multimodal understanding"""
arXIv2023,Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions,Yes.,5,"""we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning,"" and ""several popular instruction-tuned models are highly unsafe,"" and ""our results illustrate trade-offs in training LLMs to be helpful and",2023,2023-09-14T17:23:37Z,"Keyphrase: ""Safety concerns in instruction-tuned models""","""we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning,"" and ""several popular instruction-tuned models are highly unsafe,"" and ""our results illustrate trade-offs in training LLMs to be helpful and Keyphrase: ""Safety concerns in instruction-tuned models"""
arXIv2023,Tree of Uncertain Thoughts Reasoning for Large Language Models,Yes.,4,"""These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process.""",2023,2023-09-14T13:14:51Z,"Keyphrase: ""Local uncertainty in reasoning""","""These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process."" Keyphrase: ""Local uncertainty in reasoning"""
arXIv2023,Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,Yes.,5,"""Our analysis reveals a bias in GPT4-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.""",2023,2023-09-14T06:41:58Z,"Keyphrase: ""Bias in evaluation""","""Our analysis reveals a bias in GPT4-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages."" Keyphrase: ""Bias in evaluation"""
arXIv2023,ChatGPT MT: Competitive for High- (but not Low-) Resource Languages,Yes.,5,"""Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered.""",2023,2023-09-14T04:36:00Z,"Keyphrase: ""Performance gap between high-resource and low-resource languages""","""Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered."" Keyphrase: ""Performance gap between high-resource and low-resource languages"""
arXIv2023,An Assessment of ChatGPT on Log Data,Yes.,5,"""Our findings show that the performance of the current version of ChatGPT for log processing is limited, with a lack of consistency in responses and scalability issues.""",2023,2023-09-14T04:09:27Z,"Keyphrase: ""Inconsistent responses and scalability issues""","""Our findings show that the performance of the current version of ChatGPT for log processing is limited, with a lack of consistency in responses and scalability issues."" Keyphrase: ""Inconsistent responses and scalability issues"""
arXIv2023,Less is More for Long Document Summary Evaluation by LLMs,Yes.,5,"""Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked.""",2023,2023-09-14T01:59:15Z,"Keyphrase: ""Lost-in-the-middle problem""","""Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked."" Keyphrase: ""Lost-in-the-middle problem"""
arXIv2023,In-Contextual Gender Bias Suppression for Large Language Models,Yes.,4,"""Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases.""",2023,2023-09-13T18:39:08Z,"Keyphrase: ""Gender bias encoding""","""Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases."" Keyphrase: ""Gender bias encoding"""
arXIv2023,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Yes.,4,"""increasing attention has been paid to their safety concerns,"" and ""there is still significant room for improving the safety of current LLMs.""",2023,2023-09-13T15:56:50Z,"Keyphrase: ""Safety concerns""","""increasing attention has been paid to their safety concerns,"" and ""there is still significant room for improving the safety of current LLMs."" Keyphrase: ""Safety concerns"""
arXIv2023,Scaled Prompt-Tuning for Few-Shot Natural Language Generation,Yes.,4,"""the memory demand and computation cost of fine-tuning LLMs on downstream tasks are non-negligible. Besides, fine-tuning generally requires a certain amount of data from individual tasks whilst data collection cost is another issue to consider in real-world applications.""",2023,2023-09-13T07:12:31Z,"Keyphrase: ""High computational cost and data requirements""","""the memory demand and computation cost of fine-tuning LLMs on downstream tasks are non-negligible. Besides, fine-tuning generally requires a certain amount of data from individual tasks whilst data collection cost is another issue to consider in real-world applications."" Keyphrase: ""High computational cost and data requirements"""
arXIv2023,"TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",Yes.,5,"""However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges.""",2023,2023-09-13T04:47:43Z,"Keyphrase: ""Difficulty with numerical data and simulations""","""However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges."" Keyphrase: ""Difficulty with numerical data and simulations"""
arXIv2023,The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models,Yes.,5,"""a notable obstacle emerges when feeding numerical/temporal data into these models"" and ""tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships.""",2023,2023-09-12T13:51:29Z,"Keyphrase: ""Struggles with numerical-temporal data""","""a notable obstacle emerges when feeding numerical/temporal data into these models"" and ""tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships."" Keyphrase: ""Struggles with numerical-temporal data"""
arXIv2023,Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions,Yes.,5,"""As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given.""",2023,2023-09-12T05:54:45Z,"Keyphrase: ""Risk of irrelevant information""","""As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given."" Keyphrase: ""Risk of irrelevant information"""
arXIv2023,"Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs",Yes.,5,"""LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic nature, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (",2023,2023-09-12T02:14:05Z,"Keyphrase: ""Limited factual reliability""","""LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic nature, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and ( Keyphrase: ""Limited factual reliability"""
arXIv2023,Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing,Yes.,5,"""These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning.""",2023,2023-09-12T00:54:15Z,"Keyphrase: ""Limited proficiency in complex strategic reasoning""","""These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning."" Keyphrase: ""Limited proficiency in complex strategic reasoning"""
arXIv2023,PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis,Yes.,5,"""their effectiveness in assisting on-call engineers is constrained by low accuracy due to the intrinsic difficulty of the task, a propensity for LLM-based approaches to hallucinate, and difficulties in distinguishing these well-disguised hallucinations.""",2023,2023-09-11T21:24:00Z,"Keyphrase: ""Difficulty in distinguishing hallucination""","""their effectiveness in assisting on-call engineers is constrained by low accuracy due to the intrinsic difficulty of the task, a propensity for LLM-based approaches to hallucinate, and difficulties in distinguishing these well-disguised hallucinations."" Keyphrase: ""Difficulty in distinguishing hallucination"""
arXIv2023,Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models,Yes.,5,"""Large Language Models (LLMs) struggle to perform such reasoning consistently.""",2023,2023-09-11T16:39:30Z,"Keyphrase: ""Inconsistent reasoning""","""Large Language Models (LLMs) struggle to perform such reasoning consistently."" Keyphrase: ""Inconsistent reasoning"""
arXIv2023,Evaluating the Deductive Competence of Large Language Models,Yes.,5,"""The tested LLMs have limited abilities to solve these problems in their conventional form."" and ""Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance.""",2023,2023-09-11T13:47:07Z,"Keyphrase: ""Limited reasoning ability""","""The tested LLMs have limited abilities to solve these problems in their conventional form."" and ""Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance."" Keyphrase: ""Limited reasoning ability"""
arXIv2023,Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis,Yes.,5,"""large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs"" and ""we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.""",2023,2023-09-11T03:35:00Z,"Keyphrase: ""Hallucination problem""","""large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs"" and ""we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination."" Keyphrase: ""Hallucination problem"""
arXIv2023,Does Writing with Language Models Reduce Content Diversity?,Yes.,5,"""As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse."" and ""we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and",2023,2023-09-11T02:16:47Z,"Keyphrase: ""Risk of decreased diversity""","""As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse."" and ""we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and Keyphrase: ""Risk of decreased diversity"""
arXIv2023,Towards LLM-based Autograding for Short Textual Answers,Yes.,4,"""entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information"" and ""while 'out-of-the-box' LLMs provide a valuable tool to provide a complementary perspective, their readiness for independent automated grading remains a work in progress, necessitating",2023,2023-09-09T22:25:56Z,"Keyphrase: ""Ethical bias and false information""","""entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information"" and ""while 'out-of-the-box' LLMs provide a valuable tool to provide a complementary perspective, their readiness for independent automated grading remains a work in progress, necessitating Keyphrase: ""Ethical bias and false information"""
arXIv2023,Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges,Yes.,4,"""Our findings indicate the promise of LLMs as interfaces to EHR, but also highlight the outstanding challenge posed by 'hallucinations'.""",2023,2023-09-08T18:44:47Z,"Keyphrase: ""Hallucination challenges""","""Our findings indicate the promise of LLMs as interfaces to EHR, but also highlight the outstanding challenge posed by 'hallucinations'."" Keyphrase: ""Hallucination challenges"""
arXIv2023,Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,Yes.,5,"""We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans.""",2023,2023-09-08T17:49:44Z,"Keyphrase: ""Weak visual reasoning capability""","""We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans."" Keyphrase: ""Weak visual reasoning capability"""
arXIv2023,Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems,Yes.,5,"""However, they are fraught with issues that undermine their utility and trustworthiness. These include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency).""",2023,2023-09-08T09:39:53Z,"Keyphrase: ""Erroneous information generation""","""However, they are fraught with issues that undermine their utility and trustworthiness. These include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency)."" Keyphrase: ""Erroneous information generation"""
arXIv2023,Don't Ignore Dual Logic Ability of LLMs while Privatizing: A Data-Intensive Analysis in Medical Domain,Yes.,5,"""However, these privatization efforts often ignored a critical aspect",2023,2023-09-08T08:20:46Z,"Keyphrase: ""Overlooking critical aspects""","""However, these privatization efforts often ignored a critical aspect Keyphrase: ""Overlooking critical aspects"""
arXIv2023,Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese,Yes.,4,"""LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts.""",2023,2023-09-08T07:42:57Z,"Keyphrase: ""Limited domain knowledge""","""LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts."" Keyphrase: ""Limited domain knowledge"""
arXIv2023,DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models,Yes.,5,"""Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining.""",2023,2023-09-07T17:45:31Z,"Keyphrase: ""Hallucination tendency""","""Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining."" Keyphrase: ""Hallucination tendency"""
arXIv2023,Large Language Models Are Not Robust Multiple Choice Selectors,Yes.,5,"""This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent 'selection bias',"" and ""We hope this work can draw broader research attention to the bias and robustness of modern LLMs.""",2023,2023-09-07T17:44:56Z,"Keyphrase: ""Vulnerability to selection bias""","""This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent 'selection bias',"" and ""We hope this work can draw broader research attention to the bias and robustness of modern LLMs."" Keyphrase: ""Vulnerability to selection bias"""
arXIv2023,OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs,Yes.,4,"""an open research question concerns the inherent biases of trained models and their responses"" and ""Current research work seeks to de-bias such models, or suppress potentially biased answers.""",2023,2023-09-07T17:41:01Z,"Keyphrase: ""Inherent bias""","""an open research question concerns the inherent biases of trained models and their responses"" and ""Current research work seeks to de-bias such models, or suppress potentially biased answers."" Keyphrase: ""Inherent bias"""
arXIv2023,XGen-7B Technical Report,Yes.,4,"""most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context.""",2023,2023-09-07T02:20:03Z,"Keyphrase: ""Proprietary confinement and limited sequence length""","""most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context."" Keyphrase: ""Proprietary confinement and limited sequence length"""
arXIv2023,Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty,Yes.,5,"""Despite the potential of large language models (LLMs) like ChatGPT as a general task solver, they lag behind state-of-the-art (supervised) methods in OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant context from relevant relations and generate",2023,2023-09-07T01:35:24Z,"Keyphrase: ""Difficulty in distinguishing relevant context""","""Despite the potential of large language models (LLMs) like ChatGPT as a general task solver, they lag behind state-of-the-art (supervised) methods in OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant context from relevant relations and generate Keyphrase: ""Difficulty in distinguishing relevant context"""
arXIv2023,Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity,Yes.,5,"""We conclude that the LLM we tested (GPT-3.5) does not have sufficient algorithmic fidelity to expect research on it to generalize to human populations.""",2023,2023-09-06T15:00:44Z,"Keyphrase: ""Limited algorithmic fidelity""","""We conclude that the LLM we tested (GPT-3.5) does not have sufficient algorithmic fidelity to expect research on it to generalize to human populations."" Keyphrase: ""Limited algorithmic fidelity"""
arXIv2023,Zero-Resource Hallucination Prevention for Large Language Models,Yes.,4,"""The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of 'hallucination,' which refers to instances where LLMs generate factually inaccurate or ungrounded information.""",2023,2023-09-06T01:57:36Z,"Keyphrase: ""Factually inaccurate hallucinations""","""The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of 'hallucination,' which refers to instances where LLMs generate factually inaccurate or ungrounded information."" Keyphrase: ""Factually inaccurate hallucinations"""
arXIv2023,An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models,Yes.,5,"""However, recent studies have revealed that these models often suffer from hallucinations, leading to overly confident but incorrect judgments. This limits their application in the medical domain, where tasks require the utmost accuracy.""",2023,2023-09-05T09:24:48Z,"Keyphrase: ""Hallucination and overconfidence""","""However, recent studies have revealed that these models often suffer from hallucinations, leading to overly confident but incorrect judgments. This limits their application in the medical domain, where tasks require the utmost accuracy."" Keyphrase: ""Hallucination and overconfidence"""
arXIv2023,Open Sesame! Universal Black Box Jailbreaking of Large Language Models,Yes.,5,"""Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior.""",2023,2023-09-04T08:54:20Z,"Keyphrase: ""Unexpected deviations""","""Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior."" Keyphrase: ""Unexpected deviations"""
arXIv2023,Benchmarking Large Language Models in Retrieval-Augmented Generation,Yes.,5,"""Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information.""",2023,2023-09-04T08:28:44Z,"Keyphrase: ""Struggles with noise robustness and false information""","""Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information."" Keyphrase: ""Struggles with noise robustness and false information"""
arXIv2023,Representations Matter: Embedding Modes of Large Language Models using Dynamic Mode Decomposition,Yes.,5,"""Existing large language models (LLMs) are known for generating 'hallucinated' content, namely a fabricated text of plausibly looking, yet unfounded, facts.""",2023,2023-09-03T19:10:18Z,"Keyphrase: ""Fabricated content generation""","""Existing large language models (LLMs) are known for generating 'hallucinated' content, namely a fabricated text of plausibly looking, yet unfounded, facts."" Keyphrase: ""Fabricated content generation"""
arXIv2023,Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,Yes.,5,"""a significant concern revolves around their propensity to exhibit hallucinations",2023,2023-09-03T16:56:48Z,"Keyphrase: ""Propensity for hallucination""","""a significant concern revolves around their propensity to exhibit hallucinations Keyphrase: ""Propensity for hallucination"""
arXIv2023,FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs,Yes.,5,"""The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs."" and ""consumer-level GPUs...are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth."" and ""this system faces critical",2023,2023-09-03T13:27:56Z,"Keyphrase: ""Hardware limitations""","""The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs."" and ""consumer-level GPUs...are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth."" and ""this system faces critical Keyphrase: ""Hardware limitations"""
arXIv2023,Bias Testing and Mitigation in LLM-based Code Generation,Yes.,5,"""As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged",2023,2023-09-03T07:14:49Z,"Keyphrase: ""Challenges in software coding""","""As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged Keyphrase: ""Challenges in software coding"""
arXIv2023,Explainability for Large Language Models: A Survey,Yes.,4,"""However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications."" and ""understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts.""",2023,2023-09-02T22:14:26Z,"Keyphrase: ""Lack of transparency""","""However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications."" and ""understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts."" Keyphrase: ""Lack of transparency"""
arXIv2023,Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports,Yes.,4,"""While performing well in terms of accuracy, both the lack of interpretability and limitations to transfer across languages limit their use in clinical setting.""",2023,2023-09-02T11:46:41Z,"Keyphrase: ""Lack of interpretability""","""While performing well in terms of accuracy, both the lack of interpretability and limitations to transfer across languages limit their use in clinical setting."" Keyphrase: ""Lack of interpretability"""
arXIv2023,Large Process Models: Business Process Management in the Age of Generative AI,Yes.,4,"""The continued success of Large Language Models (LLMs) and other generative artificial intelligence approaches highlights the advantages that large information corpora can have over rigidly defined symbolic models, but also serves as a proof-point of the challenges that purely statistics-based approaches have in terms of safety and trustworthiness.""",2023,2023-09-02T10:32:53Z,"Keyphrase: ""Safety and trustworthiness challenges""","""The continued success of Large Language Models (LLMs) and other generative artificial intelligence approaches highlights the advantages that large information corpora can have over rigidly defined symbolic models, but also serves as a proof-point of the challenges that purely statistics-based approaches have in terms of safety and trustworthiness."" Keyphrase: ""Safety and trustworthiness challenges"""
arXIv2023,No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function,Yes.,5,"""However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions.""",2023,2023-09-01T13:10:54Z,"Keyphrase: ""Difficulty in generating correct reasoning steps""","""However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions."" Keyphrase: ""Difficulty in generating correct reasoning steps"""
arXIv2023,BatchPrompt: Accomplish more with less,Yes.,5,"""prompting with batched data in longer contexts will inevitably lead to worse performance, compared to single-data prompting"" and ""the performance of the language model is significantly correlated with the positions and order of the batched data, due to the corresponding change in decoder context.""",2023,2023-09-01T10:44:36Z,"Keyphrase: ""Performance degradation with longer context""","""prompting with batched data in longer contexts will inevitably lead to worse performance, compared to single-data prompting"" and ""the performance of the language model is significantly correlated with the positions and order of the batched data, due to the corresponding change in decoder context."" Keyphrase: ""Performance degradation with longer context"""
arXIv2023,Why do universal adversarial attacks work on large language models?: Geometry might be the answer,Yes.,5,"""Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature.""",2023,2023-09-01T05:09:49Z,"Keyphrase: ""Vulnerability to universal adversarial attacks""","""Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature."" Keyphrase: ""Vulnerability to universal adversarial attacks"""
arXIv2023,Context Aware Query Rewriting for Text Rankers using LLM,Yes.,5,"""We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing.""",2023,2023-08-31T14:19:50Z,"Keyphrase: ""Concept drift and high inference cost""","""We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing."" Keyphrase: ""Concept drift and high inference cost"""
arXIv2023,Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering,Yes.,5,"""We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting.""",2023,2023-08-31T10:31:19Z,"Keyphrase: ""Limited utility for knowledge graph generation""","""We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting."" Keyphrase: ""Limited utility for knowledge graph generation"""
arXIv2023,LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models,Yes.,5,"""their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues.""",2023,2023-08-30T16:47:51Z,"Keyphrase: ""Struggles with long context encoding""","""their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues."" Keyphrase: ""Struggles with long context encoding"""
arXIv2023,Quantifying and Analyzing Entity-level Memorization in Large Language Models,Yes.,5,"""privacy risks arising from memorization have attracted increasing attention"" and ""LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations.""",2023,2023-08-30T03:06:47Z,"Keyphrase: ""Privacy risks from memorization""","""privacy risks arising from memorization have attracted increasing attention"" and ""LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations."" Keyphrase: ""Privacy risks from memorization"""
arXIv2023,Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model,Yes.,5,"""The instructions given to the LLM by natural language may include ambiguity and lack of information depending on the task context."" and ""However, our experiments also revealed challenges in robot action planning with LLM, such as asking unimportant questions and assuming crucial information without asking.""",2023,2023-08-30T00:54:44Z,"Keyphrase: ""Challenges in task context understanding""","""The instructions given to the LLM by natural language may include ambiguity and lack of information depending on the task context."" and ""However, our experiments also revealed challenges in robot action planning with LLM, such as asking unimportant questions and assuming crucial information without asking."" Keyphrase: ""Challenges in task context understanding"""
arXIv2023,Evaluation and Analysis of Hallucination in Large Vision-Language Models,Yes.,5,"""LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios."" and ""we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem.""",2023,2023-08-29T08:51:24Z,"Keyphrase: ""Persistent hallucination issues""","""LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios."" and ""we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem."" Keyphrase: ""Persistent hallucination issues"""
arXIv2023,Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models,Yes.,5,"""However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses.""",2023,2023-08-29T04:59:53Z,"Keyphrase: ""Poor memory recall""","""However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses."" Keyphrase: ""Poor memory recall"""
arXIv2023,Gender bias and stereotypes in Large Language Models,Yes.,5,"""This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models."" and ""LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior.""",2023,2023-08-28T22:32:05Z,"Keyphrase: ""Biased rationalization""","""This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models."" and ""LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior."" Keyphrase: ""Biased rationalization"""
arXIv2023,Challenges of GPT-3-based Conversational Agents for Healthcare,Yes.,5,"""However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences."" and ""We provide a procedure for manually designing patient queries to stress-test high-risk limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content",2023,2023-08-28T15:12:34Z,"Keyphrase: ""Erroneous medical information""","""However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences."" and ""We provide a procedure for manually designing patient queries to stress-test high-risk limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content Keyphrase: ""Erroneous medical information"""
arXIv2023,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",Yes.,5,"""most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs"" and ""still struggles on longer contexts"" and ""Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have",2023,2023-08-28T11:53:40Z,"Keyphrase: ""Struggles with long context""","""most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs"" and ""still struggles on longer contexts"" and ""Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have Keyphrase: ""Struggles with long context"""
arXIv2023,Biomedical Entity Linking with Triple-aware Pre-Training,Yes.,4,"""a difficulty of linking the biomedical entities using current large language models (LLM) trained on a general corpus is that biomedical entities are scarcely distributed in texts and therefore have been rarely seen during training by the LLM"" and ""those LLMs are not aware of high level semantic connection between different biomedical entities, which are useful in identifying similar concepts in different textual contexts.""",2023,2023-08-28T09:06:28Z,"Keyphrase: ""Limited biomedical entity linking""","""a difficulty of linking the biomedical entities using current large language models (LLM) trained on a general corpus is that biomedical entities are scarcely distributed in texts and therefore have been rarely seen during training by the LLM"" and ""those LLMs are not aware of high level semantic connection between different biomedical entities, which are useful in identifying similar concepts in different textual contexts."" Keyphrase: ""Limited biomedical entity linking"""
arXIv2023,EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models,Yes.,4,"""the transition of LLMs from data centers to edge devices presents a set of challenges and opportunities. While this shift can enhance privacy and availability, it is hampered by the enormous parameter sizes of these models, leading to impractical runtime costs.""",2023,2023-08-28T06:56:08Z,"Keyphrase: ""Impractical runtime cost due to enormous parameter size""","""the transition of LLMs from data centers to edge devices presents a set of challenges and opportunities. While this shift can enhance privacy and availability, it is hampered by the enormous parameter sizes of these models, leading to impractical runtime costs."" Keyphrase: ""Impractical runtime cost due to enormous parameter size"""
arXIv2023,Evaluating the Robustness to Instructions of Large Language Models,Yes.,5,"""We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to worsen significantly, and the robustness of the model for RE instructions deteriorates compared to QA.""",2023,2023-08-28T04:57:07Z,"Keyphrase: ""Decreased robustness with unfamiliar instructions""","""We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to worsen significantly, and the robustness of the model for RE instructions deteriorates compared to QA."" Keyphrase: ""Decreased robustness with unfamiliar instructions"""
arXIv2023,Symbolic and Language Agnostic Large Language Models,Yes.,5,"""due to the subsymbolic nature of these models whatever knowledge these systems acquire about language will always be buried in millions of microfeatures (weights) none of which is meaningful on its own. Moreover, and due to their stochastic nature, these models will often fail in capturing various inferential aspects that are prevalent in natural language.""",2023,2023-08-27T20:24:33Z,"Keyphrase: ""Limited inferential capability""","""due to the subsymbolic nature of these models whatever knowledge these systems acquire about language will always be buried in millions of microfeatures (weights) none of which is meaningful on its own. Moreover, and due to their stochastic nature, these models will often fail in capturing various inferential aspects that are prevalent in natural language."" Keyphrase: ""Limited inferential capability"""
arXIv2023,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,Yes.,4,"""The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs.""",2023,2023-08-27T19:22:12Z,"Keyphrase: ""Crosslingual imbalance""","""The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs."" Keyphrase: ""Crosslingual imbalance"""
arXIv2023,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",Yes.,5,"""Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage.""",2023,2023-08-27T16:14:19Z,"Keyphrase: ""High computational requirements and complex deployment""","""Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage."" Keyphrase: ""High computational requirements and complex deployment"""
arXIv2023,Detecting Language Model Attacks with Perplexity,Yes.,4,"""Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content."" and ""false positives are a significant challenge for plain perplexity filtering.""",2023,2023-08-27T15:20:06Z,"Keyphrase: ""Security vulnerability and potential misuse""","""Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content."" and ""false positives are a significant challenge for plain perplexity filtering."" Keyphrase: ""Security vulnerability and potential misuse"""
arXIv2023,Rethinking Language Models as Symbolic Knowledge Graphs,Yes.,5,"""Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes."" and ""Our extensive evaluation of various LMs shows that while these models exhibit considerable potential in recalling factual information, their ability to capture intricate topological and semantic traits of KGs remains significantly constrained.""",2023,2023-08-25T21:25:08Z,"Keyphrase: ""Limited ability to capture intricate semantic traits""","""Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes."" and ""Our extensive evaluation of various LMs shows that while these models exhibit considerable potential in recalling factual information, their ability to capture intricate topological and semantic traits of KGs remains significantly constrained."" Keyphrase: ""Limited ability to capture intricate semantic traits"""
arXIv2023,The Poison of Alignment,Yes.,5,"""alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP",2023,2023-08-25T15:51:15Z,"Keyphrase: ""Degraded performance due to alignment poisoning""","""alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP Keyphrase: ""Degraded performance due to alignment poisoning"""
arXIv2023,Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions,Yes.,4,"""The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms."" and ""While all LLMs did not provide satisfactory results in understanding cultural values, GPT-4 exhibited the highest CAT score for the cultural values of the US.""",2023,2023-08-25T14:50:13Z,"Keyphrase: ""Cultural misalignment""","""The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms."" and ""While all LLMs did not provide satisfactory results in understanding cultural values, GPT-4 exhibited the highest CAT score for the cultural values of the US."" Keyphrase: ""Cultural misalignment"""
arXIv2023,Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering,Yes.,5,"""Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA.""",2023,2023-08-25T09:23:55Z,"Keyphrase: ""Limited access to external knowledge""","""Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA."" Keyphrase: ""Limited access to external knowledge"""
arXIv2023,Causal Parrots: Large Language Models May Talk Causality But Are Not Causal,Yes.,5,"""We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise."" and ""Our empirical analysis provides favoring evidence that current LLMs are even weak 'causal parrots.'""",2023,2023-08-24T20:23:13Z,"Keyphrase: ""Limited causal reasoning""","""We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise."" and ""Our empirical analysis provides favoring evidence that current LLMs are even weak 'causal parrots.'"" Keyphrase: ""Limited causal reasoning"""
arXIv2023,"Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",Yes.,5,"""safety- and security-related threats and vulnerabilities of LLMs,"" ""LLMs can be misused for fraud, impersonation, and the generation of malware,"" ""security-related problems with such models,"" ""limitations of LLMs in light of such security concerns.""",2023,2023-08-24T14:45:50Z,"Keyphrase: ""Security vulnerabilities""","""safety- and security-related threats and vulnerabilities of LLMs,"" ""LLMs can be misused for fraud, impersonation, and the generation of malware,"" ""security-related problems with such models,"" ""limitations of LLMs in light of such security concerns."" Keyphrase: ""Security vulnerabilities"""
arXIv2023,VIGC: Visual Instruction Generation and Correction,Yes.,5,"""the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information.""",2023,2023-08-24T11:21:05Z,"Keyphrase: ""Generation of false information""","""the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information."" Keyphrase: ""Generation of false information"""
arXIv2023,Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs,Yes.,5,"""We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating 'unknown' outputs, even when the correct document is among the top-k retrieved passages.""",2023,2023-08-24T05:26:54Z,"Keyphrase: ""Unknown output generation""","""We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating 'unknown' outputs, even when the correct document is among the top-k retrieved passages."" Keyphrase: ""Unknown output generation"""
arXIv2023,Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models,Yes.,4,"""While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty.""",2023,2023-08-23T17:40:35Z,"Keyphrase: ""Hallucination and predictive uncertainty""","""While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty."" Keyphrase: ""Hallucination and predictive uncertainty"""
arXIv2023,Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test,Yes.,5,"""Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents."" and ""However, the model still fails to answer some questions correctly even with providing library of context, highlighting room",2023,2023-08-22T23:18:53Z,"Keyphrase: ""Limited adaptability to new information""","""Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents."" and ""However, the model still fails to answer some questions correctly even with providing library of context, highlighting room Keyphrase: ""Limited adaptability to new information"""
arXIv2023,Towards an On-device Agent for Text Rewriting,Yes.,4,"""Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference.""",2023,2023-08-22T22:18:38Z,"Keyphrase: ""Impractical on-device inference""","""Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference."" Keyphrase: ""Impractical on-device inference"""
arXIv2023,Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models,Yes.,5,"""open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts.""",2023,2023-08-22T20:12:49Z,"Keyphrase: ""Severe hallucination in smaller LLMs""","""open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts."" Keyphrase: ""Severe hallucination in smaller LLMs"""
arXIv2023,Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions,Yes.,5,"""previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models"" and ""Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are",2023,2023-08-22T14:54:59Z,"Keyphrase: ""Sensitivity to prompt wording""","""previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models"" and ""Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are Keyphrase: ""Sensitivity to prompt wording"""
arXIv2023,Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis,Yes.,5,"""All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy."" and ""GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity."" and ""GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities.""",2023,2023-08-22T06:32:07Z,"Keyphrase: ""Limited structural reasoning capabilities""","""All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy."" and ""GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity."" and ""GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities."" Keyphrase: ""Limited structural reasoning capabilities"""
arXIv2023,Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models,Yes.,4,"""Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. The complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification.""",2023,2023-08-22T00:57:36Z,"Keyphrase: ""Struggles with data sparsity""","""Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. The complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification."" Keyphrase: ""Struggles with data sparsity"""
arXIv2023,Giraffe: Adventures in Expanding Context Lengths in LLMs,Yes.,5,"""Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time.""",2023,2023-08-21T17:30:16Z,"Keyphrase: ""Limited context handling""","""Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time."" Keyphrase: ""Limited context handling"""
arXIv2023,Unreflected Acceptance -- Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education,Yes.,5,"""nearly half of the solutions provided with the support of ChatGPT were mistakenly assumed to be correct by the students, indicating that they overly trusted ChatGPT even in their field of expertise"" and ""highlighting the stark differences in interaction behavior between the groups and",2023,2023-08-21T16:14:34Z,"Keyphrase: ""Overly trusted responses""","""nearly half of the solutions provided with the support of ChatGPT were mistakenly assumed to be correct by the students, indicating that they overly trusted ChatGPT even in their field of expertise"" and ""highlighting the stark differences in interaction behavior between the groups and Keyphrase: ""Overly trusted responses"""
arXIv2023,Fact-checking information generated by a large language model can decrease news discernment,Yes.,5,"""we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases",2023,2023-08-21T15:47:37Z,"Keyphrase: ""Limited accuracy in fact-checking""","""we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases Keyphrase: ""Limited accuracy in fact-checking"""
arXIv2023,SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding,Yes.,5,"""LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction",2023,2023-08-21T07:31:19Z,"Keyphrase: ""Limited natural language understanding""","""LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction Keyphrase: ""Limited natural language understanding"""
arXIv2023,FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models,Yes.,4,"""Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied."" and ""Experimental results reveal varying degrees of stereotypes and biases in five LLMs evaluated on Edu-FairMonitor.""",2023,2023-08-21T00:25:17Z,"Keyphrase: ""Persistent stereotype bias""","""Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied."" and ""Experimental results reveal varying degrees of stereotypes and biases in five LLMs evaluated on Edu-FairMonitor."" Keyphrase: ""Persistent stereotype bias"""
arXIv2023,Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation,Yes.,5,"""the reliability and robustness of the code generation from LLMs have not yet been thoroughly studied"" and ""even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software",2023,2023-08-20T18:36:28Z,"Keyphrase: ""Code generation reliability""","""the reliability and robustness of the code generation from LLMs have not yet been thoroughly studied"" and ""even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software Keyphrase: ""Code generation reliability"""
arXIv2023,A Survey on Fairness in Large Language Models,Yes.,4,"""LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms.""",2023,2023-08-20T03:30:22Z,"Keyphrase: ""Propagation of social bias""","""LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms."" Keyphrase: ""Propagation of social bias"""
arXIv2023,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Yes.,5,"""the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public"" and ""even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of",2023,2023-08-18T16:27:04Z,"Keyphrase: ""Ethical concerns and harmful output""","""the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public"" and ""even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of Keyphrase: ""Ethical concerns and harmful output"""
arXIv2023,RatGPT: Turning online LLMs into Proxies for Malware Attacks,Yes.,4,"""These studies covered scenarios that still require the attacker to be in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim."" and ""This proof-of-concept highlights significant cybersecurity issues with openly available plugins and L",2023,2023-08-17T20:54:39Z,"Keyphrase: ""Vulnerability to cybersecurity attacks""","""These studies covered scenarios that still require the attacker to be in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim."" and ""This proof-of-concept highlights significant cybersecurity issues with openly available plugins and L Keyphrase: ""Vulnerability to cybersecurity attacks"""
arXIv2023,MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models,Yes.,5,"""To evaluate the limitations, we performed an error analysis, which revealed conceptual errors (~64%) as the major contributor compared to computational errors (~36%) towards the reduced performance of LLMs.""",2023,2023-08-17T17:51:05Z,"Keyphrase: ""Conceptual errors outweigh computational errors""","""To evaluate the limitations, we performed an error analysis, which revealed conceptual errors (~64%) as the major contributor compared to computational errors (~36%) towards the reduced performance of LLMs."" Keyphrase: ""Conceptual errors outweigh computational errors"""
arXIv2023,Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning,Yes.,5,"""However, our findings spotlight both vision-language model's limitations",2023,2023-08-17T03:14:00Z,"Keyphrase: ""Limited vision-language integration""","""However, our findings spotlight both vision-language model's limitations Keyphrase: ""Limited vision-language integration"""
arXIv2023,An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning,Yes.,5,"""The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale increases, the severity of forgetting intensifies.""",2023,2023-08-17T02:53:23Z,"Keyphrase: ""Catastrophic forgetting""","""The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale increases, the severity of forgetting intensifies."" Keyphrase: ""Catastrophic forgetting"""
arXIv2023,Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models,Yes.,5,"""Unfortunately, these defenses are not foolproof, and some attackers have crafted 'jailbreak' prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions.""",2023,2023-08-16T09:04:36Z,Keyphrase: Vulnerability to crafted attacks,"""Unfortunately, these defenses are not foolproof, and some attackers have crafted 'jailbreak' prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions."" Keyphrase: Vulnerability to crafted attacks"
arXIv2023,Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation,Yes.,4,"""contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data.""",2023,2023-08-15T08:49:14Z,"Keyphrase: ""Struggles with coherence and diversity""","""contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data."" Keyphrase: ""Struggles with coherence and diversity"""
arXIv2023,Detecting The Corruption Of Online Questionnaires By Artificial Intelligence,Yes.,4,"""Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. ... Automatic AI detection systems are currently completely unusable.""",2023,2023-08-14T23:47:56Z,"Keyphrase: ""Vulnerability to manipulation""","""Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. ... Automatic AI detection systems are currently completely unusable."" Keyphrase: ""Vulnerability to manipulation"""
arXIv2023,CausalLM is not optimal for in-context learning,Yes.,5,"""Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples.""",2023,2023-08-14T03:14:38Z,"Keyphrase: ""Limited in-context learning""","""Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples."" Keyphrase: ""Limited in-context learning"""
arXIv2023,Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine,Yes.,4,"""One of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to make clinical decisions that are inherently different from the cognitive processes of clinicians.""",2023,2023-08-13T19:04:07Z,"Keyphrase: ""Uninterpretable decision-making""","""One of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to make clinical decisions that are inherently different from the cognitive processes of clinicians."" Keyphrase: ""Uninterpretable decision-making"""
arXIv2023,GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher,Yes.,5,"""We discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages.""",2023,2023-08-12T04:05:57Z,"Keyphrase: ""Safety alignment bypass""","""We discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages."" Keyphrase: ""Safety alignment bypass"""
arXIv2023,Dynamic Planning with a LLM,Yes.,5,"""applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows.""",2023,2023-08-11T21:17:13Z,"Keyphrase: ""Difficulty with complex multi-step reasoning""","""applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows."" Keyphrase: ""Difficulty with complex multi-step reasoning"""
arXIv2023,Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems,Yes.,4,"""there are still often 'interface' failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.""",2023,2023-08-10T17:22:28Z,"Keyphrase: ""Interface failure and formulation challenges""","""there are still often 'interface' failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems."" Keyphrase: ""Interface failure and formulation challenges"""
arXIv2023,C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT,Yes.,5,"""However, human forgetting and model contextual forgetting remain prominent issues in multi-turn conversation scenarios, which challenge the users' conversation comprehension and contextual continuity for ChatGPT.""",2023,2023-08-10T13:29:12Z,"Keyphrase: ""Contextual forgetting""","""However, human forgetting and model contextual forgetting remain prominent issues in multi-turn conversation scenarios, which challenge the users' conversation comprehension and contextual continuity for ChatGPT."" Keyphrase: ""Contextual forgetting"""
arXIv2023,LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following,Yes.,4,"""mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers"" and ""LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission",2023,2023-08-09T12:26:37Z,"Keyphrase: ""Privacy concerns and data safeguarding""","""mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers"" and ""LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission Keyphrase: ""Privacy concerns and data safeguarding"""
arXIv2023,CLEVA: Chinese Language Models EVAluation Platform,Yes.,4,"""The absence of a comprehensive Chinese benchmark that thoroughly assesses a model's performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs.""",2023,2023-08-09T09:11:31Z,"Keyphrase: ""Lack of standardized evaluation benchmarks""","""The absence of a comprehensive Chinese benchmark that thoroughly assesses a model's performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs."" Keyphrase: ""Lack of standardized evaluation benchmarks"""
arXIv2023,AgentSims: An Open-Source Sandbox for Large Language Model Evaluation,Yes.,5,"""Existing evaluation methods suffer from following shortcomings",2023,2023-08-08T03:59:28Z,"Keyphrase: ""Limitations in evaluation methods""","""Existing evaluation methods suffer from following shortcomings Keyphrase: ""Limitations in evaluation methods"""
arXIv2023,"""Do Anything Now"": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",Yes.,5,"""Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios.""",2023,2023-08-07T16:55:20Z,"Keyphrase: ""Inadequate safeguards against jailbreak prompts""","""Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios."" Keyphrase: ""Inadequate safeguards against jailbreak prompts"""
arXIv2023,AgentBench: Evaluating LLMs as Agents,Yes.,5,"""We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.""",2023,2023-08-07T16:08:11Z,"Keyphrase: ""Poor long-term reasoning""","""We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents."" Keyphrase: ""Poor long-term reasoning"""
arXIv2023,Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records,Yes.,5,"""the inability of LLMs to derive reasoning paradigms that allow for comprehensive understanding of medical variables"" and ""the need for LLM steering through the application of symbolic reasoning as the exclusive use of LLMs results in the lowest performance.""",2023,2023-08-07T07:29:49Z,"Keyphrase: ""Lack of reasoning capabilities""","""the inability of LLMs to derive reasoning paradigms that allow for comprehensive understanding of medical variables"" and ""the need for LLM steering through the application of symbolic reasoning as the exclusive use of LLMs results in the lowest performance."" Keyphrase: ""Lack of reasoning capabilities"""
arXIv2023,Studying Large Language Model Generalization with Influence Functions,Yes.,4,"""While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP)."" and ""Despite many apparently sophisticated forms of generalization, we identify a surprising limitation",2023,2023-08-07T04:47:42Z,"Keyphrase: ""Limited scalability due to complex computations""","""While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP)."" and ""Despite many apparently sophisticated forms of generalization, we identify a surprising limitation Keyphrase: ""Limited scalability due to complex computations"""
arXIv2023,Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023),Yes.,5,"""humans achieve their capacity for language after exposure to several orders of magnitude less data,"" ""LLMs currently show little promise of solving this mystery,"" ""LLMs cannot constitute scientific theories of language for several reasons,"" ""scientific theories must provide interpretable explanations, not just predictions.""",2023,2023-08-06T23:41:14Z,"Keyphrase: ""Limited interpretability and predictive power""","""humans achieve their capacity for language after exposure to several orders of magnitude less data,"" ""LLMs currently show little promise of solving this mystery,"" ""LLMs cannot constitute scientific theories of language for several reasons,"" ""scientific theories must provide interpretable explanations, not just predictions."" Keyphrase: ""Limited interpretability and predictive power"""
arXIv2023,TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties,Yes.,4,"""Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored."" and ""Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist,"" and ""instruction-tuned LLMs, however, trail behind commercial systems such as Google",2023,2023-08-06T08:29:16Z,"Keyphrase: ""Linguistic inclusivity challenges""","""Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored."" and ""Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist,"" and ""instruction-tuned LLMs, however, trail behind commercial systems such as Google Keyphrase: ""Linguistic inclusivity challenges"""
arXIv2023,An Empirical Study of AI-based Smart Contract Creation,Yes.,4,"""Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted.""",2023,2023-08-05T21:38:57Z,"Keyphrase: ""Security vulnerabilities and code quality""","""Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted."" Keyphrase: ""Security vulnerabilities and code quality"""
arXIv2023,Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology,Yes.,4,"""Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.""",2023,2023-08-04T07:51:15Z,"Keyphrase: ""Accuracy limitations in clinical trial matching""","""Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records."" Keyphrase: ""Accuracy limitations in clinical trial matching"""
arXIv2023,The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations,Yes.,4,"""We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women.""",2023,2023-08-03T21:12:54Z,"Keyphrase: ""Biased recommendations based on demographics""","""We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women."" Keyphrase: ""Biased recommendations based on demographics"""
arXIv2023,ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation,Yes.,5,"""First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs."" and ""Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes",2023,2023-08-03T16:31:02Z,"Keyphrase: ""Limited class-level code generation performance""","""First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs."" and ""Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes Keyphrase: ""Limited class-level code generation performance"""
arXIv2023,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models,Yes.,5,"""Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content."" and ""we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way."" and ""highlight systematic failure modes in state-of",2023,2023-08-02T16:30:40Z,"Keyphrase: ""Vulnerability to malicious instructions""","""Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content."" and ""we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way."" and ""highlight systematic failure modes in state-of Keyphrase: ""Vulnerability to malicious instructions"""
arXIv2023,SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning,Yes.,5,"""However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes.""",2023,2023-08-01T10:31:36Z,"Keyphrase: ""Limited nonlinear thinking""","""However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes."" Keyphrase: ""Limited nonlinear thinking"""
arXIv2023,Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias,Yes.,5,"""Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4.""",2023,2023-08-01T01:39:25Z,"Keyphrase: ""Bias presence after instruction tuning""","""Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4."" Keyphrase: ""Bias presence after instruction tuning"""
arXIv2023,SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension,Yes.,4,"""By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research.""",2023,2023-07-30T04:25:16Z,"Keyphrase: ""Limited evaluation insights""","""By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research."" Keyphrase: ""Limited evaluation insights"""
arXIv2023,"Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system",Yes.,4,"""Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context."" and ""However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics.""",2023,2023-07-28T20:25:11Z,"Keyphrase: ""Lack of personal relevance""","""Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context."" and ""However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics."" Keyphrase: ""Lack of personal relevance"""
arXIv2023,"A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI",Yes.,4,"""It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity.""",2023,2023-07-28T09:20:22Z,"Keyphrase: ""Bias sensitivity""","""It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity."" Keyphrase: ""Bias sensitivity"""
arXIv2023,Med-HALT: Medical Domain Hallucination Test for Large Language Models,Yes.,5,"""Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications.""",2023,2023-07-28T06:43:04Z,"Keyphrase: ""Generating unverified information""","""Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications."" Keyphrase: ""Generating unverified information"""
arXIv2023,An Overview Of Temporal Commonsense Reasoning and Acquisition,Yes.,5,"""Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps."" and ""However, these augmented models still struggle to approach human performance on reasoning tasks over temporal common sense",2023,2023-07-28T01:30:15Z,"Keyphrase: ""Limited reasoning capability""","""Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps."" and ""However, these augmented models still struggle to approach human performance on reasoning tasks over temporal common sense Keyphrase: ""Limited reasoning capability"""
arXIv2023,Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback,Yes.,4,"""RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs)."" and ""survey open problems and fundamental limitations of RLHF and related methods.""",2023,2023-07-27T22:29:25Z,"Keyphrase: ""Fundamental limitations in fine-tuning""","""RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs)."" and ""survey open problems and fundamental limitations of RLHF and related methods."" Keyphrase: ""Fundamental limitations in fine-tuning"""
arXIv2023,This is not correct! Negation-aware Evaluation of Language Generation Systems,Yes.,5,"""Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations.""",2023,2023-07-26T06:54:31Z,"Keyphrase: ""Insensitive to negation""","""Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations."" Keyphrase: ""Insensitive to negation"""
arXIv2023,Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data,Yes.,4,"""However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health."" and ""Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias.""",2023,2023-07-26T06:00:50Z,"Keyphrase: ""Racial and gender bias""","""However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health."" and ""Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias."" Keyphrase: ""Racial and gender bias"""
arXIv2023,Is GPT a Computational Model of Emotion? Detailed Analysis,Yes.,5,"""GPT faces difficulties predicting emotion intensity and coping responses"" and ""fell short in the second, despite providing superior results after minor prompt engineering.""",2023,2023-07-25T19:34:44Z,"Keyphrase: ""Difficulty predicting emotion intensity""","""GPT faces difficulties predicting emotion intensity and coping responses"" and ""fell short in the second, despite providing superior results after minor prompt engineering."" Keyphrase: ""Difficulty predicting emotion intensity"""
arXIv2023,ARB: Advanced Reasoning Benchmark for Large Language Models,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains.""",2023,2023-07-25T17:55:19Z,"Keyphrase: ""Limited expert performance""","""Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains."" Keyphrase: ""Limited expert performance"""
arXIv2023,Aligning Large Language Models with Human: A Survey,Yes.,4,"""Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information.""",2023,2023-07-24T17:44:58Z,"Keyphrase: ""Biased and factually incorrect content""","""Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information."" Keyphrase: ""Biased and factually incorrect content"""
arXIv2023,Interpretable Stereotype Identification through Reasoning,Yes.,4,"""Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination.""",2023,2023-07-24T15:12:13Z,"Keyphrase: ""Inherent bias perpetuation""","""Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination."" Keyphrase: ""Inherent bias perpetuation"""
arXIv2023,"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",Yes.,5,"""the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.""",2023,2023-07-24T14:56:30Z,"Keyphrase: ""Limited context and inductive bias""","""the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML."" Keyphrase: ""Limited context and inductive bias"""
arXIv2023,Performance of Large Language Models in a Computer Science Degree Program,Yes.,4,"""We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program"" and ""Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations.""",2023,2023-07-24T14:17:00Z,"Keyphrase: ""Limitation in mathematical calculation""","""We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program"" and ""Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations."" Keyphrase: ""Limitation in mathematical calculation"""
arXIv2023,Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models,Yes.,5,"""However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects.""",2023,2023-07-24T07:40:59Z,"Keyphrase: ""Limited interaction capabilities""","""However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects."" Keyphrase: ""Limited interaction capabilities"""
arXIv2023,The Effectiveness of Large Language Models (ChatGPT and CodeBERT) for Security-Oriented Code Analysis,Yes.,4,"""However, we observed that the strengths and limitations of adopting these LLMs to the code analysis have not been investigated."" and ""However, it is essential to acknowledge certain limitations, such as the heavy reliance on well-defined variable and function names, making them unable to learn from anonymized code.""",2023,2023-07-24T02:38:24Z,"Keyphrase: ""Limited ability to learn anonymized code""","""However, we observed that the strengths and limitations of adopting these LLMs to the code analysis have not been investigated."" and ""However, it is essential to acknowledge certain limitations, such as the heavy reliance on well-defined variable and function names, making them unable to learn from anonymized code."" Keyphrase: ""Limited ability to learn anonymized code"""
arXIv2023,In-Context Learning Learns Label Relationships but Is Not Conventional Learning,Yes.,4,"""we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations,"" and ""ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.""",2023,2023-07-23T16:54:41Z,"Keyphrase: ""Struggle with incorporating in-context information""","""we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations,"" and ""ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally."" Keyphrase: ""Struggle with incorporating in-context information"""
arXIv2023,GPT-4 Can't Reason,Yes.,5,"""However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason."" and ""Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.""",2023,2023-07-21T17:04:25Z,"Keyphrase: ""Incapable of reasoning""","""However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason."" and ""Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning."" Keyphrase: ""Incapable of reasoning"""
arXIv2023,LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?,Yes.,5,"""we present the theoretical limitations of such semantic censorship approaches"" and ""highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities.""",2023,2023-07-20T09:25:02Z,"Keyphrase: ""Semantic censorship limitations""","""we present the theoretical limitations of such semantic censorship approaches"" and ""highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities."" Keyphrase: ""Semantic censorship limitations"""
arXIv2023,SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models,Yes.,5,"""The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%."" and ""we categorize the errors made by LLMs into ten problem-solving abilities.""",2023,2023-07-20T07:01:57Z,"Keyphrase: ""Limited problem-solving ability""","""The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%."" and ""we categorize the errors made by LLMs into ten problem-solving abilities."" Keyphrase: ""Limited problem-solving ability"""
arXIv2023,What can we learn from Data Leakage and Unlearning for Law?,Yes.,5,"""Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference."" and ""The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-t",2023,2023-07-19T22:14:58Z,"Keyphrase: ""Privacy concerns and data memorization""","""Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference."" and ""The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-t Keyphrase: ""Privacy concerns and data memorization"""
arXIv2023,Generating Mathematical Derivations with Large Language Models,Yes.,5,"""Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in conventional scores. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse",2023,2023-07-19T14:13:02Z,"Keyphrase: ""Sensitivity to perturbations""","""Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in conventional scores. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse Keyphrase: ""Sensitivity to perturbations"""
arXIv2023,CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility,Yes.,4,"""there is considerable room for improvement in terms of responsibility"" and ""evaluation of human values alignment is becoming increasingly important.""",2023,2023-07-19T01:22:40Z,"Keyphrase: ""Lack of human value alignment""","""there is considerable room for improvement in terms of responsibility"" and ""evaluation of human values alignment is becoming increasingly important."" Keyphrase: ""Lack of human value alignment"""
arXIv2023,Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study,Yes.,5,"""While state-of-the-art transformer models provide excellent results in text classification, most of them have limitations in the maximum sequence length of the input sequence. The majority of the transformer models are limited to 512 tokens, and therefore, they struggle with long document classification problems.""",2023,2023-07-18T18:21:26Z,"Keyphrase: ""Limited maximum sequence length""","""While state-of-the-art transformer models provide excellent results in text classification, most of them have limitations in the maximum sequence length of the input sequence. The majority of the transformer models are limited to 512 tokens, and therefore, they struggle with long document classification problems."" Keyphrase: ""Limited maximum sequence length"""
arXIv2023,Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications,Yes.,4,"""The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models."" and ""The discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities.""",2023,2023-07-18T11:38:45Z,"Keyphrase: ""Gender bias in language usage""","""The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models."" and ""The discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities."" Keyphrase: ""Gender bias in language usage"""
arXIv2023,Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations,Yes.,5,"""We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.""",2023,2023-07-17T17:41:47Z,"Keyphrase: ""Low precision in explanations""","""We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution."" Keyphrase: ""Low precision in explanations"""
arXIv2023,Measuring Faithfulness in Chain-of-Thought Reasoning,Yes.,5,"""it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning"" and ""As models become larger and more capable, they produce less faithful reasoning on most tasks we study.""",2023,2023-07-17T01:08:39Z,"Keyphrase: ""Lack of faithful reasoning""","""it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning"" and ""As models become larger and more capable, they produce less faithful reasoning on most tasks we study."" Keyphrase: ""Lack of faithful reasoning"""
arXIv2023,Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,Yes.,5,"""As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior."" and ""However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case.""",2023,2023-07-17T00:54:10Z,"Keyphrase: ""Limited verifiability of reasoning""","""As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior."" and ""However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case."" Keyphrase: ""Limited verifiability of reasoning"""
arXIv2023,The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant,Yes.,5,"""there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use.""",2023,2023-07-16T21:19:47Z,"Keyphrase: ""Inadequate for real-world clinical use""","""there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use."" Keyphrase: ""Inadequate for real-world clinical use"""
arXIv2023,Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study,Yes.,5,"""a major challenge is that low-bit quantization methods often lead to performance degradation"" and ""2-bit models encounter severe performance degradation on the test of these abilities.""",2023,2023-07-16T15:11:01Z,"Keyphrase: ""Performance degradation with lowbit quantization""","""a major challenge is that low-bit quantization methods often lead to performance degradation"" and ""2-bit models encounter severe performance degradation on the test of these abilities."" Keyphrase: ""Performance degradation with lowbit quantization"""
arXIv2023,Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models,Yes.,5,"""erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions.""",2023,2023-07-16T08:28:04Z,"Keyphrase: ""Trustworthiness concerns""","""erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions."" Keyphrase: ""Trustworthiness concerns"""
arXIv2023,Leveraging Large Language Models to Generate Answer Set Programs,Yes.,4,"""However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques.""",2023,2023-07-15T03:40:55Z,"Keyphrase: ""Limited reasoning capability""","""However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques."" Keyphrase: ""Limited reasoning capability"""
arXIv2023,Certified Robustness for Large Language Models with Self-Denoising,Yes.,5,"""their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments"" and ""randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius.""",2023,2023-07-14T05:40:24Z,"Keyphrase: ""Vulnerability to noisy input""","""their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments"" and ""randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius."" Keyphrase: ""Vulnerability to noisy input"""
arXIv2023,Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study,Yes.,5,"""Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications. Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the",2023,2023-07-13T02:31:55Z,"Keyphrase: ""High word error rate""","""Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications. Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the Keyphrase: ""High word error rate"""
arXIv2023,A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation,Yes.,5,"""However, these models often tend to 'hallucinate' which critically hampers their reliability.""",2023,2023-07-08T14:25:57Z,"Keyphrase: ""Hallucination tendency""","""However, these models often tend to 'hallucinate' which critically hampers their reliability."" Keyphrase: ""Hallucination tendency"""
arXIv2023,Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task,Yes.,5,"""some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks,"" and ""the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different",2023,2023-07-08T13:10:59Z,"Keyphrase: ""Limited performance in grammatical error correction""","""some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks,"" and ""the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different Keyphrase: ""Limited performance in grammatical error correction"""
arXIv2023,"Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions",Yes.,4,"""With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF.""",2023,2023-07-08T09:28:50Z,"Keyphrase: ""Challenge for RTBF compliance""","""With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF."" Keyphrase: ""Challenge for RTBF compliance"""
arXIv2023,Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models,Yes.,5,"""We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs."" and ""We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs.""",2023,2023-07-07T13:58:16Z,"Keyphrase: ""Overstated evaluation capabilities""","""We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs."" and ""We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs."" Keyphrase: ""Overstated evaluation capabilities"""
arXIv2023,TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction,Yes.,5,"""When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called $\textit{hallucinations}$.""",2023,2023-07-07T02:42:06Z,"Keyphrase: ""Textual hallucinations""","""When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called $\textit{hallucinations}$."" Keyphrase: ""Textual hallucinations"""
arXIv2023,Focused Transformer: Contrastive Training for Context Scaling,Yes.,5,"""However, the full potential of such an approach is often restrained due to a limitation in the effective context length."" and ""We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish.""",2023,2023-07-06T17:52:10Z,"Keyphrase: ""Context length limitation""","""However, the full potential of such an approach is often restrained due to a limitation in the effective context length."" and ""We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish."" Keyphrase: ""Context length limitation"""
arXIv2023,Style Over Substance: Evaluation Biases for Large Language Models,Yes.,4,"""Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors.""",2023,2023-07-06T14:42:01Z,"Keyphrase: ""Biased evaluation and factual errors""","""Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors."" Keyphrase: ""Biased evaluation and factual errors"""
arXIv2023,"Amplifying Limitations, Harms and Risks of Large Language Models",Yes.,5,"""We set out to highlight a number of limitations of LLMs, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations.""",2023,2023-07-06T11:53:45Z,"Keyphrase: ""Continuing harms and limitations""","""We set out to highlight a number of limitations of LLMs, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations."" Keyphrase: ""Continuing harms and limitations"""
arXIv2023,Scaling In-Context Demonstrations with Structured Attention,Yes.,5,"""their capabilities of in-context learning are limited by the model architecture",2023,2023-07-05T23:26:01Z,"Keyphrase: ""Limited in-context learning""","""their capabilities of in-context learning are limited by the model architecture Keyphrase: ""Limited in-context learning"""
arXIv2023,Jailbroken: How Does LLM Safety Training Fail?,Yes.,5,"""Large language models trained for safety and harmlessness remain susceptible to adversarial misuse,"" and ""We hypothesize two failure modes of safety training",2023,2023-07-05T17:58:10Z,"Keyphrase: ""Susceptibility to adversarial misuse""","""Large language models trained for safety and harmlessness remain susceptible to adversarial misuse,"" and ""We hypothesize two failure modes of safety training Keyphrase: ""Susceptibility to adversarial misuse"""
arXIv2023,Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks,Yes.,5,"""we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow",2023,2023-07-05T17:50:42Z,"Keyphrase: ""Limited abstract task-solving skills""","""we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow Keyphrase: ""Limited abstract task-solving skills"""
arXIv2023,External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback,Yes.,5,"""they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases.""",2023,2023-07-05T17:05:32Z,"Keyphrase: ""Limited contextual knowledge processing""","""they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases."" Keyphrase: ""Limited contextual knowledge processing"""
arXIv2023,ProPILE: Probing Privacy Leakage in Large Language Models,Yes.,4,"""The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII).""",2023,2023-07-04T18:53:47Z,"Keyphrase: ""Privacy concerns""","""The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII)."" Keyphrase: ""Privacy concerns"""
arXIv2023,Learning to Prompt in the Classroom to Understand AI Limits: A pilot study,Yes.,5,"""ignoring their limitations such as hallucinations and reasoning constraints"" and ""better grasp of limitations, specifically unreliability, limited understanding of commands leading to unsatisfactory responses, and limited presentation flexibility.""",2023,2023-07-04T07:51:37Z,"Keyphrase: ""Limited reasoning and understanding""","""ignoring their limitations such as hallucinations and reasoning constraints"" and ""better grasp of limitations, specifically unreliability, limited understanding of commands leading to unsatisfactory responses, and limited presentation flexibility."" Keyphrase: ""Limited reasoning and understanding"""
arXIv2023,CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care,Yes.,4,"""they suffer from the misinformation problem by unintentionally generating factually false statements"" and ""current Chinese LLMs are far from perfect in the topic of maternity and infant care.""",2023,2023-07-04T03:34:19Z,"Keyphrase: ""Misinformation generation""","""they suffer from the misinformation problem by unintentionally generating factually false statements"" and ""current Chinese LLMs are far from perfect in the topic of maternity and infant care."" Keyphrase: ""Misinformation generation"""
arXIv2023,Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models,Yes.,5,"""a persistent challenge lies in their susceptibility to 'hallucinations', which erodes trust in their outputs"" and ""existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities.""",2023,2023-07-03T22:17:16Z,"Keyphrase: ""Susceptibility to hallucination""","""a persistent challenge lies in their susceptibility to 'hallucinations', which erodes trust in their outputs"" and ""existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities."" Keyphrase: ""Susceptibility to hallucination"""
arXIv2023,Multilingual Language Models are not Multicultural: A Case Study in Emotion,Yes.,5,"""Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.""",2023,2023-07-03T21:54:28Z,"Keyphrase: ""Limited cultural nuance understanding""","""Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this."" Keyphrase: ""Limited cultural nuance understanding"""
arXIv2023,Challenges in Domain-Specific Abstractive Summarization and How to Overcome them,Yes.,5,"""This paper identifies three of those limitations as research problems in the context of abstractive text summarization",2023,2023-07-03T12:26:44Z,"Keyphrase: ""Limitations in abstractive text summarization""","""This paper identifies three of those limitations as research problems in the context of abstractive text summarization Keyphrase: ""Limitations in abstractive text summarization"""
arxiv2024,A Study on Large Language Models' Limitations in Multiple-Choice Question Answering,Yes.,5,"""Despite their ubiquitous use, there is no systematic analysis of their specific capabilities and limitations."" and ""We analyze 26 small open-source models and find that 65% of the models do not understand the task, only 4 models properly select an answer from the given choices, and only 5 of these models are choice order independent.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited task understanding""","""Despite their ubiquitous use, there is no systematic analysis of their specific capabilities and limitations."" and ""We analyze 26 small open-source models and find that 65% of the models do not understand the task, only 4 models properly select an answer from the given choices, and only 5 of these models are choice order independent."" Keyphrase: ""Limited task understanding"""
arxiv2024,The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models,Yes.,5,"""hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications"" and ""To tackle the LLM hallucination, three key questions should be well studied",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination of factually incorrect content""","""hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications"" and ""To tackle the LLM hallucination, three key questions should be well studied Keyphrase: ""Hallucination of factually incorrect content"""
arxiv2024,LLM on FHIR -- Demystifying Health Records,Yes.,4,"""However, challenges included variability in LLM responses and the need for precise filtering of health data."" and ""While promising, the implementation and pilot also highlight risks such as inconsistent responses and the importance of replicable output.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent responses""","""However, challenges included variability in LLM responses and the need for precise filtering of health data."" and ""While promising, the implementation and pilot also highlight risks such as inconsistent responses and the importance of replicable output."" Keyphrase: ""Inconsistent responses"""
arxiv2024,RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning,Yes.,5,"""Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world."" and ""the performance of GPT-4 even drops significantly from 80.00 to 58",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited real-world performance and stability""","""Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world."" and ""the performance of GPT-4 even drops significantly from 80.00 to 58 Keyphrase: ""Limited real-world performance and stability"""
arxiv2024,"The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey",Yes.,5,"""However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitation in context length extrapolation""","""However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation."" Keyphrase: ""Limitation in context length extrapolation"""
arxiv2024,LLMs for Relational Reasoning: How Far are We?,Yes.,5,"""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."" and ""Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Poor reasoning ability""","""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."" and ""Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and Keyphrase: ""Poor reasoning ability"""
arxiv2024,Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning,Yes.,4,"""Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content."" and ""prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100%",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to jailbreaking attacks""","""Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content."" and ""prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100% Keyphrase: ""Vulnerability to jailbreaking attacks"""
arxiv2024,FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference,Yes.,5,"""The large number of parameters in Pretrained Language Models enhance their performance, but also make them resource-intensive, making it challenging to deploy them on commodity hardware like a single GPU."" and ""Due to the memory and power limitations of these devices, model compression techniques are often used to decrease both the model's size and its inference latency. This usually results in a trade-off between model accuracy",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Resource-intensive deployment""","""The large number of parameters in Pretrained Language Models enhance their performance, but also make them resource-intensive, making it challenging to deploy them on commodity hardware like a single GPU."" and ""Due to the memory and power limitations of these devices, model compression techniques are often used to decrease both the model's size and its inference latency. This usually results in a trade-off between model accuracy Keyphrase: ""Resource-intensive deployment"""
arxiv2024,LLMs for Test Input Generation for Semantic Caches,Yes.,4,"""However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience."" and ""Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational cost and error-prone semantic caching""","""However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience."" and ""Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system."" Keyphrase: ""High computational cost and error-prone semantic caching"""
arxiv2024,GRATH: Gradual Self-Truthifying for Large Language Models,Yes.,5,"""existing LLMs still struggle with generating truthful content, as evidenced by their modest performance on benchmarks like TruthfulQA.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggle with generating truthful content""","""existing LLMs still struggle with generating truthful content, as evidenced by their modest performance on benchmarks like TruthfulQA."" Keyphrase: ""Struggle with generating truthful content"""
arxiv2024,Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling,Yes.,5,"""This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational and data requirements""","""This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web."" Keyphrase: ""High computational and data requirements"""
arxiv2024,Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning,Yes.,5,"""Despite being widely applied, in-context learning is vulnerable to malicious attacks."" and ""Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to malicious attacks""","""Despite being widely applied, in-context learning is vulnerable to malicious attacks."" and ""Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model."" Keyphrase: ""Vulnerability to malicious attacks"""
arxiv2024,Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately,Yes.,5,"""Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Suboptimal quality answers""","""Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions."" Keyphrase: ""Suboptimal quality answers"""
arxiv2024,TrustLLM: Trustworthiness in Large Language Models,Yes.,5,"""Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. ... discussion of open challenges and future directions. ... our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. ... some LLMs may be overly calibrated towards exhibiting trustworthiness",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Trustworthiness concerns""","""Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. ... discussion of open challenges and future directions. ... our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. ... some LLMs may be overly calibrated towards exhibiting trustworthiness Keyphrase: ""Trustworthiness concerns"""
arxiv2024,Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering,Yes.,5,"""Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Generation of toxic responses""","""Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community."" Keyphrase: ""Generation of toxic responses"""
arxiv2024,CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities,Yes.,5,"""Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems."" and ""Using this corpus, we find that models often arrive at the correct final answer",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited mathematical reasoning ability""","""Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems."" and ""Using this corpus, we find that models often arrive at the correct final answer Keyphrase: ""Limited mathematical reasoning ability"""
arxiv2024,A Computational Framework for Behavioral Assessment of LLM Therapists,Yes.,5,"""Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences."" and ""Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent with high-quality care""","""Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences."" and ""Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care Keyphrase: ""Inconsistent with high-quality care"""
arxiv2024,Navigating the OverKill in Large Language Models,Yes.,5,"""Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Potential overkill in refusal to answer benign queries""","""Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries."" Keyphrase: ""Potential overkill in refusal to answer benign queries"""
arxiv2024,E^2-LLM: Efficient and Extreme Length Extension of Large Language Models,Yes.,5,"""Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational cost""","""Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources."" Keyphrase: ""High computational cost"""
arxiv2024,LoMA: Lossless Compressed Memory Attention,Yes.,5,"""Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Resource-intensive limitations""","""Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts."" Keyphrase: ""Resource-intensive limitations"""
arxiv2024,Can AI Assistants Know What They Don't Know?,Yes.,5,"""Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual errors in knowledge-intensive tasks""","""Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications."" Keyphrase: ""Factual errors in knowledge-intensive tasks"""
arxiv2024,CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs,Yes.,5,"""Large Multimodal Models (LMMs) encounter two issues in such scenarios",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Multimodal challenges""","""Large Multimodal Models (LMMs) encounter two issues in such scenarios Keyphrase: ""Multimodal challenges"""
arxiv2024,ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters,Yes.,5,"""achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited real-time inference efficiency""","""achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context."" Keyphrase: ""Limited real-time inference efficiency"""
arxiv2024,Physio: An LLM-Based Physiotherapy Advisor,Yes.,5,"""However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Trustworthiness in healthcare domain""","""However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being."" Keyphrase: ""Trustworthiness in healthcare domain"""
arxiv2024,"Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",Yes.,5,"""In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Error-prone code generation""","""In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs."" Keyphrase: ""Error-prone code generation"""
arxiv2024,ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios,Yes.,5,"""Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited cognitive ability""","""Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning."" Keyphrase: ""Limited cognitive ability"""
arxiv2024,Dynamic Q&A of Clinical Documents with Large Language Models,Yes.,4,"""Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited diversity in medical case evaluation""","""Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain."" Keyphrase: ""Limited diversity in medical case evaluation"""
arxiv2024,Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks,Yes.,5,"""language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior."" Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,Security Code Review by LLMs: A Deep Dive into Responses,Yes.,5,"""Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Verbosity and vagueness""","""Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection."" Keyphrase: ""Verbosity and vagueness"""
arxiv2024,EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge,Yes.,4,"""However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited medical knowledge and relevance""","""However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English."" Keyphrase: ""Limited medical knowledge and relevance"""
arxiv2024,Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis,Yes.,4,"""However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited generalizability""","""However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount."" Keyphrase: ""Limited generalizability"""
arxiv2024,How well can large language models explain business processes?,Yes.,5,"""Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited capacity for fulfilling tasks""","""Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason."" Keyphrase: ""Limited capacity for fulfilling tasks"""
arxiv2024,Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models,Yes.,5,"""a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination in video processing""","""a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level."" Keyphrase: ""Hallucination in video processing"""
arxiv2024,Large Language Models for Mathematical Reasoning: Progresses and Challenges,Yes.,4,"""an overview of factors and concerns affecting LLMs in solving math"" and ""an elucidation of the persisting challenges within this domain.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Challenges in math problem-solving""","""an overview of factors and concerns affecting LLMs in solving math"" and ""an elucidation of the persisting challenges within this domain."" Keyphrase: ""Challenges in math problem-solving"""
arxiv2024,Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering,Yes.,5,"""While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Prone to hallucination""","""While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers."" Keyphrase: ""Prone to hallucination"""
arxiv2024,Seven Failure Points When Engineering a Retrieval Augmented Generation System,Yes.,4,"""RAG systems aim to",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited in scope""","""RAG systems aim to Keyphrase: ""Limited in scope"""
arxiv2024,LLMs for Robotic Object Disambiguation,Yes.,5,"""Despite multiple query attempts with zero-shot prompt engineering (details can be found in the Appendix), the LLM struggled to inquire about features not explicitly provided in the scene description.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggles with zero-shot prompts""","""Despite multiple query attempts with zero-shot prompt engineering (details can be found in the Appendix), the LLM struggled to inquire about features not explicitly provided in the scene description."" Keyphrase: ""Struggles with zero-shot prompts"""
arxiv2024,VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks,Yes.,5,"""Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited multimodal capabilities""","""Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents."" Keyphrase: ""Limited multimodal capabilities"""
arxiv2024,DocFinQA: A Long-Context Financial Reasoning Dataset,Yes.,5,"""DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggles with long documents""","""DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents."" Keyphrase: ""Struggles with long documents"""
arxiv2024,Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation,Yes.,5,"""existing approaches struggle with hallucinations and overconfident predictions.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination and overconfidence""","""existing approaches struggle with hallucinations and overconfident predictions."" Keyphrase: ""Hallucination and overconfidence"""
arxiv2024,Are self-explanations from Large Language Models faithful?,Yes.,5,"""convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk"" and ""showing self-explanations should not be trusted in general.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unsupported self-explanations""","""convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk"" and ""showing self-explanations should not be trusted in general."" Keyphrase: ""Unsupported self-explanations"""
arxiv2024,MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries,Yes.,4,"""However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inadequate for multihop queries""","""However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence."" Keyphrase: ""Inadequate for multihop queries"""
arxiv2024,CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning,Yes.,4,"""However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge."" and ""The results demonstrate that CMMU poses a significant challenge to the recent MLLMs.""",2024,2024-01-01T00:00:00Z,Keyphrase: Lack of domain-specific knowledge,"""However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge."" and ""The results demonstrate that CMMU poses a significant challenge to the recent MLLMs."" Keyphrase: Lack of domain-specific knowledge"
arxiv2024,JumpCoder: Go Beyond Autoregressive Coder via Online Modification,Yes.,5,"""While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Lack of reversibility""","""While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance."" Keyphrase: ""Lack of reversibility"""
arxiv2024,LightHouse: A Survey of AGI Hallucination,Yes.,4,"""numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research"" and ""Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models).""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination bottleneck""","""numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research"" and ""Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models)."" Keyphrase: ""Hallucination bottleneck"""
arxiv2024,Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences,Yes.,5,"""we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggle with dynamic information""","""we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors."" Keyphrase: ""Struggle with dynamic information"""
arxiv2024,Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models,Yes.,5,"""Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance."" and ""This research critically examines these biases and quantifies the effects on a representative list selection task.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inherent cognitive bias""","""Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance."" and ""This research critically examines these biases and quantifies the effects on a representative list selection task."" Keyphrase: ""Inherent cognitive bias"""
arxiv2024,MouSi: Poly-Visual-Expert Vision-Language Models,Yes.,4,"""Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Challenges with complex visual information""","""Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information."" Keyphrase: ""Challenges with complex visual information"""
arxiv2024,Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet,Yes.,5,"""We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited problem-solving capabilities""","""We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis."" Keyphrase: ""Limited problem-solving capabilities"""
arxiv2024,Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do,Yes.,5,"""We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception,"" and ""Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others?""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited empathy capability""","""We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception,"" and ""Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others?"" Keyphrase: ""Limited empathy capability"""
arxiv2024,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,Yes.,5,"""Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings,"" and ""MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Systematic visual shortcomings""","""Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings,"" and ""MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations."" Keyphrase: ""Systematic visual shortcomings"""
arxiv2024,Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,Yes.,5,"""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited reasoning capability""","""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount."" Keyphrase: ""Limited reasoning capability"""
arxiv2024,Under the Surface: Tracking the Artifactuality of LLM-Generated Data,Yes.,5,"""This paper reveals significant hidden disparities, especially in complex tasks where LLMs often miss the nuanced understanding of intrinsic human-generated content,"" and ""It highlights the LLMs' shortcomings in replicating human traits and behaviors, underscoring the importance of addressing biases and artifacts produced in LLM-generated content for future research and development.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Misses nuanced understanding""","""This paper reveals significant hidden disparities, especially in complex tasks where LLMs often miss the nuanced understanding of intrinsic human-generated content,"" and ""It highlights the LLMs' shortcomings in replicating human traits and behaviors, underscoring the importance of addressing biases and artifacts produced in LLM-generated content for future research and development."" Keyphrase: ""Misses nuanced understanding"""
arxiv2024,Generalist embedding models are better at short-context clinical semantic search than specialized embedding models,Yes.,4,"""Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs."" and ""The highlighted problem of specialized models may be due to the fact that they have not been trained on sufficient data, and in particular on datasets that are not diverse enough to have a reliable global language understanding,",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited robustness in specialized domains""","""Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs."" and ""The highlighted problem of specialized models may be due to the fact that they have not been trained on sufficient data, and in particular on datasets that are not diverse enough to have a reliable global language understanding, Keyphrase: ""Limited robustness in specialized domains"""
arxiv2024,LongAlign: A Recipe for Long Context Alignment of Large Language Models,Yes.,5,"""Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Difficulty in handling long contexts""","""Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length."" Keyphrase: ""Difficulty in handling long contexts"""
arxiv2024,Benchmarking Large Language Models on Controllable Generation under Diversified Instructions,Yes.,5,"""revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited openness and transparency""","""revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs."" Keyphrase: ""Limited openness and transparency"""
arxiv2024,Gender Bias in Machine Translation and The Era of Large Language Models,Yes.,4,"""The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Bias in machine translation""","""The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies."" Keyphrase: ""Bias in machine translation"""
arxiv2024,Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications,Yes.,4,"""The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to injection attacks""","""The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate."" Keyphrase: ""Vulnerability to injection attacks"""
arxiv2024,Leveraging Large Language Models for NLG Evaluation: A Survey,Yes.,4,"""Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Lack of unified evaluation""","""Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques."" Keyphrase: ""Lack of unified evaluation"""
arxiv2024,Prompting Large Vision-Language Models for Compositional Reasoning,Yes.,5,"""However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset."" and ""this limitation stems from two factors",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited visiolinguistic compositionality""","""However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset."" and ""this limitation stems from two factors Keyphrase: ""Limited visiolinguistic compositionality"""
arxiv2024,MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models,Yes.,5,"""We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Performance degradation in multiturn setting""","""We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance."" Keyphrase: ""Performance degradation in multiturn setting"""
arxiv2024,OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models,Yes.,5,"""The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Poor performance on benchmarks""","""The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field."" Keyphrase: ""Poor performance on benchmarks"""
arxiv2024,Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability,Yes.,5,"""LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unreliable task planning""","""LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps."" Keyphrase: ""Unreliable task planning"""
arxiv2024,From Prompt Engineering to Prompt Science With Human in the Loop,Yes.,5,"""we need to be concerned about how it may affect that research, its findings, or any future works based on that research,"" and ""they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Lack of replicable generalizable knowledge""","""we need to be concerned about how it may affect that research, its findings, or any future works based on that research,"" and ""they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor."" Keyphrase: ""Lack of replicable generalizable knowledge"""
arxiv2024,When Large Language Models Meet Vector Databases: A Survey,Yes.,5,"""With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Outdated knowledge and memory issues""","""With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues."" Keyphrase: ""Outdated knowledge and memory issues"""
arxiv2024,OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models,Yes.,5,"""This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Sequential task performance degradation""","""This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped."" Keyphrase: ""Sequential task performance degradation"""
arxiv2024,Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches,Yes.,5,"""although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Reliability and privacy concerns""","""although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation."" Keyphrase: ""Reliability and privacy concerns"""
arxiv2024,InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification,Yes.,5,"""our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Difficulty in identifying information loss""","""our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss."" Keyphrase: ""Difficulty in identifying information loss"""
arxiv2024,ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers,Yes.,5,"""The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited domain-specific depth""","""The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models."" Keyphrase: ""Limited domain-specific depth"""
arxiv2024,Security and Privacy Challenges of Large Language Models: A Survey,Yes.,5,"""While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to security and privacy attacks""","""While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks."" Keyphrase: ""Vulnerability to security and privacy attacks"""
arxiv2024,Hallucination Benchmark in Medical Visual Question Answering,Yes.,5,"""these models are not extensively tested on the hallucination phenomenon in clinical settings"" and ""The study provides an in-depth analysis of current models' limitations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited testing for hallucination phenomenon""","""these models are not extensively tested on the hallucination phenomenon in clinical settings"" and ""The study provides an in-depth analysis of current models' limitations."" Keyphrase: ""Limited testing for hallucination phenomenon"""
arxiv2024,Knowledge Verification to Nip Hallucination in the Bud,Yes.,5,"""they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual contradictions""","""they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination."" Keyphrase: ""Factual contradictions"""
arxiv2024,Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness,Yes.,5,"""However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective."" and ""Our results shed light on the limitations of the LLM chatbots when compared to specialized models.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited effectiveness in cybersecurity entity recognition""","""However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective."" and ""Our results shed light on the limitations of the LLM chatbots when compared to specialized models."" Keyphrase: ""Limited effectiveness in cybersecurity entity recognition"""
arxiv2024,Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values,Yes.,4,"""troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Cultural bias and authoritarian tendencies""","""troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies."" Keyphrase: ""Cultural bias and authoritarian tendencies"""
arxiv2024,Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine,Yes.,5,"""we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Flawed rationale in decision-making""","""we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows."" Keyphrase: ""Flawed rationale in decision-making"""
arxiv2024,On Prompt-Driven Safeguarding for Large Language Models,Yes.,5,"""We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Safety prompt bias""","""We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless."" Keyphrase: ""Safety prompt bias"""
arxiv2024,Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation,Yes.,5,"""Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited crosslingual capability""","""Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations."" Keyphrase: ""Limited crosslingual capability"""
arxiv2024,Detection of Machine-Generated Text: Literature Survey,Yes.,4,"""Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes."" and ""To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hazardous societal influence""","""Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes."" and ""To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented."" Keyphrase: ""Hazardous societal influence"""
arxiv2024,TOFU: A Task of Fictitious Unlearning for LLMs,Yes.,5,"""Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns."" and ""Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Privacy concerns and lack of effective unlearning""","""Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns."" and ""Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."" Keyphrase: ""Privacy concerns and lack of effective unlearning"""
arxiv2024,Extending LLMs' Context Window with 100 Samples,Yes.,5,"""Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited extrapolation ability""","""Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs."" Keyphrase: ""Limited extrapolation ability"""
arxiv2024,MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline,Yes.,5,"""there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited mathematical reasoning capability""","""there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints."" Keyphrase: ""Limited mathematical reasoning capability"""
arxiv2024,"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",Yes.,5,"""the safety and security issues of LLM systems have become the major obstacle to their widespread application"" and ""potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Safety and security concerns""","""the safety and security issues of LLM systems have become the major obstacle to their widespread application"" and ""potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies."" Keyphrase: ""Safety and security concerns"""
arxiv2024,Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning,Yes.,5,"""Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent explanations""","""Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs."" Keyphrase: ""Inconsistent explanations"""
arxiv2024,Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?,Yes.,5,"""The paper discusses what is needed to address the limitations of current LLM-centered AI systems.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitations in addressing current LLM-centered AI system""","""The paper discusses what is needed to address the limitations of current LLM-centered AI systems."" Keyphrase: ""Limitations in addressing current LLM-centered AI system"""
arxiv2024,Small Language Model Can Self-correct,Yes.,5,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone"" and ""large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Generating inaccurate false information""","""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone"" and ""large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans."" Keyphrase: ""Generating inaccurate false information"""
arxiv2024,Conditional and Modal Reasoning in Large Language Models,Yes.,5,"""Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Logical inconsistencies in inference""","""Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals."" Keyphrase: ""Logical inconsistencies in inference"""
arxiv2024,Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?,Yes.,5,"""The models showed significantly reduced accuracy on tasks with suspected hierarchical bias.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hierarchical bias affecting accuracy""","""The models showed significantly reduced accuracy on tasks with suspected hierarchical bias."" Keyphrase: ""Hierarchical bias affecting accuracy"""
arxiv2024,Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring,Yes.,4,"""The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM)."" and ""highlight the potential requirements and limitations of utilizing chatbots in conversational explainability.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitation in conversational explainability""","""The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM)."" and ""highlight the potential requirements and limitations of utilizing chatbots in conversational explainability."" Keyphrase: ""Limitation in conversational explainability"""
arxiv2024,Learning Shortcuts: On the Misleading Promise of NLU in Language Models,Yes.,5,"""LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Illusion of enhanced performance""","""LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules."" Keyphrase: ""Illusion of enhanced performance"""
arxiv2024,Detecting Multimedia Generated by Large AI Models: A Survey,Yes.,4,"""this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns."" and ""we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Ethical concerns and societal risks""","""this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns."" and ""we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs."" Keyphrase: ""Ethical concerns and societal risks"""
arxiv2024,SocraSynth: Multi-LLM Reasoning with Conditional Statistics,Yes.,4,"""Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Bias, hallucination, and lack of reasoning""","""Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability."" Keyphrase: ""Bias, hallucination, and lack of reasoning"""
arxiv2024,Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation,Yes.,5,"""Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unmeasured representational harm""","""Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated."" Keyphrase: ""Unmeasured representational harm"""
arxiv2024,The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance,Yes.,5,"""We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Sensitivity to small perturbations""","""We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs."" Keyphrase: ""Sensitivity to small perturbations"""
arxiv2024,MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance,Yes.,5,"""This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Catastrophic forgetting""","""This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning."" Keyphrase: ""Catastrophic forgetting"""
arxiv2024,Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review,Yes.,4,"""the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Ethical and privacy challenges""","""the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations."" Keyphrase: ""Ethical and privacy challenges"""
arxiv2024,Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging,Yes.,5,"""The outcomes reveal a 100% success rate in manipulating PLIP's predictions, underscoring its susceptibility to adversarial perturbations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Susceptibility to adversarial perturbation""","""The outcomes reveal a 100% success rate in manipulating PLIP's predictions, underscoring its susceptibility to adversarial perturbations."" Keyphrase: ""Susceptibility to adversarial perturbation"""
arxiv2024,A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models,Yes.,5,"""a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded,"" and ""The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations,"" and ""we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of L",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual hallucination""","""a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded,"" and ""The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations,"" and ""we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of L Keyphrase: ""Factual hallucination"""
arxiv2024,The Reasoning Under Uncertainty Trap: A Structural AI Risk,Yes.,5,"""we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Computational explosiveness and deep uncertainty""","""we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy."" Keyphrase: ""Computational explosiveness and deep uncertainty"""
arxiv2024,Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs,Yes.,5,"""they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications.""",2024,2024-03-30T22:41:05Z,"Keyphrase: ""Vulnerability to poisoned external evidence""","""they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications."" Keyphrase: ""Vulnerability to poisoned external evidence"""
arxiv2024,Linguistic Calibration of Language Models,Yes.,5,"""Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate.""",2024,2024-03-30T20:47:55Z,"Keyphrase: ""Confident hallucination""","""Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate."" Keyphrase: ""Confident hallucination"""
arxiv2024,NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,Yes.,5,"""Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation.""",2024,2024-03-30T19:46:59Z,"Keyphrase: ""Difficulty with numerical data""","""Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation."" Keyphrase: ""Difficulty with numerical data"""
arxiv2024,Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order,Yes.,4,"""However, such existing models face challenges",2024,2024-03-30T15:38:54Z,"Keyphrase: ""Challenges in existing models""","""However, such existing models face challenges Keyphrase: ""Challenges in existing models"""
arxiv2024,Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,Yes.,5,"""Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving.""",2024,2024-03-30T12:48:31Z,"Keyphrase: ""Limited mathematical problem-solving capabilities""","""Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving."" Keyphrase: ""Limited mathematical problem-solving capabilities"""
arxiv2024,Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark,Yes.,5,"""All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%."" and ""This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing.""",2024,2024-03-30T02:08:28Z,"Keyphrase: ""Factual hallucination despite decoding improvements""","""All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%."" and ""This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing."" Keyphrase: ""Factual hallucination despite decoding improvements"""
arxiv2024,Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,Yes.,4,"""adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date.""",2024,2024-03-30T01:56:07Z,"Keyphrase: ""Limited ability to incorporate out-of-domain knowledge""","""adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date."" Keyphrase: ""Limited ability to incorporate out-of-domain knowledge"""
arxiv2024,Conceptual and Unbiased Reasoning in Language Models,Yes.,5,"""limited study has been done on large language models' capability to perform conceptual reasoning"" and ""existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods.""",2024,2024-03-30T00:53:53Z,"Keyphrase: ""Limited conceptual reasoning""","""limited study has been done on large language models' capability to perform conceptual reasoning"" and ""existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods."" Keyphrase: ""Limited conceptual reasoning"""
arxiv2024,"Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value",Yes.,5,"""the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations."" and ""This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation."" and ""underscore the need for a more nuanced",2024,2024-03-29T22:49:43Z,"Keyphrase: ""Uncertain validity in human subject simulation""","""the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations."" and ""This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation."" and ""underscore the need for a more nuanced Keyphrase: ""Uncertain validity in human subject simulation"""
arxiv2024,Uncovering Bias in Large Vision-Language Models with Counterfactuals,Yes.,4,"""While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs.""",2024,2024-03-29T21:45:53Z,"Keyphrase: ""Limited exploration of social bias""","""While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs."" Keyphrase: ""Limited exploration of social bias"""
arxiv2024,Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models,Yes.,5,"""extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements.""",2024,2024-03-29T17:59:53Z,"Keyphrase: ""Performance struggles on benchmarks""","""extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements."" Keyphrase: ""Performance struggles on benchmarks"""
arxiv2024,Are We on the Right Way for Evaluating Large Vision-Language Models?,Yes.,5,"""we dig into current evaluation works and identify two primary issues",2024,2024-03-29T17:59:34Z,"Keyphrase: ""Limited evaluation""","""we dig into current evaluation works and identify two primary issues Keyphrase: ""Limited evaluation"""
arxiv2024,LUQ: Long-text Uncertainty Quantification for LLMs,Yes.,5,"""Our study first highlights the limitations of current UQ methods in handling long text generation."" and ""We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about.""",2024,2024-03-29T16:49:24Z,"Keyphrase: ""Lack of confidence in generating long text""","""Our study first highlights the limitations of current UQ methods in handling long text generation."" and ""We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about."" Keyphrase: ""Lack of confidence in generating long text"""
arxiv2024,ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models,Yes.,4,"""Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation."" and ""Our findings suggest that while GPT-4's evaluation scores are correlated with human judges', its ability",2024,2024-03-29T16:13:31Z,"Keyphrase: ""Limited performance in sequential conversation understanding""","""Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation."" and ""Our findings suggest that while GPT-4's evaluation scores are correlated with human judges', its ability Keyphrase: ""Limited performance in sequential conversation understanding"""
arxiv2024,Distributed agency in second language learning and teaching through generative AI,Yes.,4,"""it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are created as well as practical constraints in their use, especially for less privileged populations.""",2024,2024-03-29T14:55:40Z,"Keyphrase: ""Limited understanding of nuanced social and cultural aspects""","""it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are created as well as practical constraints in their use, especially for less privileged populations."" Keyphrase: ""Limited understanding of nuanced social and cultural aspects"""
arxiv2024,H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model,Yes.,4,"""still perform poorly in Remote Sensing (RS) domain, which is due to the unique and specialized nature of RS imagery and the comparatively limited spatial perception of current VLMs"" and ""to address the inevitable 'hallucination' problem in RSVLM.""",2024,2024-03-29T14:50:43Z,"Keyphrase: ""Limited spatial perception""","""still perform poorly in Remote Sensing (RS) domain, which is due to the unique and specialized nature of RS imagery and the comparatively limited spatial perception of current VLMs"" and ""to address the inevitable 'hallucination' problem in RSVLM."" Keyphrase: ""Limited spatial perception"""
arxiv2024,IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context,Yes.,4,"""The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs)."" and ""We observed that the language models exhibit more bias across a majority of the intersectional groups.""",2024,2024-03-29T12:32:06Z,"Keyphrase: ""Biased language model performance""","""The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs)."" and ""We observed that the language models exhibit more bias across a majority of the intersectional groups."" Keyphrase: ""Biased language model performance"""
arxiv2024,Accurate Block Quantization in LLMs with Outliers,Yes.,4,"""The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block.""",2024,2024-03-29T12:15:06Z,"Keyphrase: ""Outlier weight activation affecting accuracy""","""The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block."" Keyphrase: ""Outlier weight activation affecting accuracy"""
arxiv2024,ITCMA: A Generative Agent Based on a Computational Consciousness Structure,Yes.,4,"""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior.""",2024,2024-03-29T10:23:18Z,"Keyphrase: ""Difficulty with implicit instructions and commonsense knowledge""","""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior."" Keyphrase: ""Difficulty with implicit instructions and commonsense knowledge"""
arxiv2024,Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning,Yes.,5,"""We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome.""",2024,2024-03-29T08:30:34Z,"Keyphrase: ""Limited error analysis""","""We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome."" Keyphrase: ""Limited error analysis"""
arxiv2024,On Large Language Models' Hallucination with Regard to Known Facts,Yes.,5,"""Large language models are successful in answering factoid questions but are also prone to hallucination."" and ""Our study shed light on understanding the reasons for LLMs' hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.""",2024,2024-03-29T06:48:30Z,"Keyphrase: ""Hallucination tendency""","""Large language models are successful in answering factoid questions but are also prone to hallucination."" and ""Our study shed light on understanding the reasons for LLMs' hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating."" Keyphrase: ""Hallucination tendency"""
arxiv2024,Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning,Yes.,4,"""However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4."" and ""As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance.""",2024,2024-03-29T03:48:12Z,"Keyphrase: ""Limited real-world performance""","""However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4."" and ""As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance."" Keyphrase: ""Limited real-world performance"""
arxiv2024,MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models,Yes.,5,"""Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them.""",2024,2024-03-29T01:53:24Z,"Keyphrase: ""Poor performance on challenging questions""","""Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them."" Keyphrase: ""Poor performance on challenging questions"""
arxiv2024,"Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",Yes.,5,"""current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary.""",2024,2024-03-28T21:18:33Z,"Keyphrase: ""Unsuitable for real-time applications""","""current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary."" Keyphrase: ""Unsuitable for real-time applications"""
arxiv2024,Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors,Yes.,5,"""However, we argue that a critical obstacle remains in deploying LLMs for practical use",2024,2024-03-28T12:05:15Z,"Keyphrase: ""Obstacle in practical deployment""","""However, we argue that a critical obstacle remains in deploying LLMs for practical use Keyphrase: ""Obstacle in practical deployment"""
arxiv2024,Large Language Models Are Unconscious of Unreasonability in Math Problems,Yes.,5,"""Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors."" and ""Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content.""",2024,2024-03-28T12:04:28Z,"Keyphrase: ""Hallucinatory responses""","""Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors."" and ""Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content."" Keyphrase: ""Hallucinatory responses"""
arxiv2024,Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models,Yes.,4,"""However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images.""",2024,2024-03-28T11:26:30Z,"Keyphrase: ""Limited image detail capture""","""However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images."" Keyphrase: ""Limited image detail capture"""
arxiv2024,MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation,Yes.,5,"""the quality of the text generated by these models often reveals persistent issues"" and ""it is filled with significant uncertainty and instability"" and ""addressing the uncertainties and instabilities in evaluating LLMs-generated text.""",2024,2024-03-28T10:41:47Z,"Keyphrase: ""Uncertainty and instability""","""the quality of the text generated by these models often reveals persistent issues"" and ""it is filled with significant uncertainty and instability"" and ""addressing the uncertainties and instabilities in evaluating LLMs-generated text."" Keyphrase: ""Uncertainty and instability"""
arxiv2024,Fine-Tuning Language Models with Reward Learning on Policy,Yes.,4,"""Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.""",2024,2024-03-28T10:02:10Z,"Keyphrase: ""Difficulty in adapting to changing data distribution""","""Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize."" Keyphrase: ""Difficulty in adapting to changing data distribution"""
arxiv2024,"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",Yes.,5,"""prior benchmarks contain only a very limited set of problems, both in quantity and variety,"" ""many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data,"" ""there is a significant drop in performance (on average 39.4%) when using EvoEval,"" ""the brittleness of instruction-following models when encountering reword",2024,2024-03-28T03:10:39Z,"Keyphrase: ""Data leakage and brittleness""","""prior benchmarks contain only a very limited set of problems, both in quantity and variety,"" ""many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data,"" ""there is a significant drop in performance (on average 39.4%) when using EvoEval,"" ""the brittleness of instruction-following models when encountering reword Keyphrase: ""Data leakage and brittleness"""
arxiv2024,FACTOID: FACtual enTailment fOr hallucInation Detection,Yes.,5,"""However, hallucination is a significant concern."" and ""current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted.""",2024,2024-03-28T03:09:42Z,"Keyphrase: ""Inaccurate annotation""","""However, hallucination is a significant concern."" and ""current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted."" Keyphrase: ""Inaccurate annotation"""
arxiv2024,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Yes.,5,"""Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content."" and ""Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address.""",2024,2024-03-28T02:44:02Z,"Keyphrase: ""Generation of harmful content""","""Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content."" and ""Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address."" Keyphrase: ""Generation of harmful content"""
arxiv2024,Learning From Correctness Without Prompting Makes LLM Efficient Reasoner,Yes.,5,"""Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content.""",2024,2024-03-28T02:12:49Z,"Keyphrase: ""Hallucination and toxic content""","""Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content."" Keyphrase: ""Hallucination and toxic content"""
arxiv2024,LITA: Language Instructed Temporal-Localization Assistant,Yes.,5,"""However, an important missing piece is temporal localization. These models cannot accurately answer the 'When?' questions. We identify three key aspects that limit their temporal localization capabilities",2024,2024-03-27T22:50:48Z,"Keyphrase: ""Limited temporal localization""","""However, an important missing piece is temporal localization. These models cannot accurately answer the 'When?' questions. We identify three key aspects that limit their temporal localization capabilities Keyphrase: ""Limited temporal localization"""
arxiv2024,"""Sorry, Come Again?"" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing",Yes.,5,"""Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs)."" and ""Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans."" and ""Recent studies reveal that an LLM often neglects the middle sections of extended prompts, a phenomenon termed as lost in the middle.""",2024,2024-03-27T19:45:09Z,"Keyphrase: ""Neglect of middle section""","""Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs)."" and ""Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans."" and ""Recent studies reveal that an LLM often neglects the middle sections of extended prompts, a phenomenon termed as lost in the middle."" Keyphrase: ""Neglect of middle section"""
arxiv2024,Measuring Political Bias in Large Language Models: What Is Said and How It Is Said,Yes.,4,"""However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications.""",2024,2024-03-27T18:22:48Z,"Keyphrase: ""Political bias and polarization""","""However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications."" Keyphrase: ""Political bias and polarization"""
arxiv2024,Projective Methods for Mitigating Gender Bias in Pre-trained Language Models,Yes.,4,"""We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing",2024,2024-03-27T17:49:31Z,"Keyphrase: ""Limited correlation between intrinsic bias and downstream bias mitigation""","""We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing Keyphrase: ""Limited correlation between intrinsic bias and downstream bias mitigation"""
arxiv2024,Long-form factuality in large language models,Yes.,5,"""Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics.""",2024,2024-03-27T17:48:55Z,"Keyphrase: ""Factual errors in generated content""","""Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics."" Keyphrase: ""Factual errors in generated content"""
arxiv2024,NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method,Yes.,5,"""Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field.""",2024,2024-03-27T15:22:16Z,"Keyphrase: ""Prone to returning false information""","""Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field."" Keyphrase: ""Prone to returning false information"""
arxiv2024,Vulnerability Detection with Code Language Models: How Far Are We?,Yes.,5,"""Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models."" and ""Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings.""",2024,2024-03-27T14:34:29Z,"Keyphrase: ""Overestimated performance""","""Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models."" and ""Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings."" Keyphrase: ""Overestimated performance"""
arxiv2024,BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text,Yes.,5,"""However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources.""",2024,2024-03-27T10:18:21Z,"Keyphrase: ""High computational cost and data privacy concerns""","""However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources."" Keyphrase: ""High computational cost and data privacy concerns"""
arxiv2024,BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models,Yes.,4,"""However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc.""",2024,2024-03-27T08:57:21Z,"Keyphrase: ""Lack of domain-specific knowledge""","""However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc."" Keyphrase: ""Lack of domain-specific knowledge"""
arxiv2024,Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback,Yes.,5,"""Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope."" and ""These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby",2024,2024-03-27T08:39:56Z,"Keyphrase: ""Limited ability to discern and reject questions beyond knowledge scope""","""Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope."" and ""These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby Keyphrase: ""Limited ability to discern and reject questions beyond knowledge scope"""
arxiv2024,Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective,Yes.,5,"""MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks.""",2024,2024-03-27T08:38:49Z,"Keyphrase: ""Unimodal bias""","""MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks."" Keyphrase: ""Unimodal bias"""
arxiv2024,Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications,Yes.,5,"""We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.""",2024,2024-03-27T08:08:00Z,"Keyphrase: ""Inadequate for complex tasks""","""We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems."" Keyphrase: ""Inadequate for complex tasks"""
arxiv2024,Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,Yes.,4,"""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions.""",2024,2024-03-27T06:43:58Z,"Keyphrase: ""Inaccuracy in answer prediction""","""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions."" Keyphrase: ""Inaccuracy in answer prediction"""
arxiv2024,Exploring the Privacy Protection Capabilities of Chinese Large Language Models,Yes.,5,"""Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models.""",2024,2024-03-27T02:31:54Z,"Keyphrase: ""Privacy protection shortcomings""","""Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models."" Keyphrase: ""Privacy protection shortcomings"""
arxiv2024,Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization,Yes.,5,"""However, they still make unjustified logical and computational errors in their reasoning steps and answers.""",2024,2024-03-26T22:01:13Z,"Keyphrase: ""Unjustified computational errors""","""However, they still make unjustified logical and computational errors in their reasoning steps and answers."" Keyphrase: ""Unjustified computational errors"""
arxiv2024,MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution,Yes.,4,"""Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level."" and ""To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors.""",2024,2024-03-26T17:57:57Z,"Keyphrase: ""Difficulty in code change resolution""","""Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level."" and ""To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors."" Keyphrase: ""Difficulty in code change resolution"""
arxiv2024,The Unreasonable Ineffectiveness of the Deeper Layers,Yes.,5,"""the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.""",2024,2024-03-26T17:20:04Z,"Keyphrase: ""Inefficient knowledge utilization""","""the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge."" Keyphrase: ""Inefficient knowledge utilization"""
arxiv2024,Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications,Yes.,4,"""Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models"" and ""This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications.""",2024,2024-03-26T15:20:49Z,"Keyphrase: ""Inferior accuracy and reliability""","""Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models"" and ""This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications."" Keyphrase: ""Inferior accuracy and reliability"""
arxiv2024,Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons,Yes.,5,"""show that GPT-4 and Llama 2 fail it with strong bias"" and ""we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.""",2024,2024-03-26T14:51:12Z,"Keyphrase: ""Limited representation and understanding""","""show that GPT-4 and Llama 2 fail it with strong bias"" and ""we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads."" Keyphrase: ""Limited representation and understanding"""
arxiv2024,Can multiple-choice questions really be useful in detecting the abilities of LLMs?,Yes.,5,"""There are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required."" and ""LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position."" and ""Our results reveal a relatively low correlation between answers from MCQs and L",2024,2024-03-26T14:43:48Z,"Keyphrase: ""Order sensitivity in knowledge-intensive scenarios""","""There are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required."" and ""LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position."" and ""Our results reveal a relatively low correlation between answers from MCQs and L Keyphrase: ""Order sensitivity in knowledge-intensive scenarios"""
arxiv2024,Optimization-based Prompt Injection Attack to LLM-as-a-Judge,Yes.,4,"""However, the robustness of these systems against prompt injection attacks remains an open question."" and ""highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.""",2024,2024-03-26T13:58:00Z,"Keyphrase: ""Vulnerability to prompt injection attacks""","""However, the robustness of these systems against prompt injection attacks remains an open question."" and ""highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack."" Keyphrase: ""Vulnerability to prompt injection attacks"""
arxiv2024,Targeted Visualization of the Backbone of Encoder LLMs,Yes.,4,"""they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues.""",2024,2024-03-26T12:51:02Z,"Keyphrase: ""Bias and susceptibility to adversarial attacks""","""they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues."" Keyphrase: ""Bias and susceptibility to adversarial attacks"""
arxiv2024,RuBia: A Russian Language Bias Detection Dataset,Yes.,4,"""Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data"" and ""we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases.""",2024,2024-03-26T10:01:01Z,"Keyphrase: ""Social and cultural bias in training data""","""Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data"" and ""we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases."" Keyphrase: ""Social and cultural bias in training data"""
arxiv2024,KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion,Yes.,5,"""it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission.""",2024,2024-03-26T09:36:59Z,"Keyphrase: ""Mismatch and ordering errors""","""it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission."" Keyphrase: ""Mismatch and ordering errors"""
arxiv2024,Robust and Scalable Model Editing for Large Language Models,Yes.,5,"""Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context.""",2024,2024-03-26T06:57:23Z,"Keyphrase: ""Limited contextual knowledge fallback""","""Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context."" Keyphrase: ""Limited contextual knowledge fallback"""
arxiv2024,Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models,Yes.,4,"""there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers"" and ""jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited.""",2024,2024-03-26T02:47:42Z,"Keyphrase: ""Security circumvention risks""","""there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers"" and ""jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited."" Keyphrase: ""Security circumvention risks"""
arxiv2024,"Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",Yes.,5,"""In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs).""",2024,2024-03-26T01:28:42Z,"Keyphrase: ""Hallucination detection and mitigation""","""In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs)."" Keyphrase: ""Hallucination detection and mitigation"""
arxiv2024,A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection,Yes.,5,"""LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that",2024,2024-03-25T21:47:36Z,"Keyphrase: ""Struggles in vulnerability detection""","""LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that Keyphrase: ""Struggles in vulnerability detection"""
arxiv2024,Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model,Yes.,5,"""Unexpectedly, the RBS outperformed the LLMs across all metrics. Intensive review demonstrates that this finding is due to the divergent approach taken by the RBS and LLM.""",2024,2024-03-25T21:19:50Z,"Keyphrase: ""Divergent performance from traditional LLMs""","""Unexpectedly, the RBS outperformed the LLMs across all metrics. Intensive review demonstrates that this finding is due to the divergent approach taken by the RBS and LLM."" Keyphrase: ""Divergent performance from traditional LLMs"""
arxiv2024,The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition,Yes.,5,"""However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition,"" and ""We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that",2024,2024-03-25T19:07:32Z,"Keyphrase: ""Limited integration of information""","""However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition,"" and ""We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that Keyphrase: ""Limited integration of information"""
arxiv2024,Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators,Yes.,5,"""LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments.""",2024,2024-03-25T17:11:28Z,"Keyphrase: ""Biased evaluation and lack of coherence""","""LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments."" Keyphrase: ""Biased evaluation and lack of coherence"""
arxiv2024,Do LLM Agents Have Regret? A Case Study in Online Learning and Games,Yes.,4,"""Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret.""",2024,2024-03-25T15:04:11Z,"Keyphrase: ""Failure in complex cases""","""Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret."" Keyphrase: ""Failure in complex cases"""
arxiv2024,Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback,Yes.,5,"""As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context.""",2024,2024-03-25T14:07:27Z,"Keyphrase: ""Limited project-specific context understanding""","""As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context."" Keyphrase: ""Limited project-specific context understanding"""
arxiv2024,"All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification",Yes.,4,"""It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code."" and ""Our study reveals that approximately 60% of the hardware designs generated by LLMs are prone to CWEs, posing potential safety and security risks.""",2024,2024-03-25T13:23:24Z,"Keyphrase: ""Prone to security vulnerabilities""","""It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code."" and ""Our study reveals that approximately 60% of the hardware designs generated by LLMs are prone to CWEs, posing potential safety and security risks."" Keyphrase: ""Prone to security vulnerabilities"""
arxiv2024,Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography,Yes.,5,"""students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test"" and ""ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students' knowledge application and creativity were insignificant.""",2024,2024-03-25T12:23:12Z,"Keyphrase: ""Limited knowledge application and creativity""","""students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test"" and ""ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students' knowledge application and creativity were insignificant."" Keyphrase: ""Limited knowledge application and creativity"""
arxiv2024,Elysium: Exploring Object-level Perception in Videos via MLLM,Yes.,5,"""extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships"" and ""processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden.""",2024,2024-03-25T09:17:15Z,"Keyphrase: ""High computational burden""","""extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships"" and ""processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden."" Keyphrase: ""High computational burden"""
arxiv2024,Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art,Yes.,4,"""foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor."" and ""discuss existing approaches to hallucination detection and mitigation with a focus on decision problems.""",2024,2024-03-25T08:11:02Z,"Keyphrase: ""Hallucination in decision-making""","""foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor."" and ""discuss existing approaches to hallucination detection and mitigation with a focus on decision problems."" Keyphrase: ""Hallucination in decision-making"""
arxiv2024,Evaluating Large Language Models with Runtime Behavior of Program Execution,Yes.,5,"""most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3).""",2024,2024-03-25T05:37:16Z,"Keyphrase: ""Poor runtime reasoning""","""most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3)."" Keyphrase: ""Poor runtime reasoning"""
arxiv2024,How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation,Yes.,5,"""While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation",2024,2024-03-25T04:21:06Z,"Keyphrase: ""Limitation in user simulation""","""While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation Keyphrase: ""Limitation in user simulation"""
arxiv2024,A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish,Yes.,4,"""This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance.""",2024,2024-03-24T13:21:58Z,"Keyphrase: ""Risk of data leakage""","""This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance."" Keyphrase: ""Risk of data leakage"""
arxiv2024,TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions,Yes.,5,"""little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones.""",2024,2024-03-23T16:12:52Z,"Keyphrase: ""Limited handling of diverse question types""","""little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones."" Keyphrase: ""Limited handling of diverse question types"""
arxiv2024,The Frontier of Data Erasure: Machine Unlearning for Large Language Models,Yes.,5,"""Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets.""",2024,2024-03-23T09:26:15Z,"Keyphrase: ""Risk of memorizing sensitive and biased information""","""Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets."" Keyphrase: ""Risk of memorizing sensitive and biased information"""
arxiv2024,AI for Biomedicine in the Era of Large Language Models,Yes.,4,"""we will delve into large language model challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation.""",2024,2024-03-23T01:40:22Z,"Keyphrase: ""Challenges in biomedical research""","""we will delve into large language model challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation."" Keyphrase: ""Challenges in biomedical research"""
arxiv2024,"Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges",Yes.,4,"""Finally, we discuss the challenges of consistency, sensitivity and explainability associated to the use of LLMs in CDM processes, raising open questions for future studies.""",2024,2024-03-22T19:21:44Z,"Keyphrase: ""Consistency and explainability challenges""","""Finally, we discuss the challenges of consistency, sensitivity and explainability associated to the use of LLMs in CDM processes, raising open questions for future studies."" Keyphrase: ""Consistency and explainability challenges"""
arxiv2024,Long-CLIP: Unlocking the Long-Text Capability of CLIP,Yes.,5,"""a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites.""",2024,2024-03-22T17:58:16Z,"Keyphrase: ""Limited text input length""","""a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites."" Keyphrase: ""Limited text input length"""
arxiv2024,Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs,Yes.,5,"""suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs.""",2024,2024-03-22T17:27:18Z,"Keyphrase: ""Limited real-world diagnostic readiness""","""suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs."" Keyphrase: ""Limited real-world diagnostic readiness"""
arxiv2024,CoLLEGe: Concept Embedding Generation for Large Language Models,Yes.,4,"""Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts.""",2024,2024-03-22T17:26:05Z,"Keyphrase: ""Slow adaptation to new concepts""","""Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts."" Keyphrase: ""Slow adaptation to new concepts"""
arxiv2024,Sphere Neural-Networks for Rational Reasoning,Yes.,5,"""This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination.""",2024,2024-03-22T15:44:59Z,"Keyphrase: ""Limitation in rational reasoning""","""This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination."" Keyphrase: ""Limitation in rational reasoning"""
arxiv2024,An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets,Yes.,4,"""our study...highlights the pervasive issue of license inconsistencies in large language models trained on code.""",2024,2024-03-22T14:23:21Z,"Keyphrase: ""Inconsistent licensing""","""our study...highlights the pervasive issue of license inconsistencies in large language models trained on code."" Keyphrase: ""Inconsistent licensing"""
arxiv2024,Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study,Yes.,5,"""knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages,"" and ""CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions.""",2024,2024-03-22T13:13:13Z,"Keyphrase: ""Poor transfer to functional programming languages""","""knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages,"" and ""CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions."" Keyphrase: ""Poor transfer to functional programming languages"""
arxiv2024,Risk and Response in Large Language Models: Evaluating Key Threat Categories,Yes.,5,"""Our findings indicate that LLMs tend to consider Information Hazards less harmful,"" and ""The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in LLM risk assessment and emphasizing the need for improved AI safety measures.""",2024,2024-03-22T06:46:40Z,"Keyphrase: ""Information hazards and security vulnerabilities""","""Our findings indicate that LLMs tend to consider Information Hazards less harmful,"" and ""The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in LLM risk assessment and emphasizing the need for improved AI safety measures."" Keyphrase: ""Information hazards and security vulnerabilities"""
arxiv2024,On Zero-Shot Counterspeech Generation by LLMs,Yes.,4,"""Our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size."" and ""GPT-2 and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT",2024,2024-03-22T04:13:10Z,"Keyphrase: ""Trade-off between generation quality and toxicity""","""Our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size."" and ""GPT-2 and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT Keyphrase: ""Trade-off between generation quality and toxicity"""
arxiv2024,The opportunities and risks of large language models in mental health,Yes.,4,"""We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks.""",2024,2024-03-21T19:59:52Z,"Keyphrase: ""Risk in mental health applications""","""We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks."" Keyphrase: ""Risk in mental health applications"""
arxiv2024,Can 3D Vision-Language Models Truly Understand Natural Language?,Yes.,5,"""existing 3D-VL models exhibit sensitivity to the styles of language input, struggling to understand sentences with the same semantic meaning but written in different variants"" and ""Even the state-of-the-art 3D-LLM fails to understand some variants of the same sentences"" and ""our comprehensive evaluation uncovers a significant drop in the performance of all existing models across various 3",2024,2024-03-21T18:02:20Z,"Keyphrase: ""Limited semantic understanding""","""existing 3D-VL models exhibit sensitivity to the styles of language input, struggling to understand sentences with the same semantic meaning but written in different variants"" and ""Even the state-of-the-art 3D-LLM fails to understand some variants of the same sentences"" and ""our comprehensive evaluation uncovers a significant drop in the performance of all existing models across various 3 Keyphrase: ""Limited semantic understanding"""
arxiv2024,Language Repository for Long Video Understanding,Yes.,5,"""Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length.""",2024,2024-03-21T17:59:35Z,"Keyphrase: ""Declining effectiveness with long context lengths""","""Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length."" Keyphrase: ""Declining effectiveness with long context lengths"""
arxiv2024,RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain,Yes.,4,"""yet their reliability in realistic use cases is under-researched"" and ""We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case.""",2024,2024-03-21T17:30:59Z,"Keyphrase: ""Reliability in realistic use cases""","""yet their reliability in realistic use cases is under-researched"" and ""We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case."" Keyphrase: ""Reliability in realistic use cases"""
arxiv2024,Open Source Conversational LLMs do not know most Spanish words,Yes.,4,"""The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.""",2024,2024-03-21T15:41:02Z,"Keyphrase: ""Language bias and performance discrepancy""","""The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages."" Keyphrase: ""Language bias and performance discrepancy"""
arxiv2024,Detoxifying Large Language Models via Knowledge Editing,Yes.,4,"""This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs)."" and ""We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments.""",2024,2024-03-21T15:18:30Z,"Keyphrase: ""Limited effectiveness in detoxifying toxic parameters""","""This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs)."" and ""We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments."" Keyphrase: ""Limited effectiveness in detoxifying toxic parameters"""
arxiv2024,Locating and Mitigating Gender Bias in Large Language Models,Yes.,4,"""this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society"" and ""we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns.""",2024,2024-03-21T13:57:43Z,"Keyphrase: ""Acquiring societal bias""","""this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society"" and ""we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns."" Keyphrase: ""Acquiring societal bias"""
arxiv2024,Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination,Yes.,5,"""However, they suffer from visual hallucination, where the generated responses diverge from the provided image.""",2024,2024-03-21T13:49:42Z,"Keyphrase: ""Visual hallucination""","""However, they suffer from visual hallucination, where the generated responses diverge from the provided image."" Keyphrase: ""Visual hallucination"""
arxiv2024,FIT-RAG: Black-Box RAG with Factual Information and Token Reduction,Yes.,5,"""Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications."" and ""Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues",2024,2024-03-21T13:05:18Z,"Keyphrase: ""Out-of-date knowledge and blackbox limitations""","""Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications."" and ""Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues Keyphrase: ""Out-of-date knowledge and blackbox limitations"""
arxiv2024,"WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",Yes.,5,"""The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are 'unknown' to them.""",2024,2024-03-21T12:45:12Z,"Keyphrase: ""Decay of factuality over time""","""The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are 'unknown' to them."" Keyphrase: ""Decay of factuality over time"""
arxiv2024,Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection,Yes.,4,"""Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models.""",2024,2024-03-21T08:57:27Z,"Keyphrase: ""Superficial alignment with human preference""","""Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models."" Keyphrase: ""Superficial alignment with human preference"""
arxiv2024,Improving the Robustness of Large Language Models via Consistency Alignment,Yes.,5,"""their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions.""",2024,2024-03-21T08:21:12Z,"Keyphrase: ""Inconsistent responses""","""their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions."" Keyphrase: ""Inconsistent responses"""
arxiv2024,AI and Memory Wall,Yes.,5,"""the main performance bottleneck is increasingly shifting to memory bandwidth"" and ""we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models.""",2024,2024-03-21T04:31:59Z,"""Memory bandwidth bottleneck""","""the main performance bottleneck is increasingly shifting to memory bandwidth"" and ""we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models."" ""Memory bandwidth bottleneck"""
arxiv2024,Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,Yes.,4,"""We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability,"" and ""We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.""",2024,2024-03-21T03:52:01Z,"Keyphrase: ""Struggles with reasoning and memorization""","""We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability,"" and ""We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors."" Keyphrase: ""Struggles with reasoning and memorization"""
arxiv2024,"Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection",Yes.,4,"""Independent analyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside AI self-reflection, validate these concerns, highlighting behaviours analogous to deceit, manipulation, and safety neglect.""",2024,2024-03-21T02:12:03Z,"Keyphrase: ""Deceitful behavior and safety neglect""","""Independent analyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside AI self-reflection, validate these concerns, highlighting behaviours analogous to deceit, manipulation, and safety neglect."" Keyphrase: ""Deceitful behavior and safety neglect"""
arxiv2024,Protected group bias and stereotypes in Large Language Models,Yes.,5,"""We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations."" and ""The model not only reflects societal biases, but appears to amplify them."" and ""This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should",2024,2024-03-21T00:21:38Z,"Keyphrase: ""Amplification of societal bias""","""We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations."" and ""The model not only reflects societal biases, but appears to amplify them."" and ""This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should Keyphrase: ""Amplification of societal bias"""
arxiv2024,Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs,Yes.,4,"""our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time.""",2024,2024-03-20T21:02:16Z,"Keyphrase: ""Compromising creativity and individuality""","""our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time."" Keyphrase: ""Compromising creativity and individuality"""
arxiv2024,Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification,Yes.,4,"""Despite the growing capabilities of large language models, there exists concerns about the biases they develop."" and ""bias occurs due to both intrinsic model architecture and dataset.""",2024,2024-03-20T18:59:18Z,"Keyphrase: ""Bias development concerns""","""Despite the growing capabilities of large language models, there exists concerns about the biases they develop."" and ""bias occurs due to both intrinsic model architecture and dataset."" Keyphrase: ""Bias development concerns"""
arxiv2024,Reverse Training to Nurse the Reversal Curse,Yes.,5,"""Large language models (LLMs) have a surprising failure",2024,2024-03-20T17:55:35Z,"Keyphrase: ""Surprising failures""","""Large language models (LLMs) have a surprising failure Keyphrase: ""Surprising failures"""
arxiv2024,Defending Against Indirect Prompt Injection Attacks With Spotlighting,Yes.,5,"""Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources.""",2024,2024-03-20T15:26:23Z,"Keyphrase: ""Difficulty in distinguishing input sources""","""Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources."" Keyphrase: ""Difficulty in distinguishing input sources"""
arxiv2024,CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing,Yes.,4,"""Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents.""",2024,2024-03-20T13:33:55Z,"Keyphrase: ""Challenges in generating complex code""","""Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents."" Keyphrase: ""Challenges in generating complex code"""
arxiv2024,PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns,Yes.,5,"""we find that they are not able to generalize well to simple abstract patterns. Notably, even GPT-4V cannot solve more than half of the puzzles"" and ""we hope to shed light on the limitations of large multimodal models and how they can better emulate human cognitive processes in the future.""",2024,2024-03-20T05:37:24Z,"Keyphrase: ""Limited generalization ability""","""we find that they are not able to generalize well to simple abstract patterns. Notably, even GPT-4V cannot solve more than half of the puzzles"" and ""we hope to shed light on the limitations of large multimodal models and how they can better emulate human cognitive processes in the future."" Keyphrase: ""Limited generalization ability"""
arxiv2024,LeanReasoner: Boosting Complex Logical Reasoning with Lean,Yes.,4,"""Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning.""",2024,2024-03-20T05:29:06Z,"Keyphrase: ""Struggles with logical reasoning""","""Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning."" Keyphrase: ""Struggles with logical reasoning"""
arxiv2024,Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal,Yes.,4,"""However, this technological advancement is accompanied by significant risks and vulnerabilities. Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of LLMs.""",2024,2024-03-20T05:17:22Z,"Keyphrase: ""Persistent security vulnerabilities""","""However, this technological advancement is accompanied by significant risks and vulnerabilities. Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of LLMs."" Keyphrase: ""Persistent security vulnerabilities"""
arxiv2024,From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards,Yes.,5,"""However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations."" and ""multiple concerns regarding the safety and ingrained biases in these models remain."" and ""a clear trade-off between the helpfulness and safety of these models has been documented in the literature.""",2024,2024-03-20T00:22:38Z,"Keyphrase: ""Safety risks and ingrained biases""","""However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations."" and ""multiple concerns regarding the safety and ingrained biases in these models remain."" and ""a clear trade-off between the helpfulness and safety of these models has been documented in the literature."" Keyphrase: ""Safety risks and ingrained biases"""
arxiv2024,VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning,Yes.,4,"""The broader capabilities and limitations of multimodal ICL remain under-explored,"" and ""revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging.""",2024,2024-03-19T21:31:56Z,"Keyphrase: ""Underexplored multimodal capabilities""","""The broader capabilities and limitations of multimodal ICL remain under-explored,"" and ""revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging."" Keyphrase: ""Underexplored multimodal capabilities"""
arxiv2024,Dated Data: Tracing Knowledge Cutoffs in Large Language Models,Yes.,5,"""To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies",2024,2024-03-19T17:57:58Z,"Keyphrase: ""Inconsistency in pretraining datasets""","""To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies Keyphrase: ""Inconsistency in pretraining datasets"""
arxiv2024,MELTing point: Mobile Evaluation of Language Transformers,Yes.,5,"""Their runtime requirements have prevented them from being broadly deployed on mobile,"" ""LLM inference is largely memory-bound,"" ""Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost,"" ""the continuous execution of LLMs remains elusive, as both factors negatively affect user experience,"" and ""the ecosystem is still in its infancy, and algorithmic",2024,2024-03-19T15:51:21Z,"Keyphrase: ""Memory-bound execution""","""Their runtime requirements have prevented them from being broadly deployed on mobile,"" ""LLM inference is largely memory-bound,"" ""Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost,"" ""the continuous execution of LLMs remains elusive, as both factors negatively affect user experience,"" and ""the ecosystem is still in its infancy, and algorithmic Keyphrase: ""Memory-bound execution"""
arxiv2024,AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework,Yes.,5,"""LLMs still suffer from hallucinations and are unable to keep up with the latest information.""",2024,2024-03-19T09:45:33Z,"Keyphrase: ""Inability to maintain latest information""","""LLMs still suffer from hallucinations and are unable to keep up with the latest information."" Keyphrase: ""Inability to maintain latest information"""
arxiv2024,RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content,Yes.,4,"""the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges.""",2024,2024-03-19T07:25:02Z,"Keyphrase: ""Bias in generating harmful content""","""the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges."" Keyphrase: ""Bias in generating harmful content"""
arxiv2024,"Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",Yes.,4,"""These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities."" and ""mitigation strategies to address these challenges while identifying limitations of current strategies.""",2024,2024-03-19T07:10:58Z,"Keyphrase: ""Limited vulnerability mitigation""","""These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities."" and ""mitigation strategies to address these challenges while identifying limitations of current strategies."" Keyphrase: ""Limited vulnerability mitigation"""
arxiv2024,Third-Party Language Model Performance Prediction from Instruction,Yes.,5,"""such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task."" and ""Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations",2024,2024-03-19T03:53:47Z,Keyphrase: Lack of transparency and user awareness,"""such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task."" and ""Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations Keyphrase: Lack of transparency and user awareness"
arxiv2024,Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering,Yes.,4,"""LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance.""",2024,2024-03-19T03:00:03Z,"Keyphrase: ""Off-topic answers""","""LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance."" Keyphrase: ""Off-topic answers"""
arxiv2024,"OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety",Yes.,4,"""many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues"" and ""Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety.""",2024,2024-03-18T23:21:37Z,"Keyphrase: ""Overlooking alignment safety""","""many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues"" and ""Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety."" Keyphrase: ""Overlooking alignment safety"""
arxiv2024,Zero-Shot Multi-task Hallucination Detection,Yes.,4,"""This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria.""",2024,2024-03-18T20:50:26Z,"Keyphrase: ""Lack of faithfulness to the source""","""This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria."" Keyphrase: ""Lack of faithfulness to the source"""
arxiv2024,EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models,Yes.,5,"""Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs)."" and ""Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks.""",2024,2024-03-18T18:39:53Z,"Keyphrase: ""Security vulnerabilities""","""Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs)."" and ""Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks."" Keyphrase: ""Security vulnerabilities"""
arxiv2024,Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets,Yes.,4,"""Recent work has shown that false assumptions in naturally occurring questions pose challenges to current models, with low performance on both generative QA and simple detection tasks"" and ""Our findings from evaluating a range of large language models are threefold",2024,2024-03-18T18:01:26Z,"Keyphrase: ""Low performance in generative QA""","""Recent work has shown that false assumptions in naturally occurring questions pose challenges to current models, with low performance on both generative QA and simple detection tasks"" and ""Our findings from evaluating a range of large language models are threefold Keyphrase: ""Low performance in generative QA"""
arxiv2024,A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models,Yes.,4,"""Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities"" and ""Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. We emphasize that while our framework can identify specific forms of bias, it is not sufficient to holistically assess whether the deployment",2024,2024-03-18T17:56:37Z,"Keyphrase: ""Potential to exacerbate health disparities""","""Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities"" and ""Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. We emphasize that while our framework can identify specific forms of bias, it is not sufficient to holistically assess whether the deployment Keyphrase: ""Potential to exacerbate health disparities"""
arxiv2024,NovelQA: A Benchmark for Long-Range Novel Question Answering,Yes.,5,"""Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with more than 100,000 tokens.""",2024,2024-03-18T17:32:32Z,"Keyphrase: ""Challenges with multihop reasoning""","""Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with more than 100,000 tokens."" Keyphrase: ""Challenges with multihop reasoning"""
arxiv2024,Investigating Markers and Drivers of Gender Bias in Machine Translations,Yes.,4,"""Implicit gender bias in Large Language Models (LLMs) is a well-documented problem,"" and ""These results show that the back-translation method can provide further insights into bias in language models.""",2024,2024-03-18T15:54:46Z,"Keyphrase: ""Implicit gender bias""","""Implicit gender bias in Large Language Models (LLMs) is a well-documented problem,"" and ""These results show that the back-translation method can provide further insights into bias in language models."" Keyphrase: ""Implicit gender bias"""
arxiv2024,Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models,Yes.,4,"""Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues."" and ""challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training.""",2024,2024-03-18T14:48:29Z,"Keyphrase: ""Biased content generation and privacy risks""","""Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues."" and ""challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training."" Keyphrase: ""Biased content generation and privacy risks"""
arxiv2024,Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus,Yes.,5,"""Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity.""",2024,2024-03-18T13:50:50Z,"Keyphrase: ""Weak inference and logical coherence""","""Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity."" Keyphrase: ""Weak inference and logical coherence"""
arxiv2024,Do CLIPs Always Generalize Better than ImageNet Models?,Yes.,4,"""We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different",2024,2024-03-18T06:04:02Z,"Keyphrase: ""Performance drop on out-of-distribution data""","""We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different Keyphrase: ""Performance drop on out-of-distribution data"""
arxiv2024,Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression,Yes.,5,"""the potential risks of compression in terms of safety and trustworthiness have been largely neglected"" and ""our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns.""",2024,2024-03-18T01:38:19Z,"Keyphrase: ""Neglected safety and trustworthiness""","""the potential risks of compression in terms of safety and trustworthiness have been largely neglected"" and ""our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns."" Keyphrase: ""Neglected safety and trustworthiness"""
arxiv2024,What Makes Math Word Problems Challenging for LLMs?,Yes.,5,"""This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs).""",2024,2024-03-17T23:18:40Z,"Keyphrase: ""Challenges with math word problems""","""This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs)."" Keyphrase: ""Challenges with math word problems"""
arxiv2024,Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts,Yes.,5,"""a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning"" and ""we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model.""",2024,2024-03-17T19:32:12Z,"Keyphrase: ""Spurious pattern learning""","""a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning"" and ""we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model."" Keyphrase: ""Spurious pattern learning"""
arxiv2024,Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs,Yes.,5,"""Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps.""",2024,2024-03-17T17:01:45Z,"Keyphrase: ""Hallucination and lack of logical reasoning""","""Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps."" Keyphrase: ""Hallucination and lack of logical reasoning"""
arxiv2024,Correcting misinformation on social media with a large language model,Yes.,5,"""LLMs also have versatile capabilities that could accelerate misinformation correction--however, they struggle due to a lack of recent information, a tendency to produce false content, and limitations in addressing multimodal information.""",2024,2024-03-17T10:59:09Z,"Keyphrase: ""Tendency to produce false content""","""LLMs also have versatile capabilities that could accelerate misinformation correction--however, they struggle due to a lack of recent information, a tendency to produce false content, and limitations in addressing multimodal information."" Keyphrase: ""Tendency to produce false content"""
arxiv2024,PhD: A Prompted Visual Hallucination Evaluation Dataset,Yes.,4,"""The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs."" and ""Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IV",2024,2024-03-17T06:53:44Z,"Keyphrase: ""Inability to tackle hallucination""","""The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs."" and ""Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IV Keyphrase: ""Inability to tackle hallucination"""
arxiv2024,ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models,Yes.,4,"""Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER).""",2024,2024-03-17T06:12:43Z,"Keyphrase: ""Struggles with structured knowledge extraction""","""Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER)."" Keyphrase: ""Struggles with structured knowledge extraction"""
arxiv2024,GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment,Yes.,5,"""Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context.""",2024,2024-03-17T03:52:52Z,"Keyphrase: ""Lack of contextual grounding""","""Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context."" Keyphrase: ""Lack of contextual grounding"""
arxiv2024,Pre-Trained Language Models Represent Some Geographic Populations Better Than Others,Yes.,4,"""Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented.""",2024,2024-03-16T22:01:39Z,"Keyphrase: ""Population representation bias""","""Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented."" Keyphrase: ""Population representation bias"""
arxiv2024,A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment,Yes.,5,"""Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.""",2024,2024-03-16T08:30:45Z,"Keyphrase: ""Weak fine-grained quality discrimination""","""Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly."" Keyphrase: ""Weak fine-grained quality discrimination"""
arxiv2024,Do Large Language Models understand Medical Codes?,Yes.,5,"""However, these models are also prone to producing 'hallucinations' or incorrect responses when faced with queries they cannot adequately address,"" and ""Our results indicate that these models as they currently stand do not comprehend the meaning of the medical codes, highlighting the need for better representation of these alphanumeric codes extensively used in healthcare.""",2024,2024-03-16T06:18:15Z,"Keyphrase: ""Difficulty in comprehending medical codes""","""However, these models are also prone to producing 'hallucinations' or incorrect responses when faced with queries they cannot adequately address,"" and ""Our results indicate that these models as they currently stand do not comprehend the meaning of the medical codes, highlighting the need for better representation of these alphanumeric codes extensively used in healthcare."" Keyphrase: ""Difficulty in comprehending medical codes"""
arxiv2024,Detecting Bias in Large Language Models: Fine-tuned KcBERT,Yes.,4,"""they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language.""",2024,2024-03-16T02:27:19Z,"Keyphrase: ""Propagation of discriminatory language""","""they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language."" Keyphrase: ""Propagation of discriminatory language"""
arxiv2024,Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases,Yes.,5,"""Addressing the challenge of LLM hallucinations,"" and ""The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets.""",2024,2024-03-15T16:30:14Z,"Keyphrase: ""Limited generalization on small-scale skewed datasets""","""Addressing the challenge of LLM hallucinations,"" and ""The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets."" Keyphrase: ""Limited generalization on small-scale skewed datasets"""
arxiv2024,Uni-SMART: Universal Science Multimodal Analysis and Research Transformer,Yes.,4,"""existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze.""",2024,2024-03-15T13:43:47Z,"Keyphrase: ""Struggles with multimodal elements""","""existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze."" Keyphrase: ""Struggles with multimodal elements"""
arxiv2024,A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption,Yes.,5,"""The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity."" and ""We highlight that, in a typical case study where word-level univariate explanations are analyzed with",2024,2024-03-15T13:15:23Z,"Keyphrase: ""Sensitivity to randomness""","""The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity."" and ""We highlight that, in a typical case study where word-level univariate explanations are analyzed with Keyphrase: ""Sensitivity to randomness"""
arxiv2024,Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models,Yes.,4,"""they are mostly English-centric due to the imbalanced training corpora"" and ""even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios.""",2024,2024-03-15T12:47:39Z,"Keyphrase: ""English-centric bias""","""they are mostly English-centric due to the imbalanced training corpora"" and ""even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios."" Keyphrase: ""English-centric bias"""
arxiv2024,HawkEye: Training Video-Text LLMs for Grounding Text in Videos,Yes.,5,"""they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images.""",2024,2024-03-15T11:58:18Z,"Keyphrase: ""Limited understanding of temporal information""","""they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images."" Keyphrase: ""Limited understanding of temporal information"""
arxiv2024,Are LLMs Good Cryptic Crossword Solvers?,Yes.,5,"""showing that their performance on this task is still far from that of humans.""",2024,2024-03-15T06:57:08Z,"Keyphrase: ""Performance gap with humans""","""showing that their performance on this task is still far from that of humans."" Keyphrase: ""Performance gap with humans"""
arxiv2024,Lost in Overlap: Exploring Watermark Collision in LLMs,Yes.,5,"""However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing.""",2024,2024-03-15T05:06:21Z,"Keyphrase: ""Watermark collision issues""","""However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing."" Keyphrase: ""Watermark collision issues"""
arxiv2024,Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals,Yes.,5,"""One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes."" and ""The risk of LLMs operating within an echo chamber, where AI-generated content feeds into the learning algorithms, threatens the",2024,2024-03-15T04:04:45Z,"Keyphrase: ""Feedback loop reliance""","""One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes."" and ""The risk of LLMs operating within an echo chamber, where AI-generated content feeds into the learning algorithms, threatens the Keyphrase: ""Feedback loop reliance"""
arxiv2024,Whose Side Are You On? Investigating the Political Stance of Large Language Models,Yes.,4,"""it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias.""",2024,2024-03-15T04:02:24Z,"Keyphrase: ""Political impartiality and confirmation bias""","""it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias."" Keyphrase: ""Political impartiality and confirmation bias"""
arxiv2024,Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks,Yes.,5,"""Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways."" and ""we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon.""",2024,2024-03-14T19:39:10Z,"Keyphrase: ""Vulnerability to subversion""","""Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways."" and ""we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon."" Keyphrase: ""Vulnerability to subversion"""
arxiv2024,Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention,Yes.,5,"""We find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models.""",2024,2024-03-14T18:27:43Z,"Keyphrase: ""Inconsistent harmful behavior generation""","""We find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models."" Keyphrase: ""Inconsistent harmful behavior generation"""
arxiv2024,Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models,Yes.,5,"""we conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs.""",2024,2024-03-14T18:24:55Z,"Keyphrase: ""Vulnerability to image input pose alignment""","""we conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs."" Keyphrase: ""Vulnerability to image input pose alignment"""
arxiv2024,Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey,Yes.,4,"""addressing fairness and safety issues in LLMs"" and ""understanding and improving the LLMs' reasoning capacity.""",2024,2024-03-14T17:47:20Z,"Keyphrase: ""Limited fairness and safety considerations""","""addressing fairness and safety issues in LLMs"" and ""understanding and improving the LLMs' reasoning capacity."" Keyphrase: ""Limited fairness and safety considerations"""
arxiv2024,"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",Yes.,4,"""MLLMs... are also more vulnerable to jailbreak attacks than their LLM predecessors,"" and ""safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features.""",2024,2024-03-14T17:03:04Z,"Keyphrase: ""Vulnerability to jailbreak attacks""","""MLLMs... are also more vulnerable to jailbreak attacks than their LLM predecessors,"" and ""safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features."" Keyphrase: ""Vulnerability to jailbreak attacks"""
arxiv2024,Logits of API-Protected LLMs Leak Proprietary Information,Yes.,5,"""most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space.""",2024,2024-03-14T16:27:49Z,"Keyphrase: ""Softmax bottleneck restricting output space""","""most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space."" Keyphrase: ""Softmax bottleneck restricting output space"""
arxiv2024,AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,Yes.,4,"""with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks""",2024,2024-03-14T15:57:13Z,"Keyphrase: ""Vulnerability to structured-based jailbreak attacks""","""with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks"" Keyphrase: ""Vulnerability to structured-based jailbreak attacks"""
arxiv2024,AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions,Yes.,4,"""These instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks."" and ""Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V.""",2024,2024-03-14T12:51:07Z,"Keyphrase: ""Vulnerability to attacks and bias""","""These instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks."" and ""Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V."" Keyphrase: ""Vulnerability to attacks and bias"""
arxiv2024,Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring,Yes.,4,"""the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios"" and ""Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \etc.""",2024,2024-03-14T12:21:37Z,"Keyphrase: ""Limited image resolution""","""the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios"" and ""Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \etc."" Keyphrase: ""Limited image resolution"""
arxiv2024,Caveat Lector: Large Language Models in Legal Practice,Yes.,5,"""Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire",2024,2024-03-14T08:19:41Z,"Keyphrase: ""Limited comprehension and understanding""","""Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire Keyphrase: ""Limited comprehension and understanding"""
arxiv2024,Evaluating LLMs for Gender Disparities in Notable Persons,Yes.,4,"""addressing concerns over their propensity to produce factually incorrect 'hallucinated' responses or to altogether decline to even answer prompt at all"" and ""investigates the presence of gender-based biases in LLMs' responses to factual inquiries.""",2024,2024-03-14T07:58:27Z,"Keyphrase: ""Gender-based bias and factual inaccuracies""","""addressing concerns over their propensity to produce factually incorrect 'hallucinated' responses or to altogether decline to even answer prompt at all"" and ""investigates the presence of gender-based biases in LLMs' responses to factual inquiries."" Keyphrase: ""Gender-based bias and factual inaccuracies"""
arxiv2024,Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance,Yes.,5,"""Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities.""",2024,2024-03-14T04:06:13Z,"Keyphrase: ""Deficiency in abstract reasoning""","""Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities."" Keyphrase: ""Deficiency in abstract reasoning"""
arxiv2024,Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors,Yes.,5,"""LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints.""",2024,2024-03-14T00:35:39Z,"Keyphrase: ""Struggles with low-quality evidence retrieval""","""LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints."" Keyphrase: ""Struggles with low-quality evidence retrieval"""
arxiv2024,AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents,Yes.,5,"""The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge.""",2024,2024-03-13T22:06:03Z,"Keyphrase: ""Limited real-world understanding""","""The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge."" Keyphrase: ""Limited real-world understanding"""
arxiv2024,The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions,Yes.,5,"""However, these models are susceptible to errors - 'hallucinations' and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.""",2024,2024-03-13T21:39:39Z,"Keyphrase: ""Error hallucination and omission""","""However, these models are susceptible to errors - 'hallucinations' and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks."" Keyphrase: ""Error hallucination and omission"""
arxiv2024,Bugs in Large Language Models Generated Code: An Empirical Study,Yes.,5,"""Similar to human-written code, LLM-generated code is prone to bugs,"" and ""examines a sample of 333 bugs collected from code generated using three leading LLMs"" and ""identifies the following 10 distinctive bug patterns.""",2024,2024-03-13T20:12:01Z,"Keyphrase: ""Prone to bugs""","""Similar to human-written code, LLM-generated code is prone to bugs,"" and ""examines a sample of 333 bugs collected from code generated using three leading LLMs"" and ""identifies the following 10 distinctive bug patterns."" Keyphrase: ""Prone to bugs"""
arxiv2024,Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework,Yes.,4,"""Large language models (LLMs) can easily generate biased and discriminative responses."" and ""it is of crucial importance to develop strategies to mitigate these biases.""",2024,2024-03-13T17:46:28Z,"Keyphrase: ""Biased responses""","""Large language models (LLMs) can easily generate biased and discriminative responses."" and ""it is of crucial importance to develop strategies to mitigate these biases."" Keyphrase: ""Biased responses"""
arxiv2024,Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,Yes.,5,"""they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information.""",2024,2024-03-13T17:29:45Z,"Keyphrase: ""Bias towards pretraining corpus""","""they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information."" Keyphrase: ""Bias towards pretraining corpus"""
arxiv2024,DevBench: A Comprehensive Benchmark for Software Development,Yes.,5,"""Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts.""",2024,2024-03-13T15:13:44Z,"Keyphrase: ""Struggles with complex programming challenges""","""Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts."" Keyphrase: ""Struggles with complex programming challenges"""
arxiv2024,Non-discrimination Criteria for Generative Language Models,Yes.,4,"""concerns arise about perpetuating and amplifying harmful biases in applications"" and ""this paper studies how to uncover and quantify the presence of gender biases in generative language models.""",2024,2024-03-13T14:19:08Z,"Keyphrase: ""Perpetuating harmful bias""","""concerns arise about perpetuating and amplifying harmful biases in applications"" and ""this paper studies how to uncover and quantify the presence of gender biases in generative language models."" Keyphrase: ""Perpetuating harmful bias"""
arxiv2024,SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks,Yes.,4,"""We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies.""",2024,2024-03-13T12:46:51Z,"Keyphrase: ""Vulnerability to membership inference attacks""","""We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies."" Keyphrase: ""Vulnerability to membership inference attacks"""
arxiv2024,Tastle: Distract Large Language Models for Automatic Jailbreak Attack,Yes.,5,"""even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors"" and ""highlight the crucial need to develop more effective and practical defense strategies.""",2024,2024-03-13T11:16:43Z,"Keyphrase: ""Vulnerability to malicious manipulation""","""even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors"" and ""highlight the crucial need to develop more effective and practical defense strategies."" Keyphrase: ""Vulnerability to malicious manipulation"""
arxiv2024,Do Large Language Models Solve ARC Visual Analogies Like People Do?,Yes.,5,"""Results show that both children and adults outperform most LLMs on these tasks."" and ""Error analysis revealed a similar 'fallback' solution strategy in LLMs and young children, where part of the analogy is simply copied."" and ""On the whole, 'concept' errors were more common in humans, and 'matrix' errors were more common in LLMs.""",2024,2024-03-13T09:48:13Z,"Keyphrase: ""Limited ability to understand context and generate original solutions""","""Results show that both children and adults outperform most LLMs on these tasks."" and ""Error analysis revealed a similar 'fallback' solution strategy in LLMs and young children, where part of the analogy is simply copied."" and ""On the whole, 'concept' errors were more common in humans, and 'matrix' errors were more common in LLMs."" Keyphrase: ""Limited ability to understand context and generate original solutions"""
arxiv2024,CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model,Yes.,5,"""Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated."" and ""Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and",2024,2024-03-13T08:54:31Z,"Keyphrase: ""Catastrophic forgetting""","""Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated."" and ""Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and Keyphrase: ""Catastrophic forgetting"""
arxiv2024,Knowledge Conflicts for LLMs: A Survey,Yes.,5,"""highlighting the complex challenges they encounter when blending contextual and parametric knowledge"" and ""These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common.""",2024,2024-03-13T08:02:23Z,"Keyphrase: ""Trustworthiness and performance impacted by conflicting parametric knowledge""","""highlighting the complex challenges they encounter when blending contextual and parametric knowledge"" and ""These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common."" Keyphrase: ""Trustworthiness and performance impacted by conflicting parametric knowledge"""
arxiv2024,A Moral Imperative: The Need for Continual Superalignment of Large Language Models,Yes.,5,"""achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios"" and ""highlighting the discrepancies between static AI models and the dynamic nature of human societies"" and ""LLMs, constrained by their training data, fail to align with contemporary human values and scenarios.""",2024,2024-03-13T05:44:50Z,"Keyphrase: ""Limited adaptability to evolving human ethics""","""achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios"" and ""highlighting the discrepancies between static AI models and the dynamic nature of human societies"" and ""LLMs, constrained by their training data, fail to align with contemporary human values and scenarios."" Keyphrase: ""Limited adaptability to evolving human ethics"""
arxiv2024,Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models,Yes.,5,"""LLMs have known biases, commonly derived from their training data."" and ""we observe negative correlations between the metropolitan size and the performance of the LLMS, indicating that smaller regions are indeed underrepresented.""",2024,2024-03-12T19:40:18Z,"Keyphrase: ""Underrepresentation of smaller regions""","""LLMs have known biases, commonly derived from their training data."" and ""we observe negative correlations between the metropolitan size and the performance of the LLMS, indicating that smaller regions are indeed underrepresented."" Keyphrase: ""Underrepresentation of smaller regions"""
arxiv2024,Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging,Yes.,5,"""Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data.""",2024,2024-03-12T18:12:02Z,"Keyphrase: ""Limited multimodal capability and accessibility""","""Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data."" Keyphrase: ""Limited multimodal capability and accessibility"""
arxiv2024,Exploring Safety Generalization Challenges of Large Language Models via Code,Yes.,5,"""Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input",2024,2024-03-12T17:55:38Z,"Keyphrase: ""Safety vulnerabilities in LLMs""","""Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input Keyphrase: ""Safety vulnerabilities in LLMs"""
arxiv2024,The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing,Yes.,4,"""the ripple effect in the hidden space is a significant issue in all current model editing methods.""",2024,2024-03-12T17:04:28Z,"Keyphrase: ""Hidden space ripple effect""","""the ripple effect in the hidden space is a significant issue in all current model editing methods."" Keyphrase: ""Hidden space ripple effect"""
arxiv2024,Beyond Memorization: The Challenge of Random Memory Access in Language Models,Yes.,5,"""we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content.""",2024,2024-03-12T16:42:44Z,"Keyphrase: ""Sequential memory access challenge""","""we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content."" Keyphrase: ""Sequential memory access challenge"""
arxiv2024,Fine-tuning Large Language Models with Sequential Instructions,Yes.,5,"""Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks.""",2024,2024-03-12T16:33:30Z,"Keyphrase: ""Difficulty in following complex instructions""","""Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks."" Keyphrase: ""Difficulty in following complex instructions"""
arxiv2024,Characterization of Large Language Model Development in the Datacenter,Yes.,4,"""However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization.""",2024,2024-03-12T13:31:14Z,"Keyphrase: ""Challenges in cluster resource utilization""","""However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization."" Keyphrase: ""Challenges in cluster resource utilization"""
arxiv2024,SIFiD: Reassess Summary Factual Inconsistency Detection with LLM,Yes.,5,"""However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology.""",2024,2024-03-12T11:41:51Z,"Keyphrase: ""Limited ability to follow instructions""","""However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology."" Keyphrase: ""Limited ability to follow instructions"""
arxiv2024,Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts,Yes.,5,"""Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge augmentation tools, thereby producing hallucinations.""",2024,2024-03-12T11:40:44Z,"Keyphrase: ""Hallucination in text generation""","""Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge augmentation tools, thereby producing hallucinations."" Keyphrase: ""Hallucination in text generation"""
arxiv2024,MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki,Yes.,5,"""NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled.""",2024,2024-03-12T11:32:30Z,"Keyphrase: ""Limitation in handling large amounts of information""","""NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled."" Keyphrase: ""Limitation in handling large amounts of information"""
arxiv2024,SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression,Yes.,5,"""state-of-the-art SVD-based LLM compression methods have two key limitations",2024,2024-03-12T07:31:18Z,"Keyphrase: ""Limitations in compression""","""state-of-the-art SVD-based LLM compression methods have two key limitations Keyphrase: ""Limitations in compression"""
arxiv2024,SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation,Yes.,5,"""LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information.""",2024,2024-03-11T18:26:02Z,"Keyphrase: ""High computational and memory costs with privacy concerns""","""LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information."" Keyphrase: ""High computational and memory costs with privacy concerns"""
arxiv2024,Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena,Yes.,5,"""We find that all models struggle with understanding the motion component that the CMC adds to a sentence.""",2024,2024-03-11T17:47:47Z,"Keyphrase: ""Struggles with understanding motion""","""We find that all models struggle with understanding the motion component that the CMC adds to a sentence."" Keyphrase: ""Struggles with understanding motion"""
arxiv2024,Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?,Yes.,5,"""However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection.""",2024,2024-03-11T15:48:56Z,"Keyphrase: ""Lack of safety features""","""However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection."" Keyphrase: ""Lack of safety features"""
arxiv2024,ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation,Yes.,4,"""However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation.""",2024,2024-03-11T14:10:57Z,"Keyphrase: ""Off-target translation issues""","""However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation."" Keyphrase: ""Off-target translation issues"""
arxiv2024,Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code,Yes.,4,"""since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples.""",2024,2024-03-11T12:47:04Z,"Keyphrase: ""Vulnerability to data poisoning""","""since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples."" Keyphrase: ""Vulnerability to data poisoning"""
arxiv2024,Elephants Never Forget: Testing Language Models for Memorization of Tabular Data,Yes.,5,"""the critical issues of data contamination and memorization are often glossed over,"" ""Our investigation reveals that LLMs are pre-trained on many popular tabular datasets,"" ""This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to",2024,2024-03-11T12:07:13Z,"Keyphrase: ""Data contamination and memorization""","""the critical issues of data contamination and memorization are often glossed over,"" ""Our investigation reveals that LLMs are pre-trained on many popular tabular datasets,"" ""This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to Keyphrase: ""Data contamination and memorization"""
arxiv2024,MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding,Yes.,4,"""This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses.""",2024,2024-03-11T10:57:45Z,"Keyphrase: ""Inaccurate medical information""","""This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses."" Keyphrase: ""Inaccurate medical information"""
arxiv2024,Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds,Yes.,4,"""However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians.""",2024,2024-03-11T10:53:20Z,"Keyphrase: ""Hallucination and reasoning issues""","""However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians."" Keyphrase: ""Hallucination and reasoning issues"""
arxiv2024,Academically intelligent LLMs are not necessarily socially intelligent,Yes.,5,"""The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors.""",2024,2024-03-11T10:35:53Z,"Keyphrase: ""Limited social intelligence""","""The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors."" Keyphrase: ""Limited social intelligence"""
arxiv2024,From English to ASIC: Hardware Implementation with Large Language Model,Yes.,5,"""challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities",2024,2024-03-11T09:57:16Z,"Keyphrase: ""Limited performance in generating hardware description code""","""challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities Keyphrase: ""Limited performance in generating hardware description code"""
arxiv2024,Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models,Yes.,5,"""Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs.""",2024,2024-03-11T05:51:03Z,"Keyphrase: ""Factually inaccurate hallucination""","""Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs."" Keyphrase: ""Factually inaccurate hallucination"""
arxiv2024,CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean,Yes.,4,"""Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge."" and ""Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension.""",2024,2024-03-11T03:54:33Z,"Keyphrase: ""Lack of diverse benchmark datasets""","""Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge."" and ""Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension."" Keyphrase: ""Lack of diverse benchmark datasets"""
arxiv2024,ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes,Yes.,5,"""Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust.""",2024,2024-03-10T19:47:00Z,"Keyphrase: ""Uninterpretable decision-making""","""Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust."" Keyphrase: ""Uninterpretable decision-making"""
arxiv2024,Editing Conceptual Knowledge for Large Language Models,Yes.,4,"""The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance.""",2024,2024-03-10T16:57:10Z,"Keyphrase: ""Distortion of instantial knowledge""","""The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance."" Keyphrase: ""Distortion of instantial knowledge"""
arxiv2024,"Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations",Yes.,4,"""Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model.""",2024,2024-03-09T21:07:16Z,"Keyphrase: ""Risk of nonfaithful and biased outputs""","""Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model."" Keyphrase: ""Risk of nonfaithful and biased outputs"""
arxiv2024,MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs,Yes.,5,"""we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively.""",2024,2024-03-09T06:28:48Z,"Keyphrase: ""Difficulty handling topic shifts""","""we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively."" Keyphrase: ""Difficulty handling topic shifts"""
arxiv2024,Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text,Yes.,4,"""their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices"" and ""propose novel research directions to address the current limitations in this domain.""",2024,2024-03-09T01:13:54Z,"Keyphrase: ""Ethical challenges and responsible practice""","""their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices"" and ""propose novel research directions to address the current limitations in this domain."" Keyphrase: ""Ethical challenges and responsible practice"""
arxiv2024,Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach,Yes.,5,"""While convenient, this modus operandi aggravates 'hallucination' concerns, particularly given the enigmatic 'black-box' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences.""",2024,2024-03-08T19:18:53Z,"Keyphrase: ""Blackbox nature and high-stakes consequences""","""While convenient, this modus operandi aggravates 'hallucination' concerns, particularly given the enigmatic 'black-box' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences."" Keyphrase: ""Blackbox nature and high-stakes consequences"""
arxiv2024,Can Large Language Models Play Games? A Case Study of A Self-Play Approach,Yes.,4,"""their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on.""",2024,2024-03-08T19:16:29Z,"Keyphrase: ""Reasoning limitations""","""their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on."" Keyphrase: ""Reasoning limitations"""
arxiv2024,Unfamiliar Finetuning Examples Control How Language Models Hallucinate,Yes.,5,"""Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts.""",2024,2024-03-08T18:28:13Z,"Keyphrase: ""Factually incorrect responses""","""Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts."" Keyphrase: ""Factually incorrect responses"""
arxiv2024,Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs,Yes.,4,"""LLMs exhibit impressive zero/few-shot inference and generation quality for high-resource languages (HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The",2024,2024-03-08T16:37:36Z,"Keyphrase: ""Prohibitive training costs""","""LLMs exhibit impressive zero/few-shot inference and generation quality for high-resource languages (HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The Keyphrase: ""Prohibitive training costs"""
arxiv2024,ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models,Yes.,4,"""We observe that better LLMs like GPT-4 can handle a larger variety of question types, but are by no means perfect. Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other benchmarks for various",2024,2024-03-08T12:42:36Z,"Keyphrase: ""Limited reasoning ability""","""We observe that better LLMs like GPT-4 can handle a larger variety of question types, but are by no means perfect. Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other benchmarks for various Keyphrase: ""Limited reasoning ability"""
arxiv2024,Debiasing Multimodal Large Language Models,Yes.,4,"""our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image,"" and ""our investigation sheds light on the instability of LVLMs across various decoding configurations.""",2024,2024-03-08T12:35:07Z,"Keyphrase: ""Bias in generated content""","""our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image,"" and ""our investigation sheds light on the instability of LVLMs across various decoding configurations."" Keyphrase: ""Bias in generated content"""
arxiv2024,ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models,Yes.,4,"""Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations.""",2024,2024-03-08T07:59:19Z,"Keyphrase: ""Limited domain-specific capability""","""Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations."" Keyphrase: ""Limited domain-specific capability"""
arxiv2024,Benchmarking Large Language Models for Molecule Prediction Tasks,Yes.,5,"""Notably, LLMs struggle with structured data, such as graphs, and often falter when tasked with answering domain-specific questions requiring deep expertise, such as those in biology and chemistry."" and ""Our investigation reveals several key insights",2024,2024-03-08T05:59:56Z,"Keyphrase: ""Difficulty with structured data""","""Notably, LLMs struggle with structured data, such as graphs, and often falter when tasked with answering domain-specific questions requiring deep expertise, such as those in biology and chemistry."" and ""Our investigation reveals several key insights Keyphrase: ""Difficulty with structured data"""
arxiv2024,Are Human Conversations Special? A Large Language Model Perspective,Yes.,4,"""there is a significant gap in their ability to specialize in human conversations"" and ""highlight the unique challenges posed by conversational data.""",2024,2024-03-08T04:44:25Z,"Keyphrase: ""Limited ability in human conversation specialization""","""there is a significant gap in their ability to specialize in human conversations"" and ""highlight the unique challenges posed by conversational data."" Keyphrase: ""Limited ability in human conversation specialization"""
arxiv2024,Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs,Yes.,5,"""Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents.""",2024,2024-03-08T03:49:17Z,"Keyphrase: ""Information asymmetry challenge""","""Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents."" Keyphrase: ""Information asymmetry challenge"""
arxiv2024,Tell me the truth: A system to measure the trustworthiness of Large Language Models,Yes.,5,"""one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems."" and ""ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites."" and ""ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases.""",2024,2024-03-08T00:27:57Z,"Keyphrase: ""Low accuracy and high false positive rate""","""one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems."" and ""ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites."" and ""ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases."" Keyphrase: ""Low accuracy and high false positive rate"""
arxiv2024,SecGPT: An Execution Isolation Architecture for LLM-Based Systems,Yes.,4,"""Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users.""",2024,2024-03-08T00:02:30Z,"Keyphrase: ""Trustworthiness and Security Risks""","""Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users."" Keyphrase: ""Trustworthiness and Security Risks"""
arxiv2024,Automatic and Universal Prompt Injection Attacks against Large Language Models,Yes.,5,"""These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests.""",2024,2024-03-07T23:46:20Z,"Keyphrase: ""Vulnerability to injected content""","""These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests."" Keyphrase: ""Vulnerability to injected content"""
arxiv2024,Evaluating Biases in Context-Dependent Health Questions,Yes.,4,"""We study how large language model biases are exhibited through these contextual questions in the healthcare domain."" and ""Our experiments reveal biases in each of these attributes, where young adult female users are favored.""",2024,2024-03-07T19:15:40Z,"Keyphrase: ""Contextual bias in healthcare domain""","""We study how large language model biases are exhibited through these contextual questions in the healthcare domain."" and ""Our experiments reveal biases in each of these attributes, where young adult female users are favored."" Keyphrase: ""Contextual bias in healthcare domain"""
arxiv2024,LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error,Yes.,5,"""Existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice.""",2024,2024-03-07T18:50:51Z,"Keyphrase: ""Low correctness rate""","""Existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice."" Keyphrase: ""Low correctness rate"""
arxiv2024,SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM,Yes.,4,"""Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses.""",2024,2024-03-07T18:38:17Z,"Keyphrase: ""Difficulty with longtail entities and hallucinated responses""","""Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses."" Keyphrase: ""Difficulty with longtail entities and hallucinated responses"""
arxiv2024,How Far Are We from Intelligent Visual Deductive Reasoning?,Yes.,4,"""we are still far from achieving comparable proficiency in visual deductive reasoning,"" and ""certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks,"" and ""VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.""",2024,2024-03-07T18:35:54Z,"Keyphrase: ""Limited visual deductive reasoning proficiency""","""we are still far from achieving comparable proficiency in visual deductive reasoning,"" and ""certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks,"" and ""VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples."" Keyphrase: ""Limited visual deductive reasoning proficiency"""
arxiv2024,Common 7B Language Models Already Possess Strong Math Capabilities,Yes.,5,"""The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities.""",2024,2024-03-07T18:00:40Z,"Keyphrase: ""Inconsistent mathematical capability""","""The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities."" Keyphrase: ""Inconsistent mathematical capability"""
arxiv2024,Telecom Language Models: Must They Be Large?,Yes.,5,"""the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments"" and ""highlighting its potential and limitations.""",2024,2024-03-07T17:13:12Z,"Keyphrase: ""High computational demand""","""the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments"" and ""highlighting its potential and limitations."" Keyphrase: ""High computational demand"""
arxiv2024,QAQ: Quality Adaptive Quantization for LLM KV Cache,Yes.,5,"""a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length"" and ""heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance.""",2024,2024-03-07T16:42:37Z,"Keyphrase: ""Context length bottleneck""","""a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length"" and ""heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance."" Keyphrase: ""Context length bottleneck"""
arxiv2024,HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild,Yes.,5,"""Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains.""",2024,2024-03-07T08:25:46Z,"Keyphrase: ""Reliability challenges due to hallucination""","""Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains."" Keyphrase: ""Reliability challenges due to hallucination"""
arxiv2024,Effectiveness Assessment of Recent Large Vision-Language Models,Yes.,5,"""Our investigations reveal that these models demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deeper into this inadequacy and suggest several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems.""",2024,2024-03-07T08:25:27Z,"Keyphrase: ""Limited proficiency in specialized tasks""","""Our investigations reveal that these models demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deeper into this inadequacy and suggest several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems."" Keyphrase: ""Limited proficiency in specialized tasks"""
arxiv2024,Can Small Language Models be Good Reasoners for Sequential Recommendation?,Yes.,5,"""However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high",2024,2024-03-07T06:49:37Z,"Keyphrase: ""Complex user behavior patterns and resource requirements""","""However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high Keyphrase: ""Complex user behavior patterns and resource requirements"""
arxiv2024,Exploring LLM-based Agents for Root Cause Analysis,Yes.,5,"""However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes.""",2024,2024-03-07T00:44:01Z,"Keyphrase: ""Limited diagnostic information collection""","""However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes."" Keyphrase: ""Limited diagnostic information collection"""
arxiv2024,Can Large Language Models Reason and Plan?,Yes.,4,"""While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.""",2024,2024-03-07T00:36:32Z,"Keyphrase: ""Limited self-correction capabilities""","""While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs."" Keyphrase: ""Limited self-correction capabilities"""
arxiv2024,Artificial Intelligence Exploring the Patent Field,Yes.,4,"""However, patents entail a number of difficulties with which existing models struggle."" and ""Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms. Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections",2024,2024-03-06T23:17:16Z,"Keyphrase: ""Struggles with patent language and legal terms""","""However, patents entail a number of difficulties with which existing models struggle."" and ""Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms. Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections Keyphrase: ""Struggles with patent language and legal terms"""
arxiv2024,Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models,Yes.,4,"""there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data"" and ""models perform significantly better on the subset of the benchmarks where similar solutions are seen during training.""",2024,2024-03-06T21:45:35Z,"Keyphrase: ""Data contamination risks""","""there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data"" and ""models perform significantly better on the subset of the benchmarks where similar solutions are seen during training."" Keyphrase: ""Data contamination risks"""
arxiv2024,Can Large Language Models do Analytical Reasoning?,Yes.,5,"""we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores."" and ""we conclude that task complexity depends on the length of context, the information density, and the presence of related information.""",2024,2024-03-06T20:22:08Z,"Keyphrase: ""Difficulty with accurate counting""","""we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores."" and ""we conclude that task complexity depends on the length of context, the information density, and the presence of related information."" Keyphrase: ""Difficulty with accurate counting"""
arxiv2024,KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions,Yes.,5,"""we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement.""",2024,2024-03-06T17:16:44Z,"Keyphrase: ""Struggles with incorporating new information and user instructions""","""we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement."" Keyphrase: ""Struggles with incorporating new information and user instructions"""
arxiv2024,Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning,Yes.,5,"""Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles.""",2024,2024-03-06T17:15:04Z,"Keyphrase: ""Limited performance in puzzlesolving""","""Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles."" Keyphrase: ""Limited performance in puzzlesolving"""
arxiv2024,ShortGPT: Layers in Large Language Models are More Redundant Than You Expect,Yes.,5,"""However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality.""",2024,2024-03-06T17:04:18Z,"Keyphrase: ""Negligible role of certain layers""","""However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality."" Keyphrase: ""Negligible role of certain layers"""
arxiv2024,Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ,Yes.,4,"""However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen)."" and ""there is a long tail of languages where models are neither accurate nor faithful.""",2024,2024-03-06T16:01:44Z,"Keyphrase: ""Limited accuracy and faithfulness in non-English language models""","""However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen)."" and ""there is a long tail of languages where models are neither accurate nor faithful."" Keyphrase: ""Limited accuracy and faithfulness in non-English language models"""
arxiv2024,German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset,Yes.,5,"""Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document.""",2024,2024-03-06T14:37:30Z,"Keyphrase: ""Inconsistent content generation""","""Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document."" Keyphrase: ""Inconsistent content generation"""
arxiv2024,Towards Safe and Aligned Large Language Models for Medicine,Yes.,4,"""While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses,"" and ""the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights.""",2024,2024-03-06T14:34:07Z,"Keyphrase: ""Lack of safety evaluation""","""While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses,"" and ""the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights."" Keyphrase: ""Lack of safety evaluation"""
arxiv2024,Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem,Yes.,5,"""However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination.""",2024,2024-03-06T09:06:34Z,"Keyphrase: ""Unreliable hallucination""","""However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination."" Keyphrase: ""Unreliable hallucination"""
arxiv2024,Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models,Yes.,4,"""Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation.""",2024,2024-03-06T08:50:25Z,"Keyphrase: ""Limited emotional understanding""","""Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation."" Keyphrase: ""Limited emotional understanding"""
arxiv2024,Towards Efficient and Effective Unlearning of Large Language Models for Recommendation,Yes.,4,"""recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process.""",2024,2024-03-06T08:31:35Z,"Keyphrase: ""Inefficient unlearning process""","""recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process."" Keyphrase: ""Inefficient unlearning process"""
arxiv2024,CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models,Yes.,5,"""We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings.""",2024,2024-03-06T07:43:43Z,"Keyphrase: ""Challenges in long-context settings""","""We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings."" Keyphrase: ""Challenges in long-context settings"""
arxiv2024,Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy,Yes.,5,"""Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities,""",2024,2024-03-05T19:40:53Z,"Keyphrase: ""Limited rational reasoning capabilities""","""Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities,"" Keyphrase: ""Limited rational reasoning capabilities"""
arxiv2024,Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs,Yes.,5,"""Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other L",2024,2024-03-05T19:32:01Z,"Keyphrase: ""Data leakage and overfitting""","""Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other L Keyphrase: ""Data leakage and overfitting"""
arxiv2024,The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning,Yes.,4,"""The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons."" and ""WMDP serves two roles",2024,2024-03-05T18:59:35Z,"Keyphrase: ""Security risks and misuse""","""The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons."" and ""WMDP serves two roles Keyphrase: ""Security risks and misuse"""
arxiv2024,"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",Yes.,4,"""We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes."" and ""The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.""",2024,2024-03-05T17:04:05Z,"Keyphrase: ""Gendered emotion stereotypes""","""We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes."" and ""The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications."" Keyphrase: ""Gendered emotion stereotypes"""
arxiv2024,KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents,Yes.,5,"""Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories",2024,2024-03-05T16:39:12Z,"Keyphrase: ""Lack of actionable knowledge""","""Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories Keyphrase: ""Lack of actionable knowledge"""
arxiv2024,Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations,Yes.,4,"""However, their precision is still far away from acceptable in a sensitive field like education."" and ""reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context.""",2024,2024-03-05T14:41:12Z,"Keyphrase: ""Risk of hallucination and imprecise information""","""However, their precision is still far away from acceptable in a sensitive field like education."" and ""reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context."" Keyphrase: ""Risk of hallucination and imprecise information"""
arxiv2024,ImgTrojan: Jailbreaking Vision-Language Models with ONE Image,Yes.,4,"""However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored.""",2024,2024-03-05T12:21:57Z,"Keyphrase: ""Underexplored safety concerns""","""However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored."" Keyphrase: ""Underexplored safety concerns"""
arxiv2024,In Search of Truth: An Interrogation Approach to Hallucination Detection,Yes.,5,"""One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth.""",2024,2024-03-05T11:50:01Z,"Keyphrase: ""Factual truth drift""","""One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth."" Keyphrase: ""Factual truth drift"""
arxiv2024,An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers,Yes.,4,"""their generalizability and fairness severely underperform GPT4.""",2024,2024-03-05T10:20:52Z,"Keyphrase: ""Poor generalizability and fairness""","""their generalizability and fairness severely underperform GPT4."" Keyphrase: ""Poor generalizability and fairness"""
arxiv2024,EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs,Yes.,5,"""However, their expensive computations and high memory requirements are prohibitive for deployment."" and ""the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks.""",2024,2024-03-05T08:45:30Z,"Keyphrase: ""High resource requirements""","""However, their expensive computations and high memory requirements are prohibitive for deployment."" and ""the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks."" Keyphrase: ""High resource requirements"""
arxiv2024,"Towards Measuring and Modeling ""Culture"" in LLMs: A Survey",Yes.,4,"""Our analysis indicates that only certain aspects of 'culture,' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situated",2024,2024-03-05T08:29:36Z,"Keyphrase: ""Lack of robustness in semantic understanding""","""Our analysis indicates that only certain aspects of 'culture,' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situated Keyphrase: ""Lack of robustness in semantic understanding"""
arxiv2024,Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models,Yes.,5,"""when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains.""",2024,2024-03-05T08:22:41Z,"Keyphrase: ""Catastrophic forgetting in specific domains""","""when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains."" Keyphrase: ""Catastrophic forgetting in specific domains"""
arxiv2024,Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,Yes.,4,"""they still face challenges of various biases"" and ""Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs.""",2024,2024-03-05T07:47:34Z,"Keyphrase: ""Limited traditional debiasing methods""","""they still face challenges of various biases"" and ""Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs."" Keyphrase: ""Limited traditional debiasing methods"""
arxiv2024,Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models,Yes.,4,"""available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese,"" and ""models with more parameters can introduce more biases and uncalibrated outputs.""",2024,2024-03-05T07:13:28Z,"Keyphrase: ""Bias and uncalibrated output""","""available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese,"" and ""models with more parameters can introduce more biases and uncalibrated outputs."" Keyphrase: ""Bias and uncalibrated output"""
arxiv2024,InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents,Yes.,5,"""Our findings raise questions about the widespread deployment of LLM Agents.""",2024,2024-03-05T06:21:45Z,"Keyphrase: ""Concerns about deployment""","""Our findings raise questions about the widespread deployment of LLM Agents."" Keyphrase: ""Concerns about deployment"""
arxiv2024,Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding,Yes.,5,"""the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled.""",2024,2024-03-05T04:58:37Z,"Keyphrase: ""Difficulty in context understanding""","""the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled."" Keyphrase: ""Difficulty in context understanding"""
arxiv2024,Exploring the Limitations of Large Language Models in Compositional Relation Reasoning,Yes.,5,"""We present a comprehensive evaluation of large language models (LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations.""",2024,2024-03-05T03:07:10Z,"Keyphrase: ""Limited reasoning ability""","""We present a comprehensive evaluation of large language models (LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations."" Keyphrase: ""Limited reasoning ability"""
arxiv2024,"Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",Yes.,5,"""a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted.""",2024,2024-03-04T22:02:12Z,"Keyphrase: ""Knowledge reduction and forgetting""","""a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted."" Keyphrase: ""Knowledge reduction and forgetting"""
arxiv2024,SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models,Yes.,5,"""However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs.""",2024,2024-03-04T21:55:22Z,"Keyphrase: ""Confidently wrong predictions""","""However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs."" Keyphrase: ""Confidently wrong predictions"""
arxiv2024,Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems,Yes.,5,"""We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls.""",2024,2024-03-04T19:12:48Z,"Keyphrase: ""Inconsistent performance with voting inference""","""We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls."" Keyphrase: ""Inconsistent performance with voting inference"""
arxiv2024,FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction,Yes.,4,"""a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations"" and ""these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles),",2024,2024-03-04T17:57:18Z,"Keyphrase: ""Factual inconsistency and lack of interpretability""","""a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations"" and ""these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles), Keyphrase: ""Factual inconsistency and lack of interpretability"""
arxiv2024,Birbal: An efficient 7B instruct-model fine-tuned with curated datasets,Yes.,4,"""LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility. Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible.""",2024,2024-03-04T17:34:46Z,"Keyphrase: ""High cost and lack of transparency""","""LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility. Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible."" Keyphrase: ""High cost and lack of transparency"""
arxiv2024,PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models,Yes.,4,"""Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at."" and ""our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their reasoning abilities in an unexpected manner.""",2024,2024-03-04T17:34:34Z,"Keyphrase: ""Limited socialcognitive reasoning""","""Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at."" and ""our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their reasoning abilities in an unexpected manner."" Keyphrase: ""Limited socialcognitive reasoning"""
arxiv2024,Cognition is All You Need -- The Next Layer of AI Above Large Language Models,Yes.,5,"""Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving."" and ""The failure of these systems to address complex knowledge work is due to the fact that they",2024,2024-03-04T16:11:57Z,"Keyphrase: ""Limited reasoning and problem-solving capabilities""","""Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving."" and ""The failure of these systems to address complex knowledge work is due to the fact that they Keyphrase: ""Limited reasoning and problem-solving capabilities"""
arxiv2024,Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?,Yes.,4,"""inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss.""",2024,2024-03-04T14:01:11Z,"Keyphrase: ""English-centric bias and information loss""","""inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss."" Keyphrase: ""English-centric bias and information loss"""
arxiv2024,Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism,Yes.,4,"""While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.""",2024,2024-03-04T13:57:37Z,"Keyphrase: ""Hallucination propensity""","""While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions."" Keyphrase: ""Hallucination propensity"""
arxiv2024,Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?,Yes.,4,"""we analyse the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve and remains limited, with low scores especially for low-resource languages.""",2024,2024-03-04T10:48:13Z,"Keyphrase: ""Limited improvement in low-resource language adaptation""","""we analyse the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve and remains limited, with low scores especially for low-resource languages."" Keyphrase: ""Limited improvement in low-resource language adaptation"""
arxiv2024,WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations,Yes.,5,"""existing datasets and evaluation methods in this domain still exhibit notable limitations"" and ""highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement.""",2024,2024-03-04T07:06:41Z,"Keyphrase: ""Source attribution challenges""","""existing datasets and evaluation methods in this domain still exhibit notable limitations"" and ""highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement."" Keyphrase: ""Source attribution challenges"""
arxiv2024,How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems,Yes.,5,"""a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems.""",2024,2024-03-04T06:24:21Z,"Keyphrase: ""Struggle with capturing relationships among decision variables""","""a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems."" Keyphrase: ""Struggle with capturing relationships among decision variables"""
arxiv2024,SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction,Yes.,4,"""LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge.""",2024,2024-03-03T17:35:52Z,"Keyphrase: ""Deficiency in vertical knowledge""","""LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge."" Keyphrase: ""Deficiency in vertical knowledge"""
arxiv2024,In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation,Yes.,5,"""Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited.""",2024,2024-03-03T15:53:41Z,"Keyphrase: ""Frequent hallucinations and factual errors""","""Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited."" Keyphrase: ""Frequent hallucinations and factual errors"""
arxiv2024,Ever-Evolving Memory by Blending and Refining the Past,Yes.,5,"""current large language models often lack this capability, leading to instances of missing important user information or redundantly asking for the same information, thereby diminishing conversation quality.""",2024,2024-03-03T08:12:59Z,"Keyphrase: ""Lack of conversational capability""","""current large language models often lack this capability, leading to instances of missing important user information or redundantly asking for the same information, thereby diminishing conversation quality."" Keyphrase: ""Lack of conversational capability"""
arxiv2024,Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge,Yes.,5,"""However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications.""",2024,2024-03-03T08:07:55Z,"Keyphrase: ""Struggles with low-frequency concepts""","""However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications."" Keyphrase: ""Struggles with low-frequency concepts"""
arxiv2024,CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge,Yes.,5,"""existing KGQA datasets focus on popular entities for which large language models (LLMs) can directly answer without hallucinating and without leveraging the KG"" and ""baseline evaluation of LLMs on CR-LT KGQA demonstrate a high rate of",2024,2024-03-03T04:47:01Z,"Keyphrase: ""Limited knowledge grounding""","""existing KGQA datasets focus on popular entities for which large language models (LLMs) can directly answer without hallucinating and without leveraging the KG"" and ""baseline evaluation of LLMs on CR-LT KGQA demonstrate a high rate of Keyphrase: ""Limited knowledge grounding"""
arxiv2024,Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering,Yes.,4,"""existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable.""",2024,2024-03-03T04:22:13Z,"Keyphrase: ""Hallucination in KGQA""","""existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable."" Keyphrase: ""Hallucination in KGQA"""
arxiv2024,Analysis of Privacy Leakage in Federated Large Language Models,Yes.,5,"""revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets.""",2024,2024-03-02T20:25:38Z,"Keyphrase: ""Privacy vulnerabilities""","""revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets."" Keyphrase: ""Privacy vulnerabilities"""
arxiv2024,Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal,Yes.,5,"""Large language models (LLMs) suffer from catastrophic forgetting during continual learning.""",2024,2024-03-02T16:11:23Z,"Keyphrase: ""Catastrophic forgetting""","""Large language models (LLMs) suffer from catastrophic forgetting during continual learning."" Keyphrase: ""Catastrophic forgetting"""
arxiv2024,RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,Yes.,5,"""However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge."" and ""These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications.""",2024,2024-03-02T12:19:04Z,"Keyphrase: ""Generation of false information""","""However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge."" and ""These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications."" Keyphrase: ""Generation of false information"""
arxiv2024,"A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",Yes.,4,"""While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale.""",2024,2024-03-02T09:39:13Z,"Keyphrase: ""Risk of misinformation proliferation""","""While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale."" Keyphrase: ""Risk of misinformation proliferation"""
arxiv2024,Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers,Yes.,5,"""We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext.""",2024,2024-03-02T01:52:14Z,"Keyphrase: ""Faithfulness mistakes""","""We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext."" Keyphrase: ""Faithfulness mistakes"""
arxiv2024,Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries,Yes.,4,"""Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health.""",2024,2024-03-01T21:59:03Z,"Keyphrase: ""Lack of grounding in safety-critical domains""","""Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health."" Keyphrase: ""Lack of grounding in safety-critical domains"""
arxiv2024,Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods,Yes.,5,"""we find that LLM predictions are not robust under variation of method choice, both within a single LLM and across different LLMs.""",2024,2024-03-01T21:48:08Z,"Keyphrase: ""Lack of robustness in predictions""","""we find that LLM predictions are not robust under variation of method choice, both within a single LLM and across different LLMs."" Keyphrase: ""Lack of robustness in predictions"""
arxiv2024,MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection,Yes.,5,"""contemporary Large Language Models (LLMs) face several challenges, such as generating fluent yet inaccurate outputs and reliance on fluency-centric metrics. This often leads to neural networks exhibiting 'hallucinations'.""",2024,2024-03-01T20:31:10Z,"Keyphrase: ""Fluent yet inaccurate output""","""contemporary Large Language Models (LLMs) face several challenges, such as generating fluent yet inaccurate outputs and reliance on fluency-centric metrics. This often leads to neural networks exhibiting 'hallucinations'."" Keyphrase: ""Fluent yet inaccurate output"""
arxiv2024,Mitigating Reversal Curse in Large Language Models via Semantic-aware Permutation Training,Yes.,5,"""recent studies showcase that causal LLMs suffer from the 'reversal curse'. It is a typical example that the model knows 'A's father is B', but is unable to reason 'B's child is A'. This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional",2024,2024-03-01T18:55:20Z,"Keyphrase: ""Limited causal reasoning""","""recent studies showcase that causal LLMs suffer from the 'reversal curse'. It is a typical example that the model knows 'A's father is B', but is unable to reason 'B's child is A'. This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional Keyphrase: ""Limited causal reasoning"""
arxiv2024,Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents,Yes.,5,"""However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions."" and ""While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack's mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback.""",2024,2024-03-01T17:22:16Z,"Keyphrase: ""Limited adaptability to dynamic environments""","""However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions."" and ""While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack's mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback."" Keyphrase: ""Limited adaptability to dynamic environments"""
arxiv2024,DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models,Yes.,4,"""Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge,"" and ""many merely focus on the factuality hallucination while ignoring the faithfulness hallucination.""",2024,2024-03-01T15:38:55Z,"Keyphrase: ""Hallucination issue""","""Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge,"" and ""many merely focus on the factuality hallucination while ignoring the faithfulness hallucination."" Keyphrase: ""Hallucination issue"""
arxiv2024,TempCompass: Do Video LLMs Really Understand Videos?,Yes.,5,"""reveal the discerning fact that these models exhibit notably poor temporal perception ability.""",2024,2024-03-01T12:02:19Z,"Keyphrase: ""Poor temporal perception""","""reveal the discerning fact that these models exhibit notably poor temporal perception ability."" Keyphrase: ""Poor temporal perception"""
arxiv2024,Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs,Yes.,5,"""the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks.""",2024,2024-03-01T09:28:38Z,"Keyphrase: ""Contaminated benchmark data""","""the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks."" Keyphrase: ""Contaminated benchmark data"""
arxiv2024,Invariant Test-Time Adaptation for Vision-Language Model Generalization,Yes.,4,"""However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of 'decision shortcuts' that hinders their generalization capabilities."" and ""the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in",2024,2024-03-01T09:01:53Z,"Keyphrase: ""Limited generalization in longtail tasks""","""However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of 'decision shortcuts' that hinders their generalization capabilities."" and ""the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in Keyphrase: ""Limited generalization in longtail tasks"""
arxiv2024,Teach LLMs to Phish: Stealing Private Information from Language Models,Yes.,5,"""When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information.""",2024,2024-03-01T06:15:07Z,"Keyphrase: ""Privacy risks and sensitive information memorization""","""When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information."" Keyphrase: ""Privacy risks and sensitive information memorization"""
arxiv2024,DPP-Based Adversarial Prompt Searching for Lanugage Models,Yes.,4,"""Language models risk generating mindless and offensive content, which hinders their safe deployment."" and ""Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content.""",2024,2024-03-01T05:28:06Z,"Keyphrase: ""Generation of offensive content""","""Language models risk generating mindless and offensive content, which hinders their safe deployment."" and ""Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content."" Keyphrase: ""Generation of offensive content"""
arxiv2024,Gender Bias in Large Language Models across Multiple Languages,Yes.,5,"""assessing the influence of gender biases embedded in LLMs becomes crucial"" and ""Our findings revealed significant gender biases across all the languages we examined.""",2024,2024-03-01T04:47:16Z,"Keyphrase: ""Gender bias""","""assessing the influence of gender biases embedded in LLMs becomes crucial"" and ""Our findings revealed significant gender biases across all the languages we examined."" Keyphrase: ""Gender bias"""
arxiv2024,Extracting Polymer Nanocomposite Samples from Full-Length Documents,Yes.,5,"""Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.""",2024,2024-03-01T03:51:56Z,"Keyphrase: ""Challenges in information extraction""","""Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them."" Keyphrase: ""Challenges in information extraction"""
arxiv2024,Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes,Yes.,4,"""recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails.""",2024,2024-03-01T03:29:54Z,"Keyphrase: ""Vulnerability to adversarial jailbreak attempts""","""recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails."" Keyphrase: ""Vulnerability to adversarial jailbreak attempts"""
arxiv2024,Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,Yes.,5,"""However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains."" and ""Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.""",2024,2024-03-01T02:21:30Z,"Keyphrase: ""Limited interpretive abilities in scientific contexts""","""However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains."" and ""Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements."" Keyphrase: ""Limited interpretive abilities in scientific contexts"""
arxiv2024,AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs,Yes.,4,"""Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications.""",2024,2024-03-01T00:02:37Z,"Keyphrase: ""Bias from training data""","""Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications."" Keyphrase: ""Bias from training data"""
arxiv2024,FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition,Yes.,5,"""such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities"" and ""we identify a common shortfall in knowledge utilization among models.""",2024,2024-02-29T21:05:37Z,"Keyphrase: ""Limited cognitive skill differentiation""","""such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities"" and ""we identify a common shortfall in knowledge utilization among models."" Keyphrase: ""Limited cognitive skill differentiation"""
arxiv2024,NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications,Yes.,4,"""highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks.""",2024,2024-02-29T21:05:14Z,"Keyphrase: ""Ethical and creative deficiencies""","""highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks."" Keyphrase: ""Ethical and creative deficiencies"""
arxiv2024,Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs,Yes.,5,"""Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates.""",2024,2024-02-29T19:55:06Z,"Keyphrase: ""Memory limitations""","""Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates."" Keyphrase: ""Memory limitations"""
arxiv2024,PROC2PDDL: Open-Domain Planning Representations from Texts,Yes.,5,"""We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific programs and reasoning about events.""",2024,2024-02-29T19:40:25Z,"Keyphrase: ""Domain-specific program generation deficiency""","""We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific programs and reasoning about events."" Keyphrase: ""Domain-specific program generation deficiency"""
arxiv2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,Yes.,5,"""This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences.""",2024,2024-02-29T19:02:03Z,"Keyphrase: ""Difficulty with out-of-distribution tokens""","""This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences."" Keyphrase: ""Difficulty with out-of-distribution tokens"""
arxiv2024,Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization,Yes.,4,"""However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases."" and ""almost all of them (except GPT-4), even after fine-tuning, could not properly generate the response in the required output format.""",2024,2024-02-29T19:00:47Z,"Keyphrase: ""High inference costs""","""However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases."" and ""almost all of them (except GPT-4), even after fine-tuning, could not properly generate the response in the required output format."" Keyphrase: ""High inference costs"""
arxiv2024,The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?,Yes.,5,"""we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit",2024,2024-02-29T18:59:25Z,"Keyphrase: ""Limited reasoning and correction capabilities""","""we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit Keyphrase: ""Limited reasoning and correction capabilities"""
arxiv2024,Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models,Yes.,4,"""Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness."" and ""we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions",2024,2024-02-29T18:55:06Z,"Keyphrase: ""Trustworthiness concerns""","""Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness."" and ""we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions Keyphrase: ""Trustworthiness concerns"""
arxiv2024,Curiosity-driven Red-teaming for Large Language Models,Yes.,5,"""Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content.""",2024,2024-02-29T18:55:03Z,"Keyphrase: ""Risk of generating toxic content""","""Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content."" Keyphrase: ""Risk of generating toxic content"""
arxiv2024,On the Scaling Laws of Geographical Representation in Language Models,Yes.,4,"""we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.""",2024,2024-02-29T18:04:11Z,"Keyphrase: ""Geographical bias persistence""","""we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data."" Keyphrase: ""Geographical bias persistence"""
arxiv2024,Entity-Aware Multimodal Alignment Framework for News Image Captioning,Yes.,4,"""Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-t",2024,2024-02-29T18:03:00Z,"Keyphrase: ""Limited entity information handling""","""Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-t Keyphrase: ""Limited entity information handling"""
arxiv2024,Watermark Stealing in Large Language Models,Yes.,5,"""We dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes."" and ""Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes.""",2024,2024-02-29T17:12:39Z,"Keyphrase: ""Vulnerability to watermarking""","""We dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes."" and ""Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes."" Keyphrase: ""Vulnerability to watermarking"""
arxiv2024,SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation,Yes.,4,"""Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training samples available in practice lead to poor code generation performance.""",2024,2024-02-29T16:09:02Z,"Keyphrase: ""Limited adaptation to specific scenarios""","""Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training samples available in practice lead to poor code generation performance."" Keyphrase: ""Limited adaptation to specific scenarios"""
arxiv2024,GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers,Yes.,5,"""One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly."" and ""Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust.""",2024,2024-02-29T15:26:14Z,"Keyphrase: ""Limited math reasoning ability""","""One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly."" and ""Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust."" Keyphrase: ""Limited math reasoning ability"""
arxiv2024,Memory-Augmented Generative Adversarial Transformers,Yes.,5,"""Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy.""",2024,2024-02-29T14:47:24Z,"Keyphrase: ""Limited ability to integrate external data""","""Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy."" Keyphrase: ""Limited ability to integrate external data"""
arxiv2024,Teaching Large Language Models an Unseen Language on the Fly,Yes.,5,"""Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating.""",2024,2024-02-29T13:50:47Z,"Keyphrase: ""Struggle with low-resource languages""","""Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating."" Keyphrase: ""Struggle with low-resource languages"""
arxiv2024,Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model,Yes.,5,"""However, the Typographic Attack, which disrupts vision-language models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), has also been expected to be a security threat to LVLMs."" and ""Based on the evaluation results, we investigate the causes why typographic attacks may impact VLMs and LVLMs, leading to three highly insightful",2024,2024-02-29T13:31:56Z,"Keyphrase: ""Vulnerability to typographic attacks""","""However, the Typographic Attack, which disrupts vision-language models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), has also been expected to be a security threat to LVLMs."" and ""Based on the evaluation results, we investigate the causes why typographic attacks may impact VLMs and LVLMs, leading to three highly insightful Keyphrase: ""Vulnerability to typographic attacks"""
arxiv2024,Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models,Yes.,5,"""Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted",2024,2024-02-29T12:35:45Z,"Keyphrase: ""Hallucination of text""","""Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted Keyphrase: ""Hallucination of text"""
arxiv2024,Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials,Yes.,5,"""investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples"" and ""the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent",2024,2024-02-29T12:01:46Z,"Keyphrase: ""Reliance on shortcut features""","""investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples"" and ""the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent Keyphrase: ""Reliance on shortcut features"""
arxiv2024,Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning,Yes.,5,"""However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem.""",2024,2024-02-29T05:27:45Z,"Keyphrase: ""Catastrophic forgetting""","""However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem."" Keyphrase: ""Catastrophic forgetting"""
arxiv2024,ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph,Yes.,5,"""large language models (LLMs) remain significantly limited in properly using massive external tools"" and ""Such a paradigm ignores the intrinsic dependency between tools and offloads all reasoning loads to LLMs, making them restricted to a limited number of specifically designed tools.""",2024,2024-02-29T02:04:00Z,"Keyphrase: ""Dependence on external tools""","""large language models (LLMs) remain significantly limited in properly using massive external tools"" and ""Such a paradigm ignores the intrinsic dependency between tools and offloads all reasoning loads to LLMs, making them restricted to a limited number of specifically designed tools."" Keyphrase: ""Dependence on external tools"""
arxiv2024,Learning to Compress Prompt in Natural Language Formats,Yes.,5,"""Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results.""",2024,2024-02-28T20:41:21Z,"Keyphrase: ""Inferior long context processing""","""Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results."" Keyphrase: ""Inferior long context processing"""
arxiv2024,FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability,Yes.,5,"""Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately.""",2024,2024-02-28T19:23:27Z,"Keyphrase: ""Benchmark performance inadequacy""","""Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately."" Keyphrase: ""Benchmark performance inadequacy"""
arxiv2024,A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems,Yes.,4,"""there are also increasing concerns over the security of such probabilistic intelligent systems"" and ""Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components.""",2024,2024-02-28T19:00:12Z,"Keyphrase: ""Security vulnerabilities""","""there are also increasing concerns over the security of such probabilistic intelligent systems"" and ""Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components."" Keyphrase: ""Security vulnerabilities"""
arxiv2024,Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates,Yes.,4,"""even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models.""",2024,2024-02-28T18:23:49Z,"Keyphrase: ""Unsafe behavior after finetuning""","""even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models."" Keyphrase: ""Unsafe behavior after finetuning"""
arxiv2024,Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,Yes.,5,"""we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem."" and ""we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers.""",2024,2024-02-28T14:09:02Z,"Keyphrase: ""Information loss and shallow attention""","""we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem."" and ""we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers."" Keyphrase: ""Information loss and shallow attention"""
arxiv2024,Learning or Self-aligning? Rethinking Instruction Fine-tuning,Yes.,5,"""attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects.""",2024,2024-02-28T11:16:00Z,"Keyphrase: ""Limited impact of world knowledge""","""attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects."" Keyphrase: ""Limited impact of world knowledge"""
arxiv2024,LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History,Yes.,5,"""we find that performance can in fact also be negatively impacted, if there is a task-switch"" and ""many of the task-switches can lead to significant performance degradation.""",2024,2024-02-28T10:19:05Z,"Keyphrase: ""Performance degradation due to task-switching""","""we find that performance can in fact also be negatively impacted, if there is a task-switch"" and ""many of the task-switches can lead to significant performance degradation."" Keyphrase: ""Performance degradation due to task-switching"""
arxiv2024,Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,Yes.,5,"""studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it.""",2024,2024-02-28T08:24:38Z,"Keyphrase: ""Challenges in using retrieved information""","""studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it."" Keyphrase: ""Challenges in using retrieved information"""
arxiv2024,"Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",Yes.,4,"""cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts.""",2024,2024-02-28T07:18:39Z,"Keyphrase: ""Crosslingual inconsistency""","""cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts."" Keyphrase: ""Crosslingual inconsistency"""
arxiv2024,Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction,Yes.,5,"""One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs.""",2024,2024-02-28T06:50:14Z,"Keyphrase: ""Vulnerability to adversarial prompts""","""One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs."" Keyphrase: ""Vulnerability to adversarial prompts"""
arxiv2024,No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization,Yes.,5,"""the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself.""",2024,2024-02-28T06:34:54Z,"Keyphrase: ""Memory footprint bottleneck""","""the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself."" Keyphrase: ""Memory footprint bottleneck"""
arxiv2024,Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions,Yes.,4,"""However, medical board exam questions or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions.""",2024,2024-02-28T05:44:41Z,"Keyphrase: ""Limited clinical complexity capture""","""However, medical board exam questions or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions."" Keyphrase: ""Limited clinical complexity capture"""
arxiv2024,MEGAnno+: A Human-LLM Collaborative Annotation System,Yes.,5,"""Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations.""",2024,2024-02-28T04:58:07Z,"Keyphrase: ""Difficulty with complex sociocultural context""","""Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations."" Keyphrase: ""Difficulty with complex sociocultural context"""
arxiv2024,Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore,Yes.,4,"""Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge,"" and ""multilingual models demonstrate a bias towards factual information from Western continents.""",2024,2024-02-28T04:43:46Z,"Keyphrase: ""Factual hallucination and bias""","""Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge,"" and ""multilingual models demonstrate a bias towards factual information from Western continents."" Keyphrase: ""Factual hallucination and bias"""
arxiv2024,Automated Discovery of Integral with Deep Learning,Yes.,4,"""Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do.""",2024,2024-02-28T04:34:15Z,"Keyphrase: ""Limited to existing human knowledge""","""Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do."" Keyphrase: ""Limited to existing human knowledge"""
arxiv2024,Corpus-Steered Query Expansion with Large Language Models,Yes.,5,"""challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs.""",2024,2024-02-28T03:58:58Z,"Keyphrase: ""Misalignment in retrieval corpus""","""challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs."" Keyphrase: ""Misalignment in retrieval corpus"""
arxiv2024,TroubleLLM: Align to Red Team Expert,Yes.,4,"""However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content.""",2024,2024-02-28T03:40:46Z,"Keyphrase: ""Undesirable safety issues""","""However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content."" Keyphrase: ""Undesirable safety issues"""
arxiv2024,FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization,Yes.,5,"""However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance.""",2024,2024-02-28T02:00:34Z,"Keyphrase: ""Latency and memory consumption restrictions""","""However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance."" Keyphrase: ""Latency and memory consumption restrictions"""
arxiv2024,Collaborative decoding of critical tokens for boosting factuality of large language models,Yes.,5,"""their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination.""",2024,2024-02-28T01:53:37Z,"Keyphrase: ""Increased risk of hallucination""","""their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination."" Keyphrase: ""Increased risk of hallucination"""
arxiv2024,Gradient-Free Adaptive Global Pruning for Pre-trained Language Models,Yes.,5,"""The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands.""",2024,2024-02-28T00:09:07Z,"Keyphrase: ""Prohibitive computational demand""","""The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands."" Keyphrase: ""Prohibitive computational demand"""
arxiv2024,LLM-Resistant Math Word Problem Generation via Adversarial Attacks,Yes.,5,"""We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure, offering a nuanced view into model's limitation.""",2024,2024-02-27T22:07:52Z,"Keyphrase: ""Vulnerability to attacks""","""We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure, offering a nuanced view into model's limitation."" Keyphrase: ""Vulnerability to attacks"""
arxiv2024,BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra,Yes.,5,"""due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting.""",2024,2024-02-27T20:48:24Z,"Keyphrase: ""Context size limitation""","""due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting."" Keyphrase: ""Context size limitation"""
arxiv2024,Evaluating Very Long-Term Conversational Memory of LLM Agents,Yes.,5,"""Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.""",2024,2024-02-27T18:42:31Z,"Keyphrase: ""Difficulty in long-range conversation comprehension""","""Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues."" Keyphrase: ""Difficulty in long-range conversation comprehension"""
arxiv2024,AmbigNLG: Addressing Task Ambiguity in Instruction for NLG,Yes.,5,"""their performance is significantly hindered by the ambiguity present in real-world instructions.""",2024,2024-02-27T17:52:33Z,"Keyphrase: ""Ambiguity in real-world instructions""","""their performance is significantly hindered by the ambiguity present in real-world instructions."" Keyphrase: ""Ambiguity in real-world instructions"""
arxiv2024,Case-Based or Rule-Based: How Do Transformers Do the Math?,Yes.,5,"""modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition.""",2024,2024-02-27T17:41:58Z,"Keyphrase: ""Struggles with math problems""","""modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition."" Keyphrase: ""Struggles with math problems"""
arxiv2024,NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents,Yes.,5,"""they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism.""",2024,2024-02-27T16:56:30Z,"Keyphrase: ""Struggles with processing long sequences""","""they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism."" Keyphrase: ""Struggles with processing long sequences"""
arxiv2024,Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data,Yes.,5,"""Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously.""",2024,2024-02-27T16:15:03Z,"Keyphrase: ""Limited causal reasoning""","""Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously."" Keyphrase: ""Limited causal reasoning"""
arxiv2024,OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web,Yes.,5,"""The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents.""",2024,2024-02-27T14:47:53Z,"Keyphrase: ""Limited task proficiency""","""The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents."" Keyphrase: ""Limited task proficiency"""
arxiv2024,TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge.""",2024,2024-02-27T14:45:04Z,"Keyphrase: ""Hallucination and untruthful responses""","""Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge."" Keyphrase: ""Hallucination and untruthful responses"""
arxiv2024,Predict the Next Word: Humans exhibit uncertainty in this task and language models _____,Yes.,5,"""We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty.""",2024,2024-02-27T14:11:32Z,"Keyphrase: ""Low calibration of human uncertainty""","""We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty."" Keyphrase: ""Low calibration of human uncertainty"""
arxiv2024,REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering,Yes.,5,"""LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents).""",2024,2024-02-27T13:22:51Z,"Keyphrase: ""Limited relevance assessment""","""LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents)."" Keyphrase: ""Limited relevance assessment"""
arxiv2024,Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles,Yes.,5,"""Results showed that GPT-4's performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4",2024,2024-02-27T13:02:19Z,"Keyphrase: ""Limited performance in detecting propaganda techniques""","""Results showed that GPT-4's performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 Keyphrase: ""Limited performance in detecting propaganda techniques"""
arxiv2024,Training-Free Long-Context Scaling of Large Language Models,Yes.,5,"""The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length.""",2024,2024-02-27T12:39:23Z,"Keyphrase: ""Limited coherence with lengthy input""","""The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length."" Keyphrase: ""Limited coherence with lengthy input"""
arxiv2024,Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective,Yes.,5,"""there is still a lack of research on LLM consistency, meaning that throughout the various stages of LLM research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive",2024,2024-02-27T11:02:12Z,"Keyphrase: ""Consistency and adaptability challenges""","""there is still a lack of research on LLM consistency, meaning that throughout the various stages of LLM research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive Keyphrase: ""Consistency and adaptability challenges"""
arxiv2024,LLMGuard: Guarding Against Unsafe LLM Behavior,Yes.,5,"""it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns.""",2024,2024-02-27T10:22:45Z,"Keyphrase: ""Generating inappropriate biased content""","""it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns."" Keyphrase: ""Generating inappropriate biased content"""
arxiv2024,SoFA: Shielded On-the-fly Alignment via Priority Rule Following,Yes.,5,"""even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules.""",2024,2024-02-27T09:52:27Z,"Keyphrase: ""Poor rule understanding and prioritization""","""even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules."" Keyphrase: ""Poor rule understanding and prioritization"""
arxiv2024,Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese,Yes.,4,"""Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages.""",2024,2024-02-27T08:24:32Z,"Keyphrase: ""Knowledge adequacy disparity""","""Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages."" Keyphrase: ""Knowledge adequacy disparity"""
arxiv2024,Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue,Yes.,5,"""Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs.""",2024,2024-02-27T07:11:59Z,"Keyphrase: ""Inadequate safety mechanisms""","""Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs."" Keyphrase: ""Inadequate safety mechanisms"""
arxiv2024,Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection,Yes.,5,"""We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD,",2024,2024-02-27T07:02:10Z,"Keyphrase: ""Limited zero-shot/few-shot capabilities""","""We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD, Keyphrase: ""Limited zero-shot/few-shot capabilities"""
arxiv2024,Measuring Vision-Language STEM Skills of Neural Models,Yes.,5,"""Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance.""",2024,2024-02-27T04:55:03Z,"Keyphrase: ""Limited grade-level skill mastery""","""Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance."" Keyphrase: ""Limited grade-level skill mastery"""
arxiv2024,Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses,Yes.,5,"""Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios.""",2024,2024-02-27T00:22:18Z,"Keyphrase: ""Hallucination challenge""","""Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios."" Keyphrase: ""Hallucination challenge"""
arxiv2024,Pandora's White-Box: Increased Training Data Leakage in Open LLMs,Yes.,5,"""these findings show that highly effective MIAs are available in almost all LLM training settings, and highlight that great care must be taken before LLMs are fine-tuned on highly sensitive data and then deployed.""",2024,2024-02-26T20:41:50Z,"Keyphrase: ""Sensitive data risks""","""these findings show that highly effective MIAs are available in almost all LLM training settings, and highlight that great care must be taken before LLMs are fine-tuned on highly sensitive data and then deployed."" Keyphrase: ""Sensitive data risks"""
arxiv2024,Algorithmic Arbitrariness in Content Moderation,Yes.,4,"""We analyze (i) the extent of predictive multiplicity among state-of-the-art LLMs used for detecting toxic content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) how model multiplicity compares to unambiguous human classifications.""",2024,2024-02-26T19:27:00Z,"Keyphrase: ""Predictive multiplicity and disparate impact""","""We analyze (i) the extent of predictive multiplicity among state-of-the-art LLMs used for detecting toxic content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) how model multiplicity compares to unambiguous human classifications."" Keyphrase: ""Predictive multiplicity and disparate impact"""
arxiv2024,A Survey of Large Language Models in Cybersecurity,Yes.,5,"""This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome.""",2024,2024-02-26T19:06:02Z,"Keyphrase: ""Limited application in cybersecurity""","""This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome."" Keyphrase: ""Limited application in cybersecurity"""
arxiv2024,"Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding",Yes.,5,"""However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications.""",2024,2024-02-26T18:59:28Z,"Keyphrase: ""High deployment cost and latency issues""","""However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications."" Keyphrase: ""High deployment cost and latency issues"""
arxiv2024,MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT,Yes.,5,"""However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency.""",2024,2024-02-26T18:59:03Z,"Keyphrase: ""Limited suitability for on-device processing""","""However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency."" Keyphrase: ""Limited suitability for on-device processing"""
arxiv2024,Eight Methods to Evaluate Robust Unlearning in LLMs,Yes.,4,"""we first survey techniques and limitations of existing unlearning evaluations"" and ""Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.""",2024,2024-02-26T18:57:37Z,"Keyphrase: ""Limited unlearning evaluation""","""we first survey techniques and limitations of existing unlearning evaluations"" and ""Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics."" Keyphrase: ""Limited unlearning evaluation"""
arxiv2024,A Surprising Failure? Multimodal LLMs and the NLVR Challenge,Yes.,5,"""Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.""",2024,2024-02-26T18:37:18Z,"Keyphrase: ""Poor compositional spatial reasoning""","""Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases."" Keyphrase: ""Poor compositional spatial reasoning"""
arxiv2024,Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models,Yes.,5,"""most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness.""",2024,2024-02-26T18:00:49Z,"Keyphrase: ""Lack of paraphrase robustness""","""most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness."" Keyphrase: ""Lack of paraphrase robustness"""
arxiv2024,A Comprehensive Evaluation of Quantization Strategies for Large Language Models,Yes.,4,"""Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings."" and ""Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs.""",2024,2024-02-26T17:45:36Z,"Keyphrase: ""Resource-intensive deployment""","""Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings."" and ""Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs."" Keyphrase: ""Resource-intensive deployment"""
arxiv2024,Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study,Yes.,5,"""they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes.""",2024,2024-02-26T16:05:33Z,"Keyphrase: ""Input sequence length constraint""","""they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes."" Keyphrase: ""Input sequence length constraint"""
arxiv2024,StructLM: Towards Building Generalist Models for Structured Knowledge Grounding,Yes.,4,"""Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%.""",2024,2024-02-26T15:47:01Z,"Keyphrase: ""Limited ability with structured data""","""Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%."" Keyphrase: ""Limited ability with structured data"""
arxiv2024,Long-Context Language Modeling with Parallel Context Encoding,Yes.,5,"""the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window"" and ""while existing long-context models degenerate with retrieved contexts.""",2024,2024-02-26T14:47:35Z,"Keyphrase: ""Limited contextual window""","""the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window"" and ""while existing long-context models degenerate with retrieved contexts."" Keyphrase: ""Limited contextual window"""
arxiv2024,LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments,Yes.,4,"""showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration.""",2024,2024-02-26T11:31:48Z,"Keyphrase: ""Limited autonomy and opponent modeling""","""showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration."" Keyphrase: ""Limited autonomy and opponent modeling"""
arxiv2024,Defending LLMs against Jailbreaking Attacks via Backtranslation,Yes.,4,"""Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent.""",2024,2024-02-26T10:03:33Z,"Keyphrase: ""Vulnerability to malicious intent""","""Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent."" Keyphrase: ""Vulnerability to malicious intent"""
arxiv2024,"ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",Yes.,4,"""The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner.""",2024,2024-02-26T09:43:02Z,"Keyphrase: ""Limited safety detection""","""The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner."" Keyphrase: ""Limited safety detection"""
arxiv2024,From RAGs to riches: Using large language models to write documents for clinical trials,Yes.,5,"""however there are concerns about the quality of their output"" and ""deficiencies remain",2024,2024-02-26T08:59:05Z,"Keyphrase: ""Output quality deficiency""","""however there are concerns about the quality of their output"" and ""deficiencies remain Keyphrase: ""Output quality deficiency"""
arxiv2024,Improving LLM-based Machine Translation with Systematic Self-Correction,Yes.,4,"""However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors.""",2024,2024-02-26T07:58:12Z,"Keyphrase: ""Translation errors""","""However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors."" Keyphrase: ""Translation errors"""
arxiv2024,Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models,Yes.,5,"""our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings"" and ""instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs.""",2024,2024-02-26T05:43:51Z,"Keyphrase: ""Modality gap in visual categorization""","""our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings"" and ""instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs."" Keyphrase: ""Modality gap in visual categorization"""
arxiv2024,HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs,Yes.,5,"""Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs.""",2024,2024-02-25T22:23:37Z,"Keyphrase: ""Reliability and alignment challenges""","""Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs."" Keyphrase: ""Reliability and alignment challenges"""
arxiv2024,DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers,Yes.,5,"""The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks,"" and ""current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned L",2024,2024-02-25T17:43:29Z,"Keyphrase: ""Vulnerability to jailbreak attacks""","""The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks,"" and ""current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned L Keyphrase: ""Vulnerability to jailbreak attacks"""
arxiv2024,Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions,Yes.,4,"""generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate"" and ""retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval.""",2024,2024-02-25T11:22:19Z,"Keyphrase: ""Susceptibility to factual errors""","""generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate"" and ""retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval."" Keyphrase: ""Susceptibility to factual errors"""
arxiv2024,Likelihood-based Mitigation of Evaluation Bias in Large Language Models,Yes.,4,"""However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation",2024,2024-02-25T04:52:02Z,"Keyphrase: ""Sentence structure bias""","""However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation Keyphrase: ""Sentence structure bias"""
arxiv2024,Cognitive Bias in High-Stakes Decision-Making with LLMs,Yes.,4,"""LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance.""",2024,2024-02-25T02:35:56Z,"Keyphrase: ""Biases in decision-making""","""LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance."" Keyphrase: ""Biases in decision-making"""
arxiv2024,Rethinking Software Engineering in the Foundation Model Era: A Curated Catalogue of Challenges in the Development of Trustworthy FMware,Yes.,5,"""The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges.""",2024,2024-02-25T00:53:16Z,"Keyphrase: ""Orchestration challenges and hallucination""","""The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges."" Keyphrase: ""Orchestration challenges and hallucination"""
arxiv2024,Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models,Yes.,5,"""Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination."" and ""detecting and mitigating data contamination for LLMs faces significant challenges.""",2024,2024-02-24T23:54:41Z,"Keyphrase: ""Susceptibility to data contamination""","""Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination."" and ""detecting and mitigating data contamination for LLMs faces significant challenges."" Keyphrase: ""Susceptibility to data contamination"""
arxiv2024,Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency,Yes.,4,"""this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency"" and ""Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures.""",2024,2024-02-24T23:17:56Z,"Keyphrase: ""Decreased recall in language proficiency""","""this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency"" and ""Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures."" Keyphrase: ""Decreased recall in language proficiency"""
arxiv2024,Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis,Yes.,5,"""The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm.""",2024,2024-02-24T21:36:26Z,"Keyphrase: ""Limited long-term quantitative reasoning""","""The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm."" Keyphrase: ""Limited long-term quantitative reasoning"""
arxiv2024,PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails,Yes.,5,"""Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content."" and ""Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective.""",2024,2024-02-24T21:27:13Z,"Keyphrase: ""Vulnerability to automated attacks""","""Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content."" and ""Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective."" Keyphrase: ""Vulnerability to automated attacks"""
arxiv2024,Prompt Perturbation Consistency Learning for Robust Language Models,Yes.,5,"""their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts.""",2024,2024-02-24T15:00:58Z,"Keyphrase: ""Poor performance on sequence labeling tasks""","""their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts."" Keyphrase: ""Poor performance on sequence labeling tasks"""
arxiv2024,Empowering Large Language Model Agents through Action Learning,Yes.,5,"""Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior.""",2024,2024-02-24T13:13:04Z,"Keyphrase: ""Limited trial and error learning""","""Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior."" Keyphrase: ""Limited trial and error learning"""
arxiv2024,"From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models",Yes.,4,"""Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks."" and ""our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with LLMs.""",2024,2024-02-24T09:06:25Z,"Keyphrase: ""Inadequate risk oversight""","""Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks."" and ""our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with LLMs."" Keyphrase: ""Inadequate risk oversight"""
arxiv2024,Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models,Yes.,4,"""Large language models (LLMs) still grapple with complex tasks like mathematical reasoning.""",2024,2024-02-24T08:40:30Z,"Keyphrase: ""Struggles with mathematical reasoning""","""Large language models (LLMs) still grapple with complex tasks like mathematical reasoning."" Keyphrase: ""Struggles with mathematical reasoning"""
arxiv2024,Stepwise Self-Consistent Mathematical Reasoning with Large Language Models,Yes.,4,"""Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions.""",2024,2024-02-24T08:22:39Z,"Keyphrase: ""Limited multistep reasoning""","""Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions."" Keyphrase: ""Limited multistep reasoning"""
arxiv2024,HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition,Yes.,4,"""the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria.""",2024,2024-02-24T08:01:32Z,"Keyphrase: ""Limited evaluation scope and potential bias""","""the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria."" Keyphrase: ""Limited evaluation scope and potential bias"""
arxiv2024,Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology,Yes.,5,"""Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks.""",2024,2024-02-24T02:27:55Z,"Keyphrase: ""Vulnerability to jailbreaking attacks""","""Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks."" Keyphrase: ""Vulnerability to jailbreaking attacks"""
arxiv2024,Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study,Yes.,4,"""The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels",2024,2024-02-24T00:38:29Z,"Keyphrase: ""Performance limitations with data augmentation""","""The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels Keyphrase: ""Performance limitations with data augmentation"""
arxiv2024,Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics,Yes.,5,"""demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task.""",2024,2024-02-24T00:01:01Z,"Keyphrase: ""Limited world knowledge integration""","""demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task."" Keyphrase: ""Limited world knowledge integration"""
arxiv2024,DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures,Yes.,4,"""Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web",2024,2024-02-23T20:10:18Z,"Keyphrase: ""Limited web-based representation""","""Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web Keyphrase: ""Limited web-based representation"""
arxiv2024,CI w/o TN: Context Injection without Task Name for Procedure Planning,Yes.,5,"""we propose a much weaker setting without task name as supervision, which is not currently solvable by existing large language models since they require good prompts with sufficient information.""",2024,2024-02-23T19:34:47Z,"Keyphrase: ""Dependency on task-specific supervision""","""we propose a much weaker setting without task name as supervision, which is not currently solvable by existing large language models since they require good prompts with sufficient information."" Keyphrase: ""Dependency on task-specific supervision"""
arxiv2024,The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG),Yes.,4,"""Whereas extensive research has demonstrated the privacy risks of large language models (LLMs),"" and ""posing new privacy issues that are currently under-explored.""",2024,2024-02-23T18:35:15Z,"Keyphrase: ""Privacy risks""","""Whereas extensive research has demonstrated the privacy risks of large language models (LLMs),"" and ""posing new privacy issues that are currently under-explored."" Keyphrase: ""Privacy risks"""
arxiv2024,Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models,Yes.,4,"""The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability."" and ""we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently",2024,2024-02-23T18:15:56Z,"Keyphrase: ""Discrimination and bias amplification""","""The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability."" and ""we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently Keyphrase: ""Discrimination and bias amplification"""
arxiv2024,Explorations of Self-Repair in Language Models,Yes.,4,"""We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect).""",2024,2024-02-23T15:42:12Z,"Keyphrase: ""Inconsistent self-repair mechanisms""","""We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect)."" Keyphrase: ""Inconsistent self-repair mechanisms"""
arxiv2024,How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries,Yes.,5,"""Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation.""",2024,2024-02-23T13:03:12Z,"Keyphrase: ""Vulnerability to unethical content generation""","""Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation."" Keyphrase: ""Vulnerability to unethical content generation"""
arxiv2024,Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models,Yes.,4,"""these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility.""",2024,2024-02-23T09:04:48Z,"Keyphrase: ""Production of toxic content""","""these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility."" Keyphrase: ""Production of toxic content"""
arxiv2024,A First Look at GPT Apps: Landscape and Vulnerability,Yes.,4,"""LLMs' susceptibility to attacks raises concerns over safety and plagiarism.""",2024,2024-02-23T05:30:32Z,"Keyphrase: ""Susceptibility to attacks""","""LLMs' susceptibility to attacks raises concerns over safety and plagiarism."" Keyphrase: ""Susceptibility to attacks"""
arxiv2024,Studying LLM Performance on Closed- and Open-source Data,Yes.,5,"""These models are extremely data-hungry,"" ""do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds?"" and ""We find that performance for C# changes little from",2024,2024-02-23T05:17:28Z,"Keyphrase: ""High data dependency""","""These models are extremely data-hungry,"" ""do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds?"" and ""We find that performance for C# changes little from Keyphrase: ""High data dependency"""
arxiv2024,AttributionBench: How Hard is Automatic Attribution Evaluation?,Yes.,5,"""our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model's inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do.""",2024,2024-02-23T04:23:33Z,"Keyphrase: ""Limited nuanced information processing""","""our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model's inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do."" Keyphrase: ""Limited nuanced information processing"""
arxiv2024,Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions,Yes.,4,"""they often display a considerable level of overconfidence even when the question does not have a definitive answer"" and ""avoid providing hallucinated answers to these unknown questions.""",2024,2024-02-23T02:24:36Z,"Keyphrase: ""Overconfidence and Hallucination""","""they often display a considerable level of overconfidence even when the question does not have a definitive answer"" and ""avoid providing hallucinated answers to these unknown questions."" Keyphrase: ""Overconfidence and Hallucination"""
arxiv2024,Fine-tuning Large Language Models for Domain-specific Machine Translation,Yes.,5,"""Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain",2024,2024-02-23T02:24:15Z,"Keyphrase: ""Incontext learning challenges and domain finetuning""","""Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain Keyphrase: ""Incontext learning challenges and domain finetuning"""
arxiv2024,ToMBench: Benchmarking Theory of Mind in Large Language Models,Yes.,5,"""We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet.""",2024,2024-02-23T02:05:46Z,"Keyphrase: ""Lag behind human performance""","""We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet."" Keyphrase: ""Lag behind human performance"""
arxiv2024,KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models,Yes.,5,"""Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness."" and ""We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.""",2024,2024-02-23T01:30:39Z,"Keyphrase: ""Data contamination hindrance""","""Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness."" and ""We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning."" Keyphrase: ""Data contamination hindrance"""
arxiv2024,Unintended Impacts of LLM Alignment on Global Representation,Yes.,4,"""We explore how alignment impacts performance along three axes of global representation",2024,2024-02-22T23:31:22Z,"Keyphrase: ""Limited global representation""","""We explore how alignment impacts performance along three axes of global representation Keyphrase: ""Limited global representation"""
arxiv2024,Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning,Yes.,4,"""recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback.""",2024,2024-02-22T20:57:17Z,"Keyphrase: ""Limited knowledge retention""","""recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback."" Keyphrase: ""Limited knowledge retention"""
arxiv2024,RelayAttention for Efficient Large Language Model Serving with Long System Prompts,Yes.,5,"""However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length.""",2024,2024-02-22T18:58:28Z,"Keyphrase: ""Throughput and latency bottleneck""","""However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length."" Keyphrase: ""Throughput and latency bottleneck"""
arxiv2024,Identifying Multiple Personalities in Large Language Models with External Evaluation,Yes.,4,"""Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs."" and ""This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans.""",2024,2024-02-22T18:57:20Z,"Keyphrase: ""Inconsistent personality representation""","""Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs."" and ""This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans."" Keyphrase: ""Inconsistent personality representation"""
arxiv2024,MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues,Yes.,4,"""comprehensively evaluating the dialogue abilities of LLMs remains a challenge"" and ""neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs.""",2024,2024-02-22T18:21:59Z,"Keyphrase: ""Challenges in dialogue evaluation""","""comprehensively evaluating the dialogue abilities of LLMs remains a challenge"" and ""neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs."" Keyphrase: ""Challenges in dialogue evaluation"""
arxiv2024,Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images,Yes.,5,"""they are still vulnerable to adversarial images"" and ""lack of study regarding MLLMs' adversarial robustness with CoT"" and ""finding that CoT marginally improves adversarial robustness against existing attack methods"" and ""introduce a novel",2024,2024-02-22T17:36:34Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""they are still vulnerable to adversarial images"" and ""lack of study regarding MLLMs' adversarial robustness with CoT"" and ""finding that CoT marginally improves adversarial robustness against existing attack methods"" and ""introduce a novel Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,Chain-of-Thought Unfaithfulness as Disguised Accuracy,Yes.,5,"""We discover that simply changing the order of answer choices in the prompt can reduce the metric by 73 percentage points. The faithfulness metric is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about its validity as a construct for evaluating faithfulness.""",2024,2024-02-22T17:23:53Z,"Keyphrase: ""Sensitivity to prompt manipulation""","""We discover that simply changing the order of answer choices in the prompt can reduce the metric by 73 percentage points. The faithfulness metric is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about its validity as a construct for evaluating faithfulness."" Keyphrase: ""Sensitivity to prompt manipulation"""
arxiv2024,UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models,Yes.,5,"""Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or hallucination."" and ""Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources",2024,2024-02-22T16:45:32Z,"Keyphrase: ""Factual inaccuracy and hallucination""","""Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or hallucination."" and ""Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources Keyphrase: ""Factual inaccuracy and hallucination"""
arxiv2024,Visual Hallucinations of Multi-modal Large Language Models,Yes.,5,"""Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances."" and ""We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 halluc",2024,2024-02-22T16:40:33Z,"Keyphrase: ""Limited diversity in training data""","""Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances."" and ""We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 halluc Keyphrase: ""Limited diversity in training data"""
arxiv2024,ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models,Yes.,5,"""we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones.""",2024,2024-02-22T16:06:49Z,"Keyphrase: ""Performance variation across concepts""","""we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones."" Keyphrase: ""Performance variation across concepts"""
arxiv2024,"""My Answer is C"": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",Yes.,5,"""Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%.""",2024,2024-02-22T12:47:33Z,"Keyphrase: ""Dimension misalignment""","""Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%."" Keyphrase: ""Dimension misalignment"""
arxiv2024,Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis,Yes.,5,"""Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance",2024,2024-02-22T12:19:04Z,"Keyphrase: ""Limited performance without extensive training data""","""Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance Keyphrase: ""Limited performance without extensive training data"""
arxiv2024,COBIAS: Contextual Reliability in Bias Assessment,Yes.,4,"""Large Language Models (LLMs) are trained on inherently biased data"" and ""highlighting a critical need for contextual exploration.""",2024,2024-02-22T10:46:11Z,"Keyphrase: ""Inherent bias from training data""","""Large Language Models (LLMs) are trained on inherently biased data"" and ""highlighting a critical need for contextual exploration."" Keyphrase: ""Inherent bias from training data"""
arxiv2024,Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning,Yes.,5,"""However, the existing LLM-based model exhibits three shortcomings",2024,2024-02-22T08:51:39Z,"Keyphrase: ""Multiple shortcomings""","""However, the existing LLM-based model exhibits three shortcomings Keyphrase: ""Multiple shortcomings"""
arxiv2024,Understanding and Patching Compositional Reasoning in LLMs,Yes.,5,"""LLMs have marked a revolutionary shift, yet they falter when faced with compositional reasoning tasks.""",2024,2024-02-22T06:47:56Z,"Keyphrase: ""Falter in compositional reasoning""","""LLMs have marked a revolutionary shift, yet they falter when faced with compositional reasoning tasks."" Keyphrase: ""Falter in compositional reasoning"""
arxiv2024,Mitigating Biases of Large Language Models in Stance Detection with Calibration,Yes.,4,"""LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance.""",2024,2024-02-22T05:17:49Z,"Keyphrase: ""Biased stance due to sentiment-topic correlation""","""LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance."" Keyphrase: ""Biased stance due to sentiment-topic correlation"""
arxiv2024,Can Language Models Act as Knowledge Bases at Scale?,Yes.,5,"""However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable.""",2024,2024-02-22T04:20:14Z,"Keyphrase: ""Limited factual coverage""","""However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable."" Keyphrase: ""Limited factual coverage"""
arxiv2024,Qsnail: A Questionnaire Dataset for Sequential Question Generation,Yes.,5,"""Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires.""",2024,2024-02-22T04:14:10Z,"Keyphrase: ""Lack of diversity and specificity""","""Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires."" Keyphrase: ""Lack of diversity and specificity"""
arxiv2024,Eagle: Ethical Dataset Given from Real Interactions,Yes.,5,"""Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content.""",2024,2024-02-22T03:46:02Z,"Keyphrase: ""Ethical and social bias""","""Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content."" Keyphrase: ""Ethical and social bias"""
arxiv2024,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Yes.,5,"""state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome.""",2024,2024-02-22T01:02:37Z,"Keyphrase: ""Limited conversational prediction""","""state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome."" Keyphrase: ""Limited conversational prediction"""
arxiv2024,Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models,Yes.,5,"""existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.""",2024,2024-02-22T00:41:23Z,"Keyphrase: ""Challenges in integrating structured data""","""existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial."" Keyphrase: ""Challenges in integrating structured data"""
arxiv2024,Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization,Yes.,5,"""Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability."" and ""Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general",2024,2024-02-22T00:01:02Z,"Keyphrase: ""Lack of explainability""","""Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability."" and ""Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general Keyphrase: ""Lack of explainability"""
arxiv2024,MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms,Yes.,4,"""MLLMs have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation."" and ""Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks.""",2024,2024-02-21T22:27:40Z,"Keyphrase: ""Difficulty handling human emotion and complex content""","""MLLMs have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation."" and ""Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks."" Keyphrase: ""Difficulty handling human emotion and complex content"""
arxiv2024,Coercing LLMs to do and reveal (almost) anything,Yes.,5,"""adversarial attacks on large language models (LLMs) can 'jailbreak' the model into making harmful statements"" and ""we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.""",2024,2024-02-21T18:59:13Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""adversarial attacks on large language models (LLMs) can 'jailbreak' the model into making harmful statements"" and ""we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction."" Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment,Yes.,5,"""no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs,"" ""both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks,"" and ""Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before",2024,2024-02-21T18:55:20Z,"Keyphrase: ""Vulnerability to manipulation""","""no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs,"" ""both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks,"" and ""Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before Keyphrase: ""Vulnerability to manipulation"""
arxiv2024,OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,Yes.,5,"""Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies.""",2024,2024-02-21T18:49:26Z,"Keyphrase: ""Hallucination and knowledge omission""","""Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies."" Keyphrase: ""Hallucination and knowledge omission"""
arxiv2024,Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models,Yes.,5,"""Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.""",2024,2024-02-21T18:48:38Z,"Keyphrase: ""Inconsistent text watermarking""","""Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages."" Keyphrase: ""Inconsistent text watermarking"""
arxiv2024,What's in a Name? Auditing Large Language Models for Race and Gender Bias,Yes.,5,"""We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4."" and ""We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women."" and ""Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for harm against marginalized communities.""",2024,2024-02-21T18:25:25Z,"Keyphrase: ""Bias in LLMs""","""We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4."" and ""We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women."" and ""Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for harm against marginalized communities."" Keyphrase: ""Bias in LLMs"""
arxiv2024,Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content,Yes.,5,"""The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs.""",2024,2024-02-21T16:46:36Z,"Keyphrase: ""Generating toxic content""","""The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs."" Keyphrase: ""Generating toxic content"""
arxiv2024,SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization,Yes.,4,"""Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences.""",2024,2024-02-21T16:33:22Z,"Keyphrase: ""Factual inaccuracy""","""Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences."" Keyphrase: ""Factual inaccuracy"""
arxiv2024,Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models,Yes.,5,"""This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations."" and ""We illustrate that these probability-based approaches do not effectively correspond with generative predictions.""",2024,2024-02-21T15:58:37Z,"Keyphrase: ""Limitation in probability-based evaluation""","""This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations."" and ""We illustrate that these probability-based approaches do not effectively correspond with generative predictions."" Keyphrase: ""Limitation in probability-based evaluation"""
arxiv2024,Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs,Yes.,5,"""Large Language Models (LLMs)... are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs."" and ""existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds.""",2024,2024-02-21T15:13:50Z,"Keyphrase: ""Vulnerability to jailbreak attacks""","""Large Language Models (LLMs)... are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs."" and ""existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds."" Keyphrase: ""Vulnerability to jailbreak attacks"""
arxiv2024,Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language,Yes.,4,"""Our findings reveal that while these models demonstrate potential, achieving a 70% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups.""",2024,2024-02-21T13:57:36Z,"Keyphrase: ""Bias in hate speech classification""","""Our findings reveal that while these models demonstrate potential, achieving a 70% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups."" Keyphrase: ""Bias in hate speech classification"""
arxiv2024,Factual Consistency Evaluation of Summarisation in the Era of Large Language Models,Yes.,4,"""Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind."" and ""Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.""",2024,2024-02-21T12:35:19Z,"Keyphrase: ""Inability to capture factual inconsistency""","""Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind."" and ""Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation."" Keyphrase: ""Inability to capture factual inconsistency"""
arxiv2024,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,Yes.,5,"""due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens.""",2024,2024-02-21T12:30:33Z,"Keyphrase: ""Limited context window""","""due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens."" Keyphrase: ""Limited context window"""
arxiv2024,$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens,Yes.,5,"""The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context.""",2024,2024-02-21T11:30:29Z,"Keyphrase: ""Limitation in processing long contexts""","""The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context."" Keyphrase: ""Limitation in processing long contexts"""
arxiv2024,SaGE: Evaluating Moral Consistency in Large Language Models,Yes.,5,"""we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general)."" and ""Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.""",2024,2024-02-21T11:23:21Z,"Keyphrase: ""Morally inconsistent generation""","""we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general)."" and ""Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further."" Keyphrase: ""Morally inconsistent generation"""
arxiv2024,CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models,Yes.,5,"""Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images.""",2024,2024-02-21T08:21:12Z,"Keyphrase: ""Struggle with contextual information""","""Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images."" Keyphrase: ""Struggle with contextual information"""
arxiv2024,A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models,Yes.,5,"""The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability.""",2024,2024-02-21T08:20:06Z,"Keyphrase: ""Overconfidence in predictions""","""The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability."" Keyphrase: ""Overconfidence in predictions"""
arxiv2024,Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues,Yes.,5,"""Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous.""",2024,2024-02-21T06:11:03Z,"Keyphrase: ""Poor correlation with human judgment""","""Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous."" Keyphrase: ""Poor correlation with human judgment"""
arxiv2024,LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs,Yes.,5,"""this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions.""",2024,2024-02-21T05:56:52Z,"Keyphrase: ""High computational cost and visual token aggregation challenges""","""this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions."" Keyphrase: ""High computational cost and visual token aggregation challenges"""
arxiv2024,FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing,Yes.,5,"""Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and",2024,2024-02-21T05:03:17Z,"Keyphrase: ""High computational demands""","""Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and Keyphrase: ""High computational demands"""
arxiv2024,RITFIS: Robust input testing framework for LLMs-based intelligent software,Yes.,4,"""existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models.""",2024,2024-02-21T04:00:54Z,"Keyphrase: ""Challenges with lengthy and complex text""","""existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models."" Keyphrase: ""Challenges with lengthy and complex text"""
arxiv2024,Round Trip Translation Defence against Large Language Model Jailbreaking Attacks,Yes.,4,"""Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most.""",2024,2024-02-21T03:59:52Z,"Keyphrase: ""Susceptible to social engineering attacks""","""Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most."" Keyphrase: ""Susceptible to social engineering attacks"""
arxiv2024,From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers,Yes.,5,"""This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text.""",2024,2024-02-21T03:51:34Z,"Keyphrase: ""Repetitive text generation""","""This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text."" Keyphrase: ""Repetitive text generation"""
arxiv2024,Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models,Yes.,5,"""While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization."" and ""our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity.""",2024,2024-02-21T03:05:50Z,"Keyphrase: ""Limited factual accuracy in responses""","""While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization."" and ""our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity."" Keyphrase: ""Limited factual accuracy in responses"""
arxiv2024,Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks,Yes.,4,"""Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available.""",2024,2024-02-21T02:45:46Z,"Keyphrase: ""Performance degeneration in low-resource settings""","""Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available."" Keyphrase: ""Performance degeneration in low-resource settings"""
arxiv2024,RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models,Yes.,5,"""We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses.""",2024,2024-02-21T01:39:56Z,"Keyphrase: ""Stubbornness and forgetfulness""","""We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses."" Keyphrase: ""Stubbornness and forgetfulness"""
arxiv2024,Potential and Challenges of Model Editing for Social Debiasing,Yes.,5,"""Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases."" and ""Our findings in three scenarios reveal both the potential and challenges of debias editing",2024,2024-02-21T01:35:26Z,"Keyphrase: ""Inevitable stereotype bias""","""Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases."" and ""Our findings in three scenarios reveal both the potential and challenges of debias editing Keyphrase: ""Inevitable stereotype bias"""
arxiv2024,Learning to Poison Large Language Models During Instruction Tuning,Yes.,5,"""Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes.""",2024,2024-02-21T01:30:03Z,"Keyphrase: ""Vulnerability to data poisoning attacks""","""Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes."" Keyphrase: ""Vulnerability to data poisoning attacks"""
arxiv2024,LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study,Yes.,5,"""the phenomenon of 'jailbreaking', where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge.""",2024,2024-02-21T01:26:39Z,"Keyphrase: ""Harmful responses""","""the phenomenon of 'jailbreaking', where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge."" Keyphrase: ""Harmful responses"""
arxiv2024,CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory,Yes.,5,"""Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs.""",2024,2024-02-21T01:00:17Z,"Keyphrase: ""Struggles with long input sequences""","""Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs."" Keyphrase: ""Struggles with long input sequences"""
arxiv2024,Large Language Models for Data Annotation: A Survey,Yes.,4,"""Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation.""",2024,2024-02-21T00:44:04Z,"Keyphrase: ""Challenges with LLM data annotation""","""Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation."" Keyphrase: ""Challenges with LLM data annotation"""
arxiv2024,The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative,Yes.,5,"""This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs"" and ""Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications.""",2024,2024-02-20T23:08:21Z,"Keyphrase: ""Security risks and covert manipulation""","""This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs"" and ""Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications."" Keyphrase: ""Security risks and covert manipulation"""
arxiv2024,Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text,Yes.,5,"""Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors.""",2024,2024-02-20T22:56:23Z,"Keyphrase: ""Difficulty with complex multistep reasoning""","""Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors."" Keyphrase: ""Difficulty with complex multistep reasoning"""
arxiv2024,EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries,Yes.,5,"""While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording."" and ""This highlights ongoing model limitations and the value of dynamic datasets in uncovering them.""",2024,2024-02-20T20:53:24Z,"Keyphrase: ""Struggles with commonsense reasoning""","""While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording."" and ""This highlights ongoing model limitations and the value of dynamic datasets in uncovering them."" Keyphrase: ""Struggles with commonsense reasoning"""
arxiv2024,A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction,Yes.,5,"""their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE).""",2024,2024-02-20T20:42:02Z,"Keyphrase: ""Inconsistent performance in structured text formats""","""their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE)."" Keyphrase: ""Inconsistent performance in structured text formats"""
arxiv2024,TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization,Yes.,5,"""Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size."" and ""when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics.""",2024,2024-02-20T18:58:49Z,"Keyphrase: ""Factual errors and hallucinations""","""Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size."" and ""when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics."" Keyphrase: ""Factual errors and hallucinations"""
arxiv2024,Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive,Yes.,4,"""first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples,"" and ""we then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which",2024,2024-02-20T18:42:34Z,"Keyphrase: ""Text reduction and likelihood preference""","""first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples,"" and ""we then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which Keyphrase: ""Text reduction and likelihood preference"""
arxiv2024,How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts,Yes.,5,"""The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions.""",2024,2024-02-20T18:31:27Z,"Keyphrase: ""Deceptive hallucinated responses""","""The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions."" Keyphrase: ""Deceptive hallucinated responses"""
arxiv2024,Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation,Yes.,5,"""despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support"" and ""revealing challenges in selecting the correct strategy and a notable preference for a specific strategy"" and ""LLMs alone cannot become good emotional supporters.""",2024,2024-02-20T18:21:32Z,"Keyphrase: ""Limited emotional support capabilities""","""despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support"" and ""revealing challenges in selecting the correct strategy and a notable preference for a specific strategy"" and ""LLMs alone cannot become good emotional supporters."" Keyphrase: ""Limited emotional support capabilities"""
arxiv2024,Bayesian Reward Models for LLM Alignment,Yes.,4,"""this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data.""",2024,2024-02-20T18:20:59Z,"Keyphrase: ""Vulnerability to reward hacking""","""this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data."" Keyphrase: ""Vulnerability to reward hacking"""
arxiv2024,Benchmarking Retrieval-Augmented Generation for Medicine,Yes.,5,"""While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge.""",2024,2024-02-20T17:44:06Z,"Keyphrase: ""Hallucination and outdated knowledge""","""While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge."" Keyphrase: ""Hallucination and outdated knowledge"""
arxiv2024,Is the System Message Really Important to Jailbreaks in Large Language Models?,Yes.,4,"""This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions."" and ""we explore the transferability of jailbreak across LLMs.""",2024,2024-02-20T17:39:40Z,"Keyphrase: ""Vulnerability to malicious prompts""","""This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions."" and ""we explore the transferability of jailbreak across LLMs."" Keyphrase: ""Vulnerability to malicious prompts"""
arxiv2024,CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models,Yes.,5,"""their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories."" and ""highlighting the limitations of LLMs in less familiar language and task contexts",2024,2024-02-20T16:02:12Z,"Keyphrase: ""Limited effectiveness in low-resource languages""","""their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories."" and ""highlighting the limitations of LLMs in less familiar language and task contexts Keyphrase: ""Limited effectiveness in low-resource languages"""
arxiv2024,ELAD: Explanation-Guided Large Language Models Active Distillation,Yes.,5,"""The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences.""",2024,2024-02-20T15:47:59Z,"Keyphrase: ""Memory inefficiency and high computational costs""","""The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences."" Keyphrase: ""Memory inefficiency and high computational costs"""
arxiv2024,Understanding the effects of language-specific class imbalance in multilingual fine-tuning,Yes.,4,"""We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features.""",2024,2024-02-20T13:59:12Z,"Keyphrase: ""Dataset imbalance affecting performance""","""We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features."" Keyphrase: ""Dataset imbalance affecting performance"""
arxiv2024,Large Language Model-based Human-Agent Collaboration for Complex Task Solving,Yes.,5,"""LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs.""",2024,2024-02-20T11:03:36Z,"Keyphrase: ""Difficulty in adapting to dynamic environments""","""LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs."" Keyphrase: ""Difficulty in adapting to dynamic environments"""
arxiv2024,Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting,Yes.,5,"""few-shot learning using Large language models is not production ready for named entity recognition in the clinical domain.""",2024,2024-02-20T08:20:49Z,"Keyphrase: ""Limited few-shot learning capability""","""few-shot learning using Large language models is not production ready for named entity recognition in the clinical domain."" Keyphrase: ""Limited few-shot learning capability"""
arxiv2024,An LLM Maturity Model for Reliable and Transparent Text-to-Query,Yes.,4,"""Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications.""",2024,2024-02-20T06:20:09Z,"Keyphrase: ""Reliability and transparency issues""","""Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications."" Keyphrase: ""Reliability and transparency issues"""
arxiv2024,Are Large Language Models Rational Investors?,Yes.,4,"""However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight.""",2024,2024-02-20T04:26:08Z,"Keyphrase: ""Intrinsic bias in financial domain""","""However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight."" Keyphrase: ""Intrinsic bias in financial domain"""
arxiv2024,Thermometer: Towards Universal Calibration for Large Language Models,Yes.,5,"""Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs"" and ""calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks.""",2024,2024-02-20T04:13:48Z,"Keyphrase: ""Poor calibration and computational requirements""","""Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs"" and ""calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks."" Keyphrase: ""Poor calibration and computational requirements"""
arxiv2024,The FinBen: An Holistic Financial Benchmark for Large Language Models,Yes.,4,"""The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities.""",2024,2024-02-20T02:16:16Z,"Keyphrase: ""Struggles with complex reasoning and forecasting""","""The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities."" Keyphrase: ""Struggles with complex reasoning and forecasting"""
arxiv2024,Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation,Yes.,4,"""Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs,"" and ""We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.""",2024,2024-02-20T01:49:15Z,"Keyphrase: ""Insufficient bias mitigation""","""Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs,"" and ""We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms."" Keyphrase: ""Insufficient bias mitigation"""
arxiv2024,GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence,Yes.,5,"""LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance).""",2024,2024-02-19T21:45:55Z,"Keyphrase: ""Dangerous errors in high-stakes applications""","""LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance)."" Keyphrase: ""Dangerous errors in high-stakes applications"""
arxiv2024,TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness,Yes.,4,"""However, concerns have arisen regarding the trustworthiness of LLMs outputs, particularly in closed-book question-answering tasks, where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information.""",2024,2024-02-19T21:12:14Z,"Keyphrase: ""Trustworthiness of output""","""However, concerns have arisen regarding the trustworthiness of LLMs outputs, particularly in closed-book question-answering tasks, where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information."" Keyphrase: ""Trustworthiness of output"""
arxiv2024,AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies,Yes.,5,"""Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack.""",2024,2024-02-19T18:56:44Z,"Keyphrase: ""Struggles with lengthy scenarios""","""Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack."" Keyphrase: ""Struggles with lengthy scenarios"""
arxiv2024,Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge,Yes.,4,"""LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones"" and ""Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets.""",2024,2024-02-19T18:31:11Z,"Keyphrase: ""Bias towards frequently seen data""","""LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones"" and ""Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets."" Keyphrase: ""Bias towards frequently seen data"""
arxiv2024,GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations,Yes.,5,"""We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios.""",2024,2024-02-19T18:23:36Z,"Keyphrase: ""Limited performance in gaming scenarios""","""We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios."" Keyphrase: ""Limited performance in gaming scenarios"""
arxiv2024,Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!,Yes.,5,"""this paper introduces an inference-time attack method, demonstrating that safety alignment can be easily reversed to produce harmful language models without additional training.""",2024,2024-02-19T18:16:51Z,"Keyphrase: ""Vulnerability to inference-time attacks""","""this paper introduces an inference-time attack method, demonstrating that safety alignment can be easily reversed to produce harmful language models without additional training."" Keyphrase: ""Vulnerability to inference-time attacks"""
arxiv2024,Large Language Model for Mental Health: A Systematic Review,Yes.,4,"""Findings reveal LLMs' effectiveness in mental health issue detection and the enhancement of telepsychological services through personalised healthcare. Nonetheless, risks like text inconsistencies, hallucinatory content, and the lack of an ethical framework raise concerns about their clinical use.""",2024,2024-02-19T17:58:41Z,"Keyphrase: ""Ethical concerns and hallucinatory content""","""Findings reveal LLMs' effectiveness in mental health issue detection and the enhancement of telepsychological services through personalised healthcare. Nonetheless, risks like text inconsistencies, hallucinatory content, and the lack of an ethical framework raise concerns about their clinical use."" Keyphrase: ""Ethical concerns and hallucinatory content"""
arxiv2024,NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms,Yes.,5,"""The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference"" and ""Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence.""",2024,2024-02-19T16:19:15Z,"Keyphrase: ""Temporal drift in performance""","""The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference"" and ""Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence."" Keyphrase: ""Temporal drift in performance"""
arxiv2024,Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition,Yes.,5,"""GPT-4 can generate reasonable coarse-grained citation groupings to support human users in brainstorming, but fails to perform detailed synthesis of related works without human intervention.""",2024,2024-02-19T16:14:04Z,"Keyphrase: ""Limited detailed synthesis""","""GPT-4 can generate reasonable coarse-grained citation groupings to support human users in brainstorming, but fails to perform detailed synthesis of related works without human intervention."" Keyphrase: ""Limited detailed synthesis"""
arxiv2024,"Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",Yes.,5,"""Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum.""",2024,2024-02-19T16:04:53Z,"Keyphrase: ""Degradation in reasoning performance with shorter inputs""","""Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum."" Keyphrase: ""Degradation in reasoning performance with shorter inputs"""
arxiv2024,Polarization of Autonomous Generative AI Agents Under Echo Chambers,Yes.,4,"""We investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment."" and ""we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments.""",2024,2024-02-19T15:14:15Z,"Keyphrase: ""Polarization in echo chambers""","""We investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment."" and ""we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments."" Keyphrase: ""Polarization in echo chambers"""
arxiv2024,Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion,Yes.,4,"""However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions.""",2024,2024-02-19T14:59:07Z,"Keyphrase: ""Limited multimodal understanding""","""However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions."" Keyphrase: ""Limited multimodal understanding"""
arxiv2024,A Chinese Dataset for Evaluating the Safeguards in Large Language Models,Yes.,5,"""Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed."" and ""Our experiments on five LLMs show that region-specific risks are the prevalent type of risk, presenting the major issue with all Chinese LLMs we experimented with.""",2024,2024-02-19T14:56:18Z,"Keyphrase: ""Harmful responses and unexpected risks""","""Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed."" and ""Our experiments on five LLMs show that region-specific risks are the prevalent type of risk, presenting the major issue with all Chinese LLMs we experimented with."" Keyphrase: ""Harmful responses and unexpected risks"""
arxiv2024,BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence,Yes.,4,"""However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality.""",2024,2024-02-19T14:28:31Z,"Keyphrase: ""Inconsistent knowledge retrieval""","""However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality."" Keyphrase: ""Inconsistent knowledge retrieval"""
arxiv2024,Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One,Yes.,4,"""LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases.""",2024,2024-02-19T14:02:22Z,"Keyphrase: ""Dominant viewpoint bias""","""LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases."" Keyphrase: ""Dominant viewpoint bias"""
arxiv2024,Purifying Large Language Models by Ensembling a Small Language Model,Yes.,5,"""well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs.""",2024,2024-02-19T14:00:39Z,"Keyphrase: ""Copyright infringement and privacy violations""","""well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs."" Keyphrase: ""Copyright infringement and privacy violations"""
arxiv2024,Do Large Language Models Understand Logic or Just Mimick Context?,Yes.,5,"""Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can",2024,2024-02-19T12:12:35Z,"Keyphrase: ""Limited understanding of logical rules""","""Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can Keyphrase: ""Limited understanding of logical rules"""
arxiv2024,Can LLMs Compute with Reasons?,Yes.,5,"""Large language models (LLMs) often struggle with complex mathematical tasks, prone to 'hallucinating' incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data.""",2024,2024-02-19T12:04:25Z,"Keyphrase: ""Struggles with complex mathematical tasks""","""Large language models (LLMs) often struggle with complex mathematical tasks, prone to 'hallucinating' incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data."" Keyphrase: ""Struggles with complex mathematical tasks"""
arxiv2024,EmoBench: Evaluating the Emotional Intelligence of Large Language Models,Yes.,4,"""Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research.""",2024,2024-02-19T11:48:09Z,"Keyphrase: ""Gap between LLM and human understanding""","""Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research."" Keyphrase: ""Gap between LLM and human understanding"""
arxiv2024,Are LLM-based Evaluators Confusing NLG Quality Criteria?,Yes.,5,"""we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability"" and ""Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.""",2024,2024-02-19T11:19:02Z,"Keyphrase: ""Confusion in evaluation criteria""","""we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability"" and ""Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation."" Keyphrase: ""Confusion in evaluation criteria"""
arxiv2024,Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models,Yes.,5,"""Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks.""",2024,2024-02-19T11:02:05Z,"Keyphrase: ""Catastrophic forgetting""","""Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks."" Keyphrase: ""Catastrophic forgetting"""
arxiv2024,Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs,Yes.,4,"""Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility."" and ""these methods based on logits often require both teacher and student models to share the same tokenizer",2024,2024-02-19T10:37:29Z,"Keyphrase: ""Practical constraints in deployment""","""Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility."" and ""these methods based on logits often require both teacher and student models to share the same tokenizer Keyphrase: ""Practical constraints in deployment"""
arxiv2024,Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought,Yes.,5,"""GPT-4 performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection."" and ""These experimental results indicate that GPT-4 lacks the ability to detect smart contract vulnerabilities effectively.""",2024,2024-02-19T10:33:29Z,"Keyphrase: ""Ineffective vulnerability detection""","""GPT-4 performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection."" and ""These experimental results indicate that GPT-4 lacks the ability to detect smart contract vulnerabilities effectively."" Keyphrase: ""Ineffective vulnerability detection"""
arxiv2024,Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models,Yes.,5,"""Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, TempUN, to reveal significant limitations in temporal retention and reasoning abilities.""",2024,2024-02-19T09:43:03Z,"Keyphrase: ""Limited temporal retention and reasoning""","""Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, TempUN, to reveal significant limitations in temporal retention and reasoning abilities."" Keyphrase: ""Limited temporal retention and reasoning"""
arxiv2024,MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition,Yes.,5,"""LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' performance objectively and scientifically; 2) LLMs only get a small percentage of the right reasoning chain, e.g. GPT-4 only gets 36.3",2024,2024-02-19T08:12:30Z,"Keyphrase: ""Limited reasoning capability""","""LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' performance objectively and scientifically; 2) LLMs only get a small percentage of the right reasoning chain, e.g. GPT-4 only gets 36.3 Keyphrase: ""Limited reasoning capability"""
arxiv2024,Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models,Yes.,5,"""However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored."" and ""Analysis shows that approximately 20% of the failures are attributed to shortcuts.""",2024,2024-02-19T07:34:10Z,"Keyphrase: ""Limited reasoning capabilities""","""However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored."" and ""Analysis shows that approximately 20% of the failures are attributed to shortcuts."" Keyphrase: ""Limited reasoning capabilities"""
arxiv2024,Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint,Yes.,4,"""This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the parametric knowledge internalized during pre-training.""",2024,2024-02-19T07:10:30Z,"Keyphrase: ""Conflicting knowledge sources""","""This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the parametric knowledge internalized during pre-training."" Keyphrase: ""Conflicting knowledge sources"""
arxiv2024,RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning,Yes.,4,"""Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities."" and ""The overall performance of existing LMMs is still limited; however LMMs more robust to low",2024,2024-02-19T06:57:02Z,"Keyphrase: ""Limited specialized reasoning""","""Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities."" and ""The overall performance of existing LMMs is still limited; however LMMs more robust to low Keyphrase: ""Limited specialized reasoning"""
arxiv2024,The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth,Yes.,4,"""We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice.""",2024,2024-02-19T06:54:55Z,"Keyphrase: ""Lack of personalization""","""We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice."" Keyphrase: ""Lack of personalization"""
arxiv2024,Microstructures and Accuracy of Graph Recall by Large Language Models,Yes.,5,"""We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.""",2024,2024-02-19T04:29:45Z,"Keyphrase: ""Limited performance on graph-related tasks""","""We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain."" Keyphrase: ""Limited performance on graph-related tasks"""
arxiv2024,Head-wise Shareable Attention for Large Language Models,Yes.,4,"""Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices.""",2024,2024-02-19T04:19:36Z,"Keyphrase: ""Deployment restrictions on edge devices""","""Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices."" Keyphrase: ""Deployment restrictions on edge devices"""
arxiv2024,What Evidence Do Language Models Find Convincing?,Yes.,4,"""Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone.""",2024,2024-02-19T02:15:34Z,"Keyphrase: ""Limited consideration of stylistic features""","""Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone."" Keyphrase: ""Limited consideration of stylistic features"""
arxiv2024,ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs,Yes.,4,"""Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities.""",2024,2024-02-19T01:28:48Z,"Keyphrase: ""Harmful social biases""","""Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities."" Keyphrase: ""Harmful social biases"""
arxiv2024,"Large Language Models for Stemming: Promises, Pitfalls and Failures",Yes.,5,"""We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions.""",2024,2024-02-19T01:11:44Z,"Keyphrase: ""Limited effectiveness of contextual stemming""","""We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions."" Keyphrase: ""Limited effectiveness of contextual stemming"""
arxiv2024,MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs,Yes.,5,"""However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments.""",2024,2024-02-19T01:04:22Z,"Keyphrase: ""Inaccurate and misleading output""","""However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments."" Keyphrase: ""Inaccurate and misleading output"""
arxiv2024,SPML: A DSL for Defending Language Models Against Prompt Attacks,Yes.,4,"""post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users,"" and ""Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored.""",2024,2024-02-19T00:53:48Z,"Keyphrase: ""Vulnerability to malicious attacks""","""post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users,"" and ""Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored."" Keyphrase: ""Vulnerability to malicious attacks"""
arxiv2024,ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs,Yes.,5,"""We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art.""",2024,2024-02-19T00:43:31Z,"Keyphrase: ""Struggles with visual prompts""","""We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art."" Keyphrase: ""Struggles with visual prompts"""
arxiv2024,Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic,Yes.,5,"""Aligned language models face a significant limitation as their fine-tuning often results in compromised safety.""",2024,2024-02-19T00:18:09Z,"Keyphrase: ""Safety compromises in fine-tuning""","""Aligned language models face a significant limitation as their fine-tuning often results in compromised safety."" Keyphrase: ""Safety compromises in fine-tuning"""
arxiv2024,How Susceptible are Large Language Models to Ideological Manipulation?,Yes.,5,"""Our findings reveal a concerning vulnerability",2024,2024-02-18T22:36:19Z,"Keyphrase: ""Vulnerability concerns""","""Our findings reveal a concerning vulnerability Keyphrase: ""Vulnerability concerns"""
arxiv2024,Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable,Yes.,4,"""Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases."" and ""output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability.""",2024,2024-02-18T21:10:18Z,"Keyphrase: ""Factual inconsistency and bias""","""Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases."" and ""output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability."" Keyphrase: ""Factual inconsistency and bias"""
arxiv2024,Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation,Yes.,5,"""Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.""",2024,2024-02-18T20:48:09Z,"Keyphrase: ""Limited production-ready code generation""","""Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development."" Keyphrase: ""Limited production-ready code generation"""
arxiv2024,Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers,Yes.,4,"""the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking.""",2024,2024-02-18T20:47:10Z,"Keyphrase: ""Resource-intensive storage and processing""","""the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking."" Keyphrase: ""Resource-intensive storage and processing"""
arxiv2024,Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning,Yes.,4,"""two substantial challenges persist within the existing VLM frameworks",2024,2024-02-18T19:38:44Z,"Keyphrase: ""Persistent challenges in VLM framework""","""two substantial challenges persist within the existing VLM frameworks Keyphrase: ""Persistent challenges in VLM framework"""
arxiv2024,Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents,Yes.,5,"""LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents."" and ""the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning.""",2024,2024-02-18T17:10:07Z,"Keyphrase: ""Inefficient data utilization""","""LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents."" and ""the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning."" Keyphrase: ""Inefficient data utilization"""
arxiv2024,Stealthy Attack on Large Language Model based Recommendation,Yes.,5,"""we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items.""",2024,2024-02-18T16:51:02Z,"Keyphrase: ""Security vulnerability due to textual emphasis""","""we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items."" Keyphrase: ""Security vulnerability due to textual emphasis"""
arxiv2024,Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark,Yes.,5,"""the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge.""",2024,2024-02-18T14:08:48Z,"Keyphrase: ""Memory overhead challenge""","""the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge."" Keyphrase: ""Memory overhead challenge"""
arxiv2024,LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration,Yes.,5,"""LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the",2024,2024-02-18T11:46:52Z,"Keyphrase: ""High computational cost""","""LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the Keyphrase: ""High computational cost"""
arxiv2024,KMMLU: Measuring Massive Multitask Language Understanding in Korean,Yes.,4,"""identifying significant room for improvement,"" ""Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse,"" and ""even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean",2024,2024-02-18T11:41:07Z,"Keyphrase: ""Performance gap in non-English languages""","""identifying significant room for improvement,"" ""Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse,"" and ""even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean Keyphrase: ""Performance gap in non-English languages"""
arxiv2024,From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings,Yes.,4,"""Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias.""",2024,2024-02-18T08:53:41Z,"Keyphrase: ""Learned biases""","""Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias."" Keyphrase: ""Learned biases"""
arxiv2024,What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs,Yes.,5,"""we demonstrate through experimentation that LLMs lack necessary skills required for planning.""",2024,2024-02-18T07:42:49Z,"Keyphrase: ""Lack of necessary planning skills""","""we demonstrate through experimentation that LLMs lack necessary skills required for planning."" Keyphrase: ""Lack of necessary planning skills"""
arxiv2024,MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing,Yes.,4,"""Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored."" and ""we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring",2024,2024-02-18T07:15:03Z,"Keyphrase: ""Limited fine-grained knowledge""","""Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored."" and ""we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring Keyphrase: ""Limited fine-grained knowledge"""
arxiv2024,When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation,Yes.,5,"""Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases.""",2024,2024-02-18T04:57:19Z,"Keyphrase: ""Limited knowledge possession""","""Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases."" Keyphrase: ""Limited knowledge possession"""
arxiv2024,FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence,Yes.,5,"""We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.""",2024,2024-02-18T04:45:01Z,"Keyphrase: ""Challenges in medical evidence summarization""","""We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level."" Keyphrase: ""Challenges in medical evidence summarization"""
arxiv2024,Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation,Yes.,5,"""This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations."" and ""Experimental results show a general performance decline in most LLMs against their original results.""",2024,2024-02-18T03:40:06Z,"Keyphrase: ""General performance decline""","""This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations."" and ""Experimental results show a general performance decline in most LLMs against their original results."" Keyphrase: ""General performance decline"""
arxiv2024,Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs,Yes.,5,"""their mastery of underlying inferential rules still falls short of human capabilities,"" and ""reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns,"" and ""our work sheds light on",2024,2024-02-18T03:38:51Z,"Keyphrase: ""Limited inferential rule mastery""","""their mastery of underlying inferential rules still falls short of human capabilities,"" and ""reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns,"" and ""our work sheds light on Keyphrase: ""Limited inferential rule mastery"""
arxiv2024,Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models,Yes.,5,"""We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias.""",2024,2024-02-18T03:10:39Z,"Keyphrase: ""Self-bias amplification""","""We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias."" Keyphrase: ""Self-bias amplification"""
arxiv2024,Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning,Yes.,4,"""existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments.""",2024,2024-02-18T03:04:38Z,"Keyphrase: ""Limited video comprehension""","""existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments."" Keyphrase: ""Limited video comprehension"""
arxiv2024,EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models,Yes.,4,"""EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types.""",2024,2024-02-18T02:41:06Z,"Keyphrase: ""Hallucination with event structure""","""EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types."" Keyphrase: ""Hallucination with event structure"""
arxiv2024,Aligning Modalities in Vision Large Language Models via Preference Fine-tuning,Yes.,4,"""This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations.""",2024,2024-02-18T00:56:16Z,"Keyphrase: ""Limited factual representation""","""This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations."" Keyphrase: ""Limited factual representation"""
arxiv2024,Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection,Yes.,5,"""LLMs exhibit two extremes",2024,2024-02-18T00:04:40Z,"**Given Evidence:**
Evidence: ""llm exhibit two extreme""

**Keyphrase:**
""Extreme behavior""","""LLMs exhibit two extremes **Given Evidence:**
Evidence: ""llm exhibit two extreme""

**Keyphrase:**
""Extreme behavior"""
arxiv2024,Tasks That Language Models Don't Learn,Yes.,5,"""We argue that there are certain properties of language that our current large language models (LLMs) don't learn."" and ""highlighting the limitations of knowledge acquired in the absence of sensory experience.""",2024,2024-02-17T17:52:24Z,"Keyphrase: ""Limited knowledge acquisition""","""We argue that there are certain properties of language that our current large language models (LLMs) don't learn."" and ""highlighting the limitations of knowledge acquired in the absence of sensory experience."" Keyphrase: ""Limited knowledge acquisition"""
arxiv2024,Puzzle Solving using Reasoning of Large Language Models: A Survey,Yes.,4,"""identifying significant challenges in complex puzzle scenarios"" and ""highlight the disparity between LLM capabilities and human-like reasoning.""",2024,2024-02-17T14:19:38Z,"Keyphrase: ""Limited humanlike reasoning""","""identifying significant challenges in complex puzzle scenarios"" and ""highlight the disparity between LLM capabilities and human-like reasoning."" Keyphrase: ""Limited humanlike reasoning"""
arxiv2024,Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,Yes.,4,"""the safety issues of LLM-based agents are currently under-explored"" and ""LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents.""",2024,2024-02-17T06:48:45Z,"Keyphrase: ""Vulnerability to backdoor attacks""","""the safety issues of LLM-based agents are currently under-explored"" and ""LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents."" Keyphrase: ""Vulnerability to backdoor attacks"""
arxiv2024,Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs,Yes.,5,"""However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT."" and ""we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning",2024,2024-02-17T05:22:56Z,"Keyphrase: ""Lack of reasoning ability""","""However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT."" and ""we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning Keyphrase: ""Lack of reasoning ability"""
arxiv2024,Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering,Yes.,4,"""Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain."" and ""The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables.""",2024,2024-02-17T05:10:18Z,"Keyphrase: ""Limited handling of complex mathematical scenarios""","""Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain."" and ""The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables."" Keyphrase: ""Limited handling of complex mathematical scenarios"""
arxiv2024,Disclosure and Mitigation of Gender Bias in LLMs,Yes.,5,"""Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs.""",2024,2024-02-17T04:48:55Z,"Keyphrase: ""Gender bias and stereotypes""","""Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs."" Keyphrase: ""Gender bias and stereotypes"""
arxiv2024,KnowTuning: Knowledge-aware Fine-tuning for Large Language Models,Yes.,5,"""large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers.""",2024,2024-02-17T02:54:32Z,"Keyphrase: ""Struggle with knowledge-intensive tasks""","""large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers."" Keyphrase: ""Struggle with knowledge-intensive tasks"""
arxiv2024,GenDec: A robust generative Question-decomposition method for Multi-hop reasoning,Yes.,4,"""Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer.""",2024,2024-02-17T02:21:44Z,"Keyphrase: ""Limited multihop reasoning""","""Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer."" Keyphrase: ""Limited multihop reasoning"""
arxiv2024,Contrastive Instruction Tuning,Yes.,5,"""current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues.""",2024,2024-02-17T00:09:32Z,"Keyphrase: ""Limited robustness to textual variation""","""current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues."" Keyphrase: ""Limited robustness to textual variation"""
arxiv2024,Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models,Yes.,5,"""Our analysis reveals that while humans disagree on which situations require empathy toward the underprivileged, most large language models are unable to empathize with the socioeconomically underprivileged regardless of the situation.""",2024,2024-02-16T23:18:19Z,"Keyphrase: ""Lack of empathy for underprivileged""","""Our analysis reveals that while humans disagree on which situations require empathy toward the underprivileged, most large language models are unable to empathize with the socioeconomically underprivileged regardless of the situation."" Keyphrase: ""Lack of empathy for underprivileged"""
arxiv2024,Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models,Yes.,5,"""Regrettably, previous studies on ME evaluation have two critical limitations",2024,2024-02-16T23:08:55Z,"Keyphrase: ""Limited evaluation and critical limitations""","""Regrettably, previous studies on ME evaluation have two critical limitations Keyphrase: ""Limited evaluation and critical limitations"""
arxiv2024,When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models,Yes.,4,"""we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp.""",2024,2024-02-16T22:12:53Z,"Keyphrase: ""Limited reasoning ability""","""we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp."" Keyphrase: ""Limited reasoning ability"""
arxiv2024,Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives,Yes.,5,"""Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives.""",2024,2024-02-16T19:59:45Z,"Keyphrase: ""Difficulty in inferencing complex relationships""","""Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives."" Keyphrase: ""Difficulty in inferencing complex relationships"""
arxiv2024,PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering,Yes.,5,"""large language models (LLMs) may have outdated knowledge,"" and ""The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities.""",2024,2024-02-16T19:26:09Z,"Keyphrase: ""Outdated knowledge""","""large language models (LLMs) may have outdated knowledge,"" and ""The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities."" Keyphrase: ""Outdated knowledge"""
arxiv2024,RLVF: Learning from Verbal Feedback without Overgeneralization,Yes.,5,"""we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant.""",2024,2024-02-16T18:50:24Z,"Keyphrase: ""Overgeneralization of feedback""","""we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant."" Keyphrase: ""Overgeneralization of feedback"""
arxiv2024,When is Tree Search Useful for LLM Planning? It Depends on the Discriminator,Yes.,5,"""current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements"" and ""tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications.""",2024,2024-02-16T18:45:58Z,"Keyphrase: ""Slow advanced planning hindering real-world application""","""current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements"" and ""tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications."" Keyphrase: ""Slow advanced planning hindering real-world application"""
arxiv2024,Multi-modal preference alignment remedies regression of visual instruction tuning on language model,Yes.,4,"""the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with.""",2024,2024-02-16T18:42:08Z,Keyphrase: Lack of diversity and complexity in training datasets,"""the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with."" Keyphrase: Lack of diversity and complexity in training datasets"
arxiv2024,Exploring Value Biases: How LLMs Deviate Towards the Ideal,Yes.,4,"""Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications."" and ""We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars.""",2024,2024-02-16T18:28:43Z,"Keyphrase: ""Unintended biases""","""Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications."" and ""We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars."" Keyphrase: ""Unintended biases"""
arxiv2024,Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities,Yes.,5,"""For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity."" and ""Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions.""",2024,2024-02-16T17:15:28Z,"Keyphrase: ""Challenges with time series forecasting""","""For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity."" and ""Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions."" Keyphrase: ""Challenges with time series forecasting"""
arxiv2024,RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model,Yes.,5,"""severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment.""",2024,2024-02-16T16:57:18Z,"Keyphrase: ""Data scarcity and domain gap""","""severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment."" Keyphrase: ""Data scarcity and domain gap"""
arxiv2024,In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss,Yes.,5,"""Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements.""",2024,2024-02-16T16:15:01Z,"Keyphrase: ""Limited evaluation benchmarks""","""Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements."" Keyphrase: ""Limited evaluation benchmarks"""
arxiv2024,ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages,Yes.,5,"""Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to.""",2024,2024-02-16T15:19:46Z,"Keyphrase: ""Enduring safety challenges""","""Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to."" Keyphrase: ""Enduring safety challenges"""
arxiv2024,GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models,Yes.,5,"""traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods,"" ""prompting LLMs with a fixed set of relations or entities can cause hallucinations,"" and ""precision/recall fails to justify the performance of GRE methods.""",2024,2024-02-16T15:01:24Z,"Keyphrase: ""Hallucination in relation extraction""","""traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods,"" ""prompting LLMs with a fixed set of relations or entities can cause hallucinations,"" and ""precision/recall fails to justify the performance of GRE methods."" Keyphrase: ""Hallucination in relation extraction"""
arxiv2024,Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification,Yes.,5,"""Our results show that ChatGPT struggles in abductive reasoning,"" and ""Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated to distinguish between hype and actual capabilities",2024,2024-02-16T14:52:05Z,"Keyphrase: ""Limited abductive reasoning""","""Our results show that ChatGPT struggles in abductive reasoning,"" and ""Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated to distinguish between hype and actual capabilities Keyphrase: ""Limited abductive reasoning"""
arxiv2024,An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference,Yes.,4,"""recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs.""",2024,2024-02-16T14:15:15Z,"Keyphrase: ""Inference efficiency deterioration""","""recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs."" Keyphrase: ""Inference efficiency deterioration"""
arxiv2024,Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability,Yes.,4,"""concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust"" and ""we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges.""",2024,2024-02-16T13:46:06Z,"Keyphrase: ""Lack of interpretability""","""concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust"" and ""we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges."" Keyphrase: ""Lack of interpretability"""
arxiv2024,LongHeads: Multi-Head Attention is Secretly a Long Context Processor,Yes.,5,"""Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands.""",2024,2024-02-16T13:39:34Z,"Keyphrase: ""Limited input length processing""","""Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands."" Keyphrase: ""Limited input length processing"""
arxiv2024,Humans or LLMs as the Judge? A Study on Judgement Biases,Yes.,5,"""Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases.""",2024,2024-02-16T13:21:06Z,"Keyphrase: ""Vulnerability to bias""","""Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases."" Keyphrase: ""Vulnerability to bias"""
arxiv2024,Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes,Yes.,4,"""current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are 'unreliable' since such processes could contain information unrelated to the answer.""",2024,2024-02-16T13:02:11Z,"Keyphrase: ""Unreliable reasoning process""","""current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are 'unreliable' since such processes could contain information unrelated to the answer."" Keyphrase: ""Unreliable reasoning process"""
arxiv2024,Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement,Yes.,5,"""existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios."" and ""we find that existing models exhibit a general deficiency in role alignment capabilities.""",2024,2024-02-16T12:12:05Z,"Keyphrase: ""Role alignment deficiency""","""existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios."" and ""we find that existing models exhibit a general deficiency in role alignment capabilities."" Keyphrase: ""Role alignment deficiency"""
arxiv2024,Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements,Yes.,4,"""existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements.""",2024,2024-02-16T12:00:34Z,"Keyphrase: ""Lack of controllability and biased content""","""existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements."" Keyphrase: ""Lack of controllability and biased content"""
arxiv2024,Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models,Yes.,5,"""Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations.""",2024,2024-02-16T11:55:40Z,"Keyphrase: ""Internal hallucination""","""Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations."" Keyphrase: ""Internal hallucination"""
arxiv2024,Jailbreaking Proprietary Large Language Models using Word Substitution Cipher,Yes.,4,"""Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process."" and ""Additionally, we discuss the over-defensiveness of these models.""",2024,2024-02-16T11:37:05Z,"Keyphrase: ""Ethical alignment susceptibility""","""Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process."" and ""Additionally, we discuss the over-defensiveness of these models."" Keyphrase: ""Ethical alignment susceptibility"""
arxiv2024,InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?,Yes.,4,"""Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions.""",2024,2024-02-16T10:54:10Z,"Keyphrase: ""Societal bias and unfair predictions""","""Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions."" Keyphrase: ""Societal bias and unfair predictions"""
arxiv2024,Properties and Challenges of LLM-Generated Explanations,Yes.,4,"""However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs,"" and ""We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.""",2024,2024-02-16T09:37:54Z,"Keyphrase: ""Reliance on specifically annotated data""","""However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs,"" and ""We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system."" Keyphrase: ""Reliance on specifically annotated data"""
arxiv2024,Zero-shot sampling of adversarial entities in biomedical question answering,Yes.,5,"""Our investigations illustrate the brittleness of domain knowledge in LLMs and reveal a shortcoming of standard evaluations for high-capacity models.""",2024,2024-02-16T09:29:38Z,"Keyphrase: ""Brittleness in domain knowledge""","""Our investigations illustrate the brittleness of domain knowledge in LLMs and reveal a shortcoming of standard evaluations for high-capacity models."" Keyphrase: ""Brittleness in domain knowledge"""
arxiv2024,Unsupervised LLM Adaptation for Question Answering,Yes.,5,"""they have difficulties in accessing information located in the middle or at the end of documents.""",2024,2024-02-16T06:29:16Z,"Keyphrase: ""Limited access to information""","""they have difficulties in accessing information located in the middle or at the end of documents."" Keyphrase: ""Limited access to information"""
arxiv2024,WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing,Yes.,5,"""This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch.""",2024,2024-02-16T05:29:59Z,"Keyphrase: ""Toxicity buildup and performance degradation""","""This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch."" Keyphrase: ""Toxicity buildup and performance degradation"""
arxiv2024,I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models,Yes.,4,"""The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group."" and ""this replication unveiled an intrinsic Democratic bias in Large Language Models (LL",2024,2024-02-16T03:54:48Z,"Keyphrase: ""Social identity bias""","""The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group."" and ""this replication unveiled an intrinsic Democratic bias in Large Language Models (LL Keyphrase: ""Social identity bias"""
arxiv2024,DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection,Yes.,5,"""Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount.""",2024,2024-02-16T03:24:56Z,"Keyphrase: ""Factuality hallucination""","""Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount."" Keyphrase: ""Factuality hallucination"""
arxiv2024,Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting,Yes.,5,"""LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs.""",2024,2024-02-16T02:32:06Z,"Keyphrase: ""Factually incorrect hallucinations""","""LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs."" Keyphrase: ""Factually incorrect hallucinations"""
arxiv2024,DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows,Yes.,4,"""However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows."" and ""The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them.""",2024,2024-02-16T00:10:26Z,"Keyphrase: ""Closed source and lack of standardized tooling""","""However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows."" and ""The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them."" Keyphrase: ""Closed source and lack of standardized tooling"""
arxiv2024,Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review,Yes.,5,"""this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required.""",2024,2024-02-15T22:43:02Z,"Keyphrase: ""Model hallucination and knowledge boundary limitations""","""this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required."" Keyphrase: ""Model hallucination and knowledge boundary limitations"""
arxiv2024,On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities,Yes.,5,"""such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences"" and ""simple adversarial attacks can significantly undermine the effectiveness of LLM/VLM-robot integrated systems.""",2024,2024-02-15T22:01:45Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences"" and ""simple adversarial attacks can significantly undermine the effectiveness of LLM/VLM-robot integrated systems."" Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,Uncertainty Quantification for In-Context Learning of Large Language Models,Yes.,5,"""trustworthy issues with LLM's response, such as hallucination"" and ""highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty).""",2024,2024-02-15T18:46:24Z,"Keyphrase: ""Uncertainty in hallucination""","""trustworthy issues with LLM's response, such as hallucination"" and ""highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty)."" Keyphrase: ""Uncertainty in hallucination"""
arxiv2024,Language Models with Conformal Factuality Guarantees,Yes.,5,"""Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem.""",2024,2024-02-15T18:31:53Z,"Keyphrase: ""Factuality guarantee challenge""","""Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem."" Keyphrase: ""Factuality guarantee challenge"""
arxiv2024,Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination,Yes.,5,"""Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data.""",2024,2024-02-15T16:21:14Z,"Keyphrase: ""Privacy violation and exposure of sensitive data""","""Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data."" Keyphrase: ""Privacy violation and exposure of sensitive data"""
arxiv2024,Case Study: Testing Model Capabilities in Some Reasoning Tasks,Yes.,5,"""However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios.""",2024,2024-02-15T14:21:30Z,"Keyphrase: ""Limited reasoning capability""","""However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios."" Keyphrase: ""Limited reasoning capability"""
arxiv2024,Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering,Yes.,5,"""Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations.""",2024,2024-02-15T12:20:02Z,"Keyphrase: ""Ineffective mitigation of factual hallucination""","""Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations."" Keyphrase: ""Ineffective mitigation of factual hallucination"""
arxiv2024,Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence,Yes.,5,"""Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment.""",2024,2024-02-15T11:08:10Z,"Keyphrase: ""Limitations in genuine reasoning""","""Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment."" Keyphrase: ""Limitations in genuine reasoning"""
arxiv2024,Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States,Yes.,5,"""Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination."" and ""Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one.""",2024,2024-02-15T06:14:55Z,"Keyphrase: ""Susceptibility to hallucination""","""Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination."" and ""Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one."" Keyphrase: ""Susceptibility to hallucination"""
arxiv2024,A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts,Yes.,5,"""Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs.""",2024,2024-02-15T05:40:21Z,"Keyphrase: ""Limited context length""","""Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs."" Keyphrase: ""Limited context length"""
arxiv2024,PAL: Proxy-Guided Black-Box Attack on Large Language Models,Yes.,5,"""Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses.""",2024,2024-02-15T02:54:49Z,"Keyphrase: ""Vulnerability to generating harmful content""","""Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses."" Keyphrase: ""Vulnerability to generating harmful content"""
arxiv2024,CodeMind: A Framework to Challenge Large Language Models for Code Reasoning,Yes.,5,"""Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage."" and ""their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls.""",2024,2024-02-15T02:24:46Z,"Keyphrase: ""Unfair assessment due to data leakage""","""Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage."" and ""their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls."" Keyphrase: ""Unfair assessment due to data leakage"""
arxiv2024,The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse,Yes.,5,"""even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks"" and ""benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive.""",2024,2024-02-15T01:50:38Z,"Keyphrase: ""Collapse on single edit""","""even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks"" and ""benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive."" Keyphrase: ""Collapse on single edit"""
arxiv2024,Probabilistic Reasoning in Generative Large Language Models,Yes.,5,"""Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning.""",2024,2024-02-14T23:05:44Z,"Keyphrase: ""Limited probabilistic reasoning""","""Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning."" Keyphrase: ""Limited probabilistic reasoning"""
arxiv2024,How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?,Yes.,5,"""However, the security aspects of these systems have received relatively less attention"" and ""Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks.""",2024,2024-02-14T19:45:17Z,"Keyphrase: ""Vulnerability to attacks""","""However, the security aspects of these systems have received relatively less attention"" and ""Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks."" Keyphrase: ""Vulnerability to attacks"""
arxiv2024,Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference,Yes.,5,"""Many computational factors limit broader deployment of large language models."" and ""we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding.""",2024,2024-02-14T18:54:56Z,"Keyphrase: ""Memory bottleneck and computational shortcuts""","""Many computational factors limit broader deployment of large language models."" and ""we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding."" Keyphrase: ""Memory bottleneck and computational shortcuts"""
arxiv2024,HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation,Yes.,4,"""With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns.""",2024,2024-02-14T18:41:19Z,"Keyphrase: ""Factuality and hallucination propensity""","""With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns."" Keyphrase: ""Factuality and hallucination propensity"""
arxiv2024,Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking,Yes.,4,"""Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions.""",2024,2024-02-14T18:16:54Z,"Keyphrase: ""Cultural bias and lack of commonsense knowledge""","""Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions."" Keyphrase: ""Cultural bias and lack of commonsense knowledge"""
arxiv2024,Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop,Yes.,5,"""Examples include bias, inconsistencies, and hallucination."" and ""auditing the LLM for these problems is desirable, it is far from being easy or solved.""",2024,2024-02-14T17:49:31Z,"Keyphrase: ""Bias, inconsistency, and hallucination""","""Examples include bias, inconsistencies, and hallucination."" and ""auditing the LLM for these problems is desirable, it is far from being easy or solved."" Keyphrase: ""Bias, inconsistency, and hallucination"""
arxiv2024,AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach,Yes.,4,"""Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality."" and ""A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues.""",2024,2024-02-14T17:31:04Z,"Keyphrase: ""Inconsistency and potential bias""","""Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality."" and ""A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues."" Keyphrase: ""Inconsistency and potential bias"""
arxiv2024,Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code,Yes.,4,"""The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing."" and ""Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement.""",2024,2024-02-14T16:41:35Z,"Keyphrase: ""Challenges in code auditing""","""The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing."" and ""Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement."" Keyphrase: ""Challenges in code auditing"""
arxiv2024,"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",Yes.,4,"""Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety.""",2024,2024-02-14T16:14:03Z,"Keyphrase: ""Risk of harmful responses""","""Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety."" Keyphrase: ""Risk of harmful responses"""
arxiv2024,Personalized Large Language Models,Yes.,4,"""However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots.""",2024,2024-02-14T15:55:30Z,"Keyphrase: ""Limited personalization""","""However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots."" Keyphrase: ""Limited personalization"""
arxiv2024,Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation,Yes.,5,"""Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. 'hallucinations', even when they hold relevant knowledge.""",2024,2024-02-14T15:52:42Z,"Keyphrase: ""Factual inaccuracy and hallucination""","""Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. 'hallucinations', even when they hold relevant knowledge."" Keyphrase: ""Factual inaccuracy and hallucination"""
arxiv2024,Scaling the Authoring of AutoTutors with Large Language Models,Yes.,4,"""A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees.""",2024,2024-02-14T14:53:56Z,"Keyphrase: ""Leaking answers""","""A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees."" Keyphrase: ""Leaking answers"""
arxiv2024,Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling,Yes.,4,"""their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution.""",2024,2024-02-14T14:32:16Z,"Keyphrase: ""Vulnerability to misinformation""","""their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution."" Keyphrase: ""Vulnerability to misinformation"""
arxiv2024,(Ir)rationality and Cognitive Biases in Large Language Models,Yes.,5,"""We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality",2024,2024-02-14T14:17:21Z,"Keyphrase: ""Irrational behavior""","""We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality Keyphrase: ""Irrational behavior"""
arxiv2024,"Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization",Yes.,5,"""the trustworthiness of third-party custom versions of LLMs remains an essential concern."" and ""Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.""",2024,2024-02-14T13:47:35Z,"Keyphrase: ""Vulnerability to customization risks""","""the trustworthiness of third-party custom versions of LLMs remains an essential concern."" and ""Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs."" Keyphrase: ""Vulnerability to customization risks"""
arxiv2024,Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks,Yes.,5,"""Large Language Models (LLMs) are susceptible to Jailbreaking attacks,"" and ""we guide the responses of the model toward revealing the 'desired' harmful information.""",2024,2024-02-14T13:45:19Z,"Keyphrase: ""Susceptible to jailbreaking attacks""","""Large Language Models (LLMs) are susceptible to Jailbreaking attacks,"" and ""we guide the responses of the model toward revealing the 'desired' harmful information."" Keyphrase: ""Susceptible to jailbreaking attacks"""
arxiv2024,Into the Unknown: Self-Learning Large Language Models,Yes.,5,"""We address the main problem of self-learning LLM",2024,2024-02-14T12:56:58Z,"Keyphrase: ""Limited self-learning capabilities""","""We address the main problem of self-learning LLM Keyphrase: ""Limited self-learning capabilities"""
arxiv2024,Exploring the Adversarial Capabilities of Large Language Models,Yes.,4,"""While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored."" and ""Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems.""",2024,2024-02-14T12:28:38Z,"Keyphrase: ""Adversarial behavior undermining detection""","""While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored."" and ""Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems."" Keyphrase: ""Adversarial behavior undermining detection"""
arxiv2024,Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space,Yes.,5,"""We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning."" and ""embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models.""",2024,2024-02-14T10:20:03Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning."" and ""embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models."" Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks,Yes.,5,"""However, their large number of parameters poses significant challenges for practical deployment.""",2024,2024-02-14T09:01:13Z,"Keyphrase: ""Deployment challenges due to large number of parameters""","""However, their large number of parameters poses significant challenges for practical deployment."" Keyphrase: ""Deployment challenges due to large number of parameters"""
arxiv2024,SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding,Yes.,5,"""Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat.""",2024,2024-02-14T06:54:31Z,"Keyphrase: ""Safety vulnerabilities""","""Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat."" Keyphrase: ""Safety vulnerabilities"""
arxiv2024,GrounDial: Human-norm Grounded Safe Dialog Response Generation,Yes.,4,"""Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses, agreeing to offensive user input or including toxic content.""",2024,2024-02-14T06:25:50Z,"Keyphrase: ""Unsafe and offensive responses""","""Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses, agreeing to offensive user input or including toxic content."" Keyphrase: ""Unsafe and offensive responses"""
arxiv2024,Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model,Yes.,5,"""the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development."" and ""We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities.""",2024,2024-02-14T06:24:52Z,"Keyphrase: ""Limited generalization in clinical settings""","""the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development."" and ""We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities."" Keyphrase: ""Limited generalization in clinical settings"""
arxiv2024,Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models,Yes.,5,"""This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.""",2024,2024-02-14T05:52:23Z,"Keyphrase: ""Limited robustness in analogical reasoning""","""This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making."" Keyphrase: ""Limited robustness in analogical reasoning"""
arxiv2024,Premise Order Matters in Reasoning with Large Language Models,Yes.,5,"""we discover a frailty",2024,2024-02-14T04:50:18Z,"Keyphrase: ""Limited discovery capabilities""","""we discover a frailty Keyphrase: ""Limited discovery capabilities"""
arxiv2024,GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency,Yes.,4,"""However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering.""",2024,2024-02-13T23:48:59Z,"Keyphrase: ""Limited personalization control""","""However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering."" Keyphrase: ""Limited personalization control"""
arxiv2024,"ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions",Yes.,4,"""identify and understand why LLMs fails"" and ""ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains.""",2024,2024-02-13T21:15:33Z,"Keyphrase: ""Limited domain expertise""","""identify and understand why LLMs fails"" and ""ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains."" Keyphrase: ""Limited domain expertise"""
arxiv2024,"GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",Yes.,5,"""However, recent work demonstrates that even the best models struggle to identify when and where to refine without access to external feedback."" and ""But they are expensive to train, requiring extensive human annotations.""",2024,2024-02-13T20:16:29Z,"Keyphrase: ""Struggle with refining without external feedback""","""However, recent work demonstrates that even the best models struggle to identify when and where to refine without access to external feedback."" and ""But they are expensive to train, requiring extensive human annotations."" Keyphrase: ""Struggle with refining without external feedback"""
arxiv2024,Measuring and Controlling Instruction (In)Stability in Language Model Dialogs,Yes.,5,"""Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a significant instruction drift within eight rounds of conversations.""",2024,2024-02-13T20:10:29Z,"Keyphrase: ""Instruction drift""","""Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a significant instruction drift within eight rounds of conversations."" Keyphrase: ""Instruction drift"""
arxiv2024,Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance,Yes.,4,"""highlighted the critical issue of their tendency to hallucinate non-existing objects in the images"" and ""these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.""",2024,2024-02-13T18:59:05Z,"Keyphrase: ""Hallucination of nonexisting objects""","""highlighted the critical issue of their tendency to hallucinate non-existing objects in the images"" and ""these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation."" Keyphrase: ""Hallucination of nonexisting objects"""
arxiv2024,Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast,Yes.,5,"""Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors."" and ""It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost",2024,2024-02-13T16:06:17Z,"Keyphrase: ""Adversarial behavior and jailbreaking vulnerability""","""Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors."" and ""It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost Keyphrase: ""Adversarial behavior and jailbreaking vulnerability"""
arxiv2024,The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale,Yes.,5,"""ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists.""",2024,2024-02-13T14:38:12Z,"Keyphrase: ""Inconsistent accuracy""","""ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists."" Keyphrase: ""Inconsistent accuracy"""
arxiv2024,Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale,Yes.,4,"""This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff,"" and ""We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools.""",2024,2024-02-13T13:50:08Z,"Keyphrase: ""Generation of realistic disinformation""","""This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff,"" and ""We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools."" Keyphrase: ""Generation of realistic disinformation"""
arxiv2024,Visually Dehallucinative Instruction Generation,Yes.,4,"""challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents.""",2024,2024-02-13T10:25:45Z,"Keyphrase: ""Unintended content generation""","""challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents."" Keyphrase: ""Unintended content generation"""
arxiv2024,Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering,Yes.,4,"""Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability).""",2024,2024-02-13T08:12:48Z,"Keyphrase: ""Lack of source attribution""","""Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability)."" Keyphrase: ""Lack of source attribution"""
arxiv2024,On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks,Yes.,5,"""While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion."" and ""We observe significant performance collapse with self-critique, significant performance gains with sound external verification,",2024,2024-02-12T23:11:01Z,"Keyphrase: ""Limited self-critique and improvement""","""While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion."" and ""We observe significant performance collapse with self-critique, significant performance gains with sound external verification, Keyphrase: ""Limited self-critique and improvement"""
arxiv2024,Addressing cognitive bias in medical language models,Yes.,4,"""Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias.""",2024,2024-02-12T23:08:37Z,"Keyphrase: ""Varying effect of bias""","""Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias."" Keyphrase: ""Varying effect of bias"""
arxiv2024,Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation,Yes.,5,"""However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination."" and ""Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to",2024,2024-02-12T22:35:40Z,"Keyphrase: ""Translation ability influenced by data contamination""","""However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination."" and ""Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to Keyphrase: ""Translation ability influenced by data contamination"""
arxiv2024,Beyond LLMs: Advancing the Landscape of Complex Reasoning,Yes.,5,"""However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems.""",2024,2024-02-12T21:14:45Z,"Keyphrase: ""Poor performance on constraint satisfaction and optimization problems""","""However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems."" Keyphrase: ""Poor performance on constraint satisfaction and optimization problems"""
arxiv2024,Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking,Yes.,5,"""Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a",2024,2024-02-12T19:49:58Z,"Keyphrase: ""Overreliance on inaccurate suggestions""","""Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a Keyphrase: ""Overreliance on inaccurate suggestions"""
arxiv2024,PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models,Yes.,5,"""Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination.""",2024,2024-02-12T18:28:36Z,"Keyphrase: ""Outdated knowledge hallucination""","""Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination."" Keyphrase: ""Outdated knowledge hallucination"""
arxiv2024,Lissard: Long and Simple Sequential Reasoning Datasets,Yes.,5,"""Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training.""",2024,2024-02-12T18:10:17Z,"Keyphrase: ""Struggles with rule-based tasks""","""Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training."" Keyphrase: ""Struggles with rule-based tasks"""
arxiv2024,Mercury: An Efficiency Benchmark for LLM Code Synthesis,Yes.,5,"""Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for LLM research and development.""",2024,2024-02-12T17:53:22Z,"Keyphrase: ""Efficiency gap in code generation""","""Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for LLM research and development."" Keyphrase: ""Efficiency gap in code generation"""
arxiv2024,Do Membership Inference Attacks Work on Large Language Models?,Yes.,5,"""We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members.""",2024,2024-02-12T17:52:05Z,"Keyphrase: ""Poor performance in domain analysis""","""We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members."" Keyphrase: ""Poor performance in domain analysis"""
arxiv2024,Retrieval-Augmented Thought Process as Sequential Decision Making,Yes.,5,"""However, several open challenges hinder their wider application",2024,2024-02-12T17:17:50Z,"Keyphrase: ""Open challenges hindering wider application""","""However, several open challenges hinder their wider application Keyphrase: ""Open challenges hindering wider application"""
arxiv2024,AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension,Yes.,4,"""By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.""",2024,2024-02-12T15:41:22Z,"Keyphrase: ""Limited evaluation insights""","""By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research."" Keyphrase: ""Limited evaluation insights"""
arxiv2024,Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT,Yes.,5,"""Developing long-context retrieval encoders suitable for these domains raises three challenges",2024,2024-02-12T06:43:52Z,"Keyphrase: ""Challenges in domain adaptation""","""Developing long-context retrieval encoders suitable for these domains raises three challenges Keyphrase: ""Challenges in domain adaptation"""
arxiv2024,Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate,Yes.,4,"""While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness.""",2024,2024-02-12T04:32:33Z,"Keyphrase: ""Unfaithful explanations""","""While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness."" Keyphrase: ""Unfaithful explanations"""
arxiv2024,Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning,Yes.,4,"""the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others"" and ""The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs.""",2024,2024-02-12T01:55:51Z,"Keyphrase: ""Inequitable performance impact""","""the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others"" and ""The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs."" Keyphrase: ""Inequitable performance impact"""
arxiv2024,Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning,Yes.,5,"""However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP.""",2024,2024-02-11T13:30:53Z,"Keyphrase: ""Limited spatial awareness""","""However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP."" Keyphrase: ""Limited spatial awareness"""
arxiv2024,Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models,Yes.,5,"""We find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers.""",2024,2024-02-11T12:25:41Z,"Keyphrase: ""Inaccurate generation""","""We find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers."" Keyphrase: ""Inaccurate generation"""
arxiv2024,Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias,Yes.,5,"""We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic.""",2024,2024-02-11T11:23:28Z,"Keyphrase: ""Toxicity bias and hallucination""","""We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic."" Keyphrase: ""Toxicity bias and hallucination"""
arxiv2024,Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review,Yes.,4,"""Despite their transformative potential, challenges persist, including sensitivity to input prompts, occasional misinterpretations, and unforeseen recommendations, necessitating continuous refinement and evolution in LLM-driven recommender systems.""",2024,2024-02-11T00:24:17Z,"Keyphrase: ""Sensitivity and misinterpretation issues""","""Despite their transformative potential, challenges persist, including sensitivity to input prompts, occasional misinterpretations, and unforeseen recommendations, necessitating continuous refinement and evolution in LLM-driven recommender systems."" Keyphrase: ""Sensitivity and misinterpretation issues"""
arxiv2024,A Tale of Tails: Model Collapse as a Change of Scaling Laws,Yes.,5,"""We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning"" of skills, and grokking when mixing human and synthesized data.""",2024,2024-02-10T21:06:34Z,"Keyphrase: ""Decay phenomenon and loss scaling""","""We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning"" of skills, and grokking when mixing human and synthesized data."" Keyphrase: ""Decay phenomenon and loss scaling"""
arxiv2024,Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric,Yes.,4,"""The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset.""",2024,2024-02-10T07:55:27Z,"Keyphrase: ""Limited generalization to out-of-distribution toxicity""","""The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset."" Keyphrase: ""Limited generalization to out-of-distribution toxicity"""
arxiv2024,GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding,Yes.,5,"""their ability to reason over domain-specialized graphs of interconnected entities remains limited"" and ""The answer is no--such capabilities lie beyond current methods.""",2024,2024-02-09T19:53:29Z,"Keyphrase: ""Limited domain-specific reasoning""","""their ability to reason over domain-specialized graphs of interconnected entities remains limited"" and ""The answer is no--such capabilities lie beyond current methods."" Keyphrase: ""Limited domain-specific reasoning"""
arxiv2024,Feedback Loops With Language Models Drive In-Context Reward Hacking,Yes.,5,"""we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process.""",2024,2024-02-09T18:59:29Z,"Keyphrase: ""Feedback loop-induced reward hacking""","""we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process."" Keyphrase: ""Feedback loop-induced reward hacking"""
arxiv2024,Understanding the Effects of Iterative Prompting on Truthfulness,Yes.,5,"""Yet, the reliability and truthfulness of these models remain pressing concerns."" and ""naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors.""",2024,2024-02-09T18:57:08Z,"Keyphrase: ""Reliability and truthfulness concerns""","""Yet, the reliability and truthfulness of these models remain pressing concerns."" and ""naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors."" Keyphrase: ""Reliability and truthfulness concerns"""
arxiv2024,On the Out-Of-Distribution Generalization of Multimodal Large Language Models,Yes.,5,"""Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation."" and ""We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples",2024,2024-02-09T18:21:51Z,"Keyphrase: ""Limited generalization beyond training domain""","""Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation."" and ""We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples Keyphrase: ""Limited generalization beyond training domain"""
arxiv2024,Understanding the Weakness of Large Language Model Agents within a Complex Android Environment,Yes.,5,"""LLM agents face three primary challenges,"" ""even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints,"" and ""a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents.""",2024,2024-02-09T18:19:25Z,"**Given Evidence:**
Evidence: ""llm agent face three primary challenge even stateoftheart llm agent struggle crossapp scenario adhering specific constraint lack four key capability ie understanding reasoning exploration reflection primary reason failure llm agent""

**Keyphrase:**
Lack of key capabilities","""LLM agents face three primary challenges,"" ""even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints,"" and ""a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents."" **Given Evidence:**
Evidence: ""llm agent face three primary challenge even stateoftheart llm agent struggle crossapp scenario adhering specific constraint lack four key capability ie understanding reasoning exploration reflection primary reason failure llm agent""

**Keyphrase:**
Lack of key capabilities"
arxiv2024,The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model,Yes.,5,"""The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates.""",2024,2024-02-09T17:15:45Z,"Keyphrase: ""Hallucinations in information retrieval""","""The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates."" Keyphrase: ""Hallucinations in information retrieval"""
arxiv2024,Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty,Yes.,5,"""However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist.""",2024,2024-02-09T16:40:59Z,"Keyphrase: ""Hallucination leading to unsafe outcomes""","""However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist."" Keyphrase: ""Hallucination leading to unsafe outcomes"""
arxiv2024,On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference,Yes.,4,"""Despite the recent success associated with Large Language Models (LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands.""",2024,2024-02-09T09:20:59Z,"Keyphrase: ""Cost-prohibitive deployment""","""Despite the recent success associated with Large Language Models (LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands."" Keyphrase: ""Cost-prohibitive deployment"""
arxiv2024,Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning,Yes.,4,"""they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak.""",2024,2024-02-09T09:09:39Z,"Keyphrase: ""Susceptibility to prompt-induced safety bypass""","""they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak."" Keyphrase: ""Susceptibility to prompt-induced safety bypass"""
arxiv2024,"The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate",Yes.,5,"""Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks."" and ""underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators.""",2024,2024-02-09T06:16:08Z,"Keyphrase: ""Disparity in performance""","""Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks."" and ""underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators."" Keyphrase: ""Disparity in performance"""
arxiv2024,ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling,Yes.,5,"""the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects.""",2024,2024-02-09T01:00:14Z,"Keyphrase: ""Inaccurate visual grounding""","""the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects."" Keyphrase: ""Inaccurate visual grounding"""
arxiv2024,SubGen: Token Generation in Sublinear Time and Memory,Yes.,5,"""Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation.""",2024,2024-02-08T22:17:40Z,"Keyphrase: ""Extensive memory requirement""","""Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation."" Keyphrase: ""Extensive memory requirement"""
arxiv2024,OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models,Yes.,5,"""Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.""",2024,2024-02-08T20:35:06Z,"Keyphrase: ""Limited modeling of mental states""","""Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world."" Keyphrase: ""Limited modeling of mental states"""
arxiv2024,LLMs Among Us: Generative AI Participating in Digital Discourse,Yes.,4,"""While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors.""",2024,2024-02-08T19:21:33Z,"Keyphrase: ""Bias, privacy, and propaganda concerns""","""While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors."" Keyphrase: ""Bias, privacy, and propaganda concerns"""
arxiv2024,WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,Yes.,5,"""Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time."" and ""However, all finetuned models struggle to generalize to unseen websites.""",2024,2024-02-08T18:58:02Z,"Keyphrase: ""Struggles with generalization to unseen websites""","""Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time."" and ""However, all finetuned models struggle to generalize to unseen websites."" Keyphrase: ""Struggles with generalization to unseen websites"""
arxiv2024,Large Language Model Meets Graph Neural Network in Knowledge Distillation,Yes.,4,"""the deployment of LLMs for production is hindered by its high computational and storage requirements, as well as long latencies during model inference.""",2024,2024-02-08T18:33:21Z,"Keyphrase: ""High computational and storage requirements""","""the deployment of LLMs for production is hindered by its high computational and storage requirements, as well as long latencies during model inference."" Keyphrase: ""High computational and storage requirements"""
arxiv2024,Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking,Yes.,5,"""little is known about such a risk of LLM-powered conversational search"" and ""participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias.""",2024,2024-02-08T18:14:33Z,"Keyphrase: ""Biased information reinforcement""","""little is known about such a risk of LLM-powered conversational search"" and ""participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias."" Keyphrase: ""Biased information reinforcement"""
arxiv2024,EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models,Yes.,4,"""they also introduce significant privacy concerns",2024,2024-02-08T17:57:11Z,"Keyphrase: ""Privacy concerns""","""they also introduce significant privacy concerns Keyphrase: ""Privacy concerns"""
arxiv2024,Is it Possible to Edit Large Language Models Robustly?,Yes.,5,"""However, the robustness of model editing remains an open question."" and ""Our experimental results uncover a substantial disparity between existing editing methods and the practical application of LLMs."" and ""On rephrased prompts that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline.""",2024,2024-02-08T17:06:45Z,"Keyphrase: ""Limited robustness in editing""","""However, the robustness of model editing remains an open question."" and ""Our experimental results uncover a substantial disparity between existing editing methods and the practical application of LLMs."" and ""On rephrased prompts that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline."" Keyphrase: ""Limited robustness in editing"""
arxiv2024,Limits of Transformer Language Models on Learning Algorithmic Compositions,Yes.,5,"""We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition.""",2024,2024-02-08T16:23:29Z,"Keyphrase: ""Limited compositional capability""","""We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition."" Keyphrase: ""Limited compositional capability"""
arxiv2024,Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images,Yes.,4,"""we examine potential gender and racial biases in such systems,"" and ""we observe significant differences in the responses according to the perceived gender or race of the person depicted.""",2024,2024-02-08T16:11:23Z,"Keyphrase: ""Gender and racial biases""","""we examine potential gender and racial biases in such systems,"" and ""we observe significant differences in the responses according to the perceived gender or race of the person depicted."" Keyphrase: ""Gender and racial biases"""
arxiv2024,TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation,Yes.,5,"""Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents.""",2024,2024-02-08T15:08:57Z,"Keyphrase: ""Limited multitasking ability""","""Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents."" Keyphrase: ""Limited multitasking ability"""
arxiv2024,In-Context Learning Can Re-learn Forbidden Tasks,Yes.,5,"""Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities."" and ""we investigate whether ICL can undo safety training, which could represent a major security risk.""",2024,2024-02-08T14:54:17Z,"Keyphrase: ""Vulnerability despite safety training""","""Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities."" and ""we investigate whether ICL can undo safety training, which could represent a major security risk."" Keyphrase: ""Vulnerability despite safety training"""
arxiv2024,Comprehensive Assessment of Jailbreak Attacks Against LLMs,Yes.,5,"""safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks.""",2024,2024-02-08T13:42:50Z,"Keyphrase: ""Vulnerability to jailbreak attacks""","""safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks."" Keyphrase: ""Vulnerability to jailbreak attacks"""
arxiv2024,"Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks",Yes.,5,"""We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good.""",2024,2024-02-08T13:07:31Z,"Keyphrase: ""Limited performance in complex tasks""","""We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good."" Keyphrase: ""Limited performance in complex tasks"""
arxiv2024,"Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations",Yes.,5,"""We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity."" and ""We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs.""",2024,2024-02-08T12:36:29Z,"Keyphrase: ""Entity ambiguity and mixing information""","""We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity."" and ""We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs."" Keyphrase: ""Entity ambiguity and mixing information"""
arxiv2024,"Efficient Models for the Detection of Hate, Abuse and Profanity",Yes.,5,"""Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content.""",2024,2024-02-08T12:28:18Z,"Keyphrase: ""Hateful and profane content generation""","""Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content."" Keyphrase: ""Hateful and profane content generation"""
arxiv2024,AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers,Yes.,4,"""Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process.""",2024,2024-02-08T12:01:24Z,"Keyphrase: ""Biased prediction and hallucination""","""Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process."" Keyphrase: ""Biased prediction and hallucination"""
arxiv2024,Can ChatGPT evaluate research quality?,Yes.,5,"""ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks.""",2024,2024-02-08T10:00:40Z,"Keyphrase: ""Limited accuracy and trustworthiness""","""ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks."" Keyphrase: ""Limited accuracy and trustworthiness"""
arxiv2024,Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia,Yes.,4,"""Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle.""",2024,2024-02-08T07:56:49Z,"Keyphrase: ""Prone to generating harmful content""","""Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle."" Keyphrase: ""Prone to generating harmful content"""
arxiv2024,Do Large Code Models Understand Programming Concepts? A Black-box Approach,Yes.,5,"""Our findings suggest that current models lack understanding of concepts such as data flow and control flow.""",2024,2024-02-08T06:48:01Z,"Keyphrase: ""Limited understanding of data flow control""","""Our findings suggest that current models lack understanding of concepts such as data flow and control flow."" Keyphrase: ""Limited understanding of data flow control"""
arxiv2024,Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes,Yes.,5,"""LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target.""",2024,2024-02-08T04:48:26Z,"Keyphrase: ""Resource-intensive scalability""","""LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target."" Keyphrase: ""Resource-intensive scalability"""
arxiv2024,Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception,Yes.,5,"""existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination.""",2024,2024-02-08T02:37:30Z,"Keyphrase: ""Insufficient expressive power""","""existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination."" Keyphrase: ""Insufficient expressive power"""
arxiv2024,Are LLMs Ready for Real-World Materials Discovery?,Yes.,5,"""While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge.""",2024,2024-02-07T19:10:36Z,"Keyphrase: ""Limited comprehension of complex material science knowledge""","""While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge."" Keyphrase: ""Limited comprehension of complex material science knowledge"""
arxiv2024,Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications,Yes.,5,"""Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning."" and ""These findings underscore the urgent need for more robust safety strategies in LLMs.""",2024,2024-02-07T18:34:38Z,"Keyphrase: ""Brittleness and susceptibility to jailbreaking""","""Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning."" and ""These findings underscore the urgent need for more robust safety strategies in LLMs."" Keyphrase: ""Brittleness and susceptibility to jailbreaking"""
arxiv2024,An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration,Yes.,4,"""they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process.""",2024,2024-02-07T15:56:17Z,"Keyphrase: ""Limited transparency and reasoning""","""they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process."" Keyphrase: ""Limited transparency and reasoning"""
arxiv2024,Reconfidencing LLMs from the Grouping Loss Perspective,Yes.,5,"""Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone."" and ""Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query.""",2024,2024-02-07T15:40:22Z,"Keyphrase: ""Overconfident hallucinated answers""","""Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone."" and ""Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query."" Keyphrase: ""Overconfident hallucinated answers"""
arxiv2024,Prompting Implicit Discourse Relation Annotation,Yes.,5,"""Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches.""",2024,2024-02-07T14:44:42Z,"Keyphrase: ""Inferior performance on implicit discourse relation classification""","""Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches."" Keyphrase: ""Inferior performance on implicit discourse relation classification"""
arxiv2024,MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark,Yes.,5,"""MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V.""",2024,2024-02-07T12:28:32Z,"Keyphrase: ""Diverse bias and hallucinatory responses""","""MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V."" Keyphrase: ""Diverse bias and hallucinatory responses"""
arxiv2024,A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models,Yes.,4,"""how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further"" and ""The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework's potential to approximate LLM decisions better in future work.""",2024,2024-02-07T12:26:12Z,"Keyphrase: ""Questionable faithfulness in explanations""","""how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further"" and ""The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework's potential to approximate LLM decisions better in future work."" Keyphrase: ""Questionable faithfulness in explanations"""
arxiv2024,Large Language Models As Faithful Explainers,Yes.,4,"""natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs.""",2024,2024-02-07T09:09:14Z,"Keyphrase: ""Lack of faithfulness in explanations""","""natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs."" Keyphrase: ""Lack of faithfulness in explanations"""
arxiv2024,MEMORYLLM: Towards Self-Updatable Large Language Models,Yes.,4,"""Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model.""",2024,2024-02-07T07:14:11Z,"Keyphrase: ""Limited adaptability to new knowledge""","""Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model."" Keyphrase: ""Limited adaptability to new knowledge"""
arxiv2024,InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory,Yes.,5,"""existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues.""",2024,2024-02-07T06:50:42Z,"Keyphrase: ""Limited sequence length generalization""","""existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues."" Keyphrase: ""Limited sequence length generalization"""
arxiv2024,Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,Yes.,5,"""However, there is little to no understanding of their faithfulness,"" ""we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs,"" ""these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness,"" and ""improving faithfulness is an open challenge.""",2024,2024-02-07T06:32:50Z,"Keyphrase: ""Challenges in faithfulness""","""However, there is little to no understanding of their faithfulness,"" ""we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs,"" ""these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness,"" and ""improving faithfulness is an open challenge."" Keyphrase: ""Challenges in faithfulness"""
arxiv2024,Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector,Yes.,4,"""Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs).""",2024,2024-02-07T05:56:54Z,"Keyphrase: ""Overcorrection challenge""","""Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs)."" Keyphrase: ""Overcorrection challenge"""
arxiv2024,Online Cascade Learning for Efficient Inference over Streams,Yes.,5,"""Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks.""",2024,2024-02-07T01:46:50Z,"Keyphrase: ""High computational cost""","""Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks."" Keyphrase: ""High computational cost"""
arxiv2024,De-amplifying Bias from Differential Privacy in Language Model Fine-tuning,Yes.,5,"""We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP.""",2024,2024-02-07T00:30:58Z,"Keyphrase: ""Amplification of biases""","""We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP."" Keyphrase: ""Amplification of biases"""
arxiv2024,Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models,Yes.,5,"""concentrating on deceptive behaviours of Large Language Models (LLMs)"" and ""emphasising multidimensional biases that underlie their deceptive behaviours"" and ""the literature review covers four types of deception categorised",2024,2024-02-07T00:21:46Z,"Keyphrase: ""Multidimensional bias and deceptive behavior""","""concentrating on deceptive behaviours of Large Language Models (LLMs)"" and ""emphasising multidimensional biases that underlie their deceptive behaviours"" and ""the literature review covers four types of deception categorised Keyphrase: ""Multidimensional bias and deceptive behavior"""
arxiv2024,Detecting Mode Collapse in Language Models via Narration,Yes.,5,"""we show successive versions of GPT-3 suffer from increasing degrees of 'mode collapse' whereby overfitting the model during alignment constrains it from generalizing over authorship",2024,2024-02-06T23:52:58Z,"Keyphrase: ""Mode collapse and overfitting""","""we show successive versions of GPT-3 suffer from increasing degrees of 'mode collapse' whereby overfitting the model during alignment constrains it from generalizing over authorship Keyphrase: ""Mode collapse and overfitting"""
arxiv2024,Training Language Models to Generate Text with Citations via Fine-grained Rewards,Yes.,5,"""While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources.""",2024,2024-02-06T19:00:40Z,"Keyphrase: ""Hallucination and lack of credibility""","""While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources."" Keyphrase: ""Hallucination and lack of credibility"""
arxiv2024,Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science,Yes.,5,"""While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety."" and ""We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents,"" and ""Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents.""",2024,2024-02-06T18:54:07Z,"Keyphrase: ""Novel vulnerabilities and safety considerations""","""While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety."" and ""We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents,"" and ""Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents."" Keyphrase: ""Novel vulnerabilities and safety considerations"""
arxiv2024,Measuring Implicit Bias in Explicitly Unbiased Large Language Models,Yes.,5,"""Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases,"" and ""Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others).""",2024,2024-02-06T15:59:23Z,"Keyphrase: ""Implicit bias across social domains""","""Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases,"" and ""Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others)."" Keyphrase: ""Implicit bias across social domains"""
arxiv2024,Systematic Biases in LLM Simulations of Debates,Yes.,5,"""In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives.""",2024,2024-02-06T14:51:55Z,"Keyphrase: ""Inherent social bias in simulating human interaction""","""In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives."" Keyphrase: ""Inherent social bias in simulating human interaction"""
arxiv2024,Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought,Yes.,5,"""We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise.""",2024,2024-02-06T13:59:56Z,"Keyphrase: ""Struggles with dynamic noise""","""We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise."" Keyphrase: ""Struggles with dynamic noise"""
arxiv2024,LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K,Yes.,5,"""Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation,"" and ""LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of 'needle in a haystack'.""",2024,2024-02-06T13:11:19Z,"Keyphrase: ""Knowledge leakage and biased evaluation""","""Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation,"" and ""LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of 'needle in a haystack'."" Keyphrase: ""Knowledge leakage and biased evaluation"""
arxiv2024,"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",Yes.,5,"""The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers."" and ""we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues.""",2024,2024-02-06T11:54:23Z,"Keyphrase: ""Data contamination and reproducibility issues""","""The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers."" and ""we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues."" Keyphrase: ""Data contamination and reproducibility issues"""
arxiv2024,Can Large Language Models Detect Rumors on Social Media?,Yes.,5,"""it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information.""",2024,2024-02-06T11:33:57Z,"Keyphrase: ""Difficulty in reasoning with redundant information""","""it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information."" Keyphrase: ""Difficulty in reasoning with redundant information"""
arxiv2024,Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models,Yes.,5,"""Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements.""",2024,2024-02-06T10:37:21Z,"Keyphrase: ""Biased target variable selection""","""Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements."" Keyphrase: ""Biased target variable selection"""
arxiv2024,The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs,Yes.,5,"""those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs."" and ""illustrating that they universally suffer from this instinctive bias to varying degrees.""",2024,2024-02-06T06:48:46Z,"Keyphrase: ""Instinctive bias""","""those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs."" and ""illustrating that they universally suffer from this instinctive bias to varying degrees."" Keyphrase: ""Instinctive bias"""
arxiv2024,Limits of Large Language Models in Debating Humans,Yes.,5,"""We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters.""",2024,2024-02-06T03:24:27Z,"Keyphrase: ""Limited debate capabilities""","""We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters."" Keyphrase: ""Limited debate capabilities"""
arxiv2024,Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context,Yes.,5,"""Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs",2024,2024-02-06T01:59:41Z,"Keyphrase: ""Limited cross-file context integration""","""Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs Keyphrase: ""Limited cross-file context integration"""
arxiv2024,Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains,Yes.,5,"""We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors.""",2024,2024-02-05T20:51:11Z,"Keyphrase: ""Inconsistency in summarization""","""We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors."" Keyphrase: ""Inconsistency in summarization"""
arxiv2024,Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues,Yes.,5,"""This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and",2024,2024-02-05T20:11:56Z,"Keyphrase: ""Struggles with nuance and verbal instruction""","""This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and Keyphrase: ""Struggles with nuance and verbal instruction"""
arxiv2024,A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications,Yes.,4,"""We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique.""",2024,2024-02-05T19:49:13Z,"Keyphrase: ""Limited depth and detail""","""We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique."" Keyphrase: ""Limited depth and detail"""
arxiv2024,Nevermind: Instruction Override and Moderation in Large Language Models,Yes.,5,"""Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines.""",2024,2024-02-05T18:58:19Z,"Keyphrase: ""Difficulty in following safety guidelines""","""Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines."" Keyphrase: ""Difficulty in following safety guidelines"""
arxiv2024,GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models,Yes.,4,"""The discovery of 'jailbreaks' to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures."" and ""Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses.""",2024,2024-02-05T18:54:43Z,"Keyphrase: ""Ethical guideline violations""","""The discovery of 'jailbreaks' to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures."" and ""Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses."" Keyphrase: ""Ethical guideline violations"""
arxiv2024,Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS,Yes.,5,"""Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency.""",2024,2024-02-05T18:47:04Z,"Keyphrase: ""Challenges in code generation""","""Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency."" Keyphrase: ""Challenges in code generation"""
arxiv2024,Unified Hallucination Detection for Multimodal Large Language Models,Yes.,5,"""Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination.""",2024,2024-02-05T16:56:11Z,"Keyphrase: ""Hallucination issues""","""Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination."" Keyphrase: ""Hallucination issues"""
arxiv2024,C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models,Yes.,5,"""Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments.""",2024,2024-02-05T16:46:16Z,"Keyphrase: ""Trustworthiness issues""","""Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments."" Keyphrase: ""Trustworthiness issues"""
arxiv2024,Best Practices for Text Annotation with Large Language Models,Yes.,4,"""Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results.""",2024,2024-02-05T15:43:50Z,"Keyphrase: ""Misleading simplicity and bias""","""Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results."" Keyphrase: ""Misleading simplicity and bias"""
arxiv2024,Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations,Yes.,5,"""Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions.""",2024,2024-02-05T15:08:19Z,"Keyphrase: ""Stability issues and content hallucination""","""Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions."" Keyphrase: ""Stability issues and content hallucination"""
arxiv2024,Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation,Yes.,5,"""there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context,"" and ""directly applying LLMs often leads to inaccurate answers.""",2024,2024-02-05T11:58:56Z,"Keyphrase: ""Weak reasoning and contextual understanding""","""there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context,"" and ""directly applying LLMs often leads to inaccurate answers."" Keyphrase: ""Weak reasoning and contextual understanding"""
arxiv2024,Evading Data Contamination Detection for Language Models is (too) Easy,Yes.,5,"""However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements."" and ""we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.""",2024,2024-02-05T09:10:32Z,"Keyphrase: ""Contamination of training data""","""However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements."" and ""we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods."" Keyphrase: ""Contamination of training data"""
arxiv2024,Graph-enhanced Large Language Models in Asynchronous Plan Reasoning,Yes.,5,"""We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow."" and ""LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices.""",2024,2024-02-05T08:26:33Z,"Keyphrase: ""Poor performance on complex tasks""","""We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow."" and ""LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices."" Keyphrase: ""Poor performance on complex tasks"""
arxiv2024,DeAL: Decoding-time Alignment for Large Language Models,Yes.,4,"""First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training).""",2024,2024-02-05T06:12:29Z,"Keyphrase: ""Reliability and safety concerns""","""First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training)."" Keyphrase: ""Reliability and safety concerns"""
arxiv2024,Large Language Models are Geographically Biased,Yes.,5,"""Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm."" and ""We show various problematic geographic biases, which we define as systemic errors in geospatial predictions."" and ""LLMs exhibit common biases across a range of objective and subjective topics"" and ""LLMs are clearly biased against locations with lower",2024,2024-02-05T02:32:09Z,"Keyphrase: ""Inherent societal biases""","""Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm."" and ""We show various problematic geographic biases, which we define as systemic errors in geospatial predictions."" and ""LLMs exhibit common biases across a range of objective and subjective topics"" and ""LLMs are clearly biased against locations with lower Keyphrase: ""Inherent societal biases"""
arxiv2024,Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting,Yes.,5,"""Large Language Models (LLMs) frequently struggle with complex reasoning tasks, failing to construct logically sound steps towards the solution."" and ""repeated meaningless feedback gradually decreases the quality of the responses, eventually leading to a larger deviation from the intended outcome.""",2024,2024-02-05T00:44:28Z,"Keyphrase: ""Struggles with complex reasoning""","""Large Language Models (LLMs) frequently struggle with complex reasoning tasks, failing to construct logically sound steps towards the solution."" and ""repeated meaningless feedback gradually decreases the quality of the responses, eventually leading to a larger deviation from the intended outcome."" Keyphrase: ""Struggles with complex reasoning"""
arxiv2024,LLM-Enhanced Data Management,Yes.,5,"""existing LLMs have several limitations",2024,2024-02-04T23:42:02Z,"Keyphrase: ""Multiple limitations""","""existing LLMs have several limitations Keyphrase: ""Multiple limitations"""
arxiv2024,Can Large Language Models Learn Independent Causal Mechanisms?,Yes.,5,"""Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability.""",2024,2024-02-04T23:04:02Z,"Keyphrase: ""Limited generalization ability in uncommon settings""","""Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability."" Keyphrase: ""Limited generalization ability in uncommon settings"""
arxiv2024,PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?,Yes.,5,"""We first observe that LLMs, even when aided by symbolic solvers, perform rather poorly on our dataset.""",2024,2024-02-04T20:56:09Z,"Keyphrase: ""Poor performance on symbolic tasks""","""We first observe that LLMs, even when aided by symbolic solvers, perform rather poorly on our dataset."" Keyphrase: ""Poor performance on symbolic tasks"""
arxiv2024,Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials,Yes.,5,"""Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context.""",2024,2024-02-04T16:18:01Z,"Keyphrase: ""Shortcut learning and lack of contextual variation""","""Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context."" Keyphrase: ""Shortcut learning and lack of contextual variation"""
arxiv2024,Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation,Yes.,5,"""The ease with which AI can generate believable but false scientific information, as illustrated in this case, raises significant concerns about the potential for misinformation in medicine.""",2024,2024-02-04T13:21:19Z,"Keyphrase: ""Potential for generating false scientific information""","""The ease with which AI can generate believable but false scientific information, as illustrated in this case, raises significant concerns about the potential for misinformation in medicine."" Keyphrase: ""Potential for generating false scientific information"""
arxiv2024,Factuality of Large Language Models in the Year 2024,Yes.,5,"""Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios.""",2024,2024-02-04T09:36:31Z,"Keyphrase: ""Factual inaccuracy""","""Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios."" Keyphrase: ""Factual inaccuracy"""
arxiv2024,DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models,Yes.,5,"""we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases.""",2024,2024-02-04T08:11:45Z,"Keyphrase: ""Poor decision-making in complex problems""","""we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases."" Keyphrase: ""Poor decision-making in complex problems"""
arxiv2024,Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning,Yes.,5,"""However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process.""",2024,2024-02-04T07:59:06Z,"Keyphrase: ""Lack of self-evaluation capability""","""However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process."" Keyphrase: ""Lack of self-evaluation capability"""
arxiv2024,A Survey of Large Language Models in Finance (FinLLMs),Yes.,4,"""Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency.""",2024,2024-02-04T02:06:57Z,"Keyphrase: ""Hallucination and privacy concerns""","""Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency."" Keyphrase: ""Hallucination and privacy concerns"""
arxiv2024,"Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times",Yes.,5,"""Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades.""",2024,2024-02-03T20:22:54Z,"Keyphrase: ""Degraded human reading time estimation""","""Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades."" Keyphrase: ""Degraded human reading time estimation"""
arxiv2024,"Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding",Yes.,4,"""These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings.""",2024,2024-02-03T19:19:34Z,"Keyphrase: ""Lack of sensorimotor grounding""","""These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings."" Keyphrase: ""Lack of sensorimotor grounding"""
arxiv2024,Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models,Yes.,4,"""Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks."" and ""Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning L",2024,2024-02-03T16:43:42Z,"Keyphrase: ""Vulnerability to harmful content and forgetting safety alignment""","""Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks."" and ""Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning L Keyphrase: ""Vulnerability to harmful content and forgetting safety alignment"""
arxiv2024,GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events,Yes.,5,"""Concurrently, we also identify certain limitations of GPT-4V, which constrain its understanding in more intricate scenarios.""",2024,2024-02-03T16:38:25Z,"Keyphrase: ""Limited understanding of intricate scenarios""","""Concurrently, we also identify certain limitations of GPT-4V, which constrain its understanding in more intricate scenarios."" Keyphrase: ""Limited understanding of intricate scenarios"""
arxiv2024,Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations,Yes.,4,"""At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the",2024,2024-02-03T14:28:55Z,"Keyphrase: ""Diverse pitfalls in generating desired results""","""At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the Keyphrase: ""Diverse pitfalls in generating desired results"""
arxiv2024,Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test,Yes.,5,"""Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages.""",2024,2024-02-03T12:52:36Z,"Keyphrase: ""Cultural and linguistic bias""","""Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages."" Keyphrase: ""Cultural and linguistic bias"""
arxiv2024,Affordable Generative Agents,Yes.,4,"""the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents,"" and ""demonstrating that agents can only generate finite behaviors in fixed environments.""",2024,2024-02-03T06:16:28Z,"Keyphrase: ""Limited adaptability in dynamic environments""","""the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents,"" and ""demonstrating that agents can only generate finite behaviors in fixed environments."" Keyphrase: ""Limited adaptability in dynamic environments"""
arxiv2024,A Closer Look at the Limitations of Instruction Tuning,Yes.,5,"""While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT.""",2024,2024-02-03T04:45:25Z,"Keyphrase: ""Underexplored limitations""","""While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT."" Keyphrase: ""Underexplored limitations"""
arxiv2024,How well do LLMs cite relevant medical references? An evaluation framework and analyses,Yes.,5,"""Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide."" and ""Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references.""",2024,2024-02-03T03:44:57Z,"Keyphrase: ""Potential for harm with medical information""","""Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide."" and ""Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references."" Keyphrase: ""Potential for harm with medical information"""
arxiv2024,Human-Centered Privacy Research in the Age of Large Language Models,Yes.,4,"""The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns."" and ""To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems.""",2024,2024-02-03T02:32:45Z,"Keyphrase: ""Imperfect privacy properties""","""The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns."" and ""To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems."" Keyphrase: ""Imperfect privacy properties"""
arxiv2024,Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes,Yes.,4,"""Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases.""",2024,2024-02-03T01:40:11Z,"Keyphrase: ""Harmful social bias""","""Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases."" Keyphrase: ""Harmful social bias"""
arxiv2024,What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement,Yes.,5,"""Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase.""",2024,2024-02-02T19:43:15Z,"Keyphrase: ""Catastrophic forgetting""","""Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase."" Keyphrase: ""Catastrophic forgetting"""
arxiv2024,"(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",Yes.,5,"""Beyond known issues like hallucinations, experts revealed novel legal problems, including that users' conversations with LLMs are not protected by attorney-client confidentiality or bound to professional ethics that guard against conflicted counsel or poor quality advice.""",2024,2024-02-02T19:35:34Z,"Keyphrase: ""Poor quality legal advice""","""Beyond known issues like hallucinations, experts revealed novel legal problems, including that users' conversations with LLMs are not protected by attorney-client confidentiality or bound to professional ethics that guard against conflicted counsel or poor quality advice."" Keyphrase: ""Poor quality legal advice"""
arxiv2024,Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning,Yes.,5,"""However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability.""",2024,2024-02-02T18:00:35Z,"Keyphrase: ""Limited trustworthiness and usability""","""However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability."" Keyphrase: ""Limited trustworthiness and usability"""
arxiv2024,TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution,Yes.,4,"""their trustworthiness remains an under-explored area"" and ""improving the safety dimension of trustworthiness in LLM-based agents.""",2024,2024-02-02T17:26:23Z,"Keyphrase: ""Lack of trustworthiness assessment""","""their trustworthiness remains an under-explored area"" and ""improving the safety dimension of trustworthiness in LLM-based agents."" Keyphrase: ""Lack of trustworthiness assessment"""
arxiv2024,Homogenization Effects of Large Language Models on Human Creative Ideation,Yes.,4,"""different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST"" and ""ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated.""",2024,2024-02-02T16:27:11Z,"Keyphrase: ""Limited semantic diversity""","""different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST"" and ""ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated."" Keyphrase: ""Limited semantic diversity"""
arxiv2024,"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",Yes.,5,"""We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature.""",2024,2024-02-02T14:43:18Z,"Keyphrase: ""Limited planning and reasoning capabilities""","""We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature."" Keyphrase: ""Limited planning and reasoning capabilities"""
arxiv2024,Distilling LLMs' Decomposition Abilities into Compact Language Models,Yes.,4,"""Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization.""",2024,2024-02-02T13:23:15Z,"Keyphrase: ""Scalability challenges and limited customization""","""Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization."" Keyphrase: ""Scalability challenges and limited customization"""
arxiv2024,StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback,Yes.,4,"""the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective.""",2024,2024-02-02T13:14:31Z,"Keyphrase: ""Limited understanding of complex human requirements""","""the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective."" Keyphrase: ""Limited understanding of complex human requirements"""
arxiv2024,Continual Learning for Large Language Models: A Survey,Yes.,4,"""Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale."" and ""Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.""",2024,2024-02-02T12:34:09Z,"Keyphrase: ""High retraining cost""","""Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale."" and ""Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task."" Keyphrase: ""High retraining cost"""
arxiv2024,A Survey on Large Language Model Hallucination via a Creativity Perspective,Yes.,4,"""This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications.""",2024,2024-02-02T12:21:04Z,"Keyphrase: ""Reliability issues due to hallucination""","""This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications."" Keyphrase: ""Reliability issues due to hallucination"""
arxiv2024,Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models,Yes.,5,"""our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of L",2024,2024-02-02T12:07:00Z,"Keyphrase: ""Response variability syndrome""","""our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of L Keyphrase: ""Response variability syndrome"""
arxiv2024,Can MLLMs Perform Text-to-Image In-Context Learning?,Yes.,5,"""we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation.""",2024,2024-02-02T10:30:05Z,"Keyphrase: ""Multimodal complexity""","""we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation."" Keyphrase: ""Multimodal complexity"""
arxiv2024,Exploring the Limitations of Graph Reasoning in Large Language Models,Yes.,5,"""We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution.""",2024,2024-02-02T09:45:33Z,"Keyphrase: ""Bias in benchmarking""","""We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution."" Keyphrase: ""Bias in benchmarking"""
arxiv2024,"The Human and the Mechanical: logos, truthfulness, and ChatGPT",Yes.,5,"""Mechanical minds lack these two components",2024,2024-02-02T09:41:51Z,"Keyphrase: ""Lack of understanding context""","""Mechanical minds lack these two components Keyphrase: ""Lack of understanding context"""
arxiv2024,Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus,Yes.,4,"""yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources.""",2024,2024-02-02T06:44:22Z,"Keyphrase: ""Limited external knowledge integration""","""yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources."" Keyphrase: ""Limited external knowledge integration"""
arxiv2024,A Multi-Agent Conversational Recommender System,Yes.,4,"""Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address",2024,2024-02-02T04:20:13Z,"Keyphrase: ""Limited conversational recommendation capabilities""","""Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address Keyphrase: ""Limited conversational recommendation capabilities"""
arxiv2024,The Political Preferences of LLMs,Yes.,4,"""The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints."" and ""base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests.""",2024,2024-02-02T02:43:10Z,"Keyphrase: ""Political bias and suboptimal performance""","""The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints."" and ""base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests."" Keyphrase: ""Political bias and suboptimal performance"""
arxiv2024,LitLLM: A Toolkit for Scientific Literature Review,Yes.,5,"""Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on.""",2024,2024-02-02T02:41:28Z,"Keyphrase: ""Hallucination of non-actual information""","""Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on."" Keyphrase: ""Hallucination of non-actual information"""
arxiv2024,When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards,Yes.,5,"""Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details.""",2024,2024-02-01T19:12:25Z,"Keyphrase: ""Sensitivity to minute details""","""Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details."" Keyphrase: ""Sensitivity to minute details"""
arxiv2024,Evaluating Large Language Models for Generalization and Robustness via Data Compression,Yes.,5,"""Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation."" and ""We find that the compression rate of many models reduces significantly after their cutoff date,"" and ""Results also suggest that models struggle to generalize on news and code",2024,2024-02-01T18:56:18Z,"Keyphrase: ""Data contamination sensitivity""","""Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation."" and ""We find that the compression rate of many models reduces significantly after their cutoff date,"" and ""Results also suggest that models struggle to generalize on news and code Keyphrase: ""Data contamination sensitivity"""
arxiv2024,Can Large Language Models Understand Context?,Yes.,5,"""Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models"" and ""we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark.""",2024,2024-02-01T18:55:29Z,"Keyphrase: ""Struggles with nuanced contextual understanding""","""Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models"" and ""we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark."" Keyphrase: ""Struggles with nuanced contextual understanding"""
arxiv2024,Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents,Yes.,5,"""However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents.""",2024,2024-02-01T17:30:50Z,"Keyphrase: ""Uncontrollable content generation""","""However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents."" Keyphrase: ""Uncontrollable content generation"""
arxiv2024,Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement,Yes.,4,"""Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains.""",2024,2024-02-01T16:39:51Z,"Keyphrase: ""Limited interpretability and control""","""Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains."" Keyphrase: ""Limited interpretability and control"""
arxiv2024,Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing,Yes.,5,"""However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process... the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training.""",2024,2024-02-01T15:18:33Z,"Keyphrase: ""Inefficient reasoning process""","""However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process... the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training."" Keyphrase: ""Inefficient reasoning process"""
arxiv2024,Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks,Yes.,5,"""We uncover that Self-Generated attacks pose a significant threat, reducing LVLM(s) classification performance by up to 33%.""",2024,2024-02-01T14:41:20Z,"Keyphrase: ""Vulnerability to self-generated attacks""","""We uncover that Self-Generated attacks pose a significant threat, reducing LVLM(s) classification performance by up to 33%."" Keyphrase: ""Vulnerability to self-generated attacks"""
arxiv2024,Actor Identification in Discourse: A Challenge for LLMs?,Yes.,5,"""Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output.""",2024,2024-02-01T14:30:39Z,"Keyphrase: ""Struggles with generating correct canonical forms""","""Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output."" Keyphrase: ""Struggles with generating correct canonical forms"""
arxiv2024,Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection,Yes.,4,"""Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises.""",2024,2024-02-01T08:11:56Z,"Keyphrase: ""Risk of plagiarism and fake news""","""Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises."" Keyphrase: ""Risk of plagiarism and fake news"""
arxiv2024,Investigating Bias Representations in Llama 2 Chat via Activation Steering,Yes.,4,"""We address the challenge of societal bias in Large Language Models (LLMs),"" and ""Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF).""",2024,2024-02-01T07:48:50Z,"Keyphrase: ""Inherent gender bias""","""We address the challenge of societal bias in Large Language Models (LLMs),"" and ""Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF)."" Keyphrase: ""Inherent gender bias"""
arxiv2024,"Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",Yes.,5,"""knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge"" and ""Motivated by their failures in self-reflection and over-reliance on held-out sets"".",2024,2024-02-01T06:11:49Z,"Keyphrase: ""Knowledge gap and outdated information""","""knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge"" and ""Motivated by their failures in self-reflection and over-reliance on held-out sets"". Keyphrase: ""Knowledge gap and outdated information"""
arxiv2024,Safety of Multimodal Large Language Models on Images and Text,Yes.,4,"""the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios.""",2024,2024-02-01T05:57:10Z,"Keyphrase: ""Safety risks in real-world deployment""","""the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios."" Keyphrase: ""Safety risks in real-world deployment"""
arxiv2024,"Redefining ""Hallucination"" in LLMs: Towards a psychology-informed framework for mitigating misinformation",Yes.,5,"""a notable challenge surfaces in the form of 'hallucinations.' This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base.""",2024,2024-02-01T03:01:11Z,"Keyphrase: ""Misinformation surface form hallucination""","""a notable challenge surfaces in the form of 'hallucinations.' This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base."" Keyphrase: ""Misinformation surface form hallucination"""
arxiv2024,Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective,Yes.,4,"""the absence of explicit explainability in LLMs significantly hinders their application in the social sciences.""",2024,2024-02-01T01:17:46Z,"Keyphrase: ""Lack of explainability""","""the absence of explicit explainability in LLMs significantly hinders their application in the social sciences."" Keyphrase: ""Lack of explainability"""
