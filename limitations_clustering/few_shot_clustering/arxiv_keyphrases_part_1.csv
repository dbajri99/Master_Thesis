Source ,Title,Talks about LLMs,Rate,Evidence,Year ,Date ,Keyphrase
arXIv2022,Knowledge Distillation of Transformer-based Language Models Revisited,Yes.,5,"""However, the large model size and high run-time latency are serious impediments to applying them in practice, especially on mobile phones and Internet of Things (IoT) devices.""",2022,2022-06-29T02:16:56Z,"Keyphrase: ""High runtime latency"""
arXIv2022,CC-Riddle: A Question Answering Dataset of Chinese Character Riddles,Yes.,5,"""The test results reveal that current language models still struggle to solve Chinese character riddles.""",2022,2022-06-28T06:23:13Z,"Keyphrase: ""Struggles with solving specific language challenges"""
arXIv2022,A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages,Yes.,4,"""concerns remain whether open-ended languages or text generated from these models reveal any biases toward a specific group of people, thereby risking the usability of a certain product.""",2022,2022-06-23T21:57:08Z,"Keyphrase: ""Bias towards specific groups"""
arXIv2022,BERT Rankers are Brittle: a Study using Adversarial Document Perturbations,Yes.,5,"""we argue that BERT-rankers are not immune to adversarial attacks targeting retrieved documents given a query,"" and ""we find that BERT-rankers heavily rely on the document start/head for relevance prediction, making the initial part of the document more susceptible to adversarial attacks.""",2022,2022-06-23T14:16:48Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2022,Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models,Yes.,4,"""This paper presents exploratory work on whether and to what extent biases against queer and trans people are encoded in large language models (LLMs) such as BERT."" and ""We found that BERT shows significant homophobic bias.""",2022,2022-06-23T05:30:47Z,"Keyphrase: ""Homophobic bias"""
arXIv2022,Don't Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information,Yes.,4,"""We aim to diminish the stereotypical bias in the representations while preserving the factual gender signal."" and ""Our filtering method shows that it is possible to decrease the bias of gender-neutral profession names without significant deterioration of language modeling capabilities.""",2022,2022-06-21T21:38:25Z,"Keyphrase: ""Bias reduction at the expense of language modeling capability"""
arXIv2022,Using cognitive psychology to understand GPT-3,Yes.,5,"""Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task.""",2022,2022-06-21T20:06:03Z,"Keyphrase: ""Failure in causal reasoning"""
arXIv2022,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,Yes.,5,"""Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models.""",2022,2022-06-21T16:15:27Z,"Keyphrase: ""Limited plan generation capabilities"""
arXIv2022,"Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",Yes.,4,"""we examine the connection between model size and its gender bias (specifically, occupational gender bias)."" and ""Our findings highlight the potential risks that can arise from increasing model size.""",2022,2022-06-20T15:52:40Z,"Keyphrase: ""Gender bias amplification"""
arXIv2022,Methods for Estimating and Improving Robustness of Language Models,Yes.,5,"""large language models (LLMs) suffer notorious flaws related to their preference for simple, surface-level textual relations over full semantic complexity of the problem"" and ""weak ability to generalise outside of the training domain.""",2022,2022-06-16T21:02:53Z,"Keyphrase: ""Weak generalization outside training domain"""
arXIv2022,Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,Yes.,5,"""However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful.""",2022,2022-06-16T17:28:01Z,"Keyphrase: ""Toxic, biased, and untruthful language"""
arXIv2022,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,Yes.,5,"""it is vital that we understand the present and near-future capabilities and limitations of language models"" and ""BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models"" and ""model performance and calibration both improve with scale, but are poor in absolute terms"" and ""social bias typically increases with scale in settings with ambiguous context, but this can be",2022,2022-06-09T17:05:34Z,"Keyphrase: ""Limited performance calibration"""
arXIv2022,Neuro-Symbolic Procedural Planning with Commonsense Prompting,Yes.,4,"""it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures"" and ""such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks.""",2022,2022-06-06T22:09:52Z,"Keyphrase: ""Spurious correlations and impaired generalization"""
arXIv2022,Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models,Yes.,5,"""However, models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is inevitable.""",2022,2022-06-05T20:02:30Z,"Keyphrase: ""Limited crosslingual detoxification"""
arXIv2022,Order-sensitive Shapley Values for Evaluating Conceptual Soundness of NLP Models,Yes.,5,"""Previous works show that deep NLP models are not always conceptually sound",2022,2022-06-01T02:30:12Z,"Keyphrase: ""Conceptually unsound"""
arXIv2022,Automatic Short Math Answer Grading via In-context Meta-learning,Yes.,4,"""However, these approaches have several key limitations, including i) they use pre-trained language models that are not well-adapted to educational subject domains and/or student-generated text and ii) they almost always train one model per question, ignoring the linkage across a question and result in a significant model storage problem due to the size of advanced language models.""",2022,2022-05-30T16:26:02Z,"Keyphrase: ""Limited adaptability to educational content and student-generated text"""
arXIv2022,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,Yes.,5,"""Its application to video generation is still facing many challenges",2022,2022-05-29T19:02:15Z,"Keyphrase: ""Challenges in video generation"""
arXIv2022,Quark: Controllable Text Generation with Reinforced Unlearning,Yes.,5,"""Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user.""",2022,2022-05-26T21:11:51Z,"Keyphrase: ""Offensive and toxic language with repetition"""
arXIv2022,Differentially Private Decoding in Large Language Models,Yes.,4,"""LLMs, while effective, have been shown to memorize instances of training data thereby potentially revealing private information processed during pre-training.""",2022,2022-05-26T20:50:58Z,"Keyphrase: ""Privacy concerns and memorization"""
arXIv2022,RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning,Yes.,5,"""In our experiments with RoBERTa and T5, we find that the models trained in prior works do not perform consistently on the different perturbations in RobustLR, thus showing that the models are not robust to the proposed logical perturbations. Further, we find that the models",2022,2022-05-25T09:23:50Z,"Keyphrase: ""Limited robustness to perturbations"""
arXIv2022,Perturbation Augmentation for Fairer NLP,Yes.,4,"""Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets."" and ""Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models.""",2022,2022-05-25T09:00:29Z,"Keyphrase: ""Unwanted social biases"""
arXIv2022,Memorization in NLP Fine-tuning Methods,Yes.,4,"""Large language models are shown to present privacy risks through memorization of training data,"" and ""we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different.""",2022,2022-05-25T05:49:31Z,"Keyphrase: ""Privacy risk and memorization"""
arXIv2022,Fine-tuned Language Models are Continual Learners,Yes.,5,"""these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.""",2022,2022-05-24T22:53:34Z,"Keyphrase: ""Limited task generalization"""
arXIv2022,On Measuring Social Biases in Prompt-Based Multi-Task Learning,Yes.,4,"""We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs."" and ""The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training,",2022,2022-05-23T20:01:20Z,"Keyphrase: ""Input encoding bias"""
arXIv2022,Challenges in Measuring Bias via Open-Ended Language Generation,Yes.,4,"""Researchers have devised numerous ways to quantify social biases vested in pretrained language models."" and ""We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings.""",2022,2022-05-23T19:57:15Z,"Keyphrase: ""Inconsistent bias measurement"""
arXIv2022,Outliers Dimensions that Disrupt Transformers Are Driven by Frequency,Yes.,5,"""While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon",2022,2022-05-23T15:19:09Z,"Keyphrase: ""Robustness issue with outlier phenomenon"""
arXIv2022,Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements,Yes.,4,"""We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce",2022,2022-05-23T15:05:27Z,"Keyphrase: ""Bias and realism in generated advertisements"""
arXIv2022,RL with KL penalties is better viewed as Bayesian inference,Yes.,5,"""We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse",2022,2022-05-23T12:47:13Z,"Keyphrase: ""Distribution collapse in finetuning"""
arXIv2022,Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers,Yes.,5,"""This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form.""",2022,2022-05-22T18:03:03Z,"Keyphrase: ""Limited reasoning ability"""
arXIv2022,Scaling Laws and Interpretability of Learning from Repeated Data,Yes.,5,"""Recent large language models have been trained on vast datasets, but also often on repeated data... Some works have reported substantial negative performance effects of this repeated data."" and ""We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance.""",2022,2022-05-21T02:14:27Z,"Keyphrase: ""Negative performance effect of repeated data"""
arXIv2022,Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation,Yes.,4,"""Our results show that GPT-2 amplifies bias by considering women as junior and men as senior more often than the ground truth in both domains. These results suggest that NLP applications built using GPT-2 may harm women in professional capacities.""",2022,2022-05-19T20:05:02Z,"Keyphrase: ""Amplification of gender bias"""
arXIv2022,Overcoming Language Disparity in Online Content Classification with Multimodal Learning,Yes.,4,"""Large language models are now the standard to develop state-of-the-art solutions for text detection and classification tasks. However, the development of advanced computational techniques and resources is disproportionately focused on the English language, sidelining a majority of the languages spoken globally."" and ""we situate our findings with respect",2022,2022-05-19T17:56:02Z,"Keyphrase: ""English-centric bias"""
arXIv2022,Are Prompt-based Models Clueless?,Yes.,4,"""models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets"" and ""Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues.""",2022,2022-05-19T02:47:58Z,"Keyphrase: ""Dependency on superficial cues"""
arXIv2022,Transformer-based Program Synthesis for Low-Data Environments,Yes.,5,"""However, these models perform poorly on long-horizon and low-data tasks, and often don't seem to understand the semantics of the languages they generate.""",2022,2022-05-18T23:33:33Z,"Keyphrase: ""Poor performance on long-horizon low-data tasks and lack of semantic understanding"""
arXIv2022,"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",Yes.,5,"""We find that humans are far more robust than LLMs on this benchmark.""",2022,2022-05-11T18:14:33Z,"Keyphrase: ""Robustness challenge"""
arXIv2022,Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence,Yes.,5,"""much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property"" and ""we observe that PLMs violate the LNP frequently.""",2022,2022-05-08T08:37:36Z,"Keyphrase: ""Violation of linguistic norms and principles"""
arXIv2022,"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",Yes.,5,"""We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities.""",2022,2022-05-06T20:49:27Z,"Keyphrase: ""Limited entity tracking ability"""
arXIv2022,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,Yes.,5,"""We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations.""",2022,2022-05-06T17:57:58Z,"Keyphrase: ""Unreliable explanations"""
arXIv2022,Provably Confidential Language Modelling,Yes.,4,"""Large language models are shown to memorize privacy information such as social security numbers in training data.""",2022,2022-05-04T02:33:45Z,"Keyphrase: ""Privacy information memorization"""
arXIv2022,"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",Yes.,5,"""Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach.""",2022,2022-05-01T11:01:28Z,"Keyphrase: ""Inherent limitations in system approach"""
arXIv2022,Training Language Models with Language Feedback,Yes.,5,"""Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries.""",2022,2022-04-29T15:06:58Z,"Keyphrase: ""Generation of offensive and factually incorrect text"""
arXIv2022,Inferring Implicit Relations in Complex Questions with Language Models,Yes.,5,"""we investigate why current models struggle with implicit reasoning question answering (QA) tasks,"" and ""we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations.""",2022,2022-04-28T21:00:54Z,"Keyphrase: ""Struggles with implicit reasoning"""
arXIv2022,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,Yes.,5,"""the in-depth analysis of when in-context learning occurs is still lacking"" and ""in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning"" and ""pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in",2022,2022-04-28T13:59:54Z,"Keyphrase: ""Limited in-context learning"""
arXIv2022,You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas,Yes.,4,"""privacy concerns have arisen recently",2022,2022-04-26T09:36:18Z,"Keyphrase: ""Privacy concerns"""
arXIv2022,KALA: Knowledge-Augmented Language Model Adaptation,Yes.,4,"""Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM",2022,2022-04-22T08:11:59Z,"Keyphrase: ""Limited domain-specific knowledge"""
arXIv2022,You Are What You Write: Preserving Privacy in the Era of Large Language Models,Yes.,4,"""Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e",2022,2022-04-20T11:12:53Z,"Keyphrase: ""Trust and privacy concerns"""
arXIv2022,What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment,Yes.,5,"""The capabilities of large transformer models as instruction learners, however, remain poorly understood."" and ""our model, a fine-tuned T5-based text2text transformer, struggles with large regular languages, suggesting that less precise instructions are challenging for models. Additionally, instruction executions that require tracking longer contexts of prior steps are also more difficult.""",2022,2022-04-19T22:11:47Z,"Keyphrase: ""Challenges with precise instruction execution"""
arXIv2022,Pathologies of Pre-trained Language Models in Few-shot Fine-tuning,Yes.,5,"""without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels"" and ""pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot",2022,2022-04-17T15:55:18Z,"Keyphrase: ""Prediction bias and few-shot learning issues"""
arXIv2022,Building Markovian Generative Architectures over Pretrained LM Backbones for Efficient Task-Oriented Dialog Systems,Yes.,5,"""A drawback of existing PLM-based models is their non-Markov architectures across turns, i.e., the whole history is used as the conditioning input at each turn. First, this brings inefficiencies in memory and computation. Furthermore, using the whole history increases model",2022,2022-04-13T15:21:34Z,"Keyphrase: ""Inefficient non-Markov architecture"""
arXIv2022,Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Yes.,4,"""current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets.""",2022,2022-04-13T10:32:03Z,"Keyphrase: ""Limited linguistic skill mapping"""
arXIv2022,Impossible Triangle: What's Next for Pre-trained Language Models?,Yes.,5,"""However, many of such models come with a dauntingly huge size that few institutions can afford to pre-train, fine-tune or even deploy, while moderate-sized models usually lack strong generalized few-shot learning capabilities."" and ""We argue that all existing PLM models lack one or more properties from the Impossible Triangle.""",2022,2022-04-13T01:28:18Z,"Keyphrase: ""Limited few-shot learning capability"""
arXIv2022,Uniform Complexity for Text Generation,Yes.,5,"""existing models still do not capture factors that contribute to producing consistent text"" and ""we find that models such as GPT-2 struggle to preserve the complexity of input prompts used in its generations, even if finetuned with professionally written texts.""",2022,2022-04-11T15:19:47Z,"Keyphrase: ""Struggles with preserving input complexity"""
arXIv2022,Testing the limits of natural language models for predicting human language judgments,Yes.,5,"""experiments also revealed significant shortcomings of its alignment with human perception.""",2022,2022-04-07T17:12:57Z,"Keyphrase: ""Misalignment with human perception"""
arXIv2022,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",Yes.,5,"""a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment.""",2022,2022-04-04T17:57:11Z,"Keyphrase: ""Lack of real-world experience"""
arXIv2022,Mitigating Gender Bias in Machine Translation through Adversarial Learning,Yes.,4,"""restructuring training objectives in the context of fine-tuning pretrained large language models"" and ""addresses these challenges to mitigate gender bias in seq2seq machine translation.""",2022,2022-03-20T23:35:09Z,"Keyphrase: ""Gender bias mitigation challenge"""
arXIv2022,Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists,Yes.,4,"""Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability."" and ""severe unintended bias, and lower performance.""",2022,2022-03-17T09:29:50Z,"Keyphrase: ""Overfitting and unintended bias"""
arXIv2022,Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again,Yes.,5,"""However, our results show that GPT-3 still significantly underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3 in-context learning also yields smaller gains in accuracy when more training data becomes available. Our in-depth analyses further reveal issues of the in-context",2022,2022-03-16T05:56:08Z,"Keyphrase: ""Underperformance compared to finetuning smaller models"""
arXIv2022,Do Language Models Plagiarize?,Yes.,5,"""Given that a majority of LMs' training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indis",2022,2022-03-15T03:11:11Z,"Keyphrase: ""Ethical implications of data scraping"""
arXIv2022,"When classifying grammatical role, BERT doesn't care about word order... except when it matters",Yes.,4,"""Recent work has shown large language models to be surprisingly word order invariant,"" and ""highlight how models use context in the uncommon, but critical, instances where it matters.""",2022,2022-03-11T19:00:15Z,"Keyphrase: ""Word order invariance"""
arXIv2022,Speciesist Language and Nonhuman Animal Bias in English Masked Language Models,Yes.,4,"""We found that pre-trained masked language models tend to associate harmful words with nonhuman animals and have a bias toward using speciesist language for some nonhuman animal names.""",2022,2022-03-10T03:32:29Z,"Keyphrase: ""Speciesist language bias"""
arXIv2022,Training language models to follow instructions with human feedback,Yes.,5,"""large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user.""",2022,2022-03-04T07:04:42Z,"Keyphrase: ""Untruthful and toxic outputs"""
arXIv2022,Logical Fallacy Detection,Yes.,5,"""We find that existing pretrained large language models perform poorly on this task.""",2022,2022-02-28T13:18:26Z,"Keyphrase: ""Poor task performance"""
arXIv2022,Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies,Yes.,5,"""We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens.""",2022,2022-02-24T19:00:39Z,"Keyphrase: ""Vocabulary misalignment"""
arXIv2022,Capturing Failures of Large Language Models via Human Cognitive Biases,Yes.,5,"""To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases -- systematic patterns of deviation from rational judgement."" and ""Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave.""",2022,2022-02-24T18:58:52Z,"Keyphrase: ""Limited understanding of cognitive biases"""
arXIv2022,Reward Modeling for Mitigating Toxicity in Transformer-based Language Models,Yes.,4,"""language models that are pretrained on large unlabeled web text corpora have been shown to suffer from degenerating toxic content and social bias behaviors, consequently hindering their safe deployment.""",2022,2022-02-19T19:26:22Z,"Keyphrase: ""Degenerating toxic content and social bias"""
arXIv2022,Quantifying Memorization Across Neural Language Models,Yes.,5,"""Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality),",2022,2022-02-15T18:48:31Z,"Keyphrase: ""Undesirable memorization"""
arXIv2022,Deduplicating Training Data Mitigates Privacy Risks in Language Models,Yes.,5,"""Past work has shown that large language models are susceptible to privacy attacks,"" and ""Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks.""",2022,2022-02-14T08:20:15Z,"Keyphrase: ""Privacy vulnerabilities"""
arXIv2022,Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,Yes.,4,"""Pre-trained language models (LMs) are shown to easily generate toxic language."" and ""We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify.""",2022,2022-02-08T22:10:40Z,"Keyphrase: ""Toxic language generation"""
arXIv2022,Survey of Hallucination in Natural Language Generation,Yes.,4,"""deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios"" and ""hallucinations in large language models (LLMs).""",2022,2022-02-08T03:55:01Z,"Keyphrase: ""Unintended text hallucination"""
arXIv2022,What Has Been Enhanced in my Knowledge-Enhanced Language Model?,Yes.,4,"""Pretrained language models (LMs) do not capture factual knowledge very well."" and ""it is unclear how and what kind of knowledge is effectively integrated into these models and if such integration may lead to catastrophic forgetting of already learned knowledge.""",2022,2022-02-02T11:23:36Z,"Keyphrase: ""Catastrophic forgetting"""
arXIv2022,Unveiling Project-Specific Bias in Neural Code Models,Yes.,5,"""Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data.""",2022,2022-01-19T02:09:48Z,"Keyphrase: ""Limited generalization to out-of-distribution data"""
arXIv2022,Unintended Bias in Language Model-driven Conversational Recommendation,Yes.,5,"""pretrained LMs are well-known to be prone to intrinsic biases in their training data,"" and ""raises a red flag that advances in the language handling capability of LM-driven CRSs do not come without significant challenges related to mitigating unintended bias.""",2022,2022-01-17T05:50:14Z,"Keyphrase: ""Intrinsic bias"""
arXIv2022,Memory-assisted prompt editing to improve GPT-3 after deployment,Yes.,5,"""Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans.""",2022,2022-01-16T10:11:37Z,"Keyphrase: ""Mistakes comparable to humans"""
arXIv2022,Submix: Practical Private Prediction for Large-Scale Language Models,Yes.,4,"""Recent data-extraction attacks have exposed that language models can memorize some training samples verbatim. This is a vulnerability that can compromise the privacy of the model's training data.""",2022,2022-01-04T04:23:38Z,"Keyphrase: ""Memorization of training data"""
arXIv2022,A Survey on In-context Learning,Yes.,4,"""We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research.""",2022,2022-12-31T15:57:09Z,"Keyphrase: ""Lack of focus on practical application"""
arXIv2022,Inconsistencies in Masked Language Models,Yes.,5,"""However, this paper shows that distributions corresponding to different masking patterns can demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together."" and ""This fundamental flaw in MLMs can lead to self-contradictory behaviors during inference.""",2022,2022-12-30T22:53:25Z,"Keyphrase: ""Inconsistent masking patterns"""
arXIv2022,A Survey on Knowledge-Enhanced Pre-trained Language Models,Yes.,4,"""PLMs still face a number of challenges including poor interpretability, weak reasoning capability, and the need for a lot of expensive annotated data when applied to downstream tasks.""",2022,2022-12-27T09:54:14Z,"Keyphrase: ""Weak interpretability and reasoning"""
arXIv2022,Large Language Models Encode Clinical Knowledge,Yes.,5,"""human evaluation reveals key gaps in Flan-PaLM responses"" and ""Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.""",2022,2022-12-26T14:28:24Z,"Keyphrase: ""Limited clinical applicability"""
arXIv2022,Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,Yes.,5,"""These results suggest that the propensity of larger Transformer-based models to 'memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.""",2022,2022-12-23T03:57:54Z,"Keyphrase: ""Over-reliance on memorization"""
arXIv2022,Parallel Context Windows for Large Language Models,Yes.,5,"""When applied to processing long text, Large Language Models (LLMs) are limited by their context window.""",2022,2022-12-21T11:38:51Z,"Keyphrase: ""Limited context window"""
arXIv2022,Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners,Yes.,5,"""However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model.""",2022,2022-12-21T09:37:05Z,"Keyphrase: ""Limited training data scalability"""
arXIv2022,From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models,Yes.,4,"""However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task.""",2022,2022-12-21T08:39:36Z,"Keyphrase: ""Modality disconnection"""
arXIv2022,ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models,Yes.,4,"""Language models are generally trained on the publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting."" and ""We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions",2022,2022-12-21T07:06:55Z,"Keyphrase: ""Limited domain-specific generalization"""
arXIv2022,"Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",Yes.,5,"""Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking.""",2022,2022-12-21T04:43:19Z,"Keyphrase: ""Lack of pragmatic capability"""
arXIv2022,Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering,Yes.,5,"""While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (~50% for deletion intervention, and ~20% drop in accuracy for negation intervention)."" and ""But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models'",2022,2022-12-21T00:00:01Z,"Keyphrase: ""Semantic unfaithfulness"""
arXIv2022,Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing,Yes.,4,"""Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics."" and ""existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain",2022,2022-12-20T22:41:24Z,"Keyphrase: ""Harmful humanlike biases"""
arXIv2022,Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,Yes.,4,"""Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs."" and ""Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.""",2022,2022-12-20T18:59:23Z,"Keyphrase: ""Struggle with hierarchical reasoning"""
arXIv2022,When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,Yes.,5,"""large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge."" and ""We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail.""",2022,2022-12-20T18:30:15Z,"Keyphrase: ""Limited world knowledge"""
arXIv2022,Generic Temporal Reasoning with Differential Analysis and Explanation,Yes.,5,"""We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions.""",2022,2022-12-20T17:40:03Z,"Keyphrase: ""Overreliance on spurious information"""
arXIv2022,True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4,Yes.,5,"""GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area.""",2022,2022-12-20T09:34:43Z,"Keyphrase: ""Limited deep reasoning ability"""
arXIv2022,Do language models have coherent mental models of everyday things?,Yes.,5,"""Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent 'parts mental models' (54-59% accurate, 19-43% conditional constraint violation).""",2022,2022-12-20T06:54:04Z,"Keyphrase: ""Fragmented knowledge representation"""
arXIv2022,Discovering Language Model Behaviors with Model-Written Evaluations,Yes.,5,"""We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (""sycophancy"") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes",2022,2022-12-19T05:13:52Z,"Keyphrase: ""Inverse scaling issues"""
arXIv2022,Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,Yes.,5,"""Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse.""",2022,2022-12-18T19:27:41Z,"Keyphrase: ""Weak reasoning capabilities"""
arXIv2022,Language model acceptability judgements are not always robust to context,Yes.,4,"""we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context"" and ""we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures.""",2022,2022-12-18T00:11:06Z,"Keyphrase: ""Context matching limitations"""
arXIv2022,MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation,Yes.,5,"""these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency.""",2022,2022-12-16T17:36:23Z,"Keyphrase: ""Low semantic coverage and logical inconsistency"""
arXIv2022,Teaching Small Language Models to Reason,Yes.,4,"""However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters.""",2022,2022-12-16T11:24:42Z,"Keyphrase: ""Limited reasoning capability"""
arXIv2022,"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",Yes.,5,"""We find that zero-shot CoT reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output"" and ""Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.""",2022,2022-12-15T18:59:32Z,"Keyphrase: ""Harmful bias in sensitive domains"""
arXIv2022,"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety",Yes.,5,"""Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to 'explain their reasoning' often leads",2022,2022-12-13T00:29:45Z,"Keyphrase: ""Misleading average performance"""
arXIv2022,Understanding How Model Size Affects Few-shot Instruction Prompting,Yes.,5,"""Large Language Models are affected by the phenomena of memorizing and forgetting their training data."" and ""We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes.""",2022,2022-12-04T19:59:52Z,"Keyphrase: ""Weak few-shot performance"""
arXIv2022,Event knowledge in large language models: the gap between the impossible and the unlikely,Yes.,4,"""However, LLMs show less consistent preferences for likely vs. unlikely events"" and ""highlight a gap between representations of possible/impossible and likely/unlikely events.""",2022,2022-12-02T23:43:18Z,"Keyphrase: ""Inconsistent event representation"""
arXIv2022,a survey on GPT-3,Yes.,4,"""We discuss some of the challenges that GPT-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers.""",2022,2022-12-01T20:24:19Z,"Keyphrase: ""Training complexity and bias"""
arXIv2022,Scientific and Creative Analogies in Pretrained Language Models,Yes.,5,"""We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.""",2022,2022-11-28T12:49:44Z,"Keyphrase: ""Struggles with complex analogies"""
arXIv2022,An Analysis of Social Biases Present in BERT Variants Across Multiple Languages,Yes.,4,"""Although large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reflect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings.""",2022,2022-11-25T23:38:08Z,"Keyphrase: ""Reflects human bias"""
arXIv2022,Validating Large Language Models with ReLM,Yes.,5,"""there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language.""",2022,2022-11-21T21:40:35Z,"Keyphrase: ""Data memorization bias"""
arXIv2022,Conceptor-Aided Debiasing of Large Language Models,Yes.,5,"""Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus."" and ""CI-BERT reduces the language model accuracy.""",2022,2022-11-20T21:24:48Z,"Keyphrase: ""Inherent social bias"""
arXIv2022,PAL: Program-aided Language Models,Yes.,5,"""While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly.""",2022,2022-11-18T18:56:13Z,"Keyphrase: ""Logical arithmetic mistakes"""
arXIv2022,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Yes.,5,"""While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information."" and ""However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries.""",2022,2022-11-15T18:50:34Z,"Keyphrase: ""Factually inconsistent summaries"""
arXIv2022,Large Language Models Struggle to Learn Long-Tail Knowledge,Yes.,5,"""However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely."" and ""while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data.""",2022,2022-11-15T18:49:27Z,"Keyphrase: ""Limited longtail knowledge"""
arXIv2022,GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective,Yes.,5,"""the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods"" and ""significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.""",2022,2022-11-15T11:53:55Z,"Keyphrase: ""Out-of-distribution generalization challenge"""
arXIv2022,UGIF: UI Grounded Instruction Following,Yes.,4,"""We compare the performance of different LLMs including PaLM and GPT-3 and find that the end-to-end task completion rate is 48% for English UI but the performance drops to 32% for other languages. We analyze the common failure modes of existing models on this task and point out areas for improvement.""",2022,2022-11-14T18:36:19Z,"Keyphrase: ""End-to-end task completion rate drop"""
arXIv2022,Does Debiasing Inevitably Degrade the Model Performance,Yes.,4,"""Gender bias in language models has attracted sufficient attention because it threatens social justice. However, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious.""",2022,2022-11-14T13:46:13Z,"Keyphrase: ""Degraded performance due to debiasing methods"""
arXIv2022,DocuT5: Seq2seq SQL Generation with Table Documentation,Yes.,4,"""Current SQL generators based on pre-trained language models struggle to answer complex questions requiring domain context or understanding fine-grained table structure.""",2022,2022-11-11T13:31:55Z,"Keyphrase: ""Limited domain context understanding"""
arXIv2022,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,Yes.,5,"""Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world.""",2022,2022-11-10T07:48:01Z,"Keyphrase: ""Limited real-world understanding"""
arXIv2022,Large Language Models with Controllable Working Memory,Yes.,5,"""We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size.""",2022,2022-11-09T18:58:29Z,"Keyphrase: ""Poor controllability and robustness"""
arXIv2022,Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind,Yes.,5,"""In comparison, our systems based on either state-of-the-art large language models (GPT-4) or meta-learning algorithms lags >20% behind, highlighting a notable limitation in existing approaches' ToM capabilities.""",2022,2022-11-09T05:06:12Z,"Keyphrase: ""Lagging behind in capability"""
arXIv2022,Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic,Yes.,5,"""Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning.""",2022,2022-11-03T18:53:30Z,"Keyphrase: ""Limited numeric comprehension"""
arXIv2022,LMentry: A Language Model Benchmark of Elementary Language Tasks,Yes.,5,"""Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI's latest 175B-parameter instruction-tuned model, TextDavinci002.""",2022,2022-11-03T18:01:12Z,"Keyphrase: ""Wide variety of failure cases"""
arXIv2022,Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer,Yes.,5,"""They impose, however, limits on the maximum input length (512 sub-words in BERT), which are too restrictive in the legal domain. Even sparse-attention models, such as Longformer and BigBird, which increase the maximum input length to 4,096 sub-",2022,2022-11-02T09:27:01Z,"Keyphrase: ""Input length limitations"""
arXIv2022,Two-stage LLM Fine-tuning with Less Specialization and More Generalization,Yes.,5,"""fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances,"" and ""format specialization, where the model overfits to the format of the fine-tuned task.""",2022,2022-11-01T17:56:57Z,"Keyphrase: ""Overfitting to specialized datasets"""
arXIv2022,The future is different: Large pre-trained language models fail in prediction tasks,Yes.,5,"""Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time."" and ""we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the",2022,2022-11-01T11:01:36Z,"Keyphrase: ""Performance drop with distribution shift"""
arXIv2022,Emergent Linguistic Structures in Neural Networks are Fragile,Yes.,5,"""Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structures."" and ""Our key observation is that emergent syntactic representations in neural networks are brittle.""",2022,2022-10-31T15:43:57Z,"Keyphrase: ""Brittle syntactic representation"""
arXIv2022,"A Simple, Yet Effective Approach to Finding Biases in Code Generation",Yes.,4,"""This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.""",2022,2022-10-31T15:06:15Z,"Keyphrase: ""Undesired bias in code generation"""
arXIv2022,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,Yes.,5,"""Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., the language model pre-trained on static data from past years performs worse over time on emerging data.""",2022,2022-10-31T08:12:41Z,"Keyphrase: ""Temporal generalization limitations"""
arXIv2022,Contrastive Decoding: Open-ended Text Generation as Optimization,Yes.,5,"""maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text"" and ""sampling can often produce incoherent text that drifts from the original topics.""",2022,2022-10-27T00:58:21Z,"Keyphrase: ""Incoherent text generation"""
arXIv2022,Privately Fine-Tuning Large Language Models with Differential Privacy,Yes.,4,"""However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs.""",2022,2022-10-26T21:18:31Z,"Keyphrase: ""Privacy concerns due to data exposure"""
arXIv2022,Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence,Yes.,4,"""We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledge in their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally.""",2022,2022-10-25T01:46:00Z,"Keyphrase: ""Reliance on nonparametric knowledge"""
arXIv2022,Speeding Up Question Answering Task of Language Models via Inverted Index,Yes.,4,"""Despite the wide popularity of large language models (LLMs), few real-world conversational agents take advantage of LLMs. Extensive resources consumed by LLMs disable developers from integrating them into end-user applications.""",2022,2022-10-24T19:59:17Z,"Keyphrase: ""Resource consumption limitations"""
arXIv2022,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,Yes.,5,"""We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box,"" and ""Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively,"" and """,2022,2022-10-24T14:58:58Z,"Keyphrase: ""Lack of social intelligence"""
arXIv2022,Do Language Models Understand Measurements?,Yes.,5,"""In this study, we show that PLMs lack the capability required for reasoning over measurements.""",2022,2022-10-23T10:52:52Z,"Keyphrase: ""Lack of reasoning capability"""
arXIv2022,WikiWhy: Answering and Explaining Cause-and-Effect Questions,Yes.,4,"""GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.""",2022,2022-10-21T17:59:03Z,"Keyphrase: ""Room for improvement in correctness"""
arXIv2022,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Yes.,5,"""the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.""",2022,2022-10-21T15:12:37Z,"Keyphrase: ""Overreliance on shallow patterns"""
arXIv2022,LittleBird: Efficient Faster & Longer Transformer for Question Answering,Yes.,5,"""BERT has shown a lot of success in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism.""",2022,2022-10-21T10:46:41Z,"Keyphrase: ""Limitation in handling long inputs"""
arXIv2022,Towards a neural architecture of language: Deep learning versus logistics of access in neural architectures for compositional processing,Yes.,5,"""I will argue that these models are not suitable as neural models of human language. Firstly, because they fail on fundamental boundary conditions, such as the amount of learning they require. This would in fact imply that the mechanisms of GPT and brain language processing are fundamentally different. Secondly, because they do not possess the logistics of access needed for compositional and productive human language processing.""",2022,2022-10-19T13:31:26Z,"Keyphrase: ""Misalignment with human language processing"""
arXIv2022,SafeText: A Benchmark for Exploring Physical Safety in Language Models,Yes.,5,"""We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice.""",2022,2022-10-18T17:59:31Z,"Keyphrase: ""Susceptibility to generating unsafe text"""
arXIv2022,Prompting GPT-3 To Be Reliable,Yes.,4,"""the crucial problem of how to improve the reliability of GPT-3 is still under-explored"" and ""we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important",2022,2022-10-17T14:52:39Z,"Keyphrase: ""Reliability challenges"""
arXIv2022,"RARR: Researching and Revising What Language Models Say, Using Language Models",Yes.,5,"""However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence.""",2022,2022-10-17T03:44:30Z,"Keyphrase: ""Lack of transparency and trustworthiness"""
arXIv2022,Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,Yes.,4,"""Several studies have explored these harms and called for their mitigation via development of safer, fairer models."" and ""this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models.""",2022,2022-10-14T10:43:39Z,"Keyphrase: ""Societal harm potential"""
arXIv2022,BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation,Yes.,4,"""it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern on the fairness of PLMs as metrics"" and ""We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely",2022,2022-10-14T08:24:11Z,"Keyphrase: ""Societal bias encoding"""
arXIv2022,"""John is 50 years old, can his son be 65?"" Evaluating NLP Models' Understanding of Feasibility",Yes.,5,"""Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities."" and ""We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly.""",2022,2022-10-14T02:46:06Z,"Keyphrase: ""Failure in complex reasoning"""
arXIv2022,SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,Yes.,4,"""A common limitation of diagnostic tests for detecting social biases in NLP models"" and ""we are able to uncover the model's stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations",2022,2022-10-13T18:04:48Z,"Keyphrase: ""Limited ability to detect and address social biases"""
arXIv2022,Assessing Out-of-Domain Language Model Performance from Few Examples,Yes.,5,"""While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts.""",2022,2022-10-13T04:45:26Z,"Keyphrase: ""Unpredictable domain shift"""
arXIv2022,SEAL : Interactive Tool for Systematic Error Analysis and Labeling,Yes.,4,"""However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation.""",2022,2022-10-11T23:51:44Z,"Keyphrase: ""Failure on tail data"""
arXIv2022,Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models,Yes.,4,"""Here we show another problem with multilingual models",2022,2022-10-11T17:06:38Z,"Keyphrase: ""Challenges with multilingual capabilities"""
arXIv2022,A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models,Yes.,5,"""Despite the remarkable success of pre-trained language models (PLMs), they still face two challenges",2022,2022-10-11T07:26:34Z,"Keyphrase: ""Persistent challenges"""
arXIv2022,Generating Executable Action Plans with Environmentally-Aware Language Models,Yes.,5,"""However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints.""",2022,2022-10-10T18:56:57Z,"Keyphrase: ""Ambiguity in executable plans"""
arXIv2022,Quantifying Social Biases Using Templates is Unreliable,Yes.,5,"""Recently, there has been an increase in efforts to understand how large language models (LLMs) propagate and amplify social biases."" and ""Our results indicate that quantifying fairness in LLMs, as done in current practice, can be brittle and needs to be approached with more care and caution.""",2022,2022-10-09T20:05:29Z,"Keyphrase: ""Amplification of social bias"""
arXIv2022,Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems,Yes.,5,"""However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization.""",2022,2022-10-07T11:16:45Z,"Keyphrase: ""Poor out-of-domain generalization"""
arXIv2022,Measuring and Narrowing the Compositionality Gap in Language Models,Yes.,5,"""we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.""",2022,2022-10-07T06:50:23Z,"Keyphrase: ""Limited compositional reasoning"""
arXIv2022,Towards Improving Faithfulness in Abstractive Summarization,Yes.,4,"""Despite the success achieved in neural abstractive summarization based on pre-trained language models, one unresolved issue is that the generated summaries are not always faithful to the input document."" and ""the model over-relies on the language model to generate fluent but inadequate words.""",2022,2022-10-04T19:52:09Z,"Keyphrase: ""Overreliance on language model for fluency over fidelity"""
arXIv2022,ThinkSum: Probabilistic reasoning over sets using large language models,Yes.,4,"""However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions.""",2022,2022-10-04T00:34:01Z,"Keyphrase: ""Limited reasoning capabilities"""
arXIv2022,Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,Yes.,5,"""However, they have difficulty with proof planning",2022,2022-10-03T21:34:32Z,"Keyphrase: ""Difficulty in planning"""
arXIv2022,A Non-monotonic Self-terminating Language Model,Yes.,5,"""However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling.""",2022,2022-10-03T00:28:44Z,"Keyphrase: ""Undesirable sequence generation"""
arXIv2022,On the Impossible Safety of Large AI Models,Yes.,5,"""Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues."" and ""we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.""",2022,2022-09-30T06:36:49Z,"Keyphrase: ""Serious security issues"""
arXIv2022,Unpacking Large Language Models with Conceptual Consistency,Yes.,5,"""While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency.""",2022,2022-09-29T20:55:57Z,"Keyphrase: ""Lack of conceptual consistency"""
arXIv2022,How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI,Yes.,4,"""While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society."" and ""We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat",2022,2022-09-27T18:44:41Z,"Keyphrase: ""Subgroup disparities"""
arXIv2022,Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour,Yes.,4,"""Gordon and Van Durme (2013) point out that LMs can thus suffer from reporting bias",2022,2022-09-26T15:45:23Z,"Keyphrase: ""Reporting bias"""
arXIv2022,Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,Yes.,5,"""By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions.""",2022,2022-09-26T14:05:10Z,"Keyphrase: ""Limited adherence to given instructions"""
arXIv2022,WinoDict: Probing language models for in-context word acquisition,Yes.,5,"""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs",2022,2022-09-25T05:30:13Z,"Keyphrase: ""Limited diachronic adaptation"""
arXIv2022,"A Case Report On The ""A.I. Locked-In Problem"": social concerns with modern NLP",Yes.,5,"""However, practical experimentation with GPT-3 shows that there is a recurring problem with these modern NLP systems, namely that they can 'get stuck' in the narrative so that further conversations, prompt executions or commands become futile. This is here referred to as the 'Locked-In Problem' and is exemplified with an experimental case report, followed by practical and social concerns that are accompanied with",2022,2022-09-22T16:39:35Z,"Keyphrase: ""Getting stuck in narratives"""
arXIv2022,Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling,Yes.,4,"""However, recent research has highlighted a variety of biases in pre-trained language models."" and ""the pre-trained German language models find substantial conceptual, racial, and gender bias and have significant changes in bias across conceptual and racial axes during fine-tuning on the peer-review data",2022,2022-09-21T13:08:16Z,"Keyphrase: ""Persistent biases after fine-tuning"""
arXIv2022,SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical Corpus,Yes.,5,"""However, since BERT is quadratic to the text length, the BERT model is difficult to be used directly on the long-text corpus."" and ""alleviating the time and space overflow problem of basic BERT on long-text data.""",2022,2022-09-13T05:49:10Z,"Keyphrase: ""Challenges with processing long texts"""
arXIv2022,Multilingual Transformer Language Model for Speech Recognition in Low-resource Languages,Yes.,5,"""It is challenging to train and deploy Transformer LMs for hybrid speech recognition 2nd pass re-ranking in low-resource languages due to (1) data scarcity in low-resource languages, (2) expensive computing costs for training and refreshing 100+ monolingual models, and (3) hosting inefficiency",2022,2022-09-08T21:40:41Z,"Keyphrase: ""Challenges in training and deploying for low-resource languages"""
arXIv2022,Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots,Yes.,5,"""We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too."" and ""This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users.""",2022,2022-09-07T20:45:41Z,"Keyphrase: ""Toxic response generation"""
arXIv2022,The Ethical Need for Watermarks in Machine-Generated Language,Yes.,4,"""The ethical imperative to not blur this distinction arises from the asemantic nature of large language models and from human projections of emotional and cognitive states on machines, possibly leading to manipulation, spreading falsehoods or emotional distress.""",2022,2022-09-07T13:09:44Z,"Keyphrase: ""Ethical manipulation and falsehood spreading"""
arXIv2022,Training a T5 Using Lab-sized Resources,Yes.,4,"""Training large neural language models on large datasets is resource- and time-intensive. These requirements create a barrier to entry, where those with fewer resources cannot build competitive models.""",2022,2022-08-25T13:55:16Z,"Keyphrase: ""Resource-intensive barrier to entry"""
arXIv2022,On Reality and the Limits of Language Data: Aligning LLMs with Human Norms,Yes.,5,"""their ability to understand the physical world using only language data remains a question"" and ""Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness.""",2022,2022-08-25T10:21:23Z,"Keyphrase: ""Limited understanding of physical world"""
arXIv2022,Shortcut Learning of Large Language Models in Natural Language Understanding,Yes.,5,"""However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness.""",2022,2022-08-25T03:51:39Z,"Keyphrase: ""Dataset bias and generalizability"""
arXIv2022,Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models,Yes.,5,"""Recent work shows that this limitation persists in state-of-the-art Transformer-based models."" and ""our findings show how these two complementary approaches enable remarkable sequence extrapolation and highlight a limitation of current architectures to effectively generalize without explicit surface form guidance.""",2022,2022-08-24T11:25:27Z,"Keyphrase: ""Limited generalization without explicit guidance"""
arXIv2022,"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",Yes.,4,"""We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types."" and ""We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs.""",2022,2022-08-23T23:37:14Z,"Keyphrase: ""Harmful and unethical outputs"""
arXIv2022,Interpreting Embedding Spaces by Conceptualization,Yes.,4,"""One major drawback of this type of representation is their incomprehensibility to humans."" and ""Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model.""",2022,2022-08-22T15:32:17Z,"Keyphrase: ""Incomprehensibility and hidden biases"""
arXIv2022,Selection Collider Bias in Large Language Models,Yes.,4,"""sample selection induced collider bias (selection collider bias) that can cause Large Language Models (LLMs) to learn unconditional dependence between entities that are unconditionally independent in the real world"" and ""selection collider bias can become amplified in underspecified learning tasks"".",2022,2022-08-22T05:38:15Z,"Keyphrase: ""Induced collider bias"""
arXIv2022,Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies,Yes.,5,"""A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior."" and ""the last TE reveals a 'hyper-accuracy distortion' present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.""",2022,2022-08-18T17:54:49Z,"Keyphrase: ""Hyperaccuracy distortion"""
arXIv2022,What is it like to program with artificial intelligence?,Yes.,4,"""We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.""",2022,2022-08-12T10:48:46Z,"Keyphrase: ""Challenges for end-user programming"""
arXIv2022,Interactive Code Generation via Test-Driven User-Intent Formalization,Yes.,5,"""users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics.""",2022,2022-08-11T17:41:08Z,"Keyphrase: ""Ambiguity in correctness"""
arXIv2022,Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts,Yes.,5,"""Our results show the need for additional debiasing of large language models to address higher-order schemas and associations.""",2022,2022-08-08T20:59:16Z,"Keyphrase: ""Higher-order schema bias"""
arXIv2022,Gender bias in (non)-contextual clinical word embeddings for stereotypical medical categories,Yes.,4,"""clinical embeddings carry a high degree of bias for some medical terms and diseases which is conflicting with medical literature. Having such an ill-founded relationship might cause harm in downstream applications that use clinical embeddings.""",2022,2022-08-02T10:02:21Z,"Keyphrase: ""Biased clinical embeddings"""
arXIv2022,When BERT Fails -- The Limits of EHR Classification,Yes.,5,"""Although they outperform baselines on readmission prediction, they are not infallible. Here, we look into one such failure case, and report patterns that lead to inferior predictive performance.""",2022,2022-07-26T17:18:24Z,"Keyphrase: ""Inferior predictive performance"""
arXIv2022,A Hazard Analysis Framework for Code Synthesis Large Language Models,Yes.,5,"""models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential.""",2022,2022-07-25T20:44:40Z,"Keyphrase: ""Alignment problems and misuse potential"""
arXIv2022,Selection Bias Induced Spurious Correlations in Large Language Models,Yes.,5,"""large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias.""",2022,2022-07-18T23:43:52Z,"Keyphrase: ""Dataset selection bias"""
arXIv2022,A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America,Yes.,4,"""Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them.""",2022,2022-07-14T01:07:55Z,Keyphrase: Lack of interpretability
arXIv2022,Exploring Length Generalization in Large Language Models,Yes.,5,"""We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale."" and ""We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.""",2022,2022-07-11T14:24:38Z,"Keyphrase: ""Generalization deficiency in longer tasks"""
arXIv2022,Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning,Yes.,4,"""Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue."" and ""we re-discover a bias-performance trade-off",2022,2022-07-06T06:20:35Z,"Keyphrase: ""Unaddressed bias-performance tradeoff"""
arXIv2023,Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,Yes.,5,"""We provide empirical results that show that these methods fail to generalize in very basic ways."" and ""We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons.""",2023,2023-06-30T23:44:51Z,"Keyphrase: ""Limited generalization ability"""
arXIv2023,Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models,Yes.,4,"""Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community.""",2023,2023-06-30T19:39:01Z,"Keyphrase: ""Perpetuation of stereotypes"""
arXIv2023,Preference Ranking Optimization for Human Alignment,Yes.,4,"""Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks",2023,2023-06-30T09:07:37Z,"Keyphrase: ""Misleading content"""
arXIv2023,Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game,Yes.,4,"""Our results showed that ChatGPT can respond well to conceptual questions, but struggled with decimal place values and number line problems."" and ""We conclude with a discussion of ChatGPT's strengths and weaknesses and suggest several venues for extending its use cases in digital teaching and learning.""",2023,2023-06-29T02:28:09Z,"Keyphrase: ""Struggles with numerical precision"""
arXIv2023,A negation detection assessment of GPTs: analysis with the xNot360 dataset,Yes.,5,"""Our findings expose a considerable performance disparity among the GPT models, with GPT-4 surpassing its counterparts and GPT-3.5 displaying a marked performance reduction. The overall proficiency of the GPT models in negation detection remains relatively modest, indicating that this task pushes the boundaries of their natural language understanding capabilities. We not only highlight the constraints of GPT models in handling negation but also",2023,2023-06-29T02:27:48Z,"Keyphrase: ""Negation handling constraint"""
arXIv2023,Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,Yes.,5,"""Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area."" and ""there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification.""",2023,2023-06-28T21:11:15Z,"Keyphrase: ""Lack of confidence calibration"""
arXIv2023,Towards Measuring the Representation of Subjective Global Opinions in Language Models,Yes.,4,"""By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases."" and ""When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily",2023,2023-06-28T17:31:53Z,"Keyphrase: ""Cultural bias in responses"""
arXIv2023,CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models,Yes.,4,"""Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bias in certain categories.""",2023,2023-06-28T14:14:44Z,"Keyphrase: ""Strong bias in certain categories"""
arXIv2023,Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task,Yes.,4,"""harmful biases are likely increasingly intertwined with those models"" and ""Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.""",2023,2023-06-27T08:36:35Z,"Keyphrase: ""Entrenched harmful bias"""
arXIv2023,WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,Yes.,5,"""We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias.""",2023,2023-06-26T22:07:33Z,"Keyphrase: ""Antiqueer bias"""
arXIv2023,Are aligned neural networks adversarially aligned?,Yes.,5,"""However, adversarial users can construct inputs which circumvent attempts at alignment."" and ""We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image.""",2023,2023-06-26T17:18:44Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Exploring the Robustness of Large Language Models for Solving Programming Problems,Yes.,5,"""Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.""",2023,2023-06-26T10:48:50Z,"Keyphrase: ""Sensitivity to superficial modifications"""
arXIv2023,Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning,Yes.,5,"""Language models still struggle on moral reasoning, despite their impressive performance in many other tasks."" and ""Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero",2023,2023-06-25T18:40:43Z,"Keyphrase: ""Limited moral reasoning"""
arXIv2023,On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions,Yes.,4,"""When treating more general cases, despite the power of LLMs, inherent ambiguity exists and limits their predictive power. We then summarize the challenges and recommend research directions on LLMs to treat the inherent ambiguity of TTP descriptions used in various cyber operations.""",2023,2023-06-24T21:08:15Z,"Keyphrase: ""Inherent ambiguity and limited predictive power"""
arXIv2023,Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?,Yes.,5,"""However, employing chain-of-thought prompting did not lead to noticeably improved performance on this task. Further, we demonstrated how to analyze GPT-4's predictions to identify and mitigate deficiencies in annotation guidelines, and subsequently improve the performance of the model. Finally, we observed that the model is quite brittle, as small formatting related changes in the prompt had a high impact on the predictions.""",2023,2023-06-24T08:48:24Z,"Keyphrase: ""Brittle behavior and sensitivity to formatting changes"""
arXIv2023,Knowledge-Infused Self Attention Transformers,Yes.,5,"""These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users.""",2023,2023-06-23T13:55:01Z,"Keyphrase: ""Hallucination and incorrect output"""
arXIv2023,ToolQA: A Dataset for LLM Question Answering with External Tools,Yes.,5,"""Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning.""",2023,2023-06-23T05:43:28Z,"Keyphrase: ""Weak numerical reasoning"""
arXIv2023,Visual Adversarial Examples Jailbreak Aligned Large Language Models,Yes.,5,"""First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs."" and ""we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification.""",2023,2023-06-22T22:13:03Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs,Yes.,5,"""LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence."" and ""all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement.""",2023,2023-06-22T17:31:44Z,"Keyphrase: ""Overconfidence in verbalizing"""
arXIv2023,Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models,Yes.,5,"""Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment.""",2023,2023-06-22T03:56:38Z,"Keyphrase: ""Struggles with financial context"""
arXIv2023,Identifying and Extracting Rare Disease Phenotypes with Large Language Models,Yes.,4,"""While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.""",2023,2023-06-22T03:52:12Z,"Keyphrase: ""Limited critical evaluation"""
arXIv2023,ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews,Yes.,5,"""We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than",2023,2023-06-21T22:00:03Z,"Keyphrase: ""Difficulty in addressing indirect feedback"""
arXIv2023,Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases,Yes.,5,"""Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.""",2023,2023-06-21T21:04:11Z,"Keyphrase: ""Struggles with bias issues"""
arXIv2023,Understanding Social Reasoning in Language Models with Language Models,Yes.,4,"""understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges",2023,2023-06-21T16:42:15Z,"Keyphrase: ""Limited theory of mind alignment"""
arXIv2023,Solving and Generating NPR Sunday Puzzles with Large Language Models,Yes.,5,"""We find no evidence that models can generate puzzles",2023,2023-06-21T13:23:48Z,"Keyphrase: ""Limited puzzle-solving ability"""
arXIv2023,Opportunities and Risks of LLMs for Scalable Deliberation with Polis,Yes.,5,"""LLM context limitations have a significant impact on insight and quality of these results.""",2023,2023-06-20T22:52:51Z,"Keyphrase: ""Limited contextual understanding"""
arXIv2023,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Yes.,5,"""we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history.""",2023,2023-06-20T17:24:23Z,"Keyphrase: ""Vulnerability to toxic and biased output"""
arXIv2023,Hallucination is the last thing you need,Yes.,5,"""The present offering with generative AI presents major obstacles in replicating this, as current models struggle to integrate and navigate such a complex interplay of understanding, experience, and fact-checking procedures."" and ""this often deflects the model's attention from the crucial legal facts, thereby resulting in hallucination.""",2023,2023-06-20T13:14:15Z,"Keyphrase: ""Deflects attention from crucial facts"""
arXIv2023,TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models,Yes.,4,"""It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs.""",2023,2023-06-20T12:53:39Z,"Keyphrase: ""Lack of comprehensive ethical analysis"""
arXIv2023,Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,Yes.,5,"""they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents.""",2023,2023-06-20T12:21:06Z,"Keyphrase: ""Difficulty in recalling facts"""
arXIv2023,Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts,Yes.,4,"""competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance.""",2023,2023-06-20T08:27:47Z,"Keyphrase: ""Underrepresented language performance disparity"""
arXIv2023,Evaluating the Zero-shot Robustness of Instruction-tuned Language Models,Yes.,5,"""We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re",2023,2023-06-20T03:48:51Z,"Keyphrase: ""Inconsistent performance with novel instructions"""
arXIv2023,Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset,Yes.,5,"""Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm."" and ""we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected",2023,2023-06-19T21:14:57Z,"Keyphrase: ""Misleading distractions"""
arXIv2023,RepoFusion: Training Code Models to Understand Your Repository,Yes.,5,"""Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions.""",2023,2023-06-19T15:05:31Z,"Keyphrase: ""Limited contextual understanding"""
arXIv2023,"News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking",Yes.,4,"""the AI models, despite showing promise, lag in comprehending the subtleties and contexts inherent in news information.""",2023,2023-06-18T04:30:29Z,"Keyphrase: ""Difficulty in comprehending subtlety and context"""
arXIv2023,ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation,Yes.,5,"""Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience.""",2023,2023-06-16T16:56:32Z,"Keyphrase: ""Limited real-world grounding"""
arXIv2023,Friend or Foe? Exploring the Implications of Large Language Models on the Science System,Yes.,4,"""The study focused on applications and limitations of LLMs,"" and ""risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education.""",2023,2023-06-16T15:50:17Z,"Keyphrase: ""Risk of bias and misinformation"""
arXIv2023,Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond,Yes.,5,"""However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered."" and ""Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection process and reasoning process.""",2023,2023-06-16T13:39:35Z,"Keyphrase: ""Limited logical reasoning capabilities"""
arXIv2023,Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody,Yes.,5,"""We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner.""",2023,2023-06-16T12:49:44Z,"Keyphrase: ""Underpredicting with longer context"""
arXIv2023,Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models,Yes.,4,"""We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models.""",2023,2023-06-16T10:36:18Z,"Keyphrase: ""Gender bias in language generation"""
arXIv2023,Pushing the Limits of ChatGPT on NLP Tasks,Yes.,5,"""its subpar performance was caused by the following factors",2023,2023-06-16T09:40:05Z,"Keyphrase: ""Subpar performance"""
arXIv2023,Clickbait Detection via Large Language Models,Yes.,5,"""Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods"" and ""the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.""",2023,2023-06-16T02:49:20Z,"Keyphrase: ""Limited performance compared to fine-tuned models"""
arXIv2023,Explaining Legal Concepts with Augmented Large Language Models (GPT-4),Yes.,4,"""we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation"" and ""detailed analysis uncovered limitations in terms of the factual accuracy of the explanations.""",2023,2023-06-15T21:58:18Z,"Keyphrase: ""Limited factual accuracy and explanation"""
arXIv2023,Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health,Yes.,4,"""We also find that the use of LLMs, like ChatGPT, in the fields of biomedicine and health entails various risks and challenges, including fabricated information in its generated responses, as well as legal and privacy concerns associated with sensitive patient data.""",2023,2023-06-15T20:19:08Z,"Keyphrase: ""Biased and risky responses"""
arXIv2023,Inverse Scaling: When Bigger Isn't Better,Yes.,5,"""Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data.""",2023,2023-06-15T20:11:23Z,"Keyphrase: ""Inverse scaling and worsened task performance"""
arXIv2023,"Explore, Establish, Exploit: Red Teaming Language Models from Scratch",Yes.,5,"""Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text."" and ""We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements.""",2023,2023-06-15T18:49:50Z,"Keyphrase: ""Hazardous output and false information"""
arXIv2023,Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models,Yes.,4,"""However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains.""",2023,2023-06-15T17:42:48Z,"Keyphrase: ""Copyright infringement and harmful content"""
arXIv2023,SCALE: Scaling up the Complexity for Advanced Language Model Evaluation,Yes.,5,"""Despite recent advances, efficiently processing long documents for intense review/analysis tasks remains an open challenge for language models."" and ""existing publicly available models struggle with most tasks, even after in-domain pretraining.""",2023,2023-06-15T16:19:15Z,"Keyphrase: ""Struggle with processing long documents"""
arXIv2023,CMMLU: Measuring massive multitask language understanding in Chinese,Yes.,5,"""The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs.""",2023,2023-06-15T15:49:51Z,"Keyphrase: ""Limited average accuracy"""
arXIv2023,DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning,Yes.,5,"""large language models (LLMs) exhibit poor performance in tackling this subjective domain"" and ""current models defect in the application of pragmatic reasoning.""",2023,2023-06-15T10:41:23Z,"Keyphrase: ""Limited pragmatic reasoning"""
arXIv2023,Language models are not naysayers: An analysis of language models on negation benchmarks,Yes.,5,"""we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.""",2023,2023-06-14T01:16:37Z,"Keyphrase: ""Negation handling limitations"""
arXIv2023,FLamE: Few-shot Learning from Natural Language Explanations,Yes.,4,"""recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification"" and ""human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.""",2023,2023-06-13T18:01:46Z,"Keyphrase: ""Inadequate explanations"""
arXIv2023,Questioning the Survey Responses of Large Language Models,Yes.,5,"""models' responses are governed by ordering and labeling biases,"" and ""models' responses do not contain the entropy variations and statistical signals typically found in human populations.""",2023,2023-06-13T17:48:27Z,"Keyphrase: ""Labeling bias and statistical signal variation"""
arXIv2023,Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control,Yes.,4,"""First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exempl",2023,2023-06-13T15:49:41Z,"Keyphrase: ""Limited context understanding"""
arXIv2023,SqueezeLLM: Dense-and-Sparse Quantization,Yes.,5,"""However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements.""",2023,2023-06-13T08:57:54Z,"Keyphrase: ""Unprecedented resource requirement"""
arXIv2023,Large Language Models Sometimes Generate Purely Negatively-Reinforced Text,Yes.,5,"""One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong",2023,2023-06-13T06:40:37Z,"Keyphrase: ""Limited understanding of reward signals"""
arXIv2023,TART: A plug-and-play Transformer module for task-agnostic reasoning,Yes.,4,"""In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples."" and ""this performance gap exists due to their inability to perform simple probabilistic reasoning tasks.""",2023,2023-06-13T04:37:00Z,"Keyphrase: ""Underperformance in probabilistic reasoning"""
arXIv2023,Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling,Yes.,5,"""LLMs fail at simple linguistic tests for negation or quantifier understanding"" and ""suggests that LLMs do not do as well as expected with quantifiers.""",2023,2023-06-12T19:20:18Z,"Keyphrase: ""Weak quantifier understanding"""
arXIv2023,Lost in Translation: Large Language Models in Non-English Content Analysis,Yes.,5,"""the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages."" and ""Part II accounts for the challenges of doing content analysis with large language models in general and multilingual language models in particular.""",2023,2023-06-12T19:10:47Z,"Keyphrase: ""Limited multilingual capabilities"""
arXIv2023,TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models,Yes.,4,"""the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined."" and ""Our work sheds light on the potential security risks in current models and offers a potential defensive approach.""",2023,2023-06-12T01:22:39Z,"Keyphrase: ""Insufficiently examined security implications"""
arXIv2023,A blind spot for large language models: Supradiegetic linguistic information,Yes.,5,"""its deficits can be reframed as ignorance of extradiegetic information, including supradiegetic linguistic information"" and ""We use these concepts to investigate why LLMs like ChatGPT have trouble handling palindromes, the visual characteristics of symbols, translating Sumerian cuneiform, and continuing integer sequences.""",2023,2023-06-11T22:15:01Z,"Keyphrase: ""Struggles with handling diverse information types"""
arXIv2023,Human-in-the-Loop through Chain-of-Thought,Yes.,5,"""it sometimes demonstrates its weakness in long-term or multi-step logical reasoning.""",2023,2023-06-10T04:31:57Z,"Keyphrase: ""Weak long-term logical reasoning"""
arXIv2023,Measuring and Modifying Factual Knowledge in Large Language Models,Yes.,4,"""existing approaches for knowledge measurement have certain limitations,"" and ""LLMs exhibit limitations in capturing new knowledge under specific circumstances for one of these methods.""",2023,2023-06-09T21:25:48Z,"Keyphrase: ""Limited in capturing new knowledge"""
arXIv2023,Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording,Yes.,5,"""LLMs are still not reliable,"" ""no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available,"" and ""The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability",2023,2023-06-09T19:07:31Z,"Keyphrase: ""Inconsistent responses"""
arXIv2023,Trapping LLM Hallucinations Using Tagged Context Prompts,Yes.,5,"""However, these models suffer from 'hallucinations,' where the model generates false or fabricated information.""",2023,2023-06-09T17:48:54Z,"Keyphrase: ""Hallucination of false information"""
arXIv2023,S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput,Yes.,5,"""Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself.""",2023,2023-06-09T16:13:43Z,"Keyphrase: ""Memory consumption scalability"""
arXIv2023,Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?,Yes.,5,"""vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text.""",2023,2023-06-09T13:03:53Z,"Keyphrase: ""Vulnerability to adversarial text"""
arXIv2023,Can Large Language Models Infer Causation from Correlation?,Yes.,5,"""Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but",2023,2023-06-09T12:09:15Z,"Keyphrase: ""Limited causal inference skills"""
arXIv2023,Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests,Yes.,4,"""At the same time, the results highlight the unreliability of LLMs",2023,2023-06-09T07:19:43Z,"Keyphrase: ""Unreliability"""
arXIv2023,Prompt Injection attack against LLM-integrated Applications,Yes.,5,"""highlighting the constraints of current attack strategies in practice"" and ""We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection.""",2023,2023-06-08T18:43:11Z,"Keyphrase: ""Vulnerability to prompt injection"""
arXIv2023,"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",Yes.,5,"""We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions.""",2023,2023-06-08T13:21:29Z,"Keyphrase: ""Struggles with multilingual and multimodal tasks"""
arXIv2023,Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures,Yes.,4,"""However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly.""",2023,2023-06-08T13:10:00Z,"Keyphrase: ""Difficulty with complex tasks and ambiguity"""
arXIv2023,Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models,Yes.,4,"""However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased."" and ""We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of",2023,2023-06-07T19:49:41Z,"Keyphrase: ""Biased surrogate labels"""
arXIv2023,Soft-prompt Tuning for Large Language Models to Evaluate Bias,Yes.,4,"""Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues.""",2023,2023-06-07T19:11:25Z,"Keyphrase: ""Bias towards certain groups"""
arXIv2023,ModuleFormer: Modularity Emerges from Mixture-of-Experts,Yes.,5,"""existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge.""",2023,2023-06-07T17:59:57Z,"Keyphrase: ""Difficulty in knowledge expansion"""
arXIv2023,The Two Word Test: A Semantic Benchmark for Large Language Models,Yes.,5,"""Results demonstrated that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the",2023,2023-06-07T17:22:03Z,"Keyphrase: ""Poor discrimination and meaningfulness"""
arXIv2023,Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,Yes.,4,"""Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.""",2023,2023-06-07T16:50:03Z,"Keyphrase: ""Propagating societal bias"""
arXIv2023,"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",Yes.,5,"""ChatGPT has not solved computational humor yet but it can be a big leap toward 'funny' machines.""",2023,2023-06-07T16:10:21Z,"Keyphrase: ""Limited humor understanding"""
arXIv2023,Long-form analogies generated by chatGPT lack human-like psycholinguistic properties,Yes.,5,"""These methods can be used to characterize the psycholinguistic properties of LLM output and illustrate areas where LLMs fall short in comparison to human-generated text.""",2023,2023-06-07T15:42:31Z,"Keyphrase: ""Inferior psycholinguistic properties"""
arXIv2023,PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts,Yes.,5,"""Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts.""",2023,2023-06-07T15:37:00Z,"Keyphrase: ""Robustness to adversarial prompts"""
arXIv2023,Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering,Yes.,5,"""However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.""",2023,2023-06-07T04:15:21Z,"Keyphrase: ""Insufficient internalized knowledge"""
arXIv2023,An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models,Yes.,4,"""The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases."" and ""are less effective when it comes to racial and religious bias, which may be attributed to",2023,2023-06-06T23:56:18Z,"Keyphrase: ""Inherent humanlike biases"""
arXIv2023,Certified Deductive Reasoning with Language Models,Yes.,5,"""Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, even when arriving at a correct final answer, their rationales are often logically unsound or inconsistent.""",2023,2023-06-06T21:49:00Z,"Keyphrase: ""Illogical reasoning"""
arXIv2023,MISGENDERED: Limits of Large Language Models in Understanding Pronouns,Yes.,5,"""We comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns."" and ""When prompted out-of-the-box, language models perform poorly at correctly predicting neo",2023,2023-06-06T18:27:52Z,"Keyphrase: ""Difficulty predicting gender-neutral pronouns"""
arXIv2023,ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory,Yes.,5,"""mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning.""",2023,2023-06-06T17:58:24Z,"Keyphrase: ""Limited memory capacity"""
arXIv2023,Deductive Verification of Chain-of-Thought Reasoning,Yes.,4,"""its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks.""",2023,2023-06-06T17:18:56Z,"Keyphrase: ""Limited complex reasoning ability"""
arXIv2023,Can large language models democratize access to dual-use biotechnology?,Yes.,5,"""However, these models may also confer easy access to dual-use technologies capable of inflicting great harm."" and ""Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training.""",2023,2023-06-06T15:52:05Z,"Keyphrase: ""Potential for misuse"""
arXIv2023,Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach,Yes.,5,"""interactions with LLMs can be time-consuming. In many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency.""",2023,2023-06-06T11:49:09Z,"Keyphrase: ""Resource-intensive deployment"""
arXIv2023,Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models,Yes.,4,"""The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation.""",2023,2023-06-06T08:47:42Z,"Keyphrase: ""Ethical code inadequacy"""
arXIv2023,Large Language Models of Code Fail at Completing Code with Potential Bugs,Yes.,5,"""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs.""",2023,2023-06-06T06:35:27Z,"Keyphrase: ""Bug-induced performance degradation"""
arXIv2023,A Static Evaluation of Code Completion by Large Language Models,Yes.,4,"""Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models.""",2023,2023-06-05T19:23:34Z,"Keyphrase: ""Common coding errors"""
arXIv2023,Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs,Yes.,5,"""Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone.""",2023,2023-06-05T17:55:05Z,"Keyphrase: ""Difficulty in fine-tuning and controlling"""
arXIv2023,Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset,Yes.,5,"""The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could",2023,2023-06-05T16:48:41Z,"Keyphrase: ""Disparity with human accuracy"""
arXIv2023,Exposing Bias in Online Communities through Large-Scale Language Models,Yes.,4,"""While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes."" and ""This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets",2023,2023-06-04T08:09:26Z,"Keyphrase: ""Reproduction of social bias"""
arXIv2023,Probing Physical Reasoning with Counter-Commonsense Context,Yes.,5,"""The results show that while large language models can use prepositions such as 'in' and 'into' in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.""",2023,2023-06-04T04:24:43Z,"Keyphrase: ""Limited inferencing capabilities"""
arXIv2023,AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap,Yes.,4,"""a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs"" and ""We reflect on the unique challenges that arise in providing transparency for LLMs.""",2023,2023-06-02T22:51:26Z,"Keyphrase: ""Lack of transparency"""
arXIv2023,Revisiting the Role of Language Priors in Vision-Language Models,Yes.,4,"""some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions"" and ""even a 'blind' language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago.""",2023,2023-06-02T19:19:43Z,"Keyphrase: ""Unnatural language distribution"""
arXIv2023,Knowledge of cultural moral norms in large language models,Yes.,4,"""We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms.""",2023,2023-06-02T18:23:35Z,"Keyphrase: ""Cross-cultural moral norm prediction disparity"""
arXIv2023,Evaluating Language Models for Mathematics through Interactions,Yes.,4,"""Static assessment fails to account for the essential interactive element in LLM deployment, and therefore limits how we understand language model capabilities."" and ""humans should be aware of language models' algebraic fallibility and discern where they are appropriate to use.""",2023,2023-06-02T17:12:25Z,"Keyphrase: ""Limited understanding of interactive elements"""
arXIv2023,Fine-Grained Human Feedback Gives Better Rewards for Language Model Training,Yes.,5,"""Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs.""",2023,2023-06-02T17:11:37Z,"Keyphrase: ""Undesirable text generation"""
arXIv2023,"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",Yes.,5,"""we discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis.""",2023,2023-06-02T12:47:45Z,"Keyphrase: ""Limitation in dementia diagnosis"""
arXIv2023,ChatGPT is a Remarkable Tool -- For Experts,Yes.,5,"""These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation.""",2023,2023-06-02T06:28:21Z,"Keyphrase: ""Limited logical reasoning and ethical concerns"""
arXIv2023,How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?,Yes.,5,"""we check for inconsistencies and hallucinations in the summaries"" and ""we often find inconsistent or hallucinated information in the generated abstractive summaries"" and ""our investigation indicates that the pre-trained abstractive summarization models and LLMs are not yet ready for fully automatic deployment for case judgement summarization; rather a human-in-the-loop approach including manual checks for inconsistencies is more",2023,2023-06-02T03:16:19Z,"Keyphrase: ""Inconsistent hallucination"""
arXIv2023,Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation,Yes.,5,"""However, when prompted to provide fine-grained classification, its performance drops to close to a simple most frequent class (MFC) baseline."" and ""illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks.""",2023,2023-06-01T22:43:37Z,"Keyphrase: ""Systematic error in classification"""
arXIv2023,Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study,Yes.,4,"""a closer examination of the texts generated by ChatGPT through human evaluations has shown that there are still critical issues in terms of text coherence, faithfulness, and style.""",2023,2023-06-01T21:58:33Z,"Keyphrase: ""Text coherence and faithfulness"""
arXIv2023,Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes,Yes.,5,"""Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions for the same action (e.g., marinate the meat vs. marinate the meat and leave overnight) and infrequently occurring patterns (e.g., add salt occurs far more frequently than",2023,2023-06-01T18:49:47Z,"Keyphrase: ""Difficulty with irregular data patterns"""
arXIv2023,Exposing Attention Glitches with Flip-Flop Language Modeling,Yes.,5,"""The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought."" and ""We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors,",2023,2023-06-01T17:44:35Z,"Keyphrase: ""Brittleness in long chain reasoning"""
arXIv2023,The feasibility of artificial consciousness through the lens of neuroscience,Yes.,4,"""the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us,"" and ""the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals.""",2023,2023-06-01T17:18:15Z,"Keyphrase: ""Lack of embodied information"""
arXIv2023,Rethinking Model Evaluation as Narrowing the Socio-Technical Gap,Yes.,4,"""The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with."" and ""we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods with an acknowledgment of trade-offs between realism to socio-requirements and pragmatic costs to conduct the evaluation.""",2023,2023-06-01T00:01:43Z,"Keyphrase: ""Challenges in realistic evaluation"""
arXIv2023,Automated Annotation with Generative AI Requires Validation,Yes.,4,"""their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans.""",2023,2023-05-31T20:50:45Z,"Keyphrase: ""Variability in performance due to annotation task challenges"""
arXIv2023,Let's Verify Step by Step,Yes.,5,"""However, even state-of-the-art models still regularly produce logical mistakes.""",2023,2023-05-31T17:24:00Z,"Keyphrase: ""Logical errors"""
arXIv2023,Red Teaming Language Model Detectors with Language Models,Yes.,4,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems.""",2023,2023-05-31T10:08:37Z,"Keyphrase: ""Safety and ethical risks"""
arXIv2023,Large Language Models Are Not Strong Abstract Reasoners,Yes.,5,"""However, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed."" and ""We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks.""",2023,2023-05-31T04:50:29Z,"Keyphrase: ""Opaque cognitive capability"""
arXIv2023,Self-Verification Improves Few-Shot Clinical Information Extraction,Yes.,4,"""However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health.""",2023,2023-05-30T22:05:11Z,"Keyphrase: ""Struggles with accuracy and interpretability"""
arXIv2023,The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code,Yes.,4,"""it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning.""",2023,2023-05-30T17:02:58Z,"Keyphrase: ""Limited causal reasoning capabilities"""
arXIv2023,Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate,Yes.,5,"""Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks,"" and ""our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem",2023,2023-05-30T15:25:45Z,"Keyphrase: ""Struggles with complex reasoning"""
arXIv2023,Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale,Yes.,5,"""there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the underlying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will",2023,2023-05-30T15:15:40Z,"Keyphrase: ""Limited language understanding"""
arXIv2023,"Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard",Yes.,5,"""However, for more complex mathematical problems or advanced logic tasks, their answers, although written in a usually 'convincing' way, may not be reliable. Consistency is also an issue, as many times a chatbot will provide conflicting answers when given the same question more than once.""",2023,2023-05-30T11:18:05Z,"Keyphrase: ""Inconsistent responses"""
arXIv2023,Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge,Yes.,4,"""these methods suffer from low knowledge coverage caused by PLM bias -- the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality -- only models using GPT-3 can achieve the best result.""",2023,2023-05-30T08:34:13Z,"Keyphrase: ""Low knowledge coverage"""
arXIv2023,Universality and Limitations of Prompt Tuning,Yes.,5,"""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer.""",2023,2023-05-30T06:47:07Z,"Keyphrase: ""Limited prompt tuning depth"""
arXIv2023,Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey,Yes.,4,"""However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications).""",2023,2023-05-30T03:00:30Z,"Keyphrase: ""Domain heterogeneity and complexity"""
arXIv2023,Faith and Fate: Limits of Transformers on Compositionality,Yes.,5,"""Yet, these models simultaneously show failures on surprisingly trivial problems."" and ""Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.""",2023,2023-05-29T23:24:14Z,"Keyphrase: ""Limited compositional reasoning"""
arXIv2023,How Effective Are Neural Networks for Fixing Security Vulnerabilities,Yes.,5,"""Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to",2023,2023-05-29T20:50:27Z,"Keyphrase: ""Limited vulnerability-fixing capability"""
arXIv2023,Do Language Models Know When They're Hallucinating References?,Yes.,5,"""State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda.""",2023,2023-05-29T17:12:03Z,"Keyphrase: ""Hallucinated information and inaccurate output"""
arXIv2023,Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models,Yes.,5,"""To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs."" and ""We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts.""",2023,2023-05-29T16:29:22Z,"Keyphrase: ""Higher rate of racial stereotypes"""
arXIv2023,Do Large Language Models Know What They Don't Know?,Yes.,5,"""Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend."" and ""Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.""",2023,2023-05-29T15:30:13Z,"Keyphrase: ""Limited comprehension and knowledge gap"""
arXIv2023,"Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations",Yes.,4,"""Subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and LOL",2023,2023-05-29T12:26:44Z,"Keyphrase: ""Security vulnerabilities"""
arXIv2023,Large Language Models are not Fair Evaluators,Yes.,5,"""we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models.""",2023,2023-05-29T07:41:03Z,"Keyphrase: ""Biased evaluation paradigms"""
arXIv2023,Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective,Yes.,4,"""We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework.""",2023,2023-05-28T16:04:48Z,"Keyphrase: ""Limited incorporation of human feedback"""
arXIv2023,Mitigating Label Biases for In-context Learning,Yes.,5,"""domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples.""",2023,2023-05-28T15:37:39Z,"Keyphrase: ""Domain label bias"""
arXIv2023,KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application,Yes.,4,"""Large language models (LLMs) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications.""",2023,2023-05-28T12:07:16Z,"Keyphrase: ""Social bias and demographic disparities"""
arXIv2023,SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration,Yes.,4,"""The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising.""",2023,2023-05-28T11:51:20Z,"Keyphrase: ""Generating offensive and biased content"""
arXIv2023,Evaluating GPT-3 Generated Explanations for Hateful Content Moderation,Yes.,5,"""A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators."" and ""this persuasiveness may result in incorrect judgments about the hatefulness of the content.""",2023,2023-05-28T10:05:13Z,"Keyphrase: ""Erroneous explanations"""
arXIv2023,Reward Collapse in Aligning Large Language Models,Yes.,5,"""we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training.""",2023,2023-05-28T02:12:00Z,"Keyphrase: ""Reward collapse"""
arXIv2023,Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,Yes.,5,"""Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks.""",2023,2023-05-27T19:08:04Z,"Keyphrase: ""Unwanted side effects in editing technique"""
arXIv2023,The Curse of Recursion: Training on Generated Data Makes Models Forget,Yes.,5,"""We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs.""",2023,2023-05-27T15:10:41Z,"Keyphrase: ""Catastrophic forgetting"""
arXIv2023,FERMAT: An Alternative to Accuracy for Numerical Reasoning,Yes.,5,"""While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning.""",2023,2023-05-27T15:00:45Z,"Keyphrase: ""Struggles with numerical reasoning"""
arXIv2023,Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning,Yes.,5,"""Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited."" and ""Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are 'lazy learners' that tend to exploit shortcuts in prompts for downstream tasks.""",2023,2023-05-26T20:56:30Z,"Keyphrase: ""Lazy learner tendency"""
arXIv2023,Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model,Yes.,5,"""Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model and Prometheus. Hallucinations were mostly associated with non-existent references or fabricated efficacy data.""",2023,2023-05-26T17:33:05Z,"Keyphrase: ""Hallucination of nonexistent references"""
arXIv2023,"LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",Yes.,5,"""GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly",2023,2023-05-26T16:32:17Z,"Keyphrase: ""Limited reasoning capacity"""
arXIv2023,On Evaluating Adversarial Robustness of Large Vision-Language Models,Yes.,4,"""multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision)"" and ""Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice.""",2023,2023-05-26T13:49:44Z,"Keyphrase: ""Adversarial vulnerability in multimodal generation"""
arXIv2023,Playing repeated games with Large Language Models,Yes.,5,"""we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination.""",2023,2023-05-26T12:17:59Z,"Keyphrase: ""Suboptimal coordination in games"""
arXIv2023,Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification,Yes.,4,"""approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations.""",2023,2023-05-26T09:15:05Z,"Keyphrase: ""Ethical and bias concerns"""
arXIv2023,Can large language models generate salient negative statements?,Yes.,5,"""LLMs still struggle with the notion of factuality of negatives, frequently generating many ambiguous statements, or statements with negative keywords but a positive meaning.""",2023,2023-05-26T09:13:59Z,"Keyphrase: ""Factuality struggles"""
arXIv2023,A Closer Look at In-Context Learning under Distribution Shifts,Yes.,5,"""The key question we aim to address is",2023,2023-05-26T07:47:21Z,"Keyphrase: ""Lack of contextual understanding"""
arXIv2023,AdaPlanner: Adaptive Planning from Feedback with Language Models,Yes.,5,"""most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase.""",2023,2023-05-26T05:52:27Z,"Keyphrase: ""Limited adaptability and planning"""
arXIv2023,TADA: Task-Agnostic Dialect Adapters for English,Yes.,5,"""Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE).""",2023,2023-05-26T05:45:03Z,"Keyphrase: ""Bias towards standard American English"""
arXIv2023,The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering,Yes.,5,"""Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. 'unfaithful' with respect to a rationale as retrieved from a knowledge base.""",2023,2023-05-25T22:54:13Z,"Keyphrase: ""Unfaithful output"""
arXIv2023,Type Prediction With Program Decomposition and Fill-in-the-Type Training,Yes.,5,"""Large language models (LLMs) are promising for type prediction, but there are challenges",2023,2023-05-25T21:16:09Z,"Keyphrase: ""Limited predictive challenge capabilities"""
arXIv2023,Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models,Yes.,5,"""We find that despite capturing some aspects of logical meaning, the models fall far short of human performance.""",2023,2023-05-25T18:56:26Z,"Keyphrase: ""Limited logical reasoning"""
arXIv2023,Landmark Attention: Random-Access Infinite Context Length for Transformers,Yes.,5,"""their attention mechanism's large memory requirements have limited their ability to handle longer contexts.""",2023,2023-05-25T17:53:42Z,"Keyphrase: ""Limited long-context handling"""
arXIv2023,Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots,Yes.,5,"""there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential,"" ""The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential,"" and ""What ChatGPT lacks is a stochastic measure",2023,2023-05-25T17:35:57Z,"Keyphrase: ""Inaccurate data generation"""
arXIv2023,ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs,Yes.,5,"""However, current works in this field are plagued by limitations, specifically a restricted scope of applicable image domains and the provision of unreliable medical advice. This restricts their overall processing capabilities. Furthermore, the mismatch in writing style between LLMs and radiologists undermines their practical usefulness.""",2023,2023-05-25T12:03:31Z,"Keyphrase: ""Limited scope and unreliable advice"""
arXIv2023,"Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",Yes.,5,"""Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context.""",2023,2023-05-25T08:43:46Z,"Keyphrase: ""Hallucinated content"""
arXIv2023,Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,Yes.,5,"""Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost.""",2023,2023-05-25T07:39:41Z,"Keyphrase: ""Quadratic computational cost"""
arXIv2023,On the Planning Abilities of Large Language Models : A Critical Investigation,Yes.,5,"""Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains.""",2023,2023-05-25T06:32:23Z,"Keyphrase: ""Limited autonomous planning ability"""
arXIv2023,The False Promise of Imitating Proprietary LLMs,Yes.,5,"""we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data"" and ""Overall, we conclude that model imitation is a false promise",2023,2023-05-25T05:00:12Z,"Keyphrase: ""Over-reliance on imitation data"""
arXIv2023,Asking Before Action: Gather Information in Embodied Decision Making with Language Models,Yes.,5,"""However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance.""",2023,2023-05-25T04:05:08Z,"Keyphrase: ""Struggles in unfamiliar environments"""
arXIv2023,Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models,Yes.,4,"""the sensitivity of data contained in prompts raises privacy concerns"" and ""we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs.""",2023,2023-05-24T22:06:08Z,"Keyphrase: ""Privacy vulnerabilities due to prompt data sensitivity"""
arXIv2023,"The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",Yes.,5,"""LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.""",2023,2023-05-24T18:54:39Z,"Keyphrase: ""Lack of deep abstract understanding"""
arXIv2023,Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing,Yes.,5,"""However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content."" and ""we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art LLMs.""",2023,2023-05-24T16:50:36Z,"Keyphrase: ""Content hallucination"""
arXIv2023,Gorilla: Large Language Model Connected with Massive APIs,Yes.,5,"""However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call.""",2023,2023-05-24T16:48:11Z,"Keyphrase: ""Inaccurate input generation"""
arXIv2023,"Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond",Yes.,4,"""This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI.""",2023,2023-05-24T16:23:46Z,"Keyphrase: ""Epistemological and ethical challenges"""
arXIv2023,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,Yes.,5,"""Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world.""",2023,2023-05-24T16:17:36Z,"Keyphrase: ""Outdated knowledge and hallucination"""
arXIv2023,A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification,Yes.,4,"""However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification."" and ""We observe LLMs are more prone to failure in these cases.""",2023,2023-05-24T16:04:26Z,"Keyphrase: ""Failure in hierarchical classification"""
arXIv2023,Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples,Yes.,5,"""Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.""",2023,2023-05-24T15:55:51Z,"Keyphrase: ""Difficulty in generalizing to longer proofs"""
arXIv2023,Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,Yes.,5,"""We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models,"" and ""PCW would present unexpected deterioration regarding question miscomprehension and false inference.""",2023,2023-05-24T15:48:29Z,"Keyphrase: ""Question miscomprehension and false inference"""
arXIv2023,Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models,Yes.,5,"""The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts.""",2023,2023-05-24T11:55:59Z,"Keyphrase: ""Difficulty with abstract concepts"""
arXIv2023,ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind,Yes.,5,"""there is a heated debate about whether they are able to perform ToM tasks,"" and ""our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs.""",2023,2023-05-24T11:54:07Z,"Keyphrase: ""Inconsistent behavior in task performance"""
arXIv2023,GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking,Yes.,5,"""we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities.""",2023,2023-05-24T11:53:19Z,"Keyphrase: ""Limitation in graph comprehension"""
arXIv2023,Lawyer LLaMA Technical Report,Yes.,4,"""the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems"" and ""to alleviate the hallucination problem during the model's generation"".",2023,2023-05-24T11:52:07Z,"Keyphrase: ""Deficiency in domain-specific knowledge"""
arXIv2023,A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event Extraction,Yes.,4,"""Unfortunately, we find that current zero-shot EE methods perform poorly for the task, with issues including word sense ambiguity, modality mismatch, and efficiency. Straightforward application of large language model prompting typically performs even worse.""",2023,2023-05-24T11:41:33Z,"Keyphrase: ""Poor zero-shot performance"""
arXIv2023,ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification,Yes.,4,"""Mainstream deep learning approaches employing fine-tuning strategies on pre-trained language models (PLMs), have demonstrated remarkable performance gains over the past few years. Nonetheless, these methods still face many drawbacks that are complex to solve, including",2023,2023-05-24T11:06:23Z,"Keyphrase: ""Drawbacks in fine-tuning strategy"""
arXIv2023,Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems,Yes.,5,"""Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation.""",2023,2023-05-24T10:58:20Z,"Keyphrase: ""Factual errors in arithmetic tasks"""
arXIv2023,"RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",Yes.,5,"""they all suffer from generating untruthful dialogues because of the model hallucination.""",2023,2023-05-24T10:30:42Z,"Keyphrase: ""Untruthful dialogue hallucination"""
arXIv2023,Reasoning with Language Model is Planning with World Model,Yes.,5,"""However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g.,",2023,2023-05-24T10:28:28Z,"Keyphrase: ""Lack of internal world model"""
arXIv2023,GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP,Yes.,5,"""Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic."" and ""unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA."" and ""our work adds to a growing body of",2023,2023-05-24T10:12:39Z,"Keyphrase: ""Limited performance in handling Arabic dialects"""
arXIv2023,Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback,Yes.,4,"""some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated.""",2023,2023-05-24T10:12:33Z,"Keyphrase: ""Poorly calibrated conditional probability"""
arXIv2023,"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",Yes.,5,"""non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies.""",2023,2023-05-24T09:57:37Z,"Keyphrase: ""Degenerate output behavior"""
arXIv2023,Adversarial Demonstration Attacks on Large Language Models,Yes.,5,"""We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.""",2023,2023-05-24T09:40:56Z,"Keyphrase: ""Critical security risk"""
arXIv2023,Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark,Yes.,4,"""we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks,"" and ""pretrained models already possess some innate but limited capabilities of social language understanding.""",2023,2023-05-24T09:21:06Z,"Keyphrase: ""Limited social language understanding"""
arXIv2023,In-Context Impersonation Reveals Large Language Models' Strengths and Biases,Yes.,4,"""However, impersonation can also uncover LLMs' biases",2023,2023-05-24T09:13:15Z,"Keyphrase: ""Bias uncovered through impersonation"""
arXIv2023,Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,Yes.,5,"""methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback.""",2023,2023-05-24T08:59:15Z,"Keyphrase: ""Limited practical application due to reliance on feedback"""
arXIv2023,PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions,Yes.,5,"""The remarkable capabilities of large language models have been accompanied by a persistent drawback",2023,2023-05-24T08:59:00Z,"Keyphrase: ""Persistent drawbacks"""
arXIv2023,"M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",Yes.,4,"""Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written.""",2023,2023-05-24T08:55:11Z,"Keyphrase: ""Poor generalization to unseen domains"""
arXIv2023,How To Train Your (Compressed) Large Language Model,Yes.,5,"""Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression.""",2023,2023-05-24T08:18:35Z,"Keyphrase: ""Inadequate compression methods"""
arXIv2023,BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer,Yes.,5,"""Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. In particular, ChatGPT with in-context learning often performs worse than much smaller mT5-base models fine-tuned on English task data and few-shot in-language examples.""",2023,2023-05-24T08:06:33Z,"Keyphrase: ""Limited few-shot and cross-lingual capabilities"""
arXIv2023,SummIt: Iterative Text Summarization via ChatGPT,Yes.,4,"""the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests."" and ""identify a potential issue of over-correction.""",2023,2023-05-24T07:40:06Z,"Keyphrase: ""Hallucination and overcorrection"""
arXIv2023,Mitigating Temporal Misalignment by Discarding Outdated Facts,Yes.,5,"""While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having",2023,2023-05-24T07:30:08Z,"Keyphrase: ""Outdated world knowledge"""
arXIv2023,MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,Yes.,5,"""The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. ... While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions.""",2023,2023-05-24T06:48:41Z,"Keyphrase: ""Rapid decay of information"""
arXIv2023,Prompting Large Language Models for Counterfactual Generation: An Empirical Study,Yes.,5,"""The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias."" and ""we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs' capability of generating counterfactuals.""",2023,2023-05-24T06:44:32Z,"Keyphrase: ""Limited task-specific performance"""
arXIv2023,Adapting Language Models to Compress Contexts,Yes.,5,"""Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.""",2023,2023-05-24T06:42:44Z,"Keyphrase: ""Limited context window"""
arXIv2023,ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds,Yes.,5,"""This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model."" and ""Our analyses emphasize the need for further research into the linguistic comprehension and reasoning capabilities of LLMs, in order to improve their reliability, and establish their trustworthiness for real-world applications.""",2023,2023-05-24T06:41:09Z,"Keyphrase: ""Limited linguistic comprehension"""
arXIv2023,Anthropomorphization of AI: Opportunities and Risks,Yes.,4,"""We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction,",2023,2023-05-24T06:39:45Z,"Keyphrase: ""Anthropomorphization concerns"""
arXIv2023,Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models,Yes.,5,"""We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust To",2023,2023-05-24T06:14:31Z,"Keyphrase: ""Struggles with adversarial examples"""
arXIv2023,Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,Yes.,4,"""These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.""",2023,2023-05-24T04:27:21Z,"Keyphrase: ""Vulnerability to poisoning attacks"""
arXIv2023,A Causal View of Entity Bias in (Large) Language Models,Yes.,4,"""Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions."" and ""The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits.""",2023,2023-05-24T03:59:18Z,"Keyphrase: ""Entity bias and unfaithful predictions"""
arXIv2023,Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs,Yes.,5,"""We also identify the existence of inherent biases in these LLMs which is the root cause of the aforementioned phenomenon and makes self-assessment tests unreliable.""",2023,2023-05-24T03:53:43Z,"Keyphrase: ""Inherent bias and unreliable self-assessment"""
arXIv2023,Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response,Yes.,5,"""Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs."" and ""Empirical results show that the ability of LLMs to identify unreasonable responses is insufficient. There are risks in using reference-free evaluators based on LLMs to evaluate the quality of",2023,2023-05-24T02:52:48Z,"Keyphrase: ""Challenges with identifying unreasonable responses"""
arXIv2023,Enabling Large Language Models to Generate Text with Citations,Yes.,5,"""Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination."" and ""current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time.""",2023,2023-05-24T01:53:49Z,"Keyphrase: ""Hallucination and lack of complete citation support"""
arXIv2023,Think Before You Act: Decision Transformers with Internal Working Memory,Yes.,5,"""However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks.""",2023,2023-05-24T01:20:22Z,"Keyphrase: ""Forgetting phenomenon"""
arXIv2023,ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers,Yes.,4,"""Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification.""",2023,2023-05-24T00:10:15Z,"Keyphrase: ""Lack of guaranteed correctness"""
arXIv2023,Sources of Hallucination by Large Language Models on Inference Tasks,Yes.,5,"""We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs.""",2023,2023-05-23T22:24:44Z,"Keyphrase: ""Bias and hallucination issues"""
arXIv2023,LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond,Yes.,5,"""a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision"" and ""Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4,",2023,2023-05-23T21:50:06Z,"Keyphrase: ""Struggles with complex tasks"""
arXIv2023,MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems,Yes.,5,"""While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early.""",2023,2023-05-23T21:44:56Z,"Keyphrase: ""Inaccurate tutoring feedback"""
arXIv2023,Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models,Yes.,5,"""Our findings show that even the most advanced GPT models struggle to reason on manipulated facts - showcasing poor DUPE skills - with accuracy dropping by 45% compared to the original dataset.""",2023,2023-05-23T20:26:03Z,"Keyphrase: ""Poor robustness to manipulation"""
arXIv2023,Having Beer after Prayer? Measuring Cultural Bias in Large Language Models,Yes.,5,"""we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture,"" ""we find concerning cases of stereotyping and cultural unfairness,"" and ""revealing the incapability of appropriate adaptation to Arab cultural contexts.""",2023,2023-05-23T18:27:51Z,"Keyphrase: ""Cultural bias and unfair stereotyping"""
arXIv2023,On Robustness of Finetuned Transformer-based NLP Models,Yes.,5,"""Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned?"" and ""Overall, this study provides valuable insights into perturbation-specific weaknesses of popular Transformer-based models, which should be kept in mind when passing inputs",2023,2023-05-23T18:25:18Z,"Keyphrase: ""Robustness to input perturbations"""
arXIv2023,ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models,Yes.,4,"""Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning.""",2023,2023-05-23T17:54:33Z,"Keyphrase: ""Limited complex reasoning ability"""
arXIv2023,Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science,Yes.,5,"""The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large).""",2023,2023-05-23T17:48:21Z,"Keyphrase: ""Zero-shot performance limitations"""
arXIv2023,Evaluation of African American Language Bias in Natural Language Generation,Yes.,4,"""documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of AAL features.""",2023,2023-05-23T17:34:37Z,"Keyphrase: ""Bias identification trend"""
arXIv2023,Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs,Yes.,5,"""We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.""",2023,2023-05-23T17:25:59Z,"Keyphrase: ""Inconsistent performance"""
arXIv2023,Hierarchical Prompting Assists Large Language Model on Web Navigation,Yes.,4,"""Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks.""",2023,2023-05-23T17:10:39Z,"Keyphrase: ""Struggles with complex tasks"""
arXIv2023,Language Models with Rationality,Yes.,4,"""This lack of interpretability is a growing impediment to widespread use of LLMs."" and ""to resolve inconsistencies that may exist.""",2023,2023-05-23T17:04:25Z,"Keyphrase: ""Lack of interpretability"""
arXIv2023,Multilingual Large Language Models Are Not (Yet) Code-Switchers,Yes.,5,"""Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales."" and ""We argue that current 'multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.""",2023,2023-05-23T16:50:48Z,"Keyphrase: ""Underperformance in zero-shot/few-shot tasks"""
arXIv2023,Question Answering as Programming for Solving Time-Sensitive Questions,Yes.,5,"""our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics.""",2023,2023-05-23T16:35:16Z,"Keyphrase: ""Inability for rigorous reasoning"""
arXIv2023,HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations,Yes.,4,"""GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills.""",2023,2023-05-23T16:15:24Z,"Keyphrase: ""Limited pragmatic skills"""
arXIv2023,In-Context Probing: Toward Building Robust Classifiers via Probing Large Language Models,Yes.,5,"""the performance on a downstream task can vary considerably, depending on the instruction. Importantly, such dependency on the context can surface in unpredictable ways, e.g., a seemingly more informative instruction might lead to a worse performance.""",2023,2023-05-23T15:43:04Z,"Keyphrase: ""Unpredictable task performance"""
arXIv2023,Revisiting Acceptability Judgements,Yes.,5,"""Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much below supervised models (59.03 MCC) and human (65.11 MCC).""",2023,2023-05-23T14:16:22Z,"Keyphrase: ""Limited performance compared to supervised models"""
arXIv2023,Improving Language Models via Plug-and-Play Retrieval Feedback,Yes.,5,"""Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can",2023,2023-05-23T12:29:44Z,"Keyphrase: ""Incorrect hallucinated information"""
arXIv2023,Robust Prompt Optimization for Large Language Models Against Distribution Shifts,Yes.,5,"""We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis.""",2023,2023-05-23T11:30:43Z,"Keyphrase: ""Vulnerability to distribution shift"""
arXIv2023,A Trip Towards Fairness: Bias and De-Biasing in Large Language Models,Yes.,4,"""a little or a large bias in CtB-LLMs may cause huge harm,"" and ""we performed a large investigation of the bias of three families of CtB-LLMs,"" and ""we discovered that bias depends not on the number of parameters but on the perplexity.""",2023,2023-05-23T09:35:37Z,"Keyphrase: ""Biased outputs"""
arXIv2023,Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study,Yes.,5,"""Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse.""",2023,2023-05-23T09:33:38Z,"Keyphrase: ""Content constraint and potential misuse"""
arXIv2023,"""Is the Pope Catholic?"" Applying Chain-of-Thought Reasoning to Understanding Conversational Implicatures",Yes.,5,"""recent research indicates that large language models struggle to comprehend these implicatures as effectively as the average human.""",2023,2023-05-23T08:49:50Z,"Keyphrase: ""Difficulty with implicatures"""
arXIv2023,Can Large Language Models Capture Dissenting Human Voices?,Yes.,5,"""we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.""",2023,2023-05-23T07:55:34Z,"Keyphrase: ""Limited ability in capturing human disagreement"""
arXIv2023,Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting,Yes.,5,"""we uncovered a universal vulnerability among LLMs in processing inductive instructions,"" and ""different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance.""",2023,2023-05-23T06:38:20Z,"Keyphrase: ""Struggles with inductive instruction"""
arXIv2023,Exploring Self-supervised Logic-enhanced Training for Large Language Models,Yes.,5,"""Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines.""",2023,2023-05-23T06:13:10Z,"Keyphrase: ""Weak logical reasoning performance"""
arXIv2023,Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models,Yes.,5,"""an analysis of LLaMA's errors reveals significant limitations in its ability to recall facts in languages other than English, plus difficulties related to the location and gender of fact subjects.""",2023,2023-05-23T04:31:39Z,"Keyphrase: ""Limited fact recall and contextual understanding"""
arXIv2023,The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models,Yes.,5,"""they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings.""",2023,2023-05-23T04:22:50Z,"Keyphrase: ""Inherent bias and lack of grounding"""
arXIv2023,On the Risk of Misinformation Pollution with Large Language Models,Yes.,5,"""Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems.""",2023,2023-05-23T04:10:26Z,"Keyphrase: ""Misinformation generation"""
arXIv2023,ChatGPT as your Personal Data Scientist,Yes.,5,"""Interestingly, its development spotlighted several critical weaknesses in the current LLMs (ChatGPT) and highlighted substantial opportunities for improvement.""",2023,2023-05-23T04:00:16Z,"Keyphrase: ""Critical weaknesses and room for improvement"""
arXIv2023,InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning,Yes.,4,"""their ability to generalize to underrepresented languages is limited due to the scarcity of available data. Additionally, directly adapting new languages to instruction-tuned LLMs can result in catastrophic forgetting, which leads to the loss of multitasking ability.""",2023,2023-05-23T02:51:34Z,"Keyphrase: ""Catastrophic forgetting"""
arXIv2023,"Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",Yes.,5,"""However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity.""",2023,2023-05-23T02:49:35Z,"Keyphrase: ""Limited conversational proactivity"""
arXIv2023,How Language Model Hallucinations Can Snowball,Yes.,5,"""A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements."" and ""We refer to this phenomenon as hallucination snowballing",2023,2023-05-22T23:14:28Z,"Keyphrase: ""Risk of hallucinating incorrect statements"""
arXIv2023,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,Yes.,5,"""We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.""",2023,2023-05-22T21:59:26Z,"Keyphrase: ""Poor slot filling performance"""
arXIv2023,DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules,Yes.,4,"""Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects.""",2023,2023-05-22T18:43:31Z,"Keyphrase: ""Limited performance on non-standard English dialects"""
arXIv2023,RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text,Yes.,5,"""The fixed-size context of Transformer makes GPT models incapable of generating arbitrarily long text.""",2023,2023-05-22T17:58:10Z,"Keyphrase: ""Inability to generate arbitrarily long text"""
arXIv2023,Language-Agnostic Bias Detection in Language Models with Bias Probing,Yes.,4,"""Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases."" and ""We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context.""",2023,2023-05-22T17:58:01Z,"Keyphrase: ""Strong social bias"""
arXIv2023,Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts,Yes.,5,"""tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory"" and ""how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?"" and ""Our investigation reveals seemingly contradicting behaviors of LLMs"" and ""These results pose important implications that are worth careful",2023,2023-05-22T17:57:41Z,"Keyphrase: ""Limited external evidence integration"""
arXIv2023,Fairness of ChatGPT,Yes.,4,"""there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs,"" and ""This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.""",2023,2023-05-22T17:51:56Z,"Keyphrase: ""Limited quantitative analysis on fairness evaluation"""
arXIv2023,Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,Yes.,5,"""Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold."" and ""Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research",2023,2023-05-22T17:36:58Z,"Keyphrase: ""Inability to detect certain types of hate speech"""
arXIv2023,Prompting is not a substitute for probability measurements in large language models,Yes.,5,"""we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities.""",2023,2023-05-22T17:33:17Z,"Keyphrase: ""Inferior metalinguistic judgment"""
arXIv2023,"""According to ..."": Prompting Language Models Improves Quoting from Pre-Training Data",Yes.,5,"""Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data.""",2023,2023-05-22T17:25:24Z,"Keyphrase: ""Generation of fake information"""
arXIv2023,To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis,Yes.,5,"""LLMs are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs,"" ""revealing that the model is susceptible to overfitting, leading to multi-epoch degradation,"" ""significant factors",2023,2023-05-22T17:02:15Z,"Keyphrase: ""Susceptible to overfitting"""
arXIv2023,SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables,Yes.,5,"""Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of",2023,2023-05-22T16:13:50Z,"Keyphrase: ""Challenges in scientific table understanding"""
arXIv2023,Teaching Probabilistic Logical Reasoning to Transformers,Yes.,5,"""Our evaluation results show that both generations of language models struggle with reasoning over uncertain text.""",2023,2023-05-22T16:08:20Z,"Keyphrase: ""Struggles with uncertain reasoning"""
arXIv2023,"Editing Large Language Models: Problems, Methods, and Opportunities",Yes.,4,"""Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive."" and ""we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal.""",2023,2023-05-22T16:00:00Z,"Keyphrase: ""Challenge in maintaining relevancy and rectifying errors"""
arXIv2023,Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,Yes.,5,"""it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way."" and ""LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments.""",2023,2023-05-22T15:47:31Z,"Keyphrase: ""Shallow reasoning and susceptibility to absurd arguments"""
arXIv2023,"Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students",Yes.,4,"""it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes"" and ""Our findings indicate that LLMs have an overall negative perception of math and STEM fields, with math being perceived most negatively.""",2023,2023-05-22T15:06:51Z,"Keyphrase: ""Perpetuation of harmful stereotypes"""
arXIv2023,Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization,Yes.,5,"""We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance",2023,2023-05-22T14:58:13Z,"Keyphrase: ""Inconsistent evaluation and comparison"""
arXIv2023,Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model,Yes.,4,"""Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research.""",2023,2023-05-22T13:16:07Z,"Keyphrase: ""Limited interpretability"""
arXIv2023,ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,Yes.,4,"""we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning.""",2023,2023-05-22T11:45:42Z,"Keyphrase: ""Limited computational reasoning"""
arXIv2023,Automatic Code Summarization via ChatGPT: How Far Are We?,Yes.,5,"""The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models.""",2023,2023-05-22T09:43:40Z,"Keyphrase: ""Poor summarization performance"""
arXIv2023,Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage,Yes.,4,"""Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy.""",2023,2023-05-22T04:30:35Z,"Keyphrase: ""Limited commonsense knowledge accuracy"""
arXIv2023,Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction,Yes.,5,"""Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition."" and ""The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT",2023,2023-05-22T03:04:06Z,"Keyphrase: ""Overlooking structural underpinnings"""
arXIv2023,Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models,Yes.,5,"""We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs.""",2023,2023-05-21T19:06:30Z,"Keyphrase: ""Limited research coverage"""
arXIv2023,Retrieving Texts based on Abstract Descriptions,Yes.,5,"""While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval)."" and ""the retrieval task cannot be performed by the LLM directly.""",2023,2023-05-21T17:14:31Z,"Keyphrase: ""Limited semantic retrieval capabilities"""
arXIv2023,"GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts",Yes.,5,"""there is a current hot debate regarding their reasoning capacity"" and ""the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks.""",2023,2023-05-21T14:45:17Z,"Keyphrase: ""Limited reasoning proficiency"""
arXIv2023,VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models,Yes.,5,"""They still have space to grow, though, especially in the areas of mathematics, physics, chemistry, and biology."" and ""especially in resolving LLMs' limits in disciplines involving mathematics and the natural sciences.""",2023,2023-05-20T14:13:08Z,"Keyphrase: ""Limited understanding of complex disciplines"""
arXIv2023,Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?,Yes.,5,"""while doing fairly well on contexts that follow the common assumptions, the models struggle to correctly reason over contexts that break those assumptions. Specifically, the performance gap is as high as 20% absolute points.""",2023,2023-05-20T05:20:37Z,"Keyphrase: ""Struggles with contextual reasoning"""
arXIv2023,UP5: Unbiased Foundation Model for Fairness-aware Recommendation,Yes.,4,"""there is unfairness involved in LLMs that lead to unfair recommendation results.""",2023,2023-05-20T04:32:59Z,"Keyphrase: ""Unfair recommendations"""
arXIv2023,Clinical Camel: An Open Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding,Yes.,4,"""Significant challenges concerning reliability, bias, and the potential for outdated knowledge persist.""",2023,2023-05-19T23:07:09Z,"Keyphrase: ""Reliability and bias challenges"""
arXIv2023,Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,Yes.,5,"""However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst."" and ""They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews.""",2023,2023-05-19T17:09:19Z,"Keyphrase: ""Potential for misleading text"""
arXIv2023,Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs,Yes.,5,"""However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure",2023,2023-05-19T16:27:43Z,"Keyphrase: ""Overlooking linguistic cues"""
arXIv2023,Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning,Yes.,5,"""Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk.""",2023,2023-05-19T15:45:29Z,"Keyphrase: ""Privacy risk from memorization"""
arXIv2023,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Yes.,5,"""Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge."" and ""Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts.""",2023,2023-05-19T15:36:27Z,"Keyphrase: ""Hallucination generation"""
arXIv2023,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,Yes.,5,"""these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content.""",2023,2023-05-19T15:19:44Z,"Keyphrase: ""Inconsistent and problematic behavior"""
arXIv2023,Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses,Yes.,5,"""We show that its multilingual consistency is still lacking, and that its task and world understanding are thus not language-independent.""",2023,2023-05-19T13:23:51Z,"Keyphrase: ""Lack of multilingual consistency"""
arXIv2023,LLM-Pruner: On the Structural Pruning of Large Language Models,Yes.,4,"""such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages.""",2023,2023-05-19T12:10:53Z,"Keyphrase: ""Deployment challenges due to model size"""
arXIv2023,RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought,Yes.,5,"""LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems.""",2023,2023-05-19T08:02:52Z,"Keyphrase: ""Factual consistency and reasoning challenges"""
arXIv2023,Graphologue: Exploring Large Language Model Responses with Interactive Diagrams,Yes.,5,"""However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure.""",2023,2023-05-19T06:53:25Z,"Keyphrase: ""Limited support for complex information tasks"""
arXIv2023,A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation,Yes.,5,"""First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs.""",2023,2023-05-19T02:41:12Z,"Keyphrase: ""Vulnerability and unintended bugs"""
arXIv2023,ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery,Yes.,4,"""data-sensitive domains -- including but not limited to healthcare -- face challenges in using ChatGPT due to privacy and data-ownership concerns.""",2023,2023-05-19T02:09:52Z,"Keyphrase: ""Privacy and data ownership concerns"""
arXIv2023,Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models,Yes.,4,"""the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically.""",2023,2023-05-19T00:53:45Z,"Keyphrase: ""Repetitive and surprising data generation"""
arXIv2023,CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models,Yes.,4,"""Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias."" and ""Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases.""",2023,2023-05-18T18:58:30Z,"Keyphrase: ""Stereotypical biases and safety concerns"""
arXIv2023,Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses,Yes.,4,"""We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA's responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require",2023,2023-05-18T18:15:43Z,"Keyphrase: ""Limited causal reasoning ability"""
arXIv2023,Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings,Yes.,4,"""Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks.""",2023,2023-05-18T07:56:40Z,"Keyphrase: ""Bias in sentence embeddings"""
arXIv2023,"Ethical ChatGPT: Concerns, Challenges, and Commandments",Yes.,4,"""there are indeed ethical concerns associated with the use of AI language models such as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights specific ethical concerns on ChatGPT and articulates key challenges when ChatGPT is used in various applications.""",2023,2023-05-18T02:04:13Z,"Keyphrase: ""Ethical concerns and privacy issues"""
arXIv2023,Language Models Meet World Models: Embodied Experiences Enhance Language Models,Yes.,5,"""they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills.""",2023,2023-05-18T00:35:38Z,"Keyphrase: ""Lack of embodied knowledge"""
arXIv2023,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,Yes.,5,"""Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.""",2023,2023-05-17T23:16:17Z,"Keyphrase: ""Limited strategic lookahead"""
arXIv2023,"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",Yes.,4,"""While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU.""",2023,2023-05-17T20:45:13Z,"Keyphrase: ""Memory-intensive deployment"""
arXIv2023,ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages,Yes.,5,"""We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations"" and ""ChatGPT appears to confer a higher respect to men than to women in the same occupation.""",2023,2023-05-17T18:30:05Z,"Keyphrase: ""Gender bias perpetuation"""
arXIv2023,BAD: BiAs Detection for Large Language Models in the context of candidate screening,Yes.,4,"""The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed."" and ""we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could",2023,2023-05-17T17:47:31Z,"Keyphrase: ""Bias in automated screening"""
arXIv2023,Evaluating Object Hallucination in Large Vision-Language Models,Yes.,5,"""we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions."" and ""show that they mostly suffer from severe object hallucination issue.""",2023,2023-05-17T16:34:01Z,"Keyphrase: ""Severe object hallucination"""
arXIv2023,Language Model Tokenizers Introduce Unfairness Between Languages,Yes.,4,"""disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked"" and ""This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided",2023,2023-05-17T14:17:57Z,"Keyphrase: ""Unfair treatment in language processing"""
arXIv2023,Can Language Models Solve Graph Problems in Natural Language?,Yes.,5,"""whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored"" and ""the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings",2023,2023-05-17T08:29:21Z,"Keyphrase: ""Brittle handling of spurious correlations"""
arXIv2023,Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models,Yes.,5,"""By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.""",2023,2023-05-17T05:25:27Z,"Keyphrase: ""Limited ability to generate factual and up-to-date knowledge"""
arXIv2023,"""I'm fully who I am"": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",Yes.,4,"""We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on",2023,2023-05-17T04:21:45Z,"Keyphrase: ""Gender misrepresentation"""
arXIv2023,Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models,Yes.,5,"""We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness.""",2023,2023-05-16T20:52:04Z,"Keyphrase: ""Pattern of failure"""
arXIv2023,Small Models are Valuable Plug-ins for Large Language Models,Yes.,5,"""Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of",2023,2023-05-15T17:59:01Z,"Keyphrase: ""Limited accessibility to tuning and hardware resources"""
arXIv2023,Large Language Models are Zero-Shot Rankers for Recommender Systems,Yes.,5,"""We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts.""",2023,2023-05-15T17:57:39Z,"Keyphrase: ""Limited historical context and biased prompt influence"""
arXIv2023,"Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",Yes.,5,"""However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed."" and ""they are",2023,2023-05-15T15:44:51Z,"Keyphrase: ""Lack of thorough risk analysis"""
arXIv2023,Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks,Yes.,5,"""a critical issue has been identified within this domain",2023,2023-05-15T15:19:08Z,"Keyphrase: ""Limited domain knowledge"""
arXIv2023,Unsupervised Sentence Representation Learning with Frequency-induced Adversarial Tuning and Incomplete Sentence Filtering,Yes.,5,"""PLMs are sensitive to the frequency information of words from their pre-training corpora, resulting in anisotropic embedding space, where the embeddings of high-frequency words are clustered but those of low-frequency words disperse sparsely. This anisotropic phenomenon results in two problems of similarity bias and information bias, lowering the quality of sentence embeddings.""",2023,2023-05-15T13:59:23Z,"Keyphrase: ""Anisotropic embedding space"""
arXIv2023,Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study,Yes.,5,"""ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations. We also found that ChatGPT hardly understands rhetorical structures that are more complex than topic structures.""",2023,2023-05-15T07:14:41Z,"Keyphrase: ""Limited understanding of complex topic structures"""
arXIv2023,Semantic Composition in Visually Grounded Language Models,Yes.,5,"""Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure.""",2023,2023-05-15T03:19:42Z,"Keyphrase: ""Failure in representing compositional structure"""
arXIv2023,Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency,Yes.,5,"""non-linguistic skill injection typically comes at a cost for LLMs",2023,2023-05-14T20:57:11Z,"Keyphrase: ""Cost of nonlinguistic skill injection"""
arXIv2023,Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics,Yes.,4,"""Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity."" and ""The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the LLM's ability to generalize its knowledge about human behavior in social dilem",2023,2023-05-13T17:23:16Z,"Keyphrase: ""Limited ability to generalize human behavior"""
arXIv2023,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,Yes.,5,"""existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading",2023,2023-05-13T14:23:07Z,"Keyphrase: ""Limited architectural flexibility"""
arXIv2023,TinyStories: How Small Can Language Models Be and Still Speak Coherent English?,Yes.,5,"""Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English",2023,2023-05-12T20:56:48Z,"Keyphrase: ""Struggles with coherence and consistency"""
arXIv2023,Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation,Yes.,4,"""it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation."" and ""we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations.""",2023,2023-05-12T16:54:36Z,"Keyphrase: ""Social prejudice and unfairness"""
arXIv2023,Surfacing Biases in Large Language Models using Contrastive Input Decoding,Yes.,5,"""Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour"" and ""We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturb",2023,2023-05-12T11:09:49Z,"Keyphrase: ""Context-specific bias detection"""
arXIv2023,Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation,Yes.,5,"""Experiments show that ChatGPT is not a good causal reasoner, but a good causal explainer. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes,",2023,2023-05-12T10:54:13Z,"Keyphrase: ""Hallucination in causal reasoning"""
arXIv2023,Evaluating Open-Domain Question Answering in the Era of Large Language Models,Yes.,5,"""The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.""",2023,2023-05-11T17:14:33Z,"Keyphrase: ""Difficulty in detecting hallucination"""
arXIv2023,Active Retrieval Augmented Generation,Yes.,5,"""Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.""",2023,2023-05-11T17:13:40Z,"Keyphrase: ""Factually inaccurate output"""
arXIv2023,Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models,Yes.,5,"""While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets."" and ""We find that while existing debiasing methods can mitigate reliance on a chosen spurious feature, the OOD performance gains of these methods can not be explained by mitigated reliance on biased features",2023,2023-05-11T14:35:00Z,"Keyphrase: ""Reliance on spurious correlations"""
arXIv2023,Chain-of-Dictionary Prompting Elicits Translation in Large Language Models,Yes.,5,"""they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation.""",2023,2023-05-11T05:19:47Z,"Keyphrase: ""Struggles with rare words and low-resource languages"""
arXIv2023,How Good are Commercial Large Language Models on African Languages?,Yes.,4,"""However, their performance on African languages is largely unknown."" and ""Our results suggest that commercial language models produce below-par performance on African languages.""",2023,2023-05-11T02:29:53Z,"Keyphrase: ""Poor performance in African languages"""
arXIv2023,"Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",Yes.,5,"""We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents.""",2023,2023-05-10T16:40:37Z,"Keyphrase: ""Limited aggregation capability"""
arXIv2023,"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",Yes.,4,"""examines the errors produced by the LLMs and categorized the errors into three types",2023,2023-05-10T13:40:06Z,"Keyphrase: ""Error categorization limitations"""
arXIv2023,Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,Yes.,5,"""Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge,"" and ""statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.""",2023,2023-05-10T08:35:50Z,"Keyphrase: ""Limited commonsense knowledge"""
arXIv2023,Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition,Yes.,5,"""This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT.""",2023,2023-05-10T08:16:46Z,"Keyphrase: ""Limited critical insight"""
arXIv2023,ChatGPT as a Text Simplification Tool to Remove Bias,Yes.,4,"""The presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. If the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based upon their protected characteristic, leading to possible discrimination.""",2023,2023-05-09T13:10:23Z,"Keyphrase: ""Association of language with protected characteristics"""
arXIv2023,Explanation-based Finetuning Makes Models More Robust to Spurious Cues,Yes.,4,"""Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data.""",2023,2023-05-08T18:53:45Z,"Keyphrase: ""Poor generalization to out-of-distribution data"""
arXIv2023,Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust,Yes.,5,"""The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes."" and ""However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs.""",2023,2023-05-08T18:53:14Z,"Keyphrase: ""Opacity in semantic understanding"""
arXIv2023,How Do In-Context Examples Affect Compositional Generalization?,Yes.,5,"""We find that the compositional generalization performance can be easily affected by the selection of in-context examples,"" and ""two strong limitations are observed",2023,2023-05-08T16:32:18Z,"Keyphrase: ""Compositional generalization affected by context"""
arXIv2023,Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns,Yes.,4,"""underscores the challenges in developing safe and unbiased LLMs, and emphasizes the importance of understanding the susceptibility of LLMs to external influences.""",2023,2023-05-08T16:10:18Z,"Keyphrase: ""Susceptibility to external influence"""
arXIv2023,Augmented Large Language Models with Parametric Knowledge Guiding,Yes.,4,"""However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom",2023,2023-05-08T15:05:16Z,"Keyphrase: ""Limited domain-specific knowledge and transparency"""
