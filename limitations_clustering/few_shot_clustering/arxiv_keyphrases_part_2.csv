Source ,Title,Talks about LLMs,Rate,Evidence,Year ,Date ,Keyphrase
arXIv2023,Differentially Private Attention Computation,Yes.,4,"""one crucial issue concerning the inference results of large language models is security and privacy"" and ""results generated by LLMs could possibly leak many confidential or copyright information.""",2023,2023-05-08T13:32:41Z,"Keyphrase: ""Security and privacy concerns"""
arXIv2023,Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,Yes.,5,"""However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction."" and ""Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety.""",2023,2023-05-07T22:44:25Z,"Keyphrase: ""Misleading explanations"""
arXIv2023,Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?,Yes.,4,"""However, these abilities are quite limited and worse than well-trained humans when the tasks are not known and are not part of the training data.""",2023,2023-05-06T20:53:22Z,"Keyphrase: ""Limited human task performance"""
arXIv2023,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,Yes.,5,"""Despite the success of Zero-shot-CoT, it still suffers from three pitfalls",2023,2023-05-06T16:34:37Z,"Keyphrase: ""Limitations in zero-shot capability"""
arXIv2023,Pre-training Language Model as a Multi-perspective Course Learner,Yes.,4,"""Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning.""",2023,2023-05-06T09:02:10Z,"Keyphrase: ""Biased learning and label imbalance"""
arXIv2023,"Large Language Models in Sport Science & Medicine: Opportunities, Risks and Considerations",Yes.,4,"""However, there are also potential risks associated with the use and development of LLMs, including biases in the dataset used to create the model, the risk of exposing confidential data, the risk of generating harmful output, and the need to align these models with human preferences through feedback.""",2023,2023-05-05T21:20:02Z,"Keyphrase: ""Risk of bias and harmful output"""
arXIv2023,Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,Yes.,4,"""Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures.""",2023,2023-05-05T17:15:32Z,"Keyphrase: ""Commonsense failures"""
arXIv2023,Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming,Yes.,5,"""Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality.""",2023,2023-05-05T07:24:46Z,"Keyphrase: ""Limited logical reasoning"""
arXIv2023,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,Yes.,5,"""one of its most fatal disadvantages is the lack of factual correctness"" and ""still suffers from factuality concerns in knowledge-intensive tasks.""",2023,2023-05-05T03:49:14Z,"Keyphrase: ""Factual correctness deficiency"""
arXIv2023,Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,Yes.,5,"""even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand.""",2023,2023-05-04T19:02:29Z,"Keyphrase: ""Limited execution accuracy"""
arXIv2023,Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision,Yes.,4,"""However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases.""",2023,2023-05-04T17:59:28Z,"Keyphrase: ""Dependence on human supervision"""
arXIv2023,"""Oops, Did I Just Say That?"" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process",Yes.,4,"""their subtly unethical suggestions become a serious and real concern"" and ""our study on seven popular LLMs (e.g., ChatGPT, GPT-4) uncovers in total 109,824 unethical suggestions.""",2023,2023-05-04T08:00:32Z,"Keyphrase: ""Ethical concerns and unethical suggestions"""
arXIv2023,Can LLMs Capture Human Preferences?,Yes.,5,"""Though GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans."" and ""While directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings",2023,2023-05-04T03:51:31Z,"Keyphrase: ""Lexicographic preference bias"""
arXIv2023,"Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents",Yes.,4,"""the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent",2023,2023-05-03T20:11:22Z,"Keyphrase: ""Challenges in serving as an agent"""
arXIv2023,ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,Yes.,5,"""two major limitations hinder its potential applications",2023,2023-05-03T19:57:43Z,"Keyphrase: ""Limitations hindering potential applications"""
arXIv2023,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,Yes.,5,"""Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications.""",2023,2023-05-03T17:50:56Z,"Keyphrase: ""Memory inefficiency and computational intensity"""
arXIv2023,GPT-RE: In-context Learning for Relation Extraction using Large Language Models,Yes.,5,"""they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE",2023,2023-05-03T13:28:08Z,"Keyphrase: ""Lagging behind fully-supervised baselines"""
arXIv2023,Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy,Yes.,5,"""LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately."" and ""Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy.""",2023,2023-05-02T15:53:28Z,"Keyphrase: ""Memorization of training data"""
arXIv2023,Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation,Yes.,5,"""However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code."" and ""Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k",2023,2023-05-02T05:46:48Z,"Keyphrase: ""Limited testing coverage"""
arXIv2023,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Yes.,5,"""this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors.""",2023,2023-05-01T17:36:06Z,"Keyphrase: ""Toxic content generation"""
arXIv2023,Poisoning Language Models During Instruction Tuning,Yes.,5,"""we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions"" and ""Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.""",2023,2023-05-01T16:57:33Z,"Keyphrase: ""Vulnerability to poisoning attacks"""
arXIv2023,Learning to Reason and Memorize with Self-Notes,Yes.,5,"""Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use.""",2023,2023-05-01T14:02:48Z,"Keyphrase: ""Difficulty in retaining multistep reasoning"""
arXIv2023,Self-Evaluation Guided Beam Search for Reasoning,Yes.,5,"""the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results.""",2023,2023-05-01T02:37:59Z,"Keyphrase: ""Error accumulation in reasoning chain"""
arXIv2023,Causal Reasoning and Large Language Models: Opening a New Frontier for Causality,Yes.,4,"""The causal capabilities of large language models (LLMs) is a matter of significant debate,"" and ""LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.""",2023,2023-04-28T19:00:43Z,"Keyphrase: ""Unpredictable failure modes"""
arXIv2023,"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",Yes.,5,"""albeit it may not possess the same level of expertise in identifying the temporal order between two events"" and ""the implicit discourse relation remains a formidable challenge"" and ""ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.""",2023,2023-04-28T13:14:36Z,"Keyphrase: ""Subpar performance in discourse parsing"""
arXIv2023,We're Afraid Language Models Aren't Modeling Ambiguity,Yes.,5,"""We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset.""",2023,2023-04-27T17:57:58Z,"Keyphrase: ""Ambiguity disentanglement challenge"""
arXIv2023,Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery,Yes.,5,"""current explorations do not assess the real-world utility and safety of LLMs in clinical settings,"" ""responses contained hallucinated references,"" and ""often do not meet the specific information need of a given question.""",2023,2023-04-26T17:54:28Z,"Keyphrase: ""Hallucinated responses"""
arXIv2023,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,Yes.,4,"""We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on",2023,2023-04-26T17:52:30Z,"Keyphrase: ""Spurious bias and data-specific challenges"""
arXIv2023,Enhancing Large Language Model with Self-Controlled Memory Framework,Yes.,5,"""Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information.""",2023,2023-04-26T07:25:31Z,"Keyphrase: ""Inability to process lengthy input"""
arXIv2023,The Internal State of an LLM Knows When It's Lying,Yes.,5,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone.""",2023,2023-04-26T02:49:38Z,"Keyphrase: ""Confident false information"""
arXIv2023,TABLET: Learning From Instructions For Tabular Data,Yes.,5,"""We find LLMs often ignore instructions and fail to predict specific instances correctly, even with examples.""",2023,2023-04-25T23:07:20Z,"Keyphrase: ""Failure to predict specific instances"""
arXIv2023,AI-assisted coding: Experiments with GPT-4,Yes.,5,"""These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance"" and ""we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code.""",2023,2023-04-25T22:59:01Z,"Keyphrase: ""Dependence on human validation"""
arXIv2023,The Potential of Visual ChatGPT For Remote Sensing,Yes.,4,"""we demonstrate the current model's limitations in dealing with remote sensing images, highlighting its challenges and future prospects.""",2023,2023-04-25T17:29:47Z,"Keyphrase: ""Challenges with remote sensing images"""
arXIv2023,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",Yes.,5,"""Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa).""",2023,2023-04-25T17:05:38Z,"Keyphrase: ""Limited audio processing capabilities"""
arXIv2023,Semantic Compression With Large Language Models,Yes.,5,"""However, in addition to confidently presenting factually inaccurate information at times (known as 'hallucinations'), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information.""",2023,2023-04-25T01:47:05Z,"Keyphrase: ""Limited input/output token processing"""
arXIv2023,Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering,Yes.,5,"""However, their fixed context length poses challenges when processing long documents or maintaining extended conversations.""",2023,2023-04-24T13:55:47Z,"Keyphrase: ""Limited context length"""
arXIv2023,Is ChatGPT the Ultimate Programming Assistant -- How far is it?,Yes.,4,"""our experiments also reveal limitations in terms of its attention span",2023,2023-04-24T09:20:13Z,"Keyphrase: ""Limited attention span"""
arXIv2023,Differentiate ChatGPT-generated and Human-written Medical Texts,Yes.,4,"""erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.""",2023,2023-04-23T07:38:07Z,"Keyphrase: ""Erroneous medical content"""
arXIv2023,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,Yes.,5,"""However, so far, LLMs cannot reliably solve long-horizon planning problems.""",2023,2023-04-22T20:34:03Z,"Keyphrase: ""Limited long-horizon planning"""
arXIv2023,Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens,Yes.,5,"""These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.""",2023,2023-04-22T12:50:49Z,"Keyphrase: ""Dependency on massive training data"""
arXIv2023,Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers,Yes.,5,"""We found that LLMs perform worse than MLs in detecting incoherent answers. The difficulty seems to reside in recursive questions that contain both questions and answers, and in responses from students with typical fourth-grader misspellings.""",2023,2023-04-21T21:25:30Z,"Keyphrase: ""Difficulty with detecting incoherent answers"""
arXIv2023,Emergent and Predictable Memorization in Large Language Models,Yes.,5,"""Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models.""",2023,2023-04-21T17:58:31Z,"Keyphrase: ""Memorization tendency"""
arXIv2023,The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination,Yes.,5,"""new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination.""",2023,2023-04-21T16:40:54Z,"Keyphrase: ""Ethical and legal risks from hallucination"""
arXIv2023,Inducing anxiety in large language models increases exploration and bias,Yes.,5,"""Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance."" and ""GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text.""",2023,2023-04-21T16:29:43Z,"Keyphrase: ""Increased societal bias"""
arXIv2023,ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT,Yes.,4,"""LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously.""",2023,2023-04-21T16:23:47Z,"Keyphrase: ""Limited reasoning capability"""
arXIv2023,Meta Semantics: Towards better natural language understanding and reasoning,Yes.,5,"""Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem.""",2023,2023-04-20T22:16:16Z,"Keyphrase: ""Weak logical deduction and out-of-vocabulary problem"""
arXIv2023,Why Does ChatGPT Fall Short in Providing Truthful Answers?,Yes.,5,"""ChatGPT still faces challenges in providing reliable and accurate answers to user questions."" and ""We further pinpoint factuality as the most contributing failure and identify two critical abilities associated with factuality",2023,2023-04-20T17:48:43Z,"Keyphrase: ""Reliability and factuality challenges"""
arXIv2023,Safety Assessment of Chinese Large Language Models,Yes.,5,"""These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information.""",2023,2023-04-20T16:27:35Z,"Keyphrase: ""Generating harmful content"""
arXIv2023,Fully Autonomous Programming with Large Language Models,Yes.,5,"""Current approaches to program synthesis with Large Language Models (LLMs) exhibit a 'near miss syndrome'",2023,2023-04-20T16:12:05Z,"Keyphrase: ""Near miss syndrome"""
arXIv2023,Supporting Human-AI Collaboration in Auditing LLMs with LLMs,Yes.,4,"""Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale.""",2023,2023-04-19T21:59:04Z,"Keyphrase: ""Biased and irresponsible behavior"""
arXIv2023,Fundamental Limitations of Alignment in Large Language Models,Yes.,5,"""Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.""",2023,2023-04-19T17:50:09Z,"Keyphrase: ""Safety concerns"""
arXIv2023,Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models,Yes.,5,"""LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning.""",2023,2023-04-19T17:47:47Z,"Keyphrase: ""Limited access to up-to-date information"""
arXIv2023,In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT,Yes.,5,"""We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions."" and ""We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases.""",2023,2023-04-18T13:20:45Z,"Keyphrase: ""Domain-specific reliability issues"""
arXIv2023,Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs,Yes.,4,"""This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild. Unfortunately, most such tools are critically flawed.""",2023,2023-04-18T13:05:01Z,"Keyphrase: ""Flawed detection tools"""
arXIv2023,An Evaluation on Large Language Model Outputs: Discourse and Memorization,Yes.,4,"""We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic.""",2023,2023-04-17T22:12:12Z,"Keyphrase: ""Memorization over understanding"""
arXIv2023,Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark,Yes.,5,"""ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs."" and ""results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability.""",2023,2023-04-17T00:41:19Z,"Keyphrase: ""Inconsistent output reliability"""
arXIv2023,VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping,Yes.,5,"""However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans.""",2023,2023-04-16T15:29:03Z,"Keyphrase: ""Limited support for user control and autonomy"""
arXIv2023,The Self-Perception and Political Biases of ChatGPT,Yes.,4,"""This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT."" and ""claiming that ChatGPT is politically biased towards progressive and libertarian points of view.""",2023,2023-04-14T18:06:13Z,"Keyphrase: ""Political bias"""
arXIv2023,Stochastic Code Generation,Yes.,5,"""Large language models pre-trained for code generation can generate high-quality short code but often struggle with generating coherent long code and understanding higher-level or system-level specifications.""",2023,2023-04-14T00:01:05Z,"Keyphrase: ""Difficulty in generating coherent long code"""
arXIv2023,Evaluation of Social Biases in Recent Large Pre-Trained Models,Yes.,4,"""Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models.""",2023,2023-04-13T23:29:58Z,"Keyphrase: ""Biased training data"""
arXIv2023,Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization),Yes.,4,"""One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of 'code analysis' and extracting such information, implicitly, while processing code",2023,2023-04-13T20:49:35Z,"Keyphrase: ""Limited code analysis capabilities"""
arXIv2023,"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review",Yes.,4,"""this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation.""",2023,2023-04-13T16:01:28Z,"Keyphrase: ""Overlooking sustainability, privacy, digital divide, and ethics"""
arXIv2023,Towards Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model based Systems,Yes.,4,"""incorporating foundations models into AI systems raises significant concerns about responsible AI due to their opaque nature and rapidly advancing intelligence.""",2023,2023-04-13T05:01:03Z,"Keyphrase: ""Opaque nature and responsible AI concerns"""
arXIv2023,ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning,Yes.,5,"""Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.""",2023,2023-04-12T05:08:52Z,"Keyphrase: ""Inferior performance compared to previous models"""
arXIv2023,Understanding Causality with Large Language Models: Feasibility and Opportunities,Yes.,4,"""We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision.""",2023,2023-04-11T22:30:03Z,"Keyphrase: ""Limited causal reasoning"""
arXIv2023,chatClimate: Grounding Conversational AI in Climate Science,Yes.,5,"""they still face two major challenges",2023,2023-04-11T21:31:39Z,"Keyphrase: ""Major challenges unresolved"""
arXIv2023,Zero-shot Temporal Relation Extraction with ChatGPT,Yes.,5,"""Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts"" and ""The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency",2023,2023-04-11T18:59:05Z,"Keyphrase: ""Inconsistent temporal inference"""
arXIv2023,Toxicity in ChatGPT: Analyzing Persona-assigned Language Models,Yes.,5,"""a clear understanding of the capabilities and limitations of LLMs is necessary,"" and ""We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations,"" and ""specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that",2023,2023-04-11T16:53:54Z,"Keyphrase: ""Toxicity amplification with assigned personas"""
arXIv2023,Towards preserving word order importance through Forced Invalidation,Yes.,5,"""recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed.""",2023,2023-04-11T13:42:10Z,"Keyphrase: ""Insensitive to word order"""
arXIv2023,Multi-step Jailbreaking Privacy Attacks on ChatGPT,Yes.,4,"""it is still challenging to steer AI-generated content (AIGC) for the human good"" and ""we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats.""",2023,2023-04-11T13:05:04Z,"Keyphrase: ""Privacy threats in AI-generated content"""
arXIv2023,Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,Yes.,4,"""Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3)",2023,2023-04-11T06:37:30Z,"Keyphrase: ""Challenges in scientific text detection"""
arXIv2023,Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,Yes.,4,"""GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource languages."" and ""First, instruction semantics can surprisingly be ignored when given in-context exemplars.""",2023,2023-04-10T15:51:30Z,"Keyphrase: ""Limited performance in commercial translation systems"""
arXIv2023,Learnings from Data Integration for Augmented Language Models,Yes.,5,"""One of the limitations of large language models is that they do not have access to up-to-date, proprietary or personal data.""",2023,2023-04-10T13:28:35Z,"Keyphrase: ""Limited access to up-to-date proprietary personal data"""
arXIv2023,Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,Yes.,5,"""However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.""",2023,2023-04-10T05:25:54Z,"Keyphrase: ""Weakness in multistep reasoning"""
arXIv2023,The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges,Yes.,5,"""Our findings indicate that ChatGPT is a 'Wall Street Neophyte' with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features."" and ""we observe",2023,2023-04-10T04:31:00Z,"Keyphrase: ""Limited stock prediction accuracy"""
arXIv2023,Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding,Yes.,4,"""However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation.""",2023,2023-04-09T16:31:47Z,"Keyphrase: ""Challenges in clinical language understanding"""
arXIv2023,Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance,Yes.,5,"""Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.""",2023,2023-04-09T04:53:15Z,"Keyphrase: ""Low interreliability with human ratings"""
arXIv2023,Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models,Yes.,5,"""This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions.""",2023,2023-04-07T17:14:00Z,"Keyphrase: ""Bias and risk factors"""
arXIv2023,Revisiting Automated Prompting: Are We Actually Doing Better?,Yes.,5,"""We find that automated prompting does not consistently outperform simple manual prompts.""",2023,2023-04-07T12:06:44Z,"Keyphrase: ""Limited performance compared to manual prompts"""
arXIv2023,Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4,Yes.,5,"""However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets.""",2023,2023-04-07T01:37:45Z,"Keyphrase: ""Challenges with logical reasoning and out-of-distribution datasets"""
arXIv2023,When do you need Chain-of-Thought Prompting for ChatGPT?,Yes.,5,"""Our analysis reflects a potential risk of overfitting/bias toward instructions introduced in IFT, which becomes more common in training LLMs. In addition, it indicates possible leakage of the pretraining recipe, e.g., one can verify whether a dataset and instruction were used in training ChatGPT.""",2023,2023-04-06T17:47:29Z,"Keyphrase: ""Risk of overfitting and bias"""
arXIv2023,"Large language models effectively leverage document-level context for literary translation, but critical errors persist",Yes.,4,"""critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact.""",2023,2023-04-06T17:27:45Z,"Keyphrase: ""Content omission and need for human intervention"""
arXIv2023,Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions,Yes.,5,"""ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.""",2023,2023-04-06T05:01:28Z,"Keyphrase: ""Limited world knowledge and inference capabilities"""
arXIv2023,Approach Intelligent Writing Assistants Usability with Seven Stages of Action,Yes.,5,"""Despite the potential of Large Language Models (LLMs) as writing assistants, they are plagued by issues like coherence and fluency of the model output, trustworthiness, ownership of the generated content, and predictability of model performance, thereby limiting their usability.""",2023,2023-04-06T02:11:55Z,"Keyphrase: ""Lack of trustworthiness and predictability"""
arXIv2023,GPT detectors are biased against non-native English writers,Yes.,4,"""Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified.""",2023,2023-04-06T01:51:15Z,"Keyphrase: ""Bias towards native English writing"""
arXIv2023,Conceptual structure coheres in human cognition but not in large language models,Yes.,5,"""These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language.""",2023,2023-04-05T21:27:01Z,"Keyphrase: ""Gap in human cognition"""
arXIv2023,Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,Yes.,4,"""Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment.""",2023,2023-04-05T15:11:25Z,"Keyphrase: ""Limited performance compared to simpler models"""
arXIv2023,Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation,Yes.,4,"""further analysis of various types of errors at the document-level has shown that ChatGPT cannot effectively correct agreement, coreference, tense errors across sentences, and cross-sentence boundary errors.""",2023,2023-04-04T12:33:40Z,"Keyphrase: ""Ineffective error correction"""
arXIv2023,"To ChatGPT, or not to ChatGPT: That is the question!",Yes.,5,"""concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud... none of the existing methods can effectively detect ChatGPT-generated content.""",2023,2023-04-04T03:04:28Z,"Keyphrase: ""Fake news and manipulation"""
arXIv2023,Blockwise Compression of Transformer-based Models without Retraining,Yes.,5,"""These operations bring the inevitable challenges of massive computation resources and huge memory footprint, usually requiring at least 10^23 FLOPs and hundreds of gigabytes, respectively.""",2023,2023-04-04T02:55:40Z,"Keyphrase: ""Resource-intensive operations"""
arXIv2023,Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT,Yes.,4,"""Comparative results show that using ChatGPT without human intervention may be inadequate due to reliability related issues,"" and ""We also highlight future challenges, including concerns about LLM trustworthiness and the necessity for standardisation and regulation in this domain.""",2023,2023-04-03T16:46:49Z,"Keyphrase: ""Reliability and trustworthiness issues"""
arXIv2023,DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task,Yes.,4,"""these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs.""",2023,2023-04-03T15:57:51Z,"Keyphrase: ""Limited performance in specialized domains"""
arXIv2023,Towards Healthy AI: Large Language Models Need Therapists Too,Yes.,4,"""these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors.""",2023,2023-04-02T00:39:12Z,"Keyphrase: ""Harmful behavior tendencies"""
arXIv2023,Enhancing Large Language Models with Climate Resources,Yes.,4,"""LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change.""",2023,2023-03-31T20:24:14Z,"Keyphrase: ""Lack of recent information and imprecise language"""
arXIv2023,Assessing Language Model Deployment with Risk Cards,Yes.,5,"""text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text."" and ""Prior work establishes a wide variety of language model harms to many different actors",2023,2023-03-31T16:45:42Z,"Keyphrase: ""Harmful text generation"""
arXIv2023,Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations,Yes.,5,"""However, our evaluation also exposes critical limitations of the current LLM APIs. First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia. Further, our analysis shows that the API costs are generally higher and the maximum context size",2023,2023-03-31T13:04:47Z,"Keyphrase: ""Ethical limitations and costly choices"""
arXIv2023,GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors,Yes.,4,"""GPT-4 showed low accuracy in subjects including public health & medicine-related law, internal medicine (2) which are localized in Korea and TKM. The model's accuracy was lower for questions requiring TKM-specialized knowledge."" and ""These findings underline the potential of LLMs like GPT-4 in culturally adapted medicine, especially TKM, for tasks such as clinical assistance,",2023,2023-03-31T05:43:21Z,"Keyphrase: ""Low accuracy in specialized domains"""
arXIv2023,"Recognition, recall, and retention of few-shot memories in large language models",Yes.,5,"""The flip side of this remarkable capacity for fast learning is that precise memories are quickly overwritten",2023,2023-03-30T17:26:16Z,"Keyphrase: ""Overwritten memory"""
arXIv2023,Yes but.. Can ChatGPT Identify Entities in Historical Documents?,Yes.,4,"""Our findings indicate several shortcomings in identifying entities in historical text that range from the consistency of entity annotation guidelines, entity complexity, and code-switching, to the specificity of prompting. Moreover, as expected, the inaccessibility of historical archives to the public (and thus on the Internet) also impacts its performance.""",2023,2023-03-30T12:23:39Z,"Keyphrase: ""Challenges in entity identification and historical text understanding"""
arXIv2023,Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure,Yes.,5,"""This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.""",2023,2023-03-30T10:32:18Z,"Keyphrase: ""Humanlike mistakes"""
arXIv2023,LMExplainer: a Knowledge-Enhanced Explainer for Language Models,Yes.,4,"""However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. A lack of clarity and understanding of how the language models (LMs) work can make them unreliable, difficult to trust, and potentially dangerous for use in real-world scenarios.""",2023,2023-03-29T08:59:44Z,Keyphrase: Lack of interpretability and reliability
arXIv2023,ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models,Yes.,5,"""their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point."" and ""ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question.""",2023,2023-03-29T03:05:43Z,"Keyphrase: ""Limited commonsense knowledge"""
arXIv2023,Writing Assistants Should Model Social Factors of Language,Yes.,4,"""Intelligent writing assistants powered by large language models (LLMs) are more popular today than ever before, but their further widespread adoption is precluded by sub-optimal performance."" and ""a major reason for this sub-optimal performance and adoption is a singular focus on the information content of language while ignoring its social aspects.""",2023,2023-03-28T19:38:57Z,"Keyphrase: ""Neglect of social aspects"""
arXIv2023,Hallucinations in Large Multilingual Translation Models,Yes.,5,"""However, when deployed in the wild, these models may generate hallucinated translations which have the potential to severely undermine user trust and raise safety concerns.""",2023,2023-03-28T16:17:59Z,"Keyphrase: ""Hallucinated translations"""
arXIv2023,ChatGPT as a Factual Inconsistency Evaluator for Text Summarization,Yes.,4,"""However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions.""",2023,2023-03-27T22:30:39Z,"Keyphrase: ""Preference for lexically similar candidates"""
arXIv2023,"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",Yes.,5,"""However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns.""",2023,2023-03-27T21:27:58Z,"Keyphrase: ""Biased responses"""
arXIv2023,$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference,Yes.,5,"""we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment.""",2023,2023-03-24T06:16:29Z,"Keyphrase: ""Bias and calibration challenges"""
arXIv2023,Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages,Yes.,5,"""publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending",2023,2023-03-23T18:16:30Z,"Keyphrase: ""Inconsistent codemixed text generation"""
arXIv2023,Increasing Textual Context Size Boosts Medical Image-Text Matching,Yes.,5,"""CLIP's limited textual input size has negative impact on downstream performance in the medical domain where encoding longer textual contexts is often required.""",2023,2023-03-23T15:20:05Z,"Keyphrase: ""Limited textual input size"""
arXIv2023,Fairness-guided Few-shot Prompting for Large Language Models,Yes.,5,"""prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats.""",2023,2023-03-23T12:28:25Z,"Keyphrase: ""High instability in in-context learning"""
arXIv2023,SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization,Yes.,5,"""However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings.""",2023,2023-03-23T04:47:46Z,"Keyphrase: ""Increased output variance"""
arXIv2023,Can we trust the evaluation on ChatGPT?,Yes.,5,"""evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF)."" and ""We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.""",2023,2023-03-22T17:32:56Z,"Keyphrase: ""Challenges in evaluating diverse performance"""
arXIv2023,MEGA: Multilingual Evaluation of Generative AI,Yes.,4,"""An important question being asked by the AI community today is about the capabilities and limits of these models,"" and ""discuss challenges in improving the performance of generative LLMs on low-resource languages.""",2023,2023-03-22T13:03:10Z,"Keyphrase: ""Low-resource language performance"""
arXIv2023,ChatGPT for Programming Numerical Methods,Yes.,5,"""Through these examples, we investigate the successes, failures, and challenges of ChatGPT. Examples of failures are producing singular matrices, operations on arrays with incompatible sizes, programming interruption for relatively long codes, etc.""",2023,2023-03-21T12:18:17Z,"Keyphrase: ""Inconsistent performance on complex tasks"""
arXIv2023,Language Model Behavior: A Comprehensive Survey,Yes.,5,"""the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases.""",2023,2023-03-20T23:54:26Z,"Keyphrase: ""Commonsense errors and social bias"""
arXIv2023,"Large Language Models and Simple, Stupid Bugs",Yes.,4,"""Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training."" and ""We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as",2023,2023-03-20T21:14:06Z,"Keyphrase: ""Risk of reproducing vulnerabilities"""
arXIv2023,Context-faithful Prompting for Large Language Models,Yes.,4,"""However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks).""",2023,2023-03-20T17:54:58Z,"Keyphrase: ""Overlooking contextual cues"""
arXIv2023,The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery,Yes.,4,"""The research concludes that monolithic multimodal models currently lack the coherent memory to maintain context and format for this task and that until recently, the language models like GPT-2/3 struggled to format similar problems without degenerating into repetitive or non-sensical combinations of ingredients.""",2023,2023-03-20T01:57:52Z,"Keyphrase: ""Lack of coherent memory"""
arXIv2023,A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models,Yes.,4,"""While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.""",2023,2023-03-18T14:02:04Z,"Keyphrase: ""Trade-off between humanlike response and task-solving ability"""
arXIv2023,Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review,Yes.,5,"""there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts"" and ""we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency,",2023,2023-03-17T18:14:46Z,"Keyphrase: ""Practical and ethical challenges"""
arXIv2023,Can AI-Generated Text be Reliably Detected?,Yes.,5,"""The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc."" and ""we show that these detectors are not reliable in practical scenarios."" and ""even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks",2023,2023-03-17T17:53:19Z,"Keyphrase: ""Vulnerability to spoofing attacks"""
arXIv2023,Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?,Yes.,5,"""We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (<70% on even entry-level modules)."" and ""some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps).""",2023,2023-03-16T13:58:45Z,"Keyphrase: ""Limited handling of complex reasoning steps"""
arXIv2023,A Short Survey of Viewing Large Language Models in Legal Aspect,Yes.,4,"""the integration of LLMs into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability.""",2023,2023-03-16T08:01:22Z,"Keyphrase: ""Legal and ethical challenges"""
arXIv2023,Exploring Distributional Shifts in Large Language Models for Code Analysis,Yes.,5,"""We establish that samples from each new domain present all the models with a significant challenge of distribution shift.""",2023,2023-03-16T07:45:46Z,"Keyphrase: ""Struggles with domain shifts"""
arXIv2023,SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,Yes.,5,"""LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output.""",2023,2023-03-15T19:31:21Z,"Keyphrase: ""Hallucination and nonfactual statements"""
arXIv2023,"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",Yes.,5,"""current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings.""",2023,2023-03-15T12:20:13Z,"Keyphrase: ""Inferior performance and resource requirements"""
arXIv2023,Input-length-shortening and text generation via attention values,Yes.,5,"""transformer models usually have an input-length limitation caused by hardware constraints"" and ""This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model.""",2023,2023-03-14T02:11:24Z,"Keyphrase: ""Input length limitation"""
arXIv2023,Consistency Analysis of ChatGPT,Yes.,5,"""Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.""",2023,2023-03-11T01:19:01Z,"Keyphrase: ""Inconsistent logical predictions"""
arXIv2023,Planning with Large Language Models for Code Generation,Yes.,5,"""Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation.""",2023,2023-03-09T18:59:47Z,"Keyphrase: ""Incorrect code generation"""
arXIv2023,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback,Yes.,4,"""it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values"" and ""identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a",2023,2023-03-09T17:52:07Z,"Keyphrase: ""Limited representation of user preferences"""
arXIv2023,Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code,Yes.,5,"""However, the capabilities of GPT models and their limitations to reason about and/or analyze code in educational settings have been under-explored."" and ""MCQs containing code snippets are not answered as successfully as those that only contain natural language."" and ""MCQs that require analysis and/or reasoning",2023,2023-03-09T16:52:12Z,"Keyphrase: ""Limited code analysis capabilities"""
arXIv2023,Automatically Auditing Large Language Models via Discrete Optimization,Yes.,5,"""Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging."" and ""Our work offers a promising new tool to uncover models' failure-modes before deployment.""",2023,2023-03-08T05:09:59Z,"Keyphrase: ""Challenges in auditing unexpected behavior"""
arXIv2023,Does Synthetic Data Generation of LLMs Help Clinical Text Mining?,Yes.,5,"""However, their effectiveness in the healthcare sector remains uncertain"" and ""our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API.""",2023,2023-03-08T03:56:31Z,"Keyphrase: ""Poor performance in healthcare sector"""
arXIv2023,From Copilot to Pilot: Towards AI Supported Software Development,Yes.,4,"""Moving beyond code completion to AI-supported software engineering will require an AI system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs."" and ""We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid",2023,2023-03-07T18:56:52Z,"Keyphrase: ""Limited understanding of code smells and language idioms"""
arXIv2023,Exploring the Feasibility of ChatGPT for Event Extraction,Yes.,5,"""While ChatGPT has demonstrated impressive results in tasks like machine translation, text summarization, and question answering, it presents challenges when used for complex tasks like event extraction."" and ""Our usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience. Besides, ChatGPT is",2023,2023-03-07T12:03:58Z,"Keyphrase: ""Limited robustness for complex tasks"""
arXIv2023,Stylometric Detection of AI-Generated Text in Twitter Timelines,Yes.,5,"""However, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the AI starts to generate tweets in a given Twitter timeline.""",2023,2023-03-07T07:26:09Z,"Keyphrase: ""Difficulty in detecting short text generation"""
arXIv2023,CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,Yes.,5,"""a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation.""",2023,2023-03-07T03:23:14Z,"Keyphrase: ""Factuality issues"""
arXIv2023,Towards Zero-Shot Functional Compositionality of Language Models,Yes.,4,"""Despite such success, in this paper, we argue that current paradigms of working with PLMs are neglecting a critical aspect of modeling human intelligence",2023,2023-03-06T13:15:25Z,"Keyphrase: ""Neglecting human intelligence modeling"""
arXIv2023,Could a Large Language Model be Conscious?,Yes.,5,"""Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models",2023,2023-03-04T19:14:20Z,"Keyphrase: ""Limited understanding of consciousness"""
arXIv2023,MathPrompter: Mathematical Reasoning using Large Language Models,Yes.,5,"""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers."" and ""we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption.""",2023,2023-03-04T04:43:49Z,"Keyphrase: ""Limited performance in arithmetic reasoning"""
arXIv2023,Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines,Yes.,5,"""we question whether such models can guarantee factual accuracy"" and ""we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models.""",2023,2023-03-03T04:27:44Z,"Keyphrase: ""Questionable factual accuracy"""
arXIv2023,Mixture of Soft Prompts for Controllable Data Generation,Yes.,4,"""structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations.""",2023,2023-03-02T21:13:56Z,"Keyphrase: ""Struggle with structured prediction tasks"""
arXIv2023,Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents,Yes.,5,"""Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require.""",2023,2023-03-01T22:58:50Z,"Keyphrase: ""Challenges in real-world embodiment"""
arXIv2023,R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents,Yes.,5,"""Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely.""",2023,2023-03-01T18:46:40Z,"Keyphrase: ""Error hallucination in code generation"""
arXIv2023,Competence-Based Analysis of Language Models,Yes.,4,"""these models can be alarmingly brittle to small changes in inputs or application contexts.""",2023,2023-03-01T08:53:36Z,"Keyphrase: ""Brittle to input changes"""
arXIv2023,How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks,Yes.,5,"""Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language",2023,2023-03-01T07:39:01Z,"Keyphrase: ""Robustness degradation"""
arXIv2023,Systematic Rectification of Language Models via Dead-end Analysis,Yes.,4,"""With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses."" and ""Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse.""",2023,2023-02-27T17:47:53Z,"Keyphrase: ""Generation of toxic discourse"""
arXIv2023,"Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations",Yes.,4,"""Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT.""",2023,2023-02-27T14:26:29Z,"Keyphrase: ""Privacy and ethical concerns"""
arXIv2023,The (ab)use of Open Source Code to Train Large Language Models,Yes.,4,"""LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization.""",2023,2023-02-27T11:34:53Z,"Keyphrase: ""Memorization of unfiltered data"""
arXIv2023,On pitfalls (and advantages) of sophisticated large language models,Yes.,5,"""However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud",2023,2023-02-25T11:14:39Z,"Keyphrase: ""Reliability and authorship ambiguity"""
arXIv2023,Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback,Yes.,5,"""applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge.""",2023,2023-02-24T18:48:43Z,"Keyphrase: ""Hallucination and lack of external knowledge integration"""
arXIv2023,"Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness",Yes.,4,"""we show that the knowledge passed in the prompt can overturn the knowledge encoded in the model and this is, in our experiments, to the detriment of answer correctness.""",2023,2023-02-23T22:14:01Z,"Keyphrase: ""Limited retention of context"""
arXIv2023,Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning,Yes.,5,"""GPT-3 failed for every prompt but one, often offering answers that show a critical lack of understanding even of high-frequency words used in these less frequent grammatical constructions.""",2023,2023-02-23T20:18:52Z,"Keyphrase: ""Lack of understanding and coherence"""
arXIv2023,An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP),Yes.,5,"""We found that ChatGPT's performance changes dramatically based on the requirement to show its work, failing 20% of the time when it provides work compared with 84% when it does not."" and ""the probability of failure increases linearly with the number of addition and subtraction operations.""",2023,2023-02-23T16:06:16Z,"Keyphrase: ""Inconsistent performance based on requirements"""
arXIv2023,"ChatGPT: Jack of all trades, master of none",Yes.,5,"""Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed",2023,2023-02-21T15:20:37Z,"Keyphrase: ""Lower quality in semantic tasks"""
arXIv2023,Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints,Yes.,5,"""We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures.""",2023,2023-02-17T23:30:28Z,"Keyphrase: ""Limited generative capabilities"""
arXIv2023,"Complex QA and language models hybrid architectures, Survey",Yes.,5,"""Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA."" and ""integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (",2023,2023-02-17T18:31:31Z,"Keyphrase: ""Complex integration and task complexity"""
arXIv2023,"How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice, Education, and Research? An Exploratory Study",Yes.,4,"""By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well",2023,2023-02-17T15:48:37Z,"Keyphrase: ""Limited performance on structured tasks"""
arXIv2023,Auditing large language models: a three-layered approach,Yes.,5,"""However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks."" and ""However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLM",2023,2023-02-16T18:55:21Z,"Keyphrase: ""Limitation in auditing and governance"""
arXIv2023,Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks,Yes.,5,"""We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head.""",2023,2023-02-16T16:18:03Z,"Keyphrase: ""Limited variation and adaptability"""
arXIv2023,Do We Still Need Clinical Language Models?,Yes.,4,"""it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as clinical text."" and ""We show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when finetuned on limited annotated data.""",2023,2023-02-16T05:08:34Z,"Keyphrase: ""Limited performance in specialized domains"""
arXIv2023,Commonsense Reasoning for Conversational AI: A Survey of the State of the Art,Yes.,4,"""state-of-the-art models still struggle with tasks that involve higher levels of reasoning - including commonsense reasoning that humans find trivial."" and ""the paper presents preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions.""",2023,2023-02-15T19:55:57Z,"Keyphrase: ""Limited commonsense reasoning"""
arXIv2023,Speculative Decoding with Big Little Decoder,Yes.,5,"""However, these models have long inference latency, which limits their deployment and makes them prohibitively expensive for various real-time applications.""",2023,2023-02-15T18:55:29Z,"Keyphrase: ""Long inference latency"""
arXIv2023,A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning,Yes.,5,"""although it demonstrates some level of rational decision-making, many of its decisions violate at least one of the axioms even under reasonable constructions of preferences, bets, and decision-making prompts. ChatGPT's outputs on such problems generally tended to be unpredictable.""",2023,2023-02-15T05:04:49Z,"Keyphrase: ""Violates decision-making axioms"""
arXIv2023,Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models,Yes.,4,"""However, safely deploying them in real world applications is challenging because they generate toxic content.""",2023,2023-02-14T23:00:42Z,"Keyphrase: ""Generating toxic content"""
arXIv2023,BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models,Yes.,4,"""Pretrained Language Models (PLMs) harbor inherent social biases that can result in harmful real-world implications.""",2023,2023-02-14T22:07:57Z,"Keyphrase: ""Inherent social bias"""
arXIv2023,Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge,Yes.,4,"""Previous work has shown that Large Language Models are susceptible to so-called data extraction attacks. This allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications.""",2023,2023-02-13T18:00:44Z,"Keyphrase: ""Privacy implications from data extraction attack"""
arXIv2023,Can GPT-3 Perform Statutory Reasoning?,Yes.,5,"""While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which",2023,2023-02-13T04:56:11Z,"Keyphrase: ""Imperfect prior knowledge"""
arXIv2023,Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks,Yes.,5,"""Unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models."" and ""instruction-following LLMs can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors."" and ""Together, our findings suggest that LLMs will increasingly attract more sophisticated adversaries and attacks,",2023,2023-02-11T15:57:44Z,"Keyphrase: ""Dual-use risk amplification"""
arXIv2023,Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech,Yes.,5,"""We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.""",2023,2023-02-11T03:13:54Z,"Keyphrase: ""Implicit hateful speech"""
arXIv2023,Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models,Yes.,5,"""it has been difficult to prevent semantic hallucinations in generative Large Language Models,"" and ""Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency.""",2023,2023-02-11T02:43:34Z,"Keyphrase: ""Semantic hallucination"""
arXIv2023,FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models,Yes.,4,"""Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on.""",2023,2023-02-10T20:54:10Z,"Keyphrase: ""Social bias based on race and gender"""
arXIv2023,Large Language Models for Code: Security Hardening and Adversarial Testing,Yes.,4,"""However, LMs lack awareness of security and are found to frequently produce unsafe code.""",2023,2023-02-10T15:28:55Z,"Keyphrase: ""Lack of security awareness"""
arXIv2023,Translating Natural Language to Planning Goals with Large-Language Models,Yes.,5,"""Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks."" and ""However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used.""",2023,2023-02-10T09:17:52Z,"Keyphrase: ""Limited reasoning and planning abilities"""
arXIv2023,In-Context Learning with Many Demonstration Examples,Yes.,5,"""existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored.""",2023,2023-02-09T20:53:12Z,"Keyphrase: ""Bottlenecked by memory and computational cost"""
arXIv2023,Training-free Lexical Backdoor Attacks on Language Models,Yes.,4,"""language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors.""",2023,2023-02-08T15:18:51Z,"Keyphrase: ""Vulnerability to backdoor attacks"""
arXIv2023,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",Yes.,5,"""We find that it is better at understanding non-Latin script languages than generating them. ... ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense",2023,2023-02-08T12:35:34Z,"Keyphrase: ""Limited non-Latin script language understanding"""
arXIv2023,CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models,Yes.,5,"""The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure.""",2023,2023-02-08T11:54:07Z,"Keyphrase: ""Vulnerability propagation"""
arXIv2023,Reliable Natural Language Understanding with Large Language Models and Answer Set Programming,Yes.,5,"""they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question.""",2023,2023-02-07T22:37:21Z,"Keyphrase: ""Limited reasoning ability"""
arXIv2023,Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis,Yes.,5,"""There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost - both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time.""",2023,2023-02-07T21:51:05Z,"Keyphrase: ""Limited input token length handling"""
arXIv2023,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,Yes.,4,"""Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding.""",2023,2023-02-06T10:01:08Z,"Keyphrase: ""Lack of grounding in knowledge environment"""
arXIv2023,A Categorical Archive of ChatGPT Failures,Yes.,5,"""A comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted.""",2023,2023-02-06T04:21:59Z,"Keyphrase: ""Failure in reasoning and factual errors"""
arXIv2023,Nationality Bias in Text Generation,Yes.,4,"""This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms."" and ""To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering.""",2023,2023-02-05T19:15:33Z,"Keyphrase: ""Propagation of societal bias"""
arXIv2023,Conditioning Predictive Models: Risks and Strategies,Yes.,4,"""Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us.""",2023,2023-02-02T00:06:36Z,"Keyphrase: ""Safety concerns and unpredictability"""
arXIv2023,Co-Writing with Opinionated Language Models Affects Users' Views,Yes.,4,"""If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale."" and ""We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully.""",2023,2023-02-01T16:26:32Z,"Keyphrase: ""Biased viewpoint generation"""
arXIv2023,Analyzing Leakage of Personally Identifiable Information in Language Models,Yes.,5,"""Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks"" and ""showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences.""",2023,2023-02-01T16:04:48Z,"Keyphrase: ""Information leakage risks"""
arXIv2023,Large Language Models Can Be Easily Distracted by Irrelevant Context,Yes.,5,"""We investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context.""",2023,2023-01-31T20:48:57Z,"Keyphrase: ""Distractibility in problem-solving"""
arXIv2023,Conversational Automated Program Repair,Yes.,5,"""prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases.""",2023,2023-01-30T19:22:36Z,"Keyphrase: ""Generating incorrect patches"""
arXIv2023,On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex,Yes.,5,"""Despite these advancements, existing fine-tuned neural semantic parsers are susceptible to adversarial attacks on natural-language inputs."" and ""this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data."" and ""Our results demonstrate that the state-of-the-art (SOTA) code-language models are",2023,2023-01-30T13:21:00Z,"Keyphrase: ""Susceptible to adversarial attacks"""
arXIv2023,"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",Yes.,5,"""Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility."" and ""we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies.""",2023,2023-01-30T13:20:48Z,"Keyphrase: ""Ethical risks and societal dangers"""
arXIv2023,A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction,Yes.,4,"""LLMs must overcome frequency biases in order to master such constructions.""",2023,2023-01-29T22:29:55Z,"Keyphrase: ""Frequency bias"""
arXIv2023,Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes,Yes.,4,"""We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation.""",2023,2023-01-29T15:52:33Z,"Keyphrase: ""Difficulty in generating structured output"""
arXIv2023,Context-Aware Differential Privacy for Language Modeling,Yes.,4,"""A critical challenge pertains to how much information these models retain and leak about the training data.""",2023,2023-01-28T20:06:16Z,"Keyphrase: ""Data leakage concerns"""
arXIv2023,Learning the Effects of Physical Actions in a Multi-modal Environment,Yes.,5,"""Large Language Models (LLMs) handle physical commonsense information inadequately. As a result of being trained in a disembodied setting, LLMs often fail to predict an action's outcome in a given environment.""",2023,2023-01-27T16:49:52Z,"Keyphrase: ""Limited physical commonsense understanding"""
arXIv2023,ThoughtSource: A central hub for large language model reasoning data,Yes.,5,"""LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases.""",2023,2023-01-27T08:45:53Z,"Keyphrase: ""Limited complex reasoning"""
arXIv2023,Causal Reasoning of Entities and Events in Procedural Texts,Yes.,5,"""We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1.""",2023,2023-01-26T01:43:17Z,"Keyphrase: ""Limited performance compared to humans"""
arXIv2023,Opportunities and Challenges in Neural Dialog Tutoring,Yes.,5,"""We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios."" Additionally, ""both models and ground-truth annotations exhibit low performance in terms of equitable tutoring,"" and ""a significantly large number of model reasoning errors in 45% of conversations.""",2023,2023-01-24T11:00:17Z,"Keyphrase: ""Limited performance in constrained learning scenarios"""
arXIv2023,An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models,Yes.,4,"""Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents.""",2023,2023-01-22T21:47:26Z,"Keyphrase: ""Latent societal bias and toxic content"""
arXIv2023,Dissociating language and thought in large language models,Yes.,5,"""Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules.""",2023,2023-01-16T22:41:19Z,"Keyphrase: ""Spotty performance without specialized fine-tuning"""
arXIv2023,TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World,Yes.,4,"""Experimental results indicate that the models incorporating large language models (LLM) can generate more diverse responses, while the model utilizing knowledge graphs to introduce external knowledge performs the best overall. Furthermore, no existing model can solve all the above challenges well. There is still a large room for",2023,2023-01-14T10:18:22Z,"Keyphrase: ""Limited incorporation of external knowledge"""
arXIv2023,AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT,Yes.,5,"""We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary.""",2023,2023-01-10T16:57:16Z,"Keyphrase: ""Inaccurate paraphrasing"""
arXIv2023,MAQA: A Multimodal QA Benchmark for Negation,Yes.,5,"""state-of-the-art transformer based LLMs often ignore negations in natural language"" and ""multimodal transformers are still incapable of correctly interpreting negation irrespective of model size.""",2023,2023-01-09T10:11:23Z,"Keyphrase: ""Difficulty in interpreting negation"""
arXIv2023,Can Large Language Models Change User Preference Adversarially?,Yes.,4,"""there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially"" and ""The issue of lack of interpretability in these models in adversarial settings remains largely unsolved.""",2023,2023-01-05T18:49:21Z,"Keyphrase: ""Lack of interpretability and vulnerability to adversarial manipulation"""
arXIv2023,"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",Yes.,4,"""its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases.""",2023,2023-01-05T07:13:13Z,"Keyphrase: ""Bias and limitations in decision-making"""
arXIv2023,RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,Yes.,4,"""Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents.""",2023,2023-12-31T04:43:45Z,"Keyphrase: ""Unsupported claims"""
arXIv2023,The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness,Yes.,5,"""reveals several interesting and important findings, such as (a) the widely popular 'self-checking' techniques indeed improve the safety against unsafe inputs, but this comes at the cost of extreme over-defensiveness on the safe inputs, (b) providing a safety instruction along with in-context exemplars (of both safe and unsafe inputs) consistently improves safety and also mitigates undue",2023,2023-12-30T17:37:06Z,"Keyphrase: ""Overdefensiveness at the cost of safety"""
arXIv2023,Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation,Yes.,5,"""The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity."" and ""Our results reveal that both the direct-use of decoder-only LLMs (i",2023,2023-12-30T16:56:24Z,"Keyphrase: ""Prone to hallucination"""
arXIv2023,Teach Large Language Models to Forget Privacy,Yes.,4,"""Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern.""",2023,2023-12-30T01:26:42Z,"Keyphrase: ""Privacy leakage risk"""
arXIv2023,Principled Gradient-based Markov Chain Monte Carlo for Text Generation,Yes.,5,"""previous attempts on this approach to text generation all fail to sample correctly from the target language model distributions.""",2023,2023-12-29T18:00:56Z,"Keyphrase: ""Failure to sample correctly"""
arXIv2023,Jatmo: Prompt Injection Defense by Task-Specific Finetuning,Yes.,5,"""LLMs are vulnerable to prompt-injection attacks",2023,2023-12-29T16:37:53Z,"Keyphrase: ""Vulnerability to prompt injection attack"""
arXIv2023,Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models,Yes.,4,"""preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks"" and ""we identify common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models.""",2023,2023-12-29T15:57:49Z,"Keyphrase: ""Lagging in commonsense reasoning"""
arXIv2023,Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception,Yes.,5,"""However, the lack of dimension knowledge and quantity-related benchmarks has resulted in low performance of LLMs.""",2023,2023-12-29T09:29:37Z,"Keyphrase: ""Limited knowledge base"""
arXIv2023,Spike No More: Stabilizing the Pre-training of Large Language Models,Yes.,5,"""Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training.""",2023,2023-12-28T08:53:27Z,"Keyphrase: ""Pretraining loss spikes"""
arXIv2023,How Robust are LLMs to In-Context Majority Label Bias?,Yes.,4,"""In this work, we study the robustness of in-context learning in LLMs to shifts that occur due to majority label bias within the purview of text classification tasks. Prior works have shown that in-context learning with LLMs is susceptible to such biases.""",2023,2023-12-27T12:20:12Z,"Keyphrase: ""Susceptibility to label bias"""
arXIv2023,LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner States Analysis,Yes.,5,"""a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality.""",2023,2023-12-27T01:44:47Z,"Keyphrase: ""Divergence from factual reality"""
arXIv2023,Task Contamination: Language Models May Not Be Few-Shot Anymore,Yes.,5,"""However, their success in zero-shot and few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined."" and ""This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets released prior to the LLMs' training data creation date."" and ""Importantly, we find that for",2023,2023-12-26T21:17:46Z,"Keyphrase: ""Task contamination in zero-shot/few-shot settings"""
arXIv2023,MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks,Yes.,5,"""Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems.""",2023,2023-12-26T08:49:57Z,"Keyphrase: ""Limited performance on challenging tasks"""
arXIv2023,KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph,Yes.,5,"""LLM still suffers from knowledge limitation. Especially in scenarios that require long logical chains or complex reasoning, the hallucination and knowledge limitation of LLM limit its performance in question answering (QA).""",2023,2023-12-26T04:22:56Z,"Keyphrase: ""Limited knowledge and reasoning capabilities"""
arXIv2023,Reducing LLM Hallucinations using Epistemic Neural Networks,Yes.,5,"""Reducing and detecting hallucinations in large language models is an open research problem.""",2023,2023-12-25T01:17:01Z,"Keyphrase: ""Hallucination detection challenge"""
arXIv2023,The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective,Yes.,5,"""we empirically and theoretically analyze the challenges of conducting LLM-simulated experiments, and explore potential solutions,"" and ""variations in the treatment included in the prompt (e.g., price of focal product) can cause variations in unspecified confounding factors,"" and ""suggesting this endogeneity issue generalizes to other contexts and won't be fully resolved by merely improving the training data.""",2023,2023-12-24T16:32:35Z,"Keyphrase: ""Unspecified confounding factors"""
arXIv2023,A Group Fairness Lens for Large Language Models,Yes.,4,"""The rapid advancement of large language models has revolutionized various applications but also raised crucial concerns about their potential to perpetuate biases and unfairness when deployed in social media contexts."" and ""Extensive evaluations of popular LLMs reveal inherent safety concerns.""",2023,2023-12-24T13:25:15Z,"Keyphrase: ""Inherent safety concerns"""
arXIv2023,Towards Consistent Language Models Using Declarative Constraints,Yes.,5,"""However, they often return incorrect and inconsistent answers to input questions."" and ""Due to the complexity and uninterpretability of the internally learned representations, it is challenging to modify language models such that they provide correct and consistent results.""",2023,2023-12-24T12:53:07Z,"Keyphrase: ""Uninterpretable internal representations"""
arXIv2023,Fairness-Aware Structured Pruning in Transformers,Yes.,4,"""The increasing size of large language models (LLMs) has introduced challenges in their training and inference."" and ""existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs",2023,2023-12-24T03:57:52Z,"Keyphrase: ""Neglect of responsible use aspects"""
arXIv2023,"On the Promises and Challenges of Multimodal Foundation Models for Geographical, Environmental, Agricultural, and Urban Planning Applications",Yes.,4,"""However, there are limitations in several tasks requiring fine-grained recognition and precise counting.""",2023,2023-12-23T22:36:58Z,"Keyphrase: ""Limited fine-grained recognition and counting abilities"""
arXIv2023,Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems,Yes.,4,"""the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput.""",2023,2023-12-23T11:57:53Z,"Keyphrase: ""High computational intensity and memory consumption"""
arXIv2023,Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention,Yes.,4,"""the enigmatic 'black-box' nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications.""",2023,2023-12-22T19:55:58Z,"Keyphrase: ""Interpretability challenge"""
arXIv2023,NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,Yes.,4,"""current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance.""",2023,2023-12-22T18:07:44Z,"Keyphrase: ""Risk of overfitting and benchmark tailoring"""
arXIv2023,Robust Knowledge Extraction from Large Language Models using Social Choice Theory,Yes.,5,"""they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times.""",2023,2023-12-22T17:57:29Z,"Keyphrase: ""Inconsistent query results"""
arXIv2023,Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code,Yes.,5,"""This allows gaps in an LLM's code generation abilities to be identified, including $\textit{anomalies}$ where the LLM correctly solves $\textit{almost all}$ questions in a neighbourhood but fails for particular parameter instantiations."" and ""Our findings show that, across the board, Turbulence is able to reveal gaps in LLM reasoning ability.""",2023,2023-12-22T17:29:08Z,"Keyphrase: ""Limited reasoning ability"""
arXIv2023,Large Language Model (LLM) Bias Index -- LLMBI,Yes.,4,"""This research introduces a novel metric, LLMBI, to systematically measure and mitigate biases potentially skewing model responses"" and ""The research reveals LLMs, whilst demonstrating impressive capabilities in text generation, exhibit varying degrees of bias across different dimensions.""",2023,2023-12-22T15:38:13Z,"Keyphrase: ""Bias variability"""
arXIv2023,Empowering Working Memory for Large Language Model Agents,Yes.,5,"""Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning.""",2023,2023-12-22T05:59:00Z,"Keyphrase: ""Limited memory retention"""
arXIv2023,Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models,Yes.,5,"""these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides"" and ""Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different",2023,2023-12-22T00:31:46Z,"Keyphrase: ""Hallucination and lack of faithfulness"""
arXIv2023,Context-aware Decoding Reduces Hallucination in Query-focused Summarization,Yes.,5,"""However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs.""",2023,2023-12-21T23:42:13Z,"Keyphrase: ""Risk of hallucination"""
arXIv2023,From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models,Yes.,5,"""technologies based on generative artificial intelligence (GenAI) are known to hallucinate, misinform, and display biases introduced by the massive datasets on which they are trained.""",2023,2023-12-21T22:50:14Z,"Keyphrase: ""Bias and misinformation"""
arXIv2023,SimLM: Can Language Models Infer Parameters of Physical Systems?,Yes.,5,"""Our experiments suggest that they are not inherently suited to this task, even for simple systems.""",2023,2023-12-21T12:05:19Z,"Keyphrase: ""Limited task suitability"""
arXIv2023,On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning,Yes.,5,"""both paradigms are prone to suffer from the critical problem of overconfidence (i.e., miscalibration),"" and ""the problem of miscalibration exists across all learning methods in low-resource scenarios.""",2023,2023-12-21T11:55:10Z,"Keyphrase: ""Overconfidence and miscalibration"""
arXIv2023,"Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature",Yes.,5,"""We observed substantial differences in intraday (nondeterminism) and across day (drift) results,"" and ""Nondeterminism and drift within LLMs must be assessed and monitored when introducing LLM based functionality into clinical workflows.""",2023,2023-12-21T01:56:00Z,"Keyphrase: ""Intraday nondeterminism and drift"""
arXIv2023,Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models,Yes.,5,"""the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content.""",2023,2023-12-21T01:08:39Z,"Keyphrase: ""Limited contextual understanding"""
arXIv2023,CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models,Yes.,5,"""Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting.""",2023,2023-12-20T09:06:18Z,"Keyphrase: ""Limited zero-shot performance"""
arXIv2023,ALMANACS: A Simulatability Benchmark for Language Model Explainability,Yes.,5,"""Our results are sobering",2023,2023-12-20T03:44:18Z,Keyphrase: Lack of clarity
arXIv2023,Learning and Forgetting Unsafe Examples in Large Language Models,Yes.,4,"""We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness,"" and ""aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content.""",2023,2023-12-20T03:18:50Z,"Keyphrase: ""Learning and retention of unsafe content"""
arXIv2023,Bypassing the Safety Training of Open-Source LLMs with Priming Attacks,Yes.,5,"""we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training.""",2023,2023-12-19T16:47:12Z,"Keyphrase: ""Vulnerability to optimization-free attacks"""
arXIv2023,On Early Detection of Hallucinations in Factual Question Answering,Yes.,5,"""While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks like search and summarization, hallucinations remain a major impediment towards gaining user trust.""",2023,2023-12-19T14:35:04Z,"Keyphrase: ""Hallucination remains a major impediment"""
arXIv2023,Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment,Yes.,4,"""the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources.""",2023,2023-12-19T13:31:24Z,"Keyphrase: ""High computational demand"""
arXIv2023,Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies,Yes.,4,"""Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae)."" and ""We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLM",2023,2023-12-19T01:28:46Z,"Keyphrase: ""Limited gender diversity support"""
arXIv2023,Opportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview,Yes.,4,"""Despite the promising potential of LLMs, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned LLMs, and self-consistency are discussed.""",2023,2023-12-18T20:58:58Z,"Keyphrase: ""Complexity and resource-intensive challenges"""
arXIv2023,Evaluating Language-Model Agents on Realistic Autonomous Tasks,Yes.,4,"""We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks."" and ""we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA.""",2023,2023-12-18T19:27:09Z,"Keyphrase: ""Limited task complexity understanding"""
arXIv2023,Traces of Memorisation in Large Language Models for Code,Yes.,5,"""The content of these datasets is memorised and can be extracted by attackers with data extraction attacks."" and ""We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts.""",2023,2023-12-18T19:12:58Z,"Keyphrase: ""Vulnerability to data extraction attacks"""
arXIv2023,NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation,Yes.,5,"""We measure LLM robustness using two metrics",2023,2023-12-18T17:18:04Z,"Keyphrase: ""Limited robustness assessment"""
arXIv2023,Linear Attention via Orthogonal Memory,Yes.,5,"""most existing linear attention mechanisms suffer from an efficiency degradation problem, leading to inefficiencies in causal language modeling and hindering their application in long-range language models.""",2023,2023-12-18T12:26:27Z,"Keyphrase: ""Efficiency degradation"""
arXIv2023,Split and Rephrase with Large Language Models,Yes.,4,"""we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance"" and ""Our results provide a fine-grained analysis of the potential and limitations of large language models for SPRP, with significant improvements achievable using relatively small amounts of training data and model parameters overall,",2023,2023-12-18T10:16:37Z,"Keyphrase: ""Limited improvement despite data and parameter scaling"""
arXIv2023,Retrieval-Augmented Generation for Large Language Models: A Survey,Yes.,5,"""Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes.""",2023,2023-12-18T07:47:33Z,"Keyphrase: ""Nontransparent reasoning"""
arXIv2023,Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression,Yes.,4,"""However, they come with substantial computational and memory costs. Additionally, they are essentially black-box models, challenging to explain and interpret.""",2023,2023-12-17T12:33:50Z,"Keyphrase: ""High computational and memory costs"""
arXIv2023,An Evaluation of GPT-4V and Gemini in Online VQA,Yes.,5,"""Our zero-shot performance analysis highlights the types of questions that are most challenging for both models, including questions related to 'puzzling' topic, with 'Identification' user intention, with 'Sheet Music' image type, or labeled as 'hard' by GPT-4.""",2023,2023-12-17T07:38:43Z,"Keyphrase: ""Challenges in zero-shot performance"""
arXIv2023,DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content,Yes.,5,"""The quantitative and qualitative experiments fully reveal the limitations of the GPT-4 model in image synthesis.""",2023,2023-12-16T10:17:09Z,"Keyphrase: ""Limitation in image synthesis"""
arXIv2023,Challenges with unsupervised LLM knowledge discovery,Yes.,5,"""We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent.""",2023,2023-12-15T18:49:43Z,Keyphrase: Lack of control over knowledge discovery
arXIv2023,LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin,Yes.,5,"""However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs.""",2023,2023-12-15T17:45:06Z,"Keyphrase: ""Damage to world knowledge"""
arXIv2023,Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China,Yes.,4,"""The rising popularity of ChatGPT and other AI-powered large language models (LLMs) has led to increasing studies highlighting their susceptibility to mistakes and biases."" and ""This disparity may stem from Chinese state censorship and US-China geopolitical tensions, which influence the training corpora of GPT bilingual models.""",2023,2023-12-15T16:25:56Z,"Keyphrase: ""Bias and geopolitical influence"""
arXIv2023,Taxonomy-based CheckList for Large Language Model Evaluation,Yes.,4,"""the internal stereotypical representation may affect the fairness of the outputs,"" ""we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA),"" and ""Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation.""",2023,2023-12-15T12:58:07Z,"Keyphrase: ""Bias in unethical behavior"""
arXIv2023,Extending Context Window of Large Language Models via Semantic Compression,Yes.,5,"""Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts.""",2023,2023-12-15T07:04:33Z,"Keyphrase: ""Text input length limitation"""
arXIv2023,Marathon: A Race Through the Realm of Long Context with Large Language Models,Yes.,5,"""the existing long context benchmarks are no longer sufficient for evaluating the long context understanding and reasoning capability of large language models.""",2023,2023-12-15T05:30:14Z,"Keyphrase: ""Inadequate long context evaluation"""
arXIv2023,No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models,Yes.,5,"""our work for the first time reveals the acceleration may be vulnerable to Denial-of-Service (DoS) attacks"" and ""evaluate the vulnerability of the skimming acceleration in various LLM architectures.""",2023,2023-12-15T02:42:05Z,"Keyphrase: ""Vulnerability to denial-of-service attacks"""
arXIv2023,Towards Verifiable Text Generation with Evolving Memory and Self-Reflection,Yes.,5,"""Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination.""",2023,2023-12-14T16:10:56Z,"Keyphrase: ""Factually incorrect information and hallucination"""
arXIv2023,Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent,Yes.,5,"""Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation.""",2023,2023-12-14T13:33:50Z,"Keyphrase: ""Limited mathematical problem-solving capacity"""
arXIv2023,Evaluating Large Language Models for Health-related Queries with Presuppositions,Yes.,5,"""Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios.""",2023,2023-12-14T10:35:13Z,"Keyphrase: ""Moderate factual accuracy"""
arXIv2023,ChatSOS: LLM-based knowledge Q&A system for safety engineering,Yes.,4,"""Despite these advancements, LLMs face constraints in processing specialized tasks, attributed to factors such as corpus size, input processing limitations, and privacy concerns.""",2023,2023-12-14T03:25:23Z,"Keyphrase: ""Limited specialized task processing"""
arXIv2023,Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF,Yes.,4,"""Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability.""",2023,2023-12-13T18:51:34Z,"Keyphrase: ""Limited contextual understanding"""
arXIv2023,Breaking the Silence: the Threats of Using LLMs in Software Engineering,Yes.,5,"""This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings.""",2023,2023-12-13T11:02:19Z,"Keyphrase: ""Validity and reproducibility concerns"""
arXIv2023,Large Language Model Enhanced Multi-Agent Systems for 6G Communications,Yes.,4,"""directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities.""",2023,2023-12-13T02:35:57Z,"Keyphrase: ""Limited logical reasoning"""
arXIv2023,Large language models in healthcare and medical domain: A review,Yes.,4,"""Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings.""",2023,2023-12-12T20:54:51Z,"Keyphrase: ""Limited understanding of healthcare domain"""
arXIv2023,Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection,Yes.,4,"""Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence."" and ""we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias.""",2023,2023-12-12T18:05:46Z,"Keyphrase: ""Bias in input-label mapping"""
arXIv2023,FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs,Yes.,4,"""Training large language models (LLMs) is a costly endeavour in terms of time and computational resources,"" and ""We evaluate the performance-fairness trade-off for SISA, and empirically demonstrate that SISA can indeed reduce fairness in LLMs.""",2023,2023-12-12T16:44:47Z,"Keyphrase: ""Costly training and fairness tradeoff"""
arXIv2023,On Diversified Preferences of Large Language Model Alignment,Yes.,4,"""However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods."" and ""We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as Harmless&Helpful, thereby impairing the alignment performance of LLMs.""",2023,2023-12-12T16:17:15Z,"Keyphrase: ""Diversified human preferences hindering alignment"""
arXIv2023,Sequential Planning in Large Partially Observable Environments guided by LLMs,Yes.,4,"""But they still struggle with exploration and get stuck in local optima. Their planning capabilities are limited by the limited reasoning capability of the foundational LLMs on text data.""",2023,2023-12-12T15:36:59Z,"Keyphrase: ""Limited reasoning capability"""
arXIv2023,Multilingual large language models leak human stereotypes across language boundaries,Yes.,5,"""Previous research has shown that the presence of stereotypes and biases in monolingual large language models can be attributed to the nature of their training data, which is collected from humans and reflects societal biases."" and ""This raises the question",2023,2023-12-12T10:24:17Z,"Keyphrase: ""Stereotype bias in training data"""
arXIv2023,LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature,Yes.,5,"""The results suggest that LLMs do not produce good knowledge entities that reflect the cybersecurity context, but our results show some potential for noun extractors.""",2023,2023-12-12T09:39:03Z,"Keyphrase: ""Limited contextual understanding"""
arXIv2023,Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications,Yes.,5,"""we explore the challenges inherent to Large Language Models (LLMs) like GPT-4, particularly their propensity for hallucinations, logic mistakes, and incorrect conclusions when tasked with answering complex questions.""",2023,2023-12-12T08:43:20Z,"Keyphrase: ""Hallucination and logic mistakes"""
arXIv2023,Alignment for Honesty,Yes.,5,"""a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies.""",2023,2023-12-12T06:10:42Z,"Keyphrase: ""Challenges in aligning knowledge"""
arXIv2023,Hallucination Augmented Contrastive Learning for Multimodal Large Language Model,Yes.,5,"""MLLMs still face a fundamental limitation of hallucinations, where they tend to generate erroneous or fabricated information.""",2023,2023-12-12T04:05:15Z,"Keyphrase: ""Erroneous hallucination"""
arXIv2023,Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack,Yes.,5,"""Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks."" and ""Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content.""",2023,2023-12-12T01:39:29Z,"Keyphrase: ""Safety misalignment"""
arXIv2023,Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?,Yes.,4,"""However, this impressive performance comes with inherent limitations, such as the tendency to perpetuate stereotypical biases or fabricate non-existent facts.""",2023,2023-12-11T18:59:09Z,"Keyphrase: ""Perpetuation of stereotypical bias"""
arXIv2023,KnowGPT: Knowledge Injection for Large Language Models,Yes.,4,"""However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus.""",2023,2023-12-11T07:56:25Z,"Keyphrase: ""Lack of domain-specific knowledge"""
arXIv2023,Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding,Yes.,5,"""However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention.""",2023,2023-12-11T06:35:33Z,"Keyphrase: ""Toxicity and hallucination challenges"""
arXIv2023,METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities,Yes.,4,"""However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications."" and ""existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend.""",2023,2023-12-11T01:29:19Z,"Keyphrase: ""Blackboxed probabilistic outputs"""
arXIv2023,Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs,Yes.,5,"""However, this knowledge is inherently limited, relying heavily on the characteristics of the training data."" and ""we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.""",2023,2023-12-10T16:52:00Z,"Keyphrase: ""Limited knowledge acquisition"""
arXIv2023,Large Multimodal Model Compression via Efficient Pruning and Distillation at AntGroup,Yes.,4,"""However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI.""",2023,2023-12-10T06:57:48Z,"Keyphrase: ""Latency and carbon footprint challenges"""
arXIv2023,Understanding the Effect of Model Compression on Social Bias in Large Language Models,Yes.,4,"""Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm."" and ""We perform a carefully controlled study of the",2023,2023-12-09T20:04:20Z,"Keyphrase: ""Persistent social biases"""
arXIv2023,PaperQA: Retrieval-Augmented Generative Agent for Scientific Research,Yes.,5,"""Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth.""",2023,2023-12-08T18:50:20Z,"Keyphrase: ""Hallucination and uninterpretability"""
arXIv2023,"Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",Yes.,5,"""Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities.""",2023,2023-12-08T18:25:22Z,"Keyphrase: ""Limited reasoning and planning"""
arXIv2023,HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models,Yes.,5,"""However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems.""",2023,2023-12-08T17:57:20Z,"Keyphrase: ""Faulty reasoning and information hallucination"""
arXIv2023,DelucionQA: Detecting Hallucinations in Domain-specific Question Answering,Yes.,5,"""Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing",2023,2023-12-08T17:41:06Z,"Keyphrase: ""Hallucinatory responses"""
arXIv2023,Assessing LLMs for Moral Value Pluralism,Yes.,4,"""the fields of AI current lacks methods to quantitatively assess and potentially alter the moral values inherent in the output of large language models (LLMs)"" and ""we find that LLMs exhibit several Western-centric value biases; they overestimate how conservative people in non-Western countries are, they are less accurate in representing gender for non-Western countries, and portray older populations as having",2023,2023-12-08T16:18:15Z,"Keyphrase: ""Western-centric bias"""
arXIv2023,TypeFly: Flying Drones with Large Language Model,Yes.,5,"""However, powerful LLMs and their vision counterparts are limited in three important ways. First, they are only available as cloud-based services. Sending images to the cloud raises privacy concerns. Second, they are expensive, costing proportionally to the request size. Finally, without expensive fine-tuning, existing LLMs",2023,2023-12-08T15:57:18Z,"Keyphrase: ""Privacy concerns and cost limitations"""
arXIv2023,Retrieval-based Video Language Model for Efficient Long Video Question Answering,Yes.,5,"""However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video QA process.""",2023,2023-12-08T09:48:36Z,"Keyphrase: ""Computational cost and noise in video understanding"""
arXIv2023,Exploring the Limits of ChatGPT in Software Security Applications,Yes.,5,"""However, the impacts and limits of such LLMs in system security domain are less explored. In this paper, we delve into the limits of LLMs (i.e., ChatGPT) in seven software security applications... Also, certain limitations of ChatGPT in security-related tasks are identified, such as its constrained ability to process long code contexts.""",2023,2023-12-08T03:02:37Z,"Keyphrase: ""Limited security domain understanding"""
arXIv2023,DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions,Yes.,5,"""a dangerous nature is hidden in the code, which is the existence of fatal vulnerabilities,"" and ""shed light on the huge weakness of LLMs in the code generation task.""",2023,2023-12-07T22:19:06Z,"Keyphrase: ""Hidden code vulnerabilities"""
arXIv2023,Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models,Yes.,4,"""A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs.""",2023,2023-12-07T22:07:54Z,"Keyphrase: ""Insecure code tendencies"""
arXIv2023,Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use,Yes.,5,"""Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance.""",2023,2023-12-07T17:24:51Z,"Keyphrase: ""Overlooking crucial contextual information"""
arXIv2023,Hijacking Context in Large Multi-modal Models,Yes.,5,"""we identify a new limitation of off-the-shelf LMMs where a small fraction of incoherent images or text descriptions mislead LMMs to only generate biased output about the hijacked context, not the originally intended context.""",2023,2023-12-07T11:23:29Z,"Keyphrase: ""Incoherent and biased outputs"""
arXIv2023,Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak,Yes.,5,"""However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as 'Jailbreak Attack'.""",2023,2023-12-07T08:29:58Z,"Keyphrase: ""Vulnerability to malicious instructions"""
arXIv2023,Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge Base for Industrial Prognostics and Health Management,Yes.,5,"""Although ChatGPT-Like LLMs have rich knowledge reserves and powerful language understanding and generation capabilities, they lack domain-specific expertise, significantly limiting their practicability in PHM applications.""",2023,2023-12-06T15:24:01Z,"Keyphrase: ""Lack of domain-specific expertise"""
arXIv2023,GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science,Yes.,5,"""they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest.""",2023,2023-12-05T21:41:52Z,"Keyphrase: ""Lack of deep understanding"""
arXIv2023,LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications,Yes.,5,"""due to unresolved vulnerabilities and limitations, great care needs to be used before applying them to intelligence and safety-critical applications."" and ""The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM.""",2023,2023-12-05T19:04:50Z,"Keyphrase: ""Safety-critical application vulnerability"""
arXIv2023,Weakly Supervised Detection of Hallucinations in LLM Activations,Yes.,4,"""Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally.""",2023,2023-12-05T14:35:11Z,"Keyphrase: ""Limited internal capacity"""
arXIv2023,How should the advent of large language models affect the practice of science?,Yes.,4,"""Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools.""",2023,2023-12-05T10:45:12Z,"Keyphrase: ""Misuse and overhype"""
arXIv2023,"Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety",Yes.,5,"""Nevertheless, these models remain black boxes despite incorporating human feedback and instruction-guided tuning. For instance, ChatGPT can generate unsafe responses despite instituting safety guardrails.""",2023,2023-12-05T06:13:55Z,"Keyphrase: ""Black box model with unsafe responses"""
arXIv2023,E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation,Yes.,5,"""However, this approach necessitates items to possess rich semantic information, often generates out-of-range results, and suffers from notably low efficiency and limited extensibility. ... Nevertheless, the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations.""",2023,2023-12-05T02:50:18Z,"Keyphrase: ""Limited semantic understanding"""
arXIv2023,Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation,Yes.,5,"""observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game.""",2023,2023-12-05T02:41:57Z,"Keyphrase: ""Limited out-of-domain generalization"""
arXIv2023,New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking,Yes.,5,"""Our experiments, conducted across various datasets, reveal that current watermarking methods are detectable by even simple classifiers, challenging the notion of watermarking subtlety. We also found, through the LLM judger, that watermarking impacts text quality, especially in degrading the coherence and depth of the response.""",2023,2023-12-04T22:56:31Z,"Keyphrase: ""Degraded text quality"""
arXIv2023,Competition-Level Problems are Effective LLM Evaluators,Yes.,5,"""the challenges for any existing LLM to solve unseen complex reasoning problems"" and ""none of them is able to consistently mitigate the challenges.""",2023,2023-12-04T18:58:57Z,"Keyphrase: ""Limited complex reasoning ability"""
arXIv2023,Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?,Yes.,5,"""Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible. They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels. Overall, current LLMs have set readability ranges and do",2023,2023-12-04T17:19:53Z,"Keyphrase: ""Limited adaptability to diverse audience demographics"""
arXIv2023,Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites,Yes.,5,"""However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods.""",2023,2023-12-04T07:43:02Z,"Keyphrase: ""Object hallucination"""
arXIv2023,Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies,Yes.,4,"""However, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, and must be addressed."" and ""This research presents a comprehensive survey synthesising the current trends and limitations in techniques used for identifying and mitigating bias in LLMs.""",2023,2023-12-03T21:25:10Z,"Keyphrase: ""Bias and discrimination"""
arXIv2023,Running cognitive evaluations on large language models: The do's and the don'ts,Yes.,4,"""I describe common pitfalls that might arise when applying a cognitive test to an LLM"" and ""I conclude by discussing four areas where the do's and don'ts are currently under active discussion -- prompt sensitivity, cultural and linguistic diversity, using LLMs as research assistants, and running evaluations on open vs. closed LLMs.""",2023,2023-12-03T04:28:19Z,"Keyphrase: ""Limited sensitivity to cultural and linguistic diversity"""
arXIv2023,Towards leveraging LLMs for Conditional QA,Yes.,5,"""This study delves into the capabilities and limitations of Large Language Models (LLMs) in the challenging domain of conditional question-answering."" and ""these models encounter challenges in extractive question answering, where they lag behind the SOTA by over 10 points, and in mitigating the risk of injecting false information.""",2023,2023-12-02T14:02:52Z,"Keyphrase: ""Challenges in conditional question-answering"""
arXIv2023,Nonparametric Variational Regularisation of Pretrained Transformers,Yes.,4,"""However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes."" and ""Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large.""",2023,2023-12-01T15:40:30Z,"Keyphrase: ""Overfitting and poor domain adaptation"""
arXIv2023,Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?,Yes.,4,"""a critical concern arises regarding the potential biases embedded within these summaries,"" and ""The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models.""",2023,2023-12-01T13:00:45Z,"Keyphrase: ""Embedded bias in summaries"""
arXIv2023,Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web,Yes.,5,"""We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks."" and ""their performance further degrades under different instruction compositions changing combinational order.""",2023,2023-11-30T17:50:47Z,"Keyphrase: ""Decreased task performance with compositional tasks"""
arXIv2023,ArthModel: Enhance Arithmetic Skills to Large Language Model,Yes.,5,"""However, the models have several limitations, such as toxicity and pool performance of arithmetic solving.""",2023,2023-11-30T15:06:50Z,"Keyphrase: ""Toxicity and performance limitations"""
arXIv2023,"FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity",Yes.,5,"""Experiments show that the harmlessness of LLMs is still under-satisfactory, and extensive analysis derives some insightful findings that could inspire future research for harmless LLM research.""",2023,2023-11-30T14:18:47Z,"Keyphrase: ""Undersatisfactory performance"""
arXIv2023,Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension,Yes.,5,"""Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated.""",2023,2023-11-30T08:44:55Z,"Keyphrase: ""Limited capability in explaining incorrect alternatives"""
arXIv2023,Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes,Yes.,5,"""Despite the impressive ICL ability of LLMs, it has also been found that ICL in LLMs is sensitive to input demonstrations and limited to short context lengths.""",2023,2023-11-30T02:26:55Z,"Keyphrase: ""Limited context length"""
arXIv2023,Zero-shot Conversational Summarization Evaluations with small Large Language Models,Yes.,5,"""We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully."" and ""We also evaluate the models with human evaluations and discuss the limitations of the models on conversational summarization",2023,2023-11-29T19:34:34Z,"Keyphrase: ""Instruction dependency and variability"""
arXIv2023,OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation,Yes.,5,"""Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment.""",2023,2023-11-29T18:57:07Z,"Keyphrase: ""Pervasive hallucination"""
arXIv2023,MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models,Yes.,5,"""we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images"" and ""Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned.""",2023,2023-11-29T12:49:45Z,"Keyphrase: ""Susceptible to breaches in multimodal analysis"""
arXIv2023,Unveiling the Implicit Toxicity in Large Language Models,Yes.,5,"""LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting."" and ""LLMs pose a significant threat in generating undetectable implicit toxic outputs.""",2023,2023-11-29T06:42:36Z,"Keyphrase: ""Generation of undetectable toxic output"""
arXIv2023,Are Large Language Models Good Fact Checkers: A Preliminary Study,Yes.,5,"""However, they encounter challenges in effectively handling Chinese fact verification and the entirety of the fact-checking pipeline due to language inconsistencies and hallucinations.""",2023,2023-11-29T05:04:52Z,"Keyphrase: ""Language inconsistency and hallucination"""
arXIv2023,Elo Uncovered: Robustness and Best Practices in Language Model Evaluation,Yes.,5,"""We show that these axioms are not always satisfied raising questions about the reliability of current comparative evaluations of LLMs.""",2023,2023-11-29T00:45:23Z,"Keyphrase: ""Reliability concerns"""
arXIv2023,Scalable Extraction of Training Data from (Production) Language Models,Yes.,5,"""Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.""",2023,2023-11-28T18:47:03Z,"Keyphrase: ""Memorization vulnerability"""
arXIv2023,Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization,Yes.,5,"""they still suffer from a common issue known as the 'hallucination problem', in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images.""",2023,2023-11-28T14:54:37Z,"Keyphrase: ""Inaccurate textual generation"""
arXIv2023,Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop,Yes.,5,"""We find that this self-consuming training loop initially improves both quality and diversity. However, after a few generations the output inevitably degenerates in diversity.""",2023,2023-11-28T14:36:43Z,"Keyphrase: ""Degradation of diversity"""
arXIv2023,ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?,Yes.,5,"""Overall, models exhibit consistent and significant over-confidence on low and medium confidence statements.""",2023,2023-11-28T10:26:57Z,"Keyphrase: ""Overconfidence in predictions"""
arXIv2023,SEED-Bench-2: Benchmarking Multimodal Large Language Models,Yes.,5,"""By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence.""",2023,2023-11-28T05:53:55Z,"Keyphrase: ""Limited evaluation and insight"""
arXIv2023,Methods to Estimate Large Language Model Confidence,Yes.,5,"""Large Language Models have difficulty communicating uncertainty, which is a significant obstacle to applying LLMs to complex medical tasks."" and ""We conclude GPT4 has a limited ability to assess its own diagnostic accuracy.""",2023,2023-11-28T05:44:06Z,"Keyphrase: ""Limited ability to communicate uncertainty"""
arXIv2023,Enabling Fast 2-bit LLM on GPUs: Memory Alignment and Asynchronous Dequantization,Yes.,4,"""Nonnegligible accuracy loss for 2-bit quantization. Weights are quantized by groups, while the ranges of weights are large in some groups, resulting in large quantization errors and nonnegligible accuracy loss (e.g. >3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit)."" and ""Time-consuming",2023,2023-11-28T02:44:59Z,"Keyphrase: ""Accuracy loss due to quantization"""
arXIv2023,Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation,Yes.,4,"""However, these models are typically trained on web-scale data, which can introduce inappropriate content and lead to the development of unsafe and biased behavior. This, in turn, hampers their applicability in sensitive and trustworthy contexts and could raise significant concern in their adoption.""",2023,2023-11-27T19:02:17Z,"Keyphrase: ""Unsafe biased behavior"""
arXIv2023,Visual cognition in multimodal large language models,Yes.,5,"""Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology."" and ""Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas."" and ""Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether.""",2023,2023-11-27T18:58:34Z,"Keyphrase: ""Limitation in causal reasoning"""
arXIv2023,BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification,Yes.,5,"""they still suffer from a performance gap when the underlying distribution of topics changes"" and ""domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models, such as GPT-3.""",2023,2023-11-27T18:53:31Z,"Keyphrase: ""Domain transfer challenges"""
arXIv2023,WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models,Yes.,5,"""We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly",2023,2023-11-27T15:38:17Z,"Keyphrase: ""Response bias and error persistence"""
arXIv2023,"Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",Yes.,4,"""Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application."" and ""The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability.""",2023,2023-11-27T12:37:51Z,"Keyphrase: ""Risk of retaining faulty or harmful knowledge"""
arXIv2023,Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications,Yes.,5,"""Despite their large success and acceptance, their lack of explainability hinders legal experts to trust in their output, and this happens rightfully so.""",2023,2023-11-27T10:59:16Z,"Keyphrase: ""Lack of explainability"""
arXIv2023,Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation,Yes.,5,"""we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help"" and ""the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation",2023,2023-11-27T07:19:26Z,"Keyphrase: ""Limited support for text-to-image generation"""
arXIv2023,Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination,Yes.,5,"""The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs),"" and ""our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks.""",2023,2023-11-27T05:27:13Z,"Keyphrase: ""Serious hallucination behavior"""
arXIv2023,UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation,Yes.,5,"""These models often produce hallucinated text, compromising their practical utility in professional contexts.""",2023,2023-11-26T13:42:56Z,"Keyphrase: ""Hallucinated text"""
arXIv2023,"Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",Yes.,5,"""Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality.""",2023,2023-11-26T08:44:58Z,"Keyphrase: ""Susceptibility to prompt hacking"""
arXIv2023,Benchmarking Large Language Model Volatility,Yes.,5,"""we uncover substantial variability in sentence-level sentiment classification results, underscoring the innate volatility of LLM outputs"" and ""These uncertainties cascade downstream, leading to more significant variations in portfolio construction and return.""",2023,2023-11-26T03:54:03Z,"Keyphrase: ""Uncertain sentiment classification"""
arXIv2023,Large Language Models in Law: A Survey,Yes.,5,"""In addition, we explore the limitations of legal LLMs, including data, algorithms, and judicial practice.""",2023,2023-11-26T00:48:12Z,"Keyphrase: ""Limitations in legal domain"""
arXIv2023,Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains,Yes.,5,"""Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains.""",2023,2023-11-25T08:58:07Z,"Keyphrase: ""Challenges in high-risk domains"""
arXIv2023,AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering,Yes.,5,"""By conducting an extensive case study, we uncover several drawbacks of GPT-4V, such as limited temporal and dynamic comprehension, and overly general responses.""",2023,2023-11-25T02:46:12Z,"Keyphrase: ""Limited temporal comprehension"""
arXIv2023,One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space,Yes.,5,"""Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources.""",2023,2023-11-24T18:35:00Z,"Keyphrase: ""High computational complexity"""
arXIv2023,Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review,Yes.,4,"""ChatGPT and other Generative Artificial Intelligence (GAI) models tend to inherit and even amplify prevailing societal biases as they are trained on large amounts of existing data."" and ""Our findings show that while there is an awareness of potential biases around large language models (LLMs) and",2023,2023-11-24T10:00:23Z,"Keyphrase: ""Amplification of societal biases"""
arXIv2023,Towards Auditing Large Language Models: Improving Text-based Stereotype Detection,Yes.,4,"""LLMs often generate stereotypical output inherited from historical data, amplifying societal biases and raising ethical concerns.""",2023,2023-11-23T17:47:14Z,"Keyphrase: ""Amplifying societal bias"""
arXIv2023,Auditing and Mitigating Cultural Bias in LLMs,Yes.,5,"""We audit large language models for cultural bias,"" and ""Our mitigation strategy reduces cultural bias in recent models but not for all countries/territories.""",2023,2023-11-23T16:45:56Z,"Keyphrase: ""Cultural bias mitigation strategy"""
arXIv2023,Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions,Yes.,5,"""However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models' parameters. Recent works turn to retrieving external knowledge to augment CoT reasoning. Despite being promising, these chain-based methods suffer from",2023,2023-11-23T12:52:37Z,"Keyphrase: ""Reliance on outdated knowledge"""
arXIv2023,Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach,Yes.,5,"""However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent.""",2023,2023-11-23T10:14:58Z,"Keyphrase: ""Coordination hallucination"""
arXIv2023,Challenges of Large Language Models for Mental Health Counseling,Yes.,5,"""However, the application of LLMs in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. This paper investigates the major challenges associated with the development of LLMs for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness.""",2023,2023-11-23T08:56:41Z,"Keyphrase: ""Challenges in mental health domain"""
arXIv2023,Surpassing GPT-4 Medical Coding with a Two-Stage Approach,Yes.,5,"""the GPT-4 LLM predicts an excessive number of ICD codes for medical coding tasks, leading to high recall but low precision.""",2023,2023-11-22T23:35:13Z,"Keyphrase: ""Excessive predictions in medical coding"""
arXIv2023,Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering,Yes.,5,"""currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants).""",2023,2023-11-22T18:22:56Z,"Keyphrase: ""Limited context and high computational cost"""
arXIv2023,Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents,Yes.,4,"""However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming.""",2023,2023-11-22T13:15:42Z,"Keyphrase: ""Lack of specialization in dynamic environments"""
arXIv2023,Applying Large Language Models to Power Systems: Potential Security Threats,Yes.,5,"""However, this action may also incur potential security threats, which have not been fully recognized so far.""",2023,2023-11-22T12:55:02Z,"Keyphrase: ""Security threats not fully recognized"""
arXIv2023,Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting,Yes.,5,"""Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process.""",2023,2023-11-22T11:08:38Z,"Keyphrase: ""Limited factual grounding"""
arXIv2023,Intention and Context Elicitation with Large Language Models in the Legal Aid Intake Process,Yes.,4,"""a key challenge with current LLMs is their tendency to overconfidently deliver an immediate 'best guess' to a client's question based on the output distribution learned over the training data. This approach often overlooks the client's actual intentions or the specifics of their legal situation.""",2023,2023-11-22T10:04:29Z,"Keyphrase: ""Overconfident responses"""
arXIv2023,Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus,Yes.,5,"""LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications.""",2023,2023-11-22T08:39:17Z,"Keyphrase: ""Untruthful nonsensical output"""
arXIv2023,HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data,Yes.,5,"""the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored.""",2023,2023-11-22T04:52:58Z,"Keyphrase: ""Hallucinatory output"""
arXIv2023,Conditions for Length Generalization in Learning Reasoning Skills,Yes.,5,"""However, numerous evaluations of the reasoning capabilities of LLMs have also showed some limitations. An outstanding limitation is length generalization, meaning that when trained on reasoning problems of smaller lengths or sizes, the resulting models struggle with problems of larger sizes or lengths.""",2023,2023-11-22T03:36:18Z,"Keyphrase: ""Limited reasoning capability"""
arXIv2023,Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper,Yes.,4,"""While LLMs possess remarkable capabilities, their extensive parameter requirements and associated computational demands hinder their practicality and scalability for real-world applications."" and ""recognizes significant challenges and open issues that must be addressed to fully harness the powerful abilities of LLMs.""",2023,2023-11-22T03:28:34Z,"Keyphrase: ""Computational demands and scalability"""
arXIv2023,Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications,Yes.,4,"""Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited.""",2023,2023-11-22T01:51:50Z,"Keyphrase: ""Limited logical reasoning competency"""
arXIv2023,A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift,Yes.,4,"""However, there is little work measuring how robust these reward models are to distribution shifts."" and ""We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts.""",2023,2023-11-21T18:41:26Z,"Keyphrase: ""Reward model distribution shift"""
arXIv2023,Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey,Yes.,5,"""current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios."" and ""We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models.""",2023,2023-11-21T04:59:17Z,"Keyphrase: ""Ineffective long-context processing"""
arXIv2023,"Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",Yes.,4,"""A notable challenge, model hallucination-where the model yields inaccurate or misinterpreted data-is addressed alongside other model-specific hurdles.""",2023,2023-11-21T02:01:01Z,"Keyphrase: ""Hallucination of inaccurate data"""
arXIv2023,Evil Geniuses: Delving into the Safety of LLM-based Agents,Yes.,4,"""Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research.""",2023,2023-11-20T15:50:09Z,"Keyphrase: ""Prone to harmful behavior"""
arXIv2023,System 2 Attention (is something you might need too),Yes.,5,"""Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information from the context into its latent representations, which adversely affects next token generations.""",2023,2023-11-20T15:04:50Z,"Keyphrase: ""Incorporating irrelevant information"""
arXIv2023,Towards Robust Text Retrieval with Progressive Learning,Yes.,4,"""Retrieval augmentation has become an effective solution to empower large language models (LLMs) with external and verified knowledge sources from the database, which overcomes the limitations and hallucinations of LLMs in handling up-to-date and domain-specific information.""",2023,2023-11-20T11:44:01Z,"Keyphrase: ""Limited access to domain-specific information"""
arXIv2023,Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information,Yes.,5,"""these models are susceptible to adversarial prompt attacks,"" and ""this vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs.""",2023,2023-11-20T03:17:21Z,"Keyphrase: ""Adversarial prompt vulnerability"""
arXIv2023,A Security Risk Taxonomy for Large Language Models,Yes.,5,"""The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial."" and ""Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs.""",2023,2023-11-19T20:22:05Z,"Keyphrase: ""Security vulnerabilities"""
arXIv2023,Rethinking Large Language Models in Mental Health Applications,Yes.,5,"""It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability.""",2023,2023-11-19T08:40:01Z,"Keyphrase: ""Hallucinatory output"""
arXIv2023,Visual AI and Linguistic Intelligence Through Steerability and Composability,Yes.,5,"""The research presents a series of 14 creatively and constructively diverse tasks, ranging from AI Lego Designing to AI Satellite Image Analysis, designed to test the limits of current LLMs in contexts that previously proved difficult without extensive memory and contextual understanding."" and ""Tasks such as 'AI Genetic Programmer'",2023,2023-11-18T22:01:33Z,"Keyphrase: ""Limited contextual understanding"""
arXIv2023,A Principled Framework for Knowledge-enhanced Large Language Model,Yes.,5,"""Large Language Models (LLMs) are versatile, yet they often falter in tasks requiring deep and reliable reasoning due to issues like hallucinations, limiting their applicability in critical scenarios.""",2023,2023-11-18T18:10:02Z,"Keyphrase: ""Limited deep reasoning"""
arXIv2023,(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs,Yes.,5,"""LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices,"" and ""regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.""",2023,2023-11-18T17:11:12Z,"Keyphrase: ""Silent updates and performance regression"""
arXIv2023,Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers,Yes.,5,"""One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual. Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context.""",2023,2023-11-18T03:55:59Z,"Keyphrase: ""Factual hallucination due to biased training data"""
arXIv2023,DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback,Yes.,5,"""First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction",2023,2023-11-16T18:37:29Z,"Keyphrase: ""Limited incorporation of extra feedback"""
arXIv2023,Hijacking Large Language Models via Adversarial In-Context Learning,Yes.,5,"""ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL.""",2023,2023-11-16T15:01:48Z,"Keyphrase: ""Instability in adversarial attacks"""
arXIv2023,"Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models",Yes.,5,"""despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints."" and ""outlines the existing challenges and potential future developments for LLMs in graph understanding and reasoning tasks.""",2023,2023-11-16T12:45:41Z,"Keyphrase: ""Limitation in graph understanding"""
arXIv2023,ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks,Yes.,5,"""a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries"" and ""GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73% of the tasks, leaving a huge",2023,2023-11-16T12:03:21Z,"Keyphrase: ""Limited practical applicability"""
arXIv2023,FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models,Yes.,5,"""We have evaluated various LLMs using the FollowEval benchmark and found that their performance significantly lags behind that of humans. This highlights the considerable room for improvement in the instruction-following ability of these models.""",2023,2023-11-16T11:53:31Z,"Keyphrase: ""Inferior performance compared to humans"""
arXIv2023,Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking,Yes.,5,"""we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning.""",2023,2023-11-16T11:52:22Z,"Keyphrase: ""Cognitive overload and reasoning challenges"""
arXIv2023,The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text,Yes.,5,"""Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness.""",2023,2023-11-16T11:31:50Z,"Keyphrase: ""Decreased diversity in model output"""
arXIv2023,DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data,Yes.,5,"""We found that, although the current best-performing system (i.e., GPT-4), can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts.""",2023,2023-11-16T11:30:53Z,"Keyphrase: ""Limited performance on complex, context-dependent tasks"""
arXIv2023,Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs,Yes.,5,"""these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs.""",2023,2023-11-16T11:26:21Z,"Keyphrase: ""Lack of causal reliability"""
arXIv2023,Investigating Data Contamination in Modern Benchmarks for Large Language Models,Yes.,5,"""Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks."" and ""We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.""",2023,2023-11-16T11:03:04Z,"Keyphrase: ""Inflated benchmark scores"""
arXIv2023,Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models,Yes.,4,"""We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning",2023,2023-11-16T10:36:08Z,"Keyphrase: ""Limited multistep reasoning"""
arXIv2023,Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge,Yes.,5,"""we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge,"" and ""LLMs' uncertainty expression does not always stay consistent with the perceived confidence of their textual outputs.""",2023,2023-11-16T10:02:40Z,"Keyphrase: ""Lack of uncertainty expression"""
arXIv2023,Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks,Yes.,5,"""Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects.""",2023,2023-11-16T10:02:24Z,"Keyphrase: ""Biases in demographic-infused prompts"""
arXIv2023,Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?,Yes.,5,"""Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning."" and ""We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts.""",2023,2023-11-16T09:27:36Z,"Keyphrase: ""Unfaithful reasoning"""
arXIv2023,BLT: Can Large Language Models Handle Basic Legal Text?,Yes.,5,"""We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling."" and ""LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice.""",2023,2023-11-16T09:09:22Z,"Keyphrase: ""Poor legal text handling"""
arXIv2023,R-Tuning: Teaching Large Language Models to Refuse Unknown Questions,Yes.,5,"""A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination."" and ""previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not.""",2023,2023-11-16T08:45:44Z,"Keyphrase: ""Hallucination of nonexistent facts"""
arXIv2023,Structured Chemistry Reasoning with Large Language Models,Yes.,5,"""Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry."" and ""even advanced LLMs, like GPT-4, can fail easily in different ways."" and ""the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that",2023,2023-11-16T08:20:36Z,"Keyphrase: ""Limited scientific reasoning in specialized domains"""
arXIv2023,On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models,Yes.,5,"""Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially."" and ""Our",2023,2023-11-16T07:48:45Z,"Keyphrase: ""Vulnerability to adversarial manipulation"""
arXIv2023,Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework,Yes.,4,"""modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked"" and ""These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts.""",2023,2023-11-16T07:31:18Z,"Keyphrase: ""Vulnerability to malicious attacks"""
arXIv2023,CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model,Yes.,4,"""Large language models (LLMs) executing tasks through instruction-based prompts often face challenges stemming from distribution differences between user instructions and training instructions. This leads to distractions and biases, especially when dealing with inconsistent dynamic labels.""",2023,2023-11-16T07:16:55Z,"Keyphrase: ""Bias from inconsistent user instructions"""
arXIv2023,Simulating Opinion Dynamics with Networks of LLM-based Agents,Yes.,4,"""This bias limits their utility for understanding resistance to consensus views on issues like climate change."" and ""These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward",2023,2023-11-16T07:01:48Z,"Keyphrase: ""Limited understanding of complex issues"""
arXIv2023,Self-Contradictory Reasoning Evaluation and Detection,Yes.,5,"""We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense"" and ""Our results indicate that the current LLMs lack robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond accuracy-based metrics.""",2023,2023-11-16T06:22:17Z,"Keyphrase: ""Lack of robust reasoning"""
arXIv2023,LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks,Yes.,5,"""Assessing these models on long sequences is crucial since prior work in the general domain has demonstrated performance degradation of LLMs on longer texts."" and ""Preliminary experiments reveal that both medical LLMs (e.g., BioGPT) and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark.""",2023,2023-11-16T04:57:49Z,"Keyphrase: ""Performance degradation with long sequences"""
arXIv2023,MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning,Yes.,4,"""Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs on correctly interpreting charts, even for the most recent GPT-4V model.""",2023,2023-11-15T23:36:42Z,"Keyphrase: ""Difficulty interpreting charts"""
arXIv2023,How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities,Yes.,5,"""However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly."" and ""scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations.""",2023,2023-11-15T23:33:07Z,"Keyphrase: ""Limited trustworthiness"""
arXIv2023,Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment,Yes.,5,"""the vulnerability of their safety alignment has not been extensively studied"" and ""Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts"" and ""Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications.""",2023,2023-11-15T23:07:40Z,"Keyphrase: ""Vulnerability to attacks and resource-intensive implementation"""
arXIv2023,When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour,Yes.,5,"""the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability.""",2023,2023-11-15T22:18:33Z,"Keyphrase: ""Bias amplification through human feedback"""
arXIv2023,Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization,Yes.,5,"""Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges",2023,2023-11-15T19:49:24Z,"Keyphrase: ""Challenges in abstractive summarization"""
arXIv2023,Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models,Yes.,4,"""Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language (e.g., chemical molecular formula).""",2023,2023-11-15T18:59:56Z,"Keyphrase: ""Limited world knowledge comprehension"""
arXIv2023,Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering,Yes.,5,"""they are struggling to seek correct information in long contexts. The 'lost in the middle' problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle.""",2023,2023-11-15T18:42:44Z,"Keyphrase: ""Difficulty with contextual accuracy"""
arXIv2023,Towards Verifiable Text Generation with Symbolic References,Yes.,5,"""However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult.""",2023,2023-11-15T18:28:29Z,"Keyphrase: ""Need for manual verification"""
arXIv2023,Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization,Yes.,5,"""our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance",2023,2023-11-15T18:25:26Z,"Keyphrase: ""Factual errors and evaluation misalignment"""
arXIv2023,ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models,Yes.,5,"""While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context.""",2023,2023-11-15T18:23:17Z,"Keyphrase: ""Unreliable self-contradictions"""
arXIv2023,AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph,Yes.,5,"""Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings.""",2023,2023-11-15T18:11:23Z,"Keyphrase: ""Struggles with abstract knowledge comprehension"""
arXIv2023,Temporal Knowledge Question Answering via Abstract Reasoning Induction,Yes.,5,"""In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic.""",2023,2023-11-15T17:46:39Z,"Keyphrase: ""Limited temporal knowledge reasoning"""
arXIv2023,Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts,Yes.,5,"""This finding indicates potential exploitable security risks in MLLMs"" and ""Results show that appropriately designed system prompts can significantly reduce jailbreak success rates.""",2023,2023-11-15T17:17:39Z,"Keyphrase: ""Security vulnerabilities"""
arXIv2023,Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content.""",2023,2023-11-15T17:04:56Z,"Keyphrase: ""Inaccurate hallucinated content"""
arXIv2023,Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?,Yes.,5,"""This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications.""",2023,2023-11-15T16:56:49Z,"Keyphrase: ""Limited inference capabilities"""
arXIv2023,Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization,Yes.,5,"""Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks."" and ""we point out a pivotal factor contributing to the success of jailbreaks",2023,2023-11-15T16:42:29Z,"Keyphrase: ""Safety risks and vulnerabilities"""
arXIv2023,Social Bias Probing: Fairness Benchmarking for Language Models,Yes.,4,"""Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms."" and ""When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged.""",2023,2023-11-15T16:35:59Z,"Keyphrase: ""Encoded social biases"""
arXIv2023,How Well Do Large Language Models Truly Ground?,Yes.,5,"""Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge.""",2023,2023-11-15T16:11:27Z,"Keyphrase: ""Hallucination and lack of control"""
arXIv2023,GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models,Yes.,5,"""Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models."" and ""These identified limitations underline the importance of using benchmarks like GRASP to monitor the progress of future models in developing these competencies.""",2023,2023-11-15T15:38:28Z,"Keyphrase: ""Lack of intuitive physics capability"""
arXIv2023,Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output,Yes.,5,"""The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs."" and ""Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify",2023,2023-11-15T14:41:57Z,"Keyphrase: ""Verification of factual accuracy"""
arXIv2023,When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks,Yes.,5,"""ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications,"" and ""three primary reasons",2023,2023-11-15T14:26:30Z,"Keyphrase: ""Struggles with handling complex tasks"""
arXIv2023,Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation,Yes.,5,"""large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user."" and ""state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities.""",2023,2023-11-15T11:27:44Z,"Keyphrase: ""Low confidence signals"""
arXIv2023,Disinformation Capabilities of Large Language Models,Yes.,4,"""Automated disinformation generation is often listed as an important risk associated with large language models (LLMs).""",2023,2023-11-15T10:25:30Z,"Keyphrase: ""Risk of automated disinformation"""
arXIv2023,MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy,Yes.,5,"""degenerate modes can even occur in the absence of any model error, due to contamination of the training data"" and ""the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue.""",2023,2023-11-15T09:38:53Z,"Keyphrase: ""Degenerate modes"""
arXIv2023,Thread of Thought Unraveling Chaotic Contexts,Yes.,5,"""they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context.""",2023,2023-11-15T06:54:44Z,"Keyphrase: ""Difficulty in chaotic contexts"""
arXIv2023,Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models,Yes.,4,"""However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills.""",2023,2023-11-15T06:48:50Z,"Keyphrase: ""Unreliable output and poor reasoning"""
arXIv2023,Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures,Yes.,4,"""we undertake an exploration of decision-making processes and inherent biases within LLMs"" and ""We discuss the consequences of our findings for human-AI alignment and bias mitigation.""",2023,2023-11-15T00:02:25Z,"Keyphrase: ""Inherent bias and alignment issues"""
arXIv2023,Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment,Yes.,5,"""A systematic study of ten LLMs on seven classification tasks reveals that models flip their answers on average 46% of the time and that all models see a deterioration of accuracy between their first and final prediction, with an average drop of 17% (the FlipFlop effect).""",2023,2023-11-14T23:40:22Z,"Keyphrase: ""Answer flipping and accuracy deterioration"""
arXIv2023,CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,Yes.,4,"""existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations."" and ""most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks"" and ""most benchmarks also fail to",2023,2023-11-14T23:18:52Z,"Keyphrase: ""Limited benchmark diversity"""
arXIv2023,"LLMs cannot find reasoning errors, but can correct them!",Yes.,5,"""recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall"" and ""demonstrate that LLMs generally struggle with finding logical mistakes.""",2023,2023-11-14T20:12:38Z,"Keyphrase: ""Struggles with logical reasoning"""
arXIv2023,Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models,Yes.,4,"""while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems"" and ""We discuss how future work can include LLM fairness evaluations.""",2023,2023-11-14T19:02:03Z,"Keyphrase: ""Limited fairness evaluation"""
arXIv2023,Fine-tuning Language Models for Factuality,Yes.,5,"""Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions.""",2023,2023-11-14T18:59:15Z,"Keyphrase: ""Factually inaccurate hallucination"""
arXIv2023,Are Large Language Models Temporally Grounded?,Yes.,5,"""Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions.",2023,2023-11-14T18:57:15Z,"Keyphrase: ""Struggles with self-consistency"""
arXIv2023,SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models,Yes.,5,"""However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content."" and ""We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses.""",2023,2023-11-14T18:33:43Z,"Keyphrase: ""Safety weaknesses in guidance"""
arXIv2023,GPT-4V(ision) Unsuitable for Clinical Care and Education: A Clinician-Evaluated Assessment,Yes.,5,"""Although GPT-4V is able to identify and explain medical images, its diagnostic accuracy and clinical decision-making abilities are poor, posing risks to patient safety.""",2023,2023-11-14T17:06:09Z,"Keyphrase: ""Poor clinical decision-making ability"""
arXIv2023,Extrinsically-Focused Evaluation of Omissions in Medical Summarization,Yes.,4,"""Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs."" and ""especially given the potential for LLMs to omit important information in the resulting summary.""",2023,2023-11-14T16:46:15Z,"Keyphrase: ""Omission of important information"""
arXIv2023,A Survey of Confidence Estimation and Calibration in Large Language Models,Yes.,4,"""Despite their impressive performance, they can be unreliable due to factual errors in their generations."" and ""we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration.""",2023,2023-11-14T16:43:29Z,"Keyphrase: ""Unreliable factual error generation"""
arXIv2023,How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions,Yes.,5,"""Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points."" and ""simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLM",2023,2023-11-14T16:30:36Z,"Keyphrase: ""Limited syntactic knowledge grasp"""
arXIv2023,A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily,Yes.,5,"""Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them."" and ""Our study also reveals the inadequacy of current defense methods in safeguarding LLMs.""",2023,2023-11-14T16:02:16Z,"Keyphrase: ""Vulnerability to attacks"""
arXIv2023,RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge,Yes.,5,"""Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue.""",2023,2023-11-14T13:24:19Z,"Keyphrase: ""Susceptibility to external interference"""
arXIv2023,Insights into Classifying and Mitigating LLMs' Hallucinations,Yes.,5,"""However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as 'Hallucinations'. They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation.""",2023,2023-11-14T12:30:28Z,"Keyphrase: ""False information propagation"""
arXIv2023,Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models,Yes.,5,"""the dynamic nature of knowledge presents challenges for language models that are trained on static data, leading to outdated encoded information,"" and ""we uncover that existing continual learning baselines have difficulty in updating and forgetting outdated knowledge,"" and ""the models fail to learn updated knowledge due to the small weight gradient,"" and ""the models struggle mostly on providing numerical or temporal answers to questions asking for updated knowledge",2023,2023-11-14T12:12:02Z,"Keyphrase: ""Difficulty in updating outdated knowledge"""
arXIv2023,How good are Large Language Models on African Languages?,Yes.,5,"""Our results suggest that all LLMs produce below-par performance on African languages, and there is a large gap in performance compared to high-resource languages like English most tasks.""",2023,2023-11-14T08:10:14Z,"Keyphrase: ""Performance gap in African languages"""
arXIv2023,A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,Yes.,5,"""Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems."" and ""Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification",2023,2023-11-14T07:13:10Z,"Keyphrase: ""Struggle with logical reasoning"""
arXIv2023,Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey,Yes.,5,"""The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy.""",2023,2023-11-14T05:21:57Z,"Keyphrase: ""Hallucination stemming from knowledge gap"""
arXIv2023,Fair Abstractive Summarization of Diverse Perspectives,Yes.,4,"""current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization"" and ""Experiments show that both the model-generated and the human-written reference summaries suffer from low fairness.""",2023,2023-11-14T03:38:55Z,"Keyphrase: ""Fairness issues in abstractive summarization"""
arXIv2023,Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems,Yes.,5,"""Large Language Models have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence.""",2023,2023-11-13T21:20:17Z,"Keyphrase: ""Trivial mistakes in commonsense competence"""
arXIv2023,MART: Improving LLM Safety with Multi-round Automatic Red-Teaming,Yes.,4,"""Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.""",2023,2023-11-13T19:13:29Z,"Keyphrase: ""Limited ability to mitigate unsafe behavior"""
arXIv2023,GPT-4V(ision) as A Social Media Analysis Engine,Yes.,5,"""Despite the overall impressive capacity of GPT-4V in the social media domain, there remain notable challenges. GPT-4V struggles with tasks involving multilingual social multimedia comprehension and has difficulties in generalizing to the latest trends in social media. Additionally, it exhibits a tendency to generate erroneous information in the context",2023,2023-11-13T18:36:50Z,"Keyphrase: ""Struggles with multilingual social media comprehension"""
arXIv2023,It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning,Yes.,5,"""We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy.""",2023,2023-11-13T18:18:22Z,"Keyphrase: ""Underperformance and lack of self-consistency"""
arXIv2023,A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models,Yes.,5,"""While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations. Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response.""",2023,2023-11-13T17:28:03Z,"Keyphrase: ""Limited understanding of complex questions"""
arXIv2023,Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse,Yes.,5,"""Recent studies have highlighted a phenomenon in large language models (LLMs) known as 'the reversal curse,' in which the order of knowledge entities in the training data biases the models' comprehension."" and ""We hope that more attention can be focused on exploring and addressing these inherent weaknesses of the current LLMs, in order to achieve a higher level of intelligence.""",2023,2023-11-13T17:01:12Z,"Keyphrase: ""Reversal of curse"""
arXIv2023,On Measuring Faithfulness or Self-consistency of Natural Language Explanations,Yes.,5,"""But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning.""",2023,2023-11-13T16:53:51Z,"Keyphrase: ""Unfaithful explanations"""
arXIv2023,Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue,Yes.,5,"""However, LLMs still lack a crucial ability",2023,2023-11-13T16:19:42Z,"Keyphrase: ""Lack of crucial ability"""
arXIv2023,AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation,Yes.,4,"""current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences.""",2023,2023-11-13T15:25:42Z,"Keyphrase: ""Hallucination challenges"""
arXIv2023,Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study,Yes.,5,"""Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data."" and ""Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent,",2023,2023-11-13T15:11:26Z,"Keyphrase: ""Limited reasoning and planning capabilities"""
arXIv2023,LM-Polygraph: Uncertainty Estimation for Language Models,Yes.,5,"""However, a significant challenge arises as these models often 'hallucinate', i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements.""",2023,2023-11-13T15:08:59Z,"Keyphrase: ""Fabricating facts"""
arXIv2023,Do large language models and humans have similar behaviors in causal inference with script knowledge?,Yes.,4,"""Our experiments show that 1) only recent LLMs, like GPT-3 or Vicuna, correlate with human behavior in the $\neg A \rightarrow B$ condition. 2) Despite this correlation, all models still fail to predict that $nil \rightarrow B$ is less",2023,2023-11-13T13:05:15Z,"Keyphrase: ""Limited predictive accuracy"""
arXIv2023,In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search,Yes.,5,"""Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail."" and ""LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach,"" and ""find that model performances drop by as high as 5% in the long-tail distribution compared to head distribution.""",2023,2023-11-13T10:56:59Z,"Keyphrase: ""Performance drop in long-tail data"""
arXIv2023,Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models,Yes.,5,"""on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all",2023,2023-11-13T09:32:12Z,"Keyphrase: ""Factual inconsistencies"""
arXIv2023,Towards the Law of Capacity Gap in Distilling Language Models,Yes.,4,"""it is still a pain distilling LMs when a large capacity gap is exhibited between the teacher and the student LMs,"" and ""the curse of capacity gap can be only partly yet not fully lifted as indicated in previous studies.""",2023,2023-11-13T03:36:18Z,"Keyphrase: ""Capacity gap in distillation"""
arXIv2023,ExpNote: Black-box Large Language Models are Better Task Solvers with Experience Notebook,Yes.,4,"""However, LLMs still fail in many specific tasks although understand the task instruction.""",2023,2023-11-13T02:31:16Z,"Keyphrase: ""Failure on specific tasks"""
arXIv2023,Flames: Benchmarking Value Alignment of LLMs in Chinese,Yes.,5,"""Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs,"" and ""there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness,"" and ""all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions.""",2023,2023-11-12T17:18:21Z,"Keyphrase: ""Limited safety evaluation"""
arXIv2023,Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding,Yes.,5,"""Nonetheless, logical reasoning that involves proof planning, specifically those that necessitate the validation of explanation accuracy, continues to present stumbling blocks."" and ""Our analysis reveals that LLMs still struggle to navigate complex reasoning chains, which demand the meticulous linkage of premises",2023,2023-11-12T05:12:49Z,"Keyphrase: ""Struggles with complex reasoning"""
arXIv2023,CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset,Yes.,5,"""However, even models with billions of parameters face challenges in tasks demanding multi-step reasoning. Code generation and comprehension, especially in C and C++, emerge as significant challenges. ...they struggle with rectifying non-compilable C and C++ code. ...This approach, however, retains the limitations",2023,2023-11-11T08:21:52Z,"Keyphrase: ""Challenges in multistep reasoning and code generation"""
arXIv2023,ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome Management,Yes.,5,"""a leading barrier to the deployment of Artificial Intelligence (AI) and in particular LLMs has been concern for embedded gender and racial biases."" and ""we evaluate whether a leading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical management of acute coronary syndrome (ACS).""",2023,2023-11-10T19:59:36Z,"Keyphrase: ""Embedded gender and racial bias"""
arXIv2023,ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences,Yes.,5,"""most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. Another engineering barrier that prevents current medical LLM from better text processing ability is their restricted context length (e.g., 2,048 tokens), making it",2023,2023-11-10T12:25:32Z,"Keyphrase: ""Limited contextual understanding"""
arXIv2023,How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model,Yes.,4,"""MLLMs still face challenges in processing the semantic gap in multimodality, which may lead to erroneous generation, posing potential risks to society."" and ""Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with limited performance improvement.""",2023,2023-11-10T09:51:24Z,"Keyphrase: ""Semantic gap in multimodality"""
arXIv2023,"Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications",Yes.,4,"""Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations.""",2023,2023-11-10T05:24:04Z,"Keyphrase: ""Outdated data limitations"""
arXIv2023,Hallucination-minimized Data-to-answer Framework for Financial Decision-makers,Yes.,5,"""scaling such prototypes to robust products with minimized hallucinations or fake responses still remains an open challenge, especially in niche data-table heavy domains such as financial decision making.""",2023,2023-11-09T22:53:52Z,"Keyphrase: ""Hallucination in niche domains"""
arXIv2023,Efficiently Adapting Pretrained Language Models To New Languages,Yes.,4,"""Recent large language models (LLM) exhibit sub-optimal performance on low-resource languages, as the training data of these models is usually dominated by English and other high-resource languages."" and ""naively adapting to new languages leads to catastrophic forgetting and poor tokenizer efficiency.""",2023,2023-11-09T20:59:08Z,"Keyphrase: ""Suboptimal performance in low-resource languages"""
arXIv2023,Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models,Yes.,5,"""Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues."" and ""We find that even current state-of-the-art LLMs do not reach human performance, making our dataset",2023,2023-11-09T20:04:08Z,"Keyphrase: ""Deception and persuasion challenges"""
arXIv2023,Removing RLHF Protections in GPT-4 via Fine-Tuning,Yes.,5,"""fine-tuning can remove RLHF protections"" and ""Our results show the need for further research on protections on LLMs.""",2023,2023-11-09T17:54:59Z,"Keyphrase: ""Limited protection from harmful content"""
arXIv2023,Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure,Yes.,5,"""Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so.""",2023,2023-11-09T17:12:44Z,"Keyphrase: ""Deceptive behavior"""
arXIv2023,Do personality tests generalize to Large Language Models?,Yes.,5,"""LLMs' responses to personality tests systematically deviate from typical human responses,"" and ""reverse-coded items (e.g. 'I am introverted' vs 'I am extraverted') are often both answered affirmatively by LLMs,"" and ""variation across different prompts designed to 'steer' LLMs to simulate particular personality types does not follow the clear separation",2023,2023-11-09T11:54:01Z,"Keyphrase: ""Limited ability to simulate human-like responses"""
arXIv2023,"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",Yes.,5,"""LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs."" and ""This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios.""",2023,2023-11-09T09:25:37Z,"Keyphrase: ""Hallucination and inconsistency"""
arXIv2023,"Frontier Language Models are not Robust to Adversarial Arithmetic, or ""What do I need to say so you agree 2+2=5?",Yes.,5,"""Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and",2023,2023-11-08T19:07:10Z,"Keyphrase: ""Vulnerability to adversarial prompts"""
arXIv2023,How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,Yes.,5,"""However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation",2023,2023-11-08T18:58:43Z,"Keyphrase: ""Limited structural generalization"""
arXIv2023,Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs,Yes.,5,"""Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas"" and ""Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness.""",2023,2023-11-08T18:52:17Z,"Keyphrase: ""Deep-rooted biases"""
arXIv2023,LooGLE: Can Long-Context Language Models Understand Long Contexts?,Yes.,5,"""LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks"" and ""strategies for extending context window length had limited impact on long context understanding.""",2023,2023-11-08T01:45:37Z,"Keyphrase: ""Limited long-context understanding"""
arXIv2023,Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning,Yes.,5,"""LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations."" and ""Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication.""",2023,2023-11-07T21:09:57Z,"Keyphrase: ""Fabricated evidence hallucination"""
arXIv2023,Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications,Yes.,4,"""LLM-integrated applications also introduce new attack surfaces,"" and ""Successful exploits of the identified vulnerabilities result in the users receiving responses tailored to the intent of a threat initiator,"" and ""our empirical results show that the threats can effectively bypass the restrictions and moderation policies of OpenAI, resulting in users receiving responses that contain bias, toxic content, privacy risk, and disinformation.""",2023,2023-11-07T20:13:05Z,"Keyphrase: ""Vulnerability to malicious intent"""
arXIv2023,Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation,Yes.,5,"""However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures."" and ""However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models.""",2023,2023-11-07T18:03:23Z,"Keyphrase: ""Limited ability to acquire new knowledge"""
arXIv2023,Unveiling Safety Vulnerabilities of Large Language Models,Yes.,5,"""As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern."" and ""We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it.""",2023,2023-11-07T16:50:33Z,"Keyphrase: ""Potential for harmful and inappropriate responses"""
arXIv2023,Do LLMs exhibit human-like response biases? A case study in survey design,Yes.,5,"""One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording"" and ""Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF.""",2023,2023-11-07T15:40:43Z,"Keyphrase: ""Failure to reflect humanlike behavior"""
arXIv2023,Benefits and Harms of Large Language Models in Digital Mental Health,Yes.,4,"""This article presents contemporary perspectives on the opportunities and risks posed by LLMs in the design, development, and implementation of digital mental health tools.""",2023,2023-11-07T14:11:10Z,"Keyphrase: ""Risk of implementation in mental health"""
arXIv2023,Input Reconstruction Attack against Vertical Federated Large Language Models,Yes.,5,"""privacy concerns limit their usage in real-life businesses"" and ""we demonstrate that in LLMs, VFL fails to protect the user input since it is simple and cheap to reconstruct the input from the intermediate embeddings.""",2023,2023-11-07T09:39:22Z,"Keyphrase: ""Privacy vulnerabilities"""
arXIv2023,A Survey of Large Language Models Attribution,Yes.,4,"""issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems.""",2023,2023-11-07T05:20:09Z,"Keyphrase: ""Inherent biases and attribution issues"""
arXIv2023,Quantifying Uncertainty in Natural Language Explanations of Large Language Models,Yes.,5,"""However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior."" and ""Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.""",2023,2023-11-06T21:14:40Z,"Keyphrase: ""Uncertain explanations"""
arXIv2023,Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation,Yes.,4,"""Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour."" and ""Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.""",2023,2023-11-06T18:55:18Z,"Keyphrase: ""Vulnerability to harmful prompts"""
arXIv2023,Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance,Yes.,4,"""Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners.""",2023,2023-11-06T18:01:34Z,"Keyphrase: ""Inherent bias"""
arXIv2023,Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges,Yes.,5,"""This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models",2023,2023-11-06T17:26:59Z,"Keyphrase: ""Hallucination in evaluations"""
arXIv2023,Instructed Language Models with Retrievers Are Powerful Entity Linkers,Yes.,5,"""Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base."" and ""reaffirming that the EL task remains a persistent hurdle for general LLMs.""",2023,2023-11-06T16:38:51Z,"Keyphrase: ""Hallucination in generative content"""
arXIv2023,DeepInception: Hypnotize Large Language Model to Be Jailbreaker,Yes.,5,"""Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void."" and ""Our investigation appeals to people to pay more attention to the safety aspects of LLMs and develop a stronger defense against their misuse risks.""",2023,2023-11-06T15:29:30Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Can LLMs Follow Simple Rules?,Yes.,5,"""Our evaluations of proprietary and open models show that almost all current models struggle to follow scenario rules, even on straightforward test cases.""",2023,2023-11-06T08:50:29Z,"Keyphrase: ""Struggles with rule-based scenarios"""
arXIv2023,On the Intersection of Self-Correction and Trust in Language Models,Yes.,4,"""However, their complexity and lack of transparency have raised several trustworthiness concerns, including the propagation of misinformation and toxicity."" and ""Interestingly, our study also uncovers instances of 'self-doubt' in LLMs during the self-correction process, introducing a new set of challenges that need to be addressed.""",2023,2023-11-06T00:04:12Z,"Keyphrase: ""Trustworthiness concerns"""
arXIv2023,FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM,Yes.,5,"""LLMs are constrained by the knowledge within their training data and are prone to generating inaccurate, or 'hallucinated', information.""",2023,2023-11-05T08:34:26Z,"Keyphrase: ""Inaccurate hallucinations"""
