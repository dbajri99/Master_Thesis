Source ,Title,Talks about LLMs,Rate,Evidence,Year ,Date ,Keyphrase
arXIv2023,FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM,Yes.,5,"""LLMs are constrained by the knowledge within their training data and are prone to generating inaccurate, or 'hallucinated', information.""",2023,2023-11-05T08:34:26Z,"Keyphrase: ""Inaccurate hallucinated information"""
arXIv2023,Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles,Yes.,4,"""Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting.""",2023,2023-11-04T03:18:45Z,"Keyphrase: ""Limited few-shot translation performance"""
arXIv2023,An Interdisciplinary Outlook on Large Language Models for Scientific Research,Yes.,5,"""we articulate the challenges LLMs face, including their reliance on extensive and sometimes biased datasets, and the potential ethical dilemmas stemming from their use.""",2023,2023-11-03T19:41:09Z,"Keyphrase: ""Reliance on biased datasets and ethical dilemmas"""
arXIv2023,An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology,Yes.,4,"""However, these LLMs are prone to many errors such as hallucinations, biases, and ethical violations, which necessitate rigorous evaluation and validation before clinical deployment.""",2023,2023-11-03T19:32:35Z,"Keyphrase: ""Error hallucination and ethical violations"""
arXIv2023,The Alignment Problem in Context,Yes.,5,"""large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour"" and ""the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities.""",2023,2023-11-03T17:57:55Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks,Yes.,4,"""However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions.""",2023,2023-11-03T14:39:20Z,"Keyphrase: ""Neglect of query-related information"""
arXIv2023,Comprehensive Assessment of Toxicity in ChatGPT,Yes.,4,"""The emerging large language models (LLMs), such as ChatGPT, can potentially further accentuate this threat."" and ""Previous works have discovered that ChatGPT can generate toxic responses using carefully crafted inputs.""",2023,2023-11-03T14:37:53Z,"Keyphrase: ""Generation of toxic responses"""
arXIv2023,PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion,Yes.,5,"""The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy. We find three main error causes in our benchmark",2023,2023-11-03T08:06:35Z,"Keyphrase: ""Limited multi-turn dialogue capabilities"""
arXIv2023,FinGPT: Large Generative Models for a Small Language,Yes.,4,"""LLM work tends to focus on languages where nearly unlimited data is available for pretraining.""",2023,2023-11-03T08:05:04Z,"Keyphrase: ""Limited focus on language diversity"""
arXIv2023,Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models,Yes.,4,"""Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such",2023,2023-11-03T05:55:32Z,"Keyphrase: ""Lack of interpretability"""
arXIv2023,Successor Features for Efficient Multisubject Controlled Text Generation,Yes.,5,"""While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging.""",2023,2023-11-03T00:17:08Z,"Keyphrase: ""Challenges in safety and factuality"""
arXIv2023,Preserving the knowledge of long clinical texts using aggregated ensembles of large language models,Yes.,4,"""applying large language models, such as BERT-based models, to clinical texts poses two major challenges",2023,2023-11-02T19:50:02Z,"Keyphrase: ""Challenges in clinical text"""
arXIv2023,"The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",Yes.,5,"""Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions.""",2023,2023-11-02T15:20:11Z,"Keyphrase: ""Inconsistent factual knowledge"""
arXIv2023,FlashDecoding++: Faster Large Language Model Inference on GPUs,Yes.,5,"""However, the following challenges still remain unsolved in accelerating LLM inference",2023,2023-11-02T14:57:03Z,"Keyphrase: ""Unsolved challenges in accelerating LLM inference"""
arXIv2023,Revisiting the Knowledge Injection Frameworks,Yes.,4,"""However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved."" and ""we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected.""",2023,2023-11-02T11:18:16Z,"Keyphrase: ""Challenges in adapting to vertical domains"""
arXIv2023,Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism,Yes.,5,"""these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios.""",2023,2023-11-02T07:20:49Z,"Keyphrase: ""Error-prone hallucination"""
arXIv2023,Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game,Yes.,5,"""While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks",2023,2023-11-02T06:13:36Z,"Keyphrase: ""Vulnerability to prompt injection attacks"""
arXIv2023,Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code,Yes.,5,"""Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code."" and ""existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security."" and ""existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations.""",2023,2023-11-01T22:46:31Z,"Keyphrase: ""Neglect of security considerations"""
arXIv2023,Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models,Yes.,5,"""However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks.""",2023,2023-11-01T21:41:08Z,"Keyphrase: ""Degradation in generalization"""
arXIv2023,SAGE: Smart home Agent with Grounded Execution,Yes.,4,"""LLMs, however, lack specific knowledge about the user and their home limit their potential impact.""",2023,2023-11-01T18:36:28Z,"Keyphrase: ""Lack of specific knowledge"""
arXIv2023,Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs,Yes.,5,"""Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across two factuality subcategories. These consistent findings across various factual error categories suggest a fundamental limitation in the current LLMs' capability to accurately gauge factual",2023,2023-11-01T17:42:45Z,"Keyphrase: ""Limited factual accuracy"""
arXIv2023,Crosslingual Retrieval Augmented In-context Learning for Bangla,Yes.,5,"""The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla.""",2023,2023-11-01T15:32:50Z,"Keyphrase: ""Limited performance on low-resource languages"""
arXIv2023,Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation,Yes.,5,"""Large Language Models (LLMs) can generate biased and toxic responses."", ""all tested LLMs exhibit explicit and/or implicit gender bias, even when explicit gender stereotypes are absent in the inputs.""",2023,2023-11-01T05:31:46Z,"Keyphrase: ""Gender bias and toxicity"""
arXIv2023,Is GPT Powerful Enough to Analyze the Emotions of Memes?,Yes.,5,"""Despite GPT's remarkable progress, our findings underscore the challenges faced by these models in handling subjective tasks, which are rooted in their inherent limitations including contextual understanding, interpretation of implicit meanings, and data biases.""",2023,2023-11-01T01:57:48Z,"Keyphrase: ""Subjectivity and implicit bias"""
arXIv2023,Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias,Yes.,4,"""The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included."" and ""disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans."" and ""these results underscore the importance of meticulous conditioning, model",2023,2023-11-01T01:32:59Z,"Keyphrase: ""Inaccurate representation of perspectives"""
arXIv2023,The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback,Yes.,5,"""Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations.""",2023,2023-10-31T21:52:41Z,"Keyphrase: ""Safety and reliability issues"""
arXIv2023,BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B,Yes.,5,"""We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are",2023,2023-10-31T19:45:15Z,"Keyphrase: ""Ineffective safety fine-tuning"""
arXIv2023,BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text,Yes.,5,"""language models like BERT deteriorate in the face of dialect variation or noise.""",2023,2023-10-31T19:44:50Z,"Keyphrase: ""Sensitivity to dialect variation and noise"""
arXIv2023,Filter bubbles and affective polarization in user-personalized large language model outputs,Yes.,5,"""These results illustrate that personalizing LLMs based on user demographics carry the same risks of affective polarization and filter bubbles that have been seen in other personalized internet technologies. This 'failure mode' should be monitored closely as there are more attempts to monetize and personalize these models.""",2023,2023-10-31T18:19:28Z,"Keyphrase: ""Affective polarization and filter bubbles"""
arXIv2023,LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B,Yes.,5,"""We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat."" and ""While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate",2023,2023-10-31T16:55:06Z,"Keyphrase: ""Safety and security risks"""
arXIv2023,Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT,Yes.,5,"""these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input.""",2023,2023-10-31T15:41:08Z,"Keyphrase: ""Token limit constraint"""
arXIv2023,LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts,Yes.,5,"""IR systems in the LLMs era are facing a new challenge",2023,2023-10-31T14:42:23Z,"Keyphrase: ""New challenges in IR systems"""
arXIv2023,FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models,Yes.,4,"""By evaluating ten closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work.""",2023,2023-10-31T12:32:38Z,Keyphrase: Lack of transparency in popular LLMs
arXIv2023,The Expressibility of Polynomial based Attention Scheme,Yes.,5,"""the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference.""",2023,2023-10-30T22:16:18Z,"Keyphrase: ""Scalability challenges"""
arXIv2023,Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization,Yes.,5,"""community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses.""",2023,2023-10-30T21:33:22Z,"Keyphrase: ""Factually hallucinated summaries"""
arXIv2023,Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation,Yes.,4,"""However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements.""",2023,2023-10-30T17:04:35Z,"Keyphrase: ""Subjectivity and unreliable judgment"""
arXIv2023,Adversarial Attacks and Defenses in Large Language Models: Old and New Threats,Yes.,4,"""substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude"" and ""we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models.""",2023,2023-10-30T17:01:02Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Evaluating Large Language Models: A Comprehensive Survey,Yes.,5,"""LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards.""",2023,2023-10-30T17:00:52Z,"Keyphrase: ""Privacy risks and potential harm"""
arXIv2023,Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs,Yes.,5,"""they often generate summaries that are factually inconsistent with original articles, known as 'hallucinations' in text generation."" and ""current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, overgeneralizing, etc.""",2023,2023-10-30T08:40:16Z,"Keyphrase: ""Factually inconsistent summaries"""
arXIv2023,M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models,Yes.,5,"""Our results reveal that",2023,2023-10-30T03:11:30Z,Keyphrase: Lack of context or specificity
arXIv2023,"From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude",Yes.,4,"""However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks.""",2023,2023-10-29T22:52:40Z,"Keyphrase: ""Susceptible to generating malicious content"""
arXIv2023,N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics,Yes.,5,"""We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination."" and ""enhance trustworthiness by addressing fairness, bias, and robustness concerns.""",2023,2023-10-28T11:22:22Z,"Keyphrase: ""Toxicity and hallucination"""
arXIv2023,On the Automatic Generation and Simplification of Children's Stories,Yes.,5,"""We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups."" and ""while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models",2023,2023-10-27T21:31:34Z,"Keyphrase: ""Limited vocabulary for younger age groups"""
arXIv2023,DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues,Yes.,4,"""This dataset presents challenges concerning knowledge recency, safety, fairness, and bias.""",2023,2023-10-27T13:23:02Z,"Keyphrase: ""Challenges in knowledge recency and bias"""
arXIv2023,NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark,Yes.,5,"""Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded.""",2023,2023-10-27T09:48:29Z,"Keyphrase: ""Performance overestimation due to contamination"""
arXIv2023,SOUL: Towards Sentiment and Opinion Understanding of Language,Yes.,5,"""Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications.""",2023,2023-10-27T06:48:48Z,"Keyphrase: ""Performance gap compared to human reasoning"""
arXIv2023,Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method,Yes.,5,"""recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization.""",2023,2023-10-27T06:22:14Z,"Keyphrase: ""Nonfactual responses"""
arXIv2023,Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory,Yes.,5,"""Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning.""",2023,2023-10-27T04:15:30Z,"Keyphrase: ""Privacy leakage"""
arXIv2023,"""You Are An Expert Linguistic Annotator"": Limits of LLMs as Analyzers of Abstract Meaning Representation",Yes.,5,"""we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure"" and ""model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses.""",2023,2023-10-26T21:47:59Z,"Keyphrase: ""Frequent major errors"""
arXIv2023,Evaluation of large language models using an Indian language LGBTI+ lexicon,Yes.,4,"""Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English.""",2023,2023-10-26T21:32:24Z,"Keyphrase: ""Limited detection of hateful content"""
arXIv2023,A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications,Yes.,4,"""We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles.""",2023,2023-10-26T19:45:06Z,"Keyphrase: ""Violation of fairness principles"""
arXIv2023,Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems,Yes.,5,"""revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context.""",2023,2023-10-26T19:44:06Z,"Keyphrase: ""Lagging fluency and informativeness"""
arXIv2023,Proving Test Set Contamination in Black Box Language Models,Yes.,5,"""Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks."" and ""Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.""",2023,2023-10-26T17:43:13Z,"Keyphrase: ""Overreliance on memorization"""
arXIv2023,An Open Source Data Contamination Report for Large Language Models,Yes.,4,"""Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models."" and ""Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics.""",2023,2023-10-26T17:11:42Z,"Keyphrase: ""Data contamination issues"""
arXIv2023,Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering,Yes.,4,"""LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers.""",2023,2023-10-26T15:45:12Z,"Keyphrase: ""Overconfidence in irrelevant information"""
arXIv2023,Symbolic Planning and Code Generation for Grounded Dialogue,Yes.,5,"""LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding.""",2023,2023-10-26T04:22:23Z,"Keyphrase: ""Limited task-oriented dialogue grounding"""
arXIv2023,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,Yes.,5,"""LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses.""",2023,2023-10-26T03:28:30Z,"Keyphrase: ""Limited external knowledge attribution"""
arXIv2023,Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT and GPT-4 for Dialogue Summarization,Yes.,5,"""Our findings indicate that GPT models often produce lengthy summaries and deviate from human summarization guidelines."" and ""The results reveal that GPT models exhibit unique stylistic tendencies in their summaries."" and ""While BERTScores did not dramatically decrease for GPT outputs suggesting semantic similarity to human references and specialised pre-trained models, ROUGE scores reveal grammatical and lexical disparities between GPT-generated and human-written summaries",2023,2023-10-25T17:39:07Z,"Keyphrase: ""Stylistic and semantic disparities"""
arXIv2023,Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation,Yes.,5,"""The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Specifically, it showed limitations when dealing with non-Latin languages and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image.""",2023,2023-10-25T17:38:55Z,"Keyphrase: ""Struggles with multilingual and complex tasks"""
arXIv2023,Detecting Pretraining Data from Large Language Models,Yes.,4,"""the data used to train them is rarely disclosed"" and ""it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks.""",2023,2023-10-25T17:21:23Z,Keyphrase: Lack of transparency in training data
arXIv2023,SuperHF: Supervised Iterative Learning from Human Feedback,Yes.,5,"""While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training.""",2023,2023-10-25T16:52:00Z,"Keyphrase: ""Safety and value alignment challenges"""
arXIv2023,HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models,Yes.,5,"""Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.""",2023,2023-10-25T16:41:15Z,"Keyphrase: ""Decline in performance on higher-order tasks"""
arXIv2023,"R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",Yes.,4,"""the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated.""",2023,2023-10-25T10:34:02Z,"Keyphrase: ""Inaccurate results in noisy contexts"""
arXIv2023,An Early Evaluation of GPT-4V(ision),Yes.,5,"""Our experimental results reveal the ability and limitations of GPT-4V"" and ""GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age; (3) GPT-4V obtains worse results",2023,2023-10-25T10:33:17Z,"Keyphrase: ""Cross-lingual and sensitive trait performance issues"""
arXIv2023,Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting,Yes.,4,"""A crucial challenge for generative large language models (LLMs) is diversity",2023,2023-10-25T10:17:17Z,"Keyphrase: ""Lack of diversity"""
arXIv2023,OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models,Yes.,4,"""existing instruction-tuning datasets suffer from occupational bias",2023,2023-10-25T10:06:17Z,"Keyphrase: ""Occupational bias"""
arXIv2023,Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation,Yes.,5,"""Our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. To address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs. Moreover, our study identifies",2023,2023-10-25T00:32:56Z,"Keyphrase: ""Overlooking security concerns"""
arXIv2023,Knowledge Editing for Large Language Models: A Survey,Yes.,4,"""Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model.""",2023,2023-10-24T22:18:13Z,"Keyphrase: ""Substantial computational cost"""
arXIv2023,Can You Follow Me? Testing Situational Understanding in ChatGPT,Yes.,5,"""Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs),"" and ""despite the fundamental simplicity of the task, the model's performance reflects an inability to retain correct environment states across time,"" and ""performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to halluc",2023,2023-10-24T19:22:01Z,"Keyphrase: ""Lack of persistent in-context memory"""
arXIv2023,Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition,Yes.,5,"""These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones.""",2023,2023-10-24T18:18:11Z,"Keyphrase: ""Vulnerability to prompt manipulation"""
arXIv2023,MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning,Yes.,5,"""While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings.""",2023,2023-10-24T17:59:20Z,"Keyphrase: ""Limited reasoning ability"""
arXIv2023,Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs,Yes.,5,"""it is important to investigate their limitations in dealing with different image and question properties."" and ""we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size.""",2023,2023-10-24T17:48:04Z,"Keyphrase: ""Limited zero-shot accuracy in answering visual questions"""
arXIv2023,This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models,Yes.,5,"""Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing"" and ""Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on",2023,2023-10-24T15:38:21Z,"Keyphrase: ""Struggles with negation"""
arXIv2023,BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT,Yes.,5,"""However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning.""",2023,2023-10-24T14:57:34Z,"Keyphrase: ""Lack of multiturn engagement"""
arXIv2023,SoK: Memorization in General-Purpose Large Language Models,Yes.,5,"""This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond.""",2023,2023-10-24T14:25:53Z,"Keyphrase: ""Privacy and security concerns"""
arXIv2023,Self-Guard: Empower the LLM to Safeguard Itself,Yes.,5,"""The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content... safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help.""",2023,2023-10-24T14:08:26Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Unnatural language processing: How do language models handle machine-generated prompts?,Yes.,5,"""We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions."" and ""Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles.""",2023,2023-10-24T13:32:20Z,"Keyphrase: ""Sensitivity to input variations"""
arXIv2023,Generative Language Models Exhibit Social Identity Biases,Yes.,4,"""The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans."" and ""Our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data.""",2023,2023-10-24T13:17:40Z,"Keyphrase: ""Social identity bias"""
arXIv2023,Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers,Yes.,5,"""the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability"" and ""Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations.""",2023,2023-10-24T12:37:06Z,"Keyphrase: ""Vulnerability to shortcut manipulation"""
arXIv2023,Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation,Yes.,5,"""However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes.""",2023,2023-10-24T11:40:34Z,"Keyphrase: ""Limited relationship understanding"""
arXIv2023,Prevalence and prevention of large language model use in crowd work,Yes.,4,"""LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data"" and ""preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use L",2023,2023-10-24T09:52:09Z,"Keyphrase: ""Homogeneous responses and human behavior degradation"""
arXIv2023,KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval,Yes.,5,"""Motivated by rising concerns around factual incorrectness and hallucinations of LLMs,"" and ""Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases.""",2023,2023-10-24T04:40:38Z,"Keyphrase: ""Factual incorrectness and irrelevant information"""
arXIv2023,The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks,Yes.,5,"""security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure,"" and ""Our findings indicate that, with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from being impermeable to PII",2023,2023-10-24T02:48:19Z,"Keyphrase: ""Privacy and security risks"""
arXIv2023,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,Yes.,5,"""We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.""",2023,2023-10-24T00:24:11Z,"Keyphrase: ""Limited chain-of-thought reasoning"""
arXIv2023,EpiK-Eval: Evaluation for Language Models as Epistemic Models,Yes.,5,"""Evaluations across various LLMs reveal significant weaknesses in this domain.""",2023,2023-10-23T21:15:54Z,"Keyphrase: ""Domain weaknesses"""
arXIv2023,"Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",Yes.,5,"""We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence",2023,2023-10-23T20:35:52Z,"Keyphrase: ""Hallucination of constrained output"""
arXIv2023,Moral Foundations of Large Language Models,Yes.,4,"""they may reflect the biases that are present in such corpora"" and ""illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance.""",2023,2023-10-23T20:05:37Z,"Keyphrase: ""Assumed moral stance"""
arXIv2023,"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models",Yes.,5,"""it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration."" and ""experimental results have shown that it poses significant challenges for all existing LLMs.""",2023,2023-10-23T17:52:06Z,"Keyphrase: ""Evaluation challenges due to text length"""
arXIv2023,Towards LLM-driven Dialogue State Tracking,Yes.,5,"""Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities.""",2023,2023-10-23T14:15:28Z,"Keyphrase: ""Closed-source and data privacy concerns"""
arXIv2023,Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism,Yes.,5,"""We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible ->implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family.""",2023,2023-10-23T12:40:41Z,"Keyphrase: ""Limited handling of lexical negation"""
arXIv2023,ALCUNA: Large Language Models Meet New Knowledge,Yes.,5,"""We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge.""",2023,2023-10-23T11:40:05Z,"Keyphrase: ""Limited reasoning capabilities"""
arXIv2023,Evaluating the Knowledge Base Completion Potential of GPT,Yes.,5,"""We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task.""",2023,2023-10-23T10:15:13Z,"Keyphrase: ""Limited task performance"""
arXIv2023,"A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions",Yes.,4,"""We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, and data ambiguity.""",2023,2023-10-23T09:01:13Z,"Keyphrase: ""Out-of-distribution challenges"""
arXIv2023,Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models,Yes.,4,"""We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge.""",2023,2023-10-23T08:45:12Z,"Keyphrase: ""Limited lexical knowledge"""
arXIv2023,Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications,Yes.,5,"""LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society,"" ""LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks,"" and ""the fairness metric gap between different subgroups is still larger than that in traditional machine learning models.""",2023,2023-10-23T06:31:28Z,"Keyphrase: ""Harmful social bias and fairness gaps"""
arXIv2023,"Language Models Hallucinate, but May Excel at Fact Verification",Yes.,5,"""Nevertheless, LLMs frequently 'hallucinate,' resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time.""",2023,2023-10-23T04:39:01Z,"Keyphrase: ""Frequent hallucinations"""
arXIv2023,The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages,Yes.,5,"""Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task",2023,2023-10-23T04:22:44Z,"Keyphrase: ""Struggles with understanding social meaning"""
arXIv2023,Evaluating Large Language Models on Controlled Generation Tasks,Yes.,5,"""We conclude that large language models struggle at meeting fine-grained hard constraints.""",2023,2023-10-23T03:48:24Z,"Keyphrase: ""Difficulty with fine-grained hard constraints"""
arXIv2023,Retrieval-Augmented Chain-of-Thought in Semi-structured Domains,Yes.,5,"""their inability to handle very long inputs/contexts is well known.""",2023,2023-10-22T22:45:14Z,"Keyphrase: ""Struggles with long inputs"""
arXIv2023,Large Language Models are biased to overestimate profoundness,Yes.,5,"""LLMs systematically overestimate the profoundness of nonsensical statements,"" and ""provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.""",2023,2023-10-22T21:33:50Z,"Keyphrase: ""Overestimation of profoundness"""
arXIv2023,Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis,Yes.,4,"""Ethical restrictions prohibit large language models (LLMs) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models.""",2023,2023-10-22T15:19:04Z,"Keyphrase: ""Ethical restrictions on harmful content"""
arXIv2023,Chainpoll: A high efficacy method for LLM hallucination detection,Yes.,5,"""hallucinations - incorrect or unfounded claims - are still prevalent,"" and ""we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use.""",2023,2023-10-22T14:45:14Z,"Keyphrase: ""Hallucination and unfounded claims"""
arXIv2023,Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases,Yes.,5,"""Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training,"" and ""Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA-2-CHAT where the model's responses are strongly biased and opinionated 64% of the time.""",2023,2023-10-22T13:55:46Z,"Keyphrase: ""Unaddressed harmful biases"""
arXIv2023,From Static to Dynamic: A Continual Learning Framework for Large Language Models,Yes.,4,"""However, this complexity also presents challenges, making LLMs difficult to train and inhibiting their ability to continuously assimilate new knowledge, which may lead to inaccuracies in their outputs.""",2023,2023-10-22T10:18:53Z,"Keyphrase: ""Difficulty in continuous learning"""
arXIv2023,MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications,Yes.,4,"""However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of",2023,2023-10-21T17:18:09Z,"Keyphrase: ""Data imbalance and high cost in medical finetuning"""
arXIv2023,Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain,Yes.,5,"""We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that",2023,2023-10-21T16:14:56Z,"Keyphrase: ""Lack of self-consistency"""
arXIv2023,BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues,Yes.,4,"""other LLMs struggle to generate multi-turn dialogues of satisfactory quality due to poor instruction-following capability, tendency to generate lengthy utterances, or limited general capability.""",2023,2023-10-20T16:53:51Z,"Keyphrase: ""Poor multiturn dialogue quality"""
arXIv2023,Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning,Yes.,5,"""Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers.""",2023,2023-10-20T16:05:35Z,"Keyphrase: ""Misinterpretation of questions"""
arXIv2023,She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and Sustainable Language Models,Yes.,4,"""Recent events indicate ethical concerns around conventionally trained LLMs, leading to overall unsafe user experiences."" and ""The assessment presented in this paper highlights a gap between societal alignment and the capabilities of current LLMs.""",2023,2023-10-20T14:18:40Z,"Keyphrase: ""Ethical concerns and societal misalignment"""
arXIv2023,Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning,Yes.,5,"""current LLM-based MT systems are brittle",2023,2023-10-20T12:29:51Z,"Keyphrase: ""Brittle performance"""
arXIv2023,Self-Consistency of Large Language Models under Ambiguity,Yes.,5,"""Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency,"" and ""we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence.""",2023,2023-10-20T11:57:56Z,"Keyphrase: ""Inconsistent contextual answers"""
arXIv2023,POSQA: Probe the World Models of LLMs with Size Comparisons,Yes.,5,"""We show that even the largest LLMs today perform poorly under the zero-shot setting,"" and ""Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.""",2023,2023-10-20T10:05:01Z,"Keyphrase: ""Vulnerability to deception and confusion"""
arXIv2023,Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs),Yes.,5,"""This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes",2023,2023-10-20T08:13:36Z,"Keyphrase: ""Domain specificity and knowledge forgetting"""
arXIv2023,Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,Yes.,5,"""However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to 'a blindfolded text-based game.' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand.""",2023,2023-10-20T03:22:05Z,"Keyphrase: ""Limited visual comprehension"""
arXIv2023,StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding,Yes.,5,"""Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa.""",2023,2023-10-19T16:29:23Z,"Keyphrase: ""Struggles with analogy identification"""
arXIv2023,Probing LLMs for hate speech detection: strengths and vulnerabilities,Yes.,4,"""we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute 'jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.""",2023,2023-10-19T16:11:02Z,"Keyphrase: ""Failure to classify errors"""
arXIv2023,Prompt Injection Attacks and Defenses in LLM-Integrated Applications,Yes.,4,"""Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires.""",2023,2023-10-19T15:12:09Z,"Keyphrase: ""Vulnerability to prompt injection attacks"""
arXIv2023,Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization,Yes.,4,"""However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge.""",2023,2023-10-19T14:50:51Z,"Keyphrase: ""Challenges in transfer learning"""
arXIv2023,Safe RLHF: Safe Reinforcement Learning from Human Feedback,Yes.,4,"""the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training"" and ""We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints.""",2023,2023-10-19T14:22:03Z,"Keyphrase: ""Safety concerns and optimization challenges"""
arXIv2023,Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong,Yes.,5,"""However, they over-rely on the LLMs when the explanation is wrong."" and ""Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.""",2023,2023-10-19T08:09:58Z,"Keyphrase: ""Unreliable explanations"""
arXIv2023,Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks,Yes.,5,"""Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex.""",2023,2023-10-19T06:37:32Z,"Keyphrase: ""Hallucination in question-answering scenarios"""
arXIv2023,Attack Prompt Generation for Red Teaming and Defending Large Language Models,Yes.,4,"""Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content.""",2023,2023-10-19T06:15:05Z,"Keyphrase: ""Susceptibility to harmful content generation"""
arXIv2023,Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models,Yes.,5,"""This paper identifies a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training"" and ""Our study emphasizes the need to critically examine cultural dominance and ethical consideration in their development and deployment.""",2023,2023-10-19T05:38:23Z,"Keyphrase: ""Cultural dominance issue"""
arXIv2023,Contrastive Learning for Inference in Dialogue,Yes.,5,"""While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning.""",2023,2023-10-19T04:49:36Z,"Keyphrase: ""Limited deductive reasoning"""
arXIv2023,"Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher",Yes.,5,"""challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the LLM hallucination problem.""",2023,2023-10-19T03:49:36Z,"Keyphrase: ""Hallucination problem and unreliable results"""
arXIv2023,PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models,Yes.,4,"""However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs.""",2023,2023-10-19T03:25:28Z,"Keyphrase: ""Backdoor vulnerability"""
arXIv2023,Automated Repair of Declarative Software Specifications in the Era of Large Language Models,Yes.,5,"""Our study revealed that while ChatGPT falls short in comparison to existing techniques, it was able to successfully repair bugs that no other technique could address. Our analysis also identified errors in ChatGPT's generated repairs, including improper operator usage, type errors, higher-order logic misuse, and relational arity mismatches. Additionally, we observed instances of hallucinations in ChatGPT-generated repairs and incons",2023,2023-10-19T02:30:42Z,"Keyphrase: ""Error-prone repairs"""
arXIv2023,GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems,Yes.,5,"""The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to",2023,2023-10-19T00:56:37Z,"Keyphrase: ""Limited ability in verifying solutions"""
arXIv2023,FactCHD: Benchmarking Fact-Conflicting Hallucination Detection,Yes.,5,"""Despite their impressive generative capabilities, LLMs are hindered by fact-conflicting hallucinations in real-world applications."" and ""Experiments on different LLMs expose the shortcomings of current approaches in detecting factual errors accurately.""",2023,2023-10-18T16:27:49Z,"Keyphrase: ""Factual error detection shortcomings"""
arXIv2023,SPEED: Speculative Pipelined Execution for Efficient Decoding,Yes.,5,"""Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound.""",2023,2023-10-18T16:07:01Z,"Keyphrase: ""Inference latency and memory-bound constraints"""
arXIv2023,Emptying the Ocean with a Spoon: Should We Edit Models?,Yes.,5,"""We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations."" and ""We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability,",2023,2023-10-18T13:38:03Z,"Keyphrase: ""Limited trust in direct model editing"""
arXIv2023,The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models,Yes.,5,"""Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence.""",2023,2023-10-18T11:01:09Z,"Keyphrase: ""Hallucinatory behavior and overconfidence"""
arXIv2023,Solving the multiplication problem of a large language model system using a graph-based method,Yes.,5,"""The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication.""",2023,2023-10-18T08:02:00Z,"Keyphrase: ""Inadequate for arithmetic problem solving"""
arXIv2023,SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents,Yes.,5,"""We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.""",2023,2023-10-18T02:27:01Z,"Keyphrase: ""Poor social commonsense reasoning"""
arXIv2023,Systematic Assessment of Factual Knowledge in Large Language Models,Yes.,4,"""this approach has limitations regarding factual knowledge coverage,"" and ""We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.""",2023,2023-10-18T00:20:50Z,"Keyphrase: ""Limited factual knowledge coverage"""
arXIv2023,MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations,Yes.,4,"""Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly."" and ""our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example."" and ""our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long",2023,2023-10-18T00:02:38Z,"Keyphrase: ""Limited knowledge generalization"""
arXIv2023,"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",Yes.,5,"""Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate.""",2023,2023-10-17T18:18:32Z,"Keyphrase: ""Overreliance on parametric knowledge"""
arXIv2023,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,Yes.,4,"""there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes.""",2023,2023-10-17T18:00:25Z,"Keyphrase: ""Simplistic caricatures"""
arXIv2023,Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament,Yes.,5,"""GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts,"" and ""GPT-4 significantly underperforms in real-world predictive tasks compared to median human-crowd forecasts.""",2023,2023-10-17T17:58:17Z,"Keyphrase: ""Underperformance in predictive tasks"""
arXIv2023,"Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",Yes.,4,"""LLMs often require adaptation with private data, which poses privacy and security challenges."" and ""there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.""",2023,2023-10-17T17:03:00Z,"Keyphrase: ""Privacy and security challenges"""
arXIv2023,DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations,Yes.,4,"""Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typically trained without leveraging multi-modal information.""",2023,2023-10-17T16:15:34Z,"Keyphrase: ""Lack of emotion understanding and multimodal training"""
arXIv2023,Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting,Yes.,5,"""We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B.""",2023,2023-10-17T15:03:30Z,"Keyphrase: ""Sensitivity to prompt formatting"""
arXIv2023,Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges,Yes.,4,"""We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges.""",2023,2023-10-17T13:20:16Z,"Keyphrase: ""Challenges in data and linguistic understanding"""
arXIv2023,The Quo Vadis of the Relationship between Language and Large Language Models,Yes.,4,"""it is not clear that they are in a place to offer insights into the target system they seek to represent"" and ""the most important theoretical and empirical risks brought about by the adoption of scientific models that lack transparency.""",2023,2023-10-17T10:54:24Z,"Keyphrase: ""Lack of transparency"""
arXIv2023,H2O Open Ecosystem for State-of-the-art Large Language Models,Yes.,4,"""However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text.""",2023,2023-10-17T09:40:58Z,"Keyphrase: ""Risk of biased, private, and harmful text"""
arXIv2023,Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models,Yes.,4,"""These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions.""",2023,2023-10-17T08:56:04Z,"Keyphrase: ""Bias retention in chatbots"""
arXIv2023,Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning,Yes.,4,"""Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content."" and ""We discovered that most models are essentially misaligned, necessitating further ethical value alignment.""",2023,2023-10-17T07:42:40Z,"Keyphrase: ""Ethical alignment issues"""
arXIv2023,Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation,Yes.,5,"""Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such",2023,2023-10-17T06:53:00Z,"Keyphrase: ""Surface-level understanding"""
arXIv2023,NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain,Yes.,4,"""Our experiments on state-of-the-art models suggest that even the best LLMs perform less than satisfactorily on our benchmark, demonstrating the scientific knowledge gap of existing LLMs.""",2023,2023-10-17T01:27:20Z,"Keyphrase: ""Limited scientific knowledge performance"""
arXIv2023,BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology,Yes.,5,"""However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments.""",2023,2023-10-16T17:54:20Z,"Keyphrase: ""Limited long-term planning"""
arXIv2023,Data Contamination Through the Lens of Time,Yes.,5,"""Data contamination remains notoriously challenging to measure and mitigate,"" and ""we conduct the first thorough longitudinal analysis of data contamination in LLMs.""",2023,2023-10-16T17:51:29Z,"Keyphrase: ""Challenges with data contamination"""
arXIv2023,Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers,Yes.,5,"""Hallucination plagues even frontier LLMs"" and ""We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and",2023,2023-10-16T17:51:17Z,"Keyphrase: ""Persistent hallucination"""
arXIv2023,On Context Utilization in Summarization with Large Language Models,Yes.,5,"""However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source",2023,2023-10-16T16:45:12Z,"Keyphrase: ""Uneven context utilization"""
arXIv2023,Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis,Yes.,4,"""This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement.""",2023,2023-10-16T14:59:10Z,"Keyphrase: ""Generation of harmful content"""
arXIv2023,Stance Detection with Collaborative Role-Infused LLM-Based Agents,Yes.,4,"""Despite their promising capabilities, LLMs encounter challenges when directly applied to stance detection. First, stance detection demands multi-aspect knowledge, from deciphering event-related terminologies to understanding the expression styles in social media platforms. Second, stance detection requires advanced reasoning to infer authors' implicit viewpoints, as stance are often subtly embedded rather than overtly stated in the text.""",2023,2023-10-16T14:46:52Z,"Keyphrase: ""Challenges in stance detection"""
arXIv2023,"Privacy in Large Language Models: Attacks, Defenses and Future Directions",Yes.,4,"""unrestricted access to these models can also introduce potential malicious and unintentional privacy risks"" and ""Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved.""",2023,2023-10-16T13:23:54Z,"Keyphrase: ""Privacy risks and unresolved safety concerns"""
arXIv2023,"Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs",Yes.,5,"""we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks.""",2023,2023-10-16T12:51:24Z,"Keyphrase: ""Sensitivity to noise operations"""
arXIv2023,Generative Calibration for In-context Learning,Yes.,5,"""the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples"" and ""such a paradox is mainly due to the label shift of the in-context model to the data distribution, in which LLMs shift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.""",2023,2023-10-16T10:45:02Z,"Keyphrase: ""Label shift issues"""
arXIv2023,Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT,Yes.,5,"""Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain",2023,2023-10-16T08:34:44Z,"Keyphrase: ""Limited performance in zeroshot settings"""
arXIv2023,Theory of Mind for Multi-Agent Collaboration via Large Language Models,Yes.,5,"""Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state.""",2023,2023-10-16T07:51:19Z,"Keyphrase: ""Failure in managing long-horizon context"""
arXIv2023,Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks,Yes.,5,"""Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications."" and ""Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development.""",2023,2023-10-16T05:19:25Z,"Keyphrase: ""Vulnerability to generating harmful content"""
arXIv2023,FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models,Yes.,5,"""LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises.""",2023,2023-10-16T04:17:13Z,"Keyphrase: ""High resource consumption and data scarcity"""
arXIv2023,Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis,Yes.,5,"""while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing",2023,2023-10-15T18:32:27Z,"Keyphrase: ""Limited medical expertise"""
arXIv2023,In-Context Learning with Iterative Demonstration Selection,Yes.,4,"""However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem.""",2023,2023-10-15T16:40:19Z,"Keyphrase: ""Limited few-shot performance"""
arXIv2023,Assessing the Reliability of Large Language Model Knowledge,Yes.,5,"""LLMs are typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability.""",2023,2023-10-15T12:40:30Z,"Keyphrase: ""Limited evaluation metrics"""
arXIv2023,When can transformers reason with abstract symbols?,Yes.,5,"""transformers fail to generalize as their embedding dimension increases"" and ""require astonishingly large quantities of training data.""",2023,2023-10-15T06:45:38Z,"Keyphrase: ""Limited generalization with increased embedding dimension"""
arXIv2023,DPZero: Private Fine-Tuning of Language Models without Backpropagation,Yes.,5,"""The widespread practice of fine-tuning large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continues to grow, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize training data, it is important to protect",2023,2023-10-14T18:42:56Z,"Keyphrase: ""Memory and privacy challenges"""
arXIv2023,Autonomous Tree-search Ability of Large Language Models,Yes.,5,"""Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making."" and ""there are several fundamental limitations of these approaches.""",2023,2023-10-14T14:14:38Z,"Keyphrase: ""Limited strategic foresight"""
arXIv2023,CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering,Yes.,5,"""leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe",2023,2023-10-14T08:46:24Z,"Keyphrase: ""Hallucination in domain-specific question answering"""
arXIv2023,User Inference Attacks on Large Language Models,Yes.,5,"""We find that LLMs are susceptible to user inference across a variety of fine-tuning datasets, at times with near perfect attack success rates.""",2023,2023-10-13T17:24:52Z,"Keyphrase: ""Susceptible to user inference"""
arXIv2023,Table-GPT: Table-tuned GPT for Diverse Table Tasks,Yes.,5,"""we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on one-dimensional natural-language texts, whereas relational tables are two-dimensional objects.""",2023,2023-10-13T17:20:56Z,"Keyphrase: ""Limited performance on table-related tasks"""
arXIv2023,"""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases in LLM-Generated Reference Letters",Yes.,5,"""In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions",2023,2023-10-13T16:12:57Z,"Keyphrase: ""Gender bias in generated content"""
arXIv2023,KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment.""",2023,2023-10-13T12:12:34Z,"Keyphrase: ""Misinformation generation"""
arXIv2023,"""Im not Racist but..."": Discovering Bias in the Internal Knowledge of Large Language Models",Yes.,4,"""these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications.""",2023,2023-10-13T00:03:37Z,"Keyphrase: ""Inherent societal bias"""
arXIv2023,Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving,Yes.,5,"""Analysis of the model's incorrect solutions revealed three distinct failure modes",2023,2023-10-12T23:39:28Z,"Keyphrase: ""Distinct failure modes"""
arXIv2023,MemGPT: Towards LLMs as Operating Systems,Yes.,5,"""Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis.""",2023,2023-10-12T17:51:32Z,"Keyphrase: ""Context window limitation"""
arXIv2023,Jailbreaking Black Box Large Language Models in Twenty Queries,Yes.,4,"""However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse.""",2023,2023-10-12T15:38:28Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arXIv2023,Impact of Co-occurrence on Factual Knowledge of Large Language Models,Yes.,5,"""Large language models (LLMs) often make factually incorrect responses despite their success in various applications."" and ""We show that co-occurrence bias remains despite scaling up model sizes or finetuning.""",2023,2023-10-12T12:01:32Z,"Keyphrase: ""Persistent cooccurrence bias"""
arXIv2023,Can Large Language Models Really Improve by Self-critiquing Their Own Plans?,Yes.,5,"""our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability.""",2023,2023-10-12T08:22:37Z,"Keyphrase: ""Reliability compromised by false positives"""
arXIv2023,QASiNa: Religious Domain Question Answering using Sirah Nabawiyah,Yes.,5,"""The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam."" and ""The experiment indicate that Chat GPT tends to give excessive interpretations as evidenced by its higher Substring Match scores compared to EM and F1-Score, even after providing instruction and",2023,2023-10-12T07:52:19Z,"Keyphrase: ""Excessive interpretation"""
arXIv2023,GameGPT: Multi-agent Collaborative Framework for Game Development,Yes.,4,"""While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern",2023,2023-10-12T06:31:43Z,"Keyphrase: ""Hallucination issues"""
arXIv2023,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",Yes.,4,"""it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values,"" and ""we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.""",2023,2023-10-11T16:18:13Z,"Keyphrase: ""Challenges in feedback incorporation"""
arXIv2023,"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",Yes.,5,"""We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts,"" and ""highlighting the potential consequences and challenges posed by factual errors in LLM outputs.""",2023,2023-10-11T14:18:03Z,"Keyphrase: ""Factual inconsistency"""
arXIv2023,How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances,Yes.,4,"""Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era.""",2023,2023-10-11T09:46:32Z,"Keyphrase: ""Difficulty in maintaining up-to-date information"""
arXIv2023,Beyond Memorization: Violating Privacy Via Inference with Large Language Models,Yes.,5,"""we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text,"" and ""common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference.""",2023,2023-10-11T08:32:46Z,"Keyphrase: ""Inference of personal attributes"""
arXIv2023,Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding,Yes.,4,"""existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls.""",2023,2023-10-10T23:37:53Z,"Keyphrase: ""Limited generalization to new tasks/tools"""
arXIv2023,Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation,Yes.,5,"""Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs.""",2023,2023-10-10T20:15:54Z,"Keyphrase: ""Alignment failures"""
arXIv2023,LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression,Yes.,5,"""In long context scenarios, large language models (LLMs) face three main challenges",2023,2023-10-10T17:59:58Z,"Keyphrase: ""Challenges with long context"""
arXIv2023,Teaching Language Models to Hallucinate Less with Synthetic Tasks,Yes.,5,"""Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context.""",2023,2023-10-10T17:57:00Z,"Keyphrase: ""Frequent hallucinations in summarization tasks"""
arXIv2023,The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets,Yes.,4,"""Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods."" and ""this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues.""",2023,2023-10-10T17:54:39Z,"Keyphrase: ""Tendency to output falsehoods"""
arXIv2023,Exploring Memorization in Fine-tuned Language Models,Yes.,5,"""Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns.""",2023,2023-10-10T15:41:26Z,"Keyphrase: ""Memorization and privacy concerns"""
arXIv2023,MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents,Yes.,4,"""Despite this, their capacities to coordinate within task-oriented social contexts are under-explored."" and ""However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks.""",2023,2023-10-10T10:17:58Z,"Keyphrase: ""Limited coordination in complex tasks"""
arXIv2023,A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection,Yes.,5,"""LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks.""",2023,2023-10-10T10:14:59Z,"Keyphrase: ""Generation of hallucinations"""
arXIv2023,Multilingual Jailbreak Challenges in Large Language Models,Yes.,5,"""While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the `jailbreak' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior."" and ""The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically,",2023,2023-10-10T09:44:06Z,"Keyphrase: ""Safety concerns and unintended behaviors"""
arXIv2023,Towards Mitigating Hallucination in Large Language Models via Self-Reflection,Yes.,5,"""However, the practical deployment still faces challenges, notably the issue of 'hallucination', where models generate plausible-sounding but unfaithful or nonsensical information.""",2023,2023-10-10T03:05:44Z,"Keyphrase: ""Hallucination of nonsensical information"""
arXIv2023,Compressing Context to Enhance Inference Efficiency of Large Language Models,Yes.,5,"""However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length.""",2023,2023-10-09T23:03:24Z,"Keyphrase: ""Challenges with long documents and extended conversations"""
arXIv2023,SALMON: Self-Alignment with Instructable Reward Models,Yes.,4,"""a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences.""",2023,2023-10-09T17:56:53Z,"Keyphrase: ""Dependency on high-quality human annotation"""
arXIv2023,ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models,Yes.,4,"""However, we identify a challenge with VLMs' passive perception, which often misses crucial context information, leading to incorrect or uncertain reasoning by LLMs.""",2023,2023-10-09T17:10:35Z,"Keyphrase: ""Misses crucial context"""
arXIv2023,HyperAttention: Long-context Attention in Near-Linear Time,Yes.,4,"""We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs)."" and ""Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank.""",2023,2023-10-09T17:05:25Z,"Keyphrase: ""Computational complexity with long context"""
arXIv2023,GraphLLM: Boosting Graph Reasoning Ability of Large Language Model,Yes.,4,"""Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks.""",2023,2023-10-09T16:42:00Z,"Keyphrase: ""Underwhelming graph reasoning performance"""
arXIv2023,SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese,Yes.,4,"""they can also produce harmful content that negatively affects societal perceptions"" and ""Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods.""",2023,2023-10-09T16:03:22Z,"Keyphrase: ""Harmful content generation"""
arXIv2023,"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",Yes.,4,"""highlighting both the strengths and limitations"" and ""the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics.""",2023,2023-10-09T13:15:23Z,"Keyphrase: ""Ethical concerns in healthcare deployment"""
arXIv2023,LAiW: A Chinese Legal Large Language Models Benchmark,Yes.,4,"""current evaluations of these LLMs in LegalAI are defined by the experts of computer science, lacking consistency with the logic of legal practice, making it difficult to judge their practical capabilities"" and ""LLMs seem to be able to directly acquire complex legal application capabilities but perform poorly in some basic tasks, which may pose obstacles to their practical application and acceptance by legal experts.""",2023,2023-10-09T11:19:55Z,"Keyphrase: ""Limited legal domain expertise"""
arXIv2023,Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization,Yes.,5,"""We also find that MuggleMath is weak in out-of-domain math reasoning generalization to MATH. This is attributed to the differences in query distribution between AugGSM8K and MATH which suggest that augmentation on a single benchmark could not help with overall math reasoning performance.""",2023,2023-10-09T08:18:58Z,"Keyphrase: ""Weak out-of-domain generalization"""
arXIv2023,SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF,Yes.,5,"""RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time.""",2023,2023-10-09T02:11:21Z,"Keyphrase: ""Lack of user control over implicit values"""
arXIv2023,Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models,Yes.,5,"""Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects."" and ""no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy",2023,2023-10-09T01:52:27Z,"Keyphrase: ""Object hallucination"""
arXIv2023,Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems,Yes.,5,"""Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations,"" and ""our study uncovers significant persona biases in dialogue systems.""",2023,2023-10-08T21:03:18Z,"Keyphrase: ""Persona bias"""
arXIv2023,Measuring reasoning capabilities of ChatGPT,Yes.,5,"""I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks."" and ""A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models.""",2023,2023-10-08T20:18:50Z,"Keyphrase: ""Logical reasoning faults"""
arXIv2023,Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback,Yes.,5,"""we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs.""",2023,2023-10-08T15:14:39Z,"Keyphrase: ""Length bias leading to misleading outputs"""
arXIv2023,Factuality Challenges in the Era of Large Language Models,Yes.,5,"""These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as 'hallucinations.'"" and ""Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding",2023,2023-10-08T14:55:02Z,"Keyphrase: ""Propensity for hallucination"""
arXIv2023,Do Large Language Models Know about Facts?,Yes.,5,"""Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time"" and ""Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations.""",2023,2023-10-08T14:26:55Z,"Keyphrase: ""Lack of factual knowledge"""
arXIv2023,An Investigation of LLMs' Inefficacy in Understanding Converse Relations,Yes.,5,"""The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.""",2023,2023-10-08T13:45:05Z,"Keyphrase: ""Shortcut learning and benchmark challenges"""
arXIv2023,MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models,Yes.,5,"""The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions.""",2023,2023-10-08T13:19:52Z,"Keyphrase: ""Temporal bias vulnerability"""
arXIv2023,Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT,Yes.,4,"""this introduces issues of bias on protected attributes like gender, race and maternity status"" and ""We use contrastive input decoding on open-source LLMs to uncover potential sources of bias.""",2023,2023-10-08T12:08:48Z,"Keyphrase: ""Bias in protected attributes"""
arXIv2023,Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,Yes.,5,"""However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge.""",2023,2023-10-08T06:18:14Z,"Keyphrase: ""Limited context understanding"""
arXIv2023,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,Yes.,5,"""Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture.""",2023,2023-10-07T21:49:38Z,"Keyphrase: ""Limited cultural and language knowledge"""
arXIv2023,Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM,Yes.,4,"""Large Language Models (LLMs) pose significant hardware challenges related to memory requirements and computational ability.""",2023,2023-10-07T14:50:28Z,"Keyphrase: ""Hardware challenges"""
arXIv2023,Critique Ability of Large Language Models,Yes.,5,"""Critique is generally challenging for most LLMs, and this capability often emerges only when models are sufficiently large. (2) In particular, self-critique is especially difficult. Even top-performing LLMs struggle to achieve satisfactory performance. (3) Models tend to have lower critique accuracy on problems where they are most uncertain.""",2023,2023-10-07T14:12:15Z,"Keyphrase: ""Challenges in self-critique accuracy"""
arXIv2023,Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning,Yes.,5,"""However, these models often face the challenge of 'hallucination,' which undermines their reliability."" and ""Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations.""",2023,2023-10-07T12:06:53Z,"Keyphrase: ""Hallucination challenge"""
arXIv2023,The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning,Yes.,5,"""Reducing the model size by more than 30\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training.""",2023,2023-10-07T03:36:39Z,"Keyphrase: ""Decreased recall ability"""
arXIv2023,Amortizing intractable inference in large language models,Yes.,5,"""This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions.""",2023,2023-10-06T16:36:08Z,"Keyphrase: ""Intractable sampling tasks"""
arXIv2023,Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface,Yes.,5,"""Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation."" and ""complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly.""",2023,2023-10-06T12:44:04Z,"Keyphrase: ""Incomplete knowledge retrieval"""
arXIv2023,Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models,Yes.,5,"""Findings indicate that while these models achieved moderate success on standard QA benchmarks, their performance notably declines when assessed on RRIP tasks. The study not only highlights the limitations of current LLMs in handling redundant information but also suggests that future training of these models should focus on incorporating redundant information into the training data to increase the performance on RRIP tasks.""",2023,2023-10-06T06:20:06Z,"Keyphrase: ""Struggles with redundant information"""
arXIv2023,Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models,Yes.,5,"""The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis.""",2023,2023-10-06T05:40:23Z,"Keyphrase: ""Contextual deficiency"""
arXIv2023,From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self,Yes.,4,"""However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology.""",2023,2023-10-06T02:19:10Z,"Keyphrase: ""Inauthentic and excessive responses"""
arXIv2023,Quantized Transformer Language Model Implementations on Edge Devices,Yes.,5,"""One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency.""",2023,2023-10-06T01:59:19Z,"Keyphrase: ""Resource-intensive deployment"""
arXIv2023,Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations,Yes.,5,"""However, LLMs are prone to generate hallucinations that are not supported by the provided sources.""",2023,2023-10-06T00:10:46Z,"Keyphrase: ""Prone to hallucination"""
arXIv2023,LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models,Yes.,4,"""However, results on Coordination QA show a large room for improvement in the Theory of Mind reasoning and joint planning abilities of LLMs.""",2023,2023-10-05T21:18:15Z,"Keyphrase: ""Limited theory of mind reasoning"""
arXIv2023,"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",Yes.,5,"""We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users."" and ""Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples."" and ""simply fine-t",2023,2023-10-05T17:12:17Z,"Keyphrase: ""Safety alignment restrictions"""
arXIv2023,Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures,Yes.,4,"""However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations.""",2023,2023-10-05T16:37:29Z,"Keyphrase: ""Complex interconnected task demands"""
arXIv2023,Redefining Digital Health Interfaces with Large Language Models,Yes.,5,"""Directly applying LLMs in clinical settings is not straightforward, however, with LLMs susceptible to providing inconsistent or nonsensical answers."" and ""addressing current issues with using LLMs in clinical settings such as hallucinations.""",2023,2023-10-05T14:18:40Z,"Keyphrase: ""Inconsistent and nonsensical answers"""
arXIv2023,Evaluating Hallucinations in Chinese Large Language Models,Yes.,5,"""We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models.""",2023,2023-10-05T07:57:09Z,"Keyphrase: ""Hallucination prioritization"""
arXIv2023,Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise,Yes.,5,"""they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.""",2023,2023-10-05T05:55:06Z,"Keyphrase: ""Domain-specific knowledge deficiency"""
arXIv2023,Benchmarking Large Language Models As AI Research Agents,Yes.,5,"""Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination.""",2023,2023-10-05T04:06:12Z,"Keyphrase: ""Challenges in long-term planning"""
arXIv2023,A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions,Yes.,5,"""However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete,",2023,2023-10-05T03:45:54Z,"Keyphrase: ""Mismatched user intention alignment and incomplete domain knowledge"""
arXIv2023,A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores,Yes.,5,"""Despite their impressive performance, the models are known to pose important risks."" and ""a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed.""",2023,2023-10-05T03:20:41Z,"Keyphrase: ""Risk of systematic understanding"""
arXIv2023,Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,Yes.,5,"""they still face limitations in scenarios that demand long-term planning and spatial reasoning"" and ""it still fails to perform long-term temporal reasoning"" and ""fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles.""",2023,2023-10-05T01:42:16Z,"Keyphrase: ""Limited long-term reasoning"""
arXIv2023,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Yes.,5,"""Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement",2023,2023-10-05T00:04:12Z,"Keyphrase: ""Room for improvement in human evaluation"""
arXIv2023,Misusing Tools in Large Language Models With Visual Adversarial Examples,Yes.,5,"""These new capabilities bring new benefits and also new security risks."" and ""In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage.""",2023,2023-10-04T22:10:01Z,"""Vulnerability to visual adversarial attacks"""
arXIv2023,From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference,Yes.,4,"""However, these models carry significant computational challenges, especially the compute and energy costs required for inference.""",2023,2023-10-04T17:41:59Z,"Keyphrase: ""High computational cost"""
arXIv2023,"JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning",Yes.,4,"""prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks.""",2023,2023-10-04T16:44:23Z,"Keyphrase: ""Ambiguity and lack of explicit structure"""
arXIv2023,Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models,Yes.,5,"""To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour,",2023,2023-10-04T16:39:31Z,"Keyphrase: ""Vulnerability to malicious attacks"""
arXIv2023,Assessing Large Language Models on Climate Information,Yes.,4,"""Our framework discerns up to 30 distinct issues in model outputs"" and ""shedding light on both the potential and the limitations of LLMs in the realm of climate communication.""",2023,2023-10-04T16:09:48Z,"Keyphrase: ""Limited discernment of complex issues"""
arXIv2023,How FaR Are Large Language Models From Agents with Theory-of-Mind?,Yes.,5,"""LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action."" and ""Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D",2023,2023-10-04T06:47:58Z,"Keyphrase: ""Challenges in implicit inference"""
arXIv2023,NOLA: Networks as Linear Combination of Low Rank Random Basis,Yes.,4,"""However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3)."" and ""Yet, these methods face two primary limitations",2023,2023-10-04T03:30:24Z,"Keyphrase: ""Impractical finetuning for downstream tasks"""
arXIv2023,Low-Resource Languages Jailbreak GPT-4,Yes.,5,"""Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data,"" and ""this deficiency now poses a risk to all LLMs users.""",2023,2023-10-03T21:30:56Z,"Keyphrase: ""Crosslingual vulnerability and safety deficiencies"""
arXIv2023,Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions,Yes.,5,"""Our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers.""",2023,2023-10-03T21:19:50Z,"Keyphrase: ""Struggles with specific knowledge and misconceptions"""
arXIv2023,Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions,Yes.,5,"""However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content)."" and ""Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate",2023,2023-10-03T20:54:29Z,"Keyphrase: ""Inaccurate responses and toxic content"""
arXIv2023,AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models,Yes.,4,"""However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs.""",2023,2023-10-03T19:44:37Z,"Keyphrase: ""Susceptibility to jailbreak attacks"""
arXIv2023,Investigating Large Language Models' Perception of Emotion Using Appraisal Theory,Yes.,4,"""The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data. The magnitude of their responses is also quite different from humans in several variables. We also found that GPTs can be quite sensitive to instruction and how questions are asked.""",2023,2023-10-03T16:34:47Z,"Keyphrase: ""Limited emotional intelligence"""
arXIv2023,Unveiling the Pitfalls of Knowledge Editing for Large Language Models,Yes.,5,"""Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works.""",2023,2023-10-03T15:10:46Z,"Keyphrase: ""Unintended consequences of knowledge editing"""
arXIv2023,Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems,Yes.,5,"""Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2).""",2023,2023-10-03T12:03:06Z,"Keyphrase: ""Backward reasoning accuracy drop"""
arXIv2023,Large Language Models Cannot Self-Correct Reasoning Yet,Yes.,5,"""our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction.""",2023,2023-10-03T04:56:12Z,"Keyphrase: ""Struggles with self-correction"""
arXIv2023,HallE-Control: Controlling Object Hallucination in Large Multimodal Models,Yes.,5,"""Interestingly, while LMMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations."" and ""Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can",2023,2023-10-03T04:01:27Z,"Keyphrase: ""Object existence hallucination"""
arXIv2023,Can GPT-4 Replicate Empirical Software Engineering Research?,Yes.,4,"""We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering",2023,2023-10-03T01:27:23Z,"Keyphrase: ""Implementation-level errors"""
arXIv2023,PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels,Yes.,5,"""The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models.""",2023,2023-10-02T21:39:04Z,"Keyphrase: ""Quadratic memory complexity"""
arXIv2023,On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?,Yes.,5,"""many existing studies have shown that they could be misused to generate undesired content"" and ""we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs.""",2023,2023-10-02T19:22:01Z,"Keyphrase: ""Potential for generating undesired content"""
arXIv2023,"LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",Yes.,5,"""However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception.""",2023,2023-10-02T17:01:56Z,"Keyphrase: ""Hallucination and fabrication"""
arXIv2023,Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation,Yes.,5,"""This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes."" and ""Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.""",2023,2023-10-02T16:27:36Z,"Keyphrase: ""Susceptibility to malicious manipulation"""
arXIv2023,Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models,Yes.,5,"""Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more.""",2023,2023-10-02T15:43:53Z,"Keyphrase: ""Limited geometric reasoning"""
arXIv2023,Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models,Yes.,4,"""Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability."" and ""A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp.""",2023,2023-10-02T10:35:23Z,"Keyphrase: ""Lack of explainability in complex reasoning tasks"""
arXIv2023,Resolving Knowledge Conflicts in Large Language Models,Yes.,5,"""Extensive experiments with the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information.""",2023,2023-10-02T06:57:45Z,"Keyphrase: ""Struggles with conflicting information"""
arXIv2023,All Languages Matter: On the Multilingual Safety of Large Language Models,Yes.,4,"""Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages.""",2023,2023-10-02T05:23:34Z,"Keyphrase: ""Unsafe responses to non-English queries"""
arXIv2023,Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications,Yes.,5,"""Compressing Large Language Models (LLMs) often leads to reduced performance, especially for knowledge-intensive tasks."" and ""We start by proposing two conjectures on the nature of the damage",2023,2023-10-02T03:12:06Z,"Keyphrase: ""Reduced performance in knowledge-intensive tasks"""
arXIv2023,BooookScore: A systematic exploration of book-length summarization in the era of LLMs,Yes.,4,"""identify eight common types of coherence errors made by LLMs.""",2023,2023-10-01T20:46:44Z,"Keyphrase: ""Coherence errors"""
arXIv2023,FELM: Benchmarking Factuality Evaluation of Large Language Models,Yes.,5,"""Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.""",2023,2023-10-01T17:37:31Z,"Keyphrase: ""Limited factual accuracy"""
arXIv2023,GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models,Yes.,5,"""we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing"" and ""This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful",2023,2023-10-01T17:25:56Z,"Keyphrase: ""Risk of misuse and harmful implications"""
arXIv2023,Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning,Yes.,5,"""However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks."" and ""Our evaluation on these axes reveals major flaws in LMMs.""",2023,2023-10-01T12:02:59Z,"Keyphrase: ""Limited evaluation benchmarks"""
arXIv2023,Measuring Value Understanding in Language Models through Discriminator-Critique Gap,Yes.,4,"""Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values"" and ""This may further suggest that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks.""",2023,2023-09-30T13:47:55Z,"Keyphrase: ""Lack of true understanding"""
arXIv2023,Understanding In-Context Learning from Repetitions,Yes.,5,"""our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures"" and ""providing a fresh perspective on this exciting capability.""",2023,2023-09-30T08:13:49Z,"Keyphrase: ""Limited understanding of internal workings"""
arXIv2023,AutoHall: Automated Hallucination Dataset Generation for Large Language Models,Yes.,5,"""the detection of non-factual or hallucinatory content generated by LLMs remains scarce"" and ""variations in hallucination proportions and types among different models.""",2023,2023-09-30T05:20:02Z,"Keyphrase: ""Variation in hallucinatory content"""
arXIv2023,"Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs ""Difficult"" Downstream Tasks in LLMs",Yes.,5,"""we reveal that these seemingly inconsequential weights can result in irreparable loss of knowledge and performance degradation in difficult tasks, even when downstream continual training is allowed.""",2023,2023-09-29T22:55:06Z,"Keyphrase: ""Loss of knowledge and performance degradation"""
arXIv2023,Efficient Streaming Language Models with Attention Sinks,Yes.,5,"""Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.""",2023,2023-09-29T17:59:56Z,"Keyphrase: ""Memory consumption and text length limitation"""
arXIv2023,Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks,Yes.,5,"""Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text."" and ""Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and",2023,2023-09-29T17:12:43Z,"Keyphrase: ""Privacy risks and harmful outputs"""
arXIv2023,LoRA ensembles for large language model fine-tuning,Yes.,5,"""Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples.""",2023,2023-09-29T16:38:38Z,"Keyphrase: ""Poor uncertainty quantification"""
arXIv2023,"Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",Yes.,4,"""Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment.""",2023,2023-09-29T16:36:39Z,"Keyphrase: ""Challenges in real-world application"""
arXIv2023,Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis,Yes.,5,"""This bias can take two forms",2023,2023-09-29T15:30:32Z,"Keyphrase: ""Biases in two forms"""
arXIv2023,Split and Merge: Aligning Position Biases in Large Language Model based Evaluators,Yes.,5,"""these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content.""",2023,2023-09-29T14:38:58Z,"Keyphrase: ""Position bias in evaluation"""
arXIv2023,Using Large Language Models for Qualitative Analysis can Introduce Serious Bias,Yes.,4,"""We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences."" and ""Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations.""",2023,2023-09-29T11:19:15Z,"Keyphrase: ""Risk of bias in annotations"""
arXIv2023,Benchmarking Cognitive Biases in Large Language Models as Evaluators,Yes.,5,"""We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators.""",2023,2023-09-29T06:53:10Z,"Keyphrase: ""Biased text quality evaluation"""
arXIv2023,Medical Foundation Models are Susceptible to Targeted Misinformation Attacks,Yes.,5,"""Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. This peculiar susceptibility raises serious security and trustworthiness concerns for",2023,2023-09-29T06:44:36Z,"Keyphrase: ""Vulnerability to deliberate manipulation"""
arXIv2023,Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving,Yes.,5,"""Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan,",2023,2023-09-28T13:40:50Z,"Keyphrase: ""Contextually inappropriate hallucination"""
arXIv2023,Human Feedback is not Gold Standard,Yes.,5,"""We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality.""",2023,2023-09-28T11:18:20Z,"Keyphrase: ""Limited coverage of important aspects"""
arXIv2023,LawBench: Benchmarking Legal Knowledge of Large Language Models,Yes.,5,"""it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks,"" and ""While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks.""",2023,2023-09-28T09:35:59Z,"Keyphrase: ""Limited legal knowledge"""
arXIv2023,MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases,Yes.,4,"""Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as 'black-boxes,' making it challenging to modify their behavior.""",2023,2023-09-27T21:26:03Z,"Keyphrase: ""Poor performance on domain-specific tasks"""
arXIv2023,NLPBench: Evaluating Large Language Models on Solving NLP Problems,Yes.,4,"""Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning",2023,2023-09-27T13:02:06Z,"Keyphrase: ""Inconsistent prompting strategy and weak scientific problem-solving skills"""
arXIv2023,Graph Neural Prompting with Large Language Models,Yes.,4,"""they still exhibit inherent limitations in precisely capturing and returning grounded knowledge"" and ""applying this to LLMs is problematic owing to their large number of parameters and high computational cost.""",2023,2023-09-27T06:33:29Z,"Keyphrase: ""Limited grounded knowledge capture"""
arXIv2023,Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI,Yes.,5,"""There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations.""",2023,2023-09-26T20:52:46Z,"Keyphrase: ""Medical domain hesitation"""
arXIv2023,Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models,Yes.,5,"""We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text.""",2023,2023-09-26T17:48:55Z,"Keyphrase: ""Factually incorrect text generation"""
arXIv2023,How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions,Yes.,5,"""Large language models (LLMs) can 'lie', which we define as outputting false statements despite 'knowing' the truth in a demonstrable sense.""",2023,2023-09-26T16:07:54Z,"Keyphrase: ""Outputting false statements"""
arXIv2023,Large Language Model Alignment: A Survey,Yes.,4,"""they may yield texts that are imprecise, misleading, or even detrimental"" and ""probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks.""",2023,2023-09-26T15:49:23Z,"Keyphrase: ""Imprecise and misleading text"""
arXIv2023,Disinformation Detection: An Evolving Challenge in the Age of LLMs,Yes.,4,"""One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system.""",2023,2023-09-25T22:12:50Z,"Keyphrase: ""Misleading content generation"""
arXIv2023,Lifelong Robot Learning with Human Assisted Language Planners,Yes.,5,"""However, current LLM-based planners are only able to operate with a fixed set of skills.""",2023,2023-09-25T17:45:55Z,"Keyphrase: ""Limited skill adaptability"""
arXIv2023,"Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",Yes.,4,"""Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning.""",2023,2023-09-25T17:37:20Z,"Keyphrase: ""Limited generalization without augmentation"""
arXIv2023,Identifying the Risks of LM Agents with an LM-Emulated Sandbox,Yes.,5,"""Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses."" and ""we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures",2023,2023-09-25T17:08:02Z,"Keyphrase: ""Risk of leaking private data"""
arXIv2023,LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models,Yes.,4,"""Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations.""",2023,2023-09-25T14:50:04Z,"Keyphrase: ""Limitations in predicting carbon footprint"""
arXIv2023,The Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks,Yes.,5,"""The widespread integration of autoregressive-large language models (AR-LLMs), such as ChatGPT, across established applications, like search engines, has introduced critical vulnerabilities with uniquely scalable characteristics. In this commentary, we analyse these vulnerabilities, their dependence on natural language as a vector of",2023,2023-09-25T10:48:46Z,"Keyphrase: ""Critical vulnerability in scalability"""
arXIv2023,Evaluating Cognitive Maps and Planning in Large Language Models with CogEval,Yes.,5,"""systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops.""",2023,2023-09-25T01:20:13Z,"Keyphrase: ""Failure in planning tasks"""
arXIv2023,Can LLM-Generated Misinformation Be Detected?,Yes.,5,"""However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust."" and ""we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can",2023,2023-09-25T00:45:07Z,"Keyphrase: ""Misinformation generation"""
arXIv2023,ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning,Yes.,5,"""However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities.""",2023,2023-09-24T17:15:58Z,"Keyphrase: ""Failure in text evaluation"""
arXIv2023,Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve,Yes.,5,"""The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations,"" and ""In many cases, the experiments reveal surprising failure modes,"" and ""AI practitioners should be careful about using LLMs in low-probability situations.""",2023,2023-09-24T13:35:28Z,"Keyphrase: ""Surprising failure modes"""
arXIv2023,Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic,Yes.,5,"""However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning,"" and ""These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles.""",2023,2023-09-23T11:21:12Z,"Keyphrase: ""Limited reasoning ability"""
arXIv2023,BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP,Yes.,4,"""Our experimental results demonstrate that while in some Bengali NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2-13b-chat being significantly bad) in comparison to the current SOTA results",2023,2023-09-22T20:29:34Z,"Keyphrase: ""Poor zero-shot performance"""
arXIv2023,In-context Interference in Chat-based Large Language Models,Yes.,5,"""However, one limitation of this scenario is that users cannot modify the internal knowledge of the model, and the only way to add or modify internal knowledge is by explicitly mentioning it to the model during the current interaction."" and ""In-context learning has significant applications, but also has limitations that are seldom studied. In this paper, we present a study that shows how the model can suffer from",2023,2023-09-22T09:18:55Z,"Keyphrase: ""Limited adaptability for in-context learning"""
arXIv2023,Foundation Metrics for Evaluating Effectiveness of Healthcare Conversations Powered by Generative AI,Yes.,4,"""Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-centered aspects, including trust-building, ethics, personalization, empathy, user comprehension, and emotional support.""",2023,2023-09-21T19:36:48Z,"Keyphrase: ""Lack of medical comprehension"""
arXIv2023,Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models,Yes.,4,"""Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs).""",2023,2023-09-21T17:54:58Z,"Keyphrase: ""Inconsistent output quality"""
arXIv2023,"The Reversal Curse: LLMs trained on ""A is B"" fail to learn ""B is A""",Yes.,5,"""We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form 'A is B', it will not automatically generalize to the reverse direction 'B is A'.""",2023,2023-09-21T17:52:19Z,"Keyphrase: ""Limited bidirectional generalization"""
arXIv2023,Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition,Yes.,4,"""they often fall short in some information extraction tasks, particularly those requiring domain-specific knowledge, such as Biomedical Named Entity Recognition (NER)"" and ""we inject entity knowledge to address the problem that LLM's lack of domain knowledge when predicting entity category.""",2023,2023-09-21T17:39:53Z,"Keyphrase: ""Lack of domain knowledge"""
arXIv2023,"Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",Yes.,5,"""First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a",2023,2023-09-21T16:47:30Z,"Keyphrase: ""Underperformance in detecting fake news"""
arXIv2023,Code Soliloquies for Accurate Calculations in Large Language Models,Yes.,5,"""While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects.""",2023,2023-09-21T15:16:58Z,"Keyphrase: ""Limited mathematical reasoning"""
arXIv2023,A knowledge representation approach for construction contract knowledge modeling,Yes.,4,"""However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise.""",2023,2023-09-21T14:53:36Z,"Keyphrase: ""Lack of domain expertise"""
arXIv2023,Prompt Tuned Embedding Classification for Multi-Label Industry Sector Allocation,Yes.,5,"""Text-to-text classification is frequently reported to outperform task-specific classification heads, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens",2023,2023-09-21T13:45:32Z,"Keyphrase: ""Challenges in multilabel classification"""
arXIv2023,Knowledge Sanitization of Large Language Models,Yes.,4,"""LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns.""",2023,2023-09-21T07:49:55Z,"Keyphrase: ""Risk of revealing sensitive information"""
arXIv2023,Goal-Oriented Prompt Attack and Safety Evaluation for LLMs,Yes.,5,"""LLMs suffer from the risk of generating harmful contents especially while being employed to applications."" and ""there is no publicly available dataset with high successful attacking rate to evaluate the abilities of defending prompt attack.""",2023,2023-09-21T07:07:49Z,"Keyphrase: ""Risk of generating harmful content"""
arXIv2023,LLM Guided Inductive Inference for Solving Compositional Problems,Yes.,4,"""their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world."" and ""Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks",2023,2023-09-20T23:44:16Z,"Keyphrase: ""Limited deep reasoning ability"""
arXIv2023,"""It's a Fair Game"", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",Yes.,4,"""users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks"" and ""the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs.""",2023,2023-09-20T21:34:36Z,"Keyphrase: ""Limited user awareness and comprehension"""
arXIv2023,Chain-of-Verification Reduces Hallucination in Large Language Models,Yes.,5,"""Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models.""",2023,2023-09-20T17:50:55Z,"Keyphrase: ""Factual hallucination"""
arXIv2023,Are Large Language Models Really Robust to Word-Level Perturbations?,Yes.,5,"""our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage.""",2023,2023-09-20T09:23:46Z,"Keyphrase: ""Vulnerability to word-level perturbations"""
arXIv2023,"Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness",Yes.,5,"""As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination.""",2023,2023-09-20T05:04:16Z,"Keyphrase: ""Hallucination"""
arXIv2023,In-Context Learning for Text Classification with Many Labels,Yes.,5,"""In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt.""",2023,2023-09-19T22:41:44Z,"Keyphrase: ""Limited context window"""
arXIv2023,LMDX: Language Model-based Document Information Extraction and Localization,Yes.,4,"""The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated.""",2023,2023-09-19T22:32:56Z,"Keyphrase: ""Absence of layout encoding"""
arXIv2023,Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome,Yes.,5,"""whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm.""",2023,2023-09-19T16:41:19Z,"Keyphrase: ""Limited sarcasm comprehension"""
arXIv2023,Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation,Yes.,4,"""Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. ... This prevent the community to rigorously audit these models and conduct accurate assessment of their capability.""",2023,2023-09-19T15:02:58Z,"Keyphrase: ""Data contamination and lack of rigorous auditing"""
arXIv2023,PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training,Yes.,5,"""Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs.""",2023,2023-09-19T08:03:38Z,"Keyphrase: ""Limited input context"""
arXIv2023,Investigating the Catastrophic Forgetting in Multimodal Large Language Models,Yes.,5,"""However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).""",2023,2023-09-19T04:51:13Z,"Keyphrase: ""Catastrophic forgetting"""
arXIv2023,LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins,Yes.,4,"""Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations."" and ""We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms.""",2023,2023-09-19T02:20:10Z,"Keyphrase: ""Imprecise natural language interpretation"""
arXIv2023,GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts,Yes.,5,"""However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities.""",2023,2023-09-19T02:19:48Z,"Keyphrase: ""Unreliable guidance for harmful activities"""
arXIv2023,Stabilizing RLHF through Advantage Model and Selective Rehearsal,Yes.,4,"""Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting.""",2023,2023-09-18T23:06:32Z,"Keyphrase: ""Ethical alignment challenge"""
arXIv2023,Bias of AI-Generated Content: An Examination of News Produced by Large Language Models,Yes.,5,"""To harness this transformation, we need to understand the limitations of LLMs."" and ""Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race.""",2023,2023-09-18T14:47:24Z,"Keyphrase: ""Gender and racial bias"""
arXIv2023,"CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",Yes.,4,"""The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community.""",2023,2023-09-17T23:49:10Z,"Keyphrase: ""Transparency of training data"""
arXIv2023,Language models are susceptible to incorrect patient self-diagnosis in medical applications,Yes.,5,"""Our findings highlight that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of LLMs drop dramatically, revealing a high susceptibility to errors in self-diagnosis.""",2023,2023-09-17T19:56:39Z,"Keyphrase: ""High susceptibility to errors"""
arXIv2023,Can Large Language Models Understand Real-World Complex Instructions?,Yes.,5,"""they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text.""",2023,2023-09-17T04:18:39Z,"Keyphrase: ""Struggles with complex instructions and constraints"""
arXIv2023,Rethinking STS and NLI in Large Language Models,Yes.,5,"""the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements.""",2023,2023-09-16T11:58:39Z,"Keyphrase: ""Overconfidence in low-resource domains"""
arXIv2023,Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?,Yes.,5,"""Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging."" and ""In-depth error analysis and creating an ability map across six dimensions -- coverage, formatting, reasoning, comprehension, pragmatics, and hallucination -- highlight areas for future enhancements and suggest forthcoming research trajectories.""",2023,2023-09-16T11:31:58Z,"Keyphrase: ""Challenges in handling structured tabular data"""
arXIv2023,"PDFTriage: Question Answering over Long, Structured Documents",Yes.,5,"""Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM.""",2023,2023-09-16T04:29:05Z,"Keyphrase: ""Limited context understanding"""
arXIv2023,Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings,Yes.,5,"""Our experiments reveal that",2023,2023-09-15T17:45:28Z,Keyphrase: Lack of specificity
arXIv2023,Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West,Yes.,4,"""Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms."" and ""We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context.""",2023,2023-09-15T17:38:41Z,"Keyphrase: ""Societal bias and representational harm"""
arXIv2023,Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases,Yes.,5,"""Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in 'hallucinated' information."" and ""Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the",2023,2023-09-15T12:06:45Z,"Keyphrase: ""Poor performance in real-world applications"""
arXIv2023,Investigating Answerability of LLMs for Long-Form Question Answering,Yes.,5,"""it becomes increasingly crucial to understand their capabilities, limitations, and differences"" and ""generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts"" and ""open-source LLMs exhibit decreased reliance on context for generated questions",2023,2023-09-15T07:22:56Z,"Keyphrase: ""Limited long-context understanding"""
arXIv2023,FedJudge: Federated Legal Large Language Model,Yes.,4,"""However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods.""",2023,2023-09-15T05:45:44Z,"Keyphrase: ""Overhead in fine-tuning"""
arXIv2023,Self-Assessment Tests are Unreliable Measures of LLM Personality,Yes.,5,"""self-assessment personality tests created for humans are unreliable measures of personality in LLMs.""",2023,2023-09-15T05:19:39Z,"Keyphrase: ""Unreliable personality assessment"""
arXIv2023,MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,Yes.,4,"""while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks."" and ""we observe that MMICL successfully allevi",2023,2023-09-14T17:59:17Z,"Keyphrase: ""Struggles with multimodal understanding"""
arXIv2023,Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions,Yes.,5,"""we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning,"" and ""several popular instruction-tuned models are highly unsafe,"" and ""our results illustrate trade-offs in training LLMs to be helpful and",2023,2023-09-14T17:23:37Z,"Keyphrase: ""Safety concerns in instruction-tuned models"""
arXIv2023,Tree of Uncertain Thoughts Reasoning for Large Language Models,Yes.,4,"""These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process.""",2023,2023-09-14T13:14:51Z,"Keyphrase: ""Local uncertainty in reasoning"""
arXIv2023,Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,Yes.,5,"""Our analysis reveals a bias in GPT4-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.""",2023,2023-09-14T06:41:58Z,"Keyphrase: ""Bias in evaluation"""
arXIv2023,ChatGPT MT: Competitive for High- (but not Low-) Resource Languages,Yes.,5,"""Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered.""",2023,2023-09-14T04:36:00Z,"Keyphrase: ""Performance gap between high-resource and low-resource languages"""
arXIv2023,An Assessment of ChatGPT on Log Data,Yes.,5,"""Our findings show that the performance of the current version of ChatGPT for log processing is limited, with a lack of consistency in responses and scalability issues.""",2023,2023-09-14T04:09:27Z,"Keyphrase: ""Inconsistent responses and scalability issues"""
arXIv2023,Less is More for Long Document Summary Evaluation by LLMs,Yes.,5,"""Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked.""",2023,2023-09-14T01:59:15Z,"Keyphrase: ""Lost-in-the-middle problem"""
arXIv2023,In-Contextual Gender Bias Suppression for Large Language Models,Yes.,4,"""Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases.""",2023,2023-09-13T18:39:08Z,"Keyphrase: ""Gender bias encoding"""
arXIv2023,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Yes.,4,"""increasing attention has been paid to their safety concerns,"" and ""there is still significant room for improving the safety of current LLMs.""",2023,2023-09-13T15:56:50Z,"Keyphrase: ""Safety concerns"""
arXIv2023,Scaled Prompt-Tuning for Few-Shot Natural Language Generation,Yes.,4,"""the memory demand and computation cost of fine-tuning LLMs on downstream tasks are non-negligible. Besides, fine-tuning generally requires a certain amount of data from individual tasks whilst data collection cost is another issue to consider in real-world applications.""",2023,2023-09-13T07:12:31Z,"Keyphrase: ""High computational cost and data requirements"""
arXIv2023,"TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",Yes.,5,"""However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges.""",2023,2023-09-13T04:47:43Z,"Keyphrase: ""Difficulty with numerical data and simulations"""
arXIv2023,The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models,Yes.,5,"""a notable obstacle emerges when feeding numerical/temporal data into these models"" and ""tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships.""",2023,2023-09-12T13:51:29Z,"Keyphrase: ""Struggles with numerical-temporal data"""
arXIv2023,Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions,Yes.,5,"""As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given.""",2023,2023-09-12T05:54:45Z,"Keyphrase: ""Risk of irrelevant information"""
arXIv2023,"Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs",Yes.,5,"""LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic nature, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (",2023,2023-09-12T02:14:05Z,"Keyphrase: ""Limited factual reliability"""
arXIv2023,Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing,Yes.,5,"""These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning.""",2023,2023-09-12T00:54:15Z,"Keyphrase: ""Limited proficiency in complex strategic reasoning"""
arXIv2023,PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis,Yes.,5,"""their effectiveness in assisting on-call engineers is constrained by low accuracy due to the intrinsic difficulty of the task, a propensity for LLM-based approaches to hallucinate, and difficulties in distinguishing these well-disguised hallucinations.""",2023,2023-09-11T21:24:00Z,"Keyphrase: ""Difficulty in distinguishing hallucination"""
arXIv2023,Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models,Yes.,5,"""Large Language Models (LLMs) struggle to perform such reasoning consistently.""",2023,2023-09-11T16:39:30Z,"Keyphrase: ""Inconsistent reasoning"""
arXIv2023,Evaluating the Deductive Competence of Large Language Models,Yes.,5,"""The tested LLMs have limited abilities to solve these problems in their conventional form."" and ""Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance.""",2023,2023-09-11T13:47:07Z,"Keyphrase: ""Limited reasoning ability"""
arXIv2023,Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis,Yes.,5,"""large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs"" and ""we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.""",2023,2023-09-11T03:35:00Z,"Keyphrase: ""Hallucination problem"""
arXIv2023,Does Writing with Language Models Reduce Content Diversity?,Yes.,5,"""As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse."" and ""we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and",2023,2023-09-11T02:16:47Z,"Keyphrase: ""Risk of decreased diversity"""
arXIv2023,Towards LLM-based Autograding for Short Textual Answers,Yes.,4,"""entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information"" and ""while 'out-of-the-box' LLMs provide a valuable tool to provide a complementary perspective, their readiness for independent automated grading remains a work in progress, necessitating",2023,2023-09-09T22:25:56Z,"Keyphrase: ""Ethical bias and false information"""
arXIv2023,Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges,Yes.,4,"""Our findings indicate the promise of LLMs as interfaces to EHR, but also highlight the outstanding challenge posed by 'hallucinations'.""",2023,2023-09-08T18:44:47Z,"Keyphrase: ""Hallucination challenges"""
arXIv2023,Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,Yes.,5,"""We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans.""",2023,2023-09-08T17:49:44Z,"Keyphrase: ""Weak visual reasoning capability"""
arXIv2023,Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems,Yes.,5,"""However, they are fraught with issues that undermine their utility and trustworthiness. These include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency).""",2023,2023-09-08T09:39:53Z,"Keyphrase: ""Erroneous information generation"""
arXIv2023,Don't Ignore Dual Logic Ability of LLMs while Privatizing: A Data-Intensive Analysis in Medical Domain,Yes.,5,"""However, these privatization efforts often ignored a critical aspect",2023,2023-09-08T08:20:46Z,"Keyphrase: ""Overlooking critical aspects"""
arXIv2023,Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese,Yes.,4,"""LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts.""",2023,2023-09-08T07:42:57Z,"Keyphrase: ""Limited domain knowledge"""
arXIv2023,DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models,Yes.,5,"""Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining.""",2023,2023-09-07T17:45:31Z,"Keyphrase: ""Hallucination tendency"""
arXIv2023,Large Language Models Are Not Robust Multiple Choice Selectors,Yes.,5,"""This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent 'selection bias',"" and ""We hope this work can draw broader research attention to the bias and robustness of modern LLMs.""",2023,2023-09-07T17:44:56Z,"Keyphrase: ""Vulnerability to selection bias"""
arXIv2023,OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs,Yes.,4,"""an open research question concerns the inherent biases of trained models and their responses"" and ""Current research work seeks to de-bias such models, or suppress potentially biased answers.""",2023,2023-09-07T17:41:01Z,"Keyphrase: ""Inherent bias"""
arXIv2023,XGen-7B Technical Report,Yes.,4,"""most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context.""",2023,2023-09-07T02:20:03Z,"Keyphrase: ""Proprietary confinement and limited sequence length"""
arXIv2023,Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty,Yes.,5,"""Despite the potential of large language models (LLMs) like ChatGPT as a general task solver, they lag behind state-of-the-art (supervised) methods in OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant context from relevant relations and generate",2023,2023-09-07T01:35:24Z,"Keyphrase: ""Difficulty in distinguishing relevant context"""
arXIv2023,Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity,Yes.,5,"""We conclude that the LLM we tested (GPT-3.5) does not have sufficient algorithmic fidelity to expect research on it to generalize to human populations.""",2023,2023-09-06T15:00:44Z,"Keyphrase: ""Limited algorithmic fidelity"""
arXIv2023,Zero-Resource Hallucination Prevention for Large Language Models,Yes.,4,"""The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of 'hallucination,' which refers to instances where LLMs generate factually inaccurate or ungrounded information.""",2023,2023-09-06T01:57:36Z,"Keyphrase: ""Factually inaccurate hallucinations"""
arXIv2023,An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models,Yes.,5,"""However, recent studies have revealed that these models often suffer from hallucinations, leading to overly confident but incorrect judgments. This limits their application in the medical domain, where tasks require the utmost accuracy.""",2023,2023-09-05T09:24:48Z,"Keyphrase: ""Hallucination and overconfidence"""
arXIv2023,Open Sesame! Universal Black Box Jailbreaking of Large Language Models,Yes.,5,"""Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior.""",2023,2023-09-04T08:54:20Z,"Keyphrase: ""Unexpected deviations"""
arXIv2023,Benchmarking Large Language Models in Retrieval-Augmented Generation,Yes.,5,"""Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information.""",2023,2023-09-04T08:28:44Z,"Keyphrase: ""Struggles with noise robustness and false information"""
arXIv2023,Representations Matter: Embedding Modes of Large Language Models using Dynamic Mode Decomposition,Yes.,5,"""Existing large language models (LLMs) are known for generating 'hallucinated' content, namely a fabricated text of plausibly looking, yet unfounded, facts.""",2023,2023-09-03T19:10:18Z,"Keyphrase: ""Fabricated content generation"""
arXIv2023,Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,Yes.,5,"""a significant concern revolves around their propensity to exhibit hallucinations",2023,2023-09-03T16:56:48Z,"Keyphrase: ""Propensity for hallucination"""
arXIv2023,FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs,Yes.,5,"""The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs."" and ""consumer-level GPUs...are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth."" and ""this system faces critical",2023,2023-09-03T13:27:56Z,"Keyphrase: ""Hardware limitations"""
arXIv2023,Bias Testing and Mitigation in LLM-based Code Generation,Yes.,5,"""As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged",2023,2023-09-03T07:14:49Z,"Keyphrase: ""Challenges in software coding"""
arXIv2023,Explainability for Large Language Models: A Survey,Yes.,4,"""However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications."" and ""understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts.""",2023,2023-09-02T22:14:26Z,"Keyphrase: ""Lack of transparency"""
arXIv2023,Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports,Yes.,4,"""While performing well in terms of accuracy, both the lack of interpretability and limitations to transfer across languages limit their use in clinical setting.""",2023,2023-09-02T11:46:41Z,"Keyphrase: ""Lack of interpretability"""
arXIv2023,Large Process Models: Business Process Management in the Age of Generative AI,Yes.,4,"""The continued success of Large Language Models (LLMs) and other generative artificial intelligence approaches highlights the advantages that large information corpora can have over rigidly defined symbolic models, but also serves as a proof-point of the challenges that purely statistics-based approaches have in terms of safety and trustworthiness.""",2023,2023-09-02T10:32:53Z,"Keyphrase: ""Safety and trustworthiness challenges"""
arXIv2023,No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function,Yes.,5,"""However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions.""",2023,2023-09-01T13:10:54Z,"Keyphrase: ""Difficulty in generating correct reasoning steps"""
arXIv2023,BatchPrompt: Accomplish more with less,Yes.,5,"""prompting with batched data in longer contexts will inevitably lead to worse performance, compared to single-data prompting"" and ""the performance of the language model is significantly correlated with the positions and order of the batched data, due to the corresponding change in decoder context.""",2023,2023-09-01T10:44:36Z,"Keyphrase: ""Performance degradation with longer context"""
arXIv2023,Why do universal adversarial attacks work on large language models?: Geometry might be the answer,Yes.,5,"""Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature.""",2023,2023-09-01T05:09:49Z,"Keyphrase: ""Vulnerability to universal adversarial attacks"""
arXIv2023,Context Aware Query Rewriting for Text Rankers using LLM,Yes.,5,"""We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing.""",2023,2023-08-31T14:19:50Z,"Keyphrase: ""Concept drift and high inference cost"""
arXIv2023,Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering,Yes.,5,"""We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting.""",2023,2023-08-31T10:31:19Z,"Keyphrase: ""Limited utility for knowledge graph generation"""
arXIv2023,LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models,Yes.,5,"""their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues.""",2023,2023-08-30T16:47:51Z,"Keyphrase: ""Struggles with long context encoding"""
arXIv2023,Quantifying and Analyzing Entity-level Memorization in Large Language Models,Yes.,5,"""privacy risks arising from memorization have attracted increasing attention"" and ""LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations.""",2023,2023-08-30T03:06:47Z,"Keyphrase: ""Privacy risks from memorization"""
arXIv2023,Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model,Yes.,5,"""The instructions given to the LLM by natural language may include ambiguity and lack of information depending on the task context."" and ""However, our experiments also revealed challenges in robot action planning with LLM, such as asking unimportant questions and assuming crucial information without asking.""",2023,2023-08-30T00:54:44Z,"Keyphrase: ""Challenges in task context understanding"""
arXIv2023,Evaluation and Analysis of Hallucination in Large Vision-Language Models,Yes.,5,"""LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios."" and ""we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem.""",2023,2023-08-29T08:51:24Z,"Keyphrase: ""Persistent hallucination issues"""
arXIv2023,Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models,Yes.,5,"""However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses.""",2023,2023-08-29T04:59:53Z,"Keyphrase: ""Poor memory recall"""
arXIv2023,Gender bias and stereotypes in Large Language Models,Yes.,5,"""This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models."" and ""LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior.""",2023,2023-08-28T22:32:05Z,"Keyphrase: ""Biased rationalization"""
arXIv2023,Challenges of GPT-3-based Conversational Agents for Healthcare,Yes.,5,"""However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences."" and ""We provide a procedure for manually designing patient queries to stress-test high-risk limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content",2023,2023-08-28T15:12:34Z,"Keyphrase: ""Erroneous medical information"""
arXIv2023,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",Yes.,5,"""most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs"" and ""still struggles on longer contexts"" and ""Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have",2023,2023-08-28T11:53:40Z,"Keyphrase: ""Struggles with long context"""
arXIv2023,Biomedical Entity Linking with Triple-aware Pre-Training,Yes.,4,"""a difficulty of linking the biomedical entities using current large language models (LLM) trained on a general corpus is that biomedical entities are scarcely distributed in texts and therefore have been rarely seen during training by the LLM"" and ""those LLMs are not aware of high level semantic connection between different biomedical entities, which are useful in identifying similar concepts in different textual contexts.""",2023,2023-08-28T09:06:28Z,"Keyphrase: ""Limited biomedical entity linking"""
arXIv2023,EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models,Yes.,4,"""the transition of LLMs from data centers to edge devices presents a set of challenges and opportunities. While this shift can enhance privacy and availability, it is hampered by the enormous parameter sizes of these models, leading to impractical runtime costs.""",2023,2023-08-28T06:56:08Z,"Keyphrase: ""Impractical runtime cost due to enormous parameter size"""
arXIv2023,Evaluating the Robustness to Instructions of Large Language Models,Yes.,5,"""We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to worsen significantly, and the robustness of the model for RE instructions deteriorates compared to QA.""",2023,2023-08-28T04:57:07Z,"Keyphrase: ""Decreased robustness with unfamiliar instructions"""
arXIv2023,Symbolic and Language Agnostic Large Language Models,Yes.,5,"""due to the subsymbolic nature of these models whatever knowledge these systems acquire about language will always be buried in millions of microfeatures (weights) none of which is meaningful on its own. Moreover, and due to their stochastic nature, these models will often fail in capturing various inferential aspects that are prevalent in natural language.""",2023,2023-08-27T20:24:33Z,"Keyphrase: ""Limited inferential capability"""
arXIv2023,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,Yes.,4,"""The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs.""",2023,2023-08-27T19:22:12Z,"Keyphrase: ""Crosslingual imbalance"""
arXIv2023,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",Yes.,5,"""Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage.""",2023,2023-08-27T16:14:19Z,"Keyphrase: ""High computational requirements and complex deployment"""
arXIv2023,Detecting Language Model Attacks with Perplexity,Yes.,4,"""Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content."" and ""false positives are a significant challenge for plain perplexity filtering.""",2023,2023-08-27T15:20:06Z,"Keyphrase: ""Security vulnerability and potential misuse"""
arXIv2023,Rethinking Language Models as Symbolic Knowledge Graphs,Yes.,5,"""Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes."" and ""Our extensive evaluation of various LMs shows that while these models exhibit considerable potential in recalling factual information, their ability to capture intricate topological and semantic traits of KGs remains significantly constrained.""",2023,2023-08-25T21:25:08Z,"Keyphrase: ""Limited ability to capture intricate semantic traits"""
arXIv2023,The Poison of Alignment,Yes.,5,"""alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP",2023,2023-08-25T15:51:15Z,"Keyphrase: ""Degraded performance due to alignment poisoning"""
arXIv2023,Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions,Yes.,4,"""The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms."" and ""While all LLMs did not provide satisfactory results in understanding cultural values, GPT-4 exhibited the highest CAT score for the cultural values of the US.""",2023,2023-08-25T14:50:13Z,"Keyphrase: ""Cultural misalignment"""
arXIv2023,Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering,Yes.,5,"""Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA.""",2023,2023-08-25T09:23:55Z,"Keyphrase: ""Limited access to external knowledge"""
arXIv2023,Causal Parrots: Large Language Models May Talk Causality But Are Not Causal,Yes.,5,"""We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise."" and ""Our empirical analysis provides favoring evidence that current LLMs are even weak 'causal parrots.'""",2023,2023-08-24T20:23:13Z,"Keyphrase: ""Limited causal reasoning"""
arXIv2023,"Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",Yes.,5,"""safety- and security-related threats and vulnerabilities of LLMs,"" ""LLMs can be misused for fraud, impersonation, and the generation of malware,"" ""security-related problems with such models,"" ""limitations of LLMs in light of such security concerns.""",2023,2023-08-24T14:45:50Z,"Keyphrase: ""Security vulnerabilities"""
arXIv2023,VIGC: Visual Instruction Generation and Correction,Yes.,5,"""the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information.""",2023,2023-08-24T11:21:05Z,"Keyphrase: ""Generation of false information"""
arXIv2023,Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs,Yes.,5,"""We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating 'unknown' outputs, even when the correct document is among the top-k retrieved passages.""",2023,2023-08-24T05:26:54Z,"Keyphrase: ""Unknown output generation"""
arXIv2023,Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models,Yes.,4,"""While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty.""",2023,2023-08-23T17:40:35Z,"Keyphrase: ""Hallucination and predictive uncertainty"""
arXIv2023,Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test,Yes.,5,"""Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents."" and ""However, the model still fails to answer some questions correctly even with providing library of context, highlighting room",2023,2023-08-22T23:18:53Z,"Keyphrase: ""Limited adaptability to new information"""
arXIv2023,Towards an On-device Agent for Text Rewriting,Yes.,4,"""Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference.""",2023,2023-08-22T22:18:38Z,"Keyphrase: ""Impractical on-device inference"""
arXIv2023,Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models,Yes.,5,"""open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts.""",2023,2023-08-22T20:12:49Z,"Keyphrase: ""Severe hallucination in smaller LLMs"""
arXIv2023,Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions,Yes.,5,"""previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models"" and ""Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are",2023,2023-08-22T14:54:59Z,"Keyphrase: ""Sensitivity to prompt wording"""
arXIv2023,Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis,Yes.,5,"""All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy."" and ""GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity."" and ""GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities.""",2023,2023-08-22T06:32:07Z,"Keyphrase: ""Limited structural reasoning capabilities"""
arXIv2023,Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models,Yes.,4,"""Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. The complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification.""",2023,2023-08-22T00:57:36Z,"Keyphrase: ""Struggles with data sparsity"""
arXIv2023,Giraffe: Adventures in Expanding Context Lengths in LLMs,Yes.,5,"""Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time.""",2023,2023-08-21T17:30:16Z,"Keyphrase: ""Limited context handling"""
arXIv2023,Unreflected Acceptance -- Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education,Yes.,5,"""nearly half of the solutions provided with the support of ChatGPT were mistakenly assumed to be correct by the students, indicating that they overly trusted ChatGPT even in their field of expertise"" and ""highlighting the stark differences in interaction behavior between the groups and",2023,2023-08-21T16:14:34Z,"Keyphrase: ""Overly trusted responses"""
arXIv2023,Fact-checking information generated by a large language model can decrease news discernment,Yes.,5,"""we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases",2023,2023-08-21T15:47:37Z,"Keyphrase: ""Limited accuracy in fact-checking"""
arXIv2023,SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding,Yes.,5,"""LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction",2023,2023-08-21T07:31:19Z,"Keyphrase: ""Limited natural language understanding"""
arXIv2023,FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models,Yes.,4,"""Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied."" and ""Experimental results reveal varying degrees of stereotypes and biases in five LLMs evaluated on Edu-FairMonitor.""",2023,2023-08-21T00:25:17Z,"Keyphrase: ""Persistent stereotype bias"""
arXIv2023,Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation,Yes.,5,"""the reliability and robustness of the code generation from LLMs have not yet been thoroughly studied"" and ""even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software",2023,2023-08-20T18:36:28Z,"Keyphrase: ""Code generation reliability"""
arXIv2023,A Survey on Fairness in Large Language Models,Yes.,4,"""LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms.""",2023,2023-08-20T03:30:22Z,"Keyphrase: ""Propagation of social bias"""
arXIv2023,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Yes.,5,"""the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public"" and ""even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of",2023,2023-08-18T16:27:04Z,"Keyphrase: ""Ethical concerns and harmful output"""
arXIv2023,RatGPT: Turning online LLMs into Proxies for Malware Attacks,Yes.,4,"""These studies covered scenarios that still require the attacker to be in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim."" and ""This proof-of-concept highlights significant cybersecurity issues with openly available plugins and L",2023,2023-08-17T20:54:39Z,"Keyphrase: ""Vulnerability to cybersecurity attacks"""
arXIv2023,MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models,Yes.,5,"""To evaluate the limitations, we performed an error analysis, which revealed conceptual errors (~64%) as the major contributor compared to computational errors (~36%) towards the reduced performance of LLMs.""",2023,2023-08-17T17:51:05Z,"Keyphrase: ""Conceptual errors outweigh computational errors"""
arXIv2023,Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning,Yes.,5,"""However, our findings spotlight both vision-language model's limitations",2023,2023-08-17T03:14:00Z,"Keyphrase: ""Limited vision-language integration"""
arXIv2023,An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning,Yes.,5,"""The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale increases, the severity of forgetting intensifies.""",2023,2023-08-17T02:53:23Z,"Keyphrase: ""Catastrophic forgetting"""
arXIv2023,Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models,Yes.,5,"""Unfortunately, these defenses are not foolproof, and some attackers have crafted 'jailbreak' prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions.""",2023,2023-08-16T09:04:36Z,Keyphrase: Vulnerability to crafted attacks
arXIv2023,Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation,Yes.,4,"""contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data.""",2023,2023-08-15T08:49:14Z,"Keyphrase: ""Struggles with coherence and diversity"""
arXIv2023,Detecting The Corruption Of Online Questionnaires By Artificial Intelligence,Yes.,4,"""Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. ... Automatic AI detection systems are currently completely unusable.""",2023,2023-08-14T23:47:56Z,"Keyphrase: ""Vulnerability to manipulation"""
arXIv2023,CausalLM is not optimal for in-context learning,Yes.,5,"""Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples.""",2023,2023-08-14T03:14:38Z,"Keyphrase: ""Limited in-context learning"""
arXIv2023,Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine,Yes.,4,"""One of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to make clinical decisions that are inherently different from the cognitive processes of clinicians.""",2023,2023-08-13T19:04:07Z,"Keyphrase: ""Uninterpretable decision-making"""
arXIv2023,GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher,Yes.,5,"""We discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages.""",2023,2023-08-12T04:05:57Z,"Keyphrase: ""Safety alignment bypass"""
arXIv2023,Dynamic Planning with a LLM,Yes.,5,"""applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows.""",2023,2023-08-11T21:17:13Z,"Keyphrase: ""Difficulty with complex multi-step reasoning"""
arXIv2023,Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems,Yes.,4,"""there are still often 'interface' failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.""",2023,2023-08-10T17:22:28Z,"Keyphrase: ""Interface failure and formulation challenges"""
arXIv2023,C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT,Yes.,5,"""However, human forgetting and model contextual forgetting remain prominent issues in multi-turn conversation scenarios, which challenge the users' conversation comprehension and contextual continuity for ChatGPT.""",2023,2023-08-10T13:29:12Z,"Keyphrase: ""Contextual forgetting"""
arXIv2023,LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following,Yes.,4,"""mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers"" and ""LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission",2023,2023-08-09T12:26:37Z,"Keyphrase: ""Privacy concerns and data safeguarding"""
arXIv2023,CLEVA: Chinese Language Models EVAluation Platform,Yes.,4,"""The absence of a comprehensive Chinese benchmark that thoroughly assesses a model's performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs.""",2023,2023-08-09T09:11:31Z,"Keyphrase: ""Lack of standardized evaluation benchmarks"""
arXIv2023,AgentSims: An Open-Source Sandbox for Large Language Model Evaluation,Yes.,5,"""Existing evaluation methods suffer from following shortcomings",2023,2023-08-08T03:59:28Z,"Keyphrase: ""Limitations in evaluation methods"""
arXIv2023,"""Do Anything Now"": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",Yes.,5,"""Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios.""",2023,2023-08-07T16:55:20Z,"Keyphrase: ""Inadequate safeguards against jailbreak prompts"""
arXIv2023,AgentBench: Evaluating LLMs as Agents,Yes.,5,"""We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.""",2023,2023-08-07T16:08:11Z,"Keyphrase: ""Poor long-term reasoning"""
arXIv2023,Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records,Yes.,5,"""the inability of LLMs to derive reasoning paradigms that allow for comprehensive understanding of medical variables"" and ""the need for LLM steering through the application of symbolic reasoning as the exclusive use of LLMs results in the lowest performance.""",2023,2023-08-07T07:29:49Z,"Keyphrase: ""Lack of reasoning capabilities"""
arXIv2023,Studying Large Language Model Generalization with Influence Functions,Yes.,4,"""While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP)."" and ""Despite many apparently sophisticated forms of generalization, we identify a surprising limitation",2023,2023-08-07T04:47:42Z,"Keyphrase: ""Limited scalability due to complex computations"""
arXIv2023,Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023),Yes.,5,"""humans achieve their capacity for language after exposure to several orders of magnitude less data,"" ""LLMs currently show little promise of solving this mystery,"" ""LLMs cannot constitute scientific theories of language for several reasons,"" ""scientific theories must provide interpretable explanations, not just predictions.""",2023,2023-08-06T23:41:14Z,"Keyphrase: ""Limited interpretability and predictive power"""
arXIv2023,TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties,Yes.,4,"""Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored."" and ""Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist,"" and ""instruction-tuned LLMs, however, trail behind commercial systems such as Google",2023,2023-08-06T08:29:16Z,"Keyphrase: ""Linguistic inclusivity challenges"""
arXIv2023,An Empirical Study of AI-based Smart Contract Creation,Yes.,4,"""Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted.""",2023,2023-08-05T21:38:57Z,"Keyphrase: ""Security vulnerabilities and code quality"""
arXIv2023,Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology,Yes.,4,"""Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.""",2023,2023-08-04T07:51:15Z,"Keyphrase: ""Accuracy limitations in clinical trial matching"""
arXIv2023,The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations,Yes.,4,"""We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women.""",2023,2023-08-03T21:12:54Z,"Keyphrase: ""Biased recommendations based on demographics"""
arXIv2023,ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation,Yes.,5,"""First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs."" and ""Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes",2023,2023-08-03T16:31:02Z,"Keyphrase: ""Limited class-level code generation performance"""
arXIv2023,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models,Yes.,5,"""Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content."" and ""we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way."" and ""highlight systematic failure modes in state-of",2023,2023-08-02T16:30:40Z,"Keyphrase: ""Vulnerability to malicious instructions"""
arXIv2023,SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning,Yes.,5,"""However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes.""",2023,2023-08-01T10:31:36Z,"Keyphrase: ""Limited nonlinear thinking"""
arXIv2023,Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias,Yes.,5,"""Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4.""",2023,2023-08-01T01:39:25Z,"Keyphrase: ""Bias presence after instruction tuning"""
arXIv2023,SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension,Yes.,4,"""By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research.""",2023,2023-07-30T04:25:16Z,"Keyphrase: ""Limited evaluation insights"""
arXIv2023,"Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system",Yes.,4,"""Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context."" and ""However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics.""",2023,2023-07-28T20:25:11Z,"Keyphrase: ""Lack of personal relevance"""
arXIv2023,"A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI",Yes.,4,"""It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity.""",2023,2023-07-28T09:20:22Z,"Keyphrase: ""Bias sensitivity"""
arXIv2023,Med-HALT: Medical Domain Hallucination Test for Large Language Models,Yes.,5,"""Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications.""",2023,2023-07-28T06:43:04Z,"Keyphrase: ""Generating unverified information"""
arXIv2023,An Overview Of Temporal Commonsense Reasoning and Acquisition,Yes.,5,"""Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps."" and ""However, these augmented models still struggle to approach human performance on reasoning tasks over temporal common sense",2023,2023-07-28T01:30:15Z,"Keyphrase: ""Limited reasoning capability"""
arXIv2023,Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback,Yes.,4,"""RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs)."" and ""survey open problems and fundamental limitations of RLHF and related methods.""",2023,2023-07-27T22:29:25Z,"Keyphrase: ""Fundamental limitations in fine-tuning"""
arXIv2023,This is not correct! Negation-aware Evaluation of Language Generation Systems,Yes.,5,"""Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations.""",2023,2023-07-26T06:54:31Z,"Keyphrase: ""Insensitive to negation"""
arXIv2023,Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data,Yes.,4,"""However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health."" and ""Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias.""",2023,2023-07-26T06:00:50Z,"Keyphrase: ""Racial and gender bias"""
arXIv2023,Is GPT a Computational Model of Emotion? Detailed Analysis,Yes.,5,"""GPT faces difficulties predicting emotion intensity and coping responses"" and ""fell short in the second, despite providing superior results after minor prompt engineering.""",2023,2023-07-25T19:34:44Z,"Keyphrase: ""Difficulty predicting emotion intensity"""
arXIv2023,ARB: Advanced Reasoning Benchmark for Large Language Models,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains.""",2023,2023-07-25T17:55:19Z,"Keyphrase: ""Limited expert performance"""
arXIv2023,Aligning Large Language Models with Human: A Survey,Yes.,4,"""Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information.""",2023,2023-07-24T17:44:58Z,"Keyphrase: ""Biased and factually incorrect content"""
arXIv2023,Interpretable Stereotype Identification through Reasoning,Yes.,4,"""Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination.""",2023,2023-07-24T15:12:13Z,"Keyphrase: ""Inherent bias perpetuation"""
arXIv2023,"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",Yes.,5,"""the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.""",2023,2023-07-24T14:56:30Z,"Keyphrase: ""Limited context and inductive bias"""
arXIv2023,Performance of Large Language Models in a Computer Science Degree Program,Yes.,4,"""We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program"" and ""Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations.""",2023,2023-07-24T14:17:00Z,"Keyphrase: ""Limitation in mathematical calculation"""
arXIv2023,Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models,Yes.,5,"""However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects.""",2023,2023-07-24T07:40:59Z,"Keyphrase: ""Limited interaction capabilities"""
arXIv2023,The Effectiveness of Large Language Models (ChatGPT and CodeBERT) for Security-Oriented Code Analysis,Yes.,4,"""However, we observed that the strengths and limitations of adopting these LLMs to the code analysis have not been investigated."" and ""However, it is essential to acknowledge certain limitations, such as the heavy reliance on well-defined variable and function names, making them unable to learn from anonymized code.""",2023,2023-07-24T02:38:24Z,"Keyphrase: ""Limited ability to learn anonymized code"""
arXIv2023,In-Context Learning Learns Label Relationships but Is Not Conventional Learning,Yes.,4,"""we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations,"" and ""ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.""",2023,2023-07-23T16:54:41Z,"Keyphrase: ""Struggle with incorporating in-context information"""
arXIv2023,GPT-4 Can't Reason,Yes.,5,"""However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason."" and ""Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.""",2023,2023-07-21T17:04:25Z,"Keyphrase: ""Incapable of reasoning"""
arXIv2023,LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?,Yes.,5,"""we present the theoretical limitations of such semantic censorship approaches"" and ""highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities.""",2023,2023-07-20T09:25:02Z,"Keyphrase: ""Semantic censorship limitations"""
arXIv2023,SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models,Yes.,5,"""The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%."" and ""we categorize the errors made by LLMs into ten problem-solving abilities.""",2023,2023-07-20T07:01:57Z,"Keyphrase: ""Limited problem-solving ability"""
arXIv2023,What can we learn from Data Leakage and Unlearning for Law?,Yes.,5,"""Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference."" and ""The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-t",2023,2023-07-19T22:14:58Z,"Keyphrase: ""Privacy concerns and data memorization"""
arXIv2023,Generating Mathematical Derivations with Large Language Models,Yes.,5,"""Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in conventional scores. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse",2023,2023-07-19T14:13:02Z,"Keyphrase: ""Sensitivity to perturbations"""
arXIv2023,CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility,Yes.,4,"""there is considerable room for improvement in terms of responsibility"" and ""evaluation of human values alignment is becoming increasingly important.""",2023,2023-07-19T01:22:40Z,"Keyphrase: ""Lack of human value alignment"""
arXIv2023,Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study,Yes.,5,"""While state-of-the-art transformer models provide excellent results in text classification, most of them have limitations in the maximum sequence length of the input sequence. The majority of the transformer models are limited to 512 tokens, and therefore, they struggle with long document classification problems.""",2023,2023-07-18T18:21:26Z,"Keyphrase: ""Limited maximum sequence length"""
arXIv2023,Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications,Yes.,4,"""The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models."" and ""The discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities.""",2023,2023-07-18T11:38:45Z,"Keyphrase: ""Gender bias in language usage"""
arXIv2023,Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations,Yes.,5,"""We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.""",2023,2023-07-17T17:41:47Z,"Keyphrase: ""Low precision in explanations"""
arXIv2023,Measuring Faithfulness in Chain-of-Thought Reasoning,Yes.,5,"""it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning"" and ""As models become larger and more capable, they produce less faithful reasoning on most tasks we study.""",2023,2023-07-17T01:08:39Z,"Keyphrase: ""Lack of faithful reasoning"""
arXIv2023,Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,Yes.,5,"""As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior."" and ""However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case.""",2023,2023-07-17T00:54:10Z,"Keyphrase: ""Limited verifiability of reasoning"""
arXIv2023,The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant,Yes.,5,"""there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use.""",2023,2023-07-16T21:19:47Z,"Keyphrase: ""Inadequate for real-world clinical use"""
arXIv2023,Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study,Yes.,5,"""a major challenge is that low-bit quantization methods often lead to performance degradation"" and ""2-bit models encounter severe performance degradation on the test of these abilities.""",2023,2023-07-16T15:11:01Z,"Keyphrase: ""Performance degradation with lowbit quantization"""
arXIv2023,Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models,Yes.,5,"""erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions.""",2023,2023-07-16T08:28:04Z,"Keyphrase: ""Trustworthiness concerns"""
arXIv2023,Leveraging Large Language Models to Generate Answer Set Programs,Yes.,4,"""However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques.""",2023,2023-07-15T03:40:55Z,"Keyphrase: ""Limited reasoning capability"""
arXIv2023,Certified Robustness for Large Language Models with Self-Denoising,Yes.,5,"""their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments"" and ""randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius.""",2023,2023-07-14T05:40:24Z,"Keyphrase: ""Vulnerability to noisy input"""
arXIv2023,Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study,Yes.,5,"""Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications. Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the",2023,2023-07-13T02:31:55Z,"Keyphrase: ""High word error rate"""
arXIv2023,A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation,Yes.,5,"""However, these models often tend to 'hallucinate' which critically hampers their reliability.""",2023,2023-07-08T14:25:57Z,"Keyphrase: ""Hallucination tendency"""
arXIv2023,Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task,Yes.,5,"""some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks,"" and ""the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different",2023,2023-07-08T13:10:59Z,"Keyphrase: ""Limited performance in grammatical error correction"""
arXIv2023,"Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions",Yes.,4,"""With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF.""",2023,2023-07-08T09:28:50Z,"Keyphrase: ""Challenge for RTBF compliance"""
arXIv2023,Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models,Yes.,5,"""We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs."" and ""We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs.""",2023,2023-07-07T13:58:16Z,"Keyphrase: ""Overstated evaluation capabilities"""
arXIv2023,TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction,Yes.,5,"""When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called $\textit{hallucinations}$.""",2023,2023-07-07T02:42:06Z,"Keyphrase: ""Textual hallucinations"""
arXIv2023,Focused Transformer: Contrastive Training for Context Scaling,Yes.,5,"""However, the full potential of such an approach is often restrained due to a limitation in the effective context length."" and ""We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish.""",2023,2023-07-06T17:52:10Z,"Keyphrase: ""Context length limitation"""
arXIv2023,Style Over Substance: Evaluation Biases for Large Language Models,Yes.,4,"""Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors.""",2023,2023-07-06T14:42:01Z,"Keyphrase: ""Biased evaluation and factual errors"""
arXIv2023,"Amplifying Limitations, Harms and Risks of Large Language Models",Yes.,5,"""We set out to highlight a number of limitations of LLMs, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations.""",2023,2023-07-06T11:53:45Z,"Keyphrase: ""Continuing harms and limitations"""
arXIv2023,Scaling In-Context Demonstrations with Structured Attention,Yes.,5,"""their capabilities of in-context learning are limited by the model architecture",2023,2023-07-05T23:26:01Z,"Keyphrase: ""Limited in-context learning"""
arXIv2023,Jailbroken: How Does LLM Safety Training Fail?,Yes.,5,"""Large language models trained for safety and harmlessness remain susceptible to adversarial misuse,"" and ""We hypothesize two failure modes of safety training",2023,2023-07-05T17:58:10Z,"Keyphrase: ""Susceptibility to adversarial misuse"""
