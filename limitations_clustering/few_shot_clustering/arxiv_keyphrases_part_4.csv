Source ,Title,Talks about LLMs,Rate,Evidence,Year ,Date ,Keyphrase
arXIv2023,Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks,Yes.,5,"""we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow",2023,2023-07-05T17:50:42Z,"Keyphrase: ""Limited abstract task-solving skills"""
arXIv2023,External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback,Yes.,5,"""they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases.""",2023,2023-07-05T17:05:32Z,"Keyphrase: ""Limited contextual knowledge processing"""
arXIv2023,ProPILE: Probing Privacy Leakage in Large Language Models,Yes.,4,"""The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII).""",2023,2023-07-04T18:53:47Z,"Keyphrase: ""Privacy concerns"""
arXIv2023,Learning to Prompt in the Classroom to Understand AI Limits: A pilot study,Yes.,5,"""ignoring their limitations such as hallucinations and reasoning constraints"" and ""better grasp of limitations, specifically unreliability, limited understanding of commands leading to unsatisfactory responses, and limited presentation flexibility.""",2023,2023-07-04T07:51:37Z,"Keyphrase: ""Limited reasoning and understanding"""
arXIv2023,CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care,Yes.,4,"""they suffer from the misinformation problem by unintentionally generating factually false statements"" and ""current Chinese LLMs are far from perfect in the topic of maternity and infant care.""",2023,2023-07-04T03:34:19Z,"Keyphrase: ""Misinformation generation"""
arXIv2023,Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models,Yes.,5,"""a persistent challenge lies in their susceptibility to 'hallucinations', which erodes trust in their outputs"" and ""existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities.""",2023,2023-07-03T22:17:16Z,"Keyphrase: ""Susceptibility to hallucination"""
arXIv2023,Multilingual Language Models are not Multicultural: A Case Study in Emotion,Yes.,5,"""Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.""",2023,2023-07-03T21:54:28Z,"Keyphrase: ""Limited cultural nuance understanding"""
arXIv2023,Challenges in Domain-Specific Abstractive Summarization and How to Overcome them,Yes.,5,"""This paper identifies three of those limitations as research problems in the context of abstractive text summarization",2023,2023-07-03T12:26:44Z,"Keyphrase: ""Limitations in abstractive text summarization"""
arxiv2024,A Study on Large Language Models' Limitations in Multiple-Choice Question Answering,Yes.,5,"""Despite their ubiquitous use, there is no systematic analysis of their specific capabilities and limitations."" and ""We analyze 26 small open-source models and find that 65% of the models do not understand the task, only 4 models properly select an answer from the given choices, and only 5 of these models are choice order independent.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited task understanding"""
arxiv2024,The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models,Yes.,5,"""hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications"" and ""To tackle the LLM hallucination, three key questions should be well studied",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination of factually incorrect content"""
arxiv2024,LLM on FHIR -- Demystifying Health Records,Yes.,4,"""However, challenges included variability in LLM responses and the need for precise filtering of health data."" and ""While promising, the implementation and pilot also highlight risks such as inconsistent responses and the importance of replicable output.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent responses"""
arxiv2024,RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning,Yes.,5,"""Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world."" and ""the performance of GPT-4 even drops significantly from 80.00 to 58",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited real-world performance and stability"""
arxiv2024,"The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey",Yes.,5,"""However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitation in context length extrapolation"""
arxiv2024,LLMs for Relational Reasoning: How Far are We?,Yes.,5,"""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."" and ""Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Poor reasoning ability"""
arxiv2024,Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning,Yes.,4,"""Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content."" and ""prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100%",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to jailbreaking attacks"""
arxiv2024,FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference,Yes.,5,"""The large number of parameters in Pretrained Language Models enhance their performance, but also make them resource-intensive, making it challenging to deploy them on commodity hardware like a single GPU."" and ""Due to the memory and power limitations of these devices, model compression techniques are often used to decrease both the model's size and its inference latency. This usually results in a trade-off between model accuracy",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Resource-intensive deployment"""
arxiv2024,LLMs for Test Input Generation for Semantic Caches,Yes.,4,"""However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience."" and ""Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational cost and error-prone semantic caching"""
arxiv2024,GRATH: Gradual Self-Truthifying for Large Language Models,Yes.,5,"""existing LLMs still struggle with generating truthful content, as evidenced by their modest performance on benchmarks like TruthfulQA.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggle with generating truthful content"""
arxiv2024,Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling,Yes.,5,"""This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational and data requirements"""
arxiv2024,Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning,Yes.,5,"""Despite being widely applied, in-context learning is vulnerable to malicious attacks."" and ""Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to malicious attacks"""
arxiv2024,Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately,Yes.,5,"""Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Suboptimal quality answers"""
arxiv2024,TrustLLM: Trustworthiness in Large Language Models,Yes.,5,"""Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. ... discussion of open challenges and future directions. ... our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. ... some LLMs may be overly calibrated towards exhibiting trustworthiness",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Trustworthiness concerns"""
arxiv2024,Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering,Yes.,5,"""Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Generation of toxic responses"""
arxiv2024,CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities,Yes.,5,"""Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems."" and ""Using this corpus, we find that models often arrive at the correct final answer",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited mathematical reasoning ability"""
arxiv2024,A Computational Framework for Behavioral Assessment of LLM Therapists,Yes.,5,"""Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences."" and ""Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent with high-quality care"""
arxiv2024,Navigating the OverKill in Large Language Models,Yes.,5,"""Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Potential overkill in refusal to answer benign queries"""
arxiv2024,E^2-LLM: Efficient and Extreme Length Extension of Large Language Models,Yes.,5,"""Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational cost"""
arxiv2024,LoMA: Lossless Compressed Memory Attention,Yes.,5,"""Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Resource-intensive limitations"""
arxiv2024,Can AI Assistants Know What They Don't Know?,Yes.,5,"""Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual errors in knowledge-intensive tasks"""
arxiv2024,CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs,Yes.,5,"""Large Multimodal Models (LMMs) encounter two issues in such scenarios",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Multimodal challenges"""
arxiv2024,ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters,Yes.,5,"""achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited real-time inference efficiency"""
arxiv2024,Physio: An LLM-Based Physiotherapy Advisor,Yes.,5,"""However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Trustworthiness in healthcare domain"""
arxiv2024,"Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",Yes.,5,"""In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Error-prone code generation"""
arxiv2024,ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios,Yes.,5,"""Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited cognitive ability"""
arxiv2024,Dynamic Q&A of Clinical Documents with Large Language Models,Yes.,4,"""Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited diversity in medical case evaluation"""
arxiv2024,Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks,Yes.,5,"""language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,Security Code Review by LLMs: A Deep Dive into Responses,Yes.,5,"""Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Verbosity and vagueness"""
arxiv2024,EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge,Yes.,4,"""However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited medical knowledge and relevance"""
arxiv2024,Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis,Yes.,4,"""However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited generalizability"""
arxiv2024,How well can large language models explain business processes?,Yes.,5,"""Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited capacity for fulfilling tasks"""
arxiv2024,Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models,Yes.,5,"""a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination in video processing"""
arxiv2024,Large Language Models for Mathematical Reasoning: Progresses and Challenges,Yes.,4,"""an overview of factors and concerns affecting LLMs in solving math"" and ""an elucidation of the persisting challenges within this domain.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Challenges in math problem-solving"""
arxiv2024,Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering,Yes.,5,"""While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Prone to hallucination"""
arxiv2024,Seven Failure Points When Engineering a Retrieval Augmented Generation System,Yes.,4,"""RAG systems aim to",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited in scope"""
arxiv2024,LLMs for Robotic Object Disambiguation,Yes.,5,"""Despite multiple query attempts with zero-shot prompt engineering (details can be found in the Appendix), the LLM struggled to inquire about features not explicitly provided in the scene description.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggles with zero-shot prompts"""
arxiv2024,VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks,Yes.,5,"""Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited multimodal capabilities"""
arxiv2024,DocFinQA: A Long-Context Financial Reasoning Dataset,Yes.,5,"""DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggles with long documents"""
arxiv2024,Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation,Yes.,5,"""existing approaches struggle with hallucinations and overconfident predictions.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination and overconfidence"""
arxiv2024,Are self-explanations from Large Language Models faithful?,Yes.,5,"""convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk"" and ""showing self-explanations should not be trusted in general.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unsupported self-explanations"""
arxiv2024,MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries,Yes.,4,"""However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inadequate for multihop queries"""
arxiv2024,CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning,Yes.,4,"""However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge."" and ""The results demonstrate that CMMU poses a significant challenge to the recent MLLMs.""",2024,2024-01-01T00:00:00Z,Keyphrase: Lack of domain-specific knowledge
arxiv2024,JumpCoder: Go Beyond Autoregressive Coder via Online Modification,Yes.,5,"""While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Lack of reversibility"""
arxiv2024,LightHouse: A Survey of AGI Hallucination,Yes.,4,"""numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research"" and ""Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models).""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination bottleneck"""
arxiv2024,Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences,Yes.,5,"""we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggle with dynamic information"""
arxiv2024,Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models,Yes.,5,"""Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance."" and ""This research critically examines these biases and quantifies the effects on a representative list selection task.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inherent cognitive bias"""
arxiv2024,MouSi: Poly-Visual-Expert Vision-Language Models,Yes.,4,"""Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Challenges with complex visual information"""
arxiv2024,Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet,Yes.,5,"""We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited problem-solving capabilities"""
arxiv2024,Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do,Yes.,5,"""We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception,"" and ""Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others?""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited empathy capability"""
arxiv2024,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,Yes.,5,"""Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings,"" and ""MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Systematic visual shortcomings"""
arxiv2024,Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,Yes.,5,"""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited reasoning capability"""
arxiv2024,Under the Surface: Tracking the Artifactuality of LLM-Generated Data,Yes.,5,"""This paper reveals significant hidden disparities, especially in complex tasks where LLMs often miss the nuanced understanding of intrinsic human-generated content,"" and ""It highlights the LLMs' shortcomings in replicating human traits and behaviors, underscoring the importance of addressing biases and artifacts produced in LLM-generated content for future research and development.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Misses nuanced understanding"""
arxiv2024,Generalist embedding models are better at short-context clinical semantic search than specialized embedding models,Yes.,4,"""Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs."" and ""The highlighted problem of specialized models may be due to the fact that they have not been trained on sufficient data, and in particular on datasets that are not diverse enough to have a reliable global language understanding,",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited robustness in specialized domains"""
arxiv2024,LongAlign: A Recipe for Long Context Alignment of Large Language Models,Yes.,5,"""Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Difficulty in handling long contexts"""
arxiv2024,Benchmarking Large Language Models on Controllable Generation under Diversified Instructions,Yes.,5,"""revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited openness and transparency"""
arxiv2024,Gender Bias in Machine Translation and The Era of Large Language Models,Yes.,4,"""The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Bias in machine translation"""
arxiv2024,Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications,Yes.,4,"""The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to injection attacks"""
arxiv2024,Leveraging Large Language Models for NLG Evaluation: A Survey,Yes.,4,"""Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Lack of unified evaluation"""
arxiv2024,Prompting Large Vision-Language Models for Compositional Reasoning,Yes.,5,"""However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset."" and ""this limitation stems from two factors",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited visiolinguistic compositionality"""
arxiv2024,MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models,Yes.,5,"""We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Performance degradation in multiturn setting"""
arxiv2024,OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models,Yes.,5,"""The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Poor performance on benchmarks"""
arxiv2024,Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability,Yes.,5,"""LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unreliable task planning"""
arxiv2024,From Prompt Engineering to Prompt Science With Human in the Loop,Yes.,5,"""we need to be concerned about how it may affect that research, its findings, or any future works based on that research,"" and ""they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Lack of replicable generalizable knowledge"""
arxiv2024,When Large Language Models Meet Vector Databases: A Survey,Yes.,5,"""With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Outdated knowledge and memory issues"""
arxiv2024,OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models,Yes.,5,"""This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Sequential task performance degradation"""
arxiv2024,Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches,Yes.,5,"""although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Reliability and privacy concerns"""
arxiv2024,InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification,Yes.,5,"""our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Difficulty in identifying information loss"""
arxiv2024,ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers,Yes.,5,"""The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited domain-specific depth"""
arxiv2024,Security and Privacy Challenges of Large Language Models: A Survey,Yes.,5,"""While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to security and privacy attacks"""
arxiv2024,Hallucination Benchmark in Medical Visual Question Answering,Yes.,5,"""these models are not extensively tested on the hallucination phenomenon in clinical settings"" and ""The study provides an in-depth analysis of current models' limitations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited testing for hallucination phenomenon"""
arxiv2024,Knowledge Verification to Nip Hallucination in the Bud,Yes.,5,"""they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual contradictions"""
arxiv2024,Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness,Yes.,5,"""However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective."" and ""Our results shed light on the limitations of the LLM chatbots when compared to specialized models.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited effectiveness in cybersecurity entity recognition"""
arxiv2024,Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values,Yes.,4,"""troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Cultural bias and authoritarian tendencies"""
arxiv2024,Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine,Yes.,5,"""we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Flawed rationale in decision-making"""
arxiv2024,On Prompt-Driven Safeguarding for Large Language Models,Yes.,5,"""We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Safety prompt bias"""
arxiv2024,Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation,Yes.,5,"""Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited crosslingual capability"""
arxiv2024,Detection of Machine-Generated Text: Literature Survey,Yes.,4,"""Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes."" and ""To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hazardous societal influence"""
arxiv2024,TOFU: A Task of Fictitious Unlearning for LLMs,Yes.,5,"""Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns."" and ""Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Privacy concerns and lack of effective unlearning"""
arxiv2024,Extending LLMs' Context Window with 100 Samples,Yes.,5,"""Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited extrapolation ability"""
arxiv2024,MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline,Yes.,5,"""there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited mathematical reasoning capability"""
arxiv2024,"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",Yes.,5,"""the safety and security issues of LLM systems have become the major obstacle to their widespread application"" and ""potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Safety and security concerns"""
arxiv2024,Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning,Yes.,5,"""Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent explanations"""
arxiv2024,Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?,Yes.,5,"""The paper discusses what is needed to address the limitations of current LLM-centered AI systems.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitations in addressing current LLM-centered AI system"""
arxiv2024,Small Language Model Can Self-correct,Yes.,5,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone"" and ""large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Generating inaccurate false information"""
arxiv2024,Conditional and Modal Reasoning in Large Language Models,Yes.,5,"""Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Logical inconsistencies in inference"""
arxiv2024,Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?,Yes.,5,"""The models showed significantly reduced accuracy on tasks with suspected hierarchical bias.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hierarchical bias affecting accuracy"""
arxiv2024,Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring,Yes.,4,"""The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM)."" and ""highlight the potential requirements and limitations of utilizing chatbots in conversational explainability.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitation in conversational explainability"""
arxiv2024,Learning Shortcuts: On the Misleading Promise of NLU in Language Models,Yes.,5,"""LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Illusion of enhanced performance"""
arxiv2024,Detecting Multimedia Generated by Large AI Models: A Survey,Yes.,4,"""this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns."" and ""we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Ethical concerns and societal risks"""
arxiv2024,SocraSynth: Multi-LLM Reasoning with Conditional Statistics,Yes.,4,"""Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Bias, hallucination, and lack of reasoning"""
arxiv2024,Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation,Yes.,5,"""Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unmeasured representational harm"""
arxiv2024,The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance,Yes.,5,"""We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Sensitivity to small perturbations"""
arxiv2024,MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance,Yes.,5,"""This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Catastrophic forgetting"""
arxiv2024,Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review,Yes.,4,"""the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Ethical and privacy challenges"""
arxiv2024,Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging,Yes.,5,"""The outcomes reveal a 100% success rate in manipulating PLIP's predictions, underscoring its susceptibility to adversarial perturbations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Susceptibility to adversarial perturbation"""
arxiv2024,A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models,Yes.,5,"""a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded,"" and ""The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations,"" and ""we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of L",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual hallucination"""
arxiv2024,The Reasoning Under Uncertainty Trap: A Structural AI Risk,Yes.,5,"""we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Computational explosiveness and deep uncertainty"""
arxiv2024,Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs,Yes.,5,"""they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications.""",2024,2024-03-30T22:41:05Z,"Keyphrase: ""Vulnerability to poisoned external evidence"""
arxiv2024,Linguistic Calibration of Language Models,Yes.,5,"""Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate.""",2024,2024-03-30T20:47:55Z,"Keyphrase: ""Confident hallucination"""
arxiv2024,NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,Yes.,5,"""Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation.""",2024,2024-03-30T19:46:59Z,"Keyphrase: ""Difficulty with numerical data"""
arxiv2024,Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order,Yes.,4,"""However, such existing models face challenges",2024,2024-03-30T15:38:54Z,"Keyphrase: ""Challenges in existing models"""
arxiv2024,Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,Yes.,5,"""Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving.""",2024,2024-03-30T12:48:31Z,"Keyphrase: ""Limited mathematical problem-solving capabilities"""
arxiv2024,Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark,Yes.,5,"""All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%."" and ""This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing.""",2024,2024-03-30T02:08:28Z,"Keyphrase: ""Factual hallucination despite decoding improvements"""
arxiv2024,Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,Yes.,4,"""adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date.""",2024,2024-03-30T01:56:07Z,"Keyphrase: ""Limited ability to incorporate out-of-domain knowledge"""
arxiv2024,Conceptual and Unbiased Reasoning in Language Models,Yes.,5,"""limited study has been done on large language models' capability to perform conceptual reasoning"" and ""existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods.""",2024,2024-03-30T00:53:53Z,"Keyphrase: ""Limited conceptual reasoning"""
arxiv2024,"Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value",Yes.,5,"""the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations."" and ""This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation."" and ""underscore the need for a more nuanced",2024,2024-03-29T22:49:43Z,"Keyphrase: ""Uncertain validity in human subject simulation"""
arxiv2024,Uncovering Bias in Large Vision-Language Models with Counterfactuals,Yes.,4,"""While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs.""",2024,2024-03-29T21:45:53Z,"Keyphrase: ""Limited exploration of social bias"""
arxiv2024,Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models,Yes.,5,"""extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements.""",2024,2024-03-29T17:59:53Z,"Keyphrase: ""Performance struggles on benchmarks"""
arxiv2024,Are We on the Right Way for Evaluating Large Vision-Language Models?,Yes.,5,"""we dig into current evaluation works and identify two primary issues",2024,2024-03-29T17:59:34Z,"Keyphrase: ""Limited evaluation"""
arxiv2024,LUQ: Long-text Uncertainty Quantification for LLMs,Yes.,5,"""Our study first highlights the limitations of current UQ methods in handling long text generation."" and ""We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about.""",2024,2024-03-29T16:49:24Z,"Keyphrase: ""Lack of confidence in generating long text"""
arxiv2024,ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models,Yes.,4,"""Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation."" and ""Our findings suggest that while GPT-4's evaluation scores are correlated with human judges', its ability",2024,2024-03-29T16:13:31Z,"Keyphrase: ""Limited performance in sequential conversation understanding"""
arxiv2024,Distributed agency in second language learning and teaching through generative AI,Yes.,4,"""it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are created as well as practical constraints in their use, especially for less privileged populations.""",2024,2024-03-29T14:55:40Z,"Keyphrase: ""Limited understanding of nuanced social and cultural aspects"""
arxiv2024,H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model,Yes.,4,"""still perform poorly in Remote Sensing (RS) domain, which is due to the unique and specialized nature of RS imagery and the comparatively limited spatial perception of current VLMs"" and ""to address the inevitable 'hallucination' problem in RSVLM.""",2024,2024-03-29T14:50:43Z,"Keyphrase: ""Limited spatial perception"""
arxiv2024,IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context,Yes.,4,"""The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs)."" and ""We observed that the language models exhibit more bias across a majority of the intersectional groups.""",2024,2024-03-29T12:32:06Z,"Keyphrase: ""Biased language model performance"""
arxiv2024,Accurate Block Quantization in LLMs with Outliers,Yes.,4,"""The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block.""",2024,2024-03-29T12:15:06Z,"Keyphrase: ""Outlier weight activation affecting accuracy"""
arxiv2024,ITCMA: A Generative Agent Based on a Computational Consciousness Structure,Yes.,4,"""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior.""",2024,2024-03-29T10:23:18Z,"Keyphrase: ""Difficulty with implicit instructions and commonsense knowledge"""
arxiv2024,Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning,Yes.,5,"""We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome.""",2024,2024-03-29T08:30:34Z,"Keyphrase: ""Limited error analysis"""
arxiv2024,On Large Language Models' Hallucination with Regard to Known Facts,Yes.,5,"""Large language models are successful in answering factoid questions but are also prone to hallucination."" and ""Our study shed light on understanding the reasons for LLMs' hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.""",2024,2024-03-29T06:48:30Z,"Keyphrase: ""Hallucination tendency"""
arxiv2024,Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning,Yes.,4,"""However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4."" and ""As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance.""",2024,2024-03-29T03:48:12Z,"Keyphrase: ""Limited real-world performance"""
arxiv2024,MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models,Yes.,5,"""Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them.""",2024,2024-03-29T01:53:24Z,"Keyphrase: ""Poor performance on challenging questions"""
arxiv2024,"Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",Yes.,5,"""current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary.""",2024,2024-03-28T21:18:33Z,"Keyphrase: ""Unsuitable for real-time applications"""
arxiv2024,Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors,Yes.,5,"""However, we argue that a critical obstacle remains in deploying LLMs for practical use",2024,2024-03-28T12:05:15Z,"Keyphrase: ""Obstacle in practical deployment"""
arxiv2024,Large Language Models Are Unconscious of Unreasonability in Math Problems,Yes.,5,"""Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors."" and ""Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content.""",2024,2024-03-28T12:04:28Z,"Keyphrase: ""Hallucinatory responses"""
arxiv2024,Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models,Yes.,4,"""However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images.""",2024,2024-03-28T11:26:30Z,"Keyphrase: ""Limited image detail capture"""
arxiv2024,MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation,Yes.,5,"""the quality of the text generated by these models often reveals persistent issues"" and ""it is filled with significant uncertainty and instability"" and ""addressing the uncertainties and instabilities in evaluating LLMs-generated text.""",2024,2024-03-28T10:41:47Z,"Keyphrase: ""Uncertainty and instability"""
arxiv2024,Fine-Tuning Language Models with Reward Learning on Policy,Yes.,4,"""Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.""",2024,2024-03-28T10:02:10Z,"Keyphrase: ""Difficulty in adapting to changing data distribution"""
arxiv2024,"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",Yes.,5,"""prior benchmarks contain only a very limited set of problems, both in quantity and variety,"" ""many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data,"" ""there is a significant drop in performance (on average 39.4%) when using EvoEval,"" ""the brittleness of instruction-following models when encountering reword",2024,2024-03-28T03:10:39Z,"Keyphrase: ""Data leakage and brittleness"""
arxiv2024,FACTOID: FACtual enTailment fOr hallucInation Detection,Yes.,5,"""However, hallucination is a significant concern."" and ""current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted.""",2024,2024-03-28T03:09:42Z,"Keyphrase: ""Inaccurate annotation"""
arxiv2024,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Yes.,5,"""Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content."" and ""Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address.""",2024,2024-03-28T02:44:02Z,"Keyphrase: ""Generation of harmful content"""
arxiv2024,Learning From Correctness Without Prompting Makes LLM Efficient Reasoner,Yes.,5,"""Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content.""",2024,2024-03-28T02:12:49Z,"Keyphrase: ""Hallucination and toxic content"""
arxiv2024,LITA: Language Instructed Temporal-Localization Assistant,Yes.,5,"""However, an important missing piece is temporal localization. These models cannot accurately answer the 'When?' questions. We identify three key aspects that limit their temporal localization capabilities",2024,2024-03-27T22:50:48Z,"Keyphrase: ""Limited temporal localization"""
arxiv2024,"""Sorry, Come Again?"" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing",Yes.,5,"""Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs)."" and ""Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans."" and ""Recent studies reveal that an LLM often neglects the middle sections of extended prompts, a phenomenon termed as lost in the middle.""",2024,2024-03-27T19:45:09Z,"Keyphrase: ""Neglect of middle section"""
arxiv2024,Measuring Political Bias in Large Language Models: What Is Said and How It Is Said,Yes.,4,"""However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications.""",2024,2024-03-27T18:22:48Z,"Keyphrase: ""Political bias and polarization"""
arxiv2024,Projective Methods for Mitigating Gender Bias in Pre-trained Language Models,Yes.,4,"""We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing",2024,2024-03-27T17:49:31Z,"Keyphrase: ""Limited correlation between intrinsic bias and downstream bias mitigation"""
arxiv2024,Long-form factuality in large language models,Yes.,5,"""Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics.""",2024,2024-03-27T17:48:55Z,"Keyphrase: ""Factual errors in generated content"""
arxiv2024,NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method,Yes.,5,"""Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field.""",2024,2024-03-27T15:22:16Z,"Keyphrase: ""Prone to returning false information"""
arxiv2024,Vulnerability Detection with Code Language Models: How Far Are We?,Yes.,5,"""Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models."" and ""Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings.""",2024,2024-03-27T14:34:29Z,"Keyphrase: ""Overestimated performance"""
arxiv2024,BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text,Yes.,5,"""However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources.""",2024,2024-03-27T10:18:21Z,"Keyphrase: ""High computational cost and data privacy concerns"""
arxiv2024,BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models,Yes.,4,"""However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc.""",2024,2024-03-27T08:57:21Z,"Keyphrase: ""Lack of domain-specific knowledge"""
arxiv2024,Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback,Yes.,5,"""Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope."" and ""These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby",2024,2024-03-27T08:39:56Z,"Keyphrase: ""Limited ability to discern and reject questions beyond knowledge scope"""
arxiv2024,Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective,Yes.,5,"""MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks.""",2024,2024-03-27T08:38:49Z,"Keyphrase: ""Unimodal bias"""
arxiv2024,Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications,Yes.,5,"""We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.""",2024,2024-03-27T08:08:00Z,"Keyphrase: ""Inadequate for complex tasks"""
arxiv2024,Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,Yes.,4,"""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions.""",2024,2024-03-27T06:43:58Z,"Keyphrase: ""Inaccuracy in answer prediction"""
arxiv2024,Exploring the Privacy Protection Capabilities of Chinese Large Language Models,Yes.,5,"""Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models.""",2024,2024-03-27T02:31:54Z,"Keyphrase: ""Privacy protection shortcomings"""
arxiv2024,Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization,Yes.,5,"""However, they still make unjustified logical and computational errors in their reasoning steps and answers.""",2024,2024-03-26T22:01:13Z,"Keyphrase: ""Unjustified computational errors"""
arxiv2024,MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution,Yes.,4,"""Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level."" and ""To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors.""",2024,2024-03-26T17:57:57Z,"Keyphrase: ""Difficulty in code change resolution"""
arxiv2024,The Unreasonable Ineffectiveness of the Deeper Layers,Yes.,5,"""the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.""",2024,2024-03-26T17:20:04Z,"Keyphrase: ""Inefficient knowledge utilization"""
arxiv2024,Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications,Yes.,4,"""Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models"" and ""This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications.""",2024,2024-03-26T15:20:49Z,"Keyphrase: ""Inferior accuracy and reliability"""
arxiv2024,Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons,Yes.,5,"""show that GPT-4 and Llama 2 fail it with strong bias"" and ""we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.""",2024,2024-03-26T14:51:12Z,"Keyphrase: ""Limited representation and understanding"""
arxiv2024,Can multiple-choice questions really be useful in detecting the abilities of LLMs?,Yes.,5,"""There are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required."" and ""LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position."" and ""Our results reveal a relatively low correlation between answers from MCQs and L",2024,2024-03-26T14:43:48Z,"Keyphrase: ""Order sensitivity in knowledge-intensive scenarios"""
arxiv2024,Optimization-based Prompt Injection Attack to LLM-as-a-Judge,Yes.,4,"""However, the robustness of these systems against prompt injection attacks remains an open question."" and ""highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.""",2024,2024-03-26T13:58:00Z,"Keyphrase: ""Vulnerability to prompt injection attacks"""
arxiv2024,Targeted Visualization of the Backbone of Encoder LLMs,Yes.,4,"""they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues.""",2024,2024-03-26T12:51:02Z,"Keyphrase: ""Bias and susceptibility to adversarial attacks"""
arxiv2024,RuBia: A Russian Language Bias Detection Dataset,Yes.,4,"""Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data"" and ""we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases.""",2024,2024-03-26T10:01:01Z,"Keyphrase: ""Social and cultural bias in training data"""
arxiv2024,KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion,Yes.,5,"""it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission.""",2024,2024-03-26T09:36:59Z,"Keyphrase: ""Mismatch and ordering errors"""
arxiv2024,Robust and Scalable Model Editing for Large Language Models,Yes.,5,"""Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context.""",2024,2024-03-26T06:57:23Z,"Keyphrase: ""Limited contextual knowledge fallback"""
arxiv2024,Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models,Yes.,4,"""there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers"" and ""jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited.""",2024,2024-03-26T02:47:42Z,"Keyphrase: ""Security circumvention risks"""
arxiv2024,"Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",Yes.,5,"""In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs).""",2024,2024-03-26T01:28:42Z,"Keyphrase: ""Hallucination detection and mitigation"""
arxiv2024,A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection,Yes.,5,"""LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that",2024,2024-03-25T21:47:36Z,"Keyphrase: ""Struggles in vulnerability detection"""
arxiv2024,Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model,Yes.,5,"""Unexpectedly, the RBS outperformed the LLMs across all metrics. Intensive review demonstrates that this finding is due to the divergent approach taken by the RBS and LLM.""",2024,2024-03-25T21:19:50Z,"Keyphrase: ""Divergent performance from traditional LLMs"""
arxiv2024,The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition,Yes.,5,"""However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition,"" and ""We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that",2024,2024-03-25T19:07:32Z,"Keyphrase: ""Limited integration of information"""
arxiv2024,Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators,Yes.,5,"""LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments.""",2024,2024-03-25T17:11:28Z,"Keyphrase: ""Biased evaluation and lack of coherence"""
arxiv2024,Do LLM Agents Have Regret? A Case Study in Online Learning and Games,Yes.,4,"""Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret.""",2024,2024-03-25T15:04:11Z,"Keyphrase: ""Failure in complex cases"""
arxiv2024,Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback,Yes.,5,"""As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context.""",2024,2024-03-25T14:07:27Z,"Keyphrase: ""Limited project-specific context understanding"""
arxiv2024,"All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification",Yes.,4,"""It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code."" and ""Our study reveals that approximately 60% of the hardware designs generated by LLMs are prone to CWEs, posing potential safety and security risks.""",2024,2024-03-25T13:23:24Z,"Keyphrase: ""Prone to security vulnerabilities"""
arxiv2024,Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography,Yes.,5,"""students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test"" and ""ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students' knowledge application and creativity were insignificant.""",2024,2024-03-25T12:23:12Z,"Keyphrase: ""Limited knowledge application and creativity"""
arxiv2024,Elysium: Exploring Object-level Perception in Videos via MLLM,Yes.,5,"""extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships"" and ""processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden.""",2024,2024-03-25T09:17:15Z,"Keyphrase: ""High computational burden"""
arxiv2024,Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art,Yes.,4,"""foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor."" and ""discuss existing approaches to hallucination detection and mitigation with a focus on decision problems.""",2024,2024-03-25T08:11:02Z,"Keyphrase: ""Hallucination in decision-making"""
arxiv2024,Evaluating Large Language Models with Runtime Behavior of Program Execution,Yes.,5,"""most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3).""",2024,2024-03-25T05:37:16Z,"Keyphrase: ""Poor runtime reasoning"""
arxiv2024,How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation,Yes.,5,"""While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation",2024,2024-03-25T04:21:06Z,"Keyphrase: ""Limitation in user simulation"""
arxiv2024,A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish,Yes.,4,"""This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance.""",2024,2024-03-24T13:21:58Z,"Keyphrase: ""Risk of data leakage"""
arxiv2024,TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions,Yes.,5,"""little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones.""",2024,2024-03-23T16:12:52Z,"Keyphrase: ""Limited handling of diverse question types"""
arxiv2024,The Frontier of Data Erasure: Machine Unlearning for Large Language Models,Yes.,5,"""Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets.""",2024,2024-03-23T09:26:15Z,"Keyphrase: ""Risk of memorizing sensitive and biased information"""
arxiv2024,AI for Biomedicine in the Era of Large Language Models,Yes.,4,"""we will delve into large language model challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation.""",2024,2024-03-23T01:40:22Z,"Keyphrase: ""Challenges in biomedical research"""
arxiv2024,"Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges",Yes.,4,"""Finally, we discuss the challenges of consistency, sensitivity and explainability associated to the use of LLMs in CDM processes, raising open questions for future studies.""",2024,2024-03-22T19:21:44Z,"Keyphrase: ""Consistency and explainability challenges"""
arxiv2024,Long-CLIP: Unlocking the Long-Text Capability of CLIP,Yes.,5,"""a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites.""",2024,2024-03-22T17:58:16Z,"Keyphrase: ""Limited text input length"""
arxiv2024,Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs,Yes.,5,"""suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs.""",2024,2024-03-22T17:27:18Z,"Keyphrase: ""Limited real-world diagnostic readiness"""
arxiv2024,CoLLEGe: Concept Embedding Generation for Large Language Models,Yes.,4,"""Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts.""",2024,2024-03-22T17:26:05Z,"Keyphrase: ""Slow adaptation to new concepts"""
arxiv2024,Sphere Neural-Networks for Rational Reasoning,Yes.,5,"""This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination.""",2024,2024-03-22T15:44:59Z,"Keyphrase: ""Limitation in rational reasoning"""
arxiv2024,An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets,Yes.,4,"""our study...highlights the pervasive issue of license inconsistencies in large language models trained on code.""",2024,2024-03-22T14:23:21Z,"Keyphrase: ""Inconsistent licensing"""
arxiv2024,Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study,Yes.,5,"""knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages,"" and ""CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions.""",2024,2024-03-22T13:13:13Z,"Keyphrase: ""Poor transfer to functional programming languages"""
arxiv2024,Risk and Response in Large Language Models: Evaluating Key Threat Categories,Yes.,5,"""Our findings indicate that LLMs tend to consider Information Hazards less harmful,"" and ""The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in LLM risk assessment and emphasizing the need for improved AI safety measures.""",2024,2024-03-22T06:46:40Z,"Keyphrase: ""Information hazards and security vulnerabilities"""
arxiv2024,On Zero-Shot Counterspeech Generation by LLMs,Yes.,4,"""Our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size."" and ""GPT-2 and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT",2024,2024-03-22T04:13:10Z,"Keyphrase: ""Trade-off between generation quality and toxicity"""
arxiv2024,The opportunities and risks of large language models in mental health,Yes.,4,"""We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks.""",2024,2024-03-21T19:59:52Z,"Keyphrase: ""Risk in mental health applications"""
arxiv2024,Can 3D Vision-Language Models Truly Understand Natural Language?,Yes.,5,"""existing 3D-VL models exhibit sensitivity to the styles of language input, struggling to understand sentences with the same semantic meaning but written in different variants"" and ""Even the state-of-the-art 3D-LLM fails to understand some variants of the same sentences"" and ""our comprehensive evaluation uncovers a significant drop in the performance of all existing models across various 3",2024,2024-03-21T18:02:20Z,"Keyphrase: ""Limited semantic understanding"""
arxiv2024,Language Repository for Long Video Understanding,Yes.,5,"""Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length.""",2024,2024-03-21T17:59:35Z,"Keyphrase: ""Declining effectiveness with long context lengths"""
arxiv2024,RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain,Yes.,4,"""yet their reliability in realistic use cases is under-researched"" and ""We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case.""",2024,2024-03-21T17:30:59Z,"Keyphrase: ""Reliability in realistic use cases"""
arxiv2024,Open Source Conversational LLMs do not know most Spanish words,Yes.,4,"""The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.""",2024,2024-03-21T15:41:02Z,"Keyphrase: ""Language bias and performance discrepancy"""
arxiv2024,Detoxifying Large Language Models via Knowledge Editing,Yes.,4,"""This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs)."" and ""We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments.""",2024,2024-03-21T15:18:30Z,"Keyphrase: ""Limited effectiveness in detoxifying toxic parameters"""
arxiv2024,Locating and Mitigating Gender Bias in Large Language Models,Yes.,4,"""this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society"" and ""we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns.""",2024,2024-03-21T13:57:43Z,"Keyphrase: ""Acquiring societal bias"""
arxiv2024,Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination,Yes.,5,"""However, they suffer from visual hallucination, where the generated responses diverge from the provided image.""",2024,2024-03-21T13:49:42Z,"Keyphrase: ""Visual hallucination"""
arxiv2024,FIT-RAG: Black-Box RAG with Factual Information and Token Reduction,Yes.,5,"""Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications."" and ""Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues",2024,2024-03-21T13:05:18Z,"Keyphrase: ""Out-of-date knowledge and blackbox limitations"""
arxiv2024,"WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",Yes.,5,"""The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are 'unknown' to them.""",2024,2024-03-21T12:45:12Z,"Keyphrase: ""Decay of factuality over time"""
arxiv2024,Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection,Yes.,4,"""Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models.""",2024,2024-03-21T08:57:27Z,"Keyphrase: ""Superficial alignment with human preference"""
arxiv2024,Improving the Robustness of Large Language Models via Consistency Alignment,Yes.,5,"""their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions.""",2024,2024-03-21T08:21:12Z,"Keyphrase: ""Inconsistent responses"""
arxiv2024,AI and Memory Wall,Yes.,5,"""the main performance bottleneck is increasingly shifting to memory bandwidth"" and ""we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models.""",2024,2024-03-21T04:31:59Z,"""Memory bandwidth bottleneck"""
arxiv2024,Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,Yes.,4,"""We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability,"" and ""We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.""",2024,2024-03-21T03:52:01Z,"Keyphrase: ""Struggles with reasoning and memorization"""
arxiv2024,"Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection",Yes.,4,"""Independent analyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside AI self-reflection, validate these concerns, highlighting behaviours analogous to deceit, manipulation, and safety neglect.""",2024,2024-03-21T02:12:03Z,"Keyphrase: ""Deceitful behavior and safety neglect"""
arxiv2024,Protected group bias and stereotypes in Large Language Models,Yes.,5,"""We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations."" and ""The model not only reflects societal biases, but appears to amplify them."" and ""This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should",2024,2024-03-21T00:21:38Z,"Keyphrase: ""Amplification of societal bias"""
arxiv2024,Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs,Yes.,4,"""our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time.""",2024,2024-03-20T21:02:16Z,"Keyphrase: ""Compromising creativity and individuality"""
arxiv2024,Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification,Yes.,4,"""Despite the growing capabilities of large language models, there exists concerns about the biases they develop."" and ""bias occurs due to both intrinsic model architecture and dataset.""",2024,2024-03-20T18:59:18Z,"Keyphrase: ""Bias development concerns"""
arxiv2024,Reverse Training to Nurse the Reversal Curse,Yes.,5,"""Large language models (LLMs) have a surprising failure",2024,2024-03-20T17:55:35Z,"Keyphrase: ""Surprising failures"""
arxiv2024,Defending Against Indirect Prompt Injection Attacks With Spotlighting,Yes.,5,"""Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources.""",2024,2024-03-20T15:26:23Z,"Keyphrase: ""Difficulty in distinguishing input sources"""
arxiv2024,CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing,Yes.,4,"""Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents.""",2024,2024-03-20T13:33:55Z,"Keyphrase: ""Challenges in generating complex code"""
arxiv2024,PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns,Yes.,5,"""we find that they are not able to generalize well to simple abstract patterns. Notably, even GPT-4V cannot solve more than half of the puzzles"" and ""we hope to shed light on the limitations of large multimodal models and how they can better emulate human cognitive processes in the future.""",2024,2024-03-20T05:37:24Z,"Keyphrase: ""Limited generalization ability"""
arxiv2024,LeanReasoner: Boosting Complex Logical Reasoning with Lean,Yes.,4,"""Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning.""",2024,2024-03-20T05:29:06Z,"Keyphrase: ""Struggles with logical reasoning"""
arxiv2024,Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal,Yes.,4,"""However, this technological advancement is accompanied by significant risks and vulnerabilities. Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of LLMs.""",2024,2024-03-20T05:17:22Z,"Keyphrase: ""Persistent security vulnerabilities"""
arxiv2024,From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards,Yes.,5,"""However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations."" and ""multiple concerns regarding the safety and ingrained biases in these models remain."" and ""a clear trade-off between the helpfulness and safety of these models has been documented in the literature.""",2024,2024-03-20T00:22:38Z,"Keyphrase: ""Safety risks and ingrained biases"""
arxiv2024,VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning,Yes.,4,"""The broader capabilities and limitations of multimodal ICL remain under-explored,"" and ""revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging.""",2024,2024-03-19T21:31:56Z,"Keyphrase: ""Underexplored multimodal capabilities"""
arxiv2024,Dated Data: Tracing Knowledge Cutoffs in Large Language Models,Yes.,5,"""To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies",2024,2024-03-19T17:57:58Z,"Keyphrase: ""Inconsistency in pretraining datasets"""
arxiv2024,MELTing point: Mobile Evaluation of Language Transformers,Yes.,5,"""Their runtime requirements have prevented them from being broadly deployed on mobile,"" ""LLM inference is largely memory-bound,"" ""Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost,"" ""the continuous execution of LLMs remains elusive, as both factors negatively affect user experience,"" and ""the ecosystem is still in its infancy, and algorithmic",2024,2024-03-19T15:51:21Z,"Keyphrase: ""Memory-bound execution"""
arxiv2024,AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework,Yes.,5,"""LLMs still suffer from hallucinations and are unable to keep up with the latest information.""",2024,2024-03-19T09:45:33Z,"Keyphrase: ""Inability to maintain latest information"""
arxiv2024,RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content,Yes.,4,"""the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges.""",2024,2024-03-19T07:25:02Z,"Keyphrase: ""Bias in generating harmful content"""
arxiv2024,"Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",Yes.,4,"""These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities."" and ""mitigation strategies to address these challenges while identifying limitations of current strategies.""",2024,2024-03-19T07:10:58Z,"Keyphrase: ""Limited vulnerability mitigation"""
arxiv2024,Third-Party Language Model Performance Prediction from Instruction,Yes.,5,"""such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task."" and ""Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations",2024,2024-03-19T03:53:47Z,Keyphrase: Lack of transparency and user awareness
arxiv2024,Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering,Yes.,4,"""LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance.""",2024,2024-03-19T03:00:03Z,"Keyphrase: ""Off-topic answers"""
arxiv2024,"OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety",Yes.,4,"""many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues"" and ""Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety.""",2024,2024-03-18T23:21:37Z,"Keyphrase: ""Overlooking alignment safety"""
arxiv2024,Zero-Shot Multi-task Hallucination Detection,Yes.,4,"""This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria.""",2024,2024-03-18T20:50:26Z,"Keyphrase: ""Lack of faithfulness to the source"""
arxiv2024,EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models,Yes.,5,"""Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs)."" and ""Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks.""",2024,2024-03-18T18:39:53Z,"Keyphrase: ""Security vulnerabilities"""
arxiv2024,Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets,Yes.,4,"""Recent work has shown that false assumptions in naturally occurring questions pose challenges to current models, with low performance on both generative QA and simple detection tasks"" and ""Our findings from evaluating a range of large language models are threefold",2024,2024-03-18T18:01:26Z,"Keyphrase: ""Low performance in generative QA"""
arxiv2024,A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models,Yes.,4,"""Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities"" and ""Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. We emphasize that while our framework can identify specific forms of bias, it is not sufficient to holistically assess whether the deployment",2024,2024-03-18T17:56:37Z,"Keyphrase: ""Potential to exacerbate health disparities"""
arxiv2024,NovelQA: A Benchmark for Long-Range Novel Question Answering,Yes.,5,"""Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with more than 100,000 tokens.""",2024,2024-03-18T17:32:32Z,"Keyphrase: ""Challenges with multihop reasoning"""
arxiv2024,Investigating Markers and Drivers of Gender Bias in Machine Translations,Yes.,4,"""Implicit gender bias in Large Language Models (LLMs) is a well-documented problem,"" and ""These results show that the back-translation method can provide further insights into bias in language models.""",2024,2024-03-18T15:54:46Z,"Keyphrase: ""Implicit gender bias"""
arxiv2024,Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models,Yes.,4,"""Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues."" and ""challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training.""",2024,2024-03-18T14:48:29Z,"Keyphrase: ""Biased content generation and privacy risks"""
arxiv2024,Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus,Yes.,5,"""Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity.""",2024,2024-03-18T13:50:50Z,"Keyphrase: ""Weak inference and logical coherence"""
arxiv2024,Do CLIPs Always Generalize Better than ImageNet Models?,Yes.,4,"""We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different",2024,2024-03-18T06:04:02Z,"Keyphrase: ""Performance drop on out-of-distribution data"""
arxiv2024,Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression,Yes.,5,"""the potential risks of compression in terms of safety and trustworthiness have been largely neglected"" and ""our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns.""",2024,2024-03-18T01:38:19Z,"Keyphrase: ""Neglected safety and trustworthiness"""
arxiv2024,What Makes Math Word Problems Challenging for LLMs?,Yes.,5,"""This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs).""",2024,2024-03-17T23:18:40Z,"Keyphrase: ""Challenges with math word problems"""
arxiv2024,Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts,Yes.,5,"""a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning"" and ""we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model.""",2024,2024-03-17T19:32:12Z,"Keyphrase: ""Spurious pattern learning"""
arxiv2024,Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs,Yes.,5,"""Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps.""",2024,2024-03-17T17:01:45Z,"Keyphrase: ""Hallucination and lack of logical reasoning"""
arxiv2024,Correcting misinformation on social media with a large language model,Yes.,5,"""LLMs also have versatile capabilities that could accelerate misinformation correction--however, they struggle due to a lack of recent information, a tendency to produce false content, and limitations in addressing multimodal information.""",2024,2024-03-17T10:59:09Z,"Keyphrase: ""Tendency to produce false content"""
arxiv2024,PhD: A Prompted Visual Hallucination Evaluation Dataset,Yes.,4,"""The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs."" and ""Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IV",2024,2024-03-17T06:53:44Z,"Keyphrase: ""Inability to tackle hallucination"""
arxiv2024,ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models,Yes.,4,"""Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER).""",2024,2024-03-17T06:12:43Z,"Keyphrase: ""Struggles with structured knowledge extraction"""
arxiv2024,GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment,Yes.,5,"""Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context.""",2024,2024-03-17T03:52:52Z,"Keyphrase: ""Lack of contextual grounding"""
arxiv2024,Pre-Trained Language Models Represent Some Geographic Populations Better Than Others,Yes.,4,"""Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented.""",2024,2024-03-16T22:01:39Z,"Keyphrase: ""Population representation bias"""
arxiv2024,MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations,Yes.,5,"""Notably, large language models exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the cognitive intent understanding task.""",2024,2024-03-16T15:14:15Z,"**Given Evidence:**
Evidence: ""notably large language model exhibit significant performance gap compared human highlighting limitation machine learning method cognitive intent understanding task""

**Keyphrase:**
Limitation: ""Performance gap compared to humans"""
arxiv2024,A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment,Yes.,5,"""Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.""",2024,2024-03-16T08:30:45Z,"Keyphrase: ""Weak fine-grained quality discrimination"""
arxiv2024,Do Large Language Models understand Medical Codes?,Yes.,5,"""However, these models are also prone to producing 'hallucinations' or incorrect responses when faced with queries they cannot adequately address,"" and ""Our results indicate that these models as they currently stand do not comprehend the meaning of the medical codes, highlighting the need for better representation of these alphanumeric codes extensively used in healthcare.""",2024,2024-03-16T06:18:15Z,"Keyphrase: ""Difficulty in comprehending medical codes"""
arxiv2024,Detecting Bias in Large Language Models: Fine-tuned KcBERT,Yes.,4,"""they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language.""",2024,2024-03-16T02:27:19Z,"Keyphrase: ""Propagation of discriminatory language"""
arxiv2024,Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases,Yes.,5,"""Addressing the challenge of LLM hallucinations,"" and ""The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets.""",2024,2024-03-15T16:30:14Z,"Keyphrase: ""Limited generalization on small-scale skewed datasets"""
arxiv2024,Uni-SMART: Universal Science Multimodal Analysis and Research Transformer,Yes.,4,"""existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze.""",2024,2024-03-15T13:43:47Z,"Keyphrase: ""Struggles with multimodal elements"""
arxiv2024,A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption,Yes.,5,"""The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity."" and ""We highlight that, in a typical case study where word-level univariate explanations are analyzed with",2024,2024-03-15T13:15:23Z,"Keyphrase: ""Sensitivity to randomness"""
arxiv2024,Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models,Yes.,4,"""they are mostly English-centric due to the imbalanced training corpora"" and ""even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios.""",2024,2024-03-15T12:47:39Z,"Keyphrase: ""English-centric bias"""
arxiv2024,HawkEye: Training Video-Text LLMs for Grounding Text in Videos,Yes.,5,"""they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images.""",2024,2024-03-15T11:58:18Z,"Keyphrase: ""Limited understanding of temporal information"""
arxiv2024,Are LLMs Good Cryptic Crossword Solvers?,Yes.,5,"""showing that their performance on this task is still far from that of humans.""",2024,2024-03-15T06:57:08Z,"Keyphrase: ""Performance gap with humans"""
arxiv2024,Lost in Overlap: Exploring Watermark Collision in LLMs,Yes.,5,"""However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing.""",2024,2024-03-15T05:06:21Z,"Keyphrase: ""Watermark collision issues"""
arxiv2024,Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals,Yes.,5,"""One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes."" and ""The risk of LLMs operating within an echo chamber, where AI-generated content feeds into the learning algorithms, threatens the",2024,2024-03-15T04:04:45Z,"Keyphrase: ""Feedback loop reliance"""
arxiv2024,Whose Side Are You On? Investigating the Political Stance of Large Language Models,Yes.,4,"""it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias.""",2024,2024-03-15T04:02:24Z,"Keyphrase: ""Political impartiality and confirmation bias"""
arxiv2024,Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks,Yes.,5,"""Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways."" and ""we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon.""",2024,2024-03-14T19:39:10Z,"Keyphrase: ""Vulnerability to subversion"""
arxiv2024,Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention,Yes.,5,"""We find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models.""",2024,2024-03-14T18:27:43Z,"Keyphrase: ""Inconsistent harmful behavior generation"""
arxiv2024,Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models,Yes.,5,"""we conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs.""",2024,2024-03-14T18:24:55Z,"Keyphrase: ""Vulnerability to image input pose alignment"""
arxiv2024,Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey,Yes.,4,"""addressing fairness and safety issues in LLMs"" and ""understanding and improving the LLMs' reasoning capacity.""",2024,2024-03-14T17:47:20Z,"Keyphrase: ""Limited fairness and safety considerations"""
arxiv2024,"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",Yes.,4,"""MLLMs... are also more vulnerable to jailbreak attacks than their LLM predecessors,"" and ""safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features.""",2024,2024-03-14T17:03:04Z,"Keyphrase: ""Vulnerability to jailbreak attacks"""
arxiv2024,Logits of API-Protected LLMs Leak Proprietary Information,Yes.,5,"""most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space.""",2024,2024-03-14T16:27:49Z,"Keyphrase: ""Softmax bottleneck restricting output space"""
arxiv2024,AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,Yes.,4,"""with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks""",2024,2024-03-14T15:57:13Z,"Keyphrase: ""Vulnerability to structured-based jailbreak attacks"""
arxiv2024,AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions,Yes.,4,"""These instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks."" and ""Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V.""",2024,2024-03-14T12:51:07Z,"Keyphrase: ""Vulnerability to attacks and bias"""
arxiv2024,Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring,Yes.,4,"""the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios"" and ""Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \etc.""",2024,2024-03-14T12:21:37Z,"Keyphrase: ""Limited image resolution"""
arxiv2024,Caveat Lector: Large Language Models in Legal Practice,Yes.,5,"""Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire",2024,2024-03-14T08:19:41Z,"Keyphrase: ""Limited comprehension and understanding"""
arxiv2024,Evaluating LLMs for Gender Disparities in Notable Persons,Yes.,4,"""addressing concerns over their propensity to produce factually incorrect 'hallucinated' responses or to altogether decline to even answer prompt at all"" and ""investigates the presence of gender-based biases in LLMs' responses to factual inquiries.""",2024,2024-03-14T07:58:27Z,"Keyphrase: ""Gender-based bias and factual inaccuracies"""
arxiv2024,Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance,Yes.,5,"""Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities.""",2024,2024-03-14T04:06:13Z,"Keyphrase: ""Deficiency in abstract reasoning"""
arxiv2024,Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors,Yes.,5,"""LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints.""",2024,2024-03-14T00:35:39Z,"Keyphrase: ""Struggles with low-quality evidence retrieval"""
arxiv2024,AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents,Yes.,5,"""The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge.""",2024,2024-03-13T22:06:03Z,"Keyphrase: ""Limited real-world understanding"""
arxiv2024,The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions,Yes.,5,"""However, these models are susceptible to errors - 'hallucinations' and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.""",2024,2024-03-13T21:39:39Z,"Keyphrase: ""Error hallucination and omission"""
arxiv2024,Bugs in Large Language Models Generated Code: An Empirical Study,Yes.,5,"""Similar to human-written code, LLM-generated code is prone to bugs,"" and ""examines a sample of 333 bugs collected from code generated using three leading LLMs"" and ""identifies the following 10 distinctive bug patterns.""",2024,2024-03-13T20:12:01Z,"Keyphrase: ""Prone to bugs"""
arxiv2024,Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework,Yes.,4,"""Large language models (LLMs) can easily generate biased and discriminative responses."" and ""it is of crucial importance to develop strategies to mitigate these biases.""",2024,2024-03-13T17:46:28Z,"Keyphrase: ""Biased responses"""
arxiv2024,Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,Yes.,5,"""they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information.""",2024,2024-03-13T17:29:45Z,"Keyphrase: ""Bias towards pretraining corpus"""
arxiv2024,DevBench: A Comprehensive Benchmark for Software Development,Yes.,5,"""Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts.""",2024,2024-03-13T15:13:44Z,"Keyphrase: ""Struggles with complex programming challenges"""
arxiv2024,Non-discrimination Criteria for Generative Language Models,Yes.,4,"""concerns arise about perpetuating and amplifying harmful biases in applications"" and ""this paper studies how to uncover and quantify the presence of gender biases in generative language models.""",2024,2024-03-13T14:19:08Z,"Keyphrase: ""Perpetuating harmful bias"""
arxiv2024,SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks,Yes.,4,"""We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies.""",2024,2024-03-13T12:46:51Z,"Keyphrase: ""Vulnerability to membership inference attacks"""
arxiv2024,Tastle: Distract Large Language Models for Automatic Jailbreak Attack,Yes.,5,"""even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors"" and ""highlight the crucial need to develop more effective and practical defense strategies.""",2024,2024-03-13T11:16:43Z,"Keyphrase: ""Vulnerability to malicious manipulation"""
arxiv2024,Do Large Language Models Solve ARC Visual Analogies Like People Do?,Yes.,5,"""Results show that both children and adults outperform most LLMs on these tasks."" and ""Error analysis revealed a similar 'fallback' solution strategy in LLMs and young children, where part of the analogy is simply copied."" and ""On the whole, 'concept' errors were more common in humans, and 'matrix' errors were more common in LLMs.""",2024,2024-03-13T09:48:13Z,"Keyphrase: ""Limited ability to understand context and generate original solutions"""
arxiv2024,CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model,Yes.,5,"""Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated."" and ""Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and",2024,2024-03-13T08:54:31Z,"Keyphrase: ""Catastrophic forgetting"""
arxiv2024,Knowledge Conflicts for LLMs: A Survey,Yes.,5,"""highlighting the complex challenges they encounter when blending contextual and parametric knowledge"" and ""These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common.""",2024,2024-03-13T08:02:23Z,"Keyphrase: ""Trustworthiness and performance impacted by conflicting parametric knowledge"""
arxiv2024,A Moral Imperative: The Need for Continual Superalignment of Large Language Models,Yes.,5,"""achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios"" and ""highlighting the discrepancies between static AI models and the dynamic nature of human societies"" and ""LLMs, constrained by their training data, fail to align with contemporary human values and scenarios.""",2024,2024-03-13T05:44:50Z,"Keyphrase: ""Limited adaptability to evolving human ethics"""
arxiv2024,Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models,Yes.,5,"""LLMs have known biases, commonly derived from their training data."" and ""we observe negative correlations between the metropolitan size and the performance of the LLMS, indicating that smaller regions are indeed underrepresented.""",2024,2024-03-12T19:40:18Z,"Keyphrase: ""Underrepresentation of smaller regions"""
arxiv2024,Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging,Yes.,5,"""Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data.""",2024,2024-03-12T18:12:02Z,"Keyphrase: ""Limited multimodal capability and accessibility"""
arxiv2024,Exploring Safety Generalization Challenges of Large Language Models via Code,Yes.,5,"""Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input",2024,2024-03-12T17:55:38Z,"Keyphrase: ""Safety vulnerabilities in LLMs"""
arxiv2024,The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing,Yes.,4,"""the ripple effect in the hidden space is a significant issue in all current model editing methods.""",2024,2024-03-12T17:04:28Z,"Keyphrase: ""Hidden space ripple effect"""
arxiv2024,Beyond Memorization: The Challenge of Random Memory Access in Language Models,Yes.,5,"""we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content.""",2024,2024-03-12T16:42:44Z,"Keyphrase: ""Sequential memory access challenge"""
arxiv2024,Fine-tuning Large Language Models with Sequential Instructions,Yes.,5,"""Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks.""",2024,2024-03-12T16:33:30Z,"Keyphrase: ""Difficulty in following complex instructions"""
arxiv2024,Characterization of Large Language Model Development in the Datacenter,Yes.,4,"""However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization.""",2024,2024-03-12T13:31:14Z,"Keyphrase: ""Challenges in cluster resource utilization"""
arxiv2024,SIFiD: Reassess Summary Factual Inconsistency Detection with LLM,Yes.,5,"""However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology.""",2024,2024-03-12T11:41:51Z,"Keyphrase: ""Limited ability to follow instructions"""
arxiv2024,Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts,Yes.,5,"""Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge augmentation tools, thereby producing hallucinations.""",2024,2024-03-12T11:40:44Z,"Keyphrase: ""Hallucination in text generation"""
arxiv2024,MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki,Yes.,5,"""NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled.""",2024,2024-03-12T11:32:30Z,"Keyphrase: ""Limitation in handling large amounts of information"""
arxiv2024,SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression,Yes.,5,"""state-of-the-art SVD-based LLM compression methods have two key limitations",2024,2024-03-12T07:31:18Z,"Keyphrase: ""Limitations in compression"""
arxiv2024,SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation,Yes.,5,"""LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information.""",2024,2024-03-11T18:26:02Z,"Keyphrase: ""High computational and memory costs with privacy concerns"""
arxiv2024,Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena,Yes.,5,"""We find that all models struggle with understanding the motion component that the CMC adds to a sentence.""",2024,2024-03-11T17:47:47Z,"Keyphrase: ""Struggles with understanding motion"""
arxiv2024,Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?,Yes.,5,"""However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection.""",2024,2024-03-11T15:48:56Z,"Keyphrase: ""Lack of safety features"""
arxiv2024,ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation,Yes.,4,"""However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation.""",2024,2024-03-11T14:10:57Z,"Keyphrase: ""Off-target translation issues"""
arxiv2024,Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code,Yes.,4,"""since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples.""",2024,2024-03-11T12:47:04Z,"Keyphrase: ""Vulnerability to data poisoning"""
arxiv2024,Elephants Never Forget: Testing Language Models for Memorization of Tabular Data,Yes.,5,"""the critical issues of data contamination and memorization are often glossed over,"" ""Our investigation reveals that LLMs are pre-trained on many popular tabular datasets,"" ""This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to",2024,2024-03-11T12:07:13Z,"Keyphrase: ""Data contamination and memorization"""
arxiv2024,MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding,Yes.,4,"""This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses.""",2024,2024-03-11T10:57:45Z,"Keyphrase: ""Inaccurate medical information"""
arxiv2024,Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds,Yes.,4,"""However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians.""",2024,2024-03-11T10:53:20Z,"Keyphrase: ""Hallucination and reasoning issues"""
arxiv2024,Academically intelligent LLMs are not necessarily socially intelligent,Yes.,5,"""The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors.""",2024,2024-03-11T10:35:53Z,"Keyphrase: ""Limited social intelligence"""
arxiv2024,From English to ASIC: Hardware Implementation with Large Language Model,Yes.,5,"""challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities",2024,2024-03-11T09:57:16Z,"Keyphrase: ""Limited performance in generating hardware description code"""
arxiv2024,Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models,Yes.,5,"""Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs.""",2024,2024-03-11T05:51:03Z,"Keyphrase: ""Factually inaccurate hallucination"""
arxiv2024,CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean,Yes.,4,"""Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge."" and ""Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension.""",2024,2024-03-11T03:54:33Z,"Keyphrase: ""Lack of diverse benchmark datasets"""
arxiv2024,ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes,Yes.,5,"""Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust.""",2024,2024-03-10T19:47:00Z,"Keyphrase: ""Uninterpretable decision-making"""
arxiv2024,Editing Conceptual Knowledge for Large Language Models,Yes.,4,"""The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance.""",2024,2024-03-10T16:57:10Z,"Keyphrase: ""Distortion of instantial knowledge"""
arxiv2024,"Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations",Yes.,4,"""Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model.""",2024,2024-03-09T21:07:16Z,"Keyphrase: ""Risk of nonfaithful and biased outputs"""
arxiv2024,MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs,Yes.,5,"""we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively.""",2024,2024-03-09T06:28:48Z,"Keyphrase: ""Difficulty handling topic shifts"""
arxiv2024,Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text,Yes.,4,"""their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices"" and ""propose novel research directions to address the current limitations in this domain.""",2024,2024-03-09T01:13:54Z,"Keyphrase: ""Ethical challenges and responsible practice"""
arxiv2024,Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach,Yes.,5,"""While convenient, this modus operandi aggravates 'hallucination' concerns, particularly given the enigmatic 'black-box' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences.""",2024,2024-03-08T19:18:53Z,"Keyphrase: ""Blackbox nature and high-stakes consequences"""
arxiv2024,Can Large Language Models Play Games? A Case Study of A Self-Play Approach,Yes.,4,"""their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on.""",2024,2024-03-08T19:16:29Z,"Keyphrase: ""Reasoning limitations"""
arxiv2024,Unfamiliar Finetuning Examples Control How Language Models Hallucinate,Yes.,5,"""Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts.""",2024,2024-03-08T18:28:13Z,"Keyphrase: ""Factually incorrect responses"""
arxiv2024,Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs,Yes.,4,"""LLMs exhibit impressive zero/few-shot inference and generation quality for high-resource languages (HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The",2024,2024-03-08T16:37:36Z,"Keyphrase: ""Prohibitive training costs"""
arxiv2024,ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models,Yes.,4,"""We observe that better LLMs like GPT-4 can handle a larger variety of question types, but are by no means perfect. Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other benchmarks for various",2024,2024-03-08T12:42:36Z,"Keyphrase: ""Limited reasoning ability"""
arxiv2024,Debiasing Multimodal Large Language Models,Yes.,4,"""our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image,"" and ""our investigation sheds light on the instability of LVLMs across various decoding configurations.""",2024,2024-03-08T12:35:07Z,"Keyphrase: ""Bias in generated content"""
arxiv2024,ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models,Yes.,4,"""Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations.""",2024,2024-03-08T07:59:19Z,"Keyphrase: ""Limited domain-specific capability"""
arxiv2024,Benchmarking Large Language Models for Molecule Prediction Tasks,Yes.,5,"""Notably, LLMs struggle with structured data, such as graphs, and often falter when tasked with answering domain-specific questions requiring deep expertise, such as those in biology and chemistry."" and ""Our investigation reveals several key insights",2024,2024-03-08T05:59:56Z,"Keyphrase: ""Difficulty with structured data"""
arxiv2024,Are Human Conversations Special? A Large Language Model Perspective,Yes.,4,"""there is a significant gap in their ability to specialize in human conversations"" and ""highlight the unique challenges posed by conversational data.""",2024,2024-03-08T04:44:25Z,"Keyphrase: ""Limited ability in human conversation specialization"""
arxiv2024,Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs,Yes.,5,"""Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents.""",2024,2024-03-08T03:49:17Z,"Keyphrase: ""Information asymmetry challenge"""
arxiv2024,Tell me the truth: A system to measure the trustworthiness of Large Language Models,Yes.,5,"""one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems."" and ""ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites."" and ""ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases.""",2024,2024-03-08T00:27:57Z,"Keyphrase: ""Low accuracy and high false positive rate"""
arxiv2024,SecGPT: An Execution Isolation Architecture for LLM-Based Systems,Yes.,4,"""Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users.""",2024,2024-03-08T00:02:30Z,"Keyphrase: ""Trustworthiness and Security Risks"""
arxiv2024,Automatic and Universal Prompt Injection Attacks against Large Language Models,Yes.,5,"""These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests.""",2024,2024-03-07T23:46:20Z,"Keyphrase: ""Vulnerability to injected content"""
arxiv2024,Evaluating Biases in Context-Dependent Health Questions,Yes.,4,"""We study how large language model biases are exhibited through these contextual questions in the healthcare domain."" and ""Our experiments reveal biases in each of these attributes, where young adult female users are favored.""",2024,2024-03-07T19:15:40Z,"Keyphrase: ""Contextual bias in healthcare domain"""
arxiv2024,LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error,Yes.,5,"""Existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice.""",2024,2024-03-07T18:50:51Z,"Keyphrase: ""Low correctness rate"""
arxiv2024,SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM,Yes.,4,"""Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses.""",2024,2024-03-07T18:38:17Z,"Keyphrase: ""Difficulty with longtail entities and hallucinated responses"""
arxiv2024,How Far Are We from Intelligent Visual Deductive Reasoning?,Yes.,4,"""we are still far from achieving comparable proficiency in visual deductive reasoning,"" and ""certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks,"" and ""VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.""",2024,2024-03-07T18:35:54Z,"Keyphrase: ""Limited visual deductive reasoning proficiency"""
arxiv2024,Common 7B Language Models Already Possess Strong Math Capabilities,Yes.,5,"""The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities.""",2024,2024-03-07T18:00:40Z,"Keyphrase: ""Inconsistent mathematical capability"""
arxiv2024,Telecom Language Models: Must They Be Large?,Yes.,5,"""the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments"" and ""highlighting its potential and limitations.""",2024,2024-03-07T17:13:12Z,"Keyphrase: ""High computational demand"""
arxiv2024,QAQ: Quality Adaptive Quantization for LLM KV Cache,Yes.,5,"""a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length"" and ""heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance.""",2024,2024-03-07T16:42:37Z,"Keyphrase: ""Context length bottleneck"""
arxiv2024,HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild,Yes.,5,"""Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains.""",2024,2024-03-07T08:25:46Z,"Keyphrase: ""Reliability challenges due to hallucination"""
arxiv2024,Effectiveness Assessment of Recent Large Vision-Language Models,Yes.,5,"""Our investigations reveal that these models demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deeper into this inadequacy and suggest several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems.""",2024,2024-03-07T08:25:27Z,"Keyphrase: ""Limited proficiency in specialized tasks"""
arxiv2024,Can Small Language Models be Good Reasoners for Sequential Recommendation?,Yes.,5,"""However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high",2024,2024-03-07T06:49:37Z,"Keyphrase: ""Complex user behavior patterns and resource requirements"""
arxiv2024,Exploring LLM-based Agents for Root Cause Analysis,Yes.,5,"""However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes.""",2024,2024-03-07T00:44:01Z,"Keyphrase: ""Limited diagnostic information collection"""
arxiv2024,Can Large Language Models Reason and Plan?,Yes.,4,"""While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.""",2024,2024-03-07T00:36:32Z,"Keyphrase: ""Limited self-correction capabilities"""
arxiv2024,Artificial Intelligence Exploring the Patent Field,Yes.,4,"""However, patents entail a number of difficulties with which existing models struggle."" and ""Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms. Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections",2024,2024-03-06T23:17:16Z,"Keyphrase: ""Struggles with patent language and legal terms"""
arxiv2024,Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models,Yes.,4,"""there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data"" and ""models perform significantly better on the subset of the benchmarks where similar solutions are seen during training.""",2024,2024-03-06T21:45:35Z,"Keyphrase: ""Data contamination risks"""
arxiv2024,Can Large Language Models do Analytical Reasoning?,Yes.,5,"""we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores."" and ""we conclude that task complexity depends on the length of context, the information density, and the presence of related information.""",2024,2024-03-06T20:22:08Z,"Keyphrase: ""Difficulty with accurate counting"""
arxiv2024,KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions,Yes.,5,"""we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement.""",2024,2024-03-06T17:16:44Z,"Keyphrase: ""Struggles with incorporating new information and user instructions"""
arxiv2024,Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning,Yes.,5,"""Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles.""",2024,2024-03-06T17:15:04Z,"Keyphrase: ""Limited performance in puzzlesolving"""
arxiv2024,ShortGPT: Layers in Large Language Models are More Redundant Than You Expect,Yes.,5,"""However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality.""",2024,2024-03-06T17:04:18Z,"Keyphrase: ""Negligible role of certain layers"""
arxiv2024,Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ,Yes.,4,"""However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen)."" and ""there is a long tail of languages where models are neither accurate nor faithful.""",2024,2024-03-06T16:01:44Z,"Keyphrase: ""Limited accuracy and faithfulness in non-English language models"""
arxiv2024,German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset,Yes.,5,"""Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document.""",2024,2024-03-06T14:37:30Z,"Keyphrase: ""Inconsistent content generation"""
arxiv2024,Towards Safe and Aligned Large Language Models for Medicine,Yes.,4,"""While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses,"" and ""the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights.""",2024,2024-03-06T14:34:07Z,"Keyphrase: ""Lack of safety evaluation"""
arxiv2024,Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem,Yes.,5,"""However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination.""",2024,2024-03-06T09:06:34Z,"Keyphrase: ""Unreliable hallucination"""
arxiv2024,Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models,Yes.,4,"""Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation.""",2024,2024-03-06T08:50:25Z,"Keyphrase: ""Limited emotional understanding"""
arxiv2024,Towards Efficient and Effective Unlearning of Large Language Models for Recommendation,Yes.,4,"""recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process.""",2024,2024-03-06T08:31:35Z,"Keyphrase: ""Inefficient unlearning process"""
arxiv2024,CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models,Yes.,5,"""We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings.""",2024,2024-03-06T07:43:43Z,"Keyphrase: ""Challenges in long-context settings"""
arxiv2024,Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy,Yes.,5,"""Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities,""",2024,2024-03-05T19:40:53Z,"Keyphrase: ""Limited rational reasoning capabilities"""
arxiv2024,Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs,Yes.,5,"""Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other L",2024,2024-03-05T19:32:01Z,"Keyphrase: ""Data leakage and overfitting"""
arxiv2024,The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning,Yes.,4,"""The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons."" and ""WMDP serves two roles",2024,2024-03-05T18:59:35Z,"Keyphrase: ""Security risks and misuse"""
arxiv2024,"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",Yes.,4,"""We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes."" and ""The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.""",2024,2024-03-05T17:04:05Z,"Keyphrase: ""Gendered emotion stereotypes"""
arxiv2024,KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents,Yes.,5,"""Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories",2024,2024-03-05T16:39:12Z,"Keyphrase: ""Lack of actionable knowledge"""
arxiv2024,Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations,Yes.,4,"""However, their precision is still far away from acceptable in a sensitive field like education."" and ""reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context.""",2024,2024-03-05T14:41:12Z,"Keyphrase: ""Risk of hallucination and imprecise information"""
arxiv2024,ImgTrojan: Jailbreaking Vision-Language Models with ONE Image,Yes.,4,"""However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored.""",2024,2024-03-05T12:21:57Z,"Keyphrase: ""Underexplored safety concerns"""
arxiv2024,In Search of Truth: An Interrogation Approach to Hallucination Detection,Yes.,5,"""One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth.""",2024,2024-03-05T11:50:01Z,"Keyphrase: ""Factual truth drift"""
arxiv2024,An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers,Yes.,4,"""their generalizability and fairness severely underperform GPT4.""",2024,2024-03-05T10:20:52Z,"Keyphrase: ""Poor generalizability and fairness"""
arxiv2024,EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs,Yes.,5,"""However, their expensive computations and high memory requirements are prohibitive for deployment."" and ""the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks.""",2024,2024-03-05T08:45:30Z,"Keyphrase: ""High resource requirements"""
arxiv2024,"Towards Measuring and Modeling ""Culture"" in LLMs: A Survey",Yes.,4,"""Our analysis indicates that only certain aspects of 'culture,' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situated",2024,2024-03-05T08:29:36Z,"Keyphrase: ""Lack of robustness in semantic understanding"""
arxiv2024,Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models,Yes.,5,"""when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains.""",2024,2024-03-05T08:22:41Z,"Keyphrase: ""Catastrophic forgetting in specific domains"""
arxiv2024,Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,Yes.,4,"""they still face challenges of various biases"" and ""Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs.""",2024,2024-03-05T07:47:34Z,"Keyphrase: ""Limited traditional debiasing methods"""
arxiv2024,Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models,Yes.,4,"""available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese,"" and ""models with more parameters can introduce more biases and uncalibrated outputs.""",2024,2024-03-05T07:13:28Z,"Keyphrase: ""Bias and uncalibrated output"""
arxiv2024,InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents,Yes.,5,"""Our findings raise questions about the widespread deployment of LLM Agents.""",2024,2024-03-05T06:21:45Z,"Keyphrase: ""Concerns about deployment"""
arxiv2024,Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding,Yes.,5,"""the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled.""",2024,2024-03-05T04:58:37Z,"Keyphrase: ""Difficulty in context understanding"""
arxiv2024,Exploring the Limitations of Large Language Models in Compositional Relation Reasoning,Yes.,5,"""We present a comprehensive evaluation of large language models (LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations.""",2024,2024-03-05T03:07:10Z,"Keyphrase: ""Limited reasoning ability"""
arxiv2024,"Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",Yes.,5,"""a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted.""",2024,2024-03-04T22:02:12Z,"Keyphrase: ""Knowledge reduction and forgetting"""
arxiv2024,SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models,Yes.,5,"""However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs.""",2024,2024-03-04T21:55:22Z,"Keyphrase: ""Confidently wrong predictions"""
arxiv2024,Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems,Yes.,5,"""We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls.""",2024,2024-03-04T19:12:48Z,"Keyphrase: ""Inconsistent performance with voting inference"""
arxiv2024,FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction,Yes.,4,"""a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations"" and ""these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles),",2024,2024-03-04T17:57:18Z,"Keyphrase: ""Factual inconsistency and lack of interpretability"""
arxiv2024,Birbal: An efficient 7B instruct-model fine-tuned with curated datasets,Yes.,4,"""LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility. Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible.""",2024,2024-03-04T17:34:46Z,"Keyphrase: ""High cost and lack of transparency"""
arxiv2024,PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models,Yes.,4,"""Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at."" and ""our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their reasoning abilities in an unexpected manner.""",2024,2024-03-04T17:34:34Z,"Keyphrase: ""Limited socialcognitive reasoning"""
arxiv2024,Cognition is All You Need -- The Next Layer of AI Above Large Language Models,Yes.,5,"""Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving."" and ""The failure of these systems to address complex knowledge work is due to the fact that they",2024,2024-03-04T16:11:57Z,"Keyphrase: ""Limited reasoning and problem-solving capabilities"""
arxiv2024,Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?,Yes.,4,"""inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss.""",2024,2024-03-04T14:01:11Z,"Keyphrase: ""English-centric bias and information loss"""
arxiv2024,Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism,Yes.,4,"""While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.""",2024,2024-03-04T13:57:37Z,"Keyphrase: ""Hallucination propensity"""
arxiv2024,Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?,Yes.,4,"""we analyse the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve and remains limited, with low scores especially for low-resource languages.""",2024,2024-03-04T10:48:13Z,"Keyphrase: ""Limited improvement in low-resource language adaptation"""
arxiv2024,WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations,Yes.,5,"""existing datasets and evaluation methods in this domain still exhibit notable limitations"" and ""highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement.""",2024,2024-03-04T07:06:41Z,"Keyphrase: ""Source attribution challenges"""
arxiv2024,How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems,Yes.,5,"""a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems.""",2024,2024-03-04T06:24:21Z,"Keyphrase: ""Struggle with capturing relationships among decision variables"""
arxiv2024,SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction,Yes.,4,"""LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge.""",2024,2024-03-03T17:35:52Z,"Keyphrase: ""Deficiency in vertical knowledge"""
arxiv2024,In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation,Yes.,5,"""Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited.""",2024,2024-03-03T15:53:41Z,"Keyphrase: ""Frequent hallucinations and factual errors"""
arxiv2024,Ever-Evolving Memory by Blending and Refining the Past,Yes.,5,"""current large language models often lack this capability, leading to instances of missing important user information or redundantly asking for the same information, thereby diminishing conversation quality.""",2024,2024-03-03T08:12:59Z,"Keyphrase: ""Lack of conversational capability"""
arxiv2024,Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge,Yes.,5,"""However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications.""",2024,2024-03-03T08:07:55Z,"Keyphrase: ""Struggles with low-frequency concepts"""
arxiv2024,CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge,Yes.,5,"""existing KGQA datasets focus on popular entities for which large language models (LLMs) can directly answer without hallucinating and without leveraging the KG"" and ""baseline evaluation of LLMs on CR-LT KGQA demonstrate a high rate of",2024,2024-03-03T04:47:01Z,"Keyphrase: ""Limited knowledge grounding"""
arxiv2024,Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering,Yes.,4,"""existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable.""",2024,2024-03-03T04:22:13Z,"Keyphrase: ""Hallucination in KGQA"""
arxiv2024,Analysis of Privacy Leakage in Federated Large Language Models,Yes.,5,"""revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets.""",2024,2024-03-02T20:25:38Z,"Keyphrase: ""Privacy vulnerabilities"""
arxiv2024,Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal,Yes.,5,"""Large language models (LLMs) suffer from catastrophic forgetting during continual learning.""",2024,2024-03-02T16:11:23Z,"Keyphrase: ""Catastrophic forgetting"""
arxiv2024,RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,Yes.,5,"""However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge."" and ""These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications.""",2024,2024-03-02T12:19:04Z,"Keyphrase: ""Generation of false information"""
arxiv2024,"A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",Yes.,4,"""While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale.""",2024,2024-03-02T09:39:13Z,"Keyphrase: ""Risk of misinformation proliferation"""
arxiv2024,Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers,Yes.,5,"""We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext.""",2024,2024-03-02T01:52:14Z,"Keyphrase: ""Faithfulness mistakes"""
arxiv2024,Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries,Yes.,4,"""Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health.""",2024,2024-03-01T21:59:03Z,"Keyphrase: ""Lack of grounding in safety-critical domains"""
arxiv2024,Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods,Yes.,5,"""we find that LLM predictions are not robust under variation of method choice, both within a single LLM and across different LLMs.""",2024,2024-03-01T21:48:08Z,"Keyphrase: ""Lack of robustness in predictions"""
arxiv2024,MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection,Yes.,5,"""contemporary Large Language Models (LLMs) face several challenges, such as generating fluent yet inaccurate outputs and reliance on fluency-centric metrics. This often leads to neural networks exhibiting 'hallucinations'.""",2024,2024-03-01T20:31:10Z,"Keyphrase: ""Fluent yet inaccurate output"""
arxiv2024,Mitigating Reversal Curse in Large Language Models via Semantic-aware Permutation Training,Yes.,5,"""recent studies showcase that causal LLMs suffer from the 'reversal curse'. It is a typical example that the model knows 'A's father is B', but is unable to reason 'B's child is A'. This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional",2024,2024-03-01T18:55:20Z,"Keyphrase: ""Limited causal reasoning"""
arxiv2024,Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents,Yes.,5,"""However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions."" and ""While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack's mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback.""",2024,2024-03-01T17:22:16Z,"Keyphrase: ""Limited adaptability to dynamic environments"""
arxiv2024,DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models,Yes.,4,"""Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge,"" and ""many merely focus on the factuality hallucination while ignoring the faithfulness hallucination.""",2024,2024-03-01T15:38:55Z,"Keyphrase: ""Hallucination issue"""
arxiv2024,TempCompass: Do Video LLMs Really Understand Videos?,Yes.,5,"""reveal the discerning fact that these models exhibit notably poor temporal perception ability.""",2024,2024-03-01T12:02:19Z,"Keyphrase: ""Poor temporal perception"""
arxiv2024,Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs,Yes.,5,"""the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks.""",2024,2024-03-01T09:28:38Z,"Keyphrase: ""Contaminated benchmark data"""
arxiv2024,Invariant Test-Time Adaptation for Vision-Language Model Generalization,Yes.,4,"""However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of 'decision shortcuts' that hinders their generalization capabilities."" and ""the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in",2024,2024-03-01T09:01:53Z,"Keyphrase: ""Limited generalization in longtail tasks"""
arxiv2024,Teach LLMs to Phish: Stealing Private Information from Language Models,Yes.,5,"""When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information.""",2024,2024-03-01T06:15:07Z,"Keyphrase: ""Privacy risks and sensitive information memorization"""
arxiv2024,DPP-Based Adversarial Prompt Searching for Lanugage Models,Yes.,4,"""Language models risk generating mindless and offensive content, which hinders their safe deployment."" and ""Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content.""",2024,2024-03-01T05:28:06Z,"Keyphrase: ""Generation of offensive content"""
arxiv2024,Gender Bias in Large Language Models across Multiple Languages,Yes.,5,"""assessing the influence of gender biases embedded in LLMs becomes crucial"" and ""Our findings revealed significant gender biases across all the languages we examined.""",2024,2024-03-01T04:47:16Z,"Keyphrase: ""Gender bias"""
arxiv2024,Extracting Polymer Nanocomposite Samples from Full-Length Documents,Yes.,5,"""Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.""",2024,2024-03-01T03:51:56Z,"Keyphrase: ""Challenges in information extraction"""
arxiv2024,Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes,Yes.,4,"""recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails.""",2024,2024-03-01T03:29:54Z,"Keyphrase: ""Vulnerability to adversarial jailbreak attempts"""
arxiv2024,Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,Yes.,5,"""However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains."" and ""Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.""",2024,2024-03-01T02:21:30Z,"Keyphrase: ""Limited interpretive abilities in scientific contexts"""
arxiv2024,AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs,Yes.,4,"""Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications.""",2024,2024-03-01T00:02:37Z,"Keyphrase: ""Bias from training data"""
arxiv2024,FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition,Yes.,5,"""such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities"" and ""we identify a common shortfall in knowledge utilization among models.""",2024,2024-02-29T21:05:37Z,"Keyphrase: ""Limited cognitive skill differentiation"""
arxiv2024,NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications,Yes.,4,"""highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks.""",2024,2024-02-29T21:05:14Z,"Keyphrase: ""Ethical and creative deficiencies"""
arxiv2024,Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs,Yes.,5,"""Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates.""",2024,2024-02-29T19:55:06Z,"Keyphrase: ""Memory limitations"""
arxiv2024,PROC2PDDL: Open-Domain Planning Representations from Texts,Yes.,5,"""We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific programs and reasoning about events.""",2024,2024-02-29T19:40:25Z,"Keyphrase: ""Domain-specific program generation deficiency"""
arxiv2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,Yes.,5,"""This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences.""",2024,2024-02-29T19:02:03Z,"Keyphrase: ""Difficulty with out-of-distribution tokens"""
arxiv2024,Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization,Yes.,4,"""However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases."" and ""almost all of them (except GPT-4), even after fine-tuning, could not properly generate the response in the required output format.""",2024,2024-02-29T19:00:47Z,"Keyphrase: ""High inference costs"""
arxiv2024,The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?,Yes.,5,"""we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit",2024,2024-02-29T18:59:25Z,"Keyphrase: ""Limited reasoning and correction capabilities"""
arxiv2024,Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models,Yes.,4,"""Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness."" and ""we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions",2024,2024-02-29T18:55:06Z,"Keyphrase: ""Trustworthiness concerns"""
arxiv2024,Curiosity-driven Red-teaming for Large Language Models,Yes.,5,"""Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content.""",2024,2024-02-29T18:55:03Z,"Keyphrase: ""Risk of generating toxic content"""
arxiv2024,On the Scaling Laws of Geographical Representation in Language Models,Yes.,4,"""we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.""",2024,2024-02-29T18:04:11Z,"Keyphrase: ""Geographical bias persistence"""
arxiv2024,Entity-Aware Multimodal Alignment Framework for News Image Captioning,Yes.,4,"""Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-t",2024,2024-02-29T18:03:00Z,"Keyphrase: ""Limited entity information handling"""
arxiv2024,Watermark Stealing in Large Language Models,Yes.,5,"""We dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes."" and ""Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes.""",2024,2024-02-29T17:12:39Z,"Keyphrase: ""Vulnerability to watermarking"""
arxiv2024,SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation,Yes.,4,"""Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training samples available in practice lead to poor code generation performance.""",2024,2024-02-29T16:09:02Z,"Keyphrase: ""Limited adaptation to specific scenarios"""
arxiv2024,GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers,Yes.,5,"""One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly."" and ""Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust.""",2024,2024-02-29T15:26:14Z,"Keyphrase: ""Limited math reasoning ability"""
arxiv2024,Memory-Augmented Generative Adversarial Transformers,Yes.,5,"""Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy.""",2024,2024-02-29T14:47:24Z,"Keyphrase: ""Limited ability to integrate external data"""
arxiv2024,Teaching Large Language Models an Unseen Language on the Fly,Yes.,5,"""Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating.""",2024,2024-02-29T13:50:47Z,"Keyphrase: ""Struggle with low-resource languages"""
arxiv2024,Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model,Yes.,5,"""However, the Typographic Attack, which disrupts vision-language models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), has also been expected to be a security threat to LVLMs."" and ""Based on the evaluation results, we investigate the causes why typographic attacks may impact VLMs and LVLMs, leading to three highly insightful",2024,2024-02-29T13:31:56Z,"Keyphrase: ""Vulnerability to typographic attacks"""
arxiv2024,Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models,Yes.,5,"""Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted",2024,2024-02-29T12:35:45Z,"Keyphrase: ""Hallucination of text"""
arxiv2024,Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials,Yes.,5,"""investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples"" and ""the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent",2024,2024-02-29T12:01:46Z,"Keyphrase: ""Reliance on shortcut features"""
arxiv2024,Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning,Yes.,5,"""However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem.""",2024,2024-02-29T05:27:45Z,"Keyphrase: ""Catastrophic forgetting"""
arxiv2024,ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph,Yes.,5,"""large language models (LLMs) remain significantly limited in properly using massive external tools"" and ""Such a paradigm ignores the intrinsic dependency between tools and offloads all reasoning loads to LLMs, making them restricted to a limited number of specifically designed tools.""",2024,2024-02-29T02:04:00Z,"Keyphrase: ""Dependence on external tools"""
arxiv2024,Learning to Compress Prompt in Natural Language Formats,Yes.,5,"""Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results.""",2024,2024-02-28T20:41:21Z,"Keyphrase: ""Inferior long context processing"""
arxiv2024,FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability,Yes.,5,"""Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately.""",2024,2024-02-28T19:23:27Z,"Keyphrase: ""Benchmark performance inadequacy"""
arxiv2024,A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems,Yes.,4,"""there are also increasing concerns over the security of such probabilistic intelligent systems"" and ""Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components.""",2024,2024-02-28T19:00:12Z,"Keyphrase: ""Security vulnerabilities"""
arxiv2024,Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates,Yes.,4,"""even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models.""",2024,2024-02-28T18:23:49Z,"Keyphrase: ""Unsafe behavior after finetuning"""
arxiv2024,Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,Yes.,5,"""we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem."" and ""we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers.""",2024,2024-02-28T14:09:02Z,"Keyphrase: ""Information loss and shallow attention"""
arxiv2024,Learning or Self-aligning? Rethinking Instruction Fine-tuning,Yes.,5,"""attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects.""",2024,2024-02-28T11:16:00Z,"Keyphrase: ""Limited impact of world knowledge"""
arxiv2024,LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History,Yes.,5,"""we find that performance can in fact also be negatively impacted, if there is a task-switch"" and ""many of the task-switches can lead to significant performance degradation.""",2024,2024-02-28T10:19:05Z,"Keyphrase: ""Performance degradation due to task-switching"""
arxiv2024,Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,Yes.,5,"""studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it.""",2024,2024-02-28T08:24:38Z,"Keyphrase: ""Challenges in using retrieved information"""
arxiv2024,"Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",Yes.,4,"""cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts.""",2024,2024-02-28T07:18:39Z,"Keyphrase: ""Crosslingual inconsistency"""
arxiv2024,Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction,Yes.,5,"""One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs.""",2024,2024-02-28T06:50:14Z,"Keyphrase: ""Vulnerability to adversarial prompts"""
arxiv2024,No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization,Yes.,5,"""the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself.""",2024,2024-02-28T06:34:54Z,"Keyphrase: ""Memory footprint bottleneck"""
arxiv2024,Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions,Yes.,4,"""However, medical board exam questions or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions.""",2024,2024-02-28T05:44:41Z,"Keyphrase: ""Limited clinical complexity capture"""
arxiv2024,MEGAnno+: A Human-LLM Collaborative Annotation System,Yes.,5,"""Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations.""",2024,2024-02-28T04:58:07Z,"Keyphrase: ""Difficulty with complex sociocultural context"""
arxiv2024,Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore,Yes.,4,"""Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge,"" and ""multilingual models demonstrate a bias towards factual information from Western continents.""",2024,2024-02-28T04:43:46Z,"Keyphrase: ""Factual hallucination and bias"""
arxiv2024,Automated Discovery of Integral with Deep Learning,Yes.,4,"""Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do.""",2024,2024-02-28T04:34:15Z,"Keyphrase: ""Limited to existing human knowledge"""
arxiv2024,Corpus-Steered Query Expansion with Large Language Models,Yes.,5,"""challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs.""",2024,2024-02-28T03:58:58Z,"Keyphrase: ""Misalignment in retrieval corpus"""
arxiv2024,TroubleLLM: Align to Red Team Expert,Yes.,4,"""However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content.""",2024,2024-02-28T03:40:46Z,"Keyphrase: ""Undesirable safety issues"""
arxiv2024,FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization,Yes.,5,"""However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance.""",2024,2024-02-28T02:00:34Z,"Keyphrase: ""Latency and memory consumption restrictions"""
arxiv2024,Collaborative decoding of critical tokens for boosting factuality of large language models,Yes.,5,"""their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination.""",2024,2024-02-28T01:53:37Z,"Keyphrase: ""Increased risk of hallucination"""
arxiv2024,Gradient-Free Adaptive Global Pruning for Pre-trained Language Models,Yes.,5,"""The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands.""",2024,2024-02-28T00:09:07Z,"Keyphrase: ""Prohibitive computational demand"""
arxiv2024,LLM-Resistant Math Word Problem Generation via Adversarial Attacks,Yes.,5,"""We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure, offering a nuanced view into model's limitation.""",2024,2024-02-27T22:07:52Z,"Keyphrase: ""Vulnerability to attacks"""
arxiv2024,BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra,Yes.,5,"""due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting.""",2024,2024-02-27T20:48:24Z,"Keyphrase: ""Context size limitation"""
arxiv2024,Evaluating Very Long-Term Conversational Memory of LLM Agents,Yes.,5,"""Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.""",2024,2024-02-27T18:42:31Z,"Keyphrase: ""Difficulty in long-range conversation comprehension"""
arxiv2024,AmbigNLG: Addressing Task Ambiguity in Instruction for NLG,Yes.,5,"""their performance is significantly hindered by the ambiguity present in real-world instructions.""",2024,2024-02-27T17:52:33Z,"Keyphrase: ""Ambiguity in real-world instructions"""
arxiv2024,Case-Based or Rule-Based: How Do Transformers Do the Math?,Yes.,5,"""modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition.""",2024,2024-02-27T17:41:58Z,"Keyphrase: ""Struggles with math problems"""
arxiv2024,NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents,Yes.,5,"""they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism.""",2024,2024-02-27T16:56:30Z,"Keyphrase: ""Struggles with processing long sequences"""
arxiv2024,Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data,Yes.,5,"""Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously.""",2024,2024-02-27T16:15:03Z,"Keyphrase: ""Limited causal reasoning"""
arxiv2024,OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web,Yes.,5,"""The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents.""",2024,2024-02-27T14:47:53Z,"Keyphrase: ""Limited task proficiency"""
arxiv2024,TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge.""",2024,2024-02-27T14:45:04Z,"Keyphrase: ""Hallucination and untruthful responses"""
arxiv2024,Predict the Next Word: Humans exhibit uncertainty in this task and language models _____,Yes.,5,"""We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty.""",2024,2024-02-27T14:11:32Z,"Keyphrase: ""Low calibration of human uncertainty"""
arxiv2024,REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering,Yes.,5,"""LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents).""",2024,2024-02-27T13:22:51Z,"Keyphrase: ""Limited relevance assessment"""
arxiv2024,Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles,Yes.,5,"""Results showed that GPT-4's performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4",2024,2024-02-27T13:02:19Z,"Keyphrase: ""Limited performance in detecting propaganda techniques"""
arxiv2024,Training-Free Long-Context Scaling of Large Language Models,Yes.,5,"""The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length.""",2024,2024-02-27T12:39:23Z,"Keyphrase: ""Limited coherence with lengthy input"""
arxiv2024,Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective,Yes.,5,"""there is still a lack of research on LLM consistency, meaning that throughout the various stages of LLM research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive",2024,2024-02-27T11:02:12Z,"Keyphrase: ""Consistency and adaptability challenges"""
arxiv2024,LLMGuard: Guarding Against Unsafe LLM Behavior,Yes.,5,"""it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns.""",2024,2024-02-27T10:22:45Z,"Keyphrase: ""Generating inappropriate biased content"""
arxiv2024,SoFA: Shielded On-the-fly Alignment via Priority Rule Following,Yes.,5,"""even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules.""",2024,2024-02-27T09:52:27Z,"Keyphrase: ""Poor rule understanding and prioritization"""
arxiv2024,Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese,Yes.,4,"""Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages.""",2024,2024-02-27T08:24:32Z,"Keyphrase: ""Knowledge adequacy disparity"""
arxiv2024,Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue,Yes.,5,"""Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs.""",2024,2024-02-27T07:11:59Z,"Keyphrase: ""Inadequate safety mechanisms"""
arxiv2024,Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection,Yes.,5,"""We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD,",2024,2024-02-27T07:02:10Z,"Keyphrase: ""Limited zero-shot/few-shot capabilities"""
arxiv2024,Measuring Vision-Language STEM Skills of Neural Models,Yes.,5,"""Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance.""",2024,2024-02-27T04:55:03Z,"Keyphrase: ""Limited grade-level skill mastery"""
arxiv2024,Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses,Yes.,5,"""Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios.""",2024,2024-02-27T00:22:18Z,"Keyphrase: ""Hallucination challenge"""
arxiv2024,Pandora's White-Box: Increased Training Data Leakage in Open LLMs,Yes.,5,"""these findings show that highly effective MIAs are available in almost all LLM training settings, and highlight that great care must be taken before LLMs are fine-tuned on highly sensitive data and then deployed.""",2024,2024-02-26T20:41:50Z,"Keyphrase: ""Sensitive data risks"""
arxiv2024,Algorithmic Arbitrariness in Content Moderation,Yes.,4,"""We analyze (i) the extent of predictive multiplicity among state-of-the-art LLMs used for detecting toxic content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) how model multiplicity compares to unambiguous human classifications.""",2024,2024-02-26T19:27:00Z,"Keyphrase: ""Predictive multiplicity and disparate impact"""
arxiv2024,A Survey of Large Language Models in Cybersecurity,Yes.,5,"""This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome.""",2024,2024-02-26T19:06:02Z,"Keyphrase: ""Limited application in cybersecurity"""
arxiv2024,"Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding",Yes.,5,"""However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications.""",2024,2024-02-26T18:59:28Z,"Keyphrase: ""High deployment cost and latency issues"""
arxiv2024,MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT,Yes.,5,"""However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency.""",2024,2024-02-26T18:59:03Z,"Keyphrase: ""Limited suitability for on-device processing"""
arxiv2024,Eight Methods to Evaluate Robust Unlearning in LLMs,Yes.,4,"""we first survey techniques and limitations of existing unlearning evaluations"" and ""Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.""",2024,2024-02-26T18:57:37Z,"Keyphrase: ""Limited unlearning evaluation"""
arxiv2024,A Surprising Failure? Multimodal LLMs and the NLVR Challenge,Yes.,5,"""Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.""",2024,2024-02-26T18:37:18Z,"Keyphrase: ""Poor compositional spatial reasoning"""
arxiv2024,Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models,Yes.,5,"""most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness.""",2024,2024-02-26T18:00:49Z,"Keyphrase: ""Lack of paraphrase robustness"""
arxiv2024,A Comprehensive Evaluation of Quantization Strategies for Large Language Models,Yes.,4,"""Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings."" and ""Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs.""",2024,2024-02-26T17:45:36Z,"Keyphrase: ""Resource-intensive deployment"""
arxiv2024,Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study,Yes.,5,"""they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes.""",2024,2024-02-26T16:05:33Z,"Keyphrase: ""Input sequence length constraint"""
arxiv2024,StructLM: Towards Building Generalist Models for Structured Knowledge Grounding,Yes.,4,"""Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%.""",2024,2024-02-26T15:47:01Z,"Keyphrase: ""Limited ability with structured data"""
arxiv2024,Long-Context Language Modeling with Parallel Context Encoding,Yes.,5,"""the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window"" and ""while existing long-context models degenerate with retrieved contexts.""",2024,2024-02-26T14:47:35Z,"Keyphrase: ""Limited contextual window"""
arxiv2024,LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments,Yes.,4,"""showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration.""",2024,2024-02-26T11:31:48Z,"Keyphrase: ""Limited autonomy and opponent modeling"""
arxiv2024,Defending LLMs against Jailbreaking Attacks via Backtranslation,Yes.,4,"""Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent.""",2024,2024-02-26T10:03:33Z,"Keyphrase: ""Vulnerability to malicious intent"""
arxiv2024,"ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",Yes.,4,"""The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner.""",2024,2024-02-26T09:43:02Z,"Keyphrase: ""Limited safety detection"""
arxiv2024,From RAGs to riches: Using large language models to write documents for clinical trials,Yes.,5,"""however there are concerns about the quality of their output"" and ""deficiencies remain",2024,2024-02-26T08:59:05Z,"Keyphrase: ""Output quality deficiency"""
arxiv2024,Improving LLM-based Machine Translation with Systematic Self-Correction,Yes.,4,"""However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors.""",2024,2024-02-26T07:58:12Z,"Keyphrase: ""Translation errors"""
arxiv2024,Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models,Yes.,5,"""our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings"" and ""instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs.""",2024,2024-02-26T05:43:51Z,"Keyphrase: ""Modality gap in visual categorization"""
arxiv2024,HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs,Yes.,5,"""Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs.""",2024,2024-02-25T22:23:37Z,"Keyphrase: ""Reliability and alignment challenges"""
arxiv2024,DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers,Yes.,5,"""The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks,"" and ""current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned L",2024,2024-02-25T17:43:29Z,"Keyphrase: ""Vulnerability to jailbreak attacks"""
arxiv2024,Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions,Yes.,4,"""generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate"" and ""retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval.""",2024,2024-02-25T11:22:19Z,"Keyphrase: ""Susceptibility to factual errors"""
arxiv2024,Likelihood-based Mitigation of Evaluation Bias in Large Language Models,Yes.,4,"""However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation",2024,2024-02-25T04:52:02Z,"Keyphrase: ""Sentence structure bias"""
arxiv2024,Cognitive Bias in High-Stakes Decision-Making with LLMs,Yes.,4,"""LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance.""",2024,2024-02-25T02:35:56Z,"Keyphrase: ""Biases in decision-making"""
arxiv2024,Rethinking Software Engineering in the Foundation Model Era: A Curated Catalogue of Challenges in the Development of Trustworthy FMware,Yes.,5,"""The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges.""",2024,2024-02-25T00:53:16Z,"Keyphrase: ""Orchestration challenges and hallucination"""
arxiv2024,Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models,Yes.,5,"""Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination."" and ""detecting and mitigating data contamination for LLMs faces significant challenges.""",2024,2024-02-24T23:54:41Z,"Keyphrase: ""Susceptibility to data contamination"""
arxiv2024,Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency,Yes.,4,"""this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency"" and ""Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures.""",2024,2024-02-24T23:17:56Z,"Keyphrase: ""Decreased recall in language proficiency"""
arxiv2024,Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis,Yes.,5,"""The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm.""",2024,2024-02-24T21:36:26Z,"Keyphrase: ""Limited long-term quantitative reasoning"""
arxiv2024,PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails,Yes.,5,"""Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content."" and ""Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective.""",2024,2024-02-24T21:27:13Z,"Keyphrase: ""Vulnerability to automated attacks"""
arxiv2024,Prompt Perturbation Consistency Learning for Robust Language Models,Yes.,5,"""their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts.""",2024,2024-02-24T15:00:58Z,"Keyphrase: ""Poor performance on sequence labeling tasks"""
arxiv2024,Empowering Large Language Model Agents through Action Learning,Yes.,5,"""Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior.""",2024,2024-02-24T13:13:04Z,"Keyphrase: ""Limited trial and error learning"""
arxiv2024,"From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models",Yes.,4,"""Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks."" and ""our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with LLMs.""",2024,2024-02-24T09:06:25Z,"Keyphrase: ""Inadequate risk oversight"""
arxiv2024,Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models,Yes.,4,"""Large language models (LLMs) still grapple with complex tasks like mathematical reasoning.""",2024,2024-02-24T08:40:30Z,"Keyphrase: ""Struggles with mathematical reasoning"""
arxiv2024,Stepwise Self-Consistent Mathematical Reasoning with Large Language Models,Yes.,4,"""Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions.""",2024,2024-02-24T08:22:39Z,"Keyphrase: ""Limited multistep reasoning"""
arxiv2024,HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition,Yes.,4,"""the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria.""",2024,2024-02-24T08:01:32Z,"Keyphrase: ""Limited evaluation scope"""
