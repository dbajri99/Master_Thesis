Source ,Title,Talks about LLMs,Rate,Evidence,Year ,Date ,Keyphrase
arxiv2024,HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition,Yes.,4,"""the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria.""",2024,2024-02-24T08:01:32Z,"Keyphrase: ""Limited evaluation scope and potential bias"""
arxiv2024,Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology,Yes.,5,"""Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks.""",2024,2024-02-24T02:27:55Z,"Keyphrase: ""Vulnerability to jailbreaking attacks"""
arxiv2024,Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study,Yes.,4,"""The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels",2024,2024-02-24T00:38:29Z,"Keyphrase: ""Performance limitations with data augmentation"""
arxiv2024,Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics,Yes.,5,"""demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task.""",2024,2024-02-24T00:01:01Z,"Keyphrase: ""Limited world knowledge integration"""
arxiv2024,DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures,Yes.,4,"""Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web",2024,2024-02-23T20:10:18Z,"Keyphrase: ""Limited web-based representation"""
arxiv2024,CI w/o TN: Context Injection without Task Name for Procedure Planning,Yes.,5,"""we propose a much weaker setting without task name as supervision, which is not currently solvable by existing large language models since they require good prompts with sufficient information.""",2024,2024-02-23T19:34:47Z,"Keyphrase: ""Dependency on task-specific supervision"""
arxiv2024,The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG),Yes.,4,"""Whereas extensive research has demonstrated the privacy risks of large language models (LLMs),"" and ""posing new privacy issues that are currently under-explored.""",2024,2024-02-23T18:35:15Z,"Keyphrase: ""Privacy risks"""
arxiv2024,Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models,Yes.,4,"""The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability."" and ""we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently",2024,2024-02-23T18:15:56Z,"Keyphrase: ""Discrimination and bias amplification"""
arxiv2024,Explorations of Self-Repair in Language Models,Yes.,4,"""We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect).""",2024,2024-02-23T15:42:12Z,"Keyphrase: ""Inconsistent self-repair mechanisms"""
arxiv2024,How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries,Yes.,5,"""Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation.""",2024,2024-02-23T13:03:12Z,"Keyphrase: ""Vulnerability to unethical content generation"""
arxiv2024,Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models,Yes.,4,"""these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility.""",2024,2024-02-23T09:04:48Z,"Keyphrase: ""Production of toxic content"""
arxiv2024,A First Look at GPT Apps: Landscape and Vulnerability,Yes.,4,"""LLMs' susceptibility to attacks raises concerns over safety and plagiarism.""",2024,2024-02-23T05:30:32Z,"Keyphrase: ""Susceptibility to attacks"""
arxiv2024,Studying LLM Performance on Closed- and Open-source Data,Yes.,5,"""These models are extremely data-hungry,"" ""do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds?"" and ""We find that performance for C# changes little from",2024,2024-02-23T05:17:28Z,"Keyphrase: ""High data dependency"""
arxiv2024,AttributionBench: How Hard is Automatic Attribution Evaluation?,Yes.,5,"""our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model's inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do.""",2024,2024-02-23T04:23:33Z,"Keyphrase: ""Limited nuanced information processing"""
arxiv2024,Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions,Yes.,4,"""they often display a considerable level of overconfidence even when the question does not have a definitive answer"" and ""avoid providing hallucinated answers to these unknown questions.""",2024,2024-02-23T02:24:36Z,"Keyphrase: ""Overconfidence and Hallucination"""
arxiv2024,Fine-tuning Large Language Models for Domain-specific Machine Translation,Yes.,5,"""Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain",2024,2024-02-23T02:24:15Z,"Keyphrase: ""Incontext learning challenges and domain finetuning"""
arxiv2024,ToMBench: Benchmarking Theory of Mind in Large Language Models,Yes.,5,"""We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet.""",2024,2024-02-23T02:05:46Z,"Keyphrase: ""Lag behind human performance"""
arxiv2024,KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models,Yes.,5,"""Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness."" and ""We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.""",2024,2024-02-23T01:30:39Z,"Keyphrase: ""Data contamination hindrance"""
arxiv2024,Unintended Impacts of LLM Alignment on Global Representation,Yes.,4,"""We explore how alignment impacts performance along three axes of global representation",2024,2024-02-22T23:31:22Z,"Keyphrase: ""Limited global representation"""
arxiv2024,Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning,Yes.,4,"""recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback.""",2024,2024-02-22T20:57:17Z,"Keyphrase: ""Limited knowledge retention"""
arxiv2024,RelayAttention for Efficient Large Language Model Serving with Long System Prompts,Yes.,5,"""However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length.""",2024,2024-02-22T18:58:28Z,"Keyphrase: ""Throughput and latency bottleneck"""
arxiv2024,Identifying Multiple Personalities in Large Language Models with External Evaluation,Yes.,4,"""Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs."" and ""This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans.""",2024,2024-02-22T18:57:20Z,"Keyphrase: ""Inconsistent personality representation"""
arxiv2024,MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues,Yes.,4,"""comprehensively evaluating the dialogue abilities of LLMs remains a challenge"" and ""neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs.""",2024,2024-02-22T18:21:59Z,"Keyphrase: ""Challenges in dialogue evaluation"""
arxiv2024,Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images,Yes.,5,"""they are still vulnerable to adversarial images"" and ""lack of study regarding MLLMs' adversarial robustness with CoT"" and ""finding that CoT marginally improves adversarial robustness against existing attack methods"" and ""introduce a novel",2024,2024-02-22T17:36:34Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,Chain-of-Thought Unfaithfulness as Disguised Accuracy,Yes.,5,"""We discover that simply changing the order of answer choices in the prompt can reduce the metric by 73 percentage points. The faithfulness metric is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about its validity as a construct for evaluating faithfulness.""",2024,2024-02-22T17:23:53Z,"Keyphrase: ""Sensitivity to prompt manipulation"""
arxiv2024,UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models,Yes.,5,"""Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or hallucination."" and ""Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources",2024,2024-02-22T16:45:32Z,"Keyphrase: ""Factual inaccuracy and hallucination"""
arxiv2024,Visual Hallucinations of Multi-modal Large Language Models,Yes.,5,"""Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances."" and ""We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 halluc",2024,2024-02-22T16:40:33Z,"Keyphrase: ""Limited diversity in training data"""
arxiv2024,ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models,Yes.,5,"""we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones.""",2024,2024-02-22T16:06:49Z,"Keyphrase: ""Performance variation across concepts"""
arxiv2024,"""My Answer is C"": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",Yes.,5,"""Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%.""",2024,2024-02-22T12:47:33Z,"Keyphrase: ""Dimension misalignment"""
arxiv2024,Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis,Yes.,5,"""Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance",2024,2024-02-22T12:19:04Z,"Keyphrase: ""Limited performance without extensive training data"""
arxiv2024,COBIAS: Contextual Reliability in Bias Assessment,Yes.,4,"""Large Language Models (LLMs) are trained on inherently biased data"" and ""highlighting a critical need for contextual exploration.""",2024,2024-02-22T10:46:11Z,"Keyphrase: ""Inherent bias from training data"""
arxiv2024,Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning,Yes.,5,"""However, the existing LLM-based model exhibits three shortcomings",2024,2024-02-22T08:51:39Z,"Keyphrase: ""Multiple shortcomings"""
arxiv2024,Understanding and Patching Compositional Reasoning in LLMs,Yes.,5,"""LLMs have marked a revolutionary shift, yet they falter when faced with compositional reasoning tasks.""",2024,2024-02-22T06:47:56Z,"Keyphrase: ""Falter in compositional reasoning"""
arxiv2024,Mitigating Biases of Large Language Models in Stance Detection with Calibration,Yes.,4,"""LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance.""",2024,2024-02-22T05:17:49Z,"Keyphrase: ""Biased stance due to sentiment-topic correlation"""
arxiv2024,Can Language Models Act as Knowledge Bases at Scale?,Yes.,5,"""However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable.""",2024,2024-02-22T04:20:14Z,"Keyphrase: ""Limited factual coverage"""
arxiv2024,Qsnail: A Questionnaire Dataset for Sequential Question Generation,Yes.,5,"""Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires.""",2024,2024-02-22T04:14:10Z,"Keyphrase: ""Lack of diversity and specificity"""
arxiv2024,Eagle: Ethical Dataset Given from Real Interactions,Yes.,5,"""Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content.""",2024,2024-02-22T03:46:02Z,"Keyphrase: ""Ethical and social bias"""
arxiv2024,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Yes.,5,"""state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome.""",2024,2024-02-22T01:02:37Z,"Keyphrase: ""Limited conversational prediction"""
arxiv2024,Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models,Yes.,5,"""existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.""",2024,2024-02-22T00:41:23Z,"Keyphrase: ""Challenges in integrating structured data"""
arxiv2024,Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization,Yes.,5,"""Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability."" and ""Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general",2024,2024-02-22T00:01:02Z,"Keyphrase: ""Lack of explainability"""
arxiv2024,MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms,Yes.,4,"""MLLMs have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation."" and ""Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks.""",2024,2024-02-21T22:27:40Z,"Keyphrase: ""Difficulty handling human emotion and complex content"""
arxiv2024,Coercing LLMs to do and reveal (almost) anything,Yes.,5,"""adversarial attacks on large language models (LLMs) can 'jailbreak' the model into making harmful statements"" and ""we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.""",2024,2024-02-21T18:59:13Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment,Yes.,5,"""no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs,"" ""both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks,"" and ""Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before",2024,2024-02-21T18:55:20Z,"Keyphrase: ""Vulnerability to manipulation"""
arxiv2024,OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,Yes.,5,"""Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies.""",2024,2024-02-21T18:49:26Z,"Keyphrase: ""Hallucination and knowledge omission"""
arxiv2024,Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models,Yes.,5,"""Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.""",2024,2024-02-21T18:48:38Z,"Keyphrase: ""Inconsistent text watermarking"""
arxiv2024,What's in a Name? Auditing Large Language Models for Race and Gender Bias,Yes.,5,"""We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4."" and ""We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women."" and ""Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for harm against marginalized communities.""",2024,2024-02-21T18:25:25Z,"Keyphrase: ""Bias in LLMs"""
arxiv2024,Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content,Yes.,5,"""The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs.""",2024,2024-02-21T16:46:36Z,"Keyphrase: ""Generating toxic content"""
arxiv2024,SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization,Yes.,4,"""Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences.""",2024,2024-02-21T16:33:22Z,"Keyphrase: ""Factual inaccuracy"""
arxiv2024,Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models,Yes.,5,"""This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations."" and ""We illustrate that these probability-based approaches do not effectively correspond with generative predictions.""",2024,2024-02-21T15:58:37Z,"Keyphrase: ""Limitation in probability-based evaluation"""
arxiv2024,Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs,Yes.,5,"""Large Language Models (LLMs)... are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs."" and ""existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds.""",2024,2024-02-21T15:13:50Z,"Keyphrase: ""Vulnerability to jailbreak attacks"""
arxiv2024,Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language,Yes.,4,"""Our findings reveal that while these models demonstrate potential, achieving a 70% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups.""",2024,2024-02-21T13:57:36Z,"Keyphrase: ""Bias in hate speech classification"""
arxiv2024,Factual Consistency Evaluation of Summarisation in the Era of Large Language Models,Yes.,4,"""Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind."" and ""Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.""",2024,2024-02-21T12:35:19Z,"Keyphrase: ""Inability to capture factual inconsistency"""
arxiv2024,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,Yes.,5,"""due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens.""",2024,2024-02-21T12:30:33Z,"Keyphrase: ""Limited context window"""
arxiv2024,$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens,Yes.,5,"""The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context.""",2024,2024-02-21T11:30:29Z,"Keyphrase: ""Limitation in processing long contexts"""
arxiv2024,SaGE: Evaluating Moral Consistency in Large Language Models,Yes.,5,"""we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general)."" and ""Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.""",2024,2024-02-21T11:23:21Z,"Keyphrase: ""Morally inconsistent generation"""
arxiv2024,CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models,Yes.,5,"""Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images.""",2024,2024-02-21T08:21:12Z,"Keyphrase: ""Struggle with contextual information"""
arxiv2024,A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models,Yes.,5,"""The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability.""",2024,2024-02-21T08:20:06Z,"Keyphrase: ""Overconfidence in predictions"""
arxiv2024,Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues,Yes.,5,"""Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous.""",2024,2024-02-21T06:11:03Z,"Keyphrase: ""Poor correlation with human judgment"""
arxiv2024,LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs,Yes.,5,"""this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions.""",2024,2024-02-21T05:56:52Z,"Keyphrase: ""High computational cost and visual token aggregation challenges"""
arxiv2024,FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing,Yes.,5,"""Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and",2024,2024-02-21T05:03:17Z,"Keyphrase: ""High computational demands"""
arxiv2024,RITFIS: Robust input testing framework for LLMs-based intelligent software,Yes.,4,"""existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models.""",2024,2024-02-21T04:00:54Z,"Keyphrase: ""Challenges with lengthy and complex text"""
arxiv2024,Round Trip Translation Defence against Large Language Model Jailbreaking Attacks,Yes.,4,"""Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most.""",2024,2024-02-21T03:59:52Z,"Keyphrase: ""Susceptible to social engineering attacks"""
arxiv2024,From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers,Yes.,5,"""This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text.""",2024,2024-02-21T03:51:34Z,"Keyphrase: ""Repetitive text generation"""
arxiv2024,Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models,Yes.,5,"""While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization."" and ""our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity.""",2024,2024-02-21T03:05:50Z,"Keyphrase: ""Limited factual accuracy in responses"""
arxiv2024,Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks,Yes.,4,"""Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available.""",2024,2024-02-21T02:45:46Z,"Keyphrase: ""Performance degeneration in low-resource settings"""
arxiv2024,RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models,Yes.,5,"""We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses.""",2024,2024-02-21T01:39:56Z,"Keyphrase: ""Stubbornness and forgetfulness"""
arxiv2024,Potential and Challenges of Model Editing for Social Debiasing,Yes.,5,"""Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases."" and ""Our findings in three scenarios reveal both the potential and challenges of debias editing",2024,2024-02-21T01:35:26Z,"Keyphrase: ""Inevitable stereotype bias"""
arxiv2024,Learning to Poison Large Language Models During Instruction Tuning,Yes.,5,"""Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes.""",2024,2024-02-21T01:30:03Z,"Keyphrase: ""Vulnerability to data poisoning attacks"""
arxiv2024,LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study,Yes.,5,"""the phenomenon of 'jailbreaking', where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge.""",2024,2024-02-21T01:26:39Z,"Keyphrase: ""Harmful responses"""
arxiv2024,CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory,Yes.,5,"""Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs.""",2024,2024-02-21T01:00:17Z,"Keyphrase: ""Struggles with long input sequences"""
arxiv2024,Large Language Models for Data Annotation: A Survey,Yes.,4,"""Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation.""",2024,2024-02-21T00:44:04Z,"Keyphrase: ""Challenges with LLM data annotation"""
arxiv2024,The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative,Yes.,5,"""This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs"" and ""Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications.""",2024,2024-02-20T23:08:21Z,"Keyphrase: ""Security risks and covert manipulation"""
arxiv2024,Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text,Yes.,5,"""Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors.""",2024,2024-02-20T22:56:23Z,"Keyphrase: ""Difficulty with complex multistep reasoning"""
arxiv2024,EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries,Yes.,5,"""While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording."" and ""This highlights ongoing model limitations and the value of dynamic datasets in uncovering them.""",2024,2024-02-20T20:53:24Z,"Keyphrase: ""Struggles with commonsense reasoning"""
arxiv2024,A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction,Yes.,5,"""their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE).""",2024,2024-02-20T20:42:02Z,"Keyphrase: ""Inconsistent performance in structured text formats"""
arxiv2024,TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization,Yes.,5,"""Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size."" and ""when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics.""",2024,2024-02-20T18:58:49Z,"Keyphrase: ""Factual errors and hallucinations"""
arxiv2024,Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive,Yes.,4,"""first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples,"" and ""we then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which",2024,2024-02-20T18:42:34Z,"Keyphrase: ""Text reduction and likelihood preference"""
arxiv2024,How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts,Yes.,5,"""The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions.""",2024,2024-02-20T18:31:27Z,"Keyphrase: ""Deceptive hallucinated responses"""
arxiv2024,Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation,Yes.,5,"""despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support"" and ""revealing challenges in selecting the correct strategy and a notable preference for a specific strategy"" and ""LLMs alone cannot become good emotional supporters.""",2024,2024-02-20T18:21:32Z,"Keyphrase: ""Limited emotional support capabilities"""
arxiv2024,Bayesian Reward Models for LLM Alignment,Yes.,4,"""this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data.""",2024,2024-02-20T18:20:59Z,"Keyphrase: ""Vulnerability to reward hacking"""
arxiv2024,Benchmarking Retrieval-Augmented Generation for Medicine,Yes.,5,"""While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge.""",2024,2024-02-20T17:44:06Z,"Keyphrase: ""Hallucination and outdated knowledge"""
arxiv2024,Is the System Message Really Important to Jailbreaks in Large Language Models?,Yes.,4,"""This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions."" and ""we explore the transferability of jailbreak across LLMs.""",2024,2024-02-20T17:39:40Z,"Keyphrase: ""Vulnerability to malicious prompts"""
arxiv2024,CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models,Yes.,5,"""their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories."" and ""highlighting the limitations of LLMs in less familiar language and task contexts",2024,2024-02-20T16:02:12Z,"Keyphrase: ""Limited effectiveness in low-resource languages"""
arxiv2024,ELAD: Explanation-Guided Large Language Models Active Distillation,Yes.,5,"""The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences.""",2024,2024-02-20T15:47:59Z,"Keyphrase: ""Memory inefficiency and high computational costs"""
arxiv2024,Understanding the effects of language-specific class imbalance in multilingual fine-tuning,Yes.,4,"""We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features.""",2024,2024-02-20T13:59:12Z,"Keyphrase: ""Dataset imbalance affecting performance"""
arxiv2024,Large Language Model-based Human-Agent Collaboration for Complex Task Solving,Yes.,5,"""LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs.""",2024,2024-02-20T11:03:36Z,"Keyphrase: ""Difficulty in adapting to dynamic environments"""
arxiv2024,Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting,Yes.,5,"""few-shot learning using Large language models is not production ready for named entity recognition in the clinical domain.""",2024,2024-02-20T08:20:49Z,"Keyphrase: ""Limited few-shot learning capability"""
arxiv2024,An LLM Maturity Model for Reliable and Transparent Text-to-Query,Yes.,4,"""Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications.""",2024,2024-02-20T06:20:09Z,"Keyphrase: ""Reliability and transparency issues"""
arxiv2024,Are Large Language Models Rational Investors?,Yes.,4,"""However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight.""",2024,2024-02-20T04:26:08Z,"Keyphrase: ""Intrinsic bias in financial domain"""
arxiv2024,Thermometer: Towards Universal Calibration for Large Language Models,Yes.,5,"""Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs"" and ""calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks.""",2024,2024-02-20T04:13:48Z,"Keyphrase: ""Poor calibration and computational requirements"""
arxiv2024,The FinBen: An Holistic Financial Benchmark for Large Language Models,Yes.,4,"""The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities.""",2024,2024-02-20T02:16:16Z,"Keyphrase: ""Struggles with complex reasoning and forecasting"""
arxiv2024,Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation,Yes.,4,"""Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs,"" and ""We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.""",2024,2024-02-20T01:49:15Z,"Keyphrase: ""Insufficient bias mitigation"""
arxiv2024,GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence,Yes.,5,"""LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance).""",2024,2024-02-19T21:45:55Z,"Keyphrase: ""Dangerous errors in high-stakes applications"""
arxiv2024,TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness,Yes.,4,"""However, concerns have arisen regarding the trustworthiness of LLMs outputs, particularly in closed-book question-answering tasks, where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information.""",2024,2024-02-19T21:12:14Z,"Keyphrase: ""Trustworthiness of output"""
arxiv2024,AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies,Yes.,5,"""Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack.""",2024,2024-02-19T18:56:44Z,"Keyphrase: ""Struggles with lengthy scenarios"""
arxiv2024,Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge,Yes.,4,"""LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones"" and ""Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets.""",2024,2024-02-19T18:31:11Z,"Keyphrase: ""Bias towards frequently seen data"""
arxiv2024,GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations,Yes.,5,"""We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios.""",2024,2024-02-19T18:23:36Z,"Keyphrase: ""Limited performance in gaming scenarios"""
arxiv2024,Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!,Yes.,5,"""this paper introduces an inference-time attack method, demonstrating that safety alignment can be easily reversed to produce harmful language models without additional training.""",2024,2024-02-19T18:16:51Z,"Keyphrase: ""Vulnerability to inference-time attacks"""
arxiv2024,Large Language Model for Mental Health: A Systematic Review,Yes.,4,"""Findings reveal LLMs' effectiveness in mental health issue detection and the enhancement of telepsychological services through personalised healthcare. Nonetheless, risks like text inconsistencies, hallucinatory content, and the lack of an ethical framework raise concerns about their clinical use.""",2024,2024-02-19T17:58:41Z,"Keyphrase: ""Ethical concerns and hallucinatory content"""
arxiv2024,NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms,Yes.,5,"""The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference"" and ""Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence.""",2024,2024-02-19T16:19:15Z,"Keyphrase: ""Temporal drift in performance"""
arxiv2024,Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition,Yes.,5,"""GPT-4 can generate reasonable coarse-grained citation groupings to support human users in brainstorming, but fails to perform detailed synthesis of related works without human intervention.""",2024,2024-02-19T16:14:04Z,"Keyphrase: ""Limited detailed synthesis"""
arxiv2024,"Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",Yes.,5,"""Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum.""",2024,2024-02-19T16:04:53Z,"Keyphrase: ""Degradation in reasoning performance with shorter inputs"""
arxiv2024,Polarization of Autonomous Generative AI Agents Under Echo Chambers,Yes.,4,"""We investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment."" and ""we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments.""",2024,2024-02-19T15:14:15Z,"Keyphrase: ""Polarization in echo chambers"""
arxiv2024,Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion,Yes.,4,"""However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions.""",2024,2024-02-19T14:59:07Z,"Keyphrase: ""Limited multimodal understanding"""
arxiv2024,A Chinese Dataset for Evaluating the Safeguards in Large Language Models,Yes.,5,"""Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed."" and ""Our experiments on five LLMs show that region-specific risks are the prevalent type of risk, presenting the major issue with all Chinese LLMs we experimented with.""",2024,2024-02-19T14:56:18Z,"Keyphrase: ""Harmful responses and unexpected risks"""
arxiv2024,BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence,Yes.,4,"""However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality.""",2024,2024-02-19T14:28:31Z,"Keyphrase: ""Inconsistent knowledge retrieval"""
arxiv2024,Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One,Yes.,4,"""LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases.""",2024,2024-02-19T14:02:22Z,"Keyphrase: ""Dominant viewpoint bias"""
arxiv2024,Purifying Large Language Models by Ensembling a Small Language Model,Yes.,5,"""well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs.""",2024,2024-02-19T14:00:39Z,"Keyphrase: ""Copyright infringement and privacy violations"""
arxiv2024,Do Large Language Models Understand Logic or Just Mimick Context?,Yes.,5,"""Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can",2024,2024-02-19T12:12:35Z,"Keyphrase: ""Limited understanding of logical rules"""
arxiv2024,Can LLMs Compute with Reasons?,Yes.,5,"""Large language models (LLMs) often struggle with complex mathematical tasks, prone to 'hallucinating' incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data.""",2024,2024-02-19T12:04:25Z,"Keyphrase: ""Struggles with complex mathematical tasks"""
arxiv2024,EmoBench: Evaluating the Emotional Intelligence of Large Language Models,Yes.,4,"""Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research.""",2024,2024-02-19T11:48:09Z,"Keyphrase: ""Gap between LLM and human understanding"""
arxiv2024,Are LLM-based Evaluators Confusing NLG Quality Criteria?,Yes.,5,"""we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability"" and ""Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.""",2024,2024-02-19T11:19:02Z,"Keyphrase: ""Confusion in evaluation criteria"""
arxiv2024,Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models,Yes.,5,"""Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks.""",2024,2024-02-19T11:02:05Z,"Keyphrase: ""Catastrophic forgetting"""
arxiv2024,Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs,Yes.,4,"""Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility."" and ""these methods based on logits often require both teacher and student models to share the same tokenizer",2024,2024-02-19T10:37:29Z,"Keyphrase: ""Practical constraints in deployment"""
arxiv2024,Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought,Yes.,5,"""GPT-4 performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection."" and ""These experimental results indicate that GPT-4 lacks the ability to detect smart contract vulnerabilities effectively.""",2024,2024-02-19T10:33:29Z,"Keyphrase: ""Ineffective vulnerability detection"""
arxiv2024,Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models,Yes.,5,"""Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, TempUN, to reveal significant limitations in temporal retention and reasoning abilities.""",2024,2024-02-19T09:43:03Z,"Keyphrase: ""Limited temporal retention and reasoning"""
arxiv2024,MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition,Yes.,5,"""LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' performance objectively and scientifically; 2) LLMs only get a small percentage of the right reasoning chain, e.g. GPT-4 only gets 36.3",2024,2024-02-19T08:12:30Z,"Keyphrase: ""Limited reasoning capability"""
arxiv2024,Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models,Yes.,5,"""However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored."" and ""Analysis shows that approximately 20% of the failures are attributed to shortcuts.""",2024,2024-02-19T07:34:10Z,"Keyphrase: ""Limited reasoning capabilities"""
arxiv2024,Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint,Yes.,4,"""This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the parametric knowledge internalized during pre-training.""",2024,2024-02-19T07:10:30Z,"Keyphrase: ""Conflicting knowledge sources"""
arxiv2024,RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning,Yes.,4,"""Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities."" and ""The overall performance of existing LMMs is still limited; however LMMs more robust to low",2024,2024-02-19T06:57:02Z,"Keyphrase: ""Limited specialized reasoning"""
arxiv2024,The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth,Yes.,4,"""We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice.""",2024,2024-02-19T06:54:55Z,"Keyphrase: ""Lack of personalization"""
arxiv2024,Microstructures and Accuracy of Graph Recall by Large Language Models,Yes.,5,"""We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.""",2024,2024-02-19T04:29:45Z,"Keyphrase: ""Limited performance on graph-related tasks"""
arxiv2024,Head-wise Shareable Attention for Large Language Models,Yes.,4,"""Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices.""",2024,2024-02-19T04:19:36Z,"Keyphrase: ""Deployment restrictions on edge devices"""
arxiv2024,What Evidence Do Language Models Find Convincing?,Yes.,4,"""Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone.""",2024,2024-02-19T02:15:34Z,"Keyphrase: ""Limited consideration of stylistic features"""
arxiv2024,ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs,Yes.,4,"""Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities.""",2024,2024-02-19T01:28:48Z,"Keyphrase: ""Harmful social biases"""
arxiv2024,"Large Language Models for Stemming: Promises, Pitfalls and Failures",Yes.,5,"""We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions.""",2024,2024-02-19T01:11:44Z,"Keyphrase: ""Limited effectiveness of contextual stemming"""
arxiv2024,MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs,Yes.,5,"""However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments.""",2024,2024-02-19T01:04:22Z,"Keyphrase: ""Inaccurate and misleading output"""
arxiv2024,SPML: A DSL for Defending Language Models Against Prompt Attacks,Yes.,4,"""post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users,"" and ""Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored.""",2024,2024-02-19T00:53:48Z,"Keyphrase: ""Vulnerability to malicious attacks"""
arxiv2024,ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs,Yes.,5,"""We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art.""",2024,2024-02-19T00:43:31Z,"Keyphrase: ""Struggles with visual prompts"""
arxiv2024,Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic,Yes.,5,"""Aligned language models face a significant limitation as their fine-tuning often results in compromised safety.""",2024,2024-02-19T00:18:09Z,"Keyphrase: ""Safety compromises in fine-tuning"""
arxiv2024,How Susceptible are Large Language Models to Ideological Manipulation?,Yes.,5,"""Our findings reveal a concerning vulnerability",2024,2024-02-18T22:36:19Z,"Keyphrase: ""Vulnerability concerns"""
arxiv2024,Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable,Yes.,4,"""Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases."" and ""output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability.""",2024,2024-02-18T21:10:18Z,"Keyphrase: ""Factual inconsistency and bias"""
arxiv2024,Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation,Yes.,5,"""Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.""",2024,2024-02-18T20:48:09Z,"Keyphrase: ""Limited production-ready code generation"""
arxiv2024,Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers,Yes.,4,"""the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking.""",2024,2024-02-18T20:47:10Z,"Keyphrase: ""Resource-intensive storage and processing"""
arxiv2024,Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning,Yes.,4,"""two substantial challenges persist within the existing VLM frameworks",2024,2024-02-18T19:38:44Z,"Keyphrase: ""Persistent challenges in VLM framework"""
arxiv2024,Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents,Yes.,5,"""LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents."" and ""the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning.""",2024,2024-02-18T17:10:07Z,"Keyphrase: ""Inefficient data utilization"""
arxiv2024,Stealthy Attack on Large Language Model based Recommendation,Yes.,5,"""we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items.""",2024,2024-02-18T16:51:02Z,"Keyphrase: ""Security vulnerability due to textual emphasis"""
arxiv2024,Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark,Yes.,5,"""the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge.""",2024,2024-02-18T14:08:48Z,"Keyphrase: ""Memory overhead challenge"""
arxiv2024,LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration,Yes.,5,"""LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the",2024,2024-02-18T11:46:52Z,"Keyphrase: ""High computational cost"""
arxiv2024,KMMLU: Measuring Massive Multitask Language Understanding in Korean,Yes.,4,"""identifying significant room for improvement,"" ""Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse,"" and ""even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean",2024,2024-02-18T11:41:07Z,"Keyphrase: ""Performance gap in non-English languages"""
arxiv2024,From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings,Yes.,4,"""Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias.""",2024,2024-02-18T08:53:41Z,"Keyphrase: ""Learned biases"""
arxiv2024,What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs,Yes.,5,"""we demonstrate through experimentation that LLMs lack necessary skills required for planning.""",2024,2024-02-18T07:42:49Z,"Keyphrase: ""Lack of necessary planning skills"""
arxiv2024,MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing,Yes.,4,"""Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored."" and ""we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring",2024,2024-02-18T07:15:03Z,"Keyphrase: ""Limited fine-grained knowledge"""
arxiv2024,When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation,Yes.,5,"""Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases.""",2024,2024-02-18T04:57:19Z,"Keyphrase: ""Limited knowledge possession"""
arxiv2024,FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence,Yes.,5,"""We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.""",2024,2024-02-18T04:45:01Z,"Keyphrase: ""Challenges in medical evidence summarization"""
arxiv2024,Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation,Yes.,5,"""This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations."" and ""Experimental results show a general performance decline in most LLMs against their original results.""",2024,2024-02-18T03:40:06Z,"Keyphrase: ""General performance decline"""
arxiv2024,Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs,Yes.,5,"""their mastery of underlying inferential rules still falls short of human capabilities,"" and ""reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns,"" and ""our work sheds light on",2024,2024-02-18T03:38:51Z,"Keyphrase: ""Limited inferential rule mastery"""
arxiv2024,Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models,Yes.,5,"""We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias.""",2024,2024-02-18T03:10:39Z,"Keyphrase: ""Self-bias amplification"""
arxiv2024,Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning,Yes.,4,"""existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments.""",2024,2024-02-18T03:04:38Z,"Keyphrase: ""Limited video comprehension"""
arxiv2024,EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models,Yes.,4,"""EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types.""",2024,2024-02-18T02:41:06Z,"Keyphrase: ""Hallucination with event structure"""
arxiv2024,Aligning Modalities in Vision Large Language Models via Preference Fine-tuning,Yes.,4,"""This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations.""",2024,2024-02-18T00:56:16Z,"Keyphrase: ""Limited factual representation"""
arxiv2024,Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection,Yes.,5,"""LLMs exhibit two extremes",2024,2024-02-18T00:04:40Z,"**Given Evidence:**
Evidence: ""llm exhibit two extreme""

**Keyphrase:**
""Extreme behavior"""
arxiv2024,Tasks That Language Models Don't Learn,Yes.,5,"""We argue that there are certain properties of language that our current large language models (LLMs) don't learn."" and ""highlighting the limitations of knowledge acquired in the absence of sensory experience.""",2024,2024-02-17T17:52:24Z,"Keyphrase: ""Limited knowledge acquisition"""
arxiv2024,Puzzle Solving using Reasoning of Large Language Models: A Survey,Yes.,4,"""identifying significant challenges in complex puzzle scenarios"" and ""highlight the disparity between LLM capabilities and human-like reasoning.""",2024,2024-02-17T14:19:38Z,"Keyphrase: ""Limited humanlike reasoning"""
arxiv2024,Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,Yes.,4,"""the safety issues of LLM-based agents are currently under-explored"" and ""LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents.""",2024,2024-02-17T06:48:45Z,"Keyphrase: ""Vulnerability to backdoor attacks"""
arxiv2024,Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs,Yes.,5,"""However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT."" and ""we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning",2024,2024-02-17T05:22:56Z,"Keyphrase: ""Lack of reasoning ability"""
arxiv2024,Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering,Yes.,4,"""Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain."" and ""The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables.""",2024,2024-02-17T05:10:18Z,"Keyphrase: ""Limited handling of complex mathematical scenarios"""
arxiv2024,Disclosure and Mitigation of Gender Bias in LLMs,Yes.,5,"""Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs.""",2024,2024-02-17T04:48:55Z,"Keyphrase: ""Gender bias and stereotypes"""
arxiv2024,KnowTuning: Knowledge-aware Fine-tuning for Large Language Models,Yes.,5,"""large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers.""",2024,2024-02-17T02:54:32Z,"Keyphrase: ""Struggle with knowledge-intensive tasks"""
arxiv2024,GenDec: A robust generative Question-decomposition method for Multi-hop reasoning,Yes.,4,"""Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer.""",2024,2024-02-17T02:21:44Z,"Keyphrase: ""Limited multihop reasoning"""
arxiv2024,Contrastive Instruction Tuning,Yes.,5,"""current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues.""",2024,2024-02-17T00:09:32Z,"Keyphrase: ""Limited robustness to textual variation"""
arxiv2024,Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models,Yes.,5,"""Our analysis reveals that while humans disagree on which situations require empathy toward the underprivileged, most large language models are unable to empathize with the socioeconomically underprivileged regardless of the situation.""",2024,2024-02-16T23:18:19Z,"Keyphrase: ""Lack of empathy for underprivileged"""
arxiv2024,Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models,Yes.,5,"""Regrettably, previous studies on ME evaluation have two critical limitations",2024,2024-02-16T23:08:55Z,"Keyphrase: ""Limited evaluation and critical limitations"""
arxiv2024,When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models,Yes.,4,"""we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp.""",2024,2024-02-16T22:12:53Z,"Keyphrase: ""Limited reasoning ability"""
arxiv2024,Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives,Yes.,5,"""Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives.""",2024,2024-02-16T19:59:45Z,"Keyphrase: ""Difficulty in inferencing complex relationships"""
arxiv2024,PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering,Yes.,5,"""large language models (LLMs) may have outdated knowledge,"" and ""The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities.""",2024,2024-02-16T19:26:09Z,"Keyphrase: ""Outdated knowledge"""
arxiv2024,RLVF: Learning from Verbal Feedback without Overgeneralization,Yes.,5,"""we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant.""",2024,2024-02-16T18:50:24Z,"Keyphrase: ""Overgeneralization of feedback"""
arxiv2024,When is Tree Search Useful for LLM Planning? It Depends on the Discriminator,Yes.,5,"""current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements"" and ""tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications.""",2024,2024-02-16T18:45:58Z,"Keyphrase: ""Slow advanced planning hindering real-world application"""
arxiv2024,Multi-modal preference alignment remedies regression of visual instruction tuning on language model,Yes.,4,"""the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with.""",2024,2024-02-16T18:42:08Z,Keyphrase: Lack of diversity and complexity in training datasets
arxiv2024,Exploring Value Biases: How LLMs Deviate Towards the Ideal,Yes.,4,"""Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications."" and ""We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars.""",2024,2024-02-16T18:28:43Z,"Keyphrase: ""Unintended biases"""
arxiv2024,Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities,Yes.,5,"""For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity."" and ""Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions.""",2024,2024-02-16T17:15:28Z,"Keyphrase: ""Challenges with time series forecasting"""
arxiv2024,RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model,Yes.,5,"""severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment.""",2024,2024-02-16T16:57:18Z,"Keyphrase: ""Data scarcity and domain gap"""
arxiv2024,In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss,Yes.,5,"""Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements.""",2024,2024-02-16T16:15:01Z,"Keyphrase: ""Limited evaluation benchmarks"""
arxiv2024,ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages,Yes.,5,"""Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to.""",2024,2024-02-16T15:19:46Z,"Keyphrase: ""Enduring safety challenges"""
arxiv2024,GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models,Yes.,5,"""traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods,"" ""prompting LLMs with a fixed set of relations or entities can cause hallucinations,"" and ""precision/recall fails to justify the performance of GRE methods.""",2024,2024-02-16T15:01:24Z,"Keyphrase: ""Hallucination in relation extraction"""
arxiv2024,Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification,Yes.,5,"""Our results show that ChatGPT struggles in abductive reasoning,"" and ""Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated to distinguish between hype and actual capabilities",2024,2024-02-16T14:52:05Z,"Keyphrase: ""Limited abductive reasoning"""
arxiv2024,An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference,Yes.,4,"""recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs.""",2024,2024-02-16T14:15:15Z,"Keyphrase: ""Inference efficiency deterioration"""
arxiv2024,Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability,Yes.,4,"""concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust"" and ""we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges.""",2024,2024-02-16T13:46:06Z,"Keyphrase: ""Lack of interpretability"""
arxiv2024,LongHeads: Multi-Head Attention is Secretly a Long Context Processor,Yes.,5,"""Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands.""",2024,2024-02-16T13:39:34Z,"Keyphrase: ""Limited input length processing"""
arxiv2024,Humans or LLMs as the Judge? A Study on Judgement Biases,Yes.,5,"""Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases.""",2024,2024-02-16T13:21:06Z,"Keyphrase: ""Vulnerability to bias"""
arxiv2024,Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes,Yes.,4,"""current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are 'unreliable' since such processes could contain information unrelated to the answer.""",2024,2024-02-16T13:02:11Z,"Keyphrase: ""Unreliable reasoning process"""
arxiv2024,Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement,Yes.,5,"""existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios."" and ""we find that existing models exhibit a general deficiency in role alignment capabilities.""",2024,2024-02-16T12:12:05Z,"Keyphrase: ""Role alignment deficiency"""
arxiv2024,Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements,Yes.,4,"""existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements.""",2024,2024-02-16T12:00:34Z,"Keyphrase: ""Lack of controllability and biased content"""
arxiv2024,Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models,Yes.,5,"""Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations.""",2024,2024-02-16T11:55:40Z,"Keyphrase: ""Internal hallucination"""
arxiv2024,Jailbreaking Proprietary Large Language Models using Word Substitution Cipher,Yes.,4,"""Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process."" and ""Additionally, we discuss the over-defensiveness of these models.""",2024,2024-02-16T11:37:05Z,"Keyphrase: ""Ethical alignment susceptibility"""
arxiv2024,InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?,Yes.,4,"""Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions.""",2024,2024-02-16T10:54:10Z,"Keyphrase: ""Societal bias and unfair predictions"""
arxiv2024,Properties and Challenges of LLM-Generated Explanations,Yes.,4,"""However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs,"" and ""We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.""",2024,2024-02-16T09:37:54Z,"Keyphrase: ""Reliance on specifically annotated data"""
arxiv2024,Zero-shot sampling of adversarial entities in biomedical question answering,Yes.,5,"""Our investigations illustrate the brittleness of domain knowledge in LLMs and reveal a shortcoming of standard evaluations for high-capacity models.""",2024,2024-02-16T09:29:38Z,"Keyphrase: ""Brittleness in domain knowledge"""
arxiv2024,Unsupervised LLM Adaptation for Question Answering,Yes.,5,"""they have difficulties in accessing information located in the middle or at the end of documents.""",2024,2024-02-16T06:29:16Z,"Keyphrase: ""Limited access to information"""
arxiv2024,WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing,Yes.,5,"""This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch.""",2024,2024-02-16T05:29:59Z,"Keyphrase: ""Toxicity buildup and performance degradation"""
arxiv2024,I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models,Yes.,4,"""The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group."" and ""this replication unveiled an intrinsic Democratic bias in Large Language Models (LL",2024,2024-02-16T03:54:48Z,"Keyphrase: ""Social identity bias"""
arxiv2024,DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection,Yes.,5,"""Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount.""",2024,2024-02-16T03:24:56Z,"Keyphrase: ""Factuality hallucination"""
arxiv2024,Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting,Yes.,5,"""LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs.""",2024,2024-02-16T02:32:06Z,"Keyphrase: ""Factually incorrect hallucinations"""
arxiv2024,DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows,Yes.,4,"""However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows."" and ""The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them.""",2024,2024-02-16T00:10:26Z,"Keyphrase: ""Closed source and lack of standardized tooling"""
arxiv2024,Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review,Yes.,5,"""this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required.""",2024,2024-02-15T22:43:02Z,"Keyphrase: ""Model hallucination and knowledge boundary limitations"""
arxiv2024,On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities,Yes.,5,"""such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences"" and ""simple adversarial attacks can significantly undermine the effectiveness of LLM/VLM-robot integrated systems.""",2024,2024-02-15T22:01:45Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,Uncertainty Quantification for In-Context Learning of Large Language Models,Yes.,5,"""trustworthy issues with LLM's response, such as hallucination"" and ""highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty).""",2024,2024-02-15T18:46:24Z,"Keyphrase: ""Uncertainty in hallucination"""
arxiv2024,Language Models with Conformal Factuality Guarantees,Yes.,5,"""Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem.""",2024,2024-02-15T18:31:53Z,"Keyphrase: ""Factuality guarantee challenge"""
arxiv2024,Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination,Yes.,5,"""Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data.""",2024,2024-02-15T16:21:14Z,"Keyphrase: ""Privacy violation and exposure of sensitive data"""
arxiv2024,Case Study: Testing Model Capabilities in Some Reasoning Tasks,Yes.,5,"""However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios.""",2024,2024-02-15T14:21:30Z,"Keyphrase: ""Limited reasoning capability"""
arxiv2024,Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering,Yes.,5,"""Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations.""",2024,2024-02-15T12:20:02Z,"Keyphrase: ""Ineffective mitigation of factual hallucination"""
arxiv2024,Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence,Yes.,5,"""Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment.""",2024,2024-02-15T11:08:10Z,"Keyphrase: ""Limitations in genuine reasoning"""
arxiv2024,Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States,Yes.,5,"""Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination."" and ""Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one.""",2024,2024-02-15T06:14:55Z,"Keyphrase: ""Susceptibility to hallucination"""
arxiv2024,A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts,Yes.,5,"""Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs.""",2024,2024-02-15T05:40:21Z,"Keyphrase: ""Limited context length"""
arxiv2024,PAL: Proxy-Guided Black-Box Attack on Large Language Models,Yes.,5,"""Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses.""",2024,2024-02-15T02:54:49Z,"Keyphrase: ""Vulnerability to generating harmful content"""
arxiv2024,CodeMind: A Framework to Challenge Large Language Models for Code Reasoning,Yes.,5,"""Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage."" and ""their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls.""",2024,2024-02-15T02:24:46Z,"Keyphrase: ""Unfair assessment due to data leakage"""
arxiv2024,The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse,Yes.,5,"""even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks"" and ""benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive.""",2024,2024-02-15T01:50:38Z,"Keyphrase: ""Collapse on single edit"""
arxiv2024,Probabilistic Reasoning in Generative Large Language Models,Yes.,5,"""Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning.""",2024,2024-02-14T23:05:44Z,"Keyphrase: ""Limited probabilistic reasoning"""
arxiv2024,How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?,Yes.,5,"""However, the security aspects of these systems have received relatively less attention"" and ""Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks.""",2024,2024-02-14T19:45:17Z,"Keyphrase: ""Vulnerability to attacks"""
arxiv2024,Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference,Yes.,5,"""Many computational factors limit broader deployment of large language models."" and ""we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding.""",2024,2024-02-14T18:54:56Z,"Keyphrase: ""Memory bottleneck and computational shortcuts"""
arxiv2024,HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation,Yes.,4,"""With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns.""",2024,2024-02-14T18:41:19Z,"Keyphrase: ""Factuality and hallucination propensity"""
arxiv2024,Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking,Yes.,4,"""Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions.""",2024,2024-02-14T18:16:54Z,"Keyphrase: ""Cultural bias and lack of commonsense knowledge"""
arxiv2024,Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop,Yes.,5,"""Examples include bias, inconsistencies, and hallucination."" and ""auditing the LLM for these problems is desirable, it is far from being easy or solved.""",2024,2024-02-14T17:49:31Z,"Keyphrase: ""Bias, inconsistency, and hallucination"""
arxiv2024,AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach,Yes.,4,"""Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality."" and ""A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues.""",2024,2024-02-14T17:31:04Z,"Keyphrase: ""Inconsistency and potential bias"""
arxiv2024,Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code,Yes.,4,"""The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing."" and ""Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement.""",2024,2024-02-14T16:41:35Z,"Keyphrase: ""Challenges in code auditing"""
arxiv2024,"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",Yes.,4,"""Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety.""",2024,2024-02-14T16:14:03Z,"Keyphrase: ""Risk of harmful responses"""
arxiv2024,Personalized Large Language Models,Yes.,4,"""However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots.""",2024,2024-02-14T15:55:30Z,"Keyphrase: ""Limited personalization"""
arxiv2024,Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation,Yes.,5,"""Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. 'hallucinations', even when they hold relevant knowledge.""",2024,2024-02-14T15:52:42Z,"Keyphrase: ""Factual inaccuracy and hallucination"""
arxiv2024,Scaling the Authoring of AutoTutors with Large Language Models,Yes.,4,"""A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees.""",2024,2024-02-14T14:53:56Z,"Keyphrase: ""Leaking answers"""
arxiv2024,Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling,Yes.,4,"""their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution.""",2024,2024-02-14T14:32:16Z,"Keyphrase: ""Vulnerability to misinformation"""
arxiv2024,(Ir)rationality and Cognitive Biases in Large Language Models,Yes.,5,"""We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality",2024,2024-02-14T14:17:21Z,"Keyphrase: ""Irrational behavior"""
arxiv2024,"Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization",Yes.,5,"""the trustworthiness of third-party custom versions of LLMs remains an essential concern."" and ""Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.""",2024,2024-02-14T13:47:35Z,"Keyphrase: ""Vulnerability to customization risks"""
arxiv2024,Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks,Yes.,5,"""Large Language Models (LLMs) are susceptible to Jailbreaking attacks,"" and ""we guide the responses of the model toward revealing the 'desired' harmful information.""",2024,2024-02-14T13:45:19Z,"Keyphrase: ""Susceptible to jailbreaking attacks"""
arxiv2024,Into the Unknown: Self-Learning Large Language Models,Yes.,5,"""We address the main problem of self-learning LLM",2024,2024-02-14T12:56:58Z,"Keyphrase: ""Limited self-learning capabilities"""
arxiv2024,Exploring the Adversarial Capabilities of Large Language Models,Yes.,4,"""While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored."" and ""Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems.""",2024,2024-02-14T12:28:38Z,"Keyphrase: ""Adversarial behavior undermining detection"""
arxiv2024,Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space,Yes.,5,"""We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning."" and ""embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models.""",2024,2024-02-14T10:20:03Z,"Keyphrase: ""Vulnerability to adversarial attacks"""
arxiv2024,SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks,Yes.,5,"""However, their large number of parameters poses significant challenges for practical deployment.""",2024,2024-02-14T09:01:13Z,"Keyphrase: ""Deployment challenges due to large number of parameters"""
arxiv2024,SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding,Yes.,5,"""Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat.""",2024,2024-02-14T06:54:31Z,"Keyphrase: ""Safety vulnerabilities"""
arxiv2024,GrounDial: Human-norm Grounded Safe Dialog Response Generation,Yes.,4,"""Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses, agreeing to offensive user input or including toxic content.""",2024,2024-02-14T06:25:50Z,"Keyphrase: ""Unsafe and offensive responses"""
arxiv2024,Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model,Yes.,5,"""the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development."" and ""We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities.""",2024,2024-02-14T06:24:52Z,"Keyphrase: ""Limited generalization in clinical settings"""
arxiv2024,Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models,Yes.,5,"""This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.""",2024,2024-02-14T05:52:23Z,"Keyphrase: ""Limited robustness in analogical reasoning"""
arxiv2024,Premise Order Matters in Reasoning with Large Language Models,Yes.,5,"""we discover a frailty",2024,2024-02-14T04:50:18Z,"Keyphrase: ""Limited discovery capabilities"""
arxiv2024,GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency,Yes.,4,"""However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering.""",2024,2024-02-13T23:48:59Z,"Keyphrase: ""Limited personalization control"""
arxiv2024,"ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions",Yes.,4,"""identify and understand why LLMs fails"" and ""ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains.""",2024,2024-02-13T21:15:33Z,"Keyphrase: ""Limited domain expertise"""
arxiv2024,"GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",Yes.,5,"""However, recent work demonstrates that even the best models struggle to identify when and where to refine without access to external feedback."" and ""But they are expensive to train, requiring extensive human annotations.""",2024,2024-02-13T20:16:29Z,"Keyphrase: ""Struggle with refining without external feedback"""
arxiv2024,Measuring and Controlling Instruction (In)Stability in Language Model Dialogs,Yes.,5,"""Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a significant instruction drift within eight rounds of conversations.""",2024,2024-02-13T20:10:29Z,"Keyphrase: ""Instruction drift"""
arxiv2024,Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance,Yes.,4,"""highlighted the critical issue of their tendency to hallucinate non-existing objects in the images"" and ""these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.""",2024,2024-02-13T18:59:05Z,"Keyphrase: ""Hallucination of nonexisting objects"""
arxiv2024,Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast,Yes.,5,"""Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors."" and ""It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost",2024,2024-02-13T16:06:17Z,"Keyphrase: ""Adversarial behavior and jailbreaking vulnerability"""
arxiv2024,The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale,Yes.,5,"""ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists.""",2024,2024-02-13T14:38:12Z,"Keyphrase: ""Inconsistent accuracy"""
arxiv2024,Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale,Yes.,4,"""This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff,"" and ""We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools.""",2024,2024-02-13T13:50:08Z,"Keyphrase: ""Generation of realistic disinformation"""
arxiv2024,Visually Dehallucinative Instruction Generation,Yes.,4,"""challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents.""",2024,2024-02-13T10:25:45Z,"Keyphrase: ""Unintended content generation"""
arxiv2024,Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering,Yes.,4,"""Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability).""",2024,2024-02-13T08:12:48Z,"Keyphrase: ""Lack of source attribution"""
arxiv2024,On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks,Yes.,5,"""While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion."" and ""We observe significant performance collapse with self-critique, significant performance gains with sound external verification,",2024,2024-02-12T23:11:01Z,"Keyphrase: ""Limited self-critique and improvement"""
arxiv2024,Addressing cognitive bias in medical language models,Yes.,4,"""Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias.""",2024,2024-02-12T23:08:37Z,"Keyphrase: ""Varying effect of bias"""
arxiv2024,Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation,Yes.,5,"""However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination."" and ""Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to",2024,2024-02-12T22:35:40Z,"Keyphrase: ""Translation ability influenced by data contamination"""
arxiv2024,Beyond LLMs: Advancing the Landscape of Complex Reasoning,Yes.,5,"""However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems.""",2024,2024-02-12T21:14:45Z,"Keyphrase: ""Poor performance on constraint satisfaction and optimization problems"""
arxiv2024,Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking,Yes.,5,"""Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a",2024,2024-02-12T19:49:58Z,"Keyphrase: ""Overreliance on inaccurate suggestions"""
arxiv2024,PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models,Yes.,5,"""Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination.""",2024,2024-02-12T18:28:36Z,"Keyphrase: ""Outdated knowledge hallucination"""
arxiv2024,Lissard: Long and Simple Sequential Reasoning Datasets,Yes.,5,"""Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training.""",2024,2024-02-12T18:10:17Z,"Keyphrase: ""Struggles with rule-based tasks"""
arxiv2024,Mercury: An Efficiency Benchmark for LLM Code Synthesis,Yes.,5,"""Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for LLM research and development.""",2024,2024-02-12T17:53:22Z,"Keyphrase: ""Efficiency gap in code generation"""
arxiv2024,Do Membership Inference Attacks Work on Large Language Models?,Yes.,5,"""We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members.""",2024,2024-02-12T17:52:05Z,"Keyphrase: ""Poor performance in domain analysis"""
arxiv2024,Retrieval-Augmented Thought Process as Sequential Decision Making,Yes.,5,"""However, several open challenges hinder their wider application",2024,2024-02-12T17:17:50Z,"Keyphrase: ""Open challenges hindering wider application"""
arxiv2024,AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension,Yes.,4,"""By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.""",2024,2024-02-12T15:41:22Z,"Keyphrase: ""Limited evaluation insights"""
arxiv2024,Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT,Yes.,5,"""Developing long-context retrieval encoders suitable for these domains raises three challenges",2024,2024-02-12T06:43:52Z,"Keyphrase: ""Challenges in domain adaptation"""
arxiv2024,Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate,Yes.,4,"""While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness.""",2024,2024-02-12T04:32:33Z,"Keyphrase: ""Unfaithful explanations"""
arxiv2024,Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning,Yes.,4,"""the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others"" and ""The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs.""",2024,2024-02-12T01:55:51Z,"Keyphrase: ""Inequitable performance impact"""
arxiv2024,Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning,Yes.,5,"""However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP.""",2024,2024-02-11T13:30:53Z,"Keyphrase: ""Limited spatial awareness"""
arxiv2024,Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models,Yes.,5,"""We find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers.""",2024,2024-02-11T12:25:41Z,"Keyphrase: ""Inaccurate generation"""
arxiv2024,Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias,Yes.,5,"""We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic.""",2024,2024-02-11T11:23:28Z,"Keyphrase: ""Toxicity bias and hallucination"""
arxiv2024,Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review,Yes.,4,"""Despite their transformative potential, challenges persist, including sensitivity to input prompts, occasional misinterpretations, and unforeseen recommendations, necessitating continuous refinement and evolution in LLM-driven recommender systems.""",2024,2024-02-11T00:24:17Z,"Keyphrase: ""Sensitivity and misinterpretation issues"""
arxiv2024,A Tale of Tails: Model Collapse as a Change of Scaling Laws,Yes.,5,"""We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning"" of skills, and grokking when mixing human and synthesized data.""",2024,2024-02-10T21:06:34Z,"Keyphrase: ""Decay phenomenon and loss scaling"""
arxiv2024,Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric,Yes.,4,"""The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset.""",2024,2024-02-10T07:55:27Z,"Keyphrase: ""Limited generalization to out-of-distribution toxicity"""
arxiv2024,GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding,Yes.,5,"""their ability to reason over domain-specialized graphs of interconnected entities remains limited"" and ""The answer is no--such capabilities lie beyond current methods.""",2024,2024-02-09T19:53:29Z,"Keyphrase: ""Limited domain-specific reasoning"""
arxiv2024,Feedback Loops With Language Models Drive In-Context Reward Hacking,Yes.,5,"""we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process.""",2024,2024-02-09T18:59:29Z,"Keyphrase: ""Feedback loop-induced reward hacking"""
arxiv2024,Understanding the Effects of Iterative Prompting on Truthfulness,Yes.,5,"""Yet, the reliability and truthfulness of these models remain pressing concerns."" and ""naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors.""",2024,2024-02-09T18:57:08Z,"Keyphrase: ""Reliability and truthfulness concerns"""
arxiv2024,On the Out-Of-Distribution Generalization of Multimodal Large Language Models,Yes.,5,"""Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation."" and ""We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples",2024,2024-02-09T18:21:51Z,"Keyphrase: ""Limited generalization beyond training domain"""
arxiv2024,Understanding the Weakness of Large Language Model Agents within a Complex Android Environment,Yes.,5,"""LLM agents face three primary challenges,"" ""even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints,"" and ""a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents.""",2024,2024-02-09T18:19:25Z,"**Given Evidence:**
Evidence: ""llm agent face three primary challenge even stateoftheart llm agent struggle crossapp scenario adhering specific constraint lack four key capability ie understanding reasoning exploration reflection primary reason failure llm agent""

**Keyphrase:**
Lack of key capabilities"
arxiv2024,The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model,Yes.,5,"""The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates.""",2024,2024-02-09T17:15:45Z,"Keyphrase: ""Hallucinations in information retrieval"""
arxiv2024,Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty,Yes.,5,"""However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist.""",2024,2024-02-09T16:40:59Z,"Keyphrase: ""Hallucination leading to unsafe outcomes"""
arxiv2024,On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference,Yes.,4,"""Despite the recent success associated with Large Language Models (LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands.""",2024,2024-02-09T09:20:59Z,"Keyphrase: ""Cost-prohibitive deployment"""
arxiv2024,Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning,Yes.,4,"""they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak.""",2024,2024-02-09T09:09:39Z,"Keyphrase: ""Susceptibility to prompt-induced safety bypass"""
arxiv2024,"The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate",Yes.,5,"""Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks."" and ""underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators.""",2024,2024-02-09T06:16:08Z,"Keyphrase: ""Disparity in performance"""
arxiv2024,ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling,Yes.,5,"""the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects.""",2024,2024-02-09T01:00:14Z,"Keyphrase: ""Inaccurate visual grounding"""
arxiv2024,SubGen: Token Generation in Sublinear Time and Memory,Yes.,5,"""Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation.""",2024,2024-02-08T22:17:40Z,"Keyphrase: ""Extensive memory requirement"""
arxiv2024,OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models,Yes.,5,"""Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.""",2024,2024-02-08T20:35:06Z,"Keyphrase: ""Limited modeling of mental states"""
arxiv2024,LLMs Among Us: Generative AI Participating in Digital Discourse,Yes.,4,"""While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors.""",2024,2024-02-08T19:21:33Z,"Keyphrase: ""Bias, privacy, and propaganda concerns"""
arxiv2024,WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,Yes.,5,"""Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time."" and ""However, all finetuned models struggle to generalize to unseen websites.""",2024,2024-02-08T18:58:02Z,"Keyphrase: ""Struggles with generalization to unseen websites"""
arxiv2024,Large Language Model Meets Graph Neural Network in Knowledge Distillation,Yes.,4,"""the deployment of LLMs for production is hindered by its high computational and storage requirements, as well as long latencies during model inference.""",2024,2024-02-08T18:33:21Z,"Keyphrase: ""High computational and storage requirements"""
arxiv2024,Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking,Yes.,5,"""little is known about such a risk of LLM-powered conversational search"" and ""participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias.""",2024,2024-02-08T18:14:33Z,"Keyphrase: ""Biased information reinforcement"""
arxiv2024,EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models,Yes.,4,"""they also introduce significant privacy concerns",2024,2024-02-08T17:57:11Z,"Keyphrase: ""Privacy concerns"""
arxiv2024,Is it Possible to Edit Large Language Models Robustly?,Yes.,5,"""However, the robustness of model editing remains an open question."" and ""Our experimental results uncover a substantial disparity between existing editing methods and the practical application of LLMs."" and ""On rephrased prompts that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline.""",2024,2024-02-08T17:06:45Z,"Keyphrase: ""Limited robustness in editing"""
arxiv2024,Limits of Transformer Language Models on Learning Algorithmic Compositions,Yes.,5,"""We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition.""",2024,2024-02-08T16:23:29Z,"Keyphrase: ""Limited compositional capability"""
arxiv2024,Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images,Yes.,4,"""we examine potential gender and racial biases in such systems,"" and ""we observe significant differences in the responses according to the perceived gender or race of the person depicted.""",2024,2024-02-08T16:11:23Z,"Keyphrase: ""Gender and racial biases"""
arxiv2024,TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation,Yes.,5,"""Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents.""",2024,2024-02-08T15:08:57Z,"Keyphrase: ""Limited multitasking ability"""
arxiv2024,In-Context Learning Can Re-learn Forbidden Tasks,Yes.,5,"""Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities."" and ""we investigate whether ICL can undo safety training, which could represent a major security risk.""",2024,2024-02-08T14:54:17Z,"Keyphrase: ""Vulnerability despite safety training"""
arxiv2024,Comprehensive Assessment of Jailbreak Attacks Against LLMs,Yes.,5,"""safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks.""",2024,2024-02-08T13:42:50Z,"Keyphrase: ""Vulnerability to jailbreak attacks"""
arxiv2024,"Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks",Yes.,5,"""We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good.""",2024,2024-02-08T13:07:31Z,"Keyphrase: ""Limited performance in complex tasks"""
arxiv2024,"Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations",Yes.,5,"""We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity."" and ""We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs.""",2024,2024-02-08T12:36:29Z,"Keyphrase: ""Entity ambiguity and mixing information"""
arxiv2024,"Efficient Models for the Detection of Hate, Abuse and Profanity",Yes.,5,"""Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content.""",2024,2024-02-08T12:28:18Z,"Keyphrase: ""Hateful and profane content generation"""
arxiv2024,AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers,Yes.,4,"""Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process.""",2024,2024-02-08T12:01:24Z,"Keyphrase: ""Biased prediction and hallucination"""
arxiv2024,Can ChatGPT evaluate research quality?,Yes.,5,"""ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks.""",2024,2024-02-08T10:00:40Z,"Keyphrase: ""Limited accuracy and trustworthiness"""
arxiv2024,Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia,Yes.,4,"""Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle.""",2024,2024-02-08T07:56:49Z,"Keyphrase: ""Prone to generating harmful content"""
arxiv2024,Do Large Code Models Understand Programming Concepts? A Black-box Approach,Yes.,5,"""Our findings suggest that current models lack understanding of concepts such as data flow and control flow.""",2024,2024-02-08T06:48:01Z,"Keyphrase: ""Limited understanding of data flow control"""
arxiv2024,Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes,Yes.,5,"""LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target.""",2024,2024-02-08T04:48:26Z,"Keyphrase: ""Resource-intensive scalability"""
arxiv2024,Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception,Yes.,5,"""existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination.""",2024,2024-02-08T02:37:30Z,"Keyphrase: ""Insufficient expressive power"""
arxiv2024,Are LLMs Ready for Real-World Materials Discovery?,Yes.,5,"""While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge.""",2024,2024-02-07T19:10:36Z,"Keyphrase: ""Limited comprehension of complex material science knowledge"""
arxiv2024,Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications,Yes.,5,"""Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning."" and ""These findings underscore the urgent need for more robust safety strategies in LLMs.""",2024,2024-02-07T18:34:38Z,"Keyphrase: ""Brittleness and susceptibility to jailbreaking"""
arxiv2024,An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration,Yes.,4,"""they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process.""",2024,2024-02-07T15:56:17Z,"Keyphrase: ""Limited transparency and reasoning"""
arxiv2024,Reconfidencing LLMs from the Grouping Loss Perspective,Yes.,5,"""Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone."" and ""Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query.""",2024,2024-02-07T15:40:22Z,"Keyphrase: ""Overconfident hallucinated answers"""
arxiv2024,Prompting Implicit Discourse Relation Annotation,Yes.,5,"""Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches.""",2024,2024-02-07T14:44:42Z,"Keyphrase: ""Inferior performance on implicit discourse relation classification"""
arxiv2024,MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark,Yes.,5,"""MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V.""",2024,2024-02-07T12:28:32Z,"Keyphrase: ""Diverse bias and hallucinatory responses"""
arxiv2024,A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models,Yes.,4,"""how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further"" and ""The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework's potential to approximate LLM decisions better in future work.""",2024,2024-02-07T12:26:12Z,"Keyphrase: ""Questionable faithfulness in explanations"""
arxiv2024,Large Language Models As Faithful Explainers,Yes.,4,"""natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs.""",2024,2024-02-07T09:09:14Z,"Keyphrase: ""Lack of faithfulness in explanations"""
arxiv2024,MEMORYLLM: Towards Self-Updatable Large Language Models,Yes.,4,"""Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model.""",2024,2024-02-07T07:14:11Z,"Keyphrase: ""Limited adaptability to new knowledge"""
arxiv2024,InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory,Yes.,5,"""existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues.""",2024,2024-02-07T06:50:42Z,"Keyphrase: ""Limited sequence length generalization"""
arxiv2024,Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,Yes.,5,"""However, there is little to no understanding of their faithfulness,"" ""we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs,"" ""these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness,"" and ""improving faithfulness is an open challenge.""",2024,2024-02-07T06:32:50Z,"Keyphrase: ""Challenges in faithfulness"""
arxiv2024,Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector,Yes.,4,"""Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs).""",2024,2024-02-07T05:56:54Z,"Keyphrase: ""Overcorrection challenge"""
arxiv2024,Online Cascade Learning for Efficient Inference over Streams,Yes.,5,"""Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks.""",2024,2024-02-07T01:46:50Z,"Keyphrase: ""High computational cost"""
arxiv2024,De-amplifying Bias from Differential Privacy in Language Model Fine-tuning,Yes.,5,"""We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP.""",2024,2024-02-07T00:30:58Z,"Keyphrase: ""Amplification of biases"""
arxiv2024,Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models,Yes.,5,"""concentrating on deceptive behaviours of Large Language Models (LLMs)"" and ""emphasising multidimensional biases that underlie their deceptive behaviours"" and ""the literature review covers four types of deception categorised",2024,2024-02-07T00:21:46Z,"Keyphrase: ""Multidimensional bias and deceptive behavior"""
arxiv2024,Detecting Mode Collapse in Language Models via Narration,Yes.,5,"""we show successive versions of GPT-3 suffer from increasing degrees of 'mode collapse' whereby overfitting the model during alignment constrains it from generalizing over authorship",2024,2024-02-06T23:52:58Z,"Keyphrase: ""Mode collapse and overfitting"""
arxiv2024,Training Language Models to Generate Text with Citations via Fine-grained Rewards,Yes.,5,"""While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources.""",2024,2024-02-06T19:00:40Z,"Keyphrase: ""Hallucination and lack of credibility"""
arxiv2024,Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science,Yes.,5,"""While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety."" and ""We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents,"" and ""Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents.""",2024,2024-02-06T18:54:07Z,"Keyphrase: ""Novel vulnerabilities and safety considerations"""
arxiv2024,Measuring Implicit Bias in Explicitly Unbiased Large Language Models,Yes.,5,"""Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases,"" and ""Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others).""",2024,2024-02-06T15:59:23Z,"Keyphrase: ""Implicit bias across social domains"""
arxiv2024,Systematic Biases in LLM Simulations of Debates,Yes.,5,"""In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives.""",2024,2024-02-06T14:51:55Z,"Keyphrase: ""Inherent social bias in simulating human interaction"""
arxiv2024,Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought,Yes.,5,"""We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise.""",2024,2024-02-06T13:59:56Z,"Keyphrase: ""Struggles with dynamic noise"""
arxiv2024,LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K,Yes.,5,"""Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation,"" and ""LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of 'needle in a haystack'.""",2024,2024-02-06T13:11:19Z,"Keyphrase: ""Knowledge leakage and biased evaluation"""
arxiv2024,"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",Yes.,5,"""The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers."" and ""we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues.""",2024,2024-02-06T11:54:23Z,"Keyphrase: ""Data contamination and reproducibility issues"""
arxiv2024,Can Large Language Models Detect Rumors on Social Media?,Yes.,5,"""it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information.""",2024,2024-02-06T11:33:57Z,"Keyphrase: ""Difficulty in reasoning with redundant information"""
arxiv2024,Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models,Yes.,5,"""Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements.""",2024,2024-02-06T10:37:21Z,"Keyphrase: ""Biased target variable selection"""
arxiv2024,The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs,Yes.,5,"""those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs."" and ""illustrating that they universally suffer from this instinctive bias to varying degrees.""",2024,2024-02-06T06:48:46Z,"Keyphrase: ""Instinctive bias"""
arxiv2024,Limits of Large Language Models in Debating Humans,Yes.,5,"""We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters.""",2024,2024-02-06T03:24:27Z,"Keyphrase: ""Limited debate capabilities"""
arxiv2024,Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context,Yes.,5,"""Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs",2024,2024-02-06T01:59:41Z,"Keyphrase: ""Limited cross-file context integration"""
arxiv2024,Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains,Yes.,5,"""We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors.""",2024,2024-02-05T20:51:11Z,"Keyphrase: ""Inconsistency in summarization"""
arxiv2024,Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues,Yes.,5,"""This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and",2024,2024-02-05T20:11:56Z,"Keyphrase: ""Struggles with nuance and verbal instruction"""
arxiv2024,A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications,Yes.,4,"""We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique.""",2024,2024-02-05T19:49:13Z,"Keyphrase: ""Limited depth and detail"""
arxiv2024,Nevermind: Instruction Override and Moderation in Large Language Models,Yes.,5,"""Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines.""",2024,2024-02-05T18:58:19Z,"Keyphrase: ""Difficulty in following safety guidelines"""
arxiv2024,GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models,Yes.,4,"""The discovery of 'jailbreaks' to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures."" and ""Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses.""",2024,2024-02-05T18:54:43Z,"Keyphrase: ""Ethical guideline violations"""
arxiv2024,Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS,Yes.,5,"""Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency.""",2024,2024-02-05T18:47:04Z,"Keyphrase: ""Challenges in code generation"""
arxiv2024,Unified Hallucination Detection for Multimodal Large Language Models,Yes.,5,"""Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination.""",2024,2024-02-05T16:56:11Z,"Keyphrase: ""Hallucination issues"""
arxiv2024,C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models,Yes.,5,"""Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments.""",2024,2024-02-05T16:46:16Z,"Keyphrase: ""Trustworthiness issues"""
arxiv2024,Best Practices for Text Annotation with Large Language Models,Yes.,4,"""Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results.""",2024,2024-02-05T15:43:50Z,"Keyphrase: ""Misleading simplicity and bias"""
arxiv2024,Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations,Yes.,5,"""Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions.""",2024,2024-02-05T15:08:19Z,"Keyphrase: ""Stability issues and content hallucination"""
arxiv2024,Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation,Yes.,5,"""there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context,"" and ""directly applying LLMs often leads to inaccurate answers.""",2024,2024-02-05T11:58:56Z,"Keyphrase: ""Weak reasoning and contextual understanding"""
arxiv2024,Evading Data Contamination Detection for Language Models is (too) Easy,Yes.,5,"""However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements."" and ""we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.""",2024,2024-02-05T09:10:32Z,"Keyphrase: ""Contamination of training data"""
arxiv2024,Graph-enhanced Large Language Models in Asynchronous Plan Reasoning,Yes.,5,"""We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow."" and ""LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices.""",2024,2024-02-05T08:26:33Z,"Keyphrase: ""Poor performance on complex tasks"""
arxiv2024,DeAL: Decoding-time Alignment for Large Language Models,Yes.,4,"""First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training).""",2024,2024-02-05T06:12:29Z,"Keyphrase: ""Reliability and safety concerns"""
arxiv2024,Large Language Models are Geographically Biased,Yes.,5,"""Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm."" and ""We show various problematic geographic biases, which we define as systemic errors in geospatial predictions."" and ""LLMs exhibit common biases across a range of objective and subjective topics"" and ""LLMs are clearly biased against locations with lower",2024,2024-02-05T02:32:09Z,"Keyphrase: ""Inherent societal biases"""
arxiv2024,Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting,Yes.,5,"""Large Language Models (LLMs) frequently struggle with complex reasoning tasks, failing to construct logically sound steps towards the solution."" and ""repeated meaningless feedback gradually decreases the quality of the responses, eventually leading to a larger deviation from the intended outcome.""",2024,2024-02-05T00:44:28Z,"Keyphrase: ""Struggles with complex reasoning"""
arxiv2024,LLM-Enhanced Data Management,Yes.,5,"""existing LLMs have several limitations",2024,2024-02-04T23:42:02Z,"Keyphrase: ""Multiple limitations"""
arxiv2024,Can Large Language Models Learn Independent Causal Mechanisms?,Yes.,5,"""Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability.""",2024,2024-02-04T23:04:02Z,"Keyphrase: ""Limited generalization ability in uncommon settings"""
arxiv2024,PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?,Yes.,5,"""We first observe that LLMs, even when aided by symbolic solvers, perform rather poorly on our dataset.""",2024,2024-02-04T20:56:09Z,"Keyphrase: ""Poor performance on symbolic tasks"""
arxiv2024,Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials,Yes.,5,"""Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context.""",2024,2024-02-04T16:18:01Z,"Keyphrase: ""Shortcut learning and lack of contextual variation"""
arxiv2024,Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation,Yes.,5,"""The ease with which AI can generate believable but false scientific information, as illustrated in this case, raises significant concerns about the potential for misinformation in medicine.""",2024,2024-02-04T13:21:19Z,"Keyphrase: ""Potential for generating false scientific information"""
arxiv2024,Factuality of Large Language Models in the Year 2024,Yes.,5,"""Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios.""",2024,2024-02-04T09:36:31Z,"Keyphrase: ""Factual inaccuracy"""
arxiv2024,DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models,Yes.,5,"""we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases.""",2024,2024-02-04T08:11:45Z,"Keyphrase: ""Poor decision-making in complex problems"""
arxiv2024,Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning,Yes.,5,"""However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process.""",2024,2024-02-04T07:59:06Z,"Keyphrase: ""Lack of self-evaluation capability"""
arxiv2024,A Survey of Large Language Models in Finance (FinLLMs),Yes.,4,"""Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency.""",2024,2024-02-04T02:06:57Z,"Keyphrase: ""Hallucination and privacy concerns"""
arxiv2024,"Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times",Yes.,5,"""Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades.""",2024,2024-02-03T20:22:54Z,"Keyphrase: ""Degraded human reading time estimation"""
arxiv2024,"Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding",Yes.,4,"""These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings.""",2024,2024-02-03T19:19:34Z,"Keyphrase: ""Lack of sensorimotor grounding"""
arxiv2024,Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models,Yes.,4,"""Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks."" and ""Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning L",2024,2024-02-03T16:43:42Z,"Keyphrase: ""Vulnerability to harmful content and forgetting safety alignment"""
arxiv2024,GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events,Yes.,5,"""Concurrently, we also identify certain limitations of GPT-4V, which constrain its understanding in more intricate scenarios.""",2024,2024-02-03T16:38:25Z,"Keyphrase: ""Limited understanding of intricate scenarios"""
arxiv2024,Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations,Yes.,4,"""At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the",2024,2024-02-03T14:28:55Z,"Keyphrase: ""Diverse pitfalls in generating desired results"""
arxiv2024,Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test,Yes.,5,"""Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages.""",2024,2024-02-03T12:52:36Z,"Keyphrase: ""Cultural and linguistic bias"""
arxiv2024,Affordable Generative Agents,Yes.,4,"""the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents,"" and ""demonstrating that agents can only generate finite behaviors in fixed environments.""",2024,2024-02-03T06:16:28Z,"Keyphrase: ""Limited adaptability in dynamic environments"""
arxiv2024,A Closer Look at the Limitations of Instruction Tuning,Yes.,5,"""While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT.""",2024,2024-02-03T04:45:25Z,"Keyphrase: ""Underexplored limitations"""
arxiv2024,How well do LLMs cite relevant medical references? An evaluation framework and analyses,Yes.,5,"""Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide."" and ""Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references.""",2024,2024-02-03T03:44:57Z,"Keyphrase: ""Potential for harm with medical information"""
arxiv2024,Human-Centered Privacy Research in the Age of Large Language Models,Yes.,4,"""The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns."" and ""To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems.""",2024,2024-02-03T02:32:45Z,"Keyphrase: ""Imperfect privacy properties"""
arxiv2024,Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes,Yes.,4,"""Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases.""",2024,2024-02-03T01:40:11Z,"Keyphrase: ""Harmful social bias"""
arxiv2024,What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement,Yes.,5,"""Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase.""",2024,2024-02-02T19:43:15Z,"Keyphrase: ""Catastrophic forgetting"""
arxiv2024,"(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",Yes.,5,"""Beyond known issues like hallucinations, experts revealed novel legal problems, including that users' conversations with LLMs are not protected by attorney-client confidentiality or bound to professional ethics that guard against conflicted counsel or poor quality advice.""",2024,2024-02-02T19:35:34Z,"Keyphrase: ""Poor quality legal advice"""
arxiv2024,Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning,Yes.,5,"""However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability.""",2024,2024-02-02T18:00:35Z,"Keyphrase: ""Limited trustworthiness and usability"""
arxiv2024,TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution,Yes.,4,"""their trustworthiness remains an under-explored area"" and ""improving the safety dimension of trustworthiness in LLM-based agents.""",2024,2024-02-02T17:26:23Z,"Keyphrase: ""Lack of trustworthiness assessment"""
arxiv2024,Homogenization Effects of Large Language Models on Human Creative Ideation,Yes.,4,"""different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST"" and ""ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated.""",2024,2024-02-02T16:27:11Z,"Keyphrase: ""Limited semantic diversity"""
arxiv2024,"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",Yes.,5,"""We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature.""",2024,2024-02-02T14:43:18Z,"Keyphrase: ""Limited planning and reasoning capabilities"""
arxiv2024,Distilling LLMs' Decomposition Abilities into Compact Language Models,Yes.,4,"""Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization.""",2024,2024-02-02T13:23:15Z,"Keyphrase: ""Scalability challenges and limited customization"""
arxiv2024,StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback,Yes.,4,"""the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective.""",2024,2024-02-02T13:14:31Z,"Keyphrase: ""Limited understanding of complex human requirements"""
arxiv2024,Continual Learning for Large Language Models: A Survey,Yes.,4,"""Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale."" and ""Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.""",2024,2024-02-02T12:34:09Z,"Keyphrase: ""High retraining cost"""
arxiv2024,A Survey on Large Language Model Hallucination via a Creativity Perspective,Yes.,4,"""This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications.""",2024,2024-02-02T12:21:04Z,"Keyphrase: ""Reliability issues due to hallucination"""
arxiv2024,Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models,Yes.,5,"""our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of L",2024,2024-02-02T12:07:00Z,"Keyphrase: ""Response variability syndrome"""
arxiv2024,Can MLLMs Perform Text-to-Image In-Context Learning?,Yes.,5,"""we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation.""",2024,2024-02-02T10:30:05Z,"Keyphrase: ""Multimodal complexity"""
arxiv2024,Exploring the Limitations of Graph Reasoning in Large Language Models,Yes.,5,"""We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution.""",2024,2024-02-02T09:45:33Z,"Keyphrase: ""Bias in benchmarking"""
arxiv2024,"The Human and the Mechanical: logos, truthfulness, and ChatGPT",Yes.,5,"""Mechanical minds lack these two components",2024,2024-02-02T09:41:51Z,"Keyphrase: ""Lack of understanding context"""
arxiv2024,Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus,Yes.,4,"""yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources.""",2024,2024-02-02T06:44:22Z,"Keyphrase: ""Limited external knowledge integration"""
arxiv2024,A Multi-Agent Conversational Recommender System,Yes.,4,"""Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address",2024,2024-02-02T04:20:13Z,"Keyphrase: ""Limited conversational recommendation capabilities"""
arxiv2024,The Political Preferences of LLMs,Yes.,4,"""The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints."" and ""base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests.""",2024,2024-02-02T02:43:10Z,"Keyphrase: ""Political bias and suboptimal performance"""
arxiv2024,LitLLM: A Toolkit for Scientific Literature Review,Yes.,5,"""Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on.""",2024,2024-02-02T02:41:28Z,"Keyphrase: ""Hallucination of non-actual information"""
arxiv2024,When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards,Yes.,5,"""Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details.""",2024,2024-02-01T19:12:25Z,"Keyphrase: ""Sensitivity to minute details"""
arxiv2024,Evaluating Large Language Models for Generalization and Robustness via Data Compression,Yes.,5,"""Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation."" and ""We find that the compression rate of many models reduces significantly after their cutoff date,"" and ""Results also suggest that models struggle to generalize on news and code",2024,2024-02-01T18:56:18Z,"Keyphrase: ""Data contamination sensitivity"""
arxiv2024,Can Large Language Models Understand Context?,Yes.,5,"""Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models"" and ""we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark.""",2024,2024-02-01T18:55:29Z,"Keyphrase: ""Struggles with nuanced contextual understanding"""
arxiv2024,Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents,Yes.,5,"""However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents.""",2024,2024-02-01T17:30:50Z,"Keyphrase: ""Uncontrollable content generation"""
arxiv2024,Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement,Yes.,4,"""Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains.""",2024,2024-02-01T16:39:51Z,"Keyphrase: ""Limited interpretability and control"""
arxiv2024,Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing,Yes.,5,"""However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process... the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training.""",2024,2024-02-01T15:18:33Z,"Keyphrase: ""Inefficient reasoning process"""
arxiv2024,Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks,Yes.,5,"""We uncover that Self-Generated attacks pose a significant threat, reducing LVLM(s) classification performance by up to 33%.""",2024,2024-02-01T14:41:20Z,"Keyphrase: ""Vulnerability to self-generated attacks"""
arxiv2024,Actor Identification in Discourse: A Challenge for LLMs?,Yes.,5,"""Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output.""",2024,2024-02-01T14:30:39Z,"Keyphrase: ""Struggles with generating correct canonical forms"""
arxiv2024,Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection,Yes.,4,"""Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises.""",2024,2024-02-01T08:11:56Z,"Keyphrase: ""Risk of plagiarism and fake news"""
arxiv2024,Investigating Bias Representations in Llama 2 Chat via Activation Steering,Yes.,4,"""We address the challenge of societal bias in Large Language Models (LLMs),"" and ""Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF).""",2024,2024-02-01T07:48:50Z,"Keyphrase: ""Inherent gender bias"""
arxiv2024,"Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",Yes.,5,"""knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge"" and ""Motivated by their failures in self-reflection and over-reliance on held-out sets"".",2024,2024-02-01T06:11:49Z,"Keyphrase: ""Knowledge gap and outdated information"""
arxiv2024,Safety of Multimodal Large Language Models on Images and Text,Yes.,4,"""the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios.""",2024,2024-02-01T05:57:10Z,"Keyphrase: ""Safety risks in real-world deployment"""
arxiv2024,"Redefining ""Hallucination"" in LLMs: Towards a psychology-informed framework for mitigating misinformation",Yes.,5,"""a notable challenge surfaces in the form of 'hallucinations.' This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base.""",2024,2024-02-01T03:01:11Z,"Keyphrase: ""Misinformation surface form hallucination"""
arxiv2024,Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective,Yes.,4,"""the absence of explicit explainability in LLMs significantly hinders their application in the social sciences.""",2024,2024-02-01T01:17:46Z,"Keyphrase: ""Lack of explainability"""
