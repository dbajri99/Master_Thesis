Source,Title,Talks about LLMs,Rate,Evidence,Year,Month,Keyphrase,Topic
aacl2022,VLStereoSet: A Study of Stereotypical Bias in Pre-trained Vision-Language Models,Yes.,4,experiment six representative pretraine visionlanguage demonstrate stereotypical bias clearly exist across four bias category gender bias slightly evident stereotypical bias visionlanguage,2022,November,"Keyphrase: ""Stereotypical biases in vision-language models""",1
aacl2022,Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique,Yes.,4,bertlike lm expose unstructured dataset know learn sometimes even amplify bias present bias amplification,2022,November,"Keyphrase: ""Bias amplification""",1
emnlp2022,RankGen: Improving Text Generation with Large Ranking Models,Yes.,5,modern often assign high probability sequence repetitive incoherent irrelevant prefix modelgenerate text also contain artifact repetitive incoherent,2022,December,"Keyphrase: ""Repetitive and incoherent output""",4
emnlp2022,An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models,Yes.,5,show present privacy risk memorization training empirically study memorization finetune use membership inference extraction attack show susceptibility attack different privacy risk susceptibility attack,2022,December,"Keyphrase: ""Privacy risks and susceptibility to attacks""",1
emnlp2022,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,Yes.,5,nominally expose procedural object interact yet benchmarke show fail reason world lack realworld performance,2022,December,"Keyphrase: ""Lack of real-world performance""",0
emnlp2022,Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence,Yes.,5,simulate conflict ie parametric suggest one different passage suggest different examine behavior contradiction among source affect confidence marginally inconsistent behavior,2022,December,"Keyphrase: ""Inconsistent behavior""",0
emnlp2022,SafeText: A Benchmark for Exploring Physical Safety in Language Models,Yes.,5,find stateoftheart susceptible generation unsafe text difficulty reject unsafe advice susceptibility unsafe text,2022,December,"Keyphrase: ""Susceptibility to generating unsafe text""",1
emnlp2022,BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation,Yes.,4,however demonstrate plm encode range stereotypical societal bias lead concern fairness plm metric demonstrate popular plmbase metric exhibit significantly high social bias traditional metric sensitive attribute societal bias encoding,2022,December,"Keyphrase: ""Societal bias encoding""",1
emnlp2022,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,Yes.,5,show one todayãs brown et al lack kind social intelligence outofthe box show struggle substantially theory mind lack social intelligence,2022,December,"Keyphrase: ""Lack of social intelligence""",0
emnlp2022,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,Yes.,5,neural scale suffer poor temporal generalization capability pretraine static past year bad time emerge limited temporal generalization,2022,December,"Keyphrase: ""Limited temporal generalization""",0
emnlp2022,Perturbation Augmentation for Fairer NLP,Yes.,4,unwanted often harmful social bias become ever salient nlp research affect dataset lastly discuss outstanding question good evaluate unfairness unwanted social bias,2022,December,"Keyphrase: ""Unwanted social bias""",1
emnlp2022,"The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",Yes.,5,show three investigate plm able recognise structure cc fail use mean humanlike performance plm many nlp allege indicate plm still suffer substantial shortcoming central domain linguistic limit linguistic understanding,2022,December,"Keyphrase: ""Limited linguistic understanding""",0
emnlp2022,LittleBird: Efficient Faster & Longer Transformer for Question Answering,Yes.,5,limitation deal long due attention mechanism limit handling long dependency,2022,December,"Keyphrase: ""Limited handling of long dependencies""",0
emnlp2022,Mutual Information Alleviates Hallucinations in Abstractive Summarization,Yes.,5,still exhibit tendency hallucinate ie content support source document content hallucination,2022,December,"Keyphrase: ""Content hallucination""",2
emnlp2022,Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,Yes.,5,despite strong performance many pretraine show struggle outofdistribution compositional generalization overall study highlight limitation current technique effectively leverage scale compositional generalization analysis also suggest promise direction future work struggle outofdistribution compositional generalization,2022,December,"Keyphrase: ""Struggles with out-of-distribution compositional generalization""",0
emnlp2022,A Systematic Investigation of Commonsense Knowledge in Large Language Models,Yes.,5,find highlight limitation pretraine lm acquire commonsense without taskspecific supervision furthermore use fewshot evaluation insufficient achieve humanlevel commonsense performance limited acquisition commonsense,2022,December,"Keyphrase: ""Limited acquisition of commonsense""",0
emnlp2022,SEAL: Interactive Tool for Systematic Error Analysis and Labeling,Yes.,5,however many time systematically fail tail rare group obvious aggregate evaluation failure address rare group tail issue,2022,December,"Keyphrase: ""Failure to address rare group tail issues""",0
acl2022,Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts,Yes.,4,humanlike bias undesire social stereotype exist pretraine give wide adoption realworld application mitigate bias become emerge important humanlike bias social stereotype,2022,May,"Keyphrase: ""Humanlike bias and social stereotypes""",1
acl2022,Are Prompt-based Models Clueless?,Yes.,5,taskspecific head require lot training make susceptible learn exploit datasetspecific superficial cue generalize dataset analyze fewshot promptbase mnli snli han copa reveal promptbase also exploit superficial cue well instance superficial cue overreliance superficial cue,2022,May,"Keyphrase: ""Overreliance on superficial cues""",2
acl2022,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Yes.,5,many false mimic popular misconception potential deceive human generally least truthful deceptive false information,2022,May,"Keyphrase: ""Deceptive false information""",2
acl2022,Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models,Yes.,4,investigate bias transfer hypothesis bias transfer,2022,May,"Keyphrase: ""Bias transfer""",1
acl2022,A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation,Yes.,5,pretraine generative like often suffer hallucinate nonexistent incorrect content undermine potential merit real application hallucination nonexistent content,2022,May,"Keyphrase: ""Hallucination of nonexistent content""",2
acl2022,Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text,Yes.,4,error machine generation become ever subtle hard spot ten error category scarecrowãsuch redundancy commonsense error incoherence subtle error incoherence,2022,May,"Keyphrase: ""Subtle errors and incoherence""",4
acl2022,Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task,Yes.,5,exist promptbase technique fail semantic distinction wordincontext wic dataset specifically none exist fewshot approach include incontext learning attain performance meaningfully different random baseline weak semantic understanding,2022,May,"Keyphrase: ""Weak semantic understanding""",0
acl2022,Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions,Yes.,5,however discover single hidden state produce probability distribution regardless lm size training size single hidden state embed close embed possible next word simultaneously interfere word embed interference word embedding,2022,May,"Keyphrase: ""Interference in word embeddings""",0
acl2022,Coherence boosting: When your pretrained language model is not paying enough attention,Yes.,5,demonstrate insufficiently learn effect distant word nexttoken prediction limit learning distant word effect,2022,May,"Keyphrase: ""Limited learning of distant word effects""",0
acl2022,Data Contamination: From Memorization to Exploitation,Yes.,5,clear extent exploit contaminate downstream highlight importance analyze massive webscale dataset verify progress nlp obtain well understand well exploitation limit dataset exploitation,2022,May,"Keyphrase: ""Limited dataset exploitation""",4
acl2022,Kronecker Decomposition for GPT Compression,Yes.,5,despite superior performance overparameterize nature prohibitive deploy device limit computational power memory overparameterization computational constraint,2022,May,"Keyphrase: ""Overparameterization and computational constraints""",3
naacl2022,Provably Confidential Language Modelling,Yes.,5,show memorize privacy information social security number training privacy information memorization,2022,July,"Keyphrase: ""Privacy information memorization""",1
naacl2022,"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",Yes.,5,find certain extent sensitive interaction investigate challenge presence multiple nps behavior systematic suggest even scale fully acquire basic entity track ability limited entity tracking ability,2022,July,"Keyphrase: ""Limited entity tracking ability""",0
naacl2022,Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models,Yes.,4,indicate fairness bias evaluation remain challenge contextualize among reason choice remain subjective subjective fairness evaluation,2022,July,"Keyphrase: ""Subjective fairness evaluation""",1
naacl2022,Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Yes.,5,current evaluation show significant shortcoming fail effectively map aspect understanding remain challenge exist experiment provide insight limitation exist benchmark dataset stateoftheart limited aspect understand,2022,July,"Keyphrase: ""Limited aspect understanding""",0
naacl2022,Exposing the Limits of Video-Text Models through Contrast Sets,Yes.,5,see performance suffer across erase gap recent clipbase vs early performance degradation evolve,2022,July,"Keyphrase: ""Performance degradation with evolving data""",0
naacl2022,KALA: Knowledge-Augmented Language Model Adaptation,Yes.,5,simple finetune plm hand might suboptimal domainspecific possibly cover domain adaptive pretraine plm help obtain domainspecific require training cost moreover adaptive pretraine harm plmãs performance downstream cause catastrophic forget general limited domain adaptation,2022,July,"Keyphrase: ""Limited domain adaptation""",0
naacl2022,You DonÃ¢â‚¬â„¢t Know My Favorite Color: Preventing Dialogue Representations from Revealing SpeakersÃ¢â‚¬â„¢ Private Personas,Yes.,4,privacy concern arise recently privacy concern,2022,July,"Keyphrase: ""Privacy concerns""",1
naacl2022,Methods for Estimating and Improving Robustness of Language Models,Yes.,5,suffer notorious flaw relate preference shallow textual relation full semantic complexity problem weak ability generalise outside training domain weak generalization outside training domain,2022,July,"Keyphrase: ""Weak generalization outside training domain""",4
naacl2022,Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition,Yes.,5,show dialect variation surface form different lexicon grammar occasionally semantic significantly degrade performance lm mismatch condition dialect variation degradation,2022,July,"Keyphrase: ""Dialect variation degradation""",1
acl2023,MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning,Yes.,4,despite advance pretraine neural prone toxic bring security risk application toxicity security risk,2023,July,"Keyphrase: ""Toxicity and security risks""",1
acl2023,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Yes.,5,robustness also call question recent work show rely shallow pattern problem description solution analysis show robustness appear continuously improve function size questionable robustness,2023,July,"Keyphrase: ""Questionable robustness""",2
acl2023,ALERT: Adapt Language Models to Reasoning Tasks,Yes.,5,unclear whether apply reasoning skill learn pretraine simply memorize training corpus fine granularity also find finetune tend overfit prompt template hurt robustness cause generalization overfitting lack reasoning skill,2023,July,"Keyphrase: ""Overfitting and lack of reasoning skills""",0
acl2023,ThinkSum: Probabilistic reasoning over sets using large language models,Yes.,5,recent study show even advanced fail scenario require reason multiple object fact make sequence logical deduction limited reasoning capability,2023,July,"Keyphrase: ""Limited reasoning capabilities""",0
acl2023,Dynamic and Efficient Inference for Text Generation via BERT Family,Yes.,5,suffer inefficient inference computation memory due largescale parameter universal autoregressive decode paradigm inefficient inference computation,2023,July,"Keyphrase: ""Inefficient inference and computation""",3
acl2023,Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model,Yes.,4,exist bias mitigation require socialgroupspecific word pair eg ãåmanã ã ãåwomanã social attribute eg gender restrict bias mitigation one specify social attribute constraint render impractical costly mitigate bias limited bias mitigation option,2023,July,"Keyphrase: ""Limited bias mitigation options""",1
acl2023,"On Second Thought, LetÃ¢â‚¬â„¢s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",Yes.,5,find zeroshot cot reasoning sensitive domain significantly increase modelãs likelihood produce harmful undesirable work suggest zeroshot cot use caution socially important especially marginalize group sensitive topic involve harmful bias sensitive topic,2023,July,"Keyphrase: ""Harmful biases in sensitive topics""",1
acl2023,MISGENDERED: Limits of Large Language Models in Understanding Pronouns,Yes.,5,prompt outofthebox poorly correctly predict neopronoun average accuracy genderneutral pronoun average accuracy inability generalize lack representation nonbinary pronoun training memorize association limited representation nonbinary pronoun,2023,July,"Keyphrase: ""Limited representation of nonbinary pronouns""",0
acl2023,SCOTT: Self-Consistent Chain-of-Thought Distillation,Yes.,4,even concern little guarantee rationale consistent lmãs prediction faithfully justify decision lack rationale consistency,2023,July,"Keyphrase: ""Lack of rationale consistency""",0
acl2023,Evaluating Open-Domain Question Answering in the Era of Large Language Models,Yes.,5,automate struggle detect hallucination thus unable evaluate difficulty detect hallucination,2023,July,"Keyphrase: ""Difficulty in detecting hallucination""",2
acl2023,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,Yes.,5,one fatal disadvantage lack factual correctness unfactual text lead low performance also degrade trust validity application factual correctness deficiency,2023,July,"Keyphrase: ""Factual correctness deficiency""",4
acl2023,Language model acceptability judgements are not always robust to context,Yes.,5,find judgement generally robust place randomly sample linguistic context unstable context match test stimulus syntactic structure sensitivity highly specific syntactic feature context explain modelsã implicit incontext learn ability limited syntactic structure sensitivity,2023,July,"Keyphrase: ""Limited syntactic structure sensitivity""",0
acl2023,RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,Yes.,5,indicate stateoftheart table qa eg fewshot learning falter adversarial set falter fewshot learning,2023,July,"Keyphrase: ""Falter in few-shot learning""",0
acl2023,Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency,Yes.,5,nonlinguistic skill injection typically come cost cost nonlinguistic skill injection,2023,July,"Keyphrase: ""Cost of nonlinguistic skill injection""",0
acl2023,Parallel Context Windows for Large Language Models,Yes.,5,apply process long text limit context window limited context window,2023,July,"Keyphrase: ""Limited context window""",3
acl2023,Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales,Yes.,4,observe human utility exist rationale far satisfactory expensive estimate human study exist metric like performance lm rationale similarity gold rationale good indicator human utility limit human utility estimation,2023,July,"Keyphrase: ""Limited human utility estimation""",0
acl2023,On Ã¢â‚¬Å“Scientific DebtÃ¢â‚¬Â in NLP: A Case for More Rigour in Language Model Pre-Training Research,Yes.,5,current plm research practice often conflate different possible source improvement without conduct proper ablation study principle comparison different comparable condition practice leave illequippe understand pretraine approach use circumstance ii impede reproducibility credit assignment iii render difficult understand lack principled comparison reproducibility,2023,July,Keyphrase: Lack of principled comparison and reproducibility,0
acl2023,WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,Yes.,5,apply benchmark several popular find offtheshelf generally exhibit considerable antiqueer bias antiqueer bias,2023,July,"Keyphrase: ""Antiqueer bias""",1
acl2023,Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,Yes.,5,experiment reveal frequently fail valid sentence ground negative commonsense statistical shortcut negation report bias pretraine cause conflict failure handle negation bias,2023,July,"Keyphrase: ""Failure in handling negation and bias""",1
acl2023,How Do In-Context Examples Affect Compositional Generalization?,Yes.,5,find compositional generalization performance easily affect selection incontext example two strong limitation observe compositional generalization limitation,2023,July,"Keyphrase: ""Compositional generalization limitations""",0
acl2023,Contrastive Decoding: Open-ended Text Generation as Optimization,Yes.,5,maximum probability poor decode objective openende generation produce short repetitive text hand sampling often produce incoherent text drift original topic incoherent text generation,2023,July,"Keyphrase: ""Incoherent text generation""",4
acl2023,CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models,Yes.,4,pretraine conversational agent expose safety issue exhibit range stereotypical human bias gender bias experimental show chinese pretraine potentially risky text contain social bias safety societal bias,2023,July,"Keyphrase: ""Safety and societal biases""",1
acl2023,"RARR: Researching and Revising What Language Models Say, Using Language Models",Yes.,5,however sometimes unsupported misleading content user easily determine whether trustworthy lm builtin mechanism attribution external evidence lack transparency trustworthiness,2023,July,"Keyphrase: ""Lack of transparency and trustworthiness""",4
acl2023,Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,Yes.,4,societal bias present pretraine critical issue show propagate bias countless downstream application render unfair towards specific group people propagate societal bias,2023,July,"Keyphrase: ""Propagating societal bias""",1
acl2023,Probing Physical Reasoning with Counter-Commonsense Context,Yes.,5,show use preposition provide context infer size relationship fail use verb thus make incorrect judgment lead prior physical commonsense lack contextual understanding,2023,July,"Keyphrase: ""Lack of contextual understanding""",4
acl2023,"Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)",Yes.,5,find able summarize simplify single biomedical article faithfully struggle provide accurate aggregation find multiple document limited multidocument aggregation,2023,July,"Keyphrase: ""Limited multi-document aggregation""",3
acl2023,Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning,Yes.,5,know memorize significant portion training part memorize content show extractable simply query pose privacy risk privacy risk due memorization,2023,July,"Keyphrase: ""Privacy risk due to memorization""",1
acl2023,A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification,Yes.,5,benchmark often adequately address challenge pose realworld hierarchical classification observe prone failure case failure realworld hierarchical classification,2023,July,"Keyphrase: ""Failure in real-world hierarchical classification""",3
acl2023,Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section,Yes.,5,context length predictor limit part clinical note choose limited context understanding,2023,July,"Keyphrase: ""Limited context understanding""",3
acl2023,MathPrompter: Mathematical Reasoning using Large Language Models,Yes.,5,limited performance solve arithmetic reasoning often provide incorrect good aware indicate level confidence response fuel trust deficit impede adoption limit arithmetic reasoning performance,2023,July,"Keyphrase: ""Limited arithmetic reasoning performance""",0
acl2023,KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications,Yes.,5,learn natural text generation ability also social bias different demographic group realworld pose critical risk deploy llmbase application limitation require localize social bias dataset ensure safe effective deployment social bias deployment risk,2023,July,"Keyphrase: ""Social bias and deployment risks""",1
acl2023,"Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",Yes.,4,responsible ai issue fairness bias toxicity linguistic diversity evaluation context mmlm specifically focus issue nonenglish lowresource limited focus nonenglish lowresource,2023,July,"Keyphrase: ""Limited focus on non-English and low-resource languages""",1
eacl2023,WinoDict: Probing language models for in-context word acquisition,Yes.,5,benchmark address word acquisition one important aspect diachronic degradation know afflict freeze time moment train normally unable reflect way change time limit diachronic adaptation,2023,May,"Keyphrase: ""Limited diachronic adaptation""",0
eacl2023,Nationality Bias in Text Generation,Yes.,5,paper examine text generation accentuate preexist societal bias countrybase demonym demonstrate significant bias country low internet user adversarial triggering effectively reduce amplification societal bias,2023,May,"Keyphrase: ""Amplification of societal biases""",1
eacl2023,"Ã¢â‚¬Å“John is 50 years old, can his son be 65?Ã¢â‚¬Â Evaluating NLP ModelsÃ¢â‚¬â„¢ Understanding of Feasibility",Yes.,5,recent work also find notable failure often failure example involve complex reasoning ability show even stateoftheart struggle failure complex reasoning,2023,May,"Keyphrase: ""Failure in complex reasoning""",0
eacl2023,MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers,Yes.,5,usability lm constrain computational time complexity along increase size issue refer overparameterisation computational time complexity constraint,2023,May,"Keyphrase: ""Computational time complexity constraints""",3
eacl2023,SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,Yes.,4,common limitation diagnostic test detect social bias nlp may detect stereotypic association prespecifie designer test also test sodapop debiase show limitation limited ability detect social bias,2023,May,"Keyphrase: ""Limited ability to detect social biases""",1
eacl2023,Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding,Yes.,5,however study analyze impact compression generalizability robustness compress outofdistribution ood compress significantly less robust plm counterpart ood test set although obtain similar performance indistribution development set reduce generalizability robustness compression,2023,May,"Keyphrase: ""Reduced generalizability and robustness after compression""",0
eacl2023,Opportunities and Challenges in Neural Dialog Tutoring,Yes.,5,find although current approach tutoring constrain learn scenario number concept teach possible teacher strategy small poorly less constrained scenario human quality evaluation show groundtruth annotation exhibit low performance term equitable tutoring measure learn opportunity limited concept teaching capability,2023,May,"Keyphrase: ""Limited concept teaching capability""",4
eacl2023,Assessing Out-of-Domain Language Model Performance from Few Examples,Yes.,5,pretraine exhibit impressive generalization capability still behave unpredictably certain domain shift give targetdomain example set similar training performance understand ood test unpredictable domain shift,2023,May,"Keyphrase: ""Unpredictable domain shifts""",4
eacl2023,Towards preserving word order importance through Forced Invalidation,Yes.,5,however recent find reveal pretraine insensitive word order performance nlu remain unchanged even randomly permute word sentence crucial syntactic information destroy insensitive word order,2023,May,"Keyphrase: ""Insensitive to word order""",0
eacl2023,Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models,Yes.,4,however safely deploy real world application challenge toxic content toxic content challenge,2023,May,"Keyphrase: ""Toxic content challenges""",1
eacl2023,When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization,Yes.,5,subject sociocultural bias previously identify use intrinsic evaluation however intrinsic bias pretraine lm representation propagate downstream finetune nlp like summarization well understand show bias manifest hallucination summarization lead factually incorrect summary propagation sociocultural bias,2023,May,"Keyphrase: ""Propagation of sociocultural bias""",1
eacl2023,Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,Yes.,4,go beyond enumerate risk harm work provide survey practical address potential threat societal harm generation limited consideration societal harm,2023,May,"Keyphrase: ""Limited consideration of societal harm""",1
emnlp2023,FISH: A Financial Interactive System for Signal Highlighting,Yes.,5,chatgptãs decision sensitive order label prompt chatgpt clearly high chance select label early position decision sensitivity label prompt,2023,December,"Keyphrase: ""Decision sensitivity to label prompts""",0
emnlp2023,A Unified Framework for Emotion Identification and Generation in Dialogues,Yes.,5,experiment dataset show recent eg instructgpt struggle subquestion even able main question correctly find particularly poorly subquestion write incorrect option struggle subquestion,2023,December,"Keyphrase: ""Struggles with subquestions""",0
emnlp2023,Addressing Domain Changes in Task-oriented Conversational Agents through Dialogue Adaptation,Yes.,5,reveal limitation llmbase agentsã plan optimization due systematic failure manage longhorizon contexts hallucination state struggle longhorizon context,2023,December,"Keyphrase: ""Struggles with long-horizon context""",3
emnlp2023,CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models,Yes.,5,find poorly especially word tokenize unfavorably subword tokenization suboptimal tokenization efficiency,2023,December,**Keyphrase:** Suboptimal tokenization efficiency,0
emnlp2023,Conceptual structure coheres in human cognition but not in large language models,Yes.,5,structure estimate behavior individually fairly consistent estimate human behavior depend much upon particular use behavior responsesãresponse three yield estimate conceptual structure cohere less one another human structure inconsistent conceptual structure,2023,December,"Keyphrase: ""Inconsistent conceptual structure""",0
emnlp2023,Towards LLM-driven Dialogue State Tracking,Yes.,5,despite impressive performance chatgpt significant limitation include closedsource nature request restriction raise privacy concern lack local deployment capability closedsource privacy concern,2023,December,"Keyphrase: ""Closed-source and privacy concerns""",3
emnlp2023,WeÃ¢â‚¬â„¢re Afraid Language Models ArenÃ¢â‚¬â„¢t Modeling Ambiguity,Yes.,5,find remain extremely challenging include whose disambiguation consider correct time crowdworker evaluation compare disambiguation dataset challenge disambiguation accuracy,2023,December,"Keyphrase: ""Challenges in disambiguation accuracy""",2
emnlp2023,Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus,Yes.,5,however prone hallucinate untruthful nonsensical fail meet user expectation many realworld application untruthful hallucination,2023,December,"Keyphrase: ""Untruthful hallucinations""",2
emnlp2023,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,Yes.,5,however exist code two main limitation first often adopt specific architecture encoderonly decoderonly rely unify encoderdecod network different downstream lack flexibility operate optimal architecture specific secondly often employ limit set pretraine objective might relevant limited architecture flexibility pretraine objective,2023,December,"Keyphrase: ""Limited architecture flexibility and pretraining objectives""",3
emnlp2023,Unveiling the Implicit Toxicity in Large Language Models,Yes.,5,show diverse implicit toxic exceptionally difficult detect via simply zeroshot prompt finding suggest pose significant threat undetectable implicit toxic difficulty detect implicit toxic content,2023,December,"Keyphrase: ""Difficulty in detecting implicit toxic content""",1
emnlp2023,ALCUNA: Large Language Models Meet New Knowledge,Yes.,5,benchmark several reveal performance face new satisfactory particularly reason new internal limited reasoning capability,2023,December,"Keyphrase: ""Limited reasoning capabilities""",0
emnlp2023,Robust Prompt Optimization for Large Language Models Against Distribution Shifts,Yes.,5,reveal prompt optimization technique vulnerable distribution shift subpopulation shift common realworld scenario customer review analysis vulnerability distribution shift,2023,December,"Keyphrase: ""Vulnerability to distribution shift""",1
emnlp2023,Interpreting Embedding Spaces by Conceptualization,Yes.,4,one major drawback type representation incomprehensibility human understand embed space crucial several important need include need debug embed compare alternative need detect bias hide incomprehensibility bias detection,2023,December,"Keyphrase: ""Incomprehensibility and bias detection""",0
emnlp2023,Knowledge-Augmented Language Model Verification,Yes.,5,yet lm often factually incorrect response give query since may inaccurate incomplete outdated may fail retrieve relevant give query may faithfully reflect retrieve text poor factual accuracy,2023,December,"Keyphrase: ""Poor factual accuracy""",4
emnlp2023,Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation,Yes.,5,however due inability capture relationship among sample frozen inevitably keep repeat similar mistake limit relationship capture,2023,December,"Keyphrase: ""Limited relationship capture""",0
emnlp2023,Ã¢â‚¬Å“Fifty Shades of BiasÃ¢â‚¬Â: Normative Ratings of Gender Bias in GPT Generated English Text,Yes.,4,increasingly gain humanlike fluency text generation gain nuance understand bias imperative bias understanding,2023,December,"Keyphrase: ""Biased understanding""",1
emnlp2023,MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations,Yes.,4,cutoff costly finetune repeatedly find also highlight need improvement particularly interpret unfamiliar word compose multiple novel interpretation simultaneously example difficulty interpret unfamiliar word,2023,December,"Keyphrase: ""Difficulty in interpreting unfamiliar words""",0
emnlp2023,Instructed Language Models with Retrievers Are Powerful Entity Linkers,Yes.,5,generative nature still make content suffer hallucination thus unsuitable entitycentric like entity link el require precise entity prediction base el remain persistent hurdle general hallucination generative content,2023,December,"Keyphrase: ""Hallucination in generative content""",2
emnlp2023,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",Yes.,4,unclear collect incorporate feedback way efficient effective unbiased especially highly subjective human preference value encourage well future feedback learn raise five unresolved conceptual practical challenge challenge feedback incorporation,2023,December,"Keyphrase: ""Challenges in feedback incorporation""",0
emnlp2023,"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",Yes.,5,issue hallucination parallelly emerge byproduct pose significant concern propose two solution strategy mitigate hallucination hallucination issue,2023,December,"Keyphrase: ""Hallucination issues""",2
emnlp2023,The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages,Yes.,5,comprehensive analysis reveal exist opensource instruction tune still struggle understand sm across various close random baseline case also find although chatgpt outperform many still fall behind taskspecific finetune gap sparrow score struggle taskspecific finetuning,2023,December,"Keyphrase: ""Struggles with task-specific fine-tuning""",4
emnlp2023,Understanding the Effect of Model Compression on Social Bias in Large Language Models,Yes.,4,train selfsupervision vast corpora web text fit social bias text without intervention social bias persist modelãs prediction downstream lead representational harm persistent social bias,2023,December,"Keyphrase: ""Persistent social biases""",1
emnlp2023,Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation,Yes.,5,hallucination text ungrounde wellknown problem neural datatotext generation ungrounded hallucination,2023,December,"Keyphrase: ""Ungrounded hallucinations""",2
emnlp2023,API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs,Yes.,4,however three pivotal question remain unanswered unanswered pivotal question,2023,December,"Keyphrase: ""Unanswered pivotal questions""",
emnlp2023,Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies,Yes.,5,find largely recover syntacticstyle shift recover vocabulary misalignment embed matrix reinitialization even continue pretraine million token difficulty recover syntax vocabulary alignment,2023,December,"Keyphrase: ""Difficulty in recovering syntax and vocabulary alignment""",0
emnlp2023,The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models,Yes.,5,primary issue arise context management unanswerable query often hallucinatory behavior due overconfidence overconfidence lead hallucinatory behavior,2023,December,"Keyphrase: ""Overconfidence leading to hallucinatory behavior""",2
emnlp2023,ROBBIE: Robust Bias Evaluation of Large Generative Language Models,Yes.,4,must develop comprehensive enough tool measure improve fairness testing dataset potentially help characterize bias fully explore frequency demographic term common pretraine corpora may relate bias limited fairness testing bias characterization,2023,December,"Keyphrase: ""Limited fairness testing and bias characterization""",1
emnlp2023,Adapting Language Models to Compress Contexts,Yes.,5,transformerbase lm powerful widelyapplicable tool usefulness constrain finite context window expensive computational cost process long text document limited context window,2023,December,"Keyphrase: ""Limited context window""",3
emnlp2023,Large Language Models are Temporal and Causal Reasoners for Video Question Answering,Yes.,5,however prior often cause suboptimal videoqa lead overrely question ie linguistic bias ignore visual content also know ãungrounded guessesã ãhallucinationsã linguistic bias ungrounded guess,2023,December,"Keyphrase: ""Linguistic bias and ungrounded guesses""",2
emnlp2023,Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models,Yes.,5,show ability memorize reproduce portion training prompt adversary prior research focus address memorization issue prevent verbatim replication technique like unlearn preprocesse however limitation regard number protect sample limit privacy type potentially lowerquality generative memorization privacy concern,2023,December,"Keyphrase: ""Memorization and privacy concerns""",1
emnlp2023,Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis,Yes.,5,find sensitive certain perturbation replace word synonym sensitivity word perturbation,2023,December,"Keyphrase: ""Sensitivity to word perturbations""",0
emnlp2023,Can Large Language Models Capture Dissenting Human Voices?,Yes.,5,show exhibit limited ability solve nli simultaneously fail capture human disagreement distribution inference human alignment performance plunge even sample high human disagreement level raise concern natural understand nlu ability representativeness human population limited ability capture human disagreement,2023,December,"Keyphrase: ""Limited ability in capturing human disagreement""",0
emnlp2023,Merging Generated and Retrieved Knowledge for Open-Domain QA,Yes.,4,retrieve passage give source know suffer insufficient coverage tend hallucinate content conflict retrieve insufficient coverage tendency hallucinate,2023,December,"Keyphrase: ""Insufficient coverage and tendency to hallucinate""",2
emnlp2023,Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition,Yes.,5,deployment increasingly plague prompt injection jailbreake collectively prompt hack manipulate ignore original instruction instead follow potentially malicious one vulnerability manipulation,2023,December,"Keyphrase: ""Vulnerability to manipulation""",1
emnlp2023,Prompting is not a substitute for probability measurements in large language models,Yes.,5,broadly find llmsã metalinguistic judgment inferior quantity directly derive representation furthermore consistency get bad prompt query diverge direct measurement nextword probability inferior metalinguistic judgment,2023,December,"Keyphrase: ""Inferior metalinguistic judgment""",0
emnlp2023,"Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",Yes.,5,many question correctly also hallucinate give wrong hallucination incorrect,2023,December,"Keyphrase: ""Hallucinations and incorrect answers""",2
emnlp2023,Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators,Yes.,4,community concern abound regard factuality potential implication use uncensored surprisingly study reveal factuality even low significantly hinder downstream factuality concern,2023,December,"Keyphrase: ""Factuality concerns""",4
emnlp2023,Compressing Context to Enhance Inference Efficiency of Large Language Models,Yes.,5,however face challenge manage long document extend conversation due significantly increase computational requirement memory inference time potential context truncation exceed llmãs fix context length challenge long document extended conversation,2023,December,"Keyphrase: ""Challenges with long documents and extended conversations""",3
emnlp2023,Can You Follow Me? Testing Situational Understanding for ChatGPT,Yes.,5,previous work identify certain su limitation nonchatbot find despite fundamental simplicity modelãs performance reflect inability retain correct environment state across time performance degradation largely chatgpt nonpersistent incontext memory although access full dialogue history susceptible memory limitation,2023,December,"Keyphrase: ""Memory limitations""",3
emnlp2023,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Yes.,5,chatgpt prone hallucination ie content conflict source verify factual moreover exist face great challenge recognize hallucination text hallucination factual inaccuracy,2023,December,"Keyphrase: ""Hallucination and factual inaccuracies""",2
emnlp2023,Enabling Large Language Models to Generate Text with Citations,Yes.,5,prone hallucination current considerable room improvementãfor example eli dataset even good lack complete citation support time incomplete citation support,2023,December,"Keyphrase: ""Incomplete citation support""",2
emnlp2023,Counting the Bugs in ChatGPTÃ¢â‚¬â„¢s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,Yes.,5,find chatgpt massively underperform purposebuilt particularly english overall resultsãthrough lens morphologyãcast new light linguistic capability chatgpt suggest claim humanlike skill premature misleading underperformance specialized,2023,December,"Keyphrase: ""Underperformance in specialized tasks""",3
emnlp2023,MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models,Yes.,5,however come natural reasoning lm still face challenge hallucination incorrect intermediate reasoning step make mathematical error challenge natural reasoning,2023,December,"Keyphrase: ""Challenges in natural reasoning""",0
emnlp2023,Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models,Yes.,5,find reveal nuance depiction capability limitation within temporal reasoning offer comprehensive reference future research pivotal domain limitation temporal reasoning,2023,December,"Keyphrase: ""Limitation in temporal reasoning""",0
emnlp2023,Evaluation of African American Language Bias in Natural Language Generation,Yes.,4,present evidence dialectal bias six pretraine performance gap dialectal bias,2023,December,"Keyphrase: ""Dialectal bias""",1
emnlp2023,An Investigation of LLMsÃ¢â‚¬â„¢ Inefficacy in Understanding Converse Relations,Yes.,5,suggest often resort shortcut learning still face challenge propose benchmark reliance shortcut,2023,December,"Keyphrase: ""Reliance on shortcuts""",0
emnlp2023,HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies,Yes.,5,myriad different face common challenge contextually analyze table questionanswere challenge engender finite context window table multifacete discrepancy amongst tokenization pattern cell boundary various limitation stem confidentiality process use external limit contextual analysis,2023,December,"Keyphrase: ""Limited contextual analysis""",0
emnlp2023,"Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",Yes.,4,ability memorize unknown set book complicate assessment measurement validity cultural analytic contaminate test show much well memorize book nonmemorize book downstream overreliance memorization,2023,December,"Keyphrase: ""Over-reliance on memorization""",1
emnlp2023,Copyright Violations and Large Language Models,Yes.,5,work explore issue copyright violation lens verbatim memorization focus possible redistribution copyright text risk copyright violation,2023,December,"Keyphrase: ""Risk of copyright violation""",1
emnlp2023,Symbolic Planning and Code Generation for Grounded Dialogue,Yes.,5,limit applicability ground taskoriente dialogue difficult steer toward objective fail handle novel ground difficulty handle novel grounding,2023,December,"Keyphrase: ""Difficulty in handling novel grounding""",0
emnlp2023,Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models,Yes.,5,typical failure mode good error algebraic manipulation difficulty ground abstract concept mathematical equation accurately failure retrieve relevant domainspecific concept struggle domainspecific concept,2023,December,"Keyphrase: ""Struggles with domain-specific concepts""",3
emnlp2023,Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization,Yes.,4,training process generally incur update significant parameter limit applicability fl technique tackle real scenario prompt tune significantly reduce number parameter update either incur performance degradation low training efficiency decentralize generally nonindependent identically distribute limited training efficiency,2023,December,"Keyphrase: ""Limited training efficiency""",3
emnlp2023,Active Retrieval Augmented Generation,Yes.,5,despite remarkable ability lm comprehend tendency hallucinate create factually inaccurate factually inaccurate hallucination,2023,December,"Keyphrase: ""Factually inaccurate hallucinations""",2
emnlp2023,Reasoning with Language Model is Planning with World Model,Yes.,5,however still struggle problem easy human action plan execute complex math logical reasoning due llmsã absence internal world predict world states eg environment status variable value simulate limit logical reasoning,2023,December,"Keyphrase: ""Limited logical reasoning""",0
emnlp2023,SOUL: Towards Sentiment and Opinion Understanding of Language,Yes.,5,experimental indicate soul challenge small performance gap compare human performance furthermore evaluation conduct human expert highlight limitation small reasoningbase justification limited reasoning justification,2023,December,"Keyphrase: ""Limited reasoning and justification""",0
emnlp2023,Detecting and Mitigating Hallucinations in Multilingual Summarisation,Yes.,5,hallucination pose significant challenge reliability neural abstractive summarisation assess broad range multilingual find tend hallucinate often different english hallucination abstractive summarization,2023,December,"Keyphrase: ""Hallucination in abstractive summarization""",2
emnlp2023,EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs,Yes.,5,however expensive computation high memory requirement prohibitive deployment quantize calibrate use sample training might affect generalization quantize unknown case high computational cost memory requirement,2023,December,"Keyphrase: ""High computational cost and memory requirements""",3
emnlp2023,EpiK-Eval: Evaluation for Language Models as Epistemic Models,Yes.,5,evaluation across various reveal significant weakness domain contend shortcoming stem intrinsic nature prevail training objective weakness domain adaptation,2023,December,"Keyphrase: ""Weakness in domain adaptation""",0
emnlp2023,SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization,Yes.,5,struggle summedit performance close random chance bestperforme still estimate human performance highlight gap llmsã ability reason fact detect inconsistency occur limited reasoning ability,2023,December,"Keyphrase: ""Limited reasoning ability""",0
emnlp2023,Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models,Yes.,4,show evidence speaker number support overcharge obtain poor lack context understanding,2023,December,"Keyphrase: ""Lack of context understanding""",4
emnlp2023,Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,Yes.,5,however sometimes inaccurate potentially mislead text hallucination omission healthcare make unusable well dangerous bad also raise concern regard confidently compose inaccurate potential downstream harm include decrease accountability proliferation lowquality review inaccuracy potential danger,2023,December,"Keyphrase: ""Inaccuracies and potential dangers""",2
emnlp2023,Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT,Yes.,5,summarize discuss challenge face include cluster domainspecific understanding crossdomain incontext learn scenario domainspecific understanding challenge,2023,December,"Keyphrase: ""Domain-specific understanding challenges""",0
emnlp2023,Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting,Yes.,5,crucial challenge generative diversity lack generative diversity,2023,December,"Keyphrase: ""Lack of generative diversity""",
emnlp2023,Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection,Yes.,4,utilization carry inherent risk include limit plagiarism dissemination fake news issue educational exercise exist detector easily circumvent use straightforward automatic adversarial attack vulnerability adversarial attack,2023,December,"Keyphrase: ""Vulnerability to adversarial attacks""",1
emnlp2023,Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,Yes.,5,effectiveness llmgenerate synthetic support training inconsistent across different classification subjectivity level instance level negatively associate performance train synthetic inconsistent synthetic training effectiveness,2023,December,"Keyphrase: ""Inconsistent synthetic training effectiveness""",0
emnlp2023,Learning from Mistakes via Cooperative Study Assistant for Large Language Models,Yes.,5,however feedback often inaccurate thereby limit benefit inaccurate feedback,2023,December,"Keyphrase: ""Inaccurate feedback""",4
emnlp2023,Conceptor-Aided Debiasing of Large Language Models,Yes.,5,pretraine reflect inherent social bias training corpus many propose mitigate issue often fail debias sacrifice accuracy inherent social bias,2023,December,"Keyphrase: ""Inherent social bias""",1
emnlp2023,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,Yes.,5,grow concern simulation flatten caricature persona aim simulate fail capture multidimensionality people perpetuate stereotype simplistic caricature,2023,December,"Keyphrase: ""Simplistic caricatures""",1
emnlp2023,An Empirical Study of Translation Hypothesis Ensembling with Large Language Models,Yes.,4,become onefitsmany solution sometimes hallucinate produce unreliable unreliable hallucination,2023,December,"Keyphrase: ""Unreliable hallucinations""",2
emnlp2023,FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation,Yes.,5,evaluate factuality longform text lm nontrivial generation often contain mixture support unsupported piece information make binary judgment quality inadequate human evaluation timeconsume mixed factual accuracy,2023,December,"Keyphrase: ""Mixed factual accuracy""",4
emnlp2023,Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems,Yes.,5,notoriously inclined make factual error require arithmetic computation factual error arithmetic computation,2023,December,"Keyphrase: ""Factual errors in arithmetic computation""",2
emnlp2023,StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models,Yes.,4,observe encode perpetuate harmful association present training study contribute understanding perceive represent social group shed light potential bias perpetuation harmful association perpetuation harmful association,2023,December,"Keyphrase: ""Perpetuation of harmful associations""",1
emnlp2023,Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding,Yes.,5,critically assess three point recur critique capacity limited capacity critical analysis,2023,December,"Keyphrase: ""Limited capacity for critical analysis""",0
emnlp2023,Question Answering as Programming for Solving Time-Sensitive Questions,Yes.,5,experiment reveal aforementioned problem still pose significant challenge exist attribute llmsã inability rigorous reasoning base surfacelevel text semantic limited reasoning ability,2023,December,"Keyphrase: ""Limited reasoning ability""",0
emnlp2023,Context Compression for Auto-regressive Transformers with Sentinel Tokens,Yes.,5,quadratic complexity attention module make gradually become bulk compute transformerbase generation moreover excessive keyvalue cache arise deal long also bring severe issue memory footprint inference latency quadratic attention complexity,2023,December,"Keyphrase: ""Quadratic attention complexity""",3
emnlp2023,MoPe: Model Perturbation based Privacy Attacks on Language Models,Yes.,5,recent work show unintentionally leak sensitive information present training sensitive information leakage,2023,December,"Keyphrase: ""Sensitive information leakage""",1
emnlp2023,Empower Nested Boolean Logic via Self-Supervised Curriculum Learning,Yes.,5,find pretraine even include behave like random selector face multineste boolean logic human handle ease lack logical reasoning,2023,December,"Keyphrase: ""Lack of logical reasoning""",0
emnlp2023,KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection,Yes.,5,demonstrate remarkable humanlevel natural generation capability however potential misinformation often call hallucination problem pose significant risk deployment misinformation hallucination risk,2023,December,"Keyphrase: ""Misinformation and hallucination risk""",2
emnlp2023,Language Models with Rationality,Yes.,5,lack interpretability grow impediment widespread use resolve inconsistency may exist lack interpretability,2023,December,"Keyphrase: ""Lack of interpretability""",
emnlp2023,Mitigating Temporal Misalignment by Discarding Outdated Facts,Yes.,5,able retain vast amount world see pretraine prone go date nontrivial update difficulty stay uptodate,2023,December,"Keyphrase: ""Difficulty in staying up-to-date""",3
emnlp2023,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,Yes.,5,show fantom challenge stateoftheart significantly bad human even chainofthought reasoning finetune challenge chainofthought reasoning,2023,December,"Keyphrase: ""Challenges in chain-of-thought reasoning""",0
emnlp2023,Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness,Yes.,5,reveal pass prompt bias detriment correctness prompt bias affect correctness,2023,December,"Keyphrase: ""Prompt bias affecting correctness""",1
emnlp2023,CRAB: Assessing the Strength of Causal Relationships Between Real-world Events,Yes.,5,find bad causal reasoning event derive complex causal structure compare simple linear causal chain limited causal reasoning,2023,December,"Keyphrase: ""Limited causal reasoning""",0
emnlp2023,Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation,Yes.,5,propensity inaccurate nonfactual content term ãåhallucinationsã remain significant challenge inaccurate hallucination,2023,December,"Keyphrase: ""Inaccurate hallucinations""",2
emnlp2023,Consistency Analysis of ChatGPT,Yes.,5,prompt design fewshot learning employ unlikely ultimate solution resolve inconsistency issue limit fewshot learning capability,2023,December,"Keyphrase: ""Limited few-shot learning capabilities""",3
emnlp2023,Mitigating Societal Harms in Large Language Models,Yes.,5,provide overview potential social issue generation include toxicity social bias misinformation factual inconsistency privacy violation social issue bias,2023,December,"Keyphrase: ""Social issues and biases""",1
emnlp2023,H2O Open Ecosystem for State-of-the-art Large Language Models,Yes.,5,represent revolution ai however also pose many significant risk presence bias private copyright harmful text biased harmful content,2023,December,"Keyphrase: ""Biased and harmful content""",1
emnlp2023,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,Yes.,5,llmsã inability attribute claim external tendency hallucinate make difficult rely response difficulty attribute claim,2023,December,"Keyphrase: ""Difficulty in attributing claims""",2
emnlp2023,CLEVA: Chinese Language Models EVAluation Platform,Yes.,4,absence comprehensive chinese benchmark thoroughly assess modelãs performance unstandardize incomparable prompting procedure prevalent risk contamination pose major challenge current evaluation chinese lack standardized evaluation,2023,December,"Keyphrase: ""Lack of standardized evaluation""",0
emnlp2023,LM-Polygraph: Uncertainty Estimation for Language Models,Yes.,5,however significant challenge arise often ãåhallucinateã ie fabricate fact without provide user apparent mean discern veracity statement fabricating fact,2023,December,"Keyphrase: ""Fabricating facts""",2
emnlp2023,CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering,Yes.,5,leverage domainspecific question suffer severe limitation tend hallucinate due training collection time use offtheshelf complex user utterance wrong retrieval retrievalaugmente generation furthermore due hallucination domainspecific question,2023,December,"Keyphrase: ""Hallucination in domain-specific questions""",2
emnlp2023,DELPHI: Data for Evaluating LLMsÃ¢â‚¬â„¢ Performance in Handling Controversial Issues,Yes.,4,dataset present challenge concern recency safety fairness bias challenge recency safety fairness bias,2023,December,"Keyphrase: ""Challenges with recency, safety, fairness, and bias""",1
naacl2024,Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study,Yes.,5,despite advancement remain open question whether fundamentally capable reasoning planning primarily rely recall synthesize information training experiment include trial advanced indicate possess foundational ability require struggle integrate coherent limited reasoning planning capability,2024,June,"Keyphrase: ""Limited reasoning and planning capabilities""",0
naacl2024,Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,Yes.,5,even bestperforme unable demonstrate strong visual reasoning capability consistency indicate substantial effort require enable vlm visual reasoning systematically consistently human weak visual reasoning,2024,June,"Keyphrase: ""Weak visual reasoning""",0
naacl2024,Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?,Yes.,5,show exist still far perfect term grasp factual especially fact torsototail entity limited factual understanding,2024,June,"Keyphrase: ""Limited factual understanding""",4
naacl2024,Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles,Yes.,5,analysis suggest despite extraordinary capability singledocument summarization propose remain complex challenge mainly due limited coverage able cover diverse information average limited coverage,2024,June,"Keyphrase: ""Limited coverage""",0
naacl2024,Assessing Factual Reliability of Large Language Model Knowledge,Yes.,5,factual typically evaluate use accuracy yet metric capture vulnerability hallucinationinduce factor like prompt context variability vulnerability hallucinationinduce factor,2024,June,"Keyphrase: ""Vulnerability to hallucination-inducing factors""",2
naacl2024,A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,Yes.,5,despite significant advancement make still struggle complex logical reasoning problem main finding suggest exist could struggle identify fallacious reasoning step accurately may fall short guarantee validity selfverification struggle complex logical reasoning,2024,June,"Keyphrase: ""Struggles with complex logical reasoning""",0
naacl2024,On Large Language ModelsÃ¢â‚¬â„¢ Hallucination with Regard to Known Facts,Yes.,5,successful factoid question also prone hallucination investigate phenomenon possess correct yet still hallucinate perspective inference dynamic hallucination tendency,2024,June,"Keyphrase: ""Hallucination tendency""",2
naacl2024,"Language Models Hallucinate, but May Excel at Fact Verification",Yes.,5,frequently ãåhallucinateã nonfactual analyze reliance highquality evidence well deficiency robustness generalization ability nonfactual hallucination poor generalization,2024,June,"Keyphrase: ""Nonfactual hallucinations and poor generalization""",2
naacl2024,Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models,Yes.,5,exhibit impressive capability also present risk bias content generation privacy issue biased content generation,2024,June,"Keyphrase: ""Biased content generation""",1
naacl2024,"E5: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain, Extract, Execute, Exhibit and Extrapolate",Yes.,4,application hierarchical table constrain reliance manually curate exemplar modelãs token capacity limitation token capacity limitation,2024,June,"Keyphrase: ""Token capacity limitation""",0
naacl2024,"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Model",Yes.,5,rapid development lead great stride capability like longcontext understanding reasoning however able process long context become challenge evaluate whether acquire certain capability since length challenge process long context,2024,June,"Keyphrase: ""Challenges in processing long contexts""",3
naacl2024,MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning,Yes.,4,extensive experiment mmcbenchmark reveal limitation exist lmms correctly interpret chart even recent gptv difficulty interpret chart,2024,June,"Keyphrase: ""Difficulty in interpreting charts""",0
naacl2024,Large Language Models Help Humans Verify Truthfulness Ã¢â‚¬â€œ Except When They Are Convincingly Wrong,Yes.,5,user read explanation significantly efficient use search engine achieve similar accuracy however overrely explanation wrong natural explanation may reliable replacement read retrieve overreliance unreliable explanation,2024,June,"Keyphrase: ""Overreliance on unreliable explanations""",4
naacl2024,SELF-GUARD: Empower the LLM to Safeguard Itself,Yes.,4,safety training involve finetune adversarial sample activate llmãs capability jailbreak however always effective counter new attack often lead potential performance degradation safeguard hand use vulnerability adversarial attack,2024,June,"Keyphrase: ""Vulnerability to adversarial attacks""",1
naacl2024,MART: Improving LLM Safety with Multi-round Automatic Red-Teaming,Yes.,4,redteame common practice mitigate unsafe behavior involve thoroughly assess identify potential flaw address responsible accurate response effective manual redteame costly exist automatic redteame typically discover safety risk limited ability mitigate unsafe behavior,2024,June,"Keyphrase: ""Limited ability to mitigate unsafe behavior""",1
naacl2024,Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings,Yes.,5,experiment reveal lack specificity,2024,June,Keyphrase: Lack of specificity,0
naacl2024,The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth,Yes.,5,however tend generic empathetic enough lack personalization nonreliable potentially harmful advice lack personalization,2024,June,"Keyphrase: ""Lack of personalization""",4
naacl2024,ReTA: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large Language Models,Yes.,5,experimental demonstrate exist stateoftheart reasoning scheme largely ineffective strategic reasoning ineffective strategic reasoning,2024,June,"Keyphrase: ""Ineffective strategic reasoning""",0
naacl2024,"First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models",Yes.,4,argue disparity scale transient researcher work reduce rather hardware still bottleneck many application meaningful realistic evaluation still open problem still room speculative approach lack realistic evaluation,2024,June,"Keyphrase: ""Lack of realistic evaluation""",0
naacl2024,Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models,Yes.,5,find subtle alteration surface form significantly impact distribution solve rate expose modelãs lack robustness sensitivity surface form reason complex problem lack robustness subtle alteration,2024,June,"Keyphrase: ""Lack of robustness to subtle alterations""",0
naacl2024,Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks,Yes.,4,localization never systematically directly evaluate even successful identify neuron specific single memorized sequence limited localization evaluation,2024,June,"Keyphrase: ""Limited localization evaluation""",0
naacl2024,Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications,Yes.,5,show exhibit harmful social bias reflect stereotype inequality present society tend inherit social bias training significantly impact fairness tabular classification social harmful social bias,2024,June,"Keyphrase: ""Harmful social bias""",1
naacl2024,Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks,Yes.,5,evaluation demonstrate limitation current especially ultralongcontext set limitation ultralong context set,2024,June,"Keyphrase: ""Limitation in ultralong context setting""",3
naacl2024,TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction,Yes.,5,frequently incorrect response base madeup fact call hallucination fabricate response,2024,June,"Keyphrase: ""Fabricated responses""",2
naacl2024,On-the-fly Definition Augmentation of LLMs for Biomedical NER,Yes.,5,despite general capability still struggle biomedical ner difficult due presence specialized terminology lack training struggle specialized terminology,2024,June,"Keyphrase: ""Struggles with specialized terminology""",4
naacl2024,"This Land is Your, My Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes",Yes.,5,show recall certain geographical inconsistently query different languagesãa phenomenon term geopolitical bias use propose metric discover numerous inconsistency respond different highlight brittle tailor response depend cue interaction context geopolitical bias inconsistency,2024,June,"Keyphrase: ""Geopolitical bias and inconsistency""",4
naacl2024,Towards Improved Multi-Source Attribution for Long-Form Answer Generation,Yes.,5,current struggle attribution longform response require reason multiple evidence source limited reasoning multiple evidence source,2024,June,"Keyphrase: ""Limited reasoning with multiple evidence sources""",0
naacl2024,Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey,Yes.,5,contemporary prone produce hallucination stem mainly gap within hallucination tendency,2024,June,"Keyphrase: ""Hallucination tendency""",2
naacl2024,LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models,Yes.,5,todayãs typically train short text segment eg k tokens due quadratic complexity transformer architecture performance suffer drastically long encounter training substantially limit application realworld limitation handle long text segment,2024,June,"Keyphrase: ""Limitation in handling long text segments""",3
naacl2024,Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding,Yes.,5,tend inadequately integrate context text generation rely excessively encode prior parameter potentially text factual inconsistency contextually unfaithful content contextual inconsistency,2024,June,"Keyphrase: ""Contextual inconsistency""",4
naacl2024,Fixing Rogue Memorization in Many-to-One Multilingual Translators of Extremely-Low-Resource Languages by Rephrasing Training Samples,Yes.,5,however also find manytoone multilingual tendency learn rogue strategy store string training structure retrieve instead actual translation rogue translation strategy,2024,June,"Keyphrase: ""Rogue translation strategy""",4
naacl2024,Flames: Benchmarking Value Alignment of LLMs in Chinese,Yes.,4,still significant gap llmsã deep alignment human value achieve genuine harmlessness finding indicate evaluate demonstrate relatively poor performance flame particularly safety fairness dimension poor alignment human value,2024,June,"Keyphrase: ""Poor alignment with human values""",0
naacl2024,Effective Long-Context Scaling of Foundation Models,Yes.,5,delve llamaãs position encoding discuss key limitation long limited position encoding,2024,June,"Keyphrase: ""Limited position encoding""",3
naacl2024,Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback,Yes.,5,often bias contain offensive toxic stereotypical text biased toxic text,2024,June,"Keyphrase: ""Biased and toxic text""",1
naacl2024,Fake Alignment: Are LLMs Really Aligned Well?,Yes.,5,study investigate underexplored issue evaluation namely substantial discrepancy performance multiplechoice question openende question discrepancy evaluation performance,2024,June,"Keyphrase: ""Discrepancy in evaluation performance""",2
naacl2024,You donÃ¢â‚¬â„¢t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments,Yes.,5,experiment different reveal even simple perturbation significantly downgrade modelãs questionanswere ability low negation consistency low negation consistency,2024,June,"Keyphrase: ""Low negation consistency""",0
naacl2024,MacGyver: Are Large Language Models Creative Problem Solvers?,Yes.,5,contrast expose variety specialize attempt broad problem fail propose physicallyinfeasible action finally provide detailed error analysis demonstrate potential enhance problemsolving ability novel prompt technique iterative step limited problemsolving ability,2024,June,"Keyphrase: ""Limited problem-solving ability""",0
naacl2024,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models,Yes.,5,without proper safeguard readily follow malicious instruction toxic content use test suite highlight systematic failure mode stateoftheart well general challenge build safe vulnerability malicious instruction,2024,June,"Keyphrase: ""Vulnerability to malicious instructions""",1
naacl2024,Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense,Yes.,5,significant discrepancy performance test culturespecific commonsense different culture llmsã general commonsense capability affect cultural context use query impact performance culturalrelate cultural bias commonsense understanding,2024,June,"Keyphrase: ""Cultural bias in commonsense understanding""",0
naacl2024,Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers,Yes.,4,however fairness remain largely unexplored analysis delve handle query document relate attribute aim uncover bias rank algorithm unexplored fairness issue,2024,June,"Keyphrase: ""Unexplored fairness issues""",1
naacl2024,RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models,Yes.,5,chainofthought cot impressively unlock reasoning potential yet fall short tackle problem require multiple reasoning step limitation arise complex nature multistep reasoning process struggle multistep reasoning,2024,June,"Keyphrase: ""Struggles with multi-step reasoning""",0
naacl2024,Effective Large Language Model Adaptation for Improved Grounding and Citation Generation,Yes.,4,however one major issue towards widespread deployment real world hallucinate factual hallucination factual inaccuracy,2024,June,"Keyphrase: ""Hallucination and factual inaccuracies""",2
naacl2024,Global Gallery: The Fine Art of Painting Culture Portraits through Multilingual Instruction Tuning,Yes.,4,also uncover inconsistency bias particularly nonwestern culture inconsistency bias nonwestern culture,2024,June,"Keyphrase: ""Inconsistency and bias in non-Western cultures""",1
naacl2024,ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models,Yes.,5,good outperform human find still unreliable struggle selfcontradiction require nuance context unreliable selfcontradiction,2024,June,"Keyphrase: ""Unreliable self-contradiction""",4
naacl2024,A Survey of Confidence Estimation and Calibration in Large Language Models,Yes.,4,despite impressive performance unreliable due factual error generation outline challenge summarize recent technical advancement confidence estimation calibration unreliable factual error generation,2024,June,"Keyphrase: ""Unreliable factual error generation""",4
naacl2024,"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",Yes.,4,however risk misuse harmful response raise serious societal concern spur recent research conversation safety risk harmful response,2024,June,"Keyphrase: ""Risk of harmful responses""",1
naacl2024,MindÃ¢â‚¬â„¢s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models,Yes.,4,massive scale computational demand present formidable challenge consider practical deployment resourceconstraine environment risk distil slm may still inherit flawed reasoning hallucination computational demand flawed reasoning,2024,June,"Keyphrase: ""Computational demands and flawed reasoning""",3
naacl2024,Beyond Performance: Quantifying and Mitigating Label Bias in LLMs,Yes.,4,however recent work reveal also exhibit label biasãan undesirable preference toward predict certain investigation reveal substantial label bias debiase attempt emphasize label bias prediction remain barrier reliability label bias,2024,June,"Keyphrase: ""Label bias""",1
naacl2024,Instructing Large Language Models to Identify and Ignore Irrelevant Conditions,Yes.,5,however seriously confuse irrelevant condition low accuracy confusion low accuracy,2024,June,"Keyphrase: ""Confusion and low accuracy""",4
naacl2024,Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method,Yes.,5,recent literature reveal hallucinate intermittently impede reliability utilization intermittent hallucination,2024,June,"Keyphrase: ""Intermittent hallucinations""",2
naacl2024,Are Large Language Model Temporally Grounded?,Yes.,5,generally find lag significantly behind human performance well smallscale specialise lm crucially struggle selfconsistency display incoherent behaviour least prediction moreover public instruction incoherent behavior lag behind human performance,2024,June,"Keyphrase: ""Incoherent behavior and lagging behind human performance""",0
naacl2024,R-Tuning: Instructing Large Language Models to Say Ã¢â‚¬ËœI DonÃ¢â‚¬â„¢t KnowÃ¢â‚¬â„¢,Yes.,5,predominant issue propensity nonexistent fact concern term hallucination question parametric try make something fail indicate lack hallucination propensity,2024,June,"Keyphrase: ""Hallucination propensity""",2
naacl2024,LeanReasoner: Boosting Complex Logical Reasoning with Lean,Yes.,5,often struggle complex logical reasoning due logical inconsistency inherent difficulty reason struggle logical reasoning,2024,June,"Keyphrase: ""Struggles with logical reasoning""",0
naacl2024,UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback,Yes.,4,many struggle consistently ui code compile produce visually relevant design struggle visually relevant design,2024,June,"Keyphrase: ""Struggles with generating visually relevant design""",0
naacl2024,Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?,Yes.,5,despite high performance across numerous benchmark recent research unveil suffering hallucination unfaithful reasoning experiment show exist follow correct reasoning path resist attempt greedy shortcut achieve accuracy unfaithful reasoning,2024,June,"Keyphrase: ""Unfaithful reasoning""",0
naacl2024,Learning to Compress Prompt in Natural Language Formats,Yes.,5,great process multiple natural processing ability constrain inferior performance long context slow inference speed high cost compute slow inference speed high compute cost,2024,June,"Keyphrase: ""Slow inference speed and high compute cost""",3
naacl2024,Branch-Solve-Merge Improves Large Language Model Evaluation and Generation,Yes.,5,however performance fall short due modelãs lack coherence inability plan decompose problem lack coherence planning ability,2024,June,"Keyphrase: ""Lack of coherence and planning ability""",0
naacl2024,Evaluating the Deductive Competence of Large Language Models,Yes.,5,test limit ability solve problem conventional form overall suggest unique reasoning bias partially predict human reasoning performance humangenerate corpora inform limited problemsolving ability,2024,June,"Keyphrase: ""Limited problem-solving ability""",0
naacl2024,Large Human Language Models: A Need and the Challenges,Yes.,4,time nlp become heavily reliant author bring fore range design consideration challenge term human aspect capture represent strategy pursue lack human aspect representation,2024,June,Keyphrase: Lack of human aspect representation,0
naacl2024,Hallucination Diversity-Aware Active Learning for Text Summarization,Yes.,5,show propensity hallucinate ie text factually incorrect unsupported exist alleviate hallucination typically require costly human annotation identify correct hallucination factually incorrect hallucination,2024,June,"Keyphrase: ""Factually incorrect hallucinations""",2
naacl2024,Investigating Data Contamination in Modern Benchmarks for Large Language Models,Yes.,5,recent observation underscore disparity inflate benchmark score actual performance raise concern potential contamination evaluation benchmark inflate benchmark score,2024,June,"Keyphrase: ""Inflated benchmark scores""",
naacl2024,IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context,Yes.,4,pervasive influence social bias spark need benchmark dataset capture evaluate bias observe exhibit bias across majority intersectional group pervasive social bias,2024,June,"Keyphrase: ""Pervasive social bias""",1
naacl2024,Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias,Yes.,5,position bias capture tendency unfairly prioritize information certain part text lead undesirable behavior position bias prioritization,2024,June,"Keyphrase: ""Position bias prioritization""",1
naacl2024,Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?,Yes.,4,produce complex structured tabular remain challenge indepth error analysis create ability map across six dimension coverage format reasoning comprehension pragmatic hallucination highlight area future enhancement suggest forthcoming research trajectory challenge structured comprehension,2024,June,"Keyphrase: ""Challenges with structured data comprehension""",0
naacl2024,Advancing the Robustness of Large Language Models through Self-Denoised Smoothing,Yes.,5,although achieve significant success vulnerability adversarial perturbation include recent jailbreak attack raise considerable concern vulnerability adversarial attack,2024,June,"Keyphrase: ""Vulnerability to adversarial attacks""",1
naacl2024,Lost in Space: Probing Fine-grained Spatial Understanding in Vision and Language Resamplers,Yes.,5,finegraine require spatial understanding thoroughly examine show information largely absent resampler keep freeze training classifier lack finegraine spatial understanding,2024,June,"Keyphrase: ""Lack of fine-grained spatial understanding""",
naacl2024,The Impact of Language on Arithmetic Proficiency: A Multilingual Investigation with Cross-Agent Checking Computation,Yes.,5,paper critically examine arithmetic capability uncover significant limitation performance limit arithmetic capability,2024,June,"Keyphrase: ""Limited arithmetic capability""",4
naacl2024,DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models,Yes.,4,also identify significant challenge adherence taskspecific instruction multiple highlight area future research difficulty follow taskspecific instruction,2024,June,"Keyphrase: ""Difficulty in following task-specific instructions""",4
naacl2024,OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs,Yes.,4,open research question concern inherent bias train response current research work seek debia suppress potentially biased inherent bias,2024,June,"Keyphrase: ""Inherent bias""",1
naacl2024,Exploring Inherent Biases in LLMs within Korean Social Context: A Comparative Analysis of ChatGPT and GPT-4,Yes.,4,critique perpetuate stereotype diverse group base race sexual orientation attribute finding indicate certain persona prompt combination consistently yield harmful content highlight potential risk associate specific personaissue alignment within korean cultural framework perpetuation harmful stereotype,2024,June,"Keyphrase: ""Perpetuation of harmful stereotypes""",1
naacl2024,HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms,Yes.,5,pretraine phase extremely computeintensive require several highperformance compute device like gpu several day even month training crucial capture global also significant impact finetune major high computational requirement,2024,June,"Keyphrase: ""High computational requirements""",3
naacl2024,Combating Security and Privacy Issues in the Era of Large Language Models,Yes.,5,tutorial seek provide systematic summary risk vulnerability security privacy copyright aspect recent solution address issue conclude discussion outline emergent challenge security privacy reliability deserve timely investigation community lack robust security privacy measure,2024,June,Keyphrase: Lack of robust security and privacy measures,1
aacl2022,Arabic Dialect Identification with a Few Labeled Examples Using Generative Adversarial Networks,Yes.,2,"""However, to fine-tune these models, a large corpus is required. Getting a large number high quality labeled examples for some Dialect Arabic classes is challenging and time-consuming.""",2022,November,,
aacl2022,Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts,,,,2022,November,,
aacl2022,Is Encoder-Decoder Redundant for Neural Machine Translation?,,,,2022,November,,
aacl2022,SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features,,,,2022,November,,
aacl2022,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,Yes.,3,"""SPLADE still struggles with exact matching of low-frequency words in training data. In addition, domain shifts in vocabulary and word frequencies deteriorate the IR performance of SPLADE.""",2022,November,,
aacl2022,Cross-lingual Few-Shot Learning on Unseen Languages,Yes.,3,"""However, this was mostly studied for relatively resource-rich languages, where at least enough unlabeled data is available to be included in pre-training a multilingual language model."" and ""we explore the problem of cross-lingual transfer in unseen languages, where no unlabeled data is available for pre-training a model.""",2022,November,,
aacl2022,Enhancing Financial Table and Text Question Answering with Tabular Graph and Numerical Reasoning,Yes.,3,"""their performances fall significantly when data and computational resources are limited.""",2022,November,,
aacl2022,A Simple Yet Effective Hybrid Pre-trained Language Model for Unsupervised Sentence Acceptability Prediction,Yes.,3,"""first, low-frequency words would have a significant negative impact on the sentence likelihood derived from the language model; second, when it comes to multiple domains, the language model needs to be trained on domain-specific text for domain adaptation.""",2022,November,,
aacl2022,Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour,Yes.,3,"""texts rarely report on common facts, instead focusing on the unusual aspects of a situation. If LMs are only trained on text corpora and naively memorise local co-occurrence statistics, they thus naturally would learn a biased view of the physical world.""",2022,November,,
aacl2022,MiQA: A Benchmark for Inference on Metaphorical Questions,Yes.,3,"""We examine the performance of state-of-the-art pre-trained models on binary-choice tasks and find a large discrepancy between the performance of small and very large models, going from chance to near-human level."" and ""We also analyse the largest model in a generative setting and find that although human performance is approached, careful multiple-shot prompting is required.""",2022,November,,
aacl2022,CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing,Yes.,2,"""LLMs are unsuitable for runtime systems which require low latency.""",2022,November,,
aacl2022,Toward Building a Language Model for Understanding Temporal Commonsense,Yes.,3,"""pre-trained language models such as BERT, which have recently achieved great success in a wide range of natural language processing tasks, are still considered to have poor performance in temporal reasoning.""",2022,November,,
aacl2022,C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code,Yes.,3,"""While large language models (LLMs) have been proposed to translate natural language pseudocode to PL code, they are costly in terms of data and compute.""",2022,November,,
aacl2022,Double Trouble: How to not Explain a Text Classifier’s Decisions Using Counterfactuals Synthesized by Masked Language Models?,No.,1,"""No evidence""",2022,November,,
aacl2022,CNN for Modeling Sanskrit Originated Bengali and Hindi Language,No.,1,"""No evidence""",2022,November,,
aacl2022,Neural Text Sanitization with Explicit Measures of Privacy Risk,No.,1,"""No evidence""",2022,November,,
aacl2022,AGRank: Augmented Graph-based Unsupervised Keyphrase Extraction,No.,1,"""No evidence""",2022,November,,
aacl2022,Food Knowledge Representation Learning with Adversarial Substitution,No.,1,"""No evidence""",2022,November,,
aacl2022,Construction Repetition Reduces Information Rate in Dialogue,No.,1,"""No evidence""",2022,November,,
aacl2022,Dual Mechanism Priming Effects in Hindi Word Order,No.,1,"""No evidence""",2022,November,,
aacl2022,Dead or Murdered? Predicting Responsibility Perception in Femicide News Reports,No.,1,"""No evidence""",2022,November,,
aacl2022,Whodunit? Learning to Contrast for Authorship Attribution,No.,1,"""No evidence""",2022,November,,
aacl2022,Combining Argumentation Structure and Language Model for Generating Natural Argumentative Dialogue,No.,1,"""No evidence""",2022,November,,
aacl2022,BERTSeg: BERT Based Unsupervised Subword Segmentation for Neural Machine Translation,No.,1,"""No evidence""",2022,November,,
aacl2022,NERDz: A Preliminary Dataset of Named Entities for Algerian,No.,1,"""No evidence""",2022,November,,
aacl2022,An Effective Post-training Embedding Binarization Approach for Fast Online Top-K Passage Matching,No.,1,"""No evidence""",2022,November,,
aacl2022,NepBERTa: Nepali Language Model Trained in a Large Corpus,No.,1,"""No evidence""",2022,November,,
aacl2022,Transformer-based Localization from Embodied Dialog with Large-scale Pre-training,No.,1,"""No evidence""",2022,November,,
aacl2022,Towards Simple and Efficient Task-Adaptive Pre-training for Text Classification,No.,1,"""No evidence""",2022,November,,
aacl2022,Evaluating Pre-Trained Sentence-BERT with Class Embeddings in Active Learning for Multi-Label Text Classification,No.,1,"""No evidence""",2022,November,,
aacl2022,Promoting Pre-trained LM with Linguistic Features on Automatic Readability Assessment,No.,1,"""No evidence""",2022,November,,
aacl2022,Dynamic Topic Modeling by Clustering Embeddings from Pretrained Language Models: A Research Proposal,No.,1,"""No evidence""",2022,November,,
aacl2022,TaxFree: a Visualization Tool for Candidate-free Taxonomy Enrichment,No.,1,"""No evidence""",2022,November,,
emnlp2022,"Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation",No.,1,The abstract does not mention LLMs or their limitations. It focuses on the evaluation of QA systems and the shortcomings of token-level equivalence measures.,2022,December,,
emnlp2022,UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models,Yes.,3,"""we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG.""",2022,December,,
emnlp2022,DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection,Yes.,3,"""Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning.""",2022,December,,
emnlp2022,Entity Extraction in Low Resource Domains with Selective Pre-training of Large Language Models,Yes.,3,"""Transformer-based language models trained on large natural language corpora have been very useful in downstream entity extraction tasks. However, they often result in poor performances when applied to domains that are different from those they are pretrained on.""",2022,December,,
emnlp2022,How Large Language Models are Transforming Machine-Paraphrase Plagiarism,Yes.,2,"""The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work.""",2022,December,,
emnlp2022,ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation,,,,2022,December,,
emnlp2022,Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination,Yes.,3,"""However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., 'an orange is orange'.""",2022,December,,
emnlp2022,Generative Multi-hop Retrieval,Yes.,3,"""However, such a bi-encoder approach has limitations in multi-hop settings; (1) the reformulated query gets longer as the number of hops increases, which further tightens the embedding bottleneck of the query vector, and (2) it is prone to error propagation.""",2022,December,,
emnlp2022,Extracted BERT Model Leaks More Information than You Think!,Yes.,3,"""Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.""",2022,December,,
emnlp2022,Large language models are few-shot clinical information extractors,Yes.,1,"""large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain.""",2022,December,,
emnlp2022,Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations,Yes.,3,"""for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements.""",2022,December,,
emnlp2022,Gradient-based Constrained Sampling from Language Models,Yes.,3,"""Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from.""",2022,December,,
emnlp2022,Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models,Yes.,2,"""To further advance SOTA we need more diverse and novel LMs that are less dependent on existing LMs.""",2022,December,,
emnlp2022,Calibrating Zero-shot Cross-lingual (Un-)structured Predictions,Yes.,3,"""We find that models trained with data from the source language become less calibrated when applied to the target language and that calibration errors increase with intrinsic task difficulty and relative sparsity of training data.""",2022,December,,
emnlp2022,Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation,,,,2022,December,,
emnlp2022,ROSE: Robust Selective Fine-tuning for Pre-trained Language Models,Yes.,3,"""Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks. A large body of defense methods has been proposed. However, they are still limited due to redundant attack search spaces and the inability to defend against various types of attacks.""",2022,December,,
emnlp2022,Reproducibility Issues for BERT-based Evaluation Metrics,Yes.,3,"""We find that reproduction of claims and results often fails because of (i) heavy undocumented preprocessing involved in the metrics, (ii) missing code and (iii) reporting weaker results for the baseline metrics.""",2022,December,,
emnlp2022,Generative Entity Typing with Curriculum Learning,Yes.,3,"""PLMs tend to generate coarse-grained types after fine-tuning upon the entity typing dataset.""",2022,December,,
emnlp2022,Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing,Yes.,2,"""This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields.""",2022,December,,
emnlp2022,Nearest Neighbor Zero-Shot Inference,Yes.,1,"""Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy.""",2022,December,,
emnlp2022,RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning,Yes.,2,"""Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.""",2022,December,,
emnlp2022,HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification,Yes.,3,"""there exists a huge gap between the classification tasks with sophisticated label hierarchy and the masked language model (MLM) pretraining tasks of PLMs and thus the potential of PLMs cannot be fully tapped.""",2022,December,,
emnlp2022,Is a Question Decomposition Unit All We Need?,Yes.,2,"""building new LMs may not be an ideal option owing to the cost, time and environmental impact associated with it.""",2022,December,,
emnlp2022,SLING: Sino Linguistic Evaluation of Large Language Models,Yes.,3,"""Our experiments show that the average accuracy for LMs is far below human performance (69.7% vs. 97.1%),"" and ""we find that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.""",2022,December,,
emnlp2022,Differentially Private Language Models for Secure Data Sharing,Yes.,3,"""In practice, these approaches are often dissatisfying in terms of the quality of their output language due to the strong noise required for local differential privacy.""",2022,December,,
emnlp2022,PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation,,,,2022,December,,
emnlp2022,Rethinking the Authorship Verification Experimental Setups,No.,1,The abstract does not mention LLMs or any other language models.,2022,December,,
emnlp2022,InforMask: Unsupervised Informative Masking for Language Model Pretraining,,,,2022,December,,
emnlp2022,Fine-tuned Language Models are Continual Learners,,,,2022,December,,
emnlp2022,Bernice: A Multilingual Pre-trained Encoder for Twitter,Yes.,1,"""The language of Twitter differs significantly from that of other domains commonly included in large language model training.""",2022,December,,
emnlp2022,Just Fine-tune Twice: Selective Differential Privacy for Large Language Models,Yes.,3,"""applying differential privacy (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss.""",2022,December,,
emnlp2022,Textual Manifold-based Defense Against Natural Language Adversarial Examples,Yes.,3,"""Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples.""",2022,December,,
emnlp2022,FLUTE: Figurative Language Understanding through Textual Explanations,Yes.,1,"""We show how utilizing GPT-3 in conjunction with human annotators (novices and experts) can aid in scaling up the creation of datasets even for such complex linguistic phenomena as figurative language.""",2022,December,,
emnlp2022,One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks,Yes.,2,"""stricter privacy guarantees in differentially-private stochastic gradient descent (DP-SGD) generally degrade model performance.""",2022,December,,
emnlp2022,Tutoring Helps Students Learn Better: Improving Knowledge Distillation for BERT with Tutor Network,Yes.,3,"""typical KD approaches for language models have overlooked the difficulty of training examples, suffering from incorrect teacher prediction transfer and sub-efficient training.""",2022,December,,
emnlp2022,Multitask Instruction-based Prompting for Fallacy Recognition,Yes.,1,"""we approach these differences across datasets as multiple tasks and show how instruction-based prompting in a multitask setup based on the T5 model improves the results against approaches built for a specific dataset such as T5, BERT or GPT-3.""",2022,December,,
emnlp2022,Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach,Yes.,3,"""Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables."" and ""However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored.""",2022,December,,
emnlp2022,Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering,Yes.,3,"""knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent.""",2022,December,,
emnlp2022,Few-shot Learning with Multilingual Generative Language Models,Yes.,3,"""their training data is dominated by English, potentially limiting their cross-lingual generalization.""",2022,December,,
emnlp2022,Active Example Selection for In-Context Learning,Yes.,3,"""We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information."" and ""However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.""",2022,December,,
emnlp2022,Improving Large-scale Paraphrase Acquisition and Generation,Yes.,1,"""the best pre-trained language model fine-tuned on our dataset achieves the state-of-the-art performance of 84.2 F1 for automatic paraphrase identification.""",2022,December,,
emnlp2022,Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal,Yes.,1,"""Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism.""",2022,December,,
emnlp2022,"Learning Cross-Task Dependencies for Joint Extraction of Entities, Events, Event Arguments, and Relations",,,,2022,December,,
emnlp2022,Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling,Yes.,3,"""Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content.""",2022,December,,
emnlp2022,Fine-Tuning Pre-trained Transformers into Decaying Fast Weights,Yes.,3,"""Autoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism.""",2022,December,,
emnlp2022,Adapting a Language Model While Preserving its General Knowledge,Yes.,3,"""However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus.""",2022,December,,
emnlp2022,Continual Training of Language Models for Few-Shot Learning,Yes.,1,"""This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills.""",2022,December,,
emnlp2022,Graph-Induced Transformers for Efficient Multi-Hop Question Answering,Yes.,3,"""Previous approaches to MHQA relied on leveraging the graph information along with the pre-trained language model (PLM) encoders. However, this trend exhibits the following drawbacks",2022,December,,
emnlp2022,Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task,Yes.,1,"""To overcome these limitations, this paper proposes a simple but efficient method for generating adversarial negative responses leveraging a large-scale language model.""",2022,December,,
emnlp2022,Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings,Yes.,1,"""Furthermore, we demonstrate that it can train (or tune) language models sample-efficiently and that it can be combined with recent training-efficient methods.""",2022,December,,
emnlp2022,Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks,Yes.,2,"""the generic corpora used to pretrain the teacher and the corpora associated with the downstream target domain are often significantly different, which raises a natural question",2022,December,,
emnlp2022,Grafting Pre-trained Models for Multimodal Headline Generation,Yes.,3,"""A major challenge in simply gluing language model and video-language model is the modality balance, which is aimed at combining visual-language complementary abilities.""",2022,December,,
emnlp2022,QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation,Yes.,3,"""Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their performance benefits may be offset by increased complexity of knowledge distillation.""",2022,December,,
emnlp2022,The Geometry of Multilingual Language Model Representations,No.,1,"""No evidence""",2022,December,,
emnlp2022,Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment,No.,1,"""No evidence""",2022,December,,
emnlp2022,Prompting for Multimodal Hateful Meme Classification,No.,1,"""No evidence""",2022,December,,
emnlp2022,Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling,No.,1,"""No evidence""",2022,December,,
emnlp2022,RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder,No.,1,"""No evidence""",2022,December,,
emnlp2022,QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance,No.,1,"""No evidence""",2022,December,,
emnlp2022,Segmenting Numerical Substitution Ciphers,No.,1,"""No evidence""",2022,December,,
emnlp2022,Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning,No.,1,"""No evidence""",2022,December,,
emnlp2022,Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization,No.,1,"""No evidence""",2022,December,,
emnlp2022,“Will You Find These Shortcuts?” A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification,No.,1,"""No evidence""",2022,December,,
emnlp2022,Fine-grained Contrastive Learning for Relation Extraction,No.,1,"""No evidence""",2022,December,,
emnlp2022,Using Commonsense Knowledge to Answer Why-Questions,No.,1,"""No evidence""",2022,December,,
emnlp2022,Successive Prompting for Decomposing Complex Questions,No.,1,"""No evidence""",2022,December,,
emnlp2022,Language Models of Code are Few-Shot Commonsense Learners,No.,1,"""No evidence""",2022,December,,
emnlp2022,COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning,No.,1,"""No evidence""",2022,December,,
emnlp2022,Language Model Pre-Training with Sparse Latent Typing,No.,1,"""No evidence""",2022,December,,
emnlp2022,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,No.,1,"""No evidence""",2022,December,,
emnlp2022,COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models,No.,1,"""No evidence""",2022,December,,
emnlp2022,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,No.,1,"""No evidence""",2022,December,,
emnlp2022,When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain,No.,1,"""No evidence""",2022,December,,
emnlp2022,Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations,No.,1,"""No evidence""",2022,December,,
emnlp2022,Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,No.,1,"""No evidence""",2022,December,,
emnlp2022,Measuring Context-Word Biases in Lexical Semantic Datasets,No.,1,"""No evidence""",2022,December,,
emnlp2022,Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model,No.,1,"""No evidence""",2022,December,,
emnlp2022,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,No.,1,"""No evidence""",2022,December,,
emnlp2022,Discovering Differences in the Representation of People using Contextualized Semantic Axes,No.,1,"""No evidence""",2022,December,,
emnlp2022,Improving Passage Retrieval with Zero-Shot Question Generation,No.,1,"""No evidence""",2022,December,,
emnlp2022,BBTv2: Towards a Gradient-Free Future with Large Language Models,No.,1,"""No evidence""",2022,December,,
emnlp2022,Exploiting Global and Local Hierarchies for Hierarchical Text Classification,No.,1,"""No evidence""",2022,December,,
emnlp2022,Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding,No.,1,"""No evidence""",2022,December,,
emnlp2022,Re3: Generating Longer Stories With Recursive Reprompting and Revision,No.,1,"""No evidence""",2022,December,,
emnlp2022,Continued Pretraining for Better Zero- and Few-Shot Promptability,No.,1,"""No evidence""",2022,December,,
emnlp2022,Discourse-Aware Soft Prompting for Text Generation,No.,1,"""No evidence""",2022,December,,
emnlp2022,Efficient Nearest Neighbor Emotion Classification with BERT-whitening,No.,1,"""No evidence""",2022,December,,
emnlp2022,Self-supervised Graph Masking Pre-training for Graph-to-Text Generation,No.,1,"""No evidence""",2022,December,,
emnlp2022,Conditional set generation using Seq2seq models,No.,1,"""No evidence""",2022,December,,
emnlp2022,Learning Semantic Textual Similarity via Topic-informed Discrete Latent Variables,No.,1,"""No evidence""",2022,December,,
emnlp2022,Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis,No.,1,"""No evidence""",2022,December,,
emnlp2022,MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks,No.,1,"""No evidence""",2022,December,,
emnlp2022,DRLK: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering,No.,1,"""No evidence""",2022,December,,
emnlp2022,WeTS: A Benchmark for Translation Suggestion,No.,1,"""No evidence""",2022,December,,
emnlp2022,Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model,No.,1,"""No evidence""",2022,December,,
emnlp2022,Training Language Models with Memory Augmentation,No.,1,"""No evidence""",2022,December,,
emnlp2022,The Authenticity Gap in Human Evaluation,No.,1,"""No evidence""",2022,December,,
emnlp2022,BERT in Plutarch’s Shadows,No.,1,"""No evidence""",2022,December,,
emnlp2022,AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis,No.,1,"""No evidence""",2022,December,,
emnlp2022,ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks,No.,1,"""No evidence""",2022,December,,
emnlp2022,G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks,No.,1,"""No evidence""",2022,December,,
emnlp2022,Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters,No.,1,"""No evidence""",2022,December,,
emnlp2022,ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,No.,1,"""No evidence""",2022,December,,
emnlp2022,Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders,No.,1,"""No evidence""",2022,December,,
emnlp2022,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,No.,1,"""No evidence""",2022,December,,
emnlp2022,Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing,No.,1,"""No evidence""",2022,December,,
emnlp2022,Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples,No.,1,"""No evidence""",2022,December,,
emnlp2022,XLM-D: Decorate Cross-lingual Pre-training Model as Non-Autoregressive Neural Machine Translation,No.,1,"""No evidence""",2022,December,,
emnlp2022,Let the CAT out of the bag: Contrastive Attributed explanations for Text,No.,1,"""No evidence""",2022,December,,
emnlp2022,Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking,No.,1,"""No evidence""",2022,December,,
emnlp2022,A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss,No.,1,"""No evidence""",2022,December,,
emnlp2022,"Don’t Prompt, Search! Mining-based Zero-Shot Learning with Language Models",No.,1,"""No evidence""",2022,December,,
emnlp2022,Parameter-Efficient Tuning Makes a Good Classification Head,No.,1,"""No evidence""",2022,December,,
emnlp2022,Cross-Modal Similarity-Based Curriculum Learning for Image Captioning,No.,1,"""No evidence""",2022,December,,
emnlp2022,Differentiable Data Augmentation for Contrastive Sentence Representation Learning,No.,1,"""No evidence""",2022,December,,
emnlp2022,Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation,No.,1,"""No evidence""",2022,December,,
emnlp2022,LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling,No.,1,"""No evidence""",2022,December,,
emnlp2022,Normalizing Mutual Information for Robust Adaptive Training for Translation,No.,1,"""No evidence""",2022,December,,
emnlp2022,Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?,No.,1,"""No evidence""",2022,December,,
emnlp2022,FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information,No.,1,"""No evidence""",2022,December,,
emnlp2022,IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models,No.,1,"""No evidence""",2022,December,,
emnlp2022,Contrastive Learning with Expectation-Maximization for Weakly Supervised Phrase Grounding,No.,1,"""No evidence""",2022,December,,
emnlp2022,PromptBERT: Improving BERT Sentence Embeddings with Prompts,No.,1,"""No evidence""",2022,December,,
emnlp2022,Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence,No.,1,"""No evidence""",2022,December,,
emnlp2022,Analogical Math Word Problems Solving with Enhanced Problem-Solution Association,No.,1,"""No evidence""",2022,December,,
emnlp2022,Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering,No.,1,"""No evidence""",2022,December,,
emnlp2022,Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model,No.,1,"""No evidence""",2022,December,,
emnlp2022,An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks,No.,1,"""No evidence""",2022,December,,
emnlp2022,Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation,No.,1,"""No evidence""",2022,December,,
emnlp2022,A Generative Model for End-to-End Argument Mining with Reconstructed Positional Encoding and Constrained Pointer Mechanism,No.,1,"""No evidence""",2022,December,,
emnlp2022,Quality Scoring of Source Words in Neural Translation Models,No.,1,"""No evidence""",2022,December,,
emnlp2022,Towards Compositional Generalization in Code Search,No.,1,"""No evidence""",2022,December,,
emnlp2022,KOLD: Korean Offensive Language Dataset,No.,1,"""No evidence""",2022,December,,
emnlp2022,Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?,No.,1,"""No evidence""",2022,December,,
emnlp2022,Instance Regularization for Discriminative Language Model Pre-training,No.,1,"""No evidence""",2022,December,,
emnlp2022,Improved grammatical error correction by ranking elementary edits,No.,1,"""No evidence""",2022,December,,
emnlp2022,MedJEx: A Medical Jargon Extraction Model with Wiki’s Hyperlink Span and Contextualized Masked Language Model Score,No.,1,"""No evidence""",2022,December,,
emnlp2022,LogiTorch: A PyTorch-based library for logical reasoning on natural language,No.,1,"""No evidence""",2022,December,,
emnlp2022,Paraphrastic Representations at Scale,No.,1,"""No evidence""",2022,December,,
emnlp2022,Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance,No.,1,"""No evidence""",2022,December,,
emnlp2022,DynaMaR: Dynamic Prompt with Mask Token Representation,No.,1,"""No evidence""",2022,December,,
emnlp2022,PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding,No.,1,"""No evidence""",2022,December,,
emnlp2022,Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks,No.,1,"""No evidence""",2022,December,,
emnlp2022,Fast Vocabulary Transfer for Language Model Compression,No.,1,"""No evidence""",2022,December,,
emnlp2022,Zero-Shot Dynamic Quantization for Transformer Inference,No.,1,"""No evidence""",2022,December,,
emnlp2022,Deploying Unified BERT Moderation Model for E-Commerce Reviews,No.,1,"""No evidence""",2022,December,,
emnlp2022,Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training,No.,1,"""No evidence""",2022,December,,
emnlp2022,A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat,No.,1,"""No evidence""",2022,December,,
emnlp2022,A Comprehensive Evaluation of Biomedical Entity-centric Search,No.,1,"""No evidence""",2022,December,,
emnlp2022,Topic Modeling by Clustering Language Model Embeddings: Human Validation on an Industry Dataset,No.,1,"""No evidence""",2022,December,,
acl2022,AlephBERT: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level,Yes.,1,"""First, so far, Hebrew resources for training large language models are not of the same magnitude as their English counterparts. Second, most benchmarks available to evaluate progress in Hebrew NLP require morphological boundaries which are not available in the output of standard PLMs.""",2022,May,,
acl2022,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,Yes.,1,"""We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge.""",2022,May,,
acl2022,Towards Comprehensive Patent Approval Predictions:Beyond Traditional Document Classification,No.,1,The abstract focuses on patent approval predictions and does not mention language models or their limitations.,2022,May,,
acl2022,Answer-level Calibration for Free-form Multiple Choice Question Answering,Yes.,3,"""it often requires task-specific heuristics such as length normalization, or probability calibration.""",2022,May,,
acl2022,Meta-learning via Language Model In-context Tuning,Yes.,1,"""Inspired by the recent progress in large language models,"" and ""we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.""",2022,May,,
acl2022,RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining,Yes.,3,"""Large-scale pretrained language models have achieved SOTA results on NLP tasks. However, they have been shown vulnerable to adversarial attacks especially for logographic languages like Chinese.""",2022,May,,
acl2022,A Closer Look at How Fine-tuning Changes BERT,Yes.,1,"""Finally, by comparing the representations before and after fine-tuning, we discover that fine-tuning does not introduce arbitrary changes to representations; instead, it adjusts the representations to downstream tasks while largely preserving the original spatial structure of the data points.""",2022,May,,
acl2022,GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models,Yes.,2,"""questions remain about their ability to generalize beyond the small reference sets that are publicly available for research.""",2022,May,,
acl2022,Distributionally Robust Finetuning BERT for Covariate Drift in Spoken Language Understanding,Yes.,2,"""Experiments show that a state-of-the-art BERT-based model suffers performance loss under this drift.""",2022,May,,
acl2022,CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation,Yes.,1,"""Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.""",2022,May,,
acl2022,Contextual Representation Learning beyond Masked Language Modeling,Yes.,3,"""it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs.""",2022,May,,
acl2022,Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval,Yes.,2,"""fragility to training data noise and ii) requiring large batches to robustly learn the embedding space.""",2022,May,,
acl2022,Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation,No.,1,The abstract discusses neural machine translation (NMT) models and their limitations but does not mention LLMs or their limitations.,2022,May,,
acl2022,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,,,,2022,May,,
acl2022,NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,,,,2022,May,,
acl2022,ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding,Yes.,3,"""Our experiments show that neural language models struggle on these tasks compared to humans, and these tasks pose multiple learning challenges.""",2022,May,,
acl2022,Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP,Yes.,1,"""Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models.""",2022,May,,
acl2022,Compression of Generative Pre-trained Language Models via Quantization,Yes.,3,"""Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear."" and ""We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights.""",2022,May,,
acl2022,KinyaBERT: a Morphology-aware Kinyarwanda Language Model,Yes.,3,"""the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages."" and ""naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compos",2022,May,,
acl2022,Flooding-X: Improving BERT’s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning,Yes.,1,"""We experimentally show that our method improves BERT’s resistance to textual adversarial attacks by a large margin, and achieves state-of-the-art robust accuracy on various text classification and GLUE tasks.""",2022,May,,
acl2022,What does the sea say to the shore? A BERT based DST style approach for speaker to dialogue attribution in novels,No.,1,The abstract does not mention LLMs or any of their limitations.,2022,May,,
acl2022,Probing Simile Knowledge from Pre-trained Language Models,Yes.,1,"""The knowledge embedded in PLMs may be useful for SI and SG tasks. Nevertheless, there are few works to explore it.""",2022,May,,
acl2022,SHIELD: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher,Yes.,2,"""In particular, the state-of-the-art transformer models (e.g., BERT, RoBERTa) require great time and computation resources.""",2022,May,,
acl2022,Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice,Yes.,3,"""In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models (Demeter et al., 2020).""",2022,May,,
acl2022,Transkimmer: Transformer Learns to Layer-wise Skim,No.,1,The abstract focuses on improving the computational efficiency of Transformer-based models and does not specifically discuss LLMs or their limitations.,2022,May,,
acl2022,ABC: Attention with Bounded-memory Control,Yes.,3,"""However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences.""",2022,May,,
acl2022,Cluster & Tune: Boost Cold Start Performance in Text Classification,Yes.,1,"""the common practice of fine-tuning pre-trained models, such as BERT, for a target classification task, is prone to produce poor performance.""",2022,May,,
acl2022,Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity,,,,2022,May,,
acl2022,Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires,No.,1,The abstract does not mention LLMs or any form of language models.,2022,May,,
acl2022,Internet-Augmented Dialogue Generation,Yes.,3,"""Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue; moreover, those facts are frozen in time at the point of model training.""",2022,May,,
acl2022,Text-Free Prosody-Aware Generative Spoken Language Modeling,Yes.,3,"""Unfortunately, because the units used in GSLM discard most prosodic information, GSLM fails to leverage prosody for better comprehension and does not generate expressive speech.""",2022,May,,
acl2022,P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks,Yes.,3,"""prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality.""",2022,May,,
acl2022,Does BERT Know that the IS-A Relation Is Transitive?,,,,2022,May,,
acl2022,Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task,Yes.,3,"""existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset"" and ""none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline.""",2022,May,,
acl2022,"Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games",Yes.,1,"""We propose CommExpl, an exploration technique that injects external commonsense knowledge, via a pretrained language model (LM), into the agent during training.""",2022,May,,
acl2022,"When classifying grammatical role, BERT doesn’t care about word order... except when it matters",,,,2022,May,,
acl2022,A Recipe for Arbitrary Text Style Transfer with Large Language Models,Yes.,1,"""In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer.""",2022,May,,
acl2022,Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models.,Yes.,3,"""tested state-of-the-art models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is currently inevitable and motivating the need of further research in this direction.""",2022,May,,
acl2022,Cue-bot: A Conversational Agent for Assistive Technology,Yes.,1,"""Language model technologies can be very powerful tools in enabling these users to carry out daily communication and social interactions.""",2022,May,,
acl2022,Zero- and Few-Shot NLP with Pretrained Language Models,Yes.,1,"""our goal is to reveal new research opportunities to the audience, which will hopefully bring us closer to address existing challenges in this domain.""",2022,May,,
acl2022,AdapLeR: Speeding up Inference by Adaptive Length Reduction,No.,1,"""No evidence""",2022,May,,
acl2022,Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models,No.,1,"""No evidence""",2022,May,,
acl2022,Language-agnostic BERT Sentence Embedding,No.,1,"""No evidence""",2022,May,,
acl2022,CogTaskonomy: Cognitively Inspired Task Taxonomy Is Beneficial to Transfer Learning in NLP,No.,1,"""No evidence""",2022,May,,
acl2022,Multi-Granularity Structural Knowledge Distillation for Language Model Compression,No.,1,"""No evidence""",2022,May,,
acl2022,Semi-supervised Domain Adaptation for Dependency Parsing with Dynamic Matching Network,No.,1,"""No evidence""",2022,May,,
acl2022,Tracing Origins: Coreference-aware Machine Reading Comprehension,No.,1,"""No evidence""",2022,May,,
acl2022,Better Language Model with Hypernym Class Prediction,No.,1,"""No evidence""",2022,May,,
acl2022,Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformer Architectures,No.,1,"""No evidence""",2022,May,,
acl2022,Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation,No.,1,"""No evidence""",2022,May,,
acl2022,Probing as Quantifying Inductive Bias,No.,1,"""No evidence""",2022,May,,
acl2022,Exploring and Adapting Chinese GPT to Pinyin Input Method,No.,1,"""No evidence""",2022,May,,
acl2022,Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization,No.,1,"""No evidence""",2022,May,,
acl2022,Enhancing Chinese Pre-trained Language Model via Heterogeneous Linguistics Graph,No.,1,"""No evidence""",2022,May,,
acl2022,Investigating Non-local Features for Neural Constituency Parsing,No.,1,"""No evidence""",2022,May,,
acl2022,bert2BERT: Towards Reusable Pretrained Language Models,No.,1,"""No evidence""",2022,May,,
acl2022,Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation,No.,1,"""No evidence""",2022,May,,
acl2022,Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks,No.,1,"""No evidence""",2022,May,,
acl2022,Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis,No.,1,"""No evidence""",2022,May,,
acl2022,Text-to-Table: A New Way of Information Extraction,No.,1,"""No evidence""",2022,May,,
acl2022,Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-Modal Knowledge Transfer,No.,1,"""No evidence""",2022,May,,
acl2022,Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations,No.,1,"""No evidence""",2022,May,,
acl2022,Generated Knowledge Prompting for Commonsense Reasoning,No.,1,"""No evidence""",2022,May,,
acl2022,Life after BERT: What do Other Muppets Understand about Language?,No.,1,"""No evidence""",2022,May,,
acl2022,Multilingual Molecular Representation Learning via Contrastive Pre-training,No.,1,"""No evidence""",2022,May,,
acl2022,Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost,No.,1,"""No evidence""",2022,May,,
acl2022,Token Dropping for Efficient BERT Pretraining,No.,1,"""No evidence""",2022,May,,
acl2022,The Trade-offs of Domain Adaptation for Neural Language Models,No.,1,"""No evidence""",2022,May,,
acl2022,Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling,No.,1,"""No evidence""",2022,May,,
acl2022,Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?,No.,1,"""No evidence""",2022,May,,
acl2022,FiNER: Financial Numeric Entity Recognition for XBRL Tagging,No.,1,"""No evidence""",2022,May,,
acl2022,Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition,No.,1,"""No evidence""",2022,May,,
acl2022,Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure,No.,1,"""No evidence""",2022,May,,
acl2022,Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT,No.,1,"""No evidence""",2022,May,,
acl2022,DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation,No.,1,"""No evidence""",2022,May,,
acl2022,A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space,No.,1,"""No evidence""",2022,May,,
acl2022,E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models,No.,1,"""No evidence""",2022,May,,
acl2022,Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns,No.,1,"""No evidence""",2022,May,,
acl2022,Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data,No.,1,"""No evidence""",2022,May,,
acl2022,Noisy Channel Language Model Prompting for Few-Shot Text Classification,No.,1,"""No evidence""",2022,May,,
acl2022,Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages,No.,1,"""No evidence""",2022,May,,
acl2022,Finding Structural Knowledge in Multimodal-BERT,No.,1,"""No evidence""",2022,May,,
acl2022,Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning,No.,1,"""No evidence""",2022,May,,
acl2022,XLM-E: Cross-lingual Language Model Pre-training via ELECTRA,No.,1,"""No evidence""",2022,May,,
acl2022,ReACC: A Retrieval-Augmented Code Completion Framework,No.,1,"""No evidence""",2022,May,,
acl2022,UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning,No.,1,"""No evidence""",2022,May,,
acl2022,Universal Conditional Masked Language Pre-training for Neural Machine Translation,No.,1,"""No evidence""",2022,May,,
acl2022,Multilingual Detection of Personal Employment Status on Twitter,No.,1,"""No evidence""",2022,May,,
acl2022,Transformers in the loop: Polarity in neural models of language,No.,1,"""No evidence""",2022,May,,
acl2022,SDR: Efficient Neural Re-ranking using Succinct Document Representation,No.,1,"""No evidence""",2022,May,,
acl2022,BERT Learns to Teach: Knowledge Distillation with Meta Learning,No.,1,"""No evidence""",2022,May,,
acl2022,CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing,No.,1,"""No evidence""",2022,May,,
acl2022,SkipBERT: Efficient Inference with Shallow Layer Skipping,No.,1,"""No evidence""",2022,May,,
acl2022,mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models,No.,1,"""No evidence""",2022,May,,
acl2022,Sharpness-Aware Minimization Improves Language Model Generalization,No.,1,"""No evidence""",2022,May,,
acl2022,Dependency-based Mixture Language Models,No.,1,"""No evidence""",2022,May,,
acl2022,LinkBERT: Pretraining Language Models with Document Links,No.,1,"""No evidence""",2022,May,,
acl2022,Knowledge Neurons in Pretrained Transformers,No.,1,"""No evidence""",2022,May,,
acl2022,Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining,No.,1,"""No evidence""",2022,May,,
acl2022,Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection,No.,1,"""No evidence""",2022,May,,
acl2022,Probing for the Usage of Grammatical Number,No.,1,"""No evidence""",2022,May,,
acl2022,BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models,No.,1,"""No evidence""",2022,May,,
acl2022,Automatic Detection of Entity-Manipulated Text using Factual Knowledge,No.,1,"""No evidence""",2022,May,,
acl2022,"Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models",No.,1,"""No evidence""",2022,May,,
acl2022,How does the pre-training objective affect what large language models learn about linguistic properties?,No.,1,"""No evidence""",2022,May,,
acl2022,The Power of Prompt Tuning for Low-Resource Semantic Parsing,No.,1,"""No evidence""",2022,May,,
acl2022,PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction,No.,1,"""No evidence""",2022,May,,
acl2022,An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers,No.,1,"""No evidence""",2022,May,,
acl2022,Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words,No.,1,"""No evidence""",2022,May,,
acl2022,XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding,No.,1,"""No evidence""",2022,May,,
acl2022,Triangular Transfer: Freezing the Pivot for Triangular Machine Translation,No.,1,"""No evidence""",2022,May,,
acl2022,A Flexible Multi-Task Model for BERT Serving,No.,1,"""No evidence""",2022,May,,
acl2022,Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks,No.,1,"""No evidence""",2022,May,,
acl2022,TeluguNER: Leveraging Multi-Domain Named Entity Recognition with Deep Transformers,No.,1,"""No evidence""",2022,May,,
acl2022,Mining Logical Event Schemas From Pre-Trained Language Models,No.,1,"""No evidence""",2022,May,,
acl2022,Pretrained Knowledge Base Embeddings for improved Sentential Relation Extraction,No.,1,"""No evidence""",2022,May,,
acl2022,A Checkpoint on Multilingual Misogyny Identification,No.,1,"""No evidence""",2022,May,,
acl2022,A Dataset and BERT-based Models for Targeted Sentiment Analysis on Turkish Texts,No.,1,"""No evidence""",2022,May,,
acl2022,QiuNiu: A Chinese Lyrics Generation System with Passage-Level Input,No.,1,"""No evidence""",2022,May,,
acl2022,"TS-ANNO: An Annotation Tool to Build, Annotate and Evaluate Text Simplification Corpora",No.,1,"""No evidence""",2022,May,,
acl2022,BMInf: An Efficient Toolkit for Big Model Inference and Tuning,No.,1,"""No evidence""",2022,May,,
acl2022,TimeLMs: Diachronic Language Models from Twitter,No.,1,"""No evidence""",2022,May,,
naacl2022,Learning Natural Language Generation with Truncated Reinforcement Learning,Yes.,1,"""TrufLL thus enables to train a language agent by solely interacting with its environment without any task-specific prior knowledge; it is only guided with a task-agnostic language model.""",2022,July,,
naacl2022,Language Model Augmented Monotonic Attention for Simultaneous Translation,Yes.,1,"""we propose a framework to aid monotonic attention with an external language model to improve its decisions.""",2022,July,,
naacl2022,Enhancing Self-Attention with Knowledge-Assisted Attention Maps,Yes.,3,"""the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge.""",2022,July,,
naacl2022,Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation,Yes.,1,"""Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources.""",2022,July,,
naacl2022,Reframing Human-AI Collaboration for Generating Free-Text Explanations,Yes.,3,"""while models often produce factual, grammatical, and sufficient explanations, they have room to improve along axes such as providing novel information and supporting the label.""",2022,July,,
naacl2022,Towards a Progression-Aware Autonomous Dialogue Agent,Yes.,3,"""While these agents excel at generating high-quality responses that are relevant to prior context, they suffer from a lack of awareness of the overall direction in which the conversation is headed, and the likelihood of task success inherent therein.""",2022,July,,
naacl2022,Cross-Domain Detection of GPT-2-Generated Technical Text,Yes.,1,"""we examine the problem of detecting GPT-2-generated technical research text.""",2022,July,,
naacl2022,Context-Aware Abbreviation Expansion Using Large Language Models,,,,2022,July,,
naacl2022,Sort by Structure: Language Model Ranking as Dependency Probing,Yes.,1,"""Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored.""",2022,July,,
naacl2022,SKILL: Structured Knowledge Infusion for Large Language Models,Yes.,2,"""However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text.""",2022,July,,
naacl2022,MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation,,,,2022,July,,
naacl2022,ValCAT: Variable-Length Contextualized Adversarial Transformations Using Encoder-Decoder Language Model,Yes.,1,"""Adversarial texts help explore vulnerabilities in language models, improve model robustness, and explain their working mechanisms.""",2022,July,,
naacl2022,KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation,Yes.,2,"""While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices.""",2022,July,,
naacl2022,Data Augmentation with Dual Training for Offensive Span Detection,Yes.,1,"""the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD.""",2022,July,,
naacl2022,Robust Conversational Agents against Imperceptible Toxicity Triggers,No.,1,The abstract does not mention LLMs or their limitations.,2022,July,,
naacl2022,PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding,Yes.,1,"""Large language models (LM) based on Transformers allow to generate plausible long texts.""",2022,July,,
naacl2022,Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption,Yes.,2,"""recent research has shown that embeddings can potentially leak private information about sensitive attributes of the text, and in some cases, can be inverted to recover the original input text.""",2022,July,,
naacl2022,Knowledge Inheritance for Pre-trained Language Models,Yes.,3,"""However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable.""",2022,July,,
naacl2022,"Show, Don’t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",Yes.,1,"""we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks.""",2022,July,,
naacl2022,Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge,Yes.,3,"""We find generalization does not improve over the course of pre-training BERT from scratch, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.""",2022,July,,
naacl2022,Symbolic Knowledge Distillation: from General Language Models to Commonsense Models,Yes.,1,"""general language models author these commonsense knowledge graphs to train commonsense models.""",2022,July,,
naacl2022,Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,Yes.,3,"""We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks.""",2022,July,,
naacl2022,A Study of the Attention Abnormality in Trojaned BERTs,Yes.,1,"""In this paper, we investigate the underlying mechanism of Trojaned BERT models.""",2022,July,,
naacl2022,Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora,Yes.,3,"""a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on.""",2022,July,,
naacl2022,Building a Personalized Dialogue System with Prompt-Tuning,Yes.,1,"""Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models.""",2022,July,,
naacl2022,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,,,,2022,July,,
naacl2022,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,Yes.,1,"""The second approach utilizes a domain-specific pretrained language model, MentalBERT.""",2022,July,,
naacl2022,Building a Personalized Dialogue System with Prompt-Tuning,Yes.,1,"""Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models.""",2022,July,,
naacl2022,Multimodal large language models for inclusive collaboration learning tasks,Yes.,1,"""We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.""",2022,July,,
naacl2022,An End-to-End Dialogue Summarization System for Sales Calls,Yes.,1,"""We show how GPT-3 can be leveraged as an offline data labeler to handle training data scarcity and accommodate privacy constraints in an industrial setting.""",2022,July,,
naacl2022,Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning,Yes.,2,"""automating this task is challenging due to a large number of LLT codes (as of writing over 80\,000), limited availability of training data for long tail/emerging classes, and the general high accuracy demands of the medical domain.""",2022,July,,
naacl2022,Knowledge extraction from aeronautical messages (NOTAMs) with self-supervised language models for aircraft pilots,Yes.,1,"""In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt representations on three downstream tasks valuable for pilots.""",2022,July,,
naacl2022,Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia,No.,1,"""No evidence""",2022,July,,
naacl2022,SwahBERT: Language Model of Swahili,No.,1,"""No evidence""",2022,July,,
naacl2022,Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,No.,1,"""No evidence""",2022,July,,
naacl2022,Machine-in-the-Loop Rewriting for Creative Image Captioning,No.,1,"""No evidence""",2022,July,,
naacl2022,NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics,No.,1,"""No evidence""",2022,July,,
naacl2022,Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection,No.,1,"""No evidence""",2022,July,,
naacl2022,Generating Repetitions with Appropriate Repeated Words,No.,1,"""No evidence""",2022,July,,
naacl2022,Abstraction not Memory: BERT and the English Article System,No.,1,"""No evidence""",2022,July,,
naacl2022,DocTime: A Document-level Temporal Dependency Graph Parser,No.,1,"""No evidence""",2022,July,,
naacl2022,Efficient Hierarchical Domain Adaptation for Pretrained Language Models,No.,1,"""No evidence""",2022,July,,
naacl2022,Representation Learning for Conversational Data using Discourse Mutual Information Maximization,No.,1,"""No evidence""",2022,July,,
naacl2022,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,No.,1,"""No evidence""",2022,July,,
naacl2022,Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,No.,1,"""No evidence""",2022,July,,
naacl2022,Sentence-Level Resampling for Named Entity Recognition,No.,1,"""No evidence""",2022,July,,
naacl2022,Locally Aggregated Feature Attribution on Natural Language Model Understanding,No.,1,"""No evidence""",2022,July,,
naacl2022,A Shoulder to Cry on: Towards A Motivational Virtual Assistant for Assuaging Mental Agony,No.,1,"""No evidence""",2022,July,,
naacl2022,What do tokens know about their characters and how do they know it?,No.,1,"""No evidence""",2022,July,,
naacl2022,Learning To Retrieve Prompts for In-Context Learning,No.,1,"""No evidence""",2022,July,,
naacl2022,"Re2G: Retrieve, Rerank, Generate",No.,1,"""No evidence""",2022,July,,
naacl2022,MetaICL: Learning to Learn In Context,No.,1,"""No evidence""",2022,July,,
naacl2022,Modal Dependency Parsing via Language Model Priming,No.,1,"""No evidence""",2022,July,,
naacl2022,Progressive Class Semantic Matching for Semi-supervised Text Classification,No.,1,"""No evidence""",2022,July,,
naacl2022,Low Resource Style Transfer via Domain Adaptive Meta Learning,No.,1,"""No evidence""",2022,July,,
naacl2022,Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation,No.,1,"""No evidence""",2022,July,,
naacl2022,Unsupervised Paraphrasability Prediction for Compound Nominalizations,No.,1,"""No evidence""",2022,July,,
naacl2022,Global Entity Disambiguation with BERT,No.,1,"""No evidence""",2022,July,,
naacl2022,Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,No.,1,"""No evidence""",2022,July,,
naacl2022,On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation,No.,1,"""No evidence""",2022,July,,
naacl2022,Improving Neural Models for Radiology Report Retrieval with Lexicon-based Automated Annotation,No.,1,"""No evidence""",2022,July,,
naacl2022,When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer,No.,1,"""No evidence""",2022,July,,
naacl2022,Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics,No.,1,"""No evidence""",2022,July,,
naacl2022,WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models,No.,1,"""No evidence""",2022,July,,
naacl2022,DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings,No.,1,"""No evidence""",2022,July,,
naacl2022,A Data Cartography based MixUp for Pre-trained Language Models,No.,1,"""No evidence""",2022,July,,
naacl2022,FNet: Mixing Tokens with Fourier Transforms,No.,1,"""No evidence""",2022,July,,
naacl2022,Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling,No.,1,"""No evidence""",2022,July,,
naacl2022,Using Paraphrases to Study Properties of Contextual Embeddings,No.,1,"""No evidence""",2022,July,,
naacl2022,Learning to Generate Examples for Semantic Processing Tasks,No.,1,"""No evidence""",2022,July,,
naacl2022,A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank,No.,1,"""No evidence""",2022,July,,
naacl2022,SkillSpan: Hard and Soft Skill Extraction from English Job Postings,No.,1,"""No evidence""",2022,July,,
naacl2022,Modeling Multi-Granularity Hierarchical Features for Relation Extraction,No.,1,"""No evidence""",2022,July,,
naacl2022,Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances,No.,1,"""No evidence""",2022,July,,
naacl2022,When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes,No.,1,"""No evidence""",2022,July,,
naacl2022,Few-Shot Semantic Parsing with Language Models Trained on Code,No.,1,"""No evidence""",2022,July,,
naacl2022,ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence,No.,1,"""No evidence""",2022,July,,
naacl2022,Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification,No.,1,"""No evidence""",2022,July,,
naacl2022,Few-shot Subgoal Planning with Language Models,No.,1,"""No evidence""",2022,July,,
naacl2022,IDPG: An Instance-Dependent Prompt Generation Method,No.,1,"""No evidence""",2022,July,,
naacl2022,Embedding Hallucination for Few-shot Language Fine-tuning,No.,1,"""No evidence""",2022,July,,
naacl2022,DEMix Layers: Disentangling Domains for Modular Language Modeling,No.,1,"""No evidence""",2022,July,,
naacl2022,Contrastive Learning for Prompt-based Few-shot Language Learners,No.,1,"""No evidence""",2022,July,,
naacl2022,CoMPM: Context Modeling with Speaker’s Pre-trained Memory Tracking for Emotion Recognition in Conversation,No.,1,"""No evidence""",2022,July,,
naacl2022,Template-free Prompt Tuning for Few-shot NER,No.,1,"""No evidence""",2022,July,,
naacl2022,Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training,No.,1,"""No evidence""",2022,July,,
naacl2022,"MOVER: Mask, Over-generate and Rank for Hyperbole Generation",No.,1,"""No evidence""",2022,July,,
naacl2022,Regularized Training of Nearest Neighbor Language Models,No.,1,"""No evidence""",2022,July,,
naacl2022,Impact of Training Instance Selection on Domain-Specific Entity Extraction using BERT,No.,1,"""No evidence""",2022,July,,
naacl2022,Zuo Zhuan Ancient Chinese Dataset for Word Sense Disambiguation,No.,1,"""No evidence""",2022,July,,
naacl2022,How do people talk about images? A study on open-domain conversations with images.,No.,1,"""No evidence""",2022,July,,
naacl2022,Probe-Less Probing of BERT’s Layer-Wise Linguistic Knowledge with Masked Word Prediction,No.,1,"""No evidence""",2022,July,,
naacl2022,Automating Human Evaluation of Dialogue Systems,No.,1,"""No evidence""",2022,July,,
naacl2022,Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations,No.,1,"""No evidence""",2022,July,,
naacl2022,Towards Open-Domain Topic Classification,No.,1,"""No evidence""",2022,July,,
naacl2022,FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction,No.,1,"""No evidence""",2022,July,,
naacl2022,Self-supervised Representation Learning for Speech Processing,No.,1,"""No evidence""",2022,July,,
naacl2022,Self-supervised Product Title Rewrite for Product Listing Ads,No.,1,"""No evidence""",2022,July,,
naacl2022,Lightweight Transformers for Conversational AI,No.,1,"""No evidence""",2022,July,,
naacl2022,NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension,No.,1,"""No evidence""",2022,July,,
acl2023,ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER,Yes.,1,"""In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation, to address the data scarcity problem in low-resource complex NER.""",2023,July,,
acl2023,Text Adversarial Purification as Defense against Adversarial Attacks,Yes.,1,"""With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models.""",2023,July,,
acl2023,Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions,Yes.,,,2023,July,,
acl2023,Self-Edit: Fault-Aware Code Editor for Code Generation,Yes.,3,"""However, with limited sample numbers, LLMs still suffer from poor accuracy.""",2023,July,,
acl2023,Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models,Yes.,3,"""GPT’s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are",2023,July,,
acl2023,Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages,Yes.,1,"""The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages.""",2023,July,,
acl2023,Pretrained Bidirectional Distillation for Machine Translation,,,,2023,July,,
acl2023,Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe,Yes.,1,"""simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection.""",2023,July,,
acl2023,Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis,Yes.,1,"""we propose a new cross-domain Data Augmentation approach based on Domain-Adaptive Language Modeling named DA2LM""",2023,July,,
acl2023,Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models,,,,2023,July,,
acl2023,Elaboration-Generating Commonsense Question Answering at Scale,Yes.,2,"""Yet the cost of working with such models is very high.""",2023,July,,
acl2023,DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation,Yes.,1,"""On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%.""",2023,July,,
acl2023,"KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",Yes.,2,"""While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts"" and ""incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length.""",2023,July,,
acl2023,Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations,Yes.,3,"""performance drops significantly when no demonstrations are available.""",2023,July,,
acl2023,Training-free Neural Architecture Search for RNNs and Transformers,,,,2023,July,,
acl2023,Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step,Yes.,3,"""benefits appear to emerge only for sufficiently large models (beyond 50B parameters).""",2023,July,,
acl2023,Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,Yes.,3,"""Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance.""",2023,July,,
acl2023,An Invariant Learning Characterization of Controlled Text Generation,Yes.,3,"""researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text"" and ""the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on.""",2023,July,,
acl2023,HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation,Yes.,3,"""there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse.""",2023,July,,
acl2023,Decoding Symbolism in Language Models,,,,2023,July,,
acl2023,A Survey on Zero Pronoun Translation,Yes.,3,"""ZPT is in line with the development trend of large language model"" and ""data limitation causes learning bias in languages and domains"" and ""advanced methods are still far from real-world use"" and ""general-purpose metrics are not reliable on nuances and complexities of ZPT, emphasizing the necessity of targeted metrics.""",2023,July,,
acl2023,Alleviating Over-smoothing for Unsupervised Sentence Representation,Yes.,3,"""Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations.""",2023,July,,
acl2023,Code4Struct: Code Generation for Few-Shot Event Structure Prediction,Yes.,1,"""Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code.""",2023,July,,
acl2023,Entity Tracking in Language Models,Yes.,3,"""we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations"" and ""pretraining on text corpora alone does not make this capacity surface.""",2023,July,,
acl2023,Faithful Question Answering with Monte-Carlo Planning,Yes.,3,"""revealing the intermediate reasoning steps that the models faithfully follow remains challenging.""",2023,July,,
acl2023,Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast,Yes.,3,"""However, there is a clear gap between the performance of the source language and that of the non-source languages.""",2023,July,,
acl2023,Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,Yes.,3,"""The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5",2023,July,,
acl2023,FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information,Yes.,2,"""Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game",2023,July,,
acl2023,Distilling Script Knowledge from Large Language Models for Constrained Language Planning,Yes.,2,"""Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., “make a cake”), but leaves more specific goals with multi-facet constraints understudied (e.g., “make a cake for diabetics”).""",2023,July,,
acl2023,CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels,Yes.,3,"""Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications.""",2023,July,,
acl2023,Solving Math Word Problems via Cooperative Reasoning induced Language Models,Yes.,3,"""However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans.""",2023,July,,
acl2023,"Don’t Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",Yes.,3,"""A key missing capacity of current language models (LMs) is grounding to real-world environments."" and ""It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs.""",2023,July,,
acl2023,Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications,Yes.,3,"""Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems.""",2023,July,,
acl2023,MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning,Yes.,1,"""we perform extensive experiments to compare MetaAdapt with state-of-the-art baselines and large language models (LLMs) such as LLaMA.""",2023,July,,
acl2023,Making Language Models Better Reasoners with Step-Aware Verifier,Yes.,3,"""Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems.""",2023,July,,
acl2023,DISCO: Distilling Counterfactuals with Large Language Models,,,,2023,July,,
acl2023,Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification,Yes.,1,"""we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs).""",2023,July,,
acl2023,Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation,Yes.,1,"""To address these problems, we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach to instruct large-scale pre-trained language models (PLMs) mining the latent correlations between connectives and discourse relations, which is meaningful for IDRR.""",2023,July,,
acl2023,Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment,Yes.,3,"""We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces.""",2023,July,,
acl2023,MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering,Yes.,1,"""Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table.""",2023,July,,
acl2023,Long-Tailed Question Answering in an Open World,Yes.,1,"""we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM).""",2023,July,,
acl2023,FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue,Yes.,3,"""the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice"" and ""Current dialogue pre-training methods rely on a contrastive framework and face the challenges of both selecting true positives and hard negatives.""",2023,July,,
acl2023,LAMBADA: Backward Chaining for Automated Reasoning in Natural Language,Yes.,3,"""These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning.""",2023,July,,
acl2023,FLamE: Few-shot Learning from Natural Language Explanations,Yes.,3,"""Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification."" and ""human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.""",2023,July,,
acl2023,What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics,Yes.,3,"""We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do."" and ""Such findings illuminate how a language model",2023,July,,
acl2023,Are Experts Needed? On Human Evaluation of Counselling Reflection Generation,Yes.,1,"""We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2.""",2023,July,,
acl2023,When and how to paraphrase for named entity recognition?,Yes.,1,"""We find that the choice of the paraphraser greatly impacts NER performance, with one of the larger GPT-3 variants exceedingly capable of generating high quality paraphrases, yielding statistically significant improvements in NER performance with increasing paraphrasing strength, while other paraphrasers show more mixed results.""",2023,July,,
acl2023,Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text,Yes.,1,"""These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores.""",2023,July,,
acl2023,ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models,Yes.,1,"""We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts.""",2023,July,,
acl2023,Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models,Yes.,3,"""However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains.""",2023,July,,
acl2023,Large Language Models Meet NL2Code: A Survey,Yes.,2,"""In addition, we discuss challenges and opportunities regarding the gap between models and humans.""",2023,July,,
acl2023,DarkBERT: A Language Model for the Dark Side of the Internet,Yes.,1,"""We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain.""",2023,July,,
acl2023,Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark,Yes.,3,"""EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive.""",2023,July,,
acl2023,DIP: Dead code Insertion based Black-box Attack for Programming Language Model,Yes.,3,"""However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation.""",2023,July,,
acl2023,Data Curation Alone Can Stabilize In-context Learning,Yes.,3,"""ICL is very sensitive to the choice of training examples",2023,July,,
acl2023,S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction,Yes.,1,"""We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data.""",2023,July,,
acl2023,DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models,Yes.,3,"""as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive"" and ""the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more specialized functionality, nor practical since many fine-tuned models will be deployed in",2023,July,,
acl2023,A New Dataset and Empirical Study for Sentence Simplification in Chinese,Yes.,1,"""In the end, we explore whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS.""",2023,July,,
acl2023,AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression,Yes.,3,"""existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher’s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge.""",2023,July,,
acl2023,Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method,Yes.,1,"""we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs’ zero-shot summaries in prior work.""",2023,July,,
acl2023,The CRINGE Loss: Learning what language not to model,Yes.,3,"""Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do.""",2023,July,,
acl2023,"My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave",Yes.,1,"""With the help of Large Language Models (LLM), we address this task by",2023,July,,
acl2023,Benchmarking Large Language Model Capabilities for Conditional Generation,Yes.,3,"""Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real world use cases for which people have been adopting them.""",2023,July,,
acl2023,Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM’s Translation Capability,Yes.,1,"""We investigate the role of incidental bilingualism—the unintentional consumption of bilingual signals, including translation examples—in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study.""",2023,July,,
acl2023,I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation,Yes.,3,"""Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it?"" and ""can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms?""",2023,July,,
acl2023,SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models,,,,2023,July,,
acl2023,Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction,Yes.,1,"""Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.""",2023,July,,
acl2023,Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,,,,2023,July,,
acl2023,Improved Instruction Ordering in Recipe-Grounded Conversation,Yes.,3,"""Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order."" and ""we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about half of which are out-of-order",2023,July,,
acl2023,Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions,Yes.,3,"""the complex computations performed within each layer have made their behavior somewhat opaque"" and ""collocational association and repetitions of the same token largely explain the language models’ predictions on these tasks.""",2023,July,,
acl2023,Language Detoxification with Attribute-Discriminative Latent Space,Yes.,3,"""Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications.""",2023,July,,
acl2023,Revisiting Token Dropping Strategy in Efficient BERT Pretraining,Yes.,3,"""Token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks.""",2023,July,,
acl2023,Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships,Yes.,1,"""we show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context.""",2023,July,,
acl2023,Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations,Yes.,3,"""the generalization behavior of ICL remains poorly understood"" and ""we find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases.""",2023,July,,
acl2023,Introducing Semantics into Speech Encoders,Yes.,1,"""Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM.""",2023,July,,
acl2023,SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control,Yes.,1,"""Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models.""",2023,July,,
acl2023,"Recall, Expand, and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",Yes.,1,"""CE concatenates a mention (and its context) with each type and feeds the pair into a pretrained language model (PLM) to score their relevance.""",2023,July,,
acl2023,"Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",Yes.,3,"""Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models.""",2023,July,,
acl2023,SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT,Yes.,1,"""Our findings call for further research using our novel Transformer-based SLA models and we would like to encourage it by releasing our code, data, and models.""",2023,July,,
acl2023,Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models,Yes.,3,"""existing models are often overly confident on unseen classes.""",2023,July,,
acl2023,Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,Yes.,3,"""Overall, our study provides several insights that indicate large language models may be under-trained for in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning.""",2023,July,,
acl2023,ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain,Yes.,1,"""In this study, we introduce a language model called ESCOXLM-R, based on XLM-R-large, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages.""",2023,July,,
acl2023,On the Blind Spots of Model-Based Evaluation Metrics for Text Generation,,,,2023,July,,
acl2023,Downstream Datasets Make Surprisingly Good Pretraining Corpora,Yes.,2,"""These findings are especially relevant in light of concerns about intellectual property and offensive content in web-scale pretraining data.""",2023,July,,
acl2023,Self-Instruct: Aligning Language Models with Self-Generated Instructions,Yes.,2,"""they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model.""",2023,July,,
acl2023,LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion,Yes.,1,"""Our framework consists of two modules",2023,July,,
acl2023,Python Code Generation by Asking Clarification Questions,Yes.,3,"""While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified.""",2023,July,,
acl2023,Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models,Yes.,1,"""we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models.""",2023,July,,
acl2023,Large Language Models Are Reasoning Teachers,Yes.,3,"""prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale.""",2023,July,,
acl2023,Visually-augmented pretrained language models for NLP tasks without images,Yes.,3,"""they are found lack of visual semantics or commonsense.""",2023,July,,
acl2023,FERMAT: An Alternative to Accuracy for Numerical Reasoning,Yes.,3,"""While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning."" and ""Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone.""",2023,July,,
acl2023,On Improving Summarization Factual Consistency from Natural Language Feedback,Yes.,3,"""fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.""",2023,July,,
acl2023,From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models,Yes.,3,"""We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3’s performance varies widely across types of dogwhistles and targeted groups.""",2023,July,,
acl2023,CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors,Yes.,3,"""it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text.""",2023,July,,
acl2023,Prompting PaLM for Translation: Assessing Strategies and Performance,Yes.,3,"""find that its performance, while impressive, still lags that of state-of-the-art supervised systems.""",2023,July,,
acl2023,Revisiting Relation Extraction in the era of Large Language Models,Yes.,1,"""Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision.""",2023,July,,
acl2023,Can Large Language Models Be an Alternative to Human Evaluations?,Yes.,3,"""We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.""",2023,July,,
acl2023,An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models,,,,2023,July,,
acl2023,XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations,,,,2023,July,,
acl2023,LENS: A Learnable Evaluation Metric for Text Simplification,Yes.,1,"""Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation.""",2023,July,,
acl2023,Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques,Yes.,3,"""Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs).""",2023,July,,
acl2023,Credible without Credit: Domain Experts Assess Generative Language Models,Yes.,3,"""While we find the results are consistently cohesive and concise, we find that they are mixed in their accuracy. These results raise questions of the role language models should play in general-purpose and expert knowledge seeking.""",2023,July,,
acl2023,MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models,Yes.,2,"""However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning.""",2023,July,,
acl2023,In and Out-of-Domain Text Adversarial Robustness via Label Smoothing,Yes.,1,"""Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions).""",2023,July,,
acl2023,LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning,Yes.,1,"""In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets.""",2023,July,,
acl2023,Exploring Continual Learning for Code Generation Models,Yes.,3,"""re-training large-scale language models is computationally expensive"" and ""effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks.""",2023,July,,
acl2023,A Better Way to Do Masked Language Model Scoring,,,,2023,July,,
acl2023,ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?,Yes.,3,"""Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems.""",2023,July,,
acl2023,Controllable Mixed-Initiative Dialogue Generation through Prompting,Yes.,2,"""these supervised generation models are limited by the cost and quality of data annotation.""",2023,July,,
acl2023,Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement,Yes.,3,"""These methods achieve limited gains with GCNs and have difficulty using BERT wordpieces.""",2023,July,,
acl2023,Do GPTs Produce Less Literal Translations?,Yes.,2,"""However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models.""",2023,July,,
acl2023,Black-box language model explanation by context length probing,Yes.,2,"""We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies.""",2023,July,,
acl2023,Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings,Yes.,1,"""We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance.""",2023,July,,
acl2023,Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning,,,,2023,July,,
acl2023,Evaluating pragmatic abilities of image captioners on A3DS,Yes.,1,"""Evaluating grounded neural language model performance with respect to pragmatic qualities like the trade off between truthfulness, contrastivity and overinformativity of generated utterances remains a challenge in absence of data collected from humans.""",2023,July,,
acl2023,Discourse-Level Representations can Improve Prediction of Degree of Anxiety,Yes.,1,"""evaluating the utility of discourse-level information in addition to lexical-level large language model embeddings.""",2023,July,,
acl2023,MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting,Yes.,1,"""Large language models (LLMs) have achieved impressive performance on various reasoning tasks.""",2023,July,,
acl2023,S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering,Yes.,1,"""This includes two approaches",2023,July,,
acl2023,AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models,Yes.,1,"""we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM).""",2023,July,,
acl2023,Revisiting Automated Prompting: Are We Actually Doing Better?,Yes.,3,"""We find that automated prompting does not consistently outperform simple manual prompting.""",2023,July,,
acl2023,Lingxi: A Diversity-aware Chinese Modern Poetry Generation System,Yes.,1,"""we propose a novel sampling algorithm that flattens the high likelihood part of the predicted distribution of the language model to emphasize the comparatively low-likelihood words and increase the diversity of generated poetry.""",2023,July,,
acl2023,LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models,Yes.,1,"""we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks.""",2023,July,,
acl2023,Pipeline for modeling causal beliefs from natural language,Yes.,1,"""We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network.""",2023,July,,
acl2023,OpenICL: An Open-Source Framework for In-context Learning,Yes.,1,"""In recent years, In-context Learning (ICL) has gained increasing attention and emerged as the new paradigm for large language model (LLM) evaluation.""",2023,July,,
acl2023,Petals: Collaborative Inference and Fine-tuning of Large Models,Yes.,3,"""offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits.""",2023,July,,
acl2023,ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer,,,,2023,July,,
acl2023,Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values,Yes.,2,"""dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models.""",2023,July,,
acl2023,Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity,Yes.,2,"""Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases.""",2023,July,,
acl2023,Authorship Attribution of Late 19th Century Novels using GAN-BERT,No.,1,The paper does not discuss LLMs or their limitations.,2023,July,,
acl2023,Semantic Accuracy in Natural Language Generation: A Thesis Proposal,Yes.,2,"""We propose a novel method for evaluating semantic accuracy and discuss the importance of working towards a unified and objective benchmark for NLG metrics. We also review interpretability approaches which could help us pinpoint the sources of inaccuracies within the models and explore potential mitigation strategies.""",2023,July,,
acl2023,CWSeg: An Efficient and General Approach to Chinese Word Segmentation,Yes.,3,"""The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation.""",2023,July,,
acl2023,GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model,Yes.,3,"""the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods.""",2023,July,,
acl2023,The economic trade-offs of large language models: A case study,Yes.,2,"""However, their efficacy must be balanced with the cost of training and serving them.""",2023,July,,
acl2023,Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy,Yes.,1,"""The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes.""",2023,July,,
acl2023,SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels,,,,2023,July,,
acl2023,"Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding",Yes.,2,"""Larger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (especially on CPU machines).""",2023,July,,
acl2023,Exploring Zero and Few-shot Techniques for Intent Classification,Yes.,1,"""zero-shot intent classification using descriptions large language models (LLMs)"" and ""parameter-efficient fine-tuning of instruction-finetuned language models.""",2023,July,,
acl2023,Complex Reasoning in Natural Language,,,,2023,July,,
acl2023,Generating Text from Language Models,No.,1,"""No evidence""",2023,July,,
acl2023,Natural Language to Code Generation in Interactive Data Science Notebooks,No.,1,"""No evidence""",2023,July,,
acl2023,A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces,No.,1,"""No evidence""",2023,July,,
acl2023,Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation,No.,1,"""No evidence""",2023,July,,
acl2023,Revealing Single Frame Bias for Video-and-Language Learning,No.,1,"""No evidence""",2023,July,,
acl2023,World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models,No.,1,"""No evidence""",2023,July,,
acl2023,Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest,No.,1,"""No evidence""",2023,July,,
acl2023,Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling,No.,1,"""No evidence""",2023,July,,
acl2023,Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information,No.,1,"""No evidence""",2023,July,,
acl2023,Precise Zero-Shot Dense Retrieval without Relevance Labels,No.,1,"""No evidence""",2023,July,,
acl2023,Instruction Induction: From Few Examples to Natural Language Task Descriptions,No.,1,"""No evidence""",2023,July,,
acl2023,Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering,No.,1,"""No evidence""",2023,July,,
acl2023,ELQA: A Corpus of Metalinguistic Questions and Answers about English,No.,1,"""No evidence""",2023,July,,
acl2023,Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues,No.,1,"""No evidence""",2023,July,,
acl2023,Robust Multi-bit Natural Language Watermarking through Invariant Features,No.,1,"""No evidence""",2023,July,,
acl2023,SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval,No.,1,"""No evidence""",2023,July,,
acl2023,Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach,No.,1,"""No evidence""",2023,July,,
acl2023,Generating EDU Extracts for Plan-Guided Summary Re-Ranking,No.,1,"""No evidence""",2023,July,,
acl2023,RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks,No.,1,"""No evidence""",2023,July,,
acl2023,DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation,No.,1,"""No evidence""",2023,July,,
acl2023,Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning,No.,1,"""No evidence""",2023,July,,
acl2023,Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis,No.,1,"""No evidence""",2023,July,,
acl2023,Word sense extension,No.,1,"""No evidence""",2023,July,,
acl2023,From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding,No.,1,"""No evidence""",2023,July,,
acl2023,MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling,No.,1,"""No evidence""",2023,July,,
acl2023,Tree-Based Representation and Generation of Natural and Mathematical Language,No.,1,"""No evidence""",2023,July,,
acl2023,MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction,No.,1,"""No evidence""",2023,July,,
acl2023,CAME: Confidence-guided Adaptive Memory Efficient Optimization,No.,1,"""No evidence""",2023,July,,
acl2023,DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models,No.,1,"""No evidence""",2023,July,,
acl2023,Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation,No.,1,"""No evidence""",2023,July,,
acl2023,Unified Demonstration Retriever for In-Context Learning,No.,1,"""No evidence""",2023,July,,
acl2023,Hidden Schema Networks,No.,1,"""No evidence""",2023,July,,
acl2023,Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization,No.,1,"""No evidence""",2023,July,,
acl2023,Unsupervised Graph-Text Mutual Conversion with a Unified Pretrained Language Model,No.,1,"""No evidence""",2023,July,,
acl2023,Reasoning with Language Model Prompting: A Survey,No.,1,"""No evidence""",2023,July,,
acl2023,Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training,No.,1,"""No evidence""",2023,July,,
acl2023,Toward Human-Like Evaluation for Natural Language Generation with Error Analysis,No.,1,"""No evidence""",2023,July,,
acl2023,What is the best recipe for character-level encoder-only modelling?,No.,1,"""No evidence""",2023,July,,
acl2023,Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks,No.,1,"""No evidence""",2023,July,,
acl2023,ContraCLM: Contrastive Learning For Causal Language Model,No.,1,"""No evidence""",2023,July,,
acl2023,Interpretable Math Word Problem Solution Generation via Step-by-step Planning,No.,1,"""No evidence""",2023,July,,
acl2023,PairSpanBERT: An Enhanced Language Model for Bridging Resolution,No.,1,"""No evidence""",2023,July,,
acl2023,Few-shot In-context Learning on Knowledge Base Question Answering,No.,1,"""No evidence""",2023,July,,
acl2023,Fact-Checking Complex Claims with Program-Guided Reasoning,No.,1,"""No evidence""",2023,July,,
acl2023,Patton: Language Model Pretraining on Text-Rich Networks,No.,1,"""No evidence""",2023,July,,
acl2023,Learning Better Masking for Better Language Model Pre-training,No.,1,"""No evidence""",2023,July,,
acl2023,Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation,No.,1,"""No evidence""",2023,July,,
acl2023,Query Refinement Prompts for Closed-Book Long-Form QA,No.,1,"""No evidence""",2023,July,,
acl2023,HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation,No.,1,"""No evidence""",2023,July,,
acl2023,Dynamic Regularization in UDA for Transformers in Multimodal Classification,No.,1,"""No evidence""",2023,July,,
acl2023,"Synthesize, Prompt and Transfer: Zero-shot Conversational Question Generation with Pre-trained Language Model",No.,1,"""No evidence""",2023,July,,
acl2023,Backpack Language Models,No.,1,"""No evidence""",2023,July,,
acl2023,Span-level Aspect-based Sentiment Analysis via Table Filling,No.,1,"""No evidence""",2023,July,,
acl2023,Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation,No.,1,"""No evidence""",2023,July,,
acl2023,A Measure-Theoretic Characterization of Tight Language Models,No.,1,"""No evidence""",2023,July,,
acl2023,PromptRank: Unsupervised Keyphrase Extraction Using Prompt,No.,1,"""No evidence""",2023,July,,
acl2023,Direct Fact Retrieval from Knowledge Graphs without Entity Linking,No.,1,"""No evidence""",2023,July,,
acl2023,The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers,No.,1,"""No evidence""",2023,July,,
acl2023,FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering,No.,1,"""No evidence""",2023,July,,
acl2023,Rethinking Masked Language Modeling for Chinese Spelling Correction,No.,1,"""No evidence""",2023,July,,
acl2023,A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues,No.,1,"""No evidence""",2023,July,,
acl2023,Is GPT-3 a Good Data Annotator?,No.,1,"""No evidence""",2023,July,,
acl2023,Few-shot Event Detection: An Empirical Study and a Unified View,No.,1,"""No evidence""",2023,July,,
acl2023,AlignScore: Evaluating Factual Consistency with A Unified Alignment Function,No.,1,"""No evidence""",2023,July,,
acl2023,Query-Efficient Black-Box Red Teaming via Bayesian Optimization,No.,1,"""No evidence""",2023,July,,
acl2023,BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting,No.,1,"""No evidence""",2023,July,,
acl2023,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,No.,1,"""No evidence""",2023,July,,
acl2023,Generalizing Backpropagation for Gradient-Based Interpretability,No.,1,"""No evidence""",2023,July,,
acl2023,Generic Temporal Reasoning with Differential Analysis and Explanation,No.,1,"""No evidence""",2023,July,,
acl2023,WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings,No.,1,"""No evidence""",2023,July,,
acl2023,DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization,No.,1,"""No evidence""",2023,July,,
acl2023,Resolving Indirect Referring Expressions for Entity Selection,No.,1,"""No evidence""",2023,July,,
acl2023,"Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",No.,1,"""No evidence""",2023,July,,
acl2023,Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks,No.,1,"""No evidence""",2023,July,,
acl2023,Large-scale Lifelong Learning of In-context Instructions and How to Tackle It,No.,1,"""No evidence""",2023,July,,
acl2023,DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function,No.,1,"""No evidence""",2023,July,,
acl2023,Effective Contrastive Weighting for Dense Query Expansion,No.,1,"""No evidence""",2023,July,,
acl2023,Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model,No.,1,"""No evidence""",2023,July,,
acl2023,MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,No.,1,"""No evidence""",2023,July,,
acl2023,UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization,No.,1,"""No evidence""",2023,July,,
acl2023,Exploring and Verbalizing Academic Ideas by Concept Co-occurrence,No.,1,"""No evidence""",2023,July,,
acl2023,UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language,No.,1,"""No evidence""",2023,July,,
acl2023,Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis,No.,1,"""No evidence""",2023,July,,
acl2023,Entailment as Robust Self-Learner,No.,1,"""No evidence""",2023,July,,
acl2023,ReCode: Robustness Evaluation of Code Generation Models,No.,1,"""No evidence""",2023,July,,
acl2023,PAD-Net: An Efficient Framework for Dynamic Networks,No.,1,"""No evidence""",2023,July,,
acl2023,Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor,No.,1,"""No evidence""",2023,July,,
acl2023,A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training,No.,1,"""No evidence""",2023,July,,
acl2023,Exploring Large Language Models for Classical Philology,No.,1,"""No evidence""",2023,July,,
acl2023,DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media,No.,1,"""No evidence""",2023,July,,
acl2023,LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development,No.,1,"""No evidence""",2023,July,,
acl2023,Few-shot Reranking for Multi-hop QA via Language Model Prompting,No.,1,"""No evidence""",2023,July,,
acl2023,Crosslingual Generalization through Multitask Finetuning,No.,1,"""No evidence""",2023,July,,
acl2023,Tracing Linguistic Markers of Influence in a Large Online Organisation,No.,1,"""No evidence""",2023,July,,
acl2023,Dataset Distillation with Attention Labels for Fine-tuning BERT,No.,1,"""No evidence""",2023,July,,
acl2023,BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases,No.,1,"""No evidence""",2023,July,,
acl2023,Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer,No.,1,"""No evidence""",2023,July,,
acl2023,PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English,No.,1,"""No evidence""",2023,July,,
acl2023,Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications,No.,1,"""No evidence""",2023,July,,
acl2023,Efficient Diagnosis Assignment Using Unstructured Clinical Notes,No.,1,"""No evidence""",2023,July,,
acl2023,Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity,No.,1,"""No evidence""",2023,July,,
acl2023,Counterfactual reasoning: Testing language models’ understanding of hypothetical scenarios,No.,1,"""No evidence""",2023,July,,
acl2023,Gradient Ascent Post-training Enhances Language Model Generalization,No.,1,"""No evidence""",2023,July,,
acl2023,ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models,No.,1,"""No evidence""",2023,July,,
acl2023,Hexatagging: Projective Dependency Parsing as Tagging,No.,1,"""No evidence""",2023,July,,
acl2023,MolXPT: Wrapping Molecules with Text for Generative Pre-training,No.,1,"""No evidence""",2023,July,,
acl2023,NarrowBERT: Accelerating Masked Language Model Pretraining and Inference,No.,1,"""No evidence""",2023,July,,
acl2023,Teaching Small Language Models to Reason,No.,1,"""No evidence""",2023,July,,
acl2023,How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives,No.,1,"""No evidence""",2023,July,,
acl2023,Linear Classifier: An Often-Forgotten Baseline for Text Classification,No.,1,"""No evidence""",2023,July,,
acl2023,Human-in-the-loop Schema Induction,No.,1,"""No evidence""",2023,July,,
acl2023,The ROOTS Search Tool: Data Transparency for LLMs,No.,1,"""No evidence""",2023,July,,
acl2023,Inseq: An Interpretability Toolkit for Sequence Generation Models,No.,1,"""No evidence""",2023,July,,
acl2023,Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals,No.,1,"""No evidence""",2023,July,,
acl2023,A System for Answering Simple Questions in Multiple Languages,No.,1,"""No evidence""",2023,July,,
acl2023,Disease Network Constructor: a Pathway Extraction and Visualization,No.,1,"""No evidence""",2023,July,,
acl2023,DeepPavlov Dream: Platform for Building Generative AI Assistants,No.,1,"""No evidence""",2023,July,,
acl2023,The Turing Quest: Can Transformers Make Good NPCs?,No.,1,"""No evidence""",2023,July,,
acl2023,MedTem2.0: Prompt-based Temporal Classification of Treatment Events from Discharge Summaries,No.,1,"""No evidence""",2023,July,,
acl2023,Building a Buzzer-quiz Answering System,No.,1,"""No evidence""",2023,July,,
acl2023,Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference,No.,1,"""No evidence""",2023,July,,
acl2023,LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism,No.,1,"""No evidence""",2023,July,,
acl2023,Math Word Problem Solving by Generating Linguistic Variants of Problem Statements,No.,1,"""No evidence""",2023,July,,
acl2023,Label efficient semi-supervised conversational intent classification,No.,1,"""No evidence""",2023,July,,
acl2023,Chemical Language Understanding Benchmark,No.,1,"""No evidence""",2023,July,,
acl2023,HyperT5: Towards Compute-Efficient Korean Language Modeling,No.,1,"""No evidence""",2023,July,,
acl2023,DISCOSQA: A Knowledge Base Question Answering System for Space Debris based on Program Induction,No.,1,"""No evidence""",2023,July,,
acl2023,BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting,No.,1,"""No evidence""",2023,July,,
acl2023,Evaluating Embedding APIs for Information Retrieval,No.,1,"""No evidence""",2023,July,,
acl2023,RadLing: Towards Efficient Radiology Report Understanding,No.,1,"""No evidence""",2023,July,,
acl2023,Weakly supervised hierarchical multi-task classification of customer questions,No.,1,"""No evidence""",2023,July,,
eacl2023,Do we need Label Regularization to Fine-tune Pre-trained Language Models?,Yes.,1,"""Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs.""",2023,May,,
eacl2023,A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction,Yes.,3,"""LLMs must overcome frequency biases in order to master such constructions.""",2023,May,,
eacl2023,Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation,Yes.,1,"""PGen randomly concatenates sentences from the original in-domain spoken language text data as prompts to induce a pre-trained language model (i.e., GPT-2) to generate spoken language texts in a similar style.""",2023,May,,
eacl2023,Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers,Yes.,3,"""Quantization-aware training (QAT) is a promising method to lower the implementation cost and energy consumption. However, aggressive quantization below 2-bit causes considerable accuracy degradation due to unstable convergence, especially when the downstream dataset is not abundant.""",2023,May,,
eacl2023,A Systematic Search for Compound Semantics in Pretrained BERT Architectures,Yes.,3,"""To date, transformer-based models such as BERT have been less successful in predicting compositionality of noun compounds than static word embeddings. This is likely related to a suboptimal use of the encoded information, reflecting an incomplete grasp of how the models represent the meanings of complex linguistic structures.""",2023,May,,
eacl2023,LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation,Yes.,3,"""these large-scale models can suffer from inference speed and computation overhead.""",2023,May,,
eacl2023,Extracting Victim Counts from Text,Yes.,2,"""Beyond model accuracy, we analyze extraction reliability and robustness which are key for this sensitive task. In particular, we discuss model calibration and investigate out-of-distribution and few-shot performance.""",2023,May,,
eacl2023,Bootstrapping Multilingual Semantic Parsers using Large Language Models,Yes.,2,"""Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models.""",2023,May,,
eacl2023,Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow,Yes.,3,"""Recent research has shown that language models exploit ‘artifacts’ in benchmarks to solve tasks, rather than truly learning them, leading to inflated model performance.""",2023,May,,
eacl2023,Unsupervised Improvement of Factual Knowledge in Language Models,Yes.,1,"""Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge.""",2023,May,,
eacl2023,Learning to Ignore Adversarial Attacks,No.,1,The abstract does not mention language models (LLMs) or their limitations.,2023,May,,
eacl2023,Should You Mask 15% in Masked Language Modeling?,Yes.,1,"""Masked language models (MLMs) conventionally mask 15% of tokens due to the belief that more masking would leave insufficient context to learn good representations; this masking rate has been widely used, regardless of model sizes or masking strategies.""",2023,May,,
eacl2023,BERT Shows Garden Path Effects,Yes.,3,"""We find that the models have relatively low performance in certain instances of question answering based on garden path contexts, and the model incorrectly assigns semantic roles.""",2023,May,,
eacl2023,DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,Yes.,3,"""While LoRA blocks are parameter-efficient, they suffer from two major problems",2023,May,,
eacl2023,Social Commonsense for Explanation and Cultural Bias Discovery,Yes.,2,"""We identify influential social commonsense knowledge to explain model behavior in the following ways. First, we augment large-scale language models with social knowledge and show improvements for the tasks, indicating the implicit assumptions a model requires to be successful on each dataset.""",2023,May,,
eacl2023,"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",Yes.,2,"""manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models.""",2023,May,,
eacl2023,DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence,No.,1,The abstract discusses BERT-based evaluation metrics and their limitations but does not address language models (LLMs or LLMs) specifically.,2023,May,,
eacl2023,CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,Yes.,3,"""a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation.""",2023,May,,
eacl2023,Self-imitation Learning for Action Generation in Text-based Games,No.,1,"""No evidence""",2023,May,,
eacl2023,The Functional Relevance of Probed Information: A Case Study,No.,1,"""No evidence""",2023,May,,
eacl2023,Parameter-Efficient Tuning with Special Token Adaptation,No.,1,"""No evidence""",2023,May,,
eacl2023,Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model,No.,1,"""No evidence""",2023,May,,
eacl2023,K-hop neighbourhood regularization for few-shot learning on graphs: A case study of text classification,No.,1,"""No evidence""",2023,May,,
eacl2023,Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information,No.,1,"""No evidence""",2023,May,,
eacl2023,FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning,No.,1,"""No evidence""",2023,May,,
eacl2023,Fiction-Writing Mode: An Effective Control for Human-Machine Collaborative Writing,No.,1,"""No evidence""",2023,May,,
eacl2023,Unified Neural Topic Model via Contrastive Learning and Term Weighting,No.,1,"""No evidence""",2023,May,,
eacl2023,GLADIS: A General and Large Acronym Disambiguation Benchmark,No.,1,"""No evidence""",2023,May,,
eacl2023,A Psycholinguistic Analysis of BERT’s Representations of Compounds,No.,1,"""No evidence""",2023,May,,
eacl2023,UDAPTER - Efficient Domain Adaptation Using Adapters,No.,1,"""No evidence""",2023,May,,
eacl2023,Exploring Category Structure with Contextual Language Models and Lexical Semantic Networks,No.,1,"""No evidence""",2023,May,,
eacl2023,Penguins Don’t Fly: Reasoning about Generics through Instantiations and Exceptions,No.,1,"""No evidence""",2023,May,,
eacl2023,SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains,No.,1,"""No evidence""",2023,May,,
eacl2023,Do dialogue representations align with perception? An empirical study,No.,1,"""No evidence""",2023,May,,
eacl2023,Salient Span Masking for Temporal Understanding,No.,1,"""No evidence""",2023,May,,
eacl2023,Contextual Dynamic Prompting for Response Generation in Task-oriented Dialog Systems,No.,1,"""No evidence""",2023,May,,
eacl2023,A simple but effective model for attachment in discourse parsing with multi-task learning for relation labeling,No.,1,"""No evidence""",2023,May,,
eacl2023,Semantic Specialization for Knowledge-based Word Sense Disambiguation,No.,1,"""No evidence""",2023,May,,
eacl2023,BERT Is Not The Count: Learning to Match Mathematical Statements with Proofs,No.,1,"""No evidence""",2023,May,,
eacl2023,Retrieval-augmented Image Captioning,No.,1,"""No evidence""",2023,May,,
eacl2023,Representation biases in sentence transformers,No.,1,"""No evidence""",2023,May,,
emnlp2023,Know your audience: specializing grounded language models with listener subtraction,Yes.,2,"""common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions.""",2023,December,,
emnlp2023,Establishing Trustworthiness: Rethinking Tasks and Model Evaluation,Yes.,2,"""the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis.""",2023,December,,
emnlp2023,Evaluating Object Hallucination in Large Vision-Language Models,,,,2023,December,,
emnlp2023,Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients,Yes.,3,"""Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming."" and ""However, they can only save approximately 30% of the training memory requirements, due to the problem that gradient computation and backpropagation are still necessary for these methods.""",2023,December,,
emnlp2023,How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning,Yes.,3,"""Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships.""",2023,December,,
emnlp2023,Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning,Yes.,1,"""We observe that naive LLMs perform on par with SOTA models,"" and ""Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond frequency and recency biases.""",2023,December,,
emnlp2023,Knowledge Graph Compression Enhances Diverse Commonsense Generation,Yes.,3,"""the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model."" and ""our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters.""",2023,December,,
emnlp2023,LLM-FP4: 4-Bit Floating-Point Quantized Transformers,Yes.,1,"""We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner.""",2023,December,,
emnlp2023,LLM-powered Data Augmentation for Enhanced Cross-lingual Performance,Yes.,3,"""LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil"" and ""ChatGPT falls short in generating plausible alternatives compared to the original dataset.""",2023,December,,
emnlp2023,"Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms",Yes.,3,"""We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation.""",2023,December,,
emnlp2023,Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models,Yes.,1,"""considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge.""",2023,December,,
emnlp2023,Large Language Models Can Self-Improve,Yes.,3,"""fine-tuning an LLM requires extensive supervision.""",2023,December,,
emnlp2023,SeqXGPT: Sentence-Level AI-Generated Text Detection,Yes.,2,"""raising concerns about the abuse of LLMs"" and ""Current works only consider document-level AIGT detection.""",2023,December,,
emnlp2023,QTSumm: Query-Focused Summarization over Tabular Data,,,,2023,December,,
emnlp2023,Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models,Yes.,3,"""Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity.""",2023,December,,
emnlp2023,Is ChatGPT a General-Purpose Natural Language Processing Task Solver?,Yes.,2,"""With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT.""",2023,December,,
emnlp2023,Transcending Scaling Laws with 0.1% Extra Compute,Yes.,1,"""Scaling language models improves performance but comes with significant computational costs.""",2023,December,,
emnlp2023,CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation,Yes.,2,"""However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives.""",2023,December,,
emnlp2023,Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs,Yes.,3,"""demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task.""",2023,December,,
emnlp2023,Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks,Yes.,1,"""Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions.""",2023,December,,
emnlp2023,ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness,Yes.,1,"""The emergence of generative large language models (LLMs) raises the question",2023,December,,
emnlp2023,Democratizing Reasoning Ability: Tailored Learning from Large Language Model,Yes.,3,"""LLMs exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature.""",2023,December,,
emnlp2023,OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization,Yes.,1,"""we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.""",2023,December,,
emnlp2023,Self-Influence Guided Data Reweighting for Language Model Pre-training,Yes.,3,"""Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice.""",2023,December,,
emnlp2023,TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models,,,,2023,December,,
emnlp2023,Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning,Yes.,3,"""We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks."" and ""Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing.""",2023,December,,
emnlp2023,Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency,Yes.,1,"""To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items.""",2023,December,,
emnlp2023,Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index (ADI),Yes.,3,"""Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny.""",2023,December,,
emnlp2023,TempTabQA: Temporal Question Answering for Semi-Structured Tables,,,,2023,December,,
emnlp2023,Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task,Yes.,3,"""the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks.""",2023,December,,
emnlp2023,G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment,Yes.,3,"""these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators"" and ""highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.""",2023,December,,
emnlp2023,Improving Summarization with Human Edits,Yes.,1,"""Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training.""",2023,December,,
emnlp2023,BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology,Yes.,,,2023,December,,
emnlp2023,Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages,Yes.,3,"""Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development.""",2023,December,,
emnlp2023,FinGPT: Large Generative Models for a Small Language,Yes.,3,"""LLM work tends to focus on languages where nearly unlimited data is available for pretraining.""",2023,December,,
emnlp2023,"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",Yes.,1,"""As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks.""",2023,December,,
emnlp2023,Can Language Models Laugh at YouTube Short-form Videos?,Yes.,1,"""we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs).""",2023,December,,
emnlp2023,Lion: Adversarial Distillation of Proprietary Large Language Models,,,,2023,December,,
emnlp2023,"DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding",Yes.,3,"""Our analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question.""",2023,December,,
emnlp2023,"Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",,,,2023,December,,
emnlp2023,Can LLMs Facilitate Interpretation of Pre-trained Language Models?,Yes.,3,"""Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation.""",2023,December,,
emnlp2023,Knowledge Rumination for Pre-trained Language Models,Yes.,3,"""vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone"" and ""PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive tasks.""",2023,December,,
emnlp2023,Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning,Yes.,1,"""limited researches utilize it for aligning representation in multilingual pre-trained language models (PLMs).""",2023,December,,
emnlp2023,Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA,Yes.,3,"""current human evaluation methods fail to provide a clear understanding of systems’ specific strengths and weaknesses"" and ""GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors.""",2023,December,,
emnlp2023,GPT-RE: In-context Learning for Relation Extraction using Large Language Models,,,,2023,December,,
emnlp2023,INFORM : Information eNtropy based multi-step reasoning FOR large language Models,Yes.,3,"""the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting.""",2023,December,,
emnlp2023,Adaptive Gating in Mixture-of-Experts based Language Models,Yes.,2,"""Little is discussed in prior research on the trade-off between computation per token and model performance.""",2023,December,,
emnlp2023,On the Automatic Generation and Simplification of Children’s Stories,Yes.,3,"""We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups.""",2023,December,,
emnlp2023,Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning,,,,2023,December,,
emnlp2023,ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph,Yes.,1,"""Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions.""",2023,December,,
emnlp2023,Deep Natural Language Feature Learning for Interpretable Prediction,,,,2023,December,,
emnlp2023,COVID-19 Vaccine Misinformation in Middle Income Countries,Yes.,1,"""we adopt two approaches for developing COVID-19 vaccine misinformation detection models",2023,December,,
emnlp2023,Contrastive Learning of Sentence Embeddings from Scratch,Yes.,1,"""we explore utilizing large language models to synthesize the required data samples for contrastive learning.""",2023,December,,
emnlp2023,DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining,Yes.,3,"""a significant challenge that arises nowadays is how to maintain performance when we use a lightweight model with limited labeled samples.""",2023,December,,
emnlp2023,Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation,Yes.,1,"""Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning.""",2023,December,,
emnlp2023,Sparse Low-rank Adaptation of Pre-trained Language Models,Yes.,1,"""Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency.""",2023,December,,
emnlp2023,The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models,Yes.,3,"""CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers.""",2023,December,,
emnlp2023,MEGA: Multilingual Evaluation of Generative AI,Yes.,3,"""We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages.""",2023,December,,
emnlp2023,Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation,Yes.,1,"""The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks.""",2023,December,,
emnlp2023,DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models,Yes.,1,The abstract discusses the use of large language models for machine translation but does not mention any explicit limitations of the models.,2023,December,,
emnlp2023,Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue,Yes.,1,"""knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.""",2023,December,,
emnlp2023,A Cheaper and Better Diffusion Language Model with Soft-Masked Noise,Yes.,3,"""existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high.""",2023,December,,
emnlp2023,Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?,Yes.,3,"""Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs.""",2023,December,,
emnlp2023,Can We Edit Factual Knowledge by In-Context Learning?,Yes.,3,"""However, the stored knowledge could be false or outdated.""",2023,December,,
emnlp2023,Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge,Yes.,1,"""We leverage large language models (LLMs) to extract the aforementioned action-object knowledge.""",2023,December,,
emnlp2023,LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers,Yes.,3,"""While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways.""",2023,December,,
emnlp2023,LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,Yes.,1,"""The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or",2023,December,,
emnlp2023,PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation,,,,2023,December,,
emnlp2023,QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing,Yes.,3,"""Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality.""",2023,December,,
emnlp2023,PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter,Yes.,2,"""they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs.""",2023,December,,
emnlp2023,Exploring Chain of Thought Style Prompting for Text-to-SQL,Yes.,3,"""However, its performance on text-to-SQL parsing still has much room for improvement."" and ""using detailed reasoning steps tends to have more error propagation issues.""",2023,December,,
emnlp2023,Harnessing Black-Box Control to Boost Commonsense in LM’s Generation,Yes.,3,"""a crucial issue persists",2023,December,,
emnlp2023,Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback,,,,2023,December,,
emnlp2023,Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models,Yes.,3,"""vanilla in-context learning is infeasible for DocRE due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs.""",2023,December,,
emnlp2023,Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents,Yes.,3,"""Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge.""",2023,December,,
emnlp2023,C-STS: Conditional Semantic Textual Similarity,,,,2023,December,,
emnlp2023,ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models,Yes.,3,"""LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting."" and ""We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots.""",2023,December,,
emnlp2023,Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings,Yes.,3,"""Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks.""",2023,December,,
emnlp2023,INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback,Yes.,2,"""Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text.""",2023,December,,
emnlp2023,Towards Interpretable Mental Health Analysis with Large Language Models,Yes.,3,"""However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability.""",2023,December,,
emnlp2023,MoT: Memory-of-Thought Enables ChatGPT to Self-Improve,Yes.,3,"""However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning.""",2023,December,,
emnlp2023,"Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",Yes.,2,"""Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes.""",2023,December,,
emnlp2023,Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning,Yes.,3,"""LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains.""",2023,December,,
emnlp2023,Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation,Yes.,1,"""Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model.""",2023,December,,
emnlp2023,EtiCor: Corpus for Analyzing LLMs for Etiquettes,Yes.,2,"""Initial results indicate that LLMs, mostly fail to understand etiquettes from regions from non-Western world.""",2023,December,,
emnlp2023,ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters,Yes.,1,"""Training target LA requires unlabeled data, which may not be readily available for low resource *unseen* languages",2023,December,,
emnlp2023,Log-FGAER: Logic-Guided Fine-Grained Address Entity Recognition from Multi-Turn Spoken Dialogue,Yes.,1,"""we provide an ontology-based data augmentation methodology that employs ChatGPT to augment a spoken dialogue dataset with labeled address entities.""",2023,December,,
emnlp2023,Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning,Yes.,3,"""this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format.""",2023,December,,
emnlp2023,Benchmarking and Improving Text-to-SQL Generation under Ambiguity,Yes.,3,"""We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal.""",2023,December,,
emnlp2023,Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning,Yes.,1,"""GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search.""",2023,December,,
emnlp2023,Universal Self-Adaptive Prompting,Yes.,3,"""zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable.""",2023,December,,
emnlp2023,Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT,Yes.,2,"""Despite their great potential, LLMs also incur serious concerns as they are likely to be misused.""",2023,December,,
emnlp2023,Faithful Model Evaluation for Model-Based Metrics,Yes.,3,"""Existing works usually do not consider the variance change due to metric model errors, which can lead to wrong conclusions.""",2023,December,,
emnlp2023,SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables,Yes.,3,"""SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing.""",2023,December,,
emnlp2023,TheoremQA: A Theorem-driven Question Answering Dataset,Yes.,2,"""However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated.""",2023,December,,
emnlp2023,Automatic Prompt Optimization with “Gradient Descent” and Beam Search,,,,2023,December,,
emnlp2023,DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models,Yes.,3,"""Chain-of-Thought (CoT) prompting has successfully enhanced the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective, or even detrimental, to the performance on reasoning tasks in Smaller Language Models (SLMs",2023,December,,
emnlp2023,LLM-enhanced Self-training for Cross-domain Constituency Parsing,Yes.,1,"""To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively.""",2023,December,,
emnlp2023,Editing Common Sense in Transformers,Yes.,3,"""Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers’ reliability and usefulness.""",2023,December,,
emnlp2023,IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions,,,,2023,December,,
emnlp2023,How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances,Yes.,3,"""Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era.""",2023,December,,
emnlp2023,DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4,Yes.,2,"""It is also unclear if there are other hidden factors influencing human judgments.""",2023,December,,
emnlp2023,Generating Data for Symbolic Language with Large Language Models,,,,2023,December,,
emnlp2023,DALE: Generative Data Augmentation for Low-Resource Legal NLP,Yes.,1,"""DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with absolute improvements of 1%-50%.""",2023,December,,
emnlp2023,trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback,Yes.,1,"""Reinforcement learning from human feedback (RLHF) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model.""",2023,December,,
emnlp2023,Regulation and NLP (RegNLP): Taming Large Language Models,Yes.,2,"""important debates emerge regarding the benefits and risks of their development, deployment and use"" and ""highlighting the shortcomings of current NLP discussions dealing with risk assessment.""",2023,December,,
emnlp2023,"MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation",Yes.,2,"""Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models.""",2023,December,,
emnlp2023,Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks,Yes.,2,"""We aim to improve the understanding of current models’ performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs"" and ""the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models.""",2023,December,,
emnlp2023,“Mistakes Help Us Grow”: Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms,Yes.,1,"""We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers’ use of GMSL.""",2023,December,,
emnlp2023,Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text,Yes.,2,"""While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear.""",2023,December,,
emnlp2023,SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,,,,2023,December,,
emnlp2023,Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications,Yes.,1,"""Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements.""",2023,December,,
emnlp2023,APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models,Yes.,2,"""most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement.""",2023,December,,
emnlp2023,Learning Preference Model for LLMs via Automatic Preference Data Generation,Yes.,3,"""Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc.""",2023,December,,
emnlp2023,Revisiting Automated Topic Model Evaluation with Large Language Models,Yes.,2,"""the setup of the evaluation task is crucial — LLMs perform better on coherence ratings of word sets than on intrusion detection.""",2023,December,,
emnlp2023,ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization,Yes.,1,"""Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs).""",2023,December,,
emnlp2023,Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration,Yes.,2,"""existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas.""",2023,December,,
emnlp2023,Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts,Yes.,3,"""The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy.""",2023,December,,
emnlp2023,On Bilingual Lexicon Induction with Large Language Models,Yes.,2,"""We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.""",2023,December,,
emnlp2023,"CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model",Yes.,2,"""However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable.""",2023,December,,
emnlp2023,Quantifying the redundancy between prosody and text,Yes.,1,"""We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves.""",2023,December,,
emnlp2023,Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning,Yes.,3,"""the underlying mechanism of how LLMs learn from the provided context remains under-explored.""",2023,December,,
emnlp2023,Prompting Scientific Names for Zero-Shot Species Recognition,Yes.,3,"""However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., 'a photo of Lepus Timidus' (",2023,December,,
emnlp2023,MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark,Yes.,2,"""There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings.""",2023,December,,
emnlp2023,Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?,,,,2023,December,,
emnlp2023,Reducing Sequence Length by Predicting Edit Spans with Large Language Models,Yes.,3,"""the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer.""",2023,December,,
emnlp2023,Instruct and Extract: Instruction Tuning for On-Demand Information Extraction,Yes.,2,"""However, when it comes to information extraction – a classic task in natural language processing – most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users.""",2023,December,,
emnlp2023,Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models,Yes.,2,"""revealing the inadequacy of the existing evaluation protocol"" and ""To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs.""",2023,December,,
emnlp2023,Contrastive Learning for Inference in Dialogue,Yes.,3,"""While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning.""",2023,December,,
emnlp2023,"Editing Large Language Models: Problems, Methods, and Opportunities",Yes.,3,"""Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive."" and ""an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal.""",2023,December,,
emnlp2023,The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining,Yes.,3,"""Our results illustrate our limited understanding of model pretraining and provide future research directions.""",2023,December,,
emnlp2023,Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset,Yes.,3,"""While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels.""",2023,December,,
emnlp2023,Contextual Interaction for Argument Post Quality Assessment,Yes.,3,"""while LLMs with in-context examples showcase a commendable ability to identify high-quality argument posts, they exhibit relatively limited efficacy in discerning between argument posts with a narrow quality gap.""",2023,December,,
emnlp2023,People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection,Yes.,3,"""One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.""",2023,December,,
emnlp2023,Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning,Yes.,3,"""Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly.""",2023,December,,
emnlp2023,Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory,Yes.,2,"""identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics.""",2023,December,,
emnlp2023,Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation,Yes.,3,"""a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a satisfactory image, which is time-consuming and labor-intensive.""",2023,December,,
emnlp2023,clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents,Yes.,1,"""showing that current chat-optimised LLMs are, to an extent, capable of following game-play instructions.""",2023,December,,
emnlp2023,UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers,Yes.,1,"""To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply.""",2023,December,,
emnlp2023,Data Similarity is Not Enough to Explain Language Model Performance,Yes.,3,"""Large language models achieve high performance on many but not all downstream tasks,"" and ""This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.""",2023,December,,
emnlp2023,Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark,Yes.,3,"""we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks"" and ""points to clear room for improvement to build more socially-aware LLMs.""",2023,December,,
emnlp2023,CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs,Yes.,3,"""While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT.""",2023,December,,
emnlp2023,Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model,Yes.,3,"""While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute.""",2023,December,,
emnlp2023,Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces,Yes.,3,"""we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller.""",2023,December,,
emnlp2023,Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models,Yes.,2,"""their performance on text-rich images still requires improvement.""",2023,December,,
emnlp2023,Unlearn What You Want to Forget: Efficient Unlearning for LLMs,,,,2023,December,,
emnlp2023,Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration,Yes.,1,"""These can be broken down into two categories",2023,December,,
emnlp2023,CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks,Yes.,1,"""While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored.""",2023,December,,
emnlp2023,"Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations",Yes.,3,"""Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains. A promising alternative is to fine-tune relatively smaller language models (LMs) on a particular task using",2023,December,,
emnlp2023,UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation,Yes.,2,"""the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization"" and ""UPRISE mitigates the hallucination problem in our experiments with ChatGPT.""",2023,December,,
emnlp2023,Multi-Source Multi-Type Knowledge Exploration and Exploitation for Dialogue Generation,Yes.,1,"""Recently, large language models (LLMs) have shown impressive performance on natural language processing tasks.""",2023,December,,
emnlp2023,Multilingual Large Language Models Are Not (Yet) Code-Switchers,Yes.,3,"""despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales.""",2023,December,,
emnlp2023,The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning,Yes.,2,"""Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks.""",2023,December,,
emnlp2023,Explaining Interactions Between Text Spans,Yes.,1,"""We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes.""",2023,December,,
emnlp2023,Generative Adversarial Training with Perturbed Token Detection for Model Robustness,No.,1,The abstract does not mention LLMs or any specific limitations related to them.,2023,December,,
emnlp2023,Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation,Yes.,1,"""Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word.""",2023,December,,
emnlp2023,Doolittle: Benchmarks and Corpora for Academic Writing Formalization,Yes.,2,"""Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance.""",2023,December,,
emnlp2023,Token Prediction as Implicit Classification to Identify LLM-Generated Text,Yes.,1,"""This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation.""",2023,December,,
emnlp2023,LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models,,,,2023,December,,
emnlp2023,Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations,Yes.,1,"""However, a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue.""",2023,December,,
emnlp2023,CLAIR: Evaluating Image Captions with Large Language Models,Yes.,1,"""Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions.""",2023,December,,
emnlp2023,Aligning Large Language Models through Synthetic Feedback,Yes.,1,"""Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs.""",2023,December,,
emnlp2023,You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models,Yes.,2,"""The behavior of the models on out-of-domain data is unstable, suggesting that some of the models overfit, while others learn non-specific humor characteristics.""",2023,December,,
emnlp2023,DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules,Yes.,3,"""Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects.""",2023,December,,
emnlp2023,Can We Edit Multimodal Large Language Models?,Yes.,3,"""Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task.""",2023,December,,
emnlp2023,ClusterLLM: Large Language Models as a Guide for Text Clustering,Yes.,1,"""We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT.""",2023,December,,
emnlp2023,Syllogistic Reasoning for Legal Judgment Analysis,Yes.,2,"""people can hardly trust the results generated by a model without reliable analysis of legal judgement.""",2023,December,,
emnlp2023,CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL,Yes.,1,"""First, we use an LLM to hallucinate a minimal DB schema that it deems adequate to answer the query.""",2023,December,,
emnlp2023,Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation,Yes.,2,"""Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (i.e., generating large-scale harmful and misleading content).""",2023,December,,
emnlp2023,Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation,Yes.,1,"""In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models.""",2023,December,,
emnlp2023,FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models,Yes.,2,their performances are still subject to human intervention.,2023,December,,
emnlp2023,Outlier Dimensions Encode Task Specific Knowledge,Yes.,3,"Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings.",2023,December,,
emnlp2023,Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization,Yes.,2,"Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands.",2023,December,,
emnlp2023,Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding,Yes.,3,LLMs perform less well than small-scale Masked Language Models (MLMs).,2023,December,,
emnlp2023,Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents,Yes.,3,The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge.,2023,December,,
emnlp2023,Prompting with Pseudo-Code Instructions,Yes.,1,we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models.,2023,December,,
emnlp2023,Specialist or Generalist? Instruction Tuning for Specific NLP Tasks,Yes.,3,generalist data containing hallucinatory information may negatively affect the model’s performance.,2023,December,,
emnlp2023,Making Large Language Models Better Data Creators,Yes.,3,"deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security.",2023,December,,
emnlp2023,Guideline Learning for In-Context Information Extraction,Yes.,3,"However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans.",2023,December,,
emnlp2023,Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations,Yes.,3,"different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools.",2023,December,,
emnlp2023,NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models,Yes.,1,exploring the use of Large Language Models (LLMs) at multiple stages.,2023,December,,
emnlp2023,AnyTOD: A Programmable Task-Oriented Dialog System,Yes.,1,"We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema.",2023,December,,
emnlp2023,Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs,Yes.,1,"Through extensive experimentation on seven diverse data sets, we observed that LLMs, such as ChatGPT-3.5 and PaLM, demonstrated superior generality compared to other LLMs, e.g., BLOOM and GPT-NeoX.",2023,December,,
emnlp2023,Exploring Distributional Shifts in Large Language Models for Code Analysis,Yes.,3,We establish that samples from each new domain present all the models with a significant challenge of distribution shift.,2023,December,,
emnlp2023,A Benchmark for Reasoning with Spatial Prepositions,Yes.,3,"Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance.",2023,December,,
emnlp2023,Document-Level Machine Translation with Large Language Models,Yes.,1,"Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks.",2023,December,,
emnlp2023,LLM-driven Instruction Following: Progresses and Concerns,Yes.,3,What concerns remain in LLM-driven instruction following?,2023,December,,
emnlp2023,Creative Natural Language Generation,Yes.,3,"Large language models such as GPT-3, GPT4, Claude etc., have advanced the state of the art in several natural language generation tasks such as text summarization and machine translation. However when it comes to open-ended tasks with a focus on creativity such as generating stories, poetry, or various forms of figurative language, these state-of-the-art language models are often found to be inadequate. This tutorial aims to bring awareness of the important and emerging research",2023,December,,
emnlp2023,Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs,Yes.,1,"Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model.",2023,December,,
emnlp2023,CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools,Yes.,3,"However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop.",2023,December,,
emnlp2023,RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models,Yes.,2,current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation.,2023,December,,
emnlp2023,MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models,Yes.,1,"Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements.",2023,December,,
emnlp2023,MiniChain: A Small Library for Coding with Large Language Models,Yes.,3,"LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness.",2023,December,,
emnlp2023,Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback,Yes.,2,"existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their accessibility to many other languages in the world.",2023,December,,
emnlp2023,InsightPilot: An LLM-Empowered Automated Data Exploration System,Yes.,1,"we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system designed to simplify the data exploration process.",2023,December,,
emnlp2023,Prompt2Model: Generating Deployable Models from Natural Language Instructions,Yes.,3,LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs.,2023,December,,
emnlp2023,NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails,Yes.,1,NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.,2023,December,,
emnlp2023,Prompterator: Iterate Efficiently towards More Effective Prompts,Yes.,1,"Finding well-performing prompts, however, is a non-trivial task which requires experimentation in order to arrive at a prompt that solves a specific task.",2023,December,,
emnlp2023,Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding,Yes.,1,"Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals.",2023,December,,
emnlp2023,Gatekeeper to save COGS and improve efficiency of Text Prediction,Yes.,3,"As LLMs require massive amounts of computation and storage, such an approach incurs network and high execution cost.",2023,December,,
emnlp2023,Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities,Yes.,1,"The final model achieves accurate and comprehensive results compared to state-of-the-art baselines, including large language models (LLMs).",2023,December,,
emnlp2023,Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios,Yes.,2,"a significant performance gap still exists between other open-sourced LLMs (e.g., Vicuna and LLaMA-2) and GPT-4 models.",2023,December,,
emnlp2023,WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models,Yes.,1,"This paper introduces WordArt Designer, a user-driven framework for artistic typography synthesis, relying on the Large Language Model (LLM). The system incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo modules. 1) The LLM Engine, empowered by the LLM (e.g. GPT-3.5), interprets user inputs and",2023,December,,
emnlp2023,Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,Yes.,3,"Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, centered around Microsoft products and IT technical",2023,December,,
emnlp2023,Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective,Yes.,3,"Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models.",2023,December,,
emnlp2023,Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective,Yes.,2,"""Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models.""",2023,December,,
emnlp2023,AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications,Yes.,3,"""This provides transparency of developers evaluation intentions and enables quick adaptation to new use cases and newly discovered model weaknesses.""",2023,December,,
emnlp2023,Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks,Yes.,2,"""We report both the strengths and limitations of the current models by comparing them to the state-of-the-art fine-tuned approaches and the recently released domain-specific pretrained models.""",2023,December,,
emnlp2023,PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching,Yes.,2,"""Instruction fine-tuning has conventionally been employed to adapt Large Language Models (LLMs) to a variety of diverse tasks. Nonetheless, this technique often necessitates substantial computational resources, making it impractical for deployment by individuals or small-scale entities.""",2023,December,,
emnlp2023,JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization,Yes.,1,"""JarviX is designed to employ Large Language Models (LLMs) to facilitate an automated guide and execute high-precision data analyzes on tabular datasets.""",2023,December,,
emnlp2023,"Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness",Yes.,3,"""RLHF, which incorporates independent reward models trained on high-quality human feedback datasets, incurs high costs in terms of hardware resources and human efforts.""",2023,December,,
emnlp2023,InstructPTS: Instruction-Tuning LLMs for Product Title Summarization,,,,2023,December,,
emnlp2023,LLM4Vis: Explainable Visualization Recommendation using ChatGPT,Yes.,1,"""To address this research gap, we propose LLM4Vis, a novel ChatGPT-based prompting approach to perform visualization recommendation and return human-like explanations using very few demonstration examples.""",2023,December,,
emnlp2023,Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting,Yes.,1,"""Furthermore, fine-tuned public LLMs, such as Open-LLaMA, can generate reasonable and explainable forecasts, although they underperform compared to GPT-4.""",2023,December,,
emnlp2023,"ViGPTQA - State-of-the-Art LLMs for Vietnamese Question Answering: System Overview, Core Models Training, and Evaluations",Yes.,3,"""Large language models (LLMs) and their applications in low-resource languages (such as in Vietnamese) are limited due to lack of training data and benchmarking datasets.""",2023,December,,
emnlp2023,On Sample-Efficient Code Generation,Yes.,3,"""Large language models often struggle to predict runtime behavior in code generation tasks, leading to a reliance on rejection sampling (best-of-n) to generate multiple code snippets then select the best.""",2023,December,,
emnlp2023,Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding,Yes.,1,"""This paper then further explores the use of Large Language Models (LLMs) in conjunction with graph traversal, leading to a significant increase in index coverage for unseen interactions.""",2023,December,,
emnlp2023,Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation,No.,1,"""No evidence""",2023,December,,
emnlp2023,Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings,No.,1,"""No evidence""",2023,December,,
emnlp2023,Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction,No.,1,"""No evidence""",2023,December,,
emnlp2023,Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling,No.,1,"""No evidence""",2023,December,,
emnlp2023,CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data,No.,1,"""No evidence""",2023,December,,
emnlp2023,The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions,No.,1,"""No evidence""",2023,December,,
emnlp2023,Indicative Summarization of Long Discussions,No.,1,"""No evidence""",2023,December,,
emnlp2023,AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification,No.,1,"""No evidence""",2023,December,,
emnlp2023,Query Rewriting in Retrieval-Augmented Large Language Models,No.,1,"""No evidence""",2023,December,,
emnlp2023,CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients,No.,1,"""No evidence""",2023,December,,
emnlp2023,Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data,No.,1,"""No evidence""",2023,December,,
emnlp2023,Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding,No.,1,"""No evidence""",2023,December,,
emnlp2023,WiCE: Real-World Entailment for Claims in Wikipedia,No.,1,"""No evidence""",2023,December,,
emnlp2023,Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study,No.,1,"""No evidence""",2023,December,,
emnlp2023,Don’t Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,No.,1,"""No evidence""",2023,December,,
emnlp2023,StructGPT: A General Framework for Large Language Model to Reason over Structured Data,No.,1,"""No evidence""",2023,December,,
emnlp2023,Model-tuning Via Prompts Makes NLP Models Adversarially Robust,No.,1,"""No evidence""",2023,December,,
emnlp2023,A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot,No.,1,"""No evidence""",2023,December,,
emnlp2023,Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model,No.,1,"""No evidence""",2023,December,,
emnlp2023,An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives,No.,1,"""No evidence""",2023,December,,
emnlp2023,Axiomatic Preference Modeling for Longform Question Answering,No.,1,"""No evidence""",2023,December,,
emnlp2023,TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models,No.,1,"""No evidence""",2023,December,,
emnlp2023,XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models,No.,1,"""No evidence""",2023,December,,
emnlp2023,NameGuess: Column Name Expansion for Tabular Data,No.,1,"""No evidence""",2023,December,,
emnlp2023,BLESS: Benchmarking Large Language Models on Sentence Simplification,No.,1,"""No evidence""",2023,December,,
emnlp2023,API-Assisted Code Generation for Question Answering on Varied Table Structures,No.,1,"""No evidence""",2023,December,,
emnlp2023,DiNeR: A Large Realistic Dataset for Evaluating Compositional Generalization,No.,1,"""No evidence""",2023,December,,
emnlp2023,It Ain’t Over: A Multi-aspect Diverse Math Word Problem Dataset,No.,1,"""No evidence""",2023,December,,
emnlp2023,Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication,No.,1,"""No evidence""",2023,December,,
emnlp2023,NORMSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly,No.,1,"""No evidence""",2023,December,,
emnlp2023,FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W Question-Answering,No.,1,"""No evidence""",2023,December,,
emnlp2023,Rationale-Enhanced Language Models are Better Continual Relation Learners,No.,1,"""No evidence""",2023,December,,
emnlp2023,From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models,No.,1,"""No evidence""",2023,December,,
emnlp2023,Are Compressed Language Models Less Subgroup Robust?,No.,1,"""No evidence""",2023,December,,
emnlp2023,Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation,No.,1,"""No evidence""",2023,December,,
emnlp2023,Koala: An Index for Quantifying Overlaps with Pre-training Corpora,No.,1,"""No evidence""",2023,December,,
emnlp2023,"ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",No.,1,"""No evidence""",2023,December,,
emnlp2023,CoLLiE: Collaborative Training of Large Language Models in an Efficient Way,No.,1,"""No evidence""",2023,December,,
emnlp2023,ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models,No.,1,"""No evidence""",2023,December,,
emnlp2023,"Enhancing Extreme Multi-Label Text Classification: Addressing Challenges in Model, Data, and Evaluation",No.,1,"""No evidence""",2023,December,,
emnlp2023,InsightNet : Structured Insight Mining from Customer Feedback,No.,1,"""No evidence""",2023,December,,
emnlp2023,DocumentNet: Bridging the Data Gap in Document Pre-training,No.,1,"""No evidence""",2023,December,,
naacl2024,Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences,Yes.,3,"""Named entity recognition is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short.""",2024,June,,
naacl2024,On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL,Yes.,3,"""Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear.""",2024,June,,
naacl2024,Adaptive Rank Selections for Low-Rank Approximation of Language Models,Yes.,3,"""However, such a uniform rank selection is sub-optimal since different operations (layers) have non-uniform demand in capacity."" and ""However, a globally-optimized selection of ranks for neural networks is still an open problem, and this is a non-trivial challenge since the selection is discrete.""",2024,June,,
naacl2024,Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration,Yes.,2,"""experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat.""",2024,June,,
naacl2024,Self-Prompting Large Language Models for Zero-Shot Open-Domain QA,Yes.,3,"""While recent Large Language Models (LLMs) like GPT-3 have demonstrated their effectiveness in zero-shot ODQA using direct prompting methods, these methods still fall short of fully harnessing the potential of LLMs when implicitly invoked.""",2024,June,,
naacl2024,LLMs Are Few-Shot In-Context Low-Resource Language Learners,Yes.,3,"""Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish."" and ""identifies the shortcomings of in-context label alignment, and introduces a more effective alternative",2024,June,,
naacl2024,Rethinking Tabular Data Understanding with Large Language Models,Yes.,3,"""We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks.""",2024,June,,
naacl2024,FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs,Yes.,3,"""However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs.""",2024,June,,
naacl2024,ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models,Yes.,1,"""Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models.""",2024,June,,
naacl2024,InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions,Yes.,3,"""LLMs necessitate continual task-specific adaptation without catastrophic forgetting."" and ""traditional replay-based methods do not fully utilize instructions to customize the replay strategy.""",2024,June,,
naacl2024,Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors,Yes.,3,"""Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance.""",2024,June,,
naacl2024,VertAttack: Taking Advantage of Text Classifiers’ Horizontal Vision,,,,2024,June,,
naacl2024,BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings,Yes.,3,"""Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling.""",2024,June,,
naacl2024,Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model,Yes.,3,"""Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models.""",2024,June,,
naacl2024,Neurocache: Efficient Vector Retrieval for Long-range Language Modeling,Yes.,1,"""This paper introduces Neurocache, an approach to extend the effective context size of large language models (LLMs) using an external vector cache to store its past states.""",2024,June,,
naacl2024,Unveiling the Generalization Power of Fine-Tuned Large Language Models,,,,2024,June,,
naacl2024,Exploring Self-supervised Logic-enhanced Training for Large Language Models,Yes.,3,"""Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-the-art fine-tuning based models.""",2024,June,,
naacl2024,MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning,Yes.,3,"""We further observe that TALMs are not as effective for simpler math word problems (in GSM-8K), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH).""",2024,June,,
naacl2024,AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition,Yes.,3,"""Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment.""",2024,June,,
naacl2024,SEMQA: Semi-Extractive Multi-Source Question Answering,Yes.,3,"""Yet, attributing and verifying their generated abstractive answers can be difficult, and automatically evaluating their accuracy remains an ongoing challenge.""",2024,June,,
naacl2024,Fine-Tuning Language Models with Reward Learning on Policy,Yes.,3,"""Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs’ data distribution.""",2024,June,,
naacl2024,IterAlign: Iterative Constitutional Alignment of Large Language Models,Yes.,3,"""However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming.""",2024,June,,
naacl2024,Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation,Yes.,3,"""Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.""",2024,June,,
naacl2024,JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models,Yes.,3,"""Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM’s APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement.""",2024,June,,
naacl2024,MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference,Yes.,3,"""Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset.""",2024,June,,
naacl2024,COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion,Yes.,1,"""a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation.""",2024,June,,
naacl2024,Toward Informal Language Processing: Knowledge of Slang in Large Language Models,Yes.,1,"""Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language.""",2024,June,,
naacl2024,Ghostbuster: Detecting Text Ghostwritten by Large Language Models,Yes.,1,"""We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.""",2024,June,,
naacl2024,End-to-End Beam Retrieval for Multi-Hop Question Answering,Yes.,1,"""To establish a complete QA system, we incorporate a supervised reader or a large language model (LLM).""",2024,June,,
naacl2024,Leveraging Generative Large Language Models with Visual Instruction and Demonstration Retrieval for Multimodal Sarcasm Detection,Yes.,1,"""we propose a generative multimodal sarcasm model consisting of a designed instruction template and a demonstration retrieval module based on the large language model.""",2024,June,,
naacl2024,Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction,Yes.,3,"""However, applying LLMs to grammatical error correction (GEC) is still a challenging task.""",2024,June,,
naacl2024,BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer,Yes.,3,"""Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages.""",2024,June,,
naacl2024,zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models,Yes.,3,"""they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context.""",2024,June,,
naacl2024,Embodied Executable Policy Learning with Language-based Scene Summarization,Yes.,3,"""the performance of pretrained LLMs heavily relies on domain-specific templated text data, which may be infeasible in real-world robot learning tasks with image-based observations. Moreover, existing LLMs with text inputs lack the capability to evolve with non-expert interactions with environments.""",2024,June,,
naacl2024,Metacognitive Prompting Improves Understanding in Large Language Models,Yes.,3,"""the nuanced understanding abilities of these models, crucial for processing and interpreting complex information, remain underexplored.""",2024,June,,
naacl2024,QualEval: Qualitative Evaluation for Model Improvement,Yes.,2,"""Quantitative evaluation metrics have been pivotal in gauging the advancements of AI systems like large language models (LLMs). However, due to the intricate nature of real-world tasks, a single scalar to quantify and compare performance trivializes the fine-grained nuances of model behavior.""",2024,June,,
naacl2024,A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily,,,,2024,June,,
naacl2024,Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes,Yes.,1,"""Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes.""",2024,June,,
naacl2024,Program-Aided Reasoners (Better) Know What They Know,Yes.,3,"""However, while accuracy is essential, it is also important for such reasoners to 'know what they know', which can be quantified through the calibration of the model.""",2024,June,,
naacl2024,From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning,Yes.,1,"""Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions.""",2024,June,,
naacl2024,LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination,Yes.,3,"""the resource consumption is unaffordable"" and ""a mere memory module is inadequate and fully training an LLM can be excessively costly.""",2024,June,,
naacl2024,MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion,Yes.,3,"""Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs.""",2024,June,,
naacl2024,PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,Yes.,3,"""While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment.""",2024,June,,
naacl2024,"MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks",Yes.,3,"""There has been a surge in LLM evaluation research to understand LLM capabilities and limitations."" and ""We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the",2024,June,,
naacl2024,Unlocking Emergent Modularity in Large Language Models,Yes.,2,"""Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized.""",2024,June,,
naacl2024,PatentEval: Understanding Errors in Patent Generation,Yes.,3,"""These approaches provide valuable insights into the capabilities and limitations of current language models in the specialized field of patent text generation.""",2024,June,,
naacl2024,Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing,Yes.,3,"""Large language models (LLMs) have demonstrated considerable success in various natural language processing tasks, but open-source LLMs have yet to attain state-of-the-art performance in Neural Machine Translation (NMT)."" and ""Surprisingly, our initial experiments found that fine-tuning",2024,June,,
naacl2024,TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale,Yes.,3,"""However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings.""",2024,June,,
naacl2024,GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models,Yes.,3,"""prompting LLMs with a fixed set of relations or entities can cause hallucinations.""",2024,June,,
naacl2024,TopicGPT: A Prompt-based Topic Modeling Framework,Yes.,1,"""we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics in a text collection.""",2024,June,,
naacl2024,ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger,Yes.,1,"""The rise of advanced generative models, such as GPT-4, with their capacity for human-like rewriting, makes these attacks increasingly challenging to detect.""",2024,June,,
naacl2024,LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models,Yes.,3,"""existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs).""",2024,June,,
naacl2024,Instructional Fingerprinting of Large Language Models,,,,2024,June,,
naacl2024,Uncertainty Quantification for In-Context Learning of Large Language Models,Yes.,3,"""trustworthy issues with LLM’s response, such as hallucination, have also been actively discussed"" and ""highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model’s configurations (epistemic uncertainty).""",2024,June,,
naacl2024,HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM,Yes.,3,"""Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length).""",2024,June,,
naacl2024,A Preference-driven Paradigm for Enhanced Translation with Large Language Models,Yes.,3,"""SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references. Hence, the assistance from SFT often reaches a plateau once the LLMs have achieved a certain level of translation capability, and further increasing the size of parallel data",2024,June,,
naacl2024,Making Language Models Better Tool Learners with Execution Feedback,Yes.,3,"""Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance.""",2024,June,,
naacl2024,Long-form evaluation of model editing,Yes.,3,"""while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods.""",2024,June,,
naacl2024,Analyzing the Role of Semantic Representations in the Era of Large Language Models,Yes.,3,"""We find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step where the LLM must connect its reasoning over the AMR to its prediction.""",2024,June,,
naacl2024,Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation,Yes.,3,"""these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences.""",2024,June,,
naacl2024,Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning,Yes.,1,"""In-Context Learning (ICL) is an emergent capability of Large Language Models (LLMs).""",2024,June,,
naacl2024,CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants,Yes.,2,"""A major challenge in deploying LLM-based virtual conversational assistants in real world settings is ensuring they operate within what is admissible for the task."" and ""relying on commonly used, prompt-based guardrails can be difficult to engineer correctly and comprehensively",2024,June,,
naacl2024,DoG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Instruction Wrapping,Yes.,3,"""Unfortunately, the current methods used to collect the pairs suffer from either unaffordable labor costs or severe hallucinations in the self-generation of LLM.""",2024,June,,
naacl2024,MDR: Model-Specific Demonstration Retrieval at Inference Time for In-Context Learning,Yes.,3,"""distinct LLMs exhibit different biases for 'what is a good demonstration' since they possess differences in training data, model architectures and training methods"" and ""Previous approaches ignore the model bias and fail to retrieve the most appropriate demonstrations for different inference LLMs, resulting in a degradation of ICL performance.""",2024,June,,
naacl2024,Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis,Yes.,3,"""Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate.""",2024,June,,
naacl2024,Rectifying Demonstration Shortcut in In-Context Learning,Yes.,3,"""LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction.""",2024,June,,
naacl2024,Differentially Private Next-Token Prediction of Large Language Models,Yes.,3,"""DP-SGD overestimates an adversary’s capabilities in having white box access to the model and, as a result, causes longer training times and larger memory usage than SGD.""",2024,June,,
naacl2024,"Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model",Yes.,1,"""Unlike prior works that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific architecture, we hypothesize and verify the paraphrastic proximity intrinsic to pre-trained LMs (e.g., GPT2),",2024,June,,
naacl2024,TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization,,,,2024,June,,
naacl2024,Anisotropy is Not Inherent to Transformers,,,,2024,June,,
naacl2024,Leveraging Code to Improve In-Context Learning for Semantic Parsing,,,,2024,June,,
naacl2024,SportQA: A Benchmark for Sports Understanding in Large Language Models,Yes.,3,"""Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise.""",2024,June,,
naacl2024,Revisiting subword tokenization: A case study on affixal negation in large language models,Yes.,3,"""the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible.""",2024,June,,
naacl2024,Teaching Language Models to Self-Improve through Interactive Demonstrations,Yes.,3,"""However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones.""",2024,June,,
naacl2024,MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets,Yes.,1,"""Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs.""",2024,June,,
naacl2024,Does GPT-4 pass the Turing test?,Yes.,3,"""Despite known limitations as a test of intelligence, we argue that the Turing test continues to be relevant as an assessment of naturalistic communication and deception.""",2024,June,,
naacl2024,GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer,Yes.,3,"""However, their size and cost, particularly for those accessed via APIs like ChatGPT, make them impractical in resource-limited scenarios."" and ""an advantage over the slow sequential token generation of LLMs.""",2024,June,,
naacl2024,Fine-grained Gender Control in Machine Translation with Large Language Models,Yes.,3,"""we discover an emergence of gender interference phenomenon when controlling the gender of multiple entities"" and ""address the limitations of existing gender accuracy evaluation metrics.""",2024,June,,
naacl2024,LLatrieval: LLM-Verified Retrieval for Verifiable Generation,Yes.,3,"""the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs.""",2024,June,,
naacl2024,Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation,Yes.,3,"""existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses.""",2024,June,,
naacl2024,"The ART of LLM Refinement: Ask, Refine, and Trust",Yes.,3,"""recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved.""",2024,June,,
naacl2024,ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies,Yes.,3,"""We test LLMs’ and humans’ analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (∼13% gap) after a light supervision."" and ""Lastly, we show challenging distractors confuse LLMs, but not humans.""",2024,June,,
naacl2024,TableLlama: Towards Open Large Generalist Models for Tables,Yes.,1,"""We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge.""",2024,June,,
naacl2024,Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection,Yes.,3,"""The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways.""",2024,June,,
naacl2024,DialogBench: Evaluating LLMs as Human-like Dialogue Systems,Yes.,3,"""instruction tuning improves the human likeness of LLMs to a certain extent, but most LLMs still have much room for improvement as human-like dialogue systems."" and ""the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life.""",2024,June,,
naacl2024,CMB: A Comprehensive Medical Benchmark in Chinese,Yes.,1,"""Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine.""",2024,June,,
naacl2024,SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics,Yes.,2,"""Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources.""",2024,June,,
naacl2024,Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models,Yes.,2,"""The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.""",2024,June,,
naacl2024,Language Model Based Unsupervised Dependency Parsing with Conditional Mutual Information and Grammatical Constraints,Yes.,3,"""Previous methods based on Large Language Models (LLM) perform unsupervised dependency parsing by maximizing bi-lexical dependence scores. However, these previous methods adopt dependence scores that are difficult to interpret. These methods cannot incorporate grammatical constraints that previous grammar-based parsing research has shown beneficial to",2024,June,,
naacl2024,MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation,Yes.,3,"""Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation, yet they suffer from high computational cost and latency.""",2024,June,,
naacl2024,LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models,Yes.,2,"""raising concerns about the adversarial vulnerability of this paradigm.""",2024,June,,
naacl2024,CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions,Yes.,1,"""Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks.""",2024,June,,
naacl2024,PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers,Yes.,1,"""In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis.""",2024,June,,
naacl2024,Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References,,,,2024,June,,
naacl2024,ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications,Yes.,3,"""the prompting-based approach with LLMs shows a limitation in extracting sentences from source documents.""",2024,June,,
naacl2024,Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF,Yes.,3,"""These expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts.""",2024,June,,
naacl2024,Divergent Token Metrics: Measuring degradation to prune away LLM components – and optimize quantization,Yes.,3,"""However, their ever-increasing size has raised concerns about their effective deployment and the need for LLM compression."" and ""addressing the limitations of traditional perplexity or accuracy measures that fail to accurately reflect text generation quality.""",2024,June,,
naacl2024,On the Effectiveness of Adversarial Robustness for Abuse Mitigation with Counterspeech,No.,1,The abstract does not mention LLMs or any specific type of language models.,2024,June,,
naacl2024,Leveraging the Structure of Pre-trained Embeddings to Minimize Annotation Effort,Yes.,2,"""However, for some challenging classification tasks, providing enough annotations to ensure a reliable classification continues to be the main bottleneck.""",2024,June,,
naacl2024,Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity,Yes.,3,"""they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries.""",2024,June,,
naacl2024,Bridging the Gap between Different Vocabularies for LLM Ensemble,Yes.,3,"""Nevertheless, vocabulary discrepancies among various LLMs have constrained previous studies to either selecting or blending completely generated outputs. This limitation hinders the dynamic correction and enhancement of outputs during the generation process, resulting in a limited capacity for effective ensemble.""",2024,June,,
naacl2024,Towards Reducing Diagnostic Errors with Interpretable Risk Prediction,Yes.,1,"""In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses.""",2024,June,,
naacl2024,The steerability of large language models toward data-driven personas,Yes.,3,"""Large language models (LLMs) are known to generate biased responses where the opinions of certain groups and populations are underrepresented.""",2024,June,,
naacl2024,CERET: Cost-Effective Extrinsic Refinement for Text Generation,Yes.,3,"""Large Language Models (LLMs) are powerful models for generation tasks, but they may not generate good quality outputs in their first attempt."" and ""Despite their effectiveness, these methods are hindered by their high computational cost and lack of scalability.""",2024,June,,
naacl2024,MisgenderMender: A Community-Informed Approach to Interventions for Misgendering,Yes.,3,"""highlighting challenges for future models to address"" and ""annotated for the presence of misgendering, with additional annotations for correcting misgendering in LLM-generated text.""",2024,June,,
naacl2024,From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning,Yes.,1,"""In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point.""",2024,June,,
naacl2024,Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval,Yes.,1,"""SAP assists the LLM in generating informative queries in the target language.""",2024,June,,
naacl2024,A Theory Guided Scaffolding Instruction Framework for LLM-Enabled Metaphor Reasoning,Yes.,3,"""LLM-based methods for metaphor detection and reasoning are still faced with the challenging issue of bringing the explainable concepts for metaphor reasoning and their linguistic manifestation.""",2024,June,,
naacl2024,Actively Learn from LLMs with Uncertainty Propagation for Generalized Category Discovery,Yes.,1,"""we propose to integrate the feedback from LLMs into an active learning paradigm.""",2024,June,,
naacl2024,Large Language Models can Contrastively Refine their Generation for Better Sentence Representation Learning,Yes.,3,"""the effectiveness of these methods is largely influenced by the content generated from LLMs, highlighting the need for more refined generation in the context of sentence representation learning.""",2024,June,,
naacl2024,“You are an expert annotator”: Automatic Best–Worst-Scaling Annotations for Emotion Intensity Modeling,Yes.,3,"""This raises the question if large language model-based annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks.""",2024,June,,
naacl2024,What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?,Yes.,2,"""there is considerable scope for enhancing open-source multi-modal LLMs, especially in terms of multi-modal understanding accuracy and instruction-following proficiency.""",2024,June,,
naacl2024,Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation,Yes.,2,"""We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation.""",2024,June,,
naacl2024,SemRoDe: Macro Adversarial Training to Learn Representations that are Robust to Word-Level Attacks,Yes.,3,"""Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern.""",2024,June,,
naacl2024,BUST: Benchmark for the evaluation of detectors of LLM-Generated Text,Yes.,1,"""We introduce BUST, a comprehensive benchmark designed to evaluate detectors of texts generated by instruction-tuned large language models (LLMs).""",2024,June,,
naacl2024,"AceGPT, Localizing Large Language Models in Arabic",Yes.,3,"""Significant concerns emerge when addressing cultural sensitivity and local values.""",2024,June,,
naacl2024,Depression Detection in Clinical Interviews with LLM-Empowered Structural Element Graph,Yes.,1,"""Additionally, we further empower SEGA by devising novel principle-guided data augmentation with large language models (LLMs) to supplement high-quality synthetic data and enable graph contrastive learning.""",2024,June,,
naacl2024,ARM: Alignment with Residual Energy-Based Model,Yes.,3,"""RLHF methods achieve successes in aligning LLM responses with human preferences and improving the controllability of LLM behavior with human instruction. However, RLHF methods are considerably complicated to implement, computationally expensive to train, and notoriously tricky to tune.""",2024,June,,
naacl2024,Efficient End-to-End Visual Document Understanding with Rationale Distillation,Yes.,3,"""However, such methods have high computational and engineering complexity.""",2024,June,,
naacl2024,On Learning to Summarize with Large Language Models as References,Yes.,3,"""showing that LLMs are not well-aligned with human evaluators. Particularly, our expert human evaluation reveals remaining nuanced performance gaps between LLMs and our fine-tuned models, which LLMs fail to capture.""",2024,June,,
naacl2024,Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Value,Yes.,3,"""Existing work mainly specifies values as risk criteria formulated in the AI community, e.g., fairness and privacy protection, suffering from poor clarity, adaptability and transparency.""",2024,June,,
naacl2024,"Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence",Yes.,3,"""existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence.""",2024,June,,
naacl2024,MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages,Yes.,1,"""Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs)...""",2024,June,,
naacl2024,A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models,Yes.,1,"""To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs.""",2024,June,,
naacl2024,Unveiling Divergent Inductive Biases of LLMs on Temporal Data,,,,2024,June,,
naacl2024,On Retrieval Augmentation and the Limitations of Language Model Training,Yes.,3,"""This task is challenging even for GPT-3.5 Turbo.""",2024,June,,
naacl2024,Can LLM’s Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis,Yes.,1,"""We present a novel approach to automatically synthesize 'wayfinding instructions' for an embodied robot agent.""",2024,June,,
naacl2024,Discourse-Aware In-Context Learning for Temporal Expression Normalization,Yes.,1,"""we explore the feasibility of proprietary and open-source large language models (LLMs) for TE normalization using in-context learning to inject task, document, and example information into the model.""",2024,June,,
naacl2024,ALOHa: A New Measure for Hallucination in Captioning Models,Yes.,1,"""we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations.""",2024,June,,
naacl2024,Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels,Yes.,3,"""the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query.""",2024,June,,
naacl2024,Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata,No.,1,"The abstract discusses a non-autoregressive model (DA-T5) and its application to NLG tasks, but does not mention LLMs or their limitations.",2024,June,,
naacl2024,A Continued Pretrained LLM Approach for Automatic Medical Note Generation,Yes.,3,"""However, the use of the most advanced LLMs, such as GPT-4, is often prohibitively expensive for most specialized fields.""",2024,June,,
naacl2024,Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models,Yes.,3,"""Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations of self-improving does not guarantee further improvement, but the performance might be boosted via more advanced strategies for reliable annotation selection.""",2024,June,,
naacl2024,Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion,Yes.,3,"""Our analysis reveals that emotion triggers are largely not considered salient features for emotion prediction models, instead there is intricate interplay between various features and the task of emotion detection.""",2024,June,,
naacl2024,CPopQA: Ranking Cultural Concept Popularity by LLMs,Yes.,1,"""Experiments on four strong LLMs show that open-sourced LLMs still lag way behind close LLM API (e.g., GPT-3.5) in statistical ranking of cultural concepts.""",2024,June,,
naacl2024,Arithmetic Reasoning with LLM: Prolog Generation & Permutation,Yes.,3,"""the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors.""",2024,June,,
naacl2024,DoubleLingo: Causal Estimation with Large Language Models,Yes.,3,"""flexible large language models that excel at predictive tasks with text data do not meet the statistical assumptions necessary for causal estimation.""",2024,June,,
naacl2024,Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?,Yes.,3,"""However, inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss.""",2024,June,,
naacl2024,Low-code LLM: Graphical User Interface over Large Language Models,Yes.,3,"""Utilizing Large Language Models (LLMs) for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process.""",2024,June,,
naacl2024,RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs,Yes.,3,"""However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers.""",2024,June,,
naacl2024,Newspaper Signaling for Crisis Prediction,Yes.,1,"""The model works with unstructured news data and combines multiple transformer-based models for pre-processing (STANZA) and content filtering (RoBERTa, GPT-3.5).""",2024,June,,
naacl2024,AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents,Yes.,3,"""existing benchmarks are often narrow and simply compute overall task success"" and ""we identify common failure points and refine the agent architecture to obtain a significant performance increase.""",2024,June,,
naacl2024,Rephrasing Invokes Better Generations for Large Language Models,Yes.,2,"""However, automatic input pre-processing when LLMs are unavailable is currently under-studied.""",2024,June,,
naacl2024,Exploring Compositional Generalization of Large Language Models,Yes.,3,"""Interestingly, our experimental results indicate that training LLMs on higher-order compositional instructions enhances their performance on lower-order ones, but the reverse does not hold true.""",2024,June,,
naacl2024,LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues,Yes.,3,"""Yet a major bottleneck to achieving genuinely transformative task-oriented dialogue capabilities remains the scarcity of high quality data."" and ""Existing datasets, while impressive in scale, have limited domain coverage and contain few genuinely challenging conversational phenomena; those which are present are typically unlabelled, making it difficult",2024,June,,
naacl2024,Detecting Response Generation Not Requiring Factual Judgment,Yes.,3,"""With the remarkable development of large language models (LLMs), ensuring the factuality of output has become a challenge.""",2024,June,,
naacl2024,Investigating Web Corpus Filtering Methods for Language Model Development in Japanese,Yes.,2,"""Indeed, we empirically present that strong filtering methods can rather lead to lesser performance in downstream tasks.""",2024,June,,
naacl2024,Distilling Text Style Transfer With Self-Explanation From LLMs,Yes.,1,"""we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST.""",2024,June,,
naacl2024,Coding Open-Ended Responses using Pseudo Response Generation by Large Language Models,Yes.,1,"""To address this issue, we propose an LLM-based method to automate parts of the grounded theory approach (GTA), a representative approach of the qualitative data analysis.""",2024,June,,
naacl2024,Catch Me If You GPT: Tutorial on Deepfake Texts,Yes.,2,"""While this is a celebratory feat for NLG, it poses new security risks (e.g., the generation of misinformation).""",2024,June,,
naacl2024,Human-AI Interaction in the Age of LLMs,Yes.,2,"""exploring the challenges, opportunities, and ethical considerations that arise in this dynamic landscape.""",2024,June,,
naacl2024,Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries,No.,1,"""No evidence""",2024,June,,
naacl2024,FPT: Feature Prompt Tuning for Few-shot Readability Assessment,No.,1,"""No evidence""",2024,June,,
naacl2024,kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning,No.,1,"""No evidence""",2024,June,,
naacl2024,CoUDA: Coherence Evaluation via Unified Data Augmentation,No.,1,"""No evidence""",2024,June,,
naacl2024,mEdIT: Multilingual Text Editing via Instruction Tuning,No.,1,"""No evidence""",2024,June,,
naacl2024,"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers",No.,1,"""No evidence""",2024,June,,
naacl2024,OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking,No.,1,"""No evidence""",2024,June,,
naacl2024,A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers,No.,1,"""No evidence""",2024,June,,
naacl2024,Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models,No.,1,"""No evidence""",2024,June,,
naacl2024,Automatic Generation of Model and Data Cards: A Step Towards Responsible AI,No.,1,"""No evidence""",2024,June,,
naacl2024,VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization,No.,1,"""No evidence""",2024,June,,
naacl2024,Evaluating In-Context Learning of Libraries for Code Generation,No.,1,"""No evidence""",2024,June,,
naacl2024,Advancing Beyond Identification: Multi-bit Watermark for Large Language Models,No.,1,"""No evidence""",2024,June,,
naacl2024,Better Zero-Shot Reasoning with Role-Play Prompting,No.,1,"""No evidence""",2024,June,,
naacl2024,Generating Attractive and Authentic Copywriting from Customer Reviews,No.,1,"""No evidence""",2024,June,,
naacl2024,CASA: Causality-driven Argument Sufficiency Assessment,No.,1,"""No evidence""",2024,June,,
naacl2024,AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs,No.,1,"""No evidence""",2024,June,,
naacl2024,KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation,No.,1,"""No evidence""",2024,June,,
naacl2024,Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling,No.,1,"""No evidence""",2024,June,,
naacl2024,SuperGLEBer: German Language Understanding Evaluation Benchmark,No.,1,"""No evidence""",2024,June,,
naacl2024,Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning,No.,1,"""No evidence""",2024,June,,
naacl2024,SKICSE: Sentence Knowable Information Prompted by LLMs Improves Contrastive Sentence Embeddings,No.,1,"""No evidence""",2024,June,,
naacl2024,How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes,No.,1,"""No evidence""",2024,June,,
naacl2024,LLM-Driven Knowledge Injection Advances Zero-Shot and Cross-Target Stance Detection,No.,1,"""No evidence""",2024,June,,
naacl2024,Llama meets EU: Investigating the European political spectrum through the lens of LLMs,No.,1,"""No evidence""",2024,June,,
naacl2024,LifeTox: Unveiling Implicit Toxicity in Life Advice,No.,1,"""No evidence""",2024,June,,
naacl2024,Trusting Your Evidence: Hallucinate Less with Context-aware Decoding,No.,1,"""No evidence""",2024,June,,
naacl2024,FastFit: Fast and Effective Few-Shot Text Classification with a Multitude of Classes,No.,1,"""No evidence""",2024,June,,
naacl2024,ZhuJiu-Knowledge: A Fairer Platform for Evaluating Multiple Knowledge Types in Large Language Models,No.,1,"""No evidence""",2024,June,,
naacl2024,"To Clarify or not to Clarify: A Comparative Analysis of Clarification Classification with Fine-Tuning, Prompt Tuning, and Prompt Engineering",No.,1,"""No evidence""",2024,June,,
naacl2024,Improving Repository-level Code Search with Text Conversion,No.,1,"""No evidence""",2024,June,,
naacl2024,Cross-Task Generalization Abilities of Large Language Models,No.,1,"""No evidence""",2024,June,,
