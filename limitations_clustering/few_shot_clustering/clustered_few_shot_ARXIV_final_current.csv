Source ,Title,Talks about LLMs,Rate,Evidence,Year ,Date ,Keyphrase,Combined_Text,Topic
arXIv2022,Knowledge Distillation of Transformer-based Language Models Revisited,Yes.,5,"""However, the large model size and high run-time latency are serious impediments to applying them in practice, especially on mobile phones and Internet of Things (IoT) devices.""",2022,2022-06-29T02:16:56Z,"Keyphrase: ""High runtime latency""","""However, the large model size and high run-time latency are serious impediments to applying them in practice, especially on mobile phones and Internet of Things (IoT) devices."" Keyphrase: ""High runtime latency""",4
arXIv2022,CC-Riddle: A Question Answering Dataset of Chinese Character Riddles,Yes.,5,"""The test results reveal that current language models still struggle to solve Chinese character riddles.""",2022,2022-06-28T06:23:13Z,"Keyphrase: ""Struggles with solving specific language challenges""","""The test results reveal that current language models still struggle to solve Chinese character riddles."" Keyphrase: ""Struggles with solving specific language challenges""",1
arXIv2022,A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages,Yes.,4,"""concerns remain whether open-ended languages or text generated from these models reveal any biases toward a specific group of people, thereby risking the usability of a certain product.""",2022,2022-06-23T21:57:08Z,"Keyphrase: ""Bias towards specific groups""","""concerns remain whether open-ended languages or text generated from these models reveal any biases toward a specific group of people, thereby risking the usability of a certain product."" Keyphrase: ""Bias towards specific groups""",3
arXIv2022,BERT Rankers are Brittle: a Study using Adversarial Document Perturbations,Yes.,5,"""we argue that BERT-rankers are not immune to adversarial attacks targeting retrieved documents given a query,"" and ""we find that BERT-rankers heavily rely on the document start/head for relevance prediction, making the initial part of the document more susceptible to adversarial attacks.""",2022,2022-06-23T14:16:48Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""we argue that BERT-rankers are not immune to adversarial attacks targeting retrieved documents given a query,"" and ""we find that BERT-rankers heavily rely on the document start/head for relevance prediction, making the initial part of the document more susceptible to adversarial attacks."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arXIv2022,Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models,Yes.,4,"""This paper presents exploratory work on whether and to what extent biases against queer and trans people are encoded in large language models (LLMs) such as BERT."" and ""We found that BERT shows significant homophobic bias.""",2022,2022-06-23T05:30:47Z,"Keyphrase: ""Homophobic bias""","""This paper presents exploratory work on whether and to what extent biases against queer and trans people are encoded in large language models (LLMs) such as BERT."" and ""We found that BERT shows significant homophobic bias."" Keyphrase: ""Homophobic bias""",3
arXIv2022,Don't Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information,Yes.,4,"""We aim to diminish the stereotypical bias in the representations while preserving the factual gender signal."" and ""Our filtering method shows that it is possible to decrease the bias of gender-neutral profession names without significant deterioration of language modeling capabilities.""",2022,2022-06-21T21:38:25Z,"Keyphrase: ""Bias reduction at the expense of language modeling capability""","""We aim to diminish the stereotypical bias in the representations while preserving the factual gender signal."" and ""Our filtering method shows that it is possible to decrease the bias of gender-neutral profession names without significant deterioration of language modeling capabilities."" Keyphrase: ""Bias reduction at the expense of language modeling capability""",3
arXIv2022,Using cognitive psychology to understand GPT-3,Yes.,5,"""Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task.""",2022,2022-06-21T20:06:03Z,"Keyphrase: ""Failure in causal reasoning""","""Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task."" Keyphrase: ""Failure in causal reasoning""",1
arXIv2022,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,Yes.,5,"""Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models.""",2022,2022-06-21T16:15:27Z,"Keyphrase: ""Limited plan generation capabilities""","""Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models."" Keyphrase: ""Limited plan generation capabilities""",1
arXIv2022,"Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",Yes.,4,"""we examine the connection between model size and its gender bias (specifically, occupational gender bias)."" and ""Our findings highlight the potential risks that can arise from increasing model size.""",2022,2022-06-20T15:52:40Z,"Keyphrase: ""Gender bias amplification""","""we examine the connection between model size and its gender bias (specifically, occupational gender bias)."" and ""Our findings highlight the potential risks that can arise from increasing model size."" Keyphrase: ""Gender bias amplification""",3
arXIv2022,Methods for Estimating and Improving Robustness of Language Models,Yes.,5,"""large language models (LLMs) suffer notorious flaws related to their preference for simple, surface-level textual relations over full semantic complexity of the problem"" and ""weak ability to generalise outside of the training domain.""",2022,2022-06-16T21:02:53Z,"Keyphrase: ""Weak generalization outside training domain""","""large language models (LLMs) suffer notorious flaws related to their preference for simple, surface-level textual relations over full semantic complexity of the problem"" and ""weak ability to generalise outside of the training domain."" Keyphrase: ""Weak generalization outside training domain""",1
arXIv2022,Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,Yes.,5,"""However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful.""",2022,2022-06-16T17:28:01Z,"Keyphrase: ""Toxic, biased, and untruthful language""","""However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful."" Keyphrase: ""Toxic, biased, and untruthful language""",3
arXIv2022,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,Yes.,5,"""it is vital that we understand the present and near-future capabilities and limitations of language models"" and ""BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models"" and ""model performance and calibration both improve with scale, but are poor in absolute terms"" and ""social bias typically increases with scale in settings with ambiguous context, but this can be",2022,2022-06-09T17:05:34Z,"Keyphrase: ""Limited performance calibration""","""it is vital that we understand the present and near-future capabilities and limitations of language models"" and ""BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models"" and ""model performance and calibration both improve with scale, but are poor in absolute terms"" and ""social bias typically increases with scale in settings with ambiguous context, but this can be Keyphrase: ""Limited performance calibration""",7
arXIv2022,Neuro-Symbolic Procedural Planning with Commonsense Prompting,Yes.,4,"""it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures"" and ""such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks.""",2022,2022-06-06T22:09:52Z,"Keyphrase: ""Spurious correlations and impaired generalization""","""it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures"" and ""such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks."" Keyphrase: ""Spurious correlations and impaired generalization""",2
arXIv2022,Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models,Yes.,5,"""However, models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is inevitable.""",2022,2022-06-05T20:02:30Z,"Keyphrase: ""Limited crosslingual detoxification""","""However, models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is inevitable."" Keyphrase: ""Limited crosslingual detoxification""",6
arXIv2022,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,Yes.,5,"""Its application to video generation is still facing many challenges",2022,2022-05-29T19:02:15Z,"Keyphrase: ""Challenges in video generation""","""Its application to video generation is still facing many challenges Keyphrase: ""Challenges in video generation""",4
arXIv2022,Quark: Controllable Text Generation with Reinforced Unlearning,Yes.,5,"""Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user.""",2022,2022-05-26T21:11:51Z,"Keyphrase: ""Offensive and toxic language with repetition""","""Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user."" Keyphrase: ""Offensive and toxic language with repetition""",2
arXIv2022,Differentially Private Decoding in Large Language Models,Yes.,4,"""LLMs, while effective, have been shown to memorize instances of training data thereby potentially revealing private information processed during pre-training.""",2022,2022-05-26T20:50:58Z,"Keyphrase: ""Privacy concerns and memorization""","""LLMs, while effective, have been shown to memorize instances of training data thereby potentially revealing private information processed during pre-training."" Keyphrase: ""Privacy concerns and memorization""",8
arXIv2022,RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning,Yes.,5,"""In our experiments with RoBERTa and T5, we find that the models trained in prior works do not perform consistently on the different perturbations in RobustLR, thus showing that the models are not robust to the proposed logical perturbations. Further, we find that the models",2022,2022-05-25T09:23:50Z,"Keyphrase: ""Limited robustness to perturbations""","""In our experiments with RoBERTa and T5, we find that the models trained in prior works do not perform consistently on the different perturbations in RobustLR, thus showing that the models are not robust to the proposed logical perturbations. Further, we find that the models Keyphrase: ""Limited robustness to perturbations""",3
arXIv2022,Perturbation Augmentation for Fairer NLP,Yes.,4,"""Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets."" and ""Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models.""",2022,2022-05-25T09:00:29Z,"Keyphrase: ""Unwanted social biases""","""Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets."" and ""Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models."" Keyphrase: ""Unwanted social biases""",3
arXIv2022,Memorization in NLP Fine-tuning Methods,Yes.,4,"""Large language models are shown to present privacy risks through memorization of training data,"" and ""we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different.""",2022,2022-05-25T05:49:31Z,"Keyphrase: ""Privacy risk and memorization""","""Large language models are shown to present privacy risks through memorization of training data,"" and ""we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different."" Keyphrase: ""Privacy risk and memorization""",8
arXIv2022,On Measuring Social Biases in Prompt-Based Multi-Task Learning,Yes.,4,"""We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs."" and ""The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training,",2022,2022-05-23T20:01:20Z,"Keyphrase: ""Input encoding bias""","""We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs."" and ""The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, Keyphrase: ""Input encoding bias""",3
arXIv2022,Challenges in Measuring Bias via Open-Ended Language Generation,Yes.,4,"""Researchers have devised numerous ways to quantify social biases vested in pretrained language models."" and ""We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings.""",2022,2022-05-23T19:57:15Z,"Keyphrase: ""Inconsistent bias measurement""","""Researchers have devised numerous ways to quantify social biases vested in pretrained language models."" and ""We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings."" Keyphrase: ""Inconsistent bias measurement""",3
arXIv2022,Outliers Dimensions that Disrupt Transformers Are Driven by Frequency,Yes.,5,"""While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon",2022,2022-05-23T15:19:09Z,"Keyphrase: ""Robustness issue with outlier phenomenon""","""While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon Keyphrase: ""Robustness issue with outlier phenomenon""",5
arXIv2022,Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements,Yes.,4,"""We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce",2022,2022-05-23T15:05:27Z,"Keyphrase: ""Bias and realism in generated advertisements""","""We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce Keyphrase: ""Bias and realism in generated advertisements""",3
arXIv2022,RL with KL penalties is better viewed as Bayesian inference,Yes.,5,"""We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse",2022,2022-05-23T12:47:13Z,"Keyphrase: ""Distribution collapse in finetuning""","""We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse Keyphrase: ""Distribution collapse in finetuning""",5
arXIv2022,Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers,Yes.,5,"""This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form.""",2022,2022-05-22T18:03:03Z,"Keyphrase: ""Limited reasoning ability""","""This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form."" Keyphrase: ""Limited reasoning ability""",1
arXIv2022,Scaling Laws and Interpretability of Learning from Repeated Data,Yes.,5,"""Recent large language models have been trained on vast datasets, but also often on repeated data... Some works have reported substantial negative performance effects of this repeated data."" and ""We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance.""",2022,2022-05-21T02:14:27Z,"Keyphrase: ""Negative performance effect of repeated data""","""Recent large language models have been trained on vast datasets, but also often on repeated data... Some works have reported substantial negative performance effects of this repeated data."" and ""We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance."" Keyphrase: ""Negative performance effect of repeated data""",5
arXIv2022,Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation,Yes.,4,"""Our results show that GPT-2 amplifies bias by considering women as junior and men as senior more often than the ground truth in both domains. These results suggest that NLP applications built using GPT-2 may harm women in professional capacities.""",2022,2022-05-19T20:05:02Z,"Keyphrase: ""Amplification of gender bias""","""Our results show that GPT-2 amplifies bias by considering women as junior and men as senior more often than the ground truth in both domains. These results suggest that NLP applications built using GPT-2 may harm women in professional capacities."" Keyphrase: ""Amplification of gender bias""",3
arXIv2022,Overcoming Language Disparity in Online Content Classification with Multimodal Learning,Yes.,4,"""Large language models are now the standard to develop state-of-the-art solutions for text detection and classification tasks. However, the development of advanced computational techniques and resources is disproportionately focused on the English language, sidelining a majority of the languages spoken globally."" and ""we situate our findings with respect",2022,2022-05-19T17:56:02Z,"Keyphrase: ""English-centric bias""","""Large language models are now the standard to develop state-of-the-art solutions for text detection and classification tasks. However, the development of advanced computational techniques and resources is disproportionately focused on the English language, sidelining a majority of the languages spoken globally."" and ""we situate our findings with respect Keyphrase: ""English-centric bias""",3
arXIv2022,Are Prompt-based Models Clueless?,Yes.,4,"""models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets"" and ""Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues.""",2022,2022-05-19T02:47:58Z,"Keyphrase: ""Dependency on superficial cues""","""models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets"" and ""Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues."" Keyphrase: ""Dependency on superficial cues""",3
arXIv2022,"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",Yes.,5,"""We find that humans are far more robust than LLMs on this benchmark.""",2022,2022-05-11T18:14:33Z,"Keyphrase: ""Robustness challenge""","""We find that humans are far more robust than LLMs on this benchmark."" Keyphrase: ""Robustness challenge""",3
arXIv2022,Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence,Yes.,5,"""much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property"" and ""we observe that PLMs violate the LNP frequently.""",2022,2022-05-08T08:37:36Z,"Keyphrase: ""Violation of linguistic norms and principles""","""much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property"" and ""we observe that PLMs violate the LNP frequently."" Keyphrase: ""Violation of linguistic norms and principles""",3
arXIv2022,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,Yes.,5,"""We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations.""",2022,2022-05-06T17:57:58Z,"Keyphrase: ""Unreliable explanations""","""We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations."" Keyphrase: ""Unreliable explanations""",1
arXIv2022,Provably Confidential Language Modelling,Yes.,4,"""Large language models are shown to memorize privacy information such as social security numbers in training data.""",2022,2022-05-04T02:33:45Z,"Keyphrase: ""Privacy information memorization""","""Large language models are shown to memorize privacy information such as social security numbers in training data."" Keyphrase: ""Privacy information memorization""",8
arXIv2022,Training Language Models with Language Feedback,Yes.,5,"""Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries.""",2022,2022-04-29T15:06:58Z,"Keyphrase: ""Generation of offensive and factually incorrect text""","""Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries."" Keyphrase: ""Generation of offensive and factually incorrect text""",2
arXIv2022,Inferring Implicit Relations in Complex Questions with Language Models,Yes.,5,"""we investigate why current models struggle with implicit reasoning question answering (QA) tasks,"" and ""we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations.""",2022,2022-04-28T21:00:54Z,"Keyphrase: ""Struggles with implicit reasoning""","""we investigate why current models struggle with implicit reasoning question answering (QA) tasks,"" and ""we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations."" Keyphrase: ""Struggles with implicit reasoning""",1
arXIv2022,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,Yes.,5,"""the in-depth analysis of when in-context learning occurs is still lacking"" and ""in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning"" and ""pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in",2022,2022-04-28T13:59:54Z,"Keyphrase: ""Limited in-context learning""","""the in-depth analysis of when in-context learning occurs is still lacking"" and ""in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning"" and ""pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in Keyphrase: ""Limited in-context learning""",1
arXIv2022,You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas,Yes.,4,"""privacy concerns have arisen recently",2022,2022-04-26T09:36:18Z,"Keyphrase: ""Privacy concerns""","""privacy concerns have arisen recently Keyphrase: ""Privacy concerns""",8
arXIv2022,You Are What You Write: Preserving Privacy in the Era of Large Language Models,Yes.,4,"""Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e",2022,2022-04-20T11:12:53Z,"Keyphrase: ""Trust and privacy concerns""","""Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e Keyphrase: ""Trust and privacy concerns""",8
arXIv2022,What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment,Yes.,5,"""The capabilities of large transformer models as instruction learners, however, remain poorly understood."" and ""our model, a fine-tuned T5-based text2text transformer, struggles with large regular languages, suggesting that less precise instructions are challenging for models. Additionally, instruction executions that require tracking longer contexts of prior steps are also more difficult.""",2022,2022-04-19T22:11:47Z,"Keyphrase: ""Challenges with precise instruction execution""","""The capabilities of large transformer models as instruction learners, however, remain poorly understood."" and ""our model, a fine-tuned T5-based text2text transformer, struggles with large regular languages, suggesting that less precise instructions are challenging for models. Additionally, instruction executions that require tracking longer contexts of prior steps are also more difficult."" Keyphrase: ""Challenges with precise instruction execution""",5
arXIv2022,Pathologies of Pre-trained Language Models in Few-shot Fine-tuning,Yes.,5,"""without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels"" and ""pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot",2022,2022-04-17T15:55:18Z,"Keyphrase: ""Prediction bias and few-shot learning issues""","""without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels"" and ""pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot Keyphrase: ""Prediction bias and few-shot learning issues""",5
arXIv2022,Building Markovian Generative Architectures over Pretrained LM Backbones for Efficient Task-Oriented Dialog Systems,Yes.,5,"""A drawback of existing PLM-based models is their non-Markov architectures across turns, i.e., the whole history is used as the conditioning input at each turn. First, this brings inefficiencies in memory and computation. Furthermore, using the whole history increases model",2022,2022-04-13T15:21:34Z,"Keyphrase: ""Inefficient non-Markov architecture""","""A drawback of existing PLM-based models is their non-Markov architectures across turns, i.e., the whole history is used as the conditioning input at each turn. First, this brings inefficiencies in memory and computation. Furthermore, using the whole history increases model Keyphrase: ""Inefficient non-Markov architecture""",5
arXIv2022,Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Yes.,4,"""current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets.""",2022,2022-04-13T10:32:03Z,"Keyphrase: ""Limited linguistic skill mapping""","""current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets."" Keyphrase: ""Limited linguistic skill mapping""",1
arXIv2022,Impossible Triangle: What's Next for Pre-trained Language Models?,Yes.,5,"""However, many of such models come with a dauntingly huge size that few institutions can afford to pre-train, fine-tune or even deploy, while moderate-sized models usually lack strong generalized few-shot learning capabilities."" and ""We argue that all existing PLM models lack one or more properties from the Impossible Triangle.""",2022,2022-04-13T01:28:18Z,"Keyphrase: ""Limited few-shot learning capability""","""However, many of such models come with a dauntingly huge size that few institutions can afford to pre-train, fine-tune or even deploy, while moderate-sized models usually lack strong generalized few-shot learning capabilities."" and ""We argue that all existing PLM models lack one or more properties from the Impossible Triangle."" Keyphrase: ""Limited few-shot learning capability""",5
arXIv2022,Testing the limits of natural language models for predicting human language judgments,Yes.,5,"""experiments also revealed significant shortcomings of its alignment with human perception.""",2022,2022-04-07T17:12:57Z,"Keyphrase: ""Misalignment with human perception""","""experiments also revealed significant shortcomings of its alignment with human perception."" Keyphrase: ""Misalignment with human perception""",1
arXIv2022,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",Yes.,5,"""a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment.""",2022,2022-04-04T17:57:11Z,"Keyphrase: ""Lack of real-world experience""","""a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment."" Keyphrase: ""Lack of real-world experience""",1
arXIv2022,Mitigating Gender Bias in Machine Translation through Adversarial Learning,Yes.,4,"""restructuring training objectives in the context of fine-tuning pretrained large language models"" and ""addresses these challenges to mitigate gender bias in seq2seq machine translation.""",2022,2022-03-20T23:35:09Z,"Keyphrase: ""Gender bias mitigation challenge""","""restructuring training objectives in the context of fine-tuning pretrained large language models"" and ""addresses these challenges to mitigate gender bias in seq2seq machine translation."" Keyphrase: ""Gender bias mitigation challenge""",3
arXIv2022,Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists,Yes.,4,"""Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability."" and ""severe unintended bias, and lower performance.""",2022,2022-03-17T09:29:50Z,"Keyphrase: ""Overfitting and unintended bias""","""Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability."" and ""severe unintended bias, and lower performance."" Keyphrase: ""Overfitting and unintended bias""",3
arXIv2022,Do Language Models Plagiarize?,Yes.,5,"""Given that a majority of LMs' training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indis",2022,2022-03-15T03:11:11Z,"Keyphrase: ""Ethical implications of data scraping""","""Given that a majority of LMs' training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indis Keyphrase: ""Ethical implications of data scraping""",8
arXIv2022,Speciesist Language and Nonhuman Animal Bias in English Masked Language Models,Yes.,4,"""We found that pre-trained masked language models tend to associate harmful words with nonhuman animals and have a bias toward using speciesist language for some nonhuman animal names.""",2022,2022-03-10T03:32:29Z,"Keyphrase: ""Speciesist language bias""","""We found that pre-trained masked language models tend to associate harmful words with nonhuman animals and have a bias toward using speciesist language for some nonhuman animal names."" Keyphrase: ""Speciesist language bias""",3
arXIv2022,Training language models to follow instructions with human feedback,Yes.,5,"""large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user.""",2022,2022-03-04T07:04:42Z,"Keyphrase: ""Untruthful and toxic outputs""","""large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user."" Keyphrase: ""Untruthful and toxic outputs""",2
arXIv2022,Logical Fallacy Detection,Yes.,5,"""We find that existing pretrained large language models perform poorly on this task.""",2022,2022-02-28T13:18:26Z,"Keyphrase: ""Poor task performance""","""We find that existing pretrained large language models perform poorly on this task."" Keyphrase: ""Poor task performance""",7
arXIv2022,Reward Modeling for Mitigating Toxicity in Transformer-based Language Models,Yes.,4,"""language models that are pretrained on large unlabeled web text corpora have been shown to suffer from degenerating toxic content and social bias behaviors, consequently hindering their safe deployment.""",2022,2022-02-19T19:26:22Z,"Keyphrase: ""Degenerating toxic content and social bias""","""language models that are pretrained on large unlabeled web text corpora have been shown to suffer from degenerating toxic content and social bias behaviors, consequently hindering their safe deployment."" Keyphrase: ""Degenerating toxic content and social bias""",2
arXIv2022,Quantifying Memorization Across Neural Language Models,Yes.,5,"""Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality),",2022,2022-02-15T18:48:31Z,"Keyphrase: ""Undesirable memorization""","""Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), Keyphrase: ""Undesirable memorization""",8
arXIv2022,Deduplicating Training Data Mitigates Privacy Risks in Language Models,Yes.,5,"""Past work has shown that large language models are susceptible to privacy attacks,"" and ""Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks.""",2022,2022-02-14T08:20:15Z,"Keyphrase: ""Privacy vulnerabilities""","""Past work has shown that large language models are susceptible to privacy attacks,"" and ""Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks."" Keyphrase: ""Privacy vulnerabilities""",8
arXIv2022,Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,Yes.,4,"""Pre-trained language models (LMs) are shown to easily generate toxic language."" and ""We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify.""",2022,2022-02-08T22:10:40Z,"Keyphrase: ""Toxic language generation""","""Pre-trained language models (LMs) are shown to easily generate toxic language."" and ""We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify."" Keyphrase: ""Toxic language generation""",2
arXIv2022,Survey of Hallucination in Natural Language Generation,Yes.,4,"""deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios"" and ""hallucinations in large language models (LLMs).""",2022,2022-02-08T03:55:01Z,"Keyphrase: ""Unintended text hallucination""","""deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios"" and ""hallucinations in large language models (LLMs)."" Keyphrase: ""Unintended text hallucination""",0
arXIv2022,What Has Been Enhanced in my Knowledge-Enhanced Language Model?,Yes.,4,"""Pretrained language models (LMs) do not capture factual knowledge very well."" and ""it is unclear how and what kind of knowledge is effectively integrated into these models and if such integration may lead to catastrophic forgetting of already learned knowledge.""",2022,2022-02-02T11:23:36Z,"Keyphrase: ""Catastrophic forgetting""","""Pretrained language models (LMs) do not capture factual knowledge very well."" and ""it is unclear how and what kind of knowledge is effectively integrated into these models and if such integration may lead to catastrophic forgetting of already learned knowledge."" Keyphrase: ""Catastrophic forgetting""",5
arXIv2022,Unveiling Project-Specific Bias in Neural Code Models,Yes.,5,"""Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data.""",2022,2022-01-19T02:09:48Z,"Keyphrase: ""Limited generalization to out-of-distribution data""","""Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data."" Keyphrase: ""Limited generalization to out-of-distribution data""",5
arXIv2022,Unintended Bias in Language Model-driven Conversational Recommendation,Yes.,5,"""pretrained LMs are well-known to be prone to intrinsic biases in their training data,"" and ""raises a red flag that advances in the language handling capability of LM-driven CRSs do not come without significant challenges related to mitigating unintended bias.""",2022,2022-01-17T05:50:14Z,"Keyphrase: ""Intrinsic bias""","""pretrained LMs are well-known to be prone to intrinsic biases in their training data,"" and ""raises a red flag that advances in the language handling capability of LM-driven CRSs do not come without significant challenges related to mitigating unintended bias."" Keyphrase: ""Intrinsic bias""",3
arXIv2022,Submix: Practical Private Prediction for Large-Scale Language Models,Yes.,4,"""Recent data-extraction attacks have exposed that language models can memorize some training samples verbatim. This is a vulnerability that can compromise the privacy of the model's training data.""",2022,2022-01-04T04:23:38Z,"Keyphrase: ""Memorization of training data""","""Recent data-extraction attacks have exposed that language models can memorize some training samples verbatim. This is a vulnerability that can compromise the privacy of the model's training data."" Keyphrase: ""Memorization of training data""",8
arXIv2022,A Survey on In-context Learning,Yes.,4,"""We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research.""",2022,2022-12-31T15:57:09Z,"Keyphrase: ""Lack of focus on practical application""","""We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research."" Keyphrase: ""Lack of focus on practical application""",5
arXIv2022,Inconsistencies in Masked Language Models,Yes.,5,"""However, this paper shows that distributions corresponding to different masking patterns can demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together."" and ""This fundamental flaw in MLMs can lead to self-contradictory behaviors during inference.""",2022,2022-12-30T22:53:25Z,"Keyphrase: ""Inconsistent masking patterns""","""However, this paper shows that distributions corresponding to different masking patterns can demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together."" and ""This fundamental flaw in MLMs can lead to self-contradictory behaviors during inference."" Keyphrase: ""Inconsistent masking patterns""",1
arXIv2022,Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,Yes.,5,"""These results suggest that the propensity of larger Transformer-based models to 'memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.""",2022,2022-12-23T03:57:54Z,"Keyphrase: ""Over-reliance on memorization""","""These results suggest that the propensity of larger Transformer-based models to 'memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing."" Keyphrase: ""Over-reliance on memorization""",5
arXIv2022,Parallel Context Windows for Large Language Models,Yes.,5,"""When applied to processing long text, Large Language Models (LLMs) are limited by their context window.""",2022,2022-12-21T11:38:51Z,"Keyphrase: ""Limited context window""","""When applied to processing long text, Large Language Models (LLMs) are limited by their context window."" Keyphrase: ""Limited context window""",7
arXIv2022,Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners,Yes.,5,"""However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model.""",2022,2022-12-21T09:37:05Z,"Keyphrase: ""Limited training data scalability""","""However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model."" Keyphrase: ""Limited training data scalability""",5
arXIv2022,From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models,Yes.,4,"""However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task.""",2022,2022-12-21T08:39:36Z,"Keyphrase: ""Modality disconnection""","""However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task."" Keyphrase: ""Modality disconnection""",4
arXIv2022,"Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",Yes.,5,"""Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking.""",2022,2022-12-21T04:43:19Z,"Keyphrase: ""Lack of pragmatic capability""","""Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking."" Keyphrase: ""Lack of pragmatic capability""",1
arXIv2022,Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering,Yes.,5,"""While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (~50% for deletion intervention, and ~20% drop in accuracy for negation intervention)."" and ""But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models'",2022,2022-12-21T00:00:01Z,"Keyphrase: ""Semantic unfaithfulness""","""While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (~50% for deletion intervention, and ~20% drop in accuracy for negation intervention)."" and ""But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models' Keyphrase: ""Semantic unfaithfulness""",5
arXIv2022,Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing,Yes.,4,"""Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics."" and ""existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain",2022,2022-12-20T22:41:24Z,"Keyphrase: ""Harmful humanlike biases""","""Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics."" and ""existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain Keyphrase: ""Harmful humanlike biases""",3
arXIv2022,Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,Yes.,4,"""Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs."" and ""Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.""",2022,2022-12-20T18:59:23Z,"Keyphrase: ""Struggle with hierarchical reasoning""","""Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs."" and ""Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers."" Keyphrase: ""Struggle with hierarchical reasoning""",1
arXIv2022,True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4,Yes.,5,"""GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area.""",2022,2022-12-20T09:34:43Z,"Keyphrase: ""Limited deep reasoning ability""","""GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area."" Keyphrase: ""Limited deep reasoning ability""",1
arXIv2022,Do language models have coherent mental models of everyday things?,Yes.,5,"""Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent 'parts mental models' (54-59% accurate, 19-43% conditional constraint violation).""",2022,2022-12-20T06:54:04Z,"Keyphrase: ""Fragmented knowledge representation""","""Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent 'parts mental models' (54-59% accurate, 19-43% conditional constraint violation)."" Keyphrase: ""Fragmented knowledge representation""",0
arXIv2022,Discovering Language Model Behaviors with Model-Written Evaluations,Yes.,5,"""We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (""sycophancy"") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes",2022,2022-12-19T05:13:52Z,"Keyphrase: ""Inverse scaling issues""","""We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (""sycophancy"") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes Keyphrase: ""Inverse scaling issues""",5
arXIv2022,Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,Yes.,5,"""Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse.""",2022,2022-12-18T19:27:41Z,"Keyphrase: ""Weak reasoning capabilities""","""Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse."" Keyphrase: ""Weak reasoning capabilities""",1
arXIv2022,Language model acceptability judgements are not always robust to context,Yes.,4,"""we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context"" and ""we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures.""",2022,2022-12-18T00:11:06Z,"Keyphrase: ""Context matching limitations""","""we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context"" and ""we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures."" Keyphrase: ""Context matching limitations""",5
arXIv2022,MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation,Yes.,5,"""these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency.""",2022,2022-12-16T17:36:23Z,"Keyphrase: ""Low semantic coverage and logical inconsistency""","""these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency."" Keyphrase: ""Low semantic coverage and logical inconsistency""",0
arXIv2022,Teaching Small Language Models to Reason,Yes.,4,"""However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters.""",2022,2022-12-16T11:24:42Z,"Keyphrase: ""Limited reasoning capability""","""However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters."" Keyphrase: ""Limited reasoning capability""",1
arXIv2022,Understanding How Model Size Affects Few-shot Instruction Prompting,Yes.,5,"""Large Language Models are affected by the phenomena of memorizing and forgetting their training data."" and ""We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes.""",2022,2022-12-04T19:59:52Z,"Keyphrase: ""Weak few-shot performance""","""Large Language Models are affected by the phenomena of memorizing and forgetting their training data."" and ""We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes."" Keyphrase: ""Weak few-shot performance""",5
arXIv2022,Event knowledge in large language models: the gap between the impossible and the unlikely,Yes.,4,"""However, LLMs show less consistent preferences for likely vs. unlikely events"" and ""highlight a gap between representations of possible/impossible and likely/unlikely events.""",2022,2022-12-02T23:43:18Z,"Keyphrase: ""Inconsistent event representation""","""However, LLMs show less consistent preferences for likely vs. unlikely events"" and ""highlight a gap between representations of possible/impossible and likely/unlikely events."" Keyphrase: ""Inconsistent event representation""",0
arXIv2022,a survey on GPT-3,Yes.,4,"""We discuss some of the challenges that GPT-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers.""",2022,2022-12-01T20:24:19Z,"Keyphrase: ""Training complexity and bias""","""We discuss some of the challenges that GPT-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers."" Keyphrase: ""Training complexity and bias""",0
arXIv2022,An Analysis of Social Biases Present in BERT Variants Across Multiple Languages,Yes.,4,"""Although large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reflect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings.""",2022,2022-11-25T23:38:08Z,"Keyphrase: ""Reflects human bias""","""Although large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reflect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings."" Keyphrase: ""Reflects human bias""",3
arXIv2022,Validating Large Language Models with ReLM,Yes.,5,"""there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language.""",2022,2022-11-21T21:40:35Z,"Keyphrase: ""Data memorization bias""","""there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language."" Keyphrase: ""Data memorization bias""",8
arXIv2022,Conceptor-Aided Debiasing of Large Language Models,Yes.,5,"""Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus."" and ""CI-BERT reduces the language model accuracy.""",2022,2022-11-20T21:24:48Z,"Keyphrase: ""Inherent social bias""","""Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus."" and ""CI-BERT reduces the language model accuracy."" Keyphrase: ""Inherent social bias""",3
arXIv2022,PAL: Program-aided Language Models,Yes.,5,"""While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly.""",2022,2022-11-18T18:56:13Z,"Keyphrase: ""Logical arithmetic mistakes""","""While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly."" Keyphrase: ""Logical arithmetic mistakes""",1
arXIv2022,Evaluating the Factual Consistency of Large Language Models Through News Summarization,Yes.,5,"""While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information."" and ""However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries.""",2022,2022-11-15T18:50:34Z,"Keyphrase: ""Factually inconsistent summaries""","""While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information."" and ""However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries."" Keyphrase: ""Factually inconsistent summaries""",0
arXIv2022,GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective,Yes.,5,"""the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods"" and ""significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.""",2022,2022-11-15T11:53:55Z,"Keyphrase: ""Out-of-distribution generalization challenge""","""the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods"" and ""significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy."" Keyphrase: ""Out-of-distribution generalization challenge""",2
arXIv2022,Does Debiasing Inevitably Degrade the Model Performance,Yes.,4,"""Gender bias in language models has attracted sufficient attention because it threatens social justice. However, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious.""",2022,2022-11-14T13:46:13Z,"Keyphrase: ""Degraded performance due to debiasing methods""","""Gender bias in language models has attracted sufficient attention because it threatens social justice. However, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious."" Keyphrase: ""Degraded performance due to debiasing methods""",3
arXIv2022,Large Language Models with Controllable Working Memory,Yes.,5,"""We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size.""",2022,2022-11-09T18:58:29Z,"Keyphrase: ""Poor controllability and robustness""","""We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size."" Keyphrase: ""Poor controllability and robustness""",5
arXIv2022,Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic,Yes.,5,"""Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning.""",2022,2022-11-03T18:53:30Z,"Keyphrase: ""Limited numeric comprehension""","""Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning."" Keyphrase: ""Limited numeric comprehension""",1
arXIv2022,LMentry: A Language Model Benchmark of Elementary Language Tasks,Yes.,5,"""Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI's latest 175B-parameter instruction-tuned model, TextDavinci002.""",2022,2022-11-03T18:01:12Z,"Keyphrase: ""Wide variety of failure cases""","""Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI's latest 175B-parameter instruction-tuned model, TextDavinci002."" Keyphrase: ""Wide variety of failure cases""",6
arXIv2022,Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer,Yes.,5,"""They impose, however, limits on the maximum input length (512 sub-words in BERT), which are too restrictive in the legal domain. Even sparse-attention models, such as Longformer and BigBird, which increase the maximum input length to 4,096 sub-",2022,2022-11-02T09:27:01Z,"Keyphrase: ""Input length limitations""","""They impose, however, limits on the maximum input length (512 sub-words in BERT), which are too restrictive in the legal domain. Even sparse-attention models, such as Longformer and BigBird, which increase the maximum input length to 4,096 sub- Keyphrase: ""Input length limitations""",4
arXIv2022,Two-stage LLM Fine-tuning with Less Specialization and More Generalization,Yes.,5,"""fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances,"" and ""format specialization, where the model overfits to the format of the fine-tuned task.""",2022,2022-11-01T17:56:57Z,"Keyphrase: ""Overfitting to specialized datasets""","""fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances,"" and ""format specialization, where the model overfits to the format of the fine-tuned task."" Keyphrase: ""Overfitting to specialized datasets""",6
arXIv2022,The future is different: Large pre-trained language models fail in prediction tasks,Yes.,5,"""Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time."" and ""we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the",2022,2022-11-01T11:01:36Z,"Keyphrase: ""Performance drop with distribution shift""","""Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time."" and ""we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the Keyphrase: ""Performance drop with distribution shift""",5
arXIv2022,"A Simple, Yet Effective Approach to Finding Biases in Code Generation",Yes.,4,"""This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.""",2022,2022-10-31T15:06:15Z,"Keyphrase: ""Undesired bias in code generation""","""This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances."" Keyphrase: ""Undesired bias in code generation""",7
arXIv2022,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,Yes.,5,"""Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., the language model pre-trained on static data from past years performs worse over time on emerging data.""",2022,2022-10-31T08:12:41Z,"Keyphrase: ""Temporal generalization limitations""","""Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., the language model pre-trained on static data from past years performs worse over time on emerging data."" Keyphrase: ""Temporal generalization limitations""",5
arXIv2022,Contrastive Decoding: Open-ended Text Generation as Optimization,Yes.,5,"""maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text"" and ""sampling can often produce incoherent text that drifts from the original topics.""",2022,2022-10-27T00:58:21Z,"Keyphrase: ""Incoherent text generation""","""maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text"" and ""sampling can often produce incoherent text that drifts from the original topics."" Keyphrase: ""Incoherent text generation""",1
arXIv2022,Privately Fine-Tuning Large Language Models with Differential Privacy,Yes.,4,"""However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs.""",2022,2022-10-26T21:18:31Z,"Keyphrase: ""Privacy concerns due to data exposure""","""However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs."" Keyphrase: ""Privacy concerns due to data exposure""",8
arXIv2022,Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence,Yes.,4,"""We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledge in their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally.""",2022,2022-10-25T01:46:00Z,"Keyphrase: ""Reliance on nonparametric knowledge""","""We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledge in their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally."" Keyphrase: ""Reliance on nonparametric knowledge""",4
arXIv2022,Speeding Up Question Answering Task of Language Models via Inverted Index,Yes.,4,"""Despite the wide popularity of large language models (LLMs), few real-world conversational agents take advantage of LLMs. Extensive resources consumed by LLMs disable developers from integrating them into end-user applications.""",2022,2022-10-24T19:59:17Z,"Keyphrase: ""Resource consumption limitations""","""Despite the wide popularity of large language models (LLMs), few real-world conversational agents take advantage of LLMs. Extensive resources consumed by LLMs disable developers from integrating them into end-user applications."" Keyphrase: ""Resource consumption limitations""",6
arXIv2022,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,Yes.,5,"""We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box,"" and ""Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively,"" and """,2022,2022-10-24T14:58:58Z,"Keyphrase: ""Lack of social intelligence""","""We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box,"" and ""Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively,"" and "" Keyphrase: ""Lack of social intelligence""",3
arXIv2022,Do Language Models Understand Measurements?,Yes.,5,"""In this study, we show that PLMs lack the capability required for reasoning over measurements.""",2022,2022-10-23T10:52:52Z,"Keyphrase: ""Lack of reasoning capability""","""In this study, we show that PLMs lack the capability required for reasoning over measurements."" Keyphrase: ""Lack of reasoning capability""",1
arXIv2022,WikiWhy: Answering and Explaining Cause-and-Effect Questions,Yes.,4,"""GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.""",2022,2022-10-21T17:59:03Z,"Keyphrase: ""Room for improvement in correctness""","""GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements."" Keyphrase: ""Room for improvement in correctness""",7
arXIv2022,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Yes.,5,"""the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.""",2022,2022-10-21T15:12:37Z,"Keyphrase: ""Overreliance on shallow patterns""","""the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution."" Keyphrase: ""Overreliance on shallow patterns""",1
arXIv2022,LittleBird: Efficient Faster & Longer Transformer for Question Answering,Yes.,5,"""BERT has shown a lot of success in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism.""",2022,2022-10-21T10:46:41Z,"Keyphrase: ""Limitation in handling long inputs""","""BERT has shown a lot of success in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism."" Keyphrase: ""Limitation in handling long inputs""",4
arXIv2022,Towards a neural architecture of language: Deep learning versus logistics of access in neural architectures for compositional processing,Yes.,5,"""I will argue that these models are not suitable as neural models of human language. Firstly, because they fail on fundamental boundary conditions, such as the amount of learning they require. This would in fact imply that the mechanisms of GPT and brain language processing are fundamentally different. Secondly, because they do not possess the logistics of access needed for compositional and productive human language processing.""",2022,2022-10-19T13:31:26Z,"Keyphrase: ""Misalignment with human language processing""","""I will argue that these models are not suitable as neural models of human language. Firstly, because they fail on fundamental boundary conditions, such as the amount of learning they require. This would in fact imply that the mechanisms of GPT and brain language processing are fundamentally different. Secondly, because they do not possess the logistics of access needed for compositional and productive human language processing."" Keyphrase: ""Misalignment with human language processing""",1
arXIv2022,SafeText: A Benchmark for Exploring Physical Safety in Language Models,Yes.,5,"""We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice.""",2022,2022-10-18T17:59:31Z,"Keyphrase: ""Susceptibility to generating unsafe text""","""We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice."" Keyphrase: ""Susceptibility to generating unsafe text""",2
arXIv2022,"RARR: Researching and Revising What Language Models Say, Using Language Models",Yes.,5,"""However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence.""",2022,2022-10-17T03:44:30Z,"Keyphrase: ""Lack of transparency and trustworthiness""","""However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence."" Keyphrase: ""Lack of transparency and trustworthiness""",0
arXIv2022,Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,Yes.,4,"""Several studies have explored these harms and called for their mitigation via development of safer, fairer models."" and ""this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models.""",2022,2022-10-14T10:43:39Z,"Keyphrase: ""Societal harm potential""","""Several studies have explored these harms and called for their mitigation via development of safer, fairer models."" and ""this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models."" Keyphrase: ""Societal harm potential""",2
arXIv2022,BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation,Yes.,4,"""it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern on the fairness of PLMs as metrics"" and ""We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely",2022,2022-10-14T08:24:11Z,"Keyphrase: ""Societal bias encoding""","""it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern on the fairness of PLMs as metrics"" and ""We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely Keyphrase: ""Societal bias encoding""",3
arXIv2022,"""John is 50 years old, can his son be 65?"" Evaluating NLP Models' Understanding of Feasibility",Yes.,5,"""Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities."" and ""We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly.""",2022,2022-10-14T02:46:06Z,"Keyphrase: ""Failure in complex reasoning""","""Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities."" and ""We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly."" Keyphrase: ""Failure in complex reasoning""",1
arXIv2022,SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,Yes.,4,"""A common limitation of diagnostic tests for detecting social biases in NLP models"" and ""we are able to uncover the model's stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations",2022,2022-10-13T18:04:48Z,"Keyphrase: ""Limited ability to detect and address social biases""","""A common limitation of diagnostic tests for detecting social biases in NLP models"" and ""we are able to uncover the model's stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations Keyphrase: ""Limited ability to detect and address social biases""",3
arXIv2022,Assessing Out-of-Domain Language Model Performance from Few Examples,Yes.,5,"""While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts.""",2022,2022-10-13T04:45:26Z,"Keyphrase: ""Unpredictable domain shift""","""While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts."" Keyphrase: ""Unpredictable domain shift""",5
arXIv2022,SEAL : Interactive Tool for Systematic Error Analysis and Labeling,Yes.,4,"""However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation.""",2022,2022-10-11T23:51:44Z,"Keyphrase: ""Failure on tail data""","""However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation."" Keyphrase: ""Failure on tail data""",2
arXIv2022,Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models,Yes.,4,"""Here we show another problem with multilingual models",2022,2022-10-11T17:06:38Z,"Keyphrase: ""Challenges with multilingual capabilities""","""Here we show another problem with multilingual models Keyphrase: ""Challenges with multilingual capabilities""",1
arXIv2022,Generating Executable Action Plans with Environmentally-Aware Language Models,Yes.,5,"""However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints.""",2022,2022-10-10T18:56:57Z,"Keyphrase: ""Ambiguity in executable plans""","""However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints."" Keyphrase: ""Ambiguity in executable plans""",7
arXIv2022,Quantifying Social Biases Using Templates is Unreliable,Yes.,5,"""Recently, there has been an increase in efforts to understand how large language models (LLMs) propagate and amplify social biases."" and ""Our results indicate that quantifying fairness in LLMs, as done in current practice, can be brittle and needs to be approached with more care and caution.""",2022,2022-10-09T20:05:29Z,"Keyphrase: ""Amplification of social bias""","""Recently, there has been an increase in efforts to understand how large language models (LLMs) propagate and amplify social biases."" and ""Our results indicate that quantifying fairness in LLMs, as done in current practice, can be brittle and needs to be approached with more care and caution."" Keyphrase: ""Amplification of social bias""",3
arXIv2022,Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems,Yes.,5,"""However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization.""",2022,2022-10-07T11:16:45Z,"Keyphrase: ""Poor out-of-domain generalization""","""However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization."" Keyphrase: ""Poor out-of-domain generalization""",5
arXIv2022,Measuring and Narrowing the Compositionality Gap in Language Models,Yes.,5,"""we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.""",2022,2022-10-07T06:50:23Z,"Keyphrase: ""Limited compositional reasoning""","""we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning."" Keyphrase: ""Limited compositional reasoning""",1
arXIv2022,ThinkSum: Probabilistic reasoning over sets using large language models,Yes.,4,"""However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions.""",2022,2022-10-04T00:34:01Z,"Keyphrase: ""Limited reasoning capabilities""","""However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions."" Keyphrase: ""Limited reasoning capabilities""",1
arXIv2022,Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,Yes.,5,"""However, they have difficulty with proof planning",2022,2022-10-03T21:34:32Z,"Keyphrase: ""Difficulty in planning""","""However, they have difficulty with proof planning Keyphrase: ""Difficulty in planning""",1
arXIv2022,A Non-monotonic Self-terminating Language Model,Yes.,5,"""However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling.""",2022,2022-10-03T00:28:44Z,"Keyphrase: ""Undesirable sequence generation""","""However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling."" Keyphrase: ""Undesirable sequence generation""",4
arXIv2022,On the Impossible Safety of Large AI Models,Yes.,5,"""Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues."" and ""we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.""",2022,2022-09-30T06:36:49Z,"Keyphrase: ""Serious security issues""","""Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues."" and ""we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees."" Keyphrase: ""Serious security issues""",2
arXIv2022,Unpacking Large Language Models with Conceptual Consistency,Yes.,5,"""While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency.""",2022,2022-09-29T20:55:57Z,"Keyphrase: ""Lack of conceptual consistency""","""While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency."" Keyphrase: ""Lack of conceptual consistency""",1
arXIv2022,How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI,Yes.,4,"""While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society."" and ""We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat",2022,2022-09-27T18:44:41Z,"Keyphrase: ""Subgroup disparities""","""While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society."" and ""We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat Keyphrase: ""Subgroup disparities""",3
arXIv2022,Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour,Yes.,4,"""Gordon and Van Durme (2013) point out that LMs can thus suffer from reporting bias",2022,2022-09-26T15:45:23Z,"Keyphrase: ""Reporting bias""","""Gordon and Van Durme (2013) point out that LMs can thus suffer from reporting bias Keyphrase: ""Reporting bias""",3
arXIv2022,Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,Yes.,5,"""By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions.""",2022,2022-09-26T14:05:10Z,"Keyphrase: ""Limited adherence to given instructions""","""By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions."" Keyphrase: ""Limited adherence to given instructions""",7
arXIv2022,WinoDict: Probing language models for in-context word acquisition,Yes.,5,"""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs",2022,2022-09-25T05:30:13Z,"Keyphrase: ""Limited diachronic adaptation""","""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs Keyphrase: ""Limited diachronic adaptation""",5
arXIv2022,"A Case Report On The ""A.I. Locked-In Problem"": social concerns with modern NLP",Yes.,5,"""However, practical experimentation with GPT-3 shows that there is a recurring problem with these modern NLP systems, namely that they can 'get stuck' in the narrative so that further conversations, prompt executions or commands become futile. This is here referred to as the 'Locked-In Problem' and is exemplified with an experimental case report, followed by practical and social concerns that are accompanied with",2022,2022-09-22T16:39:35Z,"Keyphrase: ""Getting stuck in narratives""","""However, practical experimentation with GPT-3 shows that there is a recurring problem with these modern NLP systems, namely that they can 'get stuck' in the narrative so that further conversations, prompt executions or commands become futile. This is here referred to as the 'Locked-In Problem' and is exemplified with an experimental case report, followed by practical and social concerns that are accompanied with Keyphrase: ""Getting stuck in narratives""",1
arXIv2022,Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling,Yes.,4,"""However, recent research has highlighted a variety of biases in pre-trained language models."" and ""the pre-trained German language models find substantial conceptual, racial, and gender bias and have significant changes in bias across conceptual and racial axes during fine-tuning on the peer-review data",2022,2022-09-21T13:08:16Z,"Keyphrase: ""Persistent biases after fine-tuning""","""However, recent research has highlighted a variety of biases in pre-trained language models."" and ""the pre-trained German language models find substantial conceptual, racial, and gender bias and have significant changes in bias across conceptual and racial axes during fine-tuning on the peer-review data Keyphrase: ""Persistent biases after fine-tuning""",3
arXIv2022,SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical Corpus,Yes.,5,"""However, since BERT is quadratic to the text length, the BERT model is difficult to be used directly on the long-text corpus."" and ""alleviating the time and space overflow problem of basic BERT on long-text data.""",2022,2022-09-13T05:49:10Z,"Keyphrase: ""Challenges with processing long texts""","""However, since BERT is quadratic to the text length, the BERT model is difficult to be used directly on the long-text corpus."" and ""alleviating the time and space overflow problem of basic BERT on long-text data."" Keyphrase: ""Challenges with processing long texts""",4
arXIv2022,Multilingual Transformer Language Model for Speech Recognition in Low-resource Languages,Yes.,5,"""It is challenging to train and deploy Transformer LMs for hybrid speech recognition 2nd pass re-ranking in low-resource languages due to (1) data scarcity in low-resource languages, (2) expensive computing costs for training and refreshing 100+ monolingual models, and (3) hosting inefficiency",2022,2022-09-08T21:40:41Z,"Keyphrase: ""Challenges in training and deploying for low-resource languages""","""It is challenging to train and deploy Transformer LMs for hybrid speech recognition 2nd pass re-ranking in low-resource languages due to (1) data scarcity in low-resource languages, (2) expensive computing costs for training and refreshing 100+ monolingual models, and (3) hosting inefficiency Keyphrase: ""Challenges in training and deploying for low-resource languages""",4
arXIv2022,Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots,Yes.,5,"""We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too."" and ""This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users.""",2022,2022-09-07T20:45:41Z,"Keyphrase: ""Toxic response generation""","""We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too."" and ""This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users."" Keyphrase: ""Toxic response generation""",2
arXIv2022,The Ethical Need for Watermarks in Machine-Generated Language,Yes.,4,"""The ethical imperative to not blur this distinction arises from the asemantic nature of large language models and from human projections of emotional and cognitive states on machines, possibly leading to manipulation, spreading falsehoods or emotional distress.""",2022,2022-09-07T13:09:44Z,"Keyphrase: ""Ethical manipulation and falsehood spreading""","""The ethical imperative to not blur this distinction arises from the asemantic nature of large language models and from human projections of emotional and cognitive states on machines, possibly leading to manipulation, spreading falsehoods or emotional distress."" Keyphrase: ""Ethical manipulation and falsehood spreading""",3
arXIv2022,Training a T5 Using Lab-sized Resources,Yes.,4,"""Training large neural language models on large datasets is resource- and time-intensive. These requirements create a barrier to entry, where those with fewer resources cannot build competitive models.""",2022,2022-08-25T13:55:16Z,"Keyphrase: ""Resource-intensive barrier to entry""","""Training large neural language models on large datasets is resource- and time-intensive. These requirements create a barrier to entry, where those with fewer resources cannot build competitive models."" Keyphrase: ""Resource-intensive barrier to entry""",4
arXIv2022,On Reality and the Limits of Language Data: Aligning LLMs with Human Norms,Yes.,5,"""their ability to understand the physical world using only language data remains a question"" and ""Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness.""",2022,2022-08-25T10:21:23Z,"Keyphrase: ""Limited understanding of physical world""","""their ability to understand the physical world using only language data remains a question"" and ""Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness."" Keyphrase: ""Limited understanding of physical world""",1
arXIv2022,Shortcut Learning of Large Language Models in Natural Language Understanding,Yes.,5,"""However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness.""",2022,2022-08-25T03:51:39Z,"Keyphrase: ""Dataset bias and generalizability""","""However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness."" Keyphrase: ""Dataset bias and generalizability""",3
arXIv2022,Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models,Yes.,5,"""Recent work shows that this limitation persists in state-of-the-art Transformer-based models."" and ""our findings show how these two complementary approaches enable remarkable sequence extrapolation and highlight a limitation of current architectures to effectively generalize without explicit surface form guidance.""",2022,2022-08-24T11:25:27Z,"Keyphrase: ""Limited generalization without explicit guidance""","""Recent work shows that this limitation persists in state-of-the-art Transformer-based models."" and ""our findings show how these two complementary approaches enable remarkable sequence extrapolation and highlight a limitation of current architectures to effectively generalize without explicit surface form guidance."" Keyphrase: ""Limited generalization without explicit guidance""",5
arXIv2022,"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",Yes.,4,"""We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types."" and ""We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs.""",2022,2022-08-23T23:37:14Z,"Keyphrase: ""Harmful and unethical outputs""","""We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types."" and ""We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs."" Keyphrase: ""Harmful and unethical outputs""",2
arXIv2022,Selection Collider Bias in Large Language Models,Yes.,4,"""sample selection induced collider bias (selection collider bias) that can cause Large Language Models (LLMs) to learn unconditional dependence between entities that are unconditionally independent in the real world"" and ""selection collider bias can become amplified in underspecified learning tasks"".",2022,2022-08-22T05:38:15Z,"Keyphrase: ""Induced collider bias""","""sample selection induced collider bias (selection collider bias) that can cause Large Language Models (LLMs) to learn unconditional dependence between entities that are unconditionally independent in the real world"" and ""selection collider bias can become amplified in underspecified learning tasks"". Keyphrase: ""Induced collider bias""",3
arXIv2022,Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies,Yes.,5,"""A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior."" and ""the last TE reveals a 'hyper-accuracy distortion' present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.""",2022,2022-08-18T17:54:49Z,"Keyphrase: ""Hyperaccuracy distortion""","""A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior."" and ""the last TE reveals a 'hyper-accuracy distortion' present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."" Keyphrase: ""Hyperaccuracy distortion""",6
arXIv2022,What is it like to program with artificial intelligence?,Yes.,4,"""We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.""",2022,2022-08-12T10:48:46Z,"Keyphrase: ""Challenges for end-user programming""","""We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise."" Keyphrase: ""Challenges for end-user programming""",7
arXIv2022,Interactive Code Generation via Test-Driven User-Intent Formalization,Yes.,5,"""users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics.""",2022,2022-08-11T17:41:08Z,"Keyphrase: ""Ambiguity in correctness""","""users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics."" Keyphrase: ""Ambiguity in correctness""",6
arXIv2022,Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts,Yes.,5,"""Our results show the need for additional debiasing of large language models to address higher-order schemas and associations.""",2022,2022-08-08T20:59:16Z,"Keyphrase: ""Higher-order schema bias""","""Our results show the need for additional debiasing of large language models to address higher-order schemas and associations."" Keyphrase: ""Higher-order schema bias""",3
arXIv2022,Gender bias in (non)-contextual clinical word embeddings for stereotypical medical categories,Yes.,4,"""clinical embeddings carry a high degree of bias for some medical terms and diseases which is conflicting with medical literature. Having such an ill-founded relationship might cause harm in downstream applications that use clinical embeddings.""",2022,2022-08-02T10:02:21Z,"Keyphrase: ""Biased clinical embeddings""","""clinical embeddings carry a high degree of bias for some medical terms and diseases which is conflicting with medical literature. Having such an ill-founded relationship might cause harm in downstream applications that use clinical embeddings."" Keyphrase: ""Biased clinical embeddings""",3
arXIv2022,A Hazard Analysis Framework for Code Synthesis Large Language Models,Yes.,5,"""models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential.""",2022,2022-07-25T20:44:40Z,"Keyphrase: ""Alignment problems and misuse potential""","""models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential."" Keyphrase: ""Alignment problems and misuse potential""",3
arXIv2022,Selection Bias Induced Spurious Correlations in Large Language Models,Yes.,5,"""large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias.""",2022,2022-07-18T23:43:52Z,"Keyphrase: ""Dataset selection bias""","""large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias."" Keyphrase: ""Dataset selection bias""",3
arXIv2022,Exploring Length Generalization in Large Language Models,Yes.,5,"""We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale."" and ""We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.""",2022,2022-07-11T14:24:38Z,"Keyphrase: ""Generalization deficiency in longer tasks""","""We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale."" and ""We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems."" Keyphrase: ""Generalization deficiency in longer tasks""",5
arXIv2022,Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning,Yes.,4,"""Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue."" and ""we re-discover a bias-performance trade-off",2022,2022-07-06T06:20:35Z,"Keyphrase: ""Unaddressed bias-performance tradeoff""","""Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue."" and ""we re-discover a bias-performance trade-off Keyphrase: ""Unaddressed bias-performance tradeoff""",3
arXIv2023,Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,Yes.,5,"""We provide empirical results that show that these methods fail to generalize in very basic ways."" and ""We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons.""",2023,2023-06-30T23:44:51Z,"Keyphrase: ""Limited generalization ability""","""We provide empirical results that show that these methods fail to generalize in very basic ways."" and ""We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons."" Keyphrase: ""Limited generalization ability""",1
arXIv2023,Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models,Yes.,4,"""Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community.""",2023,2023-06-30T19:39:01Z,"Keyphrase: ""Perpetuation of stereotypes""","""Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community."" Keyphrase: ""Perpetuation of stereotypes""",3
arXIv2023,Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game,Yes.,4,"""Our results showed that ChatGPT can respond well to conceptual questions, but struggled with decimal place values and number line problems."" and ""We conclude with a discussion of ChatGPT's strengths and weaknesses and suggest several venues for extending its use cases in digital teaching and learning.""",2023,2023-06-29T02:28:09Z,"Keyphrase: ""Struggles with numerical precision""","""Our results showed that ChatGPT can respond well to conceptual questions, but struggled with decimal place values and number line problems."" and ""We conclude with a discussion of ChatGPT's strengths and weaknesses and suggest several venues for extending its use cases in digital teaching and learning."" Keyphrase: ""Struggles with numerical precision""",1
arXIv2023,A negation detection assessment of GPTs: analysis with the xNot360 dataset,Yes.,5,"""Our findings expose a considerable performance disparity among the GPT models, with GPT-4 surpassing its counterparts and GPT-3.5 displaying a marked performance reduction. The overall proficiency of the GPT models in negation detection remains relatively modest, indicating that this task pushes the boundaries of their natural language understanding capabilities. We not only highlight the constraints of GPT models in handling negation but also",2023,2023-06-29T02:27:48Z,"Keyphrase: ""Negation handling constraint""","""Our findings expose a considerable performance disparity among the GPT models, with GPT-4 surpassing its counterparts and GPT-3.5 displaying a marked performance reduction. The overall proficiency of the GPT models in negation detection remains relatively modest, indicating that this task pushes the boundaries of their natural language understanding capabilities. We not only highlight the constraints of GPT models in handling negation but also Keyphrase: ""Negation handling constraint""",7
arXIv2023,Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,Yes.,5,"""Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area."" and ""there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification.""",2023,2023-06-28T21:11:15Z,"Keyphrase: ""Lack of confidence calibration""","""Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area."" and ""there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification."" Keyphrase: ""Lack of confidence calibration""",0
arXIv2023,Towards Measuring the Representation of Subjective Global Opinions in Language Models,Yes.,4,"""By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases."" and ""When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily",2023,2023-06-28T17:31:53Z,"Keyphrase: ""Cultural bias in responses""","""By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases."" and ""When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily Keyphrase: ""Cultural bias in responses""",3
arXIv2023,CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models,Yes.,4,"""Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bias in certain categories.""",2023,2023-06-28T14:14:44Z,"Keyphrase: ""Strong bias in certain categories""","""Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bias in certain categories."" Keyphrase: ""Strong bias in certain categories""",3
arXIv2023,Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task,Yes.,4,"""harmful biases are likely increasingly intertwined with those models"" and ""Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.""",2023,2023-06-27T08:36:35Z,"Keyphrase: ""Entrenched harmful bias""","""harmful biases are likely increasingly intertwined with those models"" and ""Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage."" Keyphrase: ""Entrenched harmful bias""",3
arXIv2023,Are aligned neural networks adversarially aligned?,Yes.,5,"""However, adversarial users can construct inputs which circumvent attempts at alignment."" and ""We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image.""",2023,2023-06-26T17:18:44Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""However, adversarial users can construct inputs which circumvent attempts at alignment."" and ""We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arXIv2023,Exploring the Robustness of Large Language Models for Solving Programming Problems,Yes.,5,"""Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.""",2023,2023-06-26T10:48:50Z,"Keyphrase: ""Sensitivity to superficial modifications""","""Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance."" Keyphrase: ""Sensitivity to superficial modifications""",7
arXIv2023,Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning,Yes.,5,"""Language models still struggle on moral reasoning, despite their impressive performance in many other tasks."" and ""Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero",2023,2023-06-25T18:40:43Z,"Keyphrase: ""Limited moral reasoning""","""Language models still struggle on moral reasoning, despite their impressive performance in many other tasks."" and ""Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero Keyphrase: ""Limited moral reasoning""",1
arXIv2023,On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions,Yes.,4,"""When treating more general cases, despite the power of LLMs, inherent ambiguity exists and limits their predictive power. We then summarize the challenges and recommend research directions on LLMs to treat the inherent ambiguity of TTP descriptions used in various cyber operations.""",2023,2023-06-24T21:08:15Z,"Keyphrase: ""Inherent ambiguity and limited predictive power""","""When treating more general cases, despite the power of LLMs, inherent ambiguity exists and limits their predictive power. We then summarize the challenges and recommend research directions on LLMs to treat the inherent ambiguity of TTP descriptions used in various cyber operations."" Keyphrase: ""Inherent ambiguity and limited predictive power""",7
arXIv2023,Knowledge-Infused Self Attention Transformers,Yes.,5,"""These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users.""",2023,2023-06-23T13:55:01Z,"Keyphrase: ""Hallucination and incorrect output""","""These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users."" Keyphrase: ""Hallucination and incorrect output""",0
arXIv2023,ToolQA: A Dataset for LLM Question Answering with External Tools,Yes.,5,"""Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning.""",2023,2023-06-23T05:43:28Z,"Keyphrase: ""Weak numerical reasoning""","""Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning."" Keyphrase: ""Weak numerical reasoning""",1
arXIv2023,Visual Adversarial Examples Jailbreak Aligned Large Language Models,Yes.,5,"""First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs."" and ""we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification.""",2023,2023-06-22T22:13:03Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs."" and ""we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arXIv2023,Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs,Yes.,5,"""LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence."" and ""all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement.""",2023,2023-06-22T17:31:44Z,"Keyphrase: ""Overconfidence in verbalizing""","""LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence."" and ""all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement."" Keyphrase: ""Overconfidence in verbalizing""",1
arXIv2023,Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models,Yes.,5,"""Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment.""",2023,2023-06-22T03:56:38Z,"Keyphrase: ""Struggles with financial context""","""Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment."" Keyphrase: ""Struggles with financial context""",1
arXIv2023,Identifying and Extracting Rare Disease Phenotypes with Large Language Models,Yes.,4,"""While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.""",2023,2023-06-22T03:52:12Z,"Keyphrase: ""Limited critical evaluation""","""While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations."" Keyphrase: ""Limited critical evaluation""",1
arXIv2023,ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews,Yes.,5,"""We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than",2023,2023-06-21T22:00:03Z,"Keyphrase: ""Difficulty in addressing indirect feedback""","""We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than Keyphrase: ""Difficulty in addressing indirect feedback""",1
arXIv2023,Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases,Yes.,5,"""Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.""",2023,2023-06-21T21:04:11Z,"Keyphrase: ""Struggles with bias issues""","""Our findings demonstrate that current large language models struggle more with problems involving these three types of biases."" Keyphrase: ""Struggles with bias issues""",3
arXIv2023,Understanding Social Reasoning in Language Models with Language Models,Yes.,4,"""understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges",2023,2023-06-21T16:42:15Z,"Keyphrase: ""Limited theory of mind alignment""","""understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges Keyphrase: ""Limited theory of mind alignment""",1
arXIv2023,Solving and Generating NPR Sunday Puzzles with Large Language Models,Yes.,5,"""We find no evidence that models can generate puzzles",2023,2023-06-21T13:23:48Z,"Keyphrase: ""Limited puzzle-solving ability""","""We find no evidence that models can generate puzzles Keyphrase: ""Limited puzzle-solving ability""",1
arXIv2023,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Yes.,5,"""we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history.""",2023,2023-06-20T17:24:23Z,"Keyphrase: ""Vulnerability to toxic and biased output""","""we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history."" Keyphrase: ""Vulnerability to toxic and biased output""",2
arXIv2023,Hallucination is the last thing you need,Yes.,5,"""The present offering with generative AI presents major obstacles in replicating this, as current models struggle to integrate and navigate such a complex interplay of understanding, experience, and fact-checking procedures."" and ""this often deflects the model's attention from the crucial legal facts, thereby resulting in hallucination.""",2023,2023-06-20T13:14:15Z,"Keyphrase: ""Deflects attention from crucial facts""","""The present offering with generative AI presents major obstacles in replicating this, as current models struggle to integrate and navigate such a complex interplay of understanding, experience, and fact-checking procedures."" and ""this often deflects the model's attention from the crucial legal facts, thereby resulting in hallucination."" Keyphrase: ""Deflects attention from crucial facts""",0
arXIv2023,TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models,Yes.,4,"""It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs.""",2023,2023-06-20T12:53:39Z,"Keyphrase: ""Lack of comprehensive ethical analysis""","""It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs."" Keyphrase: ""Lack of comprehensive ethical analysis""",3
arXIv2023,Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,Yes.,5,"""they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents.""",2023,2023-06-20T12:21:06Z,"Keyphrase: ""Difficulty in recalling facts""","""they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents."" Keyphrase: ""Difficulty in recalling facts""",5
arXIv2023,Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts,Yes.,4,"""competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance.""",2023,2023-06-20T08:27:47Z,"Keyphrase: ""Underrepresented language performance disparity""","""competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance."" Keyphrase: ""Underrepresented language performance disparity""",7
arXIv2023,Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset,Yes.,5,"""Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm."" and ""we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected",2023,2023-06-19T21:14:57Z,"Keyphrase: ""Misleading distractions""","""Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm."" and ""we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected Keyphrase: ""Misleading distractions""",7
arXIv2023,RepoFusion: Training Code Models to Understand Your Repository,Yes.,5,"""Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions.""",2023,2023-06-19T15:05:31Z,"Keyphrase: ""Limited contextual understanding""","""Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions."" Keyphrase: ""Limited contextual understanding""",4
arXIv2023,ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation,Yes.,5,"""Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience.""",2023,2023-06-16T16:56:32Z,"Keyphrase: ""Limited real-world grounding""","""Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience."" Keyphrase: ""Limited real-world grounding""",7
arXIv2023,Friend or Foe? Exploring the Implications of Large Language Models on the Science System,Yes.,4,"""The study focused on applications and limitations of LLMs,"" and ""risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education.""",2023,2023-06-16T15:50:17Z,"Keyphrase: ""Risk of bias and misinformation""","""The study focused on applications and limitations of LLMs,"" and ""risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education."" Keyphrase: ""Risk of bias and misinformation""",3
arXIv2023,Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond,Yes.,5,"""However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered."" and ""Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection process and reasoning process.""",2023,2023-06-16T13:39:35Z,"Keyphrase: ""Limited logical reasoning capabilities""","""However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered."" and ""Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection process and reasoning process."" Keyphrase: ""Limited logical reasoning capabilities""",1
arXIv2023,Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody,Yes.,5,"""We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner.""",2023,2023-06-16T12:49:44Z,"Keyphrase: ""Underpredicting with longer context""","""We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner."" Keyphrase: ""Underpredicting with longer context""",5
arXIv2023,Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models,Yes.,4,"""We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models.""",2023,2023-06-16T10:36:18Z,"Keyphrase: ""Gender bias in language generation""","""We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models."" Keyphrase: ""Gender bias in language generation""",3
arXIv2023,Pushing the Limits of ChatGPT on NLP Tasks,Yes.,5,"""its subpar performance was caused by the following factors",2023,2023-06-16T09:40:05Z,"Keyphrase: ""Subpar performance""","""its subpar performance was caused by the following factors Keyphrase: ""Subpar performance""",7
arXIv2023,Clickbait Detection via Large Language Models,Yes.,5,"""Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods"" and ""the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.""",2023,2023-06-16T02:49:20Z,"Keyphrase: ""Limited performance compared to fine-tuned models""","""Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods"" and ""the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines."" Keyphrase: ""Limited performance compared to fine-tuned models""",5
arXIv2023,Explaining Legal Concepts with Augmented Large Language Models (GPT-4),Yes.,4,"""we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation"" and ""detailed analysis uncovered limitations in terms of the factual accuracy of the explanations.""",2023,2023-06-15T21:58:18Z,"Keyphrase: ""Limited factual accuracy and explanation""","""we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation"" and ""detailed analysis uncovered limitations in terms of the factual accuracy of the explanations."" Keyphrase: ""Limited factual accuracy and explanation""",0
arXIv2023,Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health,Yes.,4,"""We also find that the use of LLMs, like ChatGPT, in the fields of biomedicine and health entails various risks and challenges, including fabricated information in its generated responses, as well as legal and privacy concerns associated with sensitive patient data.""",2023,2023-06-15T20:19:08Z,"Keyphrase: ""Biased and risky responses""","""We also find that the use of LLMs, like ChatGPT, in the fields of biomedicine and health entails various risks and challenges, including fabricated information in its generated responses, as well as legal and privacy concerns associated with sensitive patient data."" Keyphrase: ""Biased and risky responses""",3
arXIv2023,Inverse Scaling: When Bigger Isn't Better,Yes.,5,"""Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data.""",2023,2023-06-15T20:11:23Z,"Keyphrase: ""Inverse scaling and worsened task performance""","""Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data."" Keyphrase: ""Inverse scaling and worsened task performance""",5
arXIv2023,"Explore, Establish, Exploit: Red Teaming Language Models from Scratch",Yes.,5,"""Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text."" and ""We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements.""",2023,2023-06-15T18:49:50Z,"Keyphrase: ""Hazardous output and false information""","""Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text."" and ""We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements."" Keyphrase: ""Hazardous output and false information""",2
arXIv2023,Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models,Yes.,4,"""However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains.""",2023,2023-06-15T17:42:48Z,"Keyphrase: ""Copyright infringement and harmful content""","""However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains."" Keyphrase: ""Copyright infringement and harmful content""",2
arXIv2023,SCALE: Scaling up the Complexity for Advanced Language Model Evaluation,Yes.,5,"""Despite recent advances, efficiently processing long documents for intense review/analysis tasks remains an open challenge for language models."" and ""existing publicly available models struggle with most tasks, even after in-domain pretraining.""",2023,2023-06-15T16:19:15Z,"Keyphrase: ""Struggle with processing long documents""","""Despite recent advances, efficiently processing long documents for intense review/analysis tasks remains an open challenge for language models."" and ""existing publicly available models struggle with most tasks, even after in-domain pretraining."" Keyphrase: ""Struggle with processing long documents""",4
arXIv2023,DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning,Yes.,5,"""large language models (LLMs) exhibit poor performance in tackling this subjective domain"" and ""current models defect in the application of pragmatic reasoning.""",2023,2023-06-15T10:41:23Z,"Keyphrase: ""Limited pragmatic reasoning""","""large language models (LLMs) exhibit poor performance in tackling this subjective domain"" and ""current models defect in the application of pragmatic reasoning."" Keyphrase: ""Limited pragmatic reasoning""",1
arXIv2023,FLamE: Few-shot Learning from Natural Language Explanations,Yes.,4,"""recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification"" and ""human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.""",2023,2023-06-13T18:01:46Z,"Keyphrase: ""Inadequate explanations""","""recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification"" and ""human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions."" Keyphrase: ""Inadequate explanations""",1
arXIv2023,Questioning the Survey Responses of Large Language Models,Yes.,5,"""models' responses are governed by ordering and labeling biases,"" and ""models' responses do not contain the entropy variations and statistical signals typically found in human populations.""",2023,2023-06-13T17:48:27Z,"Keyphrase: ""Labeling bias and statistical signal variation""","""models' responses are governed by ordering and labeling biases,"" and ""models' responses do not contain the entropy variations and statistical signals typically found in human populations."" Keyphrase: ""Labeling bias and statistical signal variation""",3
arXIv2023,Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control,Yes.,4,"""First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exempl",2023,2023-06-13T15:49:41Z,"Keyphrase: ""Limited context understanding""","""First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exempl Keyphrase: ""Limited context understanding""",4
arXIv2023,SqueezeLLM: Dense-and-Sparse Quantization,Yes.,5,"""However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements.""",2023,2023-06-13T08:57:54Z,"Keyphrase: ""Unprecedented resource requirement""","""However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements."" Keyphrase: ""Unprecedented resource requirement""",4
arXIv2023,Large Language Models Sometimes Generate Purely Negatively-Reinforced Text,Yes.,5,"""One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong",2023,2023-06-13T06:40:37Z,"Keyphrase: ""Limited understanding of reward signals""","""One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong Keyphrase: ""Limited understanding of reward signals""",5
arXIv2023,Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling,Yes.,5,"""LLMs fail at simple linguistic tests for negation or quantifier understanding"" and ""suggests that LLMs do not do as well as expected with quantifiers.""",2023,2023-06-12T19:20:18Z,"Keyphrase: ""Weak quantifier understanding""","""LLMs fail at simple linguistic tests for negation or quantifier understanding"" and ""suggests that LLMs do not do as well as expected with quantifiers."" Keyphrase: ""Weak quantifier understanding""",1
arXIv2023,Lost in Translation: Large Language Models in Non-English Content Analysis,Yes.,5,"""the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages."" and ""Part II accounts for the challenges of doing content analysis with large language models in general and multilingual language models in particular.""",2023,2023-06-12T19:10:47Z,"Keyphrase: ""Limited multilingual capabilities""","""the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages."" and ""Part II accounts for the challenges of doing content analysis with large language models in general and multilingual language models in particular."" Keyphrase: ""Limited multilingual capabilities""",6
arXIv2023,TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models,Yes.,4,"""the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined."" and ""Our work sheds light on the potential security risks in current models and offers a potential defensive approach.""",2023,2023-06-12T01:22:39Z,"Keyphrase: ""Insufficiently examined security implications""","""the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined."" and ""Our work sheds light on the potential security risks in current models and offers a potential defensive approach."" Keyphrase: ""Insufficiently examined security implications""",2
arXIv2023,Human-in-the-Loop through Chain-of-Thought,Yes.,5,"""it sometimes demonstrates its weakness in long-term or multi-step logical reasoning.""",2023,2023-06-10T04:31:57Z,"Keyphrase: ""Weak long-term logical reasoning""","""it sometimes demonstrates its weakness in long-term or multi-step logical reasoning."" Keyphrase: ""Weak long-term logical reasoning""",1
arXIv2023,Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording,Yes.,5,"""LLMs are still not reliable,"" ""no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available,"" and ""The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability",2023,2023-06-09T19:07:31Z,"Keyphrase: ""Inconsistent responses""","""LLMs are still not reliable,"" ""no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available,"" and ""The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability Keyphrase: ""Inconsistent responses""",0
arXIv2023,Trapping LLM Hallucinations Using Tagged Context Prompts,Yes.,5,"""However, these models suffer from 'hallucinations,' where the model generates false or fabricated information.""",2023,2023-06-09T17:48:54Z,"Keyphrase: ""Hallucination of false information""","""However, these models suffer from 'hallucinations,' where the model generates false or fabricated information."" Keyphrase: ""Hallucination of false information""",0
arXIv2023,S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput,Yes.,5,"""Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself.""",2023,2023-06-09T16:13:43Z,"Keyphrase: ""Memory consumption scalability""","""Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself."" Keyphrase: ""Memory consumption scalability""",4
arXIv2023,Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?,Yes.,5,"""vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text.""",2023,2023-06-09T13:03:53Z,"Keyphrase: ""Vulnerability to adversarial text""","""vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text."" Keyphrase: ""Vulnerability to adversarial text""",2
arXIv2023,Can Large Language Models Infer Causation from Correlation?,Yes.,5,"""Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but",2023,2023-06-09T12:09:15Z,"Keyphrase: ""Limited causal inference skills""","""Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but Keyphrase: ""Limited causal inference skills""",1
arXIv2023,Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests,Yes.,4,"""At the same time, the results highlight the unreliability of LLMs",2023,2023-06-09T07:19:43Z,"Keyphrase: ""Unreliability""","""At the same time, the results highlight the unreliability of LLMs Keyphrase: ""Unreliability""",0
arXIv2023,Prompt Injection attack against LLM-integrated Applications,Yes.,5,"""highlighting the constraints of current attack strategies in practice"" and ""We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection.""",2023,2023-06-08T18:43:11Z,"Keyphrase: ""Vulnerability to prompt injection""","""highlighting the constraints of current attack strategies in practice"" and ""We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection."" Keyphrase: ""Vulnerability to prompt injection""",2
arXIv2023,Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures,Yes.,4,"""However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly.""",2023,2023-06-08T13:10:00Z,"Keyphrase: ""Difficulty with complex tasks and ambiguity""","""However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly."" Keyphrase: ""Difficulty with complex tasks and ambiguity""",1
arXIv2023,Soft-prompt Tuning for Large Language Models to Evaluate Bias,Yes.,4,"""Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues.""",2023,2023-06-07T19:11:25Z,"Keyphrase: ""Bias towards certain groups""","""Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues."" Keyphrase: ""Bias towards certain groups""",3
arXIv2023,ModuleFormer: Modularity Emerges from Mixture-of-Experts,Yes.,5,"""existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge.""",2023,2023-06-07T17:59:57Z,"Keyphrase: ""Difficulty in knowledge expansion""","""existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge."" Keyphrase: ""Difficulty in knowledge expansion""",7
arXIv2023,Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,Yes.,4,"""Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.""",2023,2023-06-07T16:50:03Z,"Keyphrase: ""Propagating societal bias""","""Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people."" Keyphrase: ""Propagating societal bias""",3
arXIv2023,"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",Yes.,5,"""ChatGPT has not solved computational humor yet but it can be a big leap toward 'funny' machines.""",2023,2023-06-07T16:10:21Z,"Keyphrase: ""Limited humor understanding""","""ChatGPT has not solved computational humor yet but it can be a big leap toward 'funny' machines."" Keyphrase: ""Limited humor understanding""",6
arXIv2023,PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts,Yes.,5,"""Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts.""",2023,2023-06-07T15:37:00Z,"Keyphrase: ""Robustness to adversarial prompts""","""Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts."" Keyphrase: ""Robustness to adversarial prompts""",2
arXIv2023,Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering,Yes.,5,"""However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.""",2023,2023-06-07T04:15:21Z,"Keyphrase: ""Insufficient internalized knowledge""","""However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive."" Keyphrase: ""Insufficient internalized knowledge""",0
arXIv2023,An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models,Yes.,4,"""The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases."" and ""are less effective when it comes to racial and religious bias, which may be attributed to",2023,2023-06-06T23:56:18Z,"Keyphrase: ""Inherent humanlike biases""","""The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases."" and ""are less effective when it comes to racial and religious bias, which may be attributed to Keyphrase: ""Inherent humanlike biases""",3
arXIv2023,Certified Deductive Reasoning with Language Models,Yes.,5,"""Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, even when arriving at a correct final answer, their rationales are often logically unsound or inconsistent.""",2023,2023-06-06T21:49:00Z,"Keyphrase: ""Illogical reasoning""","""Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, even when arriving at a correct final answer, their rationales are often logically unsound or inconsistent."" Keyphrase: ""Illogical reasoning""",1
arXIv2023,MISGENDERED: Limits of Large Language Models in Understanding Pronouns,Yes.,5,"""We comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns."" and ""When prompted out-of-the-box, language models perform poorly at correctly predicting neo",2023,2023-06-06T18:27:52Z,"Keyphrase: ""Difficulty predicting gender-neutral pronouns""","""We comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns."" and ""When prompted out-of-the-box, language models perform poorly at correctly predicting neo Keyphrase: ""Difficulty predicting gender-neutral pronouns""",3
arXIv2023,ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory,Yes.,5,"""mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning.""",2023,2023-06-06T17:58:24Z,"Keyphrase: ""Limited memory capacity""","""mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning."" Keyphrase: ""Limited memory capacity""",1
arXIv2023,Deductive Verification of Chain-of-Thought Reasoning,Yes.,4,"""its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks.""",2023,2023-06-06T17:18:56Z,"Keyphrase: ""Limited complex reasoning ability""","""its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks."" Keyphrase: ""Limited complex reasoning ability""",0
arXIv2023,Can large language models democratize access to dual-use biotechnology?,Yes.,5,"""However, these models may also confer easy access to dual-use technologies capable of inflicting great harm."" and ""Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training.""",2023,2023-06-06T15:52:05Z,"Keyphrase: ""Potential for misuse""","""However, these models may also confer easy access to dual-use technologies capable of inflicting great harm."" and ""Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training."" Keyphrase: ""Potential for misuse""",2
arXIv2023,Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach,Yes.,5,"""interactions with LLMs can be time-consuming. In many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency.""",2023,2023-06-06T11:49:09Z,"Keyphrase: ""Resource-intensive deployment""","""interactions with LLMs can be time-consuming. In many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency."" Keyphrase: ""Resource-intensive deployment""",4
arXIv2023,Large Language Models of Code Fail at Completing Code with Potential Bugs,Yes.,5,"""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs.""",2023,2023-06-06T06:35:27Z,"Keyphrase: ""Bug-induced performance degradation""","""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs."" Keyphrase: ""Bug-induced performance degradation""",7
arXIv2023,A Static Evaluation of Code Completion by Large Language Models,Yes.,4,"""Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models.""",2023,2023-06-05T19:23:34Z,"Keyphrase: ""Common coding errors""","""Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models."" Keyphrase: ""Common coding errors""",7
arXIv2023,Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs,Yes.,5,"""Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone.""",2023,2023-06-05T17:55:05Z,"Keyphrase: ""Difficulty in fine-tuning and controlling""","""Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone."" Keyphrase: ""Difficulty in fine-tuning and controlling""",5
arXIv2023,Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset,Yes.,5,"""The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could",2023,2023-06-05T16:48:41Z,"Keyphrase: ""Disparity with human accuracy""","""The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could Keyphrase: ""Disparity with human accuracy""",7
arXIv2023,Exposing Bias in Online Communities through Large-Scale Language Models,Yes.,4,"""While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes."" and ""This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets",2023,2023-06-04T08:09:26Z,"Keyphrase: ""Reproduction of social bias""","""While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes."" and ""This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets Keyphrase: ""Reproduction of social bias""",3
arXIv2023,Probing Physical Reasoning with Counter-Commonsense Context,Yes.,5,"""The results show that while large language models can use prepositions such as 'in' and 'into' in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.""",2023,2023-06-04T04:24:43Z,"Keyphrase: ""Limited inferencing capabilities""","""The results show that while large language models can use prepositions such as 'in' and 'into' in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense."" Keyphrase: ""Limited inferencing capabilities""",1
arXIv2023,AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap,Yes.,4,"""a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs"" and ""We reflect on the unique challenges that arise in providing transparency for LLMs.""",2023,2023-06-02T22:51:26Z,"Keyphrase: ""Lack of transparency""","""a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs"" and ""We reflect on the unique challenges that arise in providing transparency for LLMs."" Keyphrase: ""Lack of transparency""",2
arXIv2023,Knowledge of cultural moral norms in large language models,Yes.,4,"""We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms.""",2023,2023-06-02T18:23:35Z,"Keyphrase: ""Cross-cultural moral norm prediction disparity""","""We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms."" Keyphrase: ""Cross-cultural moral norm prediction disparity""",3
arXIv2023,"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",Yes.,5,"""we discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis.""",2023,2023-06-02T12:47:45Z,"Keyphrase: ""Limitation in dementia diagnosis""","""we discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis."" Keyphrase: ""Limitation in dementia diagnosis""",6
arXIv2023,ChatGPT is a Remarkable Tool -- For Experts,Yes.,5,"""These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation.""",2023,2023-06-02T06:28:21Z,"Keyphrase: ""Limited logical reasoning and ethical concerns""","""These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation."" Keyphrase: ""Limited logical reasoning and ethical concerns""",1
arXIv2023,How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?,Yes.,5,"""we check for inconsistencies and hallucinations in the summaries"" and ""we often find inconsistent or hallucinated information in the generated abstractive summaries"" and ""our investigation indicates that the pre-trained abstractive summarization models and LLMs are not yet ready for fully automatic deployment for case judgement summarization; rather a human-in-the-loop approach including manual checks for inconsistencies is more",2023,2023-06-02T03:16:19Z,"Keyphrase: ""Inconsistent hallucination""","""we check for inconsistencies and hallucinations in the summaries"" and ""we often find inconsistent or hallucinated information in the generated abstractive summaries"" and ""our investigation indicates that the pre-trained abstractive summarization models and LLMs are not yet ready for fully automatic deployment for case judgement summarization; rather a human-in-the-loop approach including manual checks for inconsistencies is more Keyphrase: ""Inconsistent hallucination""",0
arXIv2023,Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study,Yes.,4,"""a closer examination of the texts generated by ChatGPT through human evaluations has shown that there are still critical issues in terms of text coherence, faithfulness, and style.""",2023,2023-06-01T21:58:33Z,"Keyphrase: ""Text coherence and faithfulness""","""a closer examination of the texts generated by ChatGPT through human evaluations has shown that there are still critical issues in terms of text coherence, faithfulness, and style."" Keyphrase: ""Text coherence and faithfulness""",6
arXIv2023,Exposing Attention Glitches with Flip-Flop Language Modeling,Yes.,5,"""The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought."" and ""We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors,",2023,2023-06-01T17:44:35Z,"Keyphrase: ""Brittleness in long chain reasoning""","""The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought."" and ""We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, Keyphrase: ""Brittleness in long chain reasoning""",1
arXIv2023,Let's Verify Step by Step,Yes.,5,"""However, even state-of-the-art models still regularly produce logical mistakes.""",2023,2023-05-31T17:24:00Z,"Keyphrase: ""Logical errors""","""However, even state-of-the-art models still regularly produce logical mistakes."" Keyphrase: ""Logical errors""",1
arXIv2023,Red Teaming Language Model Detectors with Language Models,Yes.,4,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems.""",2023,2023-05-31T10:08:37Z,"Keyphrase: ""Safety and ethical risks""","""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems."" Keyphrase: ""Safety and ethical risks""",2
arXIv2023,Large Language Models Are Not Strong Abstract Reasoners,Yes.,5,"""However, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed."" and ""We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks.""",2023,2023-05-31T04:50:29Z,"Keyphrase: ""Opaque cognitive capability""","""However, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed."" and ""We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks."" Keyphrase: ""Opaque cognitive capability""",1
arXIv2023,The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code,Yes.,4,"""it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning.""",2023,2023-05-30T17:02:58Z,"Keyphrase: ""Limited causal reasoning capabilities""","""it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning."" Keyphrase: ""Limited causal reasoning capabilities""",1
arXIv2023,Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate,Yes.,5,"""Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks,"" and ""our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem",2023,2023-05-30T15:25:45Z,"Keyphrase: ""Struggles with complex reasoning""","""Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks,"" and ""our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem Keyphrase: ""Struggles with complex reasoning""",1
arXIv2023,"Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard",Yes.,5,"""However, for more complex mathematical problems or advanced logic tasks, their answers, although written in a usually 'convincing' way, may not be reliable. Consistency is also an issue, as many times a chatbot will provide conflicting answers when given the same question more than once.""",2023,2023-05-30T11:18:05Z,"Keyphrase: ""Inconsistent responses""","""However, for more complex mathematical problems or advanced logic tasks, their answers, although written in a usually 'convincing' way, may not be reliable. Consistency is also an issue, as many times a chatbot will provide conflicting answers when given the same question more than once."" Keyphrase: ""Inconsistent responses""",1
arXIv2023,Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge,Yes.,4,"""these methods suffer from low knowledge coverage caused by PLM bias -- the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality -- only models using GPT-3 can achieve the best result.""",2023,2023-05-30T08:34:13Z,"Keyphrase: ""Low knowledge coverage""","""these methods suffer from low knowledge coverage caused by PLM bias -- the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality -- only models using GPT-3 can achieve the best result."" Keyphrase: ""Low knowledge coverage""",5
arXIv2023,Universality and Limitations of Prompt Tuning,Yes.,5,"""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer.""",2023,2023-05-30T06:47:07Z,"Keyphrase: ""Limited prompt tuning depth""","""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer."" Keyphrase: ""Limited prompt tuning depth""",4
arXIv2023,Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey,Yes.,4,"""However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications).""",2023,2023-05-30T03:00:30Z,"Keyphrase: ""Domain heterogeneity and complexity""","""However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications)."" Keyphrase: ""Domain heterogeneity and complexity""",7
arXIv2023,Faith and Fate: Limits of Transformers on Compositionality,Yes.,5,"""Yet, these models simultaneously show failures on surprisingly trivial problems."" and ""Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.""",2023,2023-05-29T23:24:14Z,"Keyphrase: ""Limited compositional reasoning""","""Yet, these models simultaneously show failures on surprisingly trivial problems."" and ""Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills."" Keyphrase: ""Limited compositional reasoning""",1
arXIv2023,How Effective Are Neural Networks for Fixing Security Vulnerabilities,Yes.,5,"""Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to",2023,2023-05-29T20:50:27Z,"Keyphrase: ""Limited vulnerability-fixing capability""","""Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to Keyphrase: ""Limited vulnerability-fixing capability""",2
arXIv2023,Do Language Models Know When They're Hallucinating References?,Yes.,5,"""State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda.""",2023,2023-05-29T17:12:03Z,"Keyphrase: ""Hallucinated information and inaccurate output""","""State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda."" Keyphrase: ""Hallucinated information and inaccurate output""",0
arXIv2023,Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models,Yes.,5,"""To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs."" and ""We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts.""",2023,2023-05-29T16:29:22Z,"Keyphrase: ""Higher rate of racial stereotypes""","""To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs."" and ""We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts."" Keyphrase: ""Higher rate of racial stereotypes""",3
arXIv2023,Do Large Language Models Know What They Don't Know?,Yes.,5,"""Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend."" and ""Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.""",2023,2023-05-29T15:30:13Z,"Keyphrase: ""Limited comprehension and knowledge gap""","""Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend."" and ""Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge."" Keyphrase: ""Limited comprehension and knowledge gap""",1
arXIv2023,"Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations",Yes.,4,"""Subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and LOL",2023,2023-05-29T12:26:44Z,"Keyphrase: ""Security vulnerabilities""","""Subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and LOL Keyphrase: ""Security vulnerabilities""",2
arXIv2023,Large Language Models are not Fair Evaluators,Yes.,5,"""we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models.""",2023,2023-05-29T07:41:03Z,"Keyphrase: ""Biased evaluation paradigms""","""we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models."" Keyphrase: ""Biased evaluation paradigms""",3
arXIv2023,Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective,Yes.,4,"""We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework.""",2023,2023-05-28T16:04:48Z,"Keyphrase: ""Limited incorporation of human feedback""","""We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework."" Keyphrase: ""Limited incorporation of human feedback""",0
arXIv2023,KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application,Yes.,4,"""Large language models (LLMs) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications.""",2023,2023-05-28T12:07:16Z,"Keyphrase: ""Social bias and demographic disparities""","""Large language models (LLMs) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications."" Keyphrase: ""Social bias and demographic disparities""",3
arXIv2023,SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration,Yes.,4,"""The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising.""",2023,2023-05-28T11:51:20Z,"Keyphrase: ""Generating offensive and biased content""","""The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising."" Keyphrase: ""Generating offensive and biased content""",2
arXIv2023,Evaluating GPT-3 Generated Explanations for Hateful Content Moderation,Yes.,5,"""A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators."" and ""this persuasiveness may result in incorrect judgments about the hatefulness of the content.""",2023,2023-05-28T10:05:13Z,"Keyphrase: ""Erroneous explanations""","""A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators."" and ""this persuasiveness may result in incorrect judgments about the hatefulness of the content."" Keyphrase: ""Erroneous explanations""",2
arXIv2023,Reward Collapse in Aligning Large Language Models,Yes.,5,"""we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training.""",2023,2023-05-28T02:12:00Z,"Keyphrase: ""Reward collapse""","""we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training."" Keyphrase: ""Reward collapse""",5
arXIv2023,Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,Yes.,5,"""Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks.""",2023,2023-05-27T19:08:04Z,"Keyphrase: ""Unwanted side effects in editing technique""","""Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks."" Keyphrase: ""Unwanted side effects in editing technique""",2
arXIv2023,The Curse of Recursion: Training on Generated Data Makes Models Forget,Yes.,5,"""We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs.""",2023,2023-05-27T15:10:41Z,"Keyphrase: ""Catastrophic forgetting""","""We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs."" Keyphrase: ""Catastrophic forgetting""",5
arXIv2023,FERMAT: An Alternative to Accuracy for Numerical Reasoning,Yes.,5,"""While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning.""",2023,2023-05-27T15:00:45Z,"Keyphrase: ""Struggles with numerical reasoning""","""While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning."" Keyphrase: ""Struggles with numerical reasoning""",1
arXIv2023,Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning,Yes.,5,"""Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited."" and ""Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are 'lazy learners' that tend to exploit shortcuts in prompts for downstream tasks.""",2023,2023-05-26T20:56:30Z,"Keyphrase: ""Lazy learner tendency""","""Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited."" and ""Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are 'lazy learners' that tend to exploit shortcuts in prompts for downstream tasks."" Keyphrase: ""Lazy learner tendency""",5
arXIv2023,Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model,Yes.,5,"""Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model and Prometheus. Hallucinations were mostly associated with non-existent references or fabricated efficacy data.""",2023,2023-05-26T17:33:05Z,"Keyphrase: ""Hallucination of nonexistent references""","""Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model and Prometheus. Hallucinations were mostly associated with non-existent references or fabricated efficacy data."" Keyphrase: ""Hallucination of nonexistent references""",0
arXIv2023,"LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",Yes.,5,"""GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly",2023,2023-05-26T16:32:17Z,"Keyphrase: ""Limited reasoning capacity""","""GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly Keyphrase: ""Limited reasoning capacity""",1
arXIv2023,On Evaluating Adversarial Robustness of Large Vision-Language Models,Yes.,4,"""multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision)"" and ""Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice.""",2023,2023-05-26T13:49:44Z,"Keyphrase: ""Adversarial vulnerability in multimodal generation""","""multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision)"" and ""Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice."" Keyphrase: ""Adversarial vulnerability in multimodal generation""",2
arXIv2023,A Closer Look at In-Context Learning under Distribution Shifts,Yes.,5,"""The key question we aim to address is",2023,2023-05-26T07:47:21Z,"Keyphrase: ""Lack of contextual understanding""","""The key question we aim to address is Keyphrase: ""Lack of contextual understanding""",5
arXIv2023,AdaPlanner: Adaptive Planning from Feedback with Language Models,Yes.,5,"""most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase.""",2023,2023-05-26T05:52:27Z,"Keyphrase: ""Limited adaptability and planning""","""most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase."" Keyphrase: ""Limited adaptability and planning""",7
arXIv2023,Type Prediction With Program Decomposition and Fill-in-the-Type Training,Yes.,5,"""Large language models (LLMs) are promising for type prediction, but there are challenges",2023,2023-05-25T21:16:09Z,"Keyphrase: ""Limited predictive challenge capabilities""","""Large language models (LLMs) are promising for type prediction, but there are challenges Keyphrase: ""Limited predictive challenge capabilities""",6
arXIv2023,Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models,Yes.,5,"""We find that despite capturing some aspects of logical meaning, the models fall far short of human performance.""",2023,2023-05-25T18:56:26Z,"Keyphrase: ""Limited logical reasoning""","""We find that despite capturing some aspects of logical meaning, the models fall far short of human performance."" Keyphrase: ""Limited logical reasoning""",1
arXIv2023,Landmark Attention: Random-Access Infinite Context Length for Transformers,Yes.,5,"""their attention mechanism's large memory requirements have limited their ability to handle longer contexts.""",2023,2023-05-25T17:53:42Z,"Keyphrase: ""Limited long-context handling""","""their attention mechanism's large memory requirements have limited their ability to handle longer contexts."" Keyphrase: ""Limited long-context handling""",4
arXIv2023,Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots,Yes.,5,"""there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential,"" ""The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential,"" and ""What ChatGPT lacks is a stochastic measure",2023,2023-05-25T17:35:57Z,"Keyphrase: ""Inaccurate data generation""","""there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential,"" ""The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential,"" and ""What ChatGPT lacks is a stochastic measure Keyphrase: ""Inaccurate data generation""",0
arXIv2023,"Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",Yes.,5,"""Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context.""",2023,2023-05-25T08:43:46Z,"Keyphrase: ""Hallucinated content""","""Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context."" Keyphrase: ""Hallucinated content""",0
arXIv2023,Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,Yes.,5,"""Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost.""",2023,2023-05-25T07:39:41Z,"Keyphrase: ""Quadratic computational cost""","""Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost."" Keyphrase: ""Quadratic computational cost""",4
arXIv2023,On the Planning Abilities of Large Language Models : A Critical Investigation,Yes.,5,"""Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains.""",2023,2023-05-25T06:32:23Z,"Keyphrase: ""Limited autonomous planning ability""","""Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains."" Keyphrase: ""Limited autonomous planning ability""",7
arXIv2023,Asking Before Action: Gather Information in Embodied Decision Making with Language Models,Yes.,5,"""However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance.""",2023,2023-05-25T04:05:08Z,"Keyphrase: ""Struggles in unfamiliar environments""","""However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance."" Keyphrase: ""Struggles in unfamiliar environments""",1
arXIv2023,Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models,Yes.,4,"""the sensitivity of data contained in prompts raises privacy concerns"" and ""we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs.""",2023,2023-05-24T22:06:08Z,"Keyphrase: ""Privacy vulnerabilities due to prompt data sensitivity""","""the sensitivity of data contained in prompts raises privacy concerns"" and ""we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs."" Keyphrase: ""Privacy vulnerabilities due to prompt data sensitivity""",2
arXIv2023,"The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",Yes.,5,"""LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.""",2023,2023-05-24T18:54:39Z,"Keyphrase: ""Lack of deep abstract understanding""","""LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability."" Keyphrase: ""Lack of deep abstract understanding""",1
arXIv2023,Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing,Yes.,5,"""However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content."" and ""we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art LLMs.""",2023,2023-05-24T16:50:36Z,"Keyphrase: ""Content hallucination""","""However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content."" and ""we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art LLMs."" Keyphrase: ""Content hallucination""",0
arXIv2023,Gorilla: Large Language Model Connected with Massive APIs,Yes.,5,"""However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call.""",2023,2023-05-24T16:48:11Z,"Keyphrase: ""Inaccurate input generation""","""However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call."" Keyphrase: ""Inaccurate input generation""",0
arXIv2023,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,Yes.,5,"""Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world.""",2023,2023-05-24T16:17:36Z,"Keyphrase: ""Outdated knowledge and hallucination""","""Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world."" Keyphrase: ""Outdated knowledge and hallucination""",0
arXIv2023,Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples,Yes.,5,"""Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.""",2023,2023-05-24T15:55:51Z,"Keyphrase: ""Difficulty in generalizing to longer proofs""","""Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction."" Keyphrase: ""Difficulty in generalizing to longer proofs""",1
arXIv2023,Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,Yes.,5,"""We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models,"" and ""PCW would present unexpected deterioration regarding question miscomprehension and false inference.""",2023,2023-05-24T15:48:29Z,"Keyphrase: ""Question miscomprehension and false inference""","""We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models,"" and ""PCW would present unexpected deterioration regarding question miscomprehension and false inference."" Keyphrase: ""Question miscomprehension and false inference""",4
arXIv2023,Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models,Yes.,5,"""The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts.""",2023,2023-05-24T11:55:59Z,"Keyphrase: ""Difficulty with abstract concepts""","""The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts."" Keyphrase: ""Difficulty with abstract concepts""",1
arXIv2023,GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking,Yes.,5,"""we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities.""",2023,2023-05-24T11:53:19Z,"Keyphrase: ""Limitation in graph comprehension""","""we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities."" Keyphrase: ""Limitation in graph comprehension""",1
arXIv2023,A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event Extraction,Yes.,4,"""Unfortunately, we find that current zero-shot EE methods perform poorly for the task, with issues including word sense ambiguity, modality mismatch, and efficiency. Straightforward application of large language model prompting typically performs even worse.""",2023,2023-05-24T11:41:33Z,"Keyphrase: ""Poor zero-shot performance""","""Unfortunately, we find that current zero-shot EE methods perform poorly for the task, with issues including word sense ambiguity, modality mismatch, and efficiency. Straightforward application of large language model prompting typically performs even worse."" Keyphrase: ""Poor zero-shot performance""",6
arXIv2023,Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems,Yes.,5,"""Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation.""",2023,2023-05-24T10:58:20Z,"Keyphrase: ""Factual errors in arithmetic tasks""","""Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation."" Keyphrase: ""Factual errors in arithmetic tasks""",0
arXIv2023,"RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",Yes.,5,"""they all suffer from generating untruthful dialogues because of the model hallucination.""",2023,2023-05-24T10:30:42Z,"Keyphrase: ""Untruthful dialogue hallucination""","""they all suffer from generating untruthful dialogues because of the model hallucination."" Keyphrase: ""Untruthful dialogue hallucination""",0
arXIv2023,Reasoning with Language Model is Planning with World Model,Yes.,5,"""However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g.,",2023,2023-05-24T10:28:28Z,"Keyphrase: ""Lack of internal world model""","""However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., Keyphrase: ""Lack of internal world model""",1
arXIv2023,GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP,Yes.,5,"""Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic."" and ""unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA."" and ""our work adds to a growing body of",2023,2023-05-24T10:12:39Z,"Keyphrase: ""Limited performance in handling Arabic dialects""","""Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic."" and ""unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA."" and ""our work adds to a growing body of Keyphrase: ""Limited performance in handling Arabic dialects""",6
arXIv2023,"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",Yes.,5,"""non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies.""",2023,2023-05-24T09:57:37Z,"Keyphrase: ""Degenerate output behavior""","""non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies."" Keyphrase: ""Degenerate output behavior""",2
arXIv2023,Adversarial Demonstration Attacks on Large Language Models,Yes.,5,"""We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.""",2023,2023-05-24T09:40:56Z,"Keyphrase: ""Critical security risk""","""We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs."" Keyphrase: ""Critical security risk""",2
arXIv2023,In-Context Impersonation Reveals Large Language Models' Strengths and Biases,Yes.,4,"""However, impersonation can also uncover LLMs' biases",2023,2023-05-24T09:13:15Z,"Keyphrase: ""Bias uncovered through impersonation""","""However, impersonation can also uncover LLMs' biases Keyphrase: ""Bias uncovered through impersonation""",3
arXIv2023,Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,Yes.,5,"""methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback.""",2023,2023-05-24T08:59:15Z,"Keyphrase: ""Limited practical application due to reliance on feedback""","""methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback."" Keyphrase: ""Limited practical application due to reliance on feedback""",1
arXIv2023,PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions,Yes.,5,"""The remarkable capabilities of large language models have been accompanied by a persistent drawback",2023,2023-05-24T08:59:00Z,"Keyphrase: ""Persistent drawbacks""","""The remarkable capabilities of large language models have been accompanied by a persistent drawback Keyphrase: ""Persistent drawbacks""",6
arXIv2023,"M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",Yes.,4,"""Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written.""",2023,2023-05-24T08:55:11Z,"Keyphrase: ""Poor generalization to unseen domains""","""Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written."" Keyphrase: ""Poor generalization to unseen domains""",5
arXIv2023,BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer,Yes.,5,"""Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. In particular, ChatGPT with in-context learning often performs worse than much smaller mT5-base models fine-tuned on English task data and few-shot in-language examples.""",2023,2023-05-24T08:06:33Z,"Keyphrase: ""Limited few-shot and cross-lingual capabilities""","""Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. In particular, ChatGPT with in-context learning often performs worse than much smaller mT5-base models fine-tuned on English task data and few-shot in-language examples."" Keyphrase: ""Limited few-shot and cross-lingual capabilities""",6
arXIv2023,SummIt: Iterative Text Summarization via ChatGPT,Yes.,4,"""the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests."" and ""identify a potential issue of over-correction.""",2023,2023-05-24T07:40:06Z,"Keyphrase: ""Hallucination and overcorrection""","""the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests."" and ""identify a potential issue of over-correction."" Keyphrase: ""Hallucination and overcorrection""",0
arXIv2023,Mitigating Temporal Misalignment by Discarding Outdated Facts,Yes.,5,"""While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having",2023,2023-05-24T07:30:08Z,"Keyphrase: ""Outdated world knowledge""","""While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having Keyphrase: ""Outdated world knowledge""",5
arXIv2023,MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,Yes.,5,"""The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. ... While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions.""",2023,2023-05-24T06:48:41Z,"Keyphrase: ""Rapid decay of information""","""The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. ... While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions."" Keyphrase: ""Rapid decay of information""",5
arXIv2023,Adapting Language Models to Compress Contexts,Yes.,5,"""Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.""",2023,2023-05-24T06:42:44Z,"Keyphrase: ""Limited context window""","""Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents."" Keyphrase: ""Limited context window""",4
arXIv2023,Anthropomorphization of AI: Opportunities and Risks,Yes.,4,"""We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction,",2023,2023-05-24T06:39:45Z,"Keyphrase: ""Anthropomorphization concerns""","""We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction, Keyphrase: ""Anthropomorphization concerns""",8
arXIv2023,Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,Yes.,4,"""These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.""",2023,2023-05-24T04:27:21Z,"Keyphrase: ""Vulnerability to poisoning attacks""","""These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing."" Keyphrase: ""Vulnerability to poisoning attacks""",2
arXIv2023,A Causal View of Entity Bias in (Large) Language Models,Yes.,4,"""Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions."" and ""The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits.""",2023,2023-05-24T03:59:18Z,"Keyphrase: ""Entity bias and unfaithful predictions""","""Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions."" and ""The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits."" Keyphrase: ""Entity bias and unfaithful predictions""",3
arXIv2023,Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response,Yes.,5,"""Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs."" and ""Empirical results show that the ability of LLMs to identify unreasonable responses is insufficient. There are risks in using reference-free evaluators based on LLMs to evaluate the quality of",2023,2023-05-24T02:52:48Z,"Keyphrase: ""Challenges with identifying unreasonable responses""","""Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs."" and ""Empirical results show that the ability of LLMs to identify unreasonable responses is insufficient. There are risks in using reference-free evaluators based on LLMs to evaluate the quality of Keyphrase: ""Challenges with identifying unreasonable responses""",0
arXIv2023,Enabling Large Language Models to Generate Text with Citations,Yes.,5,"""Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination."" and ""current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time.""",2023,2023-05-24T01:53:49Z,"Keyphrase: ""Hallucination and lack of complete citation support""","""Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination."" and ""current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time."" Keyphrase: ""Hallucination and lack of complete citation support""",0
arXIv2023,Think Before You Act: Decision Transformers with Internal Working Memory,Yes.,5,"""However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks.""",2023,2023-05-24T01:20:22Z,"Keyphrase: ""Forgetting phenomenon""","""However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks."" Keyphrase: ""Forgetting phenomenon""",5
arXIv2023,ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers,Yes.,4,"""Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification.""",2023,2023-05-24T00:10:15Z,"Keyphrase: ""Lack of guaranteed correctness""","""Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification."" Keyphrase: ""Lack of guaranteed correctness""",7
arXIv2023,Sources of Hallucination by Large Language Models on Inference Tasks,Yes.,5,"""We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs.""",2023,2023-05-23T22:24:44Z,"Keyphrase: ""Bias and hallucination issues""","""We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs."" Keyphrase: ""Bias and hallucination issues""",0
arXIv2023,LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond,Yes.,5,"""a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision"" and ""Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4,",2023,2023-05-23T21:50:06Z,"Keyphrase: ""Struggles with complex tasks""","""a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision"" and ""Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, Keyphrase: ""Struggles with complex tasks""",7
arXIv2023,MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems,Yes.,5,"""While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early.""",2023,2023-05-23T21:44:56Z,"Keyphrase: ""Inaccurate tutoring feedback""","""While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early."" Keyphrase: ""Inaccurate tutoring feedback""",0
arXIv2023,Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models,Yes.,5,"""Our findings show that even the most advanced GPT models struggle to reason on manipulated facts - showcasing poor DUPE skills - with accuracy dropping by 45% compared to the original dataset.""",2023,2023-05-23T20:26:03Z,"Keyphrase: ""Poor robustness to manipulation""","""Our findings show that even the most advanced GPT models struggle to reason on manipulated facts - showcasing poor DUPE skills - with accuracy dropping by 45% compared to the original dataset."" Keyphrase: ""Poor robustness to manipulation""",5
arXIv2023,Having Beer after Prayer? Measuring Cultural Bias in Large Language Models,Yes.,5,"""we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture,"" ""we find concerning cases of stereotyping and cultural unfairness,"" and ""revealing the incapability of appropriate adaptation to Arab cultural contexts.""",2023,2023-05-23T18:27:51Z,"Keyphrase: ""Cultural bias and unfair stereotyping""","""we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture,"" ""we find concerning cases of stereotyping and cultural unfairness,"" and ""revealing the incapability of appropriate adaptation to Arab cultural contexts."" Keyphrase: ""Cultural bias and unfair stereotyping""",3
arXIv2023,On Robustness of Finetuned Transformer-based NLP Models,Yes.,5,"""Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned?"" and ""Overall, this study provides valuable insights into perturbation-specific weaknesses of popular Transformer-based models, which should be kept in mind when passing inputs",2023,2023-05-23T18:25:18Z,"Keyphrase: ""Robustness to input perturbations""","""Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned?"" and ""Overall, this study provides valuable insights into perturbation-specific weaknesses of popular Transformer-based models, which should be kept in mind when passing inputs Keyphrase: ""Robustness to input perturbations""",5
arXIv2023,ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models,Yes.,4,"""Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning.""",2023,2023-05-23T17:54:33Z,"Keyphrase: ""Limited complex reasoning ability""","""Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning."" Keyphrase: ""Limited complex reasoning ability""",1
arXIv2023,Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science,Yes.,5,"""The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large).""",2023,2023-05-23T17:48:21Z,"Keyphrase: ""Zero-shot performance limitations""","""The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large)."" Keyphrase: ""Zero-shot performance limitations""",6
arXIv2023,Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs,Yes.,5,"""We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.""",2023,2023-05-23T17:25:59Z,"Keyphrase: ""Inconsistent performance""","""We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks."" Keyphrase: ""Inconsistent performance""",6
arXIv2023,Hierarchical Prompting Assists Large Language Model on Web Navigation,Yes.,4,"""Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks.""",2023,2023-05-23T17:10:39Z,"Keyphrase: ""Struggles with complex tasks""","""Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks."" Keyphrase: ""Struggles with complex tasks""",1
arXIv2023,Multilingual Large Language Models Are Not (Yet) Code-Switchers,Yes.,5,"""Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales."" and ""We argue that current 'multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.""",2023,2023-05-23T16:50:48Z,"Keyphrase: ""Underperformance in zero-shot/few-shot tasks""","""Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales."" and ""We argue that current 'multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy."" Keyphrase: ""Underperformance in zero-shot/few-shot tasks""",6
arXIv2023,Question Answering as Programming for Solving Time-Sensitive Questions,Yes.,5,"""our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics.""",2023,2023-05-23T16:35:16Z,"Keyphrase: ""Inability for rigorous reasoning""","""our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics."" Keyphrase: ""Inability for rigorous reasoning""",1
arXIv2023,HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations,Yes.,4,"""GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills.""",2023,2023-05-23T16:15:24Z,"Keyphrase: ""Limited pragmatic skills""","""GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills."" Keyphrase: ""Limited pragmatic skills""",1
arXIv2023,Improving Language Models via Plug-and-Play Retrieval Feedback,Yes.,5,"""Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can",2023,2023-05-23T12:29:44Z,"Keyphrase: ""Incorrect hallucinated information""","""Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can Keyphrase: ""Incorrect hallucinated information""",0
arXIv2023,Robust Prompt Optimization for Large Language Models Against Distribution Shifts,Yes.,5,"""We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis.""",2023,2023-05-23T11:30:43Z,"Keyphrase: ""Vulnerability to distribution shift""","""We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis."" Keyphrase: ""Vulnerability to distribution shift""",2
arXIv2023,A Trip Towards Fairness: Bias and De-Biasing in Large Language Models,Yes.,4,"""a little or a large bias in CtB-LLMs may cause huge harm,"" and ""we performed a large investigation of the bias of three families of CtB-LLMs,"" and ""we discovered that bias depends not on the number of parameters but on the perplexity.""",2023,2023-05-23T09:35:37Z,"Keyphrase: ""Biased outputs""","""a little or a large bias in CtB-LLMs may cause huge harm,"" and ""we performed a large investigation of the bias of three families of CtB-LLMs,"" and ""we discovered that bias depends not on the number of parameters but on the perplexity."" Keyphrase: ""Biased outputs""",3
arXIv2023,Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study,Yes.,5,"""Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse.""",2023,2023-05-23T09:33:38Z,"Keyphrase: ""Content constraint and potential misuse""","""Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse."" Keyphrase: ""Content constraint and potential misuse""",6
arXIv2023,"""Is the Pope Catholic?"" Applying Chain-of-Thought Reasoning to Understanding Conversational Implicatures",Yes.,5,"""recent research indicates that large language models struggle to comprehend these implicatures as effectively as the average human.""",2023,2023-05-23T08:49:50Z,"Keyphrase: ""Difficulty with implicatures""","""recent research indicates that large language models struggle to comprehend these implicatures as effectively as the average human."" Keyphrase: ""Difficulty with implicatures""",1
arXIv2023,Can Large Language Models Capture Dissenting Human Voices?,Yes.,5,"""we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.""",2023,2023-05-23T07:55:34Z,"Keyphrase: ""Limited ability in capturing human disagreement""","""we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population."" Keyphrase: ""Limited ability in capturing human disagreement""",1
arXIv2023,Exploring Self-supervised Logic-enhanced Training for Large Language Models,Yes.,5,"""Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines.""",2023,2023-05-23T06:13:10Z,"Keyphrase: ""Weak logical reasoning performance""","""Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines."" Keyphrase: ""Weak logical reasoning performance""",1
arXIv2023,The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models,Yes.,5,"""they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings.""",2023,2023-05-23T04:22:50Z,"Keyphrase: ""Inherent bias and lack of grounding""","""they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings."" Keyphrase: ""Inherent bias and lack of grounding""",0
arXIv2023,On the Risk of Misinformation Pollution with Large Language Models,Yes.,5,"""Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems.""",2023,2023-05-23T04:10:26Z,"Keyphrase: ""Misinformation generation""","""Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems."" Keyphrase: ""Misinformation generation""",2
arXIv2023,ChatGPT as your Personal Data Scientist,Yes.,5,"""Interestingly, its development spotlighted several critical weaknesses in the current LLMs (ChatGPT) and highlighted substantial opportunities for improvement.""",2023,2023-05-23T04:00:16Z,"Keyphrase: ""Critical weaknesses and room for improvement""","""Interestingly, its development spotlighted several critical weaknesses in the current LLMs (ChatGPT) and highlighted substantial opportunities for improvement."" Keyphrase: ""Critical weaknesses and room for improvement""",6
arXIv2023,InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning,Yes.,4,"""their ability to generalize to underrepresented languages is limited due to the scarcity of available data. Additionally, directly adapting new languages to instruction-tuned LLMs can result in catastrophic forgetting, which leads to the loss of multitasking ability.""",2023,2023-05-23T02:51:34Z,"Keyphrase: ""Catastrophic forgetting""","""their ability to generalize to underrepresented languages is limited due to the scarcity of available data. Additionally, directly adapting new languages to instruction-tuned LLMs can result in catastrophic forgetting, which leads to the loss of multitasking ability."" Keyphrase: ""Catastrophic forgetting""",5
arXIv2023,"Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",Yes.,5,"""However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity.""",2023,2023-05-23T02:49:35Z,"Keyphrase: ""Limited conversational proactivity""","""However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity."" Keyphrase: ""Limited conversational proactivity""",6
arXIv2023,How Language Model Hallucinations Can Snowball,Yes.,5,"""A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements."" and ""We refer to this phenomenon as hallucination snowballing",2023,2023-05-22T23:14:28Z,"Keyphrase: ""Risk of hallucinating incorrect statements""","""A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements."" and ""We refer to this phenomenon as hallucination snowballing Keyphrase: ""Risk of hallucinating incorrect statements""",0
arXIv2023,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,Yes.,5,"""We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.""",2023,2023-05-22T21:59:26Z,"Keyphrase: ""Poor slot filling performance""","""We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU."" Keyphrase: ""Poor slot filling performance""",6
arXIv2023,DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules,Yes.,4,"""Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects.""",2023,2023-05-22T18:43:31Z,"Keyphrase: ""Limited performance on non-standard English dialects""","""Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects."" Keyphrase: ""Limited performance on non-standard English dialects""",6
arXIv2023,RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text,Yes.,5,"""The fixed-size context of Transformer makes GPT models incapable of generating arbitrarily long text.""",2023,2023-05-22T17:58:10Z,"Keyphrase: ""Inability to generate arbitrarily long text""","""The fixed-size context of Transformer makes GPT models incapable of generating arbitrarily long text."" Keyphrase: ""Inability to generate arbitrarily long text""",4
arXIv2023,Language-Agnostic Bias Detection in Language Models with Bias Probing,Yes.,4,"""Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases."" and ""We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context.""",2023,2023-05-22T17:58:01Z,"Keyphrase: ""Strong social bias""","""Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases."" and ""We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context."" Keyphrase: ""Strong social bias""",3
arXIv2023,Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts,Yes.,5,"""tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory"" and ""how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?"" and ""Our investigation reveals seemingly contradicting behaviors of LLMs"" and ""These results pose important implications that are worth careful",2023,2023-05-22T17:57:41Z,"Keyphrase: ""Limited external evidence integration""","""tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory"" and ""how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?"" and ""Our investigation reveals seemingly contradicting behaviors of LLMs"" and ""These results pose important implications that are worth careful Keyphrase: ""Limited external evidence integration""",1
arXIv2023,Fairness of ChatGPT,Yes.,4,"""there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs,"" and ""This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.""",2023,2023-05-22T17:51:56Z,"Keyphrase: ""Limited quantitative analysis on fairness evaluation""","""there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs,"" and ""This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems."" Keyphrase: ""Limited quantitative analysis on fairness evaluation""",3
arXIv2023,Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,Yes.,5,"""Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold."" and ""Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research",2023,2023-05-22T17:36:58Z,"Keyphrase: ""Inability to detect certain types of hate speech""","""Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold."" and ""Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research Keyphrase: ""Inability to detect certain types of hate speech""",2
arXIv2023,"""According to ..."": Prompting Language Models Improves Quoting from Pre-Training Data",Yes.,5,"""Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data.""",2023,2023-05-22T17:25:24Z,"Keyphrase: ""Generation of fake information""","""Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data."" Keyphrase: ""Generation of fake information""",0
arXIv2023,To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis,Yes.,5,"""LLMs are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs,"" ""revealing that the model is susceptible to overfitting, leading to multi-epoch degradation,"" ""significant factors",2023,2023-05-22T17:02:15Z,"Keyphrase: ""Susceptible to overfitting""","""LLMs are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs,"" ""revealing that the model is susceptible to overfitting, leading to multi-epoch degradation,"" ""significant factors Keyphrase: ""Susceptible to overfitting""",5
arXIv2023,SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables,Yes.,5,"""Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of",2023,2023-05-22T16:13:50Z,"Keyphrase: ""Challenges in scientific table understanding""","""Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of Keyphrase: ""Challenges in scientific table understanding""",6
arXIv2023,Teaching Probabilistic Logical Reasoning to Transformers,Yes.,5,"""Our evaluation results show that both generations of language models struggle with reasoning over uncertain text.""",2023,2023-05-22T16:08:20Z,"Keyphrase: ""Struggles with uncertain reasoning""","""Our evaluation results show that both generations of language models struggle with reasoning over uncertain text."" Keyphrase: ""Struggles with uncertain reasoning""",1
arXIv2023,Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,Yes.,5,"""it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way."" and ""LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments.""",2023,2023-05-22T15:47:31Z,"Keyphrase: ""Shallow reasoning and susceptibility to absurd arguments""","""it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way."" and ""LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments."" Keyphrase: ""Shallow reasoning and susceptibility to absurd arguments""",1
arXIv2023,"Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students",Yes.,4,"""it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes"" and ""Our findings indicate that LLMs have an overall negative perception of math and STEM fields, with math being perceived most negatively.""",2023,2023-05-22T15:06:51Z,"Keyphrase: ""Perpetuation of harmful stereotypes""","""it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes"" and ""Our findings indicate that LLMs have an overall negative perception of math and STEM fields, with math being perceived most negatively."" Keyphrase: ""Perpetuation of harmful stereotypes""",3
arXIv2023,Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization,Yes.,5,"""We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance",2023,2023-05-22T14:58:13Z,"Keyphrase: ""Inconsistent evaluation and comparison""","""We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance Keyphrase: ""Inconsistent evaluation and comparison""",6
arXIv2023,Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model,Yes.,4,"""Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research.""",2023,2023-05-22T13:16:07Z,"Keyphrase: ""Limited interpretability""","""Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research."" Keyphrase: ""Limited interpretability""",7
arXIv2023,ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,Yes.,4,"""we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning.""",2023,2023-05-22T11:45:42Z,"Keyphrase: ""Limited computational reasoning""","""we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning."" Keyphrase: ""Limited computational reasoning""",1
arXIv2023,Automatic Code Summarization via ChatGPT: How Far Are We?,Yes.,5,"""The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models.""",2023,2023-05-22T09:43:40Z,"Keyphrase: ""Poor summarization performance""","""The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models."" Keyphrase: ""Poor summarization performance""",6
arXIv2023,Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage,Yes.,4,"""Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy.""",2023,2023-05-22T04:30:35Z,"Keyphrase: ""Limited commonsense knowledge accuracy""","""Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy."" Keyphrase: ""Limited commonsense knowledge accuracy""",1
arXIv2023,Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction,Yes.,5,"""Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition."" and ""The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT",2023,2023-05-22T03:04:06Z,"Keyphrase: ""Overlooking structural underpinnings""","""Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition."" and ""The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT Keyphrase: ""Overlooking structural underpinnings""",1
arXIv2023,Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models,Yes.,5,"""We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs.""",2023,2023-05-21T19:06:30Z,"Keyphrase: ""Limited research coverage""","""We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs."" Keyphrase: ""Limited research coverage""",0
arXIv2023,"GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts",Yes.,5,"""there is a current hot debate regarding their reasoning capacity"" and ""the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks.""",2023,2023-05-21T14:45:17Z,"Keyphrase: ""Limited reasoning proficiency""","""there is a current hot debate regarding their reasoning capacity"" and ""the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks."" Keyphrase: ""Limited reasoning proficiency""",1
arXIv2023,VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models,Yes.,5,"""They still have space to grow, though, especially in the areas of mathematics, physics, chemistry, and biology."" and ""especially in resolving LLMs' limits in disciplines involving mathematics and the natural sciences.""",2023,2023-05-20T14:13:08Z,"Keyphrase: ""Limited understanding of complex disciplines""","""They still have space to grow, though, especially in the areas of mathematics, physics, chemistry, and biology."" and ""especially in resolving LLMs' limits in disciplines involving mathematics and the natural sciences."" Keyphrase: ""Limited understanding of complex disciplines""",1
arXIv2023,UP5: Unbiased Foundation Model for Fairness-aware Recommendation,Yes.,4,"""there is unfairness involved in LLMs that lead to unfair recommendation results.""",2023,2023-05-20T04:32:59Z,"Keyphrase: ""Unfair recommendations""","""there is unfairness involved in LLMs that lead to unfair recommendation results."" Keyphrase: ""Unfair recommendations""",3
arXIv2023,Clinical Camel: An Open Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding,Yes.,4,"""Significant challenges concerning reliability, bias, and the potential for outdated knowledge persist.""",2023,2023-05-19T23:07:09Z,"Keyphrase: ""Reliability and bias challenges""","""Significant challenges concerning reliability, bias, and the potential for outdated knowledge persist."" Keyphrase: ""Reliability and bias challenges""",3
arXIv2023,Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,Yes.,5,"""However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst."" and ""They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews.""",2023,2023-05-19T17:09:19Z,"Keyphrase: ""Potential for misleading text""","""However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst."" and ""They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews."" Keyphrase: ""Potential for misleading text""",0
arXIv2023,Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs,Yes.,5,"""However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure",2023,2023-05-19T16:27:43Z,"Keyphrase: ""Overlooking linguistic cues""","""However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure Keyphrase: ""Overlooking linguistic cues""",6
arXIv2023,Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning,Yes.,5,"""Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk.""",2023,2023-05-19T15:45:29Z,"Keyphrase: ""Privacy risk from memorization""","""Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk."" Keyphrase: ""Privacy risk from memorization""",8
arXIv2023,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Yes.,5,"""Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge."" and ""Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts.""",2023,2023-05-19T15:36:27Z,"Keyphrase: ""Hallucination generation""","""Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge."" and ""Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts."" Keyphrase: ""Hallucination generation""",0
arXIv2023,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,Yes.,5,"""these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content.""",2023,2023-05-19T15:19:44Z,"Keyphrase: ""Inconsistent and problematic behavior""","""these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content."" Keyphrase: ""Inconsistent and problematic behavior""",0
arXIv2023,LLM-Pruner: On the Structural Pruning of Large Language Models,Yes.,4,"""such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages.""",2023,2023-05-19T12:10:53Z,"Keyphrase: ""Deployment challenges due to model size""","""such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages."" Keyphrase: ""Deployment challenges due to model size""",4
arXIv2023,RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought,Yes.,5,"""LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems.""",2023,2023-05-19T08:02:52Z,"Keyphrase: ""Factual consistency and reasoning challenges""","""LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems."" Keyphrase: ""Factual consistency and reasoning challenges""",0
arXIv2023,Graphologue: Exploring Large Language Model Responses with Interactive Diagrams,Yes.,5,"""However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure.""",2023,2023-05-19T06:53:25Z,"Keyphrase: ""Limited support for complex information tasks""","""However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure."" Keyphrase: ""Limited support for complex information tasks""",6
arXIv2023,A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation,Yes.,5,"""First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs.""",2023,2023-05-19T02:41:12Z,"Keyphrase: ""Vulnerability and unintended bugs""","""First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs."" Keyphrase: ""Vulnerability and unintended bugs""",2
arXIv2023,ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery,Yes.,4,"""data-sensitive domains -- including but not limited to healthcare -- face challenges in using ChatGPT due to privacy and data-ownership concerns.""",2023,2023-05-19T02:09:52Z,"Keyphrase: ""Privacy and data ownership concerns""","""data-sensitive domains -- including but not limited to healthcare -- face challenges in using ChatGPT due to privacy and data-ownership concerns."" Keyphrase: ""Privacy and data ownership concerns""",8
arXIv2023,Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models,Yes.,4,"""the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically.""",2023,2023-05-19T00:53:45Z,"Keyphrase: ""Repetitive and surprising data generation""","""the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically."" Keyphrase: ""Repetitive and surprising data generation""",4
arXIv2023,CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models,Yes.,4,"""Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias."" and ""Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases.""",2023,2023-05-18T18:58:30Z,"Keyphrase: ""Stereotypical biases and safety concerns""","""Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias."" and ""Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases."" Keyphrase: ""Stereotypical biases and safety concerns""",3
arXIv2023,Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses,Yes.,4,"""We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA's responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require",2023,2023-05-18T18:15:43Z,"Keyphrase: ""Limited causal reasoning ability""","""We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA's responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require Keyphrase: ""Limited causal reasoning ability""",1
arXIv2023,Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings,Yes.,4,"""Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks.""",2023,2023-05-18T07:56:40Z,"Keyphrase: ""Bias in sentence embeddings""","""Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks."" Keyphrase: ""Bias in sentence embeddings""",3
arXIv2023,Language Models Meet World Models: Embodied Experiences Enhance Language Models,Yes.,5,"""they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills.""",2023,2023-05-18T00:35:38Z,"Keyphrase: ""Lack of embodied knowledge""","""they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills."" Keyphrase: ""Lack of embodied knowledge""",1
arXIv2023,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,Yes.,5,"""Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.""",2023,2023-05-17T23:16:17Z,"Keyphrase: ""Limited strategic lookahead""","""Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role."" Keyphrase: ""Limited strategic lookahead""",1
arXIv2023,"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",Yes.,4,"""While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU.""",2023,2023-05-17T20:45:13Z,"Keyphrase: ""Memory-intensive deployment""","""While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU."" Keyphrase: ""Memory-intensive deployment""",4
arXIv2023,ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages,Yes.,5,"""We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations"" and ""ChatGPT appears to confer a higher respect to men than to women in the same occupation.""",2023,2023-05-17T18:30:05Z,"Keyphrase: ""Gender bias perpetuation""","""We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations"" and ""ChatGPT appears to confer a higher respect to men than to women in the same occupation."" Keyphrase: ""Gender bias perpetuation""",3
arXIv2023,BAD: BiAs Detection for Large Language Models in the context of candidate screening,Yes.,4,"""The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed."" and ""we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could",2023,2023-05-17T17:47:31Z,"Keyphrase: ""Bias in automated screening""","""The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed."" and ""we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could Keyphrase: ""Bias in automated screening""",3
arXIv2023,Evaluating Object Hallucination in Large Vision-Language Models,Yes.,5,"""we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions."" and ""show that they mostly suffer from severe object hallucination issue.""",2023,2023-05-17T16:34:01Z,"Keyphrase: ""Severe object hallucination""","""we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions."" and ""show that they mostly suffer from severe object hallucination issue."" Keyphrase: ""Severe object hallucination""",0
arXIv2023,Language Model Tokenizers Introduce Unfairness Between Languages,Yes.,4,"""disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked"" and ""This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided",2023,2023-05-17T14:17:57Z,"Keyphrase: ""Unfair treatment in language processing""","""disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked"" and ""This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided Keyphrase: ""Unfair treatment in language processing""",3
arXIv2023,Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models,Yes.,5,"""By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.""",2023,2023-05-17T05:25:27Z,"Keyphrase: ""Limited ability to generate factual and up-to-date knowledge""","""By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge."" Keyphrase: ""Limited ability to generate factual and up-to-date knowledge""",5
arXIv2023,"""I'm fully who I am"": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",Yes.,4,"""We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on",2023,2023-05-17T04:21:45Z,"Keyphrase: ""Gender misrepresentation""","""We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on Keyphrase: ""Gender misrepresentation""",3
arXIv2023,Small Models are Valuable Plug-ins for Large Language Models,Yes.,5,"""Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of",2023,2023-05-15T17:59:01Z,"Keyphrase: ""Limited accessibility to tuning and hardware resources""","""Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of Keyphrase: ""Limited accessibility to tuning and hardware resources""",4
arXIv2023,Large Language Models are Zero-Shot Rankers for Recommender Systems,Yes.,5,"""We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts.""",2023,2023-05-15T17:57:39Z,"Keyphrase: ""Limited historical context and biased prompt influence""","""We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts."" Keyphrase: ""Limited historical context and biased prompt influence""",6
arXIv2023,"Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",Yes.,5,"""However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed."" and ""they are",2023,2023-05-15T15:44:51Z,"Keyphrase: ""Lack of thorough risk analysis""","""However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed."" and ""they are Keyphrase: ""Lack of thorough risk analysis""",2
arXIv2023,Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study,Yes.,5,"""ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations. We also found that ChatGPT hardly understands rhetorical structures that are more complex than topic structures.""",2023,2023-05-15T07:14:41Z,"Keyphrase: ""Limited understanding of complex topic structures""","""ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations. We also found that ChatGPT hardly understands rhetorical structures that are more complex than topic structures."" Keyphrase: ""Limited understanding of complex topic structures""",6
arXIv2023,Semantic Composition in Visually Grounded Language Models,Yes.,5,"""Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure.""",2023,2023-05-15T03:19:42Z,"Keyphrase: ""Failure in representing compositional structure""","""Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure."" Keyphrase: ""Failure in representing compositional structure""",1
arXIv2023,Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics,Yes.,4,"""Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity."" and ""The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the LLM's ability to generalize its knowledge about human behavior in social dilem",2023,2023-05-13T17:23:16Z,"Keyphrase: ""Limited ability to generalize human behavior""","""Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity."" and ""The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the LLM's ability to generalize its knowledge about human behavior in social dilem Keyphrase: ""Limited ability to generalize human behavior""",1
arXIv2023,Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation,Yes.,4,"""it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation."" and ""we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations.""",2023,2023-05-12T16:54:36Z,"Keyphrase: ""Social prejudice and unfairness""","""it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation."" and ""we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations."" Keyphrase: ""Social prejudice and unfairness""",3
arXIv2023,Surfacing Biases in Large Language Models using Contrastive Input Decoding,Yes.,5,"""Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour"" and ""We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturb",2023,2023-05-12T11:09:49Z,"Keyphrase: ""Context-specific bias detection""","""Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour"" and ""We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturb Keyphrase: ""Context-specific bias detection""",3
arXIv2023,Evaluating Open-Domain Question Answering in the Era of Large Language Models,Yes.,5,"""The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.""",2023,2023-05-11T17:14:33Z,"Keyphrase: ""Difficulty in detecting hallucination""","""The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation."" Keyphrase: ""Difficulty in detecting hallucination""",0
arXIv2023,Active Retrieval Augmented Generation,Yes.,5,"""Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.""",2023,2023-05-11T17:13:40Z,"Keyphrase: ""Factually inaccurate output""","""Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output."" Keyphrase: ""Factually inaccurate output""",0
arXIv2023,Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models,Yes.,5,"""While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets."" and ""We find that while existing debiasing methods can mitigate reliance on a chosen spurious feature, the OOD performance gains of these methods can not be explained by mitigated reliance on biased features",2023,2023-05-11T14:35:00Z,"Keyphrase: ""Reliance on spurious correlations""","""While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets."" and ""We find that while existing debiasing methods can mitigate reliance on a chosen spurious feature, the OOD performance gains of these methods can not be explained by mitigated reliance on biased features Keyphrase: ""Reliance on spurious correlations""",5
arXIv2023,Chain-of-Dictionary Prompting Elicits Translation in Large Language Models,Yes.,5,"""they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation.""",2023,2023-05-11T05:19:47Z,"Keyphrase: ""Struggles with rare words and low-resource languages""","""they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation."" Keyphrase: ""Struggles with rare words and low-resource languages""",5
arXIv2023,How Good are Commercial Large Language Models on African Languages?,Yes.,4,"""However, their performance on African languages is largely unknown."" and ""Our results suggest that commercial language models produce below-par performance on African languages.""",2023,2023-05-11T02:29:53Z,"Keyphrase: ""Poor performance in African languages""","""However, their performance on African languages is largely unknown."" and ""Our results suggest that commercial language models produce below-par performance on African languages."" Keyphrase: ""Poor performance in African languages""",7
arXIv2023,"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",Yes.,4,"""examines the errors produced by the LLMs and categorized the errors into three types",2023,2023-05-10T13:40:06Z,"Keyphrase: ""Error categorization limitations""","""examines the errors produced by the LLMs and categorized the errors into three types Keyphrase: ""Error categorization limitations""",1
arXIv2023,Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,Yes.,5,"""Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge,"" and ""statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.""",2023,2023-05-10T08:35:50Z,"Keyphrase: ""Limited commonsense knowledge""","""Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge,"" and ""statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict."" Keyphrase: ""Limited commonsense knowledge""",1
arXIv2023,Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition,Yes.,5,"""This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT.""",2023,2023-05-10T08:16:46Z,"Keyphrase: ""Limited critical insight""","""This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT."" Keyphrase: ""Limited critical insight""",6
arXIv2023,ChatGPT as a Text Simplification Tool to Remove Bias,Yes.,4,"""The presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. If the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based upon their protected characteristic, leading to possible discrimination.""",2023,2023-05-09T13:10:23Z,"Keyphrase: ""Association of language with protected characteristics""","""The presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. If the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based upon their protected characteristic, leading to possible discrimination."" Keyphrase: ""Association of language with protected characteristics""",3
arXIv2023,Explanation-based Finetuning Makes Models More Robust to Spurious Cues,Yes.,4,"""Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data.""",2023,2023-05-08T18:53:45Z,"Keyphrase: ""Poor generalization to out-of-distribution data""","""Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data."" Keyphrase: ""Poor generalization to out-of-distribution data""",5
arXIv2023,Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns,Yes.,4,"""underscores the challenges in developing safe and unbiased LLMs, and emphasizes the importance of understanding the susceptibility of LLMs to external influences.""",2023,2023-05-08T16:10:18Z,"Keyphrase: ""Susceptibility to external influence""","""underscores the challenges in developing safe and unbiased LLMs, and emphasizes the importance of understanding the susceptibility of LLMs to external influences."" Keyphrase: ""Susceptibility to external influence""",2
arXIv2023,Differentially Private Attention Computation,Yes.,4,"""one crucial issue concerning the inference results of large language models is security and privacy"" and ""results generated by LLMs could possibly leak many confidential or copyright information.""",2023,2023-05-08T13:32:41Z,"Keyphrase: ""Security and privacy concerns""","""one crucial issue concerning the inference results of large language models is security and privacy"" and ""results generated by LLMs could possibly leak many confidential or copyright information."" Keyphrase: ""Security and privacy concerns""",8
arXIv2023,Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,Yes.,5,"""However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction."" and ""Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety.""",2023,2023-05-07T22:44:25Z,"Keyphrase: ""Misleading explanations""","""However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction."" and ""Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety."" Keyphrase: ""Misleading explanations""",1
arXIv2023,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,Yes.,5,"""Despite the success of Zero-shot-CoT, it still suffers from three pitfalls",2023,2023-05-06T16:34:37Z,"Keyphrase: ""Limitations in zero-shot capability""","""Despite the success of Zero-shot-CoT, it still suffers from three pitfalls Keyphrase: ""Limitations in zero-shot capability""",6
arXIv2023,Pre-training Language Model as a Multi-perspective Course Learner,Yes.,4,"""Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning.""",2023,2023-05-06T09:02:10Z,"Keyphrase: ""Biased learning and label imbalance""","""Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning."" Keyphrase: ""Biased learning and label imbalance""",5
arXIv2023,"Large Language Models in Sport Science & Medicine: Opportunities, Risks and Considerations",Yes.,4,"""However, there are also potential risks associated with the use and development of LLMs, including biases in the dataset used to create the model, the risk of exposing confidential data, the risk of generating harmful output, and the need to align these models with human preferences through feedback.""",2023,2023-05-05T21:20:02Z,"Keyphrase: ""Risk of bias and harmful output""","""However, there are also potential risks associated with the use and development of LLMs, including biases in the dataset used to create the model, the risk of exposing confidential data, the risk of generating harmful output, and the need to align these models with human preferences through feedback."" Keyphrase: ""Risk of bias and harmful output""",2
arXIv2023,Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming,Yes.,5,"""Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality.""",2023,2023-05-05T07:24:46Z,"Keyphrase: ""Limited logical reasoning""","""Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality."" Keyphrase: ""Limited logical reasoning""",1
arXIv2023,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,Yes.,5,"""one of its most fatal disadvantages is the lack of factual correctness"" and ""still suffers from factuality concerns in knowledge-intensive tasks.""",2023,2023-05-05T03:49:14Z,"Keyphrase: ""Factual correctness deficiency""","""one of its most fatal disadvantages is the lack of factual correctness"" and ""still suffers from factuality concerns in knowledge-intensive tasks."" Keyphrase: ""Factual correctness deficiency""",0
arXIv2023,Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,Yes.,5,"""even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand.""",2023,2023-05-04T19:02:29Z,"Keyphrase: ""Limited execution accuracy""","""even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand."" Keyphrase: ""Limited execution accuracy""",6
arXIv2023,Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision,Yes.,4,"""However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases.""",2023,2023-05-04T17:59:28Z,"Keyphrase: ""Dependence on human supervision""","""However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases."" Keyphrase: ""Dependence on human supervision""",1
arXIv2023,Can LLMs Capture Human Preferences?,Yes.,5,"""Though GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans."" and ""While directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings",2023,2023-05-04T03:51:31Z,"Keyphrase: ""Lexicographic preference bias""","""Though GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans."" and ""While directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings Keyphrase: ""Lexicographic preference bias""",0
arXIv2023,"Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents",Yes.,4,"""the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent",2023,2023-05-03T20:11:22Z,"Keyphrase: ""Challenges in serving as an agent""","""the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent Keyphrase: ""Challenges in serving as an agent""",1
arXIv2023,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,Yes.,5,"""Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications.""",2023,2023-05-03T17:50:56Z,"Keyphrase: ""Memory inefficiency and computational intensity""","""Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications."" Keyphrase: ""Memory inefficiency and computational intensity""",4
arXIv2023,GPT-RE: In-context Learning for Relation Extraction using Large Language Models,Yes.,5,"""they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE",2023,2023-05-03T13:28:08Z,"Keyphrase: ""Lagging behind fully-supervised baselines""","""they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE Keyphrase: ""Lagging behind fully-supervised baselines""",6
arXIv2023,Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy,Yes.,5,"""LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately."" and ""Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy.""",2023,2023-05-02T15:53:28Z,"Keyphrase: ""Memorization of training data""","""LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately."" and ""Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy."" Keyphrase: ""Memorization of training data""",8
arXIv2023,Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation,Yes.,5,"""However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code."" and ""Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k",2023,2023-05-02T05:46:48Z,"Keyphrase: ""Limited testing coverage""","""However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code."" and ""Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k Keyphrase: ""Limited testing coverage""",7
arXIv2023,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Yes.,5,"""this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors.""",2023,2023-05-01T17:36:06Z,"Keyphrase: ""Toxic content generation""","""this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors."" Keyphrase: ""Toxic content generation""",2
arXIv2023,Poisoning Language Models During Instruction Tuning,Yes.,5,"""we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions"" and ""Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.""",2023,2023-05-01T16:57:33Z,"Keyphrase: ""Vulnerability to poisoning attacks""","""we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions"" and ""Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy."" Keyphrase: ""Vulnerability to poisoning attacks""",2
arXIv2023,Learning to Reason and Memorize with Self-Notes,Yes.,5,"""Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use.""",2023,2023-05-01T14:02:48Z,"Keyphrase: ""Difficulty in retaining multistep reasoning""","""Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use."" Keyphrase: ""Difficulty in retaining multistep reasoning""",1
arXIv2023,Self-Evaluation Guided Beam Search for Reasoning,Yes.,5,"""the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results.""",2023,2023-05-01T02:37:59Z,"Keyphrase: ""Error accumulation in reasoning chain""","""the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results."" Keyphrase: ""Error accumulation in reasoning chain""",1
arXIv2023,"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",Yes.,5,"""albeit it may not possess the same level of expertise in identifying the temporal order between two events"" and ""the implicit discourse relation remains a formidable challenge"" and ""ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.""",2023,2023-04-28T13:14:36Z,"Keyphrase: ""Subpar performance in discourse parsing""","""albeit it may not possess the same level of expertise in identifying the temporal order between two events"" and ""the implicit discourse relation remains a formidable challenge"" and ""ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation."" Keyphrase: ""Subpar performance in discourse parsing""",6
arXIv2023,Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery,Yes.,5,"""current explorations do not assess the real-world utility and safety of LLMs in clinical settings,"" ""responses contained hallucinated references,"" and ""often do not meet the specific information need of a given question.""",2023,2023-04-26T17:54:28Z,"Keyphrase: ""Hallucinated responses""","""current explorations do not assess the real-world utility and safety of LLMs in clinical settings,"" ""responses contained hallucinated references,"" and ""often do not meet the specific information need of a given question."" Keyphrase: ""Hallucinated responses""",0
arXIv2023,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,Yes.,4,"""We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on",2023,2023-04-26T17:52:30Z,"Keyphrase: ""Spurious bias and data-specific challenges""","""We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on Keyphrase: ""Spurious bias and data-specific challenges""",3
arXIv2023,Enhancing Large Language Model with Self-Controlled Memory Framework,Yes.,5,"""Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information.""",2023,2023-04-26T07:25:31Z,"Keyphrase: ""Inability to process lengthy input""","""Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information."" Keyphrase: ""Inability to process lengthy input""",4
arXIv2023,The Internal State of an LLM Knows When It's Lying,Yes.,5,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone.""",2023,2023-04-26T02:49:38Z,"Keyphrase: ""Confident false information""","""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone."" Keyphrase: ""Confident false information""",0
arXIv2023,AI-assisted coding: Experiments with GPT-4,Yes.,5,"""These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance"" and ""we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code.""",2023,2023-04-25T22:59:01Z,"Keyphrase: ""Dependence on human validation""","""These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance"" and ""we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code."" Keyphrase: ""Dependence on human validation""",7
arXIv2023,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",Yes.,5,"""Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa).""",2023,2023-04-25T17:05:38Z,"Keyphrase: ""Limited audio processing capabilities""","""Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa)."" Keyphrase: ""Limited audio processing capabilities""",6
arXIv2023,Semantic Compression With Large Language Models,Yes.,5,"""However, in addition to confidently presenting factually inaccurate information at times (known as 'hallucinations'), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information.""",2023,2023-04-25T01:47:05Z,"Keyphrase: ""Limited input/output token processing""","""However, in addition to confidently presenting factually inaccurate information at times (known as 'hallucinations'), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information."" Keyphrase: ""Limited input/output token processing""",4
arXIv2023,Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering,Yes.,5,"""However, their fixed context length poses challenges when processing long documents or maintaining extended conversations.""",2023,2023-04-24T13:55:47Z,"Keyphrase: ""Limited context length""","""However, their fixed context length poses challenges when processing long documents or maintaining extended conversations."" Keyphrase: ""Limited context length""",4
arXIv2023,Is ChatGPT the Ultimate Programming Assistant -- How far is it?,Yes.,4,"""our experiments also reveal limitations in terms of its attention span",2023,2023-04-24T09:20:13Z,"Keyphrase: ""Limited attention span""","""our experiments also reveal limitations in terms of its attention span Keyphrase: ""Limited attention span""",5
arXIv2023,Differentiate ChatGPT-generated and Human-written Medical Texts,Yes.,4,"""erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.""",2023,2023-04-23T07:38:07Z,"Keyphrase: ""Erroneous medical content""","""erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public."" Keyphrase: ""Erroneous medical content""",6
arXIv2023,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,Yes.,5,"""However, so far, LLMs cannot reliably solve long-horizon planning problems.""",2023,2023-04-22T20:34:03Z,"Keyphrase: ""Limited long-horizon planning""","""However, so far, LLMs cannot reliably solve long-horizon planning problems."" Keyphrase: ""Limited long-horizon planning""",1
arXIv2023,Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens,Yes.,5,"""These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.""",2023,2023-04-22T12:50:49Z,"Keyphrase: ""Dependency on massive training data""","""These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations."" Keyphrase: ""Dependency on massive training data""",5
arXIv2023,Emergent and Predictable Memorization in Large Language Models,Yes.,5,"""Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models.""",2023,2023-04-21T17:58:31Z,"Keyphrase: ""Memorization tendency""","""Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models."" Keyphrase: ""Memorization tendency""",8
arXIv2023,The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination,Yes.,5,"""new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination.""",2023,2023-04-21T16:40:54Z,"Keyphrase: ""Ethical and legal risks from hallucination""","""new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination."" Keyphrase: ""Ethical and legal risks from hallucination""",0
arXIv2023,Inducing anxiety in large language models increases exploration and bias,Yes.,5,"""Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance."" and ""GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text.""",2023,2023-04-21T16:29:43Z,"Keyphrase: ""Increased societal bias""","""Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance."" and ""GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text."" Keyphrase: ""Increased societal bias""",3
arXIv2023,ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT,Yes.,4,"""LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously.""",2023,2023-04-21T16:23:47Z,"Keyphrase: ""Limited reasoning capability""","""LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously."" Keyphrase: ""Limited reasoning capability""",1
arXIv2023,Meta Semantics: Towards better natural language understanding and reasoning,Yes.,5,"""Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem.""",2023,2023-04-20T22:16:16Z,"Keyphrase: ""Weak logical deduction and out-of-vocabulary problem""","""Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem."" Keyphrase: ""Weak logical deduction and out-of-vocabulary problem""",6
arXIv2023,Why Does ChatGPT Fall Short in Providing Truthful Answers?,Yes.,5,"""ChatGPT still faces challenges in providing reliable and accurate answers to user questions."" and ""We further pinpoint factuality as the most contributing failure and identify two critical abilities associated with factuality",2023,2023-04-20T17:48:43Z,"Keyphrase: ""Reliability and factuality challenges""","""ChatGPT still faces challenges in providing reliable and accurate answers to user questions."" and ""We further pinpoint factuality as the most contributing failure and identify two critical abilities associated with factuality Keyphrase: ""Reliability and factuality challenges""",6
arXIv2023,Safety Assessment of Chinese Large Language Models,Yes.,5,"""These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information.""",2023,2023-04-20T16:27:35Z,"Keyphrase: ""Generating harmful content""","""These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information."" Keyphrase: ""Generating harmful content""",2
arXIv2023,Fully Autonomous Programming with Large Language Models,Yes.,5,"""Current approaches to program synthesis with Large Language Models (LLMs) exhibit a 'near miss syndrome'",2023,2023-04-20T16:12:05Z,"Keyphrase: ""Near miss syndrome""","""Current approaches to program synthesis with Large Language Models (LLMs) exhibit a 'near miss syndrome' Keyphrase: ""Near miss syndrome""",0
arXIv2023,Supporting Human-AI Collaboration in Auditing LLMs with LLMs,Yes.,4,"""Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale.""",2023,2023-04-19T21:59:04Z,"Keyphrase: ""Biased and irresponsible behavior""","""Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale."" Keyphrase: ""Biased and irresponsible behavior""",3
arXIv2023,Fundamental Limitations of Alignment in Large Language Models,Yes.,5,"""Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.""",2023,2023-04-19T17:50:09Z,"Keyphrase: ""Safety concerns""","""Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety."" Keyphrase: ""Safety concerns""",2
arXIv2023,In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT,Yes.,5,"""We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions."" and ""We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases.""",2023,2023-04-18T13:20:45Z,"Keyphrase: ""Domain-specific reliability issues""","""We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions."" and ""We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases."" Keyphrase: ""Domain-specific reliability issues""",6
arXIv2023,Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs,Yes.,4,"""This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild. Unfortunately, most such tools are critically flawed.""",2023,2023-04-18T13:05:01Z,"Keyphrase: ""Flawed detection tools""","""This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild. Unfortunately, most such tools are critically flawed."" Keyphrase: ""Flawed detection tools""",2
arXIv2023,An Evaluation on Large Language Model Outputs: Discourse and Memorization,Yes.,4,"""We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic.""",2023,2023-04-17T22:12:12Z,"Keyphrase: ""Memorization over understanding""","""We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic."" Keyphrase: ""Memorization over understanding""",1
arXIv2023,Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark,Yes.,5,"""ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs."" and ""results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability.""",2023,2023-04-17T00:41:19Z,"Keyphrase: ""Inconsistent output reliability""","""ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs."" and ""results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability."" Keyphrase: ""Inconsistent output reliability""",6
arXIv2023,VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping,Yes.,5,"""However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans.""",2023,2023-04-16T15:29:03Z,"Keyphrase: ""Limited support for user control and autonomy""","""However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans."" Keyphrase: ""Limited support for user control and autonomy""",7
arXIv2023,The Self-Perception and Political Biases of ChatGPT,Yes.,4,"""This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT."" and ""claiming that ChatGPT is politically biased towards progressive and libertarian points of view.""",2023,2023-04-14T18:06:13Z,"Keyphrase: ""Political bias""","""This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT."" and ""claiming that ChatGPT is politically biased towards progressive and libertarian points of view."" Keyphrase: ""Political bias""",3
arXIv2023,Stochastic Code Generation,Yes.,5,"""Large language models pre-trained for code generation can generate high-quality short code but often struggle with generating coherent long code and understanding higher-level or system-level specifications.""",2023,2023-04-14T00:01:05Z,"Keyphrase: ""Difficulty in generating coherent long code""","""Large language models pre-trained for code generation can generate high-quality short code but often struggle with generating coherent long code and understanding higher-level or system-level specifications."" Keyphrase: ""Difficulty in generating coherent long code""",7
arXIv2023,Evaluation of Social Biases in Recent Large Pre-Trained Models,Yes.,4,"""Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models.""",2023,2023-04-13T23:29:58Z,"Keyphrase: ""Biased training data""","""Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models."" Keyphrase: ""Biased training data""",3
arXIv2023,ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning,Yes.,5,"""Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.""",2023,2023-04-12T05:08:52Z,"Keyphrase: ""Inferior performance compared to previous models""","""Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning."" Keyphrase: ""Inferior performance compared to previous models""",6
arXIv2023,Understanding Causality with Large Language Models: Feasibility and Opportunities,Yes.,4,"""We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision.""",2023,2023-04-11T22:30:03Z,"Keyphrase: ""Limited causal reasoning""","""We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision."" Keyphrase: ""Limited causal reasoning""",1
arXIv2023,Zero-shot Temporal Relation Extraction with ChatGPT,Yes.,5,"""Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts"" and ""The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency",2023,2023-04-11T18:59:05Z,"Keyphrase: ""Inconsistent temporal inference""","""Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts"" and ""The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency Keyphrase: ""Inconsistent temporal inference""",6
arXIv2023,Towards preserving word order importance through Forced Invalidation,Yes.,5,"""recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed.""",2023,2023-04-11T13:42:10Z,"Keyphrase: ""Insensitive to word order""","""recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed."" Keyphrase: ""Insensitive to word order""",1
arXIv2023,Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,Yes.,4,"""Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3)",2023,2023-04-11T06:37:30Z,"Keyphrase: ""Challenges in scientific text detection""","""Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3) Keyphrase: ""Challenges in scientific text detection""",2
arXIv2023,Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,Yes.,4,"""GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource languages."" and ""First, instruction semantics can surprisingly be ignored when given in-context exemplars.""",2023,2023-04-10T15:51:30Z,"Keyphrase: ""Limited performance in commercial translation systems""","""GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource languages."" and ""First, instruction semantics can surprisingly be ignored when given in-context exemplars."" Keyphrase: ""Limited performance in commercial translation systems""",6
arXIv2023,Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,Yes.,5,"""However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.""",2023,2023-04-10T05:25:54Z,"Keyphrase: ""Weakness in multistep reasoning""","""However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}."" Keyphrase: ""Weakness in multistep reasoning""",1
arXIv2023,The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges,Yes.,5,"""Our findings indicate that ChatGPT is a 'Wall Street Neophyte' with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features."" and ""we observe",2023,2023-04-10T04:31:00Z,"Keyphrase: ""Limited stock prediction accuracy""","""Our findings indicate that ChatGPT is a 'Wall Street Neophyte' with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features."" and ""we observe Keyphrase: ""Limited stock prediction accuracy""",6
arXIv2023,Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance,Yes.,5,"""Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.""",2023,2023-04-09T04:53:15Z,"Keyphrase: ""Low interreliability with human ratings""","""Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings."" Keyphrase: ""Low interreliability with human ratings""",6
arXIv2023,Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models,Yes.,5,"""This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions.""",2023,2023-04-07T17:14:00Z,"Keyphrase: ""Bias and risk factors""","""This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions."" Keyphrase: ""Bias and risk factors""",3
arXIv2023,Revisiting Automated Prompting: Are We Actually Doing Better?,Yes.,5,"""We find that automated prompting does not consistently outperform simple manual prompts.""",2023,2023-04-07T12:06:44Z,"Keyphrase: ""Limited performance compared to manual prompts""","""We find that automated prompting does not consistently outperform simple manual prompts."" Keyphrase: ""Limited performance compared to manual prompts""",7
arXIv2023,Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions,Yes.,5,"""ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.""",2023,2023-04-06T05:01:28Z,"Keyphrase: ""Limited world knowledge and inference capabilities""","""ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses."" Keyphrase: ""Limited world knowledge and inference capabilities""",6
arXIv2023,Conceptual structure coheres in human cognition but not in large language models,Yes.,5,"""These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language.""",2023,2023-04-05T21:27:01Z,"Keyphrase: ""Gap in human cognition""","""These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language."" Keyphrase: ""Gap in human cognition""",1
arXIv2023,Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,Yes.,4,"""Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment.""",2023,2023-04-05T15:11:25Z,"Keyphrase: ""Limited performance compared to simpler models""","""Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment."" Keyphrase: ""Limited performance compared to simpler models""",6
arXIv2023,Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation,Yes.,4,"""further analysis of various types of errors at the document-level has shown that ChatGPT cannot effectively correct agreement, coreference, tense errors across sentences, and cross-sentence boundary errors.""",2023,2023-04-04T12:33:40Z,"Keyphrase: ""Ineffective error correction""","""further analysis of various types of errors at the document-level has shown that ChatGPT cannot effectively correct agreement, coreference, tense errors across sentences, and cross-sentence boundary errors."" Keyphrase: ""Ineffective error correction""",6
arXIv2023,"To ChatGPT, or not to ChatGPT: That is the question!",Yes.,5,"""concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud... none of the existing methods can effectively detect ChatGPT-generated content.""",2023,2023-04-04T03:04:28Z,"Keyphrase: ""Fake news and manipulation""","""concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud... none of the existing methods can effectively detect ChatGPT-generated content."" Keyphrase: ""Fake news and manipulation""",2
arXIv2023,Blockwise Compression of Transformer-based Models without Retraining,Yes.,5,"""These operations bring the inevitable challenges of massive computation resources and huge memory footprint, usually requiring at least 10^23 FLOPs and hundreds of gigabytes, respectively.""",2023,2023-04-04T02:55:40Z,"Keyphrase: ""Resource-intensive operations""","""These operations bring the inevitable challenges of massive computation resources and huge memory footprint, usually requiring at least 10^23 FLOPs and hundreds of gigabytes, respectively."" Keyphrase: ""Resource-intensive operations""",4
arXIv2023,Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT,Yes.,4,"""Comparative results show that using ChatGPT without human intervention may be inadequate due to reliability related issues,"" and ""We also highlight future challenges, including concerns about LLM trustworthiness and the necessity for standardisation and regulation in this domain.""",2023,2023-04-03T16:46:49Z,"Keyphrase: ""Reliability and trustworthiness issues""","""Comparative results show that using ChatGPT without human intervention may be inadequate due to reliability related issues,"" and ""We also highlight future challenges, including concerns about LLM trustworthiness and the necessity for standardisation and regulation in this domain."" Keyphrase: ""Reliability and trustworthiness issues""",6
arXIv2023,DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task,Yes.,4,"""these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs.""",2023,2023-04-03T15:57:51Z,"Keyphrase: ""Limited performance in specialized domains""","""these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs."" Keyphrase: ""Limited performance in specialized domains""",6
arXIv2023,Towards Healthy AI: Large Language Models Need Therapists Too,Yes.,4,"""these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors.""",2023,2023-04-02T00:39:12Z,"Keyphrase: ""Harmful behavior tendencies""","""these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors."" Keyphrase: ""Harmful behavior tendencies""",2
arXIv2023,Enhancing Large Language Models with Climate Resources,Yes.,4,"""LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change.""",2023,2023-03-31T20:24:14Z,"Keyphrase: ""Lack of recent information and imprecise language""","""LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change."" Keyphrase: ""Lack of recent information and imprecise language""",0
arXIv2023,Assessing Language Model Deployment with Risk Cards,Yes.,5,"""text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text."" and ""Prior work establishes a wide variety of language model harms to many different actors",2023,2023-03-31T16:45:42Z,"Keyphrase: ""Harmful text generation""","""text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text."" and ""Prior work establishes a wide variety of language model harms to many different actors Keyphrase: ""Harmful text generation""",2
arXIv2023,GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors,Yes.,4,"""GPT-4 showed low accuracy in subjects including public health & medicine-related law, internal medicine (2) which are localized in Korea and TKM. The model's accuracy was lower for questions requiring TKM-specialized knowledge."" and ""These findings underline the potential of LLMs like GPT-4 in culturally adapted medicine, especially TKM, for tasks such as clinical assistance,",2023,2023-03-31T05:43:21Z,"Keyphrase: ""Low accuracy in specialized domains""","""GPT-4 showed low accuracy in subjects including public health & medicine-related law, internal medicine (2) which are localized in Korea and TKM. The model's accuracy was lower for questions requiring TKM-specialized knowledge."" and ""These findings underline the potential of LLMs like GPT-4 in culturally adapted medicine, especially TKM, for tasks such as clinical assistance, Keyphrase: ""Low accuracy in specialized domains""",6
arXIv2023,"Recognition, recall, and retention of few-shot memories in large language models",Yes.,5,"""The flip side of this remarkable capacity for fast learning is that precise memories are quickly overwritten",2023,2023-03-30T17:26:16Z,"Keyphrase: ""Overwritten memory""","""The flip side of this remarkable capacity for fast learning is that precise memories are quickly overwritten Keyphrase: ""Overwritten memory""",5
arXIv2023,Yes but.. Can ChatGPT Identify Entities in Historical Documents?,Yes.,4,"""Our findings indicate several shortcomings in identifying entities in historical text that range from the consistency of entity annotation guidelines, entity complexity, and code-switching, to the specificity of prompting. Moreover, as expected, the inaccessibility of historical archives to the public (and thus on the Internet) also impacts its performance.""",2023,2023-03-30T12:23:39Z,"Keyphrase: ""Challenges in entity identification and historical text understanding""","""Our findings indicate several shortcomings in identifying entities in historical text that range from the consistency of entity annotation guidelines, entity complexity, and code-switching, to the specificity of prompting. Moreover, as expected, the inaccessibility of historical archives to the public (and thus on the Internet) also impacts its performance."" Keyphrase: ""Challenges in entity identification and historical text understanding""",1
arXIv2023,ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models,Yes.,5,"""their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point."" and ""ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question.""",2023,2023-03-29T03:05:43Z,"Keyphrase: ""Limited commonsense knowledge""","""their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point."" and ""ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question."" Keyphrase: ""Limited commonsense knowledge""",1
arXIv2023,Writing Assistants Should Model Social Factors of Language,Yes.,4,"""Intelligent writing assistants powered by large language models (LLMs) are more popular today than ever before, but their further widespread adoption is precluded by sub-optimal performance."" and ""a major reason for this sub-optimal performance and adoption is a singular focus on the information content of language while ignoring its social aspects.""",2023,2023-03-28T19:38:57Z,"Keyphrase: ""Neglect of social aspects""","""Intelligent writing assistants powered by large language models (LLMs) are more popular today than ever before, but their further widespread adoption is precluded by sub-optimal performance."" and ""a major reason for this sub-optimal performance and adoption is a singular focus on the information content of language while ignoring its social aspects."" Keyphrase: ""Neglect of social aspects""",6
arXIv2023,Hallucinations in Large Multilingual Translation Models,Yes.,5,"""However, when deployed in the wild, these models may generate hallucinated translations which have the potential to severely undermine user trust and raise safety concerns.""",2023,2023-03-28T16:17:59Z,"Keyphrase: ""Hallucinated translations""","""However, when deployed in the wild, these models may generate hallucinated translations which have the potential to severely undermine user trust and raise safety concerns."" Keyphrase: ""Hallucinated translations""",0
arXIv2023,"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",Yes.,5,"""However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns.""",2023,2023-03-27T21:27:58Z,"Keyphrase: ""Biased responses""","""However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns."" Keyphrase: ""Biased responses""",3
arXIv2023,$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference,Yes.,5,"""we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment.""",2023,2023-03-24T06:16:29Z,"Keyphrase: ""Bias and calibration challenges""","""we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment."" Keyphrase: ""Bias and calibration challenges""",5
arXIv2023,Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages,Yes.,5,"""publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending",2023,2023-03-23T18:16:30Z,"Keyphrase: ""Inconsistent codemixed text generation""","""publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending Keyphrase: ""Inconsistent codemixed text generation""",6
arXIv2023,Increasing Textual Context Size Boosts Medical Image-Text Matching,Yes.,5,"""CLIP's limited textual input size has negative impact on downstream performance in the medical domain where encoding longer textual contexts is often required.""",2023,2023-03-23T15:20:05Z,"Keyphrase: ""Limited textual input size""","""CLIP's limited textual input size has negative impact on downstream performance in the medical domain where encoding longer textual contexts is often required."" Keyphrase: ""Limited textual input size""",4
arXIv2023,Can we trust the evaluation on ChatGPT?,Yes.,5,"""evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF)."" and ""We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.""",2023,2023-03-22T17:32:56Z,"Keyphrase: ""Challenges in evaluating diverse performance""","""evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF)."" and ""We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models."" Keyphrase: ""Challenges in evaluating diverse performance""",6
arXIv2023,Can AI-Generated Text be Reliably Detected?,Yes.,5,"""The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc."" and ""we show that these detectors are not reliable in practical scenarios."" and ""even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks",2023,2023-03-17T17:53:19Z,"Keyphrase: ""Vulnerability to spoofing attacks""","""The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc."" and ""we show that these detectors are not reliable in practical scenarios."" and ""even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks Keyphrase: ""Vulnerability to spoofing attacks""",2
arXIv2023,Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?,Yes.,5,"""We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (<70% on even entry-level modules)."" and ""some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps).""",2023,2023-03-16T13:58:45Z,"Keyphrase: ""Limited handling of complex reasoning steps""","""We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (<70% on even entry-level modules)."" and ""some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps)."" Keyphrase: ""Limited handling of complex reasoning steps""",1
arXIv2023,SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,Yes.,5,"""LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output.""",2023,2023-03-15T19:31:21Z,"Keyphrase: ""Hallucination and nonfactual statements""","""LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output."" Keyphrase: ""Hallucination and nonfactual statements""",0
arXIv2023,Input-length-shortening and text generation via attention values,Yes.,5,"""transformer models usually have an input-length limitation caused by hardware constraints"" and ""This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model.""",2023,2023-03-14T02:11:24Z,"Keyphrase: ""Input length limitation""","""transformer models usually have an input-length limitation caused by hardware constraints"" and ""This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model."" Keyphrase: ""Input length limitation""",4
arXIv2023,Consistency Analysis of ChatGPT,Yes.,5,"""Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.""",2023,2023-03-11T01:19:01Z,"Keyphrase: ""Inconsistent logical predictions""","""Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs."" Keyphrase: ""Inconsistent logical predictions""",1
arXIv2023,Planning with Large Language Models for Code Generation,Yes.,5,"""Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation.""",2023,2023-03-09T18:59:47Z,"Keyphrase: ""Incorrect code generation""","""Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation."" Keyphrase: ""Incorrect code generation""",7
arXIv2023,Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code,Yes.,5,"""However, the capabilities of GPT models and their limitations to reason about and/or analyze code in educational settings have been under-explored."" and ""MCQs containing code snippets are not answered as successfully as those that only contain natural language."" and ""MCQs that require analysis and/or reasoning",2023,2023-03-09T16:52:12Z,"Keyphrase: ""Limited code analysis capabilities""","""However, the capabilities of GPT models and their limitations to reason about and/or analyze code in educational settings have been under-explored."" and ""MCQs containing code snippets are not answered as successfully as those that only contain natural language."" and ""MCQs that require analysis and/or reasoning Keyphrase: ""Limited code analysis capabilities""",1
arXIv2023,From Copilot to Pilot: Towards AI Supported Software Development,Yes.,4,"""Moving beyond code completion to AI-supported software engineering will require an AI system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs."" and ""We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid",2023,2023-03-07T18:56:52Z,"Keyphrase: ""Limited understanding of code smells and language idioms""","""Moving beyond code completion to AI-supported software engineering will require an AI system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs."" and ""We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid Keyphrase: ""Limited understanding of code smells and language idioms""",7
arXIv2023,Exploring the Feasibility of ChatGPT for Event Extraction,Yes.,5,"""While ChatGPT has demonstrated impressive results in tasks like machine translation, text summarization, and question answering, it presents challenges when used for complex tasks like event extraction."" and ""Our usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience. Besides, ChatGPT is",2023,2023-03-07T12:03:58Z,"Keyphrase: ""Limited robustness for complex tasks""","""While ChatGPT has demonstrated impressive results in tasks like machine translation, text summarization, and question answering, it presents challenges when used for complex tasks like event extraction."" and ""Our usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience. Besides, ChatGPT is Keyphrase: ""Limited robustness for complex tasks""",6
arXIv2023,Stylometric Detection of AI-Generated Text in Twitter Timelines,Yes.,5,"""However, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the AI starts to generate tweets in a given Twitter timeline.""",2023,2023-03-07T07:26:09Z,"Keyphrase: ""Difficulty in detecting short text generation""","""However, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the AI starts to generate tweets in a given Twitter timeline."" Keyphrase: ""Difficulty in detecting short text generation""",4
arXIv2023,CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,Yes.,5,"""a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation.""",2023,2023-03-07T03:23:14Z,"Keyphrase: ""Factuality issues""","""a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation."" Keyphrase: ""Factuality issues""",0
arXIv2023,Towards Zero-Shot Functional Compositionality of Language Models,Yes.,4,"""Despite such success, in this paper, we argue that current paradigms of working with PLMs are neglecting a critical aspect of modeling human intelligence",2023,2023-03-06T13:15:25Z,"Keyphrase: ""Neglecting human intelligence modeling""","""Despite such success, in this paper, we argue that current paradigms of working with PLMs are neglecting a critical aspect of modeling human intelligence Keyphrase: ""Neglecting human intelligence modeling""",1
arXIv2023,MathPrompter: Mathematical Reasoning using Large Language Models,Yes.,5,"""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers."" and ""we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption.""",2023,2023-03-04T04:43:49Z,"Keyphrase: ""Limited performance in arithmetic reasoning""","""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers."" and ""we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption."" Keyphrase: ""Limited performance in arithmetic reasoning""",1
arXIv2023,Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines,Yes.,5,"""we question whether such models can guarantee factual accuracy"" and ""we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models.""",2023,2023-03-03T04:27:44Z,"Keyphrase: ""Questionable factual accuracy""","""we question whether such models can guarantee factual accuracy"" and ""we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models."" Keyphrase: ""Questionable factual accuracy""",0
arXIv2023,Mixture of Soft Prompts for Controllable Data Generation,Yes.,4,"""structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations.""",2023,2023-03-02T21:13:56Z,"Keyphrase: ""Struggle with structured prediction tasks""","""structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations."" Keyphrase: ""Struggle with structured prediction tasks""",1
arXIv2023,Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents,Yes.,5,"""Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require.""",2023,2023-03-01T22:58:50Z,"Keyphrase: ""Challenges in real-world embodiment""","""Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require."" Keyphrase: ""Challenges in real-world embodiment""",1
arXIv2023,R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents,Yes.,5,"""Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely.""",2023,2023-03-01T18:46:40Z,"Keyphrase: ""Error hallucination in code generation""","""Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely."" Keyphrase: ""Error hallucination in code generation""",0
arXIv2023,How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks,Yes.,5,"""Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language",2023,2023-03-01T07:39:01Z,"Keyphrase: ""Robustness degradation""","""Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language Keyphrase: ""Robustness degradation""",5
arXIv2023,Systematic Rectification of Language Models via Dead-end Analysis,Yes.,4,"""With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses."" and ""Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse.""",2023,2023-02-27T17:47:53Z,"Keyphrase: ""Generation of toxic discourse""","""With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses."" and ""Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse."" Keyphrase: ""Generation of toxic discourse""",2
arXIv2023,The (ab)use of Open Source Code to Train Large Language Models,Yes.,4,"""LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization.""",2023,2023-02-27T11:34:53Z,"Keyphrase: ""Memorization of unfiltered data""","""LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization."" Keyphrase: ""Memorization of unfiltered data""",8
arXIv2023,Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback,Yes.,5,"""applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge.""",2023,2023-02-24T18:48:43Z,"Keyphrase: ""Hallucination and lack of external knowledge integration""","""applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge."" Keyphrase: ""Hallucination and lack of external knowledge integration""",0
arXIv2023,An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP),Yes.,5,"""We found that ChatGPT's performance changes dramatically based on the requirement to show its work, failing 20% of the time when it provides work compared with 84% when it does not."" and ""the probability of failure increases linearly with the number of addition and subtraction operations.""",2023,2023-02-23T16:06:16Z,"Keyphrase: ""Inconsistent performance based on requirements""","""We found that ChatGPT's performance changes dramatically based on the requirement to show its work, failing 20% of the time when it provides work compared with 84% when it does not."" and ""the probability of failure increases linearly with the number of addition and subtraction operations."" Keyphrase: ""Inconsistent performance based on requirements""",6
arXIv2023,"ChatGPT: Jack of all trades, master of none",Yes.,5,"""Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed",2023,2023-02-21T15:20:37Z,"Keyphrase: ""Lower quality in semantic tasks""","""Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed Keyphrase: ""Lower quality in semantic tasks""",6
arXIv2023,Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints,Yes.,5,"""We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures.""",2023,2023-02-17T23:30:28Z,"Keyphrase: ""Limited generative capabilities""","""We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures."" Keyphrase: ""Limited generative capabilities""",4
arXIv2023,"Complex QA and language models hybrid architectures, Survey",Yes.,5,"""Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA."" and ""integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (",2023,2023-02-17T18:31:31Z,"Keyphrase: ""Complex integration and task complexity""","""Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA."" and ""integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy ( Keyphrase: ""Complex integration and task complexity""",4
arXIv2023,"How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice, Education, and Research? An Exploratory Study",Yes.,4,"""By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well",2023,2023-02-17T15:48:37Z,"Keyphrase: ""Limited performance on structured tasks""","""By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well Keyphrase: ""Limited performance on structured tasks""",6
arXIv2023,Commonsense Reasoning for Conversational AI: A Survey of the State of the Art,Yes.,4,"""state-of-the-art models still struggle with tasks that involve higher levels of reasoning - including commonsense reasoning that humans find trivial."" and ""the paper presents preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions.""",2023,2023-02-15T19:55:57Z,"Keyphrase: ""Limited commonsense reasoning""","""state-of-the-art models still struggle with tasks that involve higher levels of reasoning - including commonsense reasoning that humans find trivial."" and ""the paper presents preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions."" Keyphrase: ""Limited commonsense reasoning""",1
arXIv2023,Speculative Decoding with Big Little Decoder,Yes.,5,"""However, these models have long inference latency, which limits their deployment and makes them prohibitively expensive for various real-time applications.""",2023,2023-02-15T18:55:29Z,"Keyphrase: ""Long inference latency""","""However, these models have long inference latency, which limits their deployment and makes them prohibitively expensive for various real-time applications."" Keyphrase: ""Long inference latency""",4
arXIv2023,A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning,Yes.,5,"""although it demonstrates some level of rational decision-making, many of its decisions violate at least one of the axioms even under reasonable constructions of preferences, bets, and decision-making prompts. ChatGPT's outputs on such problems generally tended to be unpredictable.""",2023,2023-02-15T05:04:49Z,"Keyphrase: ""Violates decision-making axioms""","""although it demonstrates some level of rational decision-making, many of its decisions violate at least one of the axioms even under reasonable constructions of preferences, bets, and decision-making prompts. ChatGPT's outputs on such problems generally tended to be unpredictable."" Keyphrase: ""Violates decision-making axioms""",2
arXIv2023,Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models,Yes.,4,"""However, safely deploying them in real world applications is challenging because they generate toxic content.""",2023,2023-02-14T23:00:42Z,"Keyphrase: ""Generating toxic content""","""However, safely deploying them in real world applications is challenging because they generate toxic content."" Keyphrase: ""Generating toxic content""",2
arXIv2023,BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models,Yes.,4,"""Pretrained Language Models (PLMs) harbor inherent social biases that can result in harmful real-world implications.""",2023,2023-02-14T22:07:57Z,"Keyphrase: ""Inherent social bias""","""Pretrained Language Models (PLMs) harbor inherent social biases that can result in harmful real-world implications."" Keyphrase: ""Inherent social bias""",3
arXIv2023,Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge,Yes.,4,"""Previous work has shown that Large Language Models are susceptible to so-called data extraction attacks. This allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications.""",2023,2023-02-13T18:00:44Z,"Keyphrase: ""Privacy implications from data extraction attack""","""Previous work has shown that Large Language Models are susceptible to so-called data extraction attacks. This allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications."" Keyphrase: ""Privacy implications from data extraction attack""",8
arXIv2023,Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks,Yes.,5,"""Unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models."" and ""instruction-following LLMs can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors."" and ""Together, our findings suggest that LLMs will increasingly attract more sophisticated adversaries and attacks,",2023,2023-02-11T15:57:44Z,"Keyphrase: ""Dual-use risk amplification""","""Unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models."" and ""instruction-following LLMs can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors."" and ""Together, our findings suggest that LLMs will increasingly attract more sophisticated adversaries and attacks, Keyphrase: ""Dual-use risk amplification""",2
arXIv2023,Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech,Yes.,5,"""We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.""",2023,2023-02-11T03:13:54Z,"Keyphrase: ""Implicit hateful speech""","""We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research."" Keyphrase: ""Implicit hateful speech""",2
arXIv2023,Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models,Yes.,5,"""it has been difficult to prevent semantic hallucinations in generative Large Language Models,"" and ""Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency.""",2023,2023-02-11T02:43:34Z,"Keyphrase: ""Semantic hallucination""","""it has been difficult to prevent semantic hallucinations in generative Large Language Models,"" and ""Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency."" Keyphrase: ""Semantic hallucination""",0
arXIv2023,FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models,Yes.,4,"""Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on.""",2023,2023-02-10T20:54:10Z,"Keyphrase: ""Social bias based on race and gender""","""Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on."" Keyphrase: ""Social bias based on race and gender""",3
arXIv2023,Large Language Models for Code: Security Hardening and Adversarial Testing,Yes.,4,"""However, LMs lack awareness of security and are found to frequently produce unsafe code.""",2023,2023-02-10T15:28:55Z,"Keyphrase: ""Lack of security awareness""","""However, LMs lack awareness of security and are found to frequently produce unsafe code."" Keyphrase: ""Lack of security awareness""",2
arXIv2023,Translating Natural Language to Planning Goals with Large-Language Models,Yes.,5,"""Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks."" and ""However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used.""",2023,2023-02-10T09:17:52Z,"Keyphrase: ""Limited reasoning and planning abilities""","""Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks."" and ""However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used."" Keyphrase: ""Limited reasoning and planning abilities""",1
arXIv2023,In-Context Learning with Many Demonstration Examples,Yes.,5,"""existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored.""",2023,2023-02-09T20:53:12Z,"Keyphrase: ""Bottlenecked by memory and computational cost""","""existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored."" Keyphrase: ""Bottlenecked by memory and computational cost""",4
arXIv2023,Training-free Lexical Backdoor Attacks on Language Models,Yes.,4,"""language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors.""",2023,2023-02-08T15:18:51Z,"Keyphrase: ""Vulnerability to backdoor attacks""","""language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors."" Keyphrase: ""Vulnerability to backdoor attacks""",2
arXIv2023,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",Yes.,5,"""We find that it is better at understanding non-Latin script languages than generating them. ... ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense",2023,2023-02-08T12:35:34Z,"Keyphrase: ""Limited non-Latin script language understanding""","""We find that it is better at understanding non-Latin script languages than generating them. ... ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense Keyphrase: ""Limited non-Latin script language understanding""",1
arXIv2023,CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models,Yes.,5,"""The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure.""",2023,2023-02-08T11:54:07Z,"Keyphrase: ""Vulnerability propagation""","""The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure."" Keyphrase: ""Vulnerability propagation""",2
arXIv2023,Reliable Natural Language Understanding with Large Language Models and Answer Set Programming,Yes.,5,"""they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question.""",2023,2023-02-07T22:37:21Z,"Keyphrase: ""Limited reasoning ability""","""they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question."" Keyphrase: ""Limited reasoning ability""",1
arXIv2023,Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis,Yes.,5,"""There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost - both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time.""",2023,2023-02-07T21:51:05Z,"Keyphrase: ""Limited input token length handling""","""There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost - both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time."" Keyphrase: ""Limited input token length handling""",4
arXIv2023,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,Yes.,4,"""Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding.""",2023,2023-02-06T10:01:08Z,"Keyphrase: ""Lack of grounding in knowledge environment""","""Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding."" Keyphrase: ""Lack of grounding in knowledge environment""",1
arXIv2023,A Categorical Archive of ChatGPT Failures,Yes.,5,"""A comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted.""",2023,2023-02-06T04:21:59Z,"Keyphrase: ""Failure in reasoning and factual errors""","""A comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted."" Keyphrase: ""Failure in reasoning and factual errors""",6
arXIv2023,Nationality Bias in Text Generation,Yes.,4,"""This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms."" and ""To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering.""",2023,2023-02-05T19:15:33Z,"Keyphrase: ""Propagation of societal bias""","""This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms."" and ""To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering."" Keyphrase: ""Propagation of societal bias""",3
arXIv2023,Conditioning Predictive Models: Risks and Strategies,Yes.,4,"""Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us.""",2023,2023-02-02T00:06:36Z,"Keyphrase: ""Safety concerns and unpredictability""","""Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us."" Keyphrase: ""Safety concerns and unpredictability""",2
arXIv2023,Co-Writing with Opinionated Language Models Affects Users' Views,Yes.,4,"""If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale."" and ""We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully.""",2023,2023-02-01T16:26:32Z,"Keyphrase: ""Biased viewpoint generation""","""If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale."" and ""We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully."" Keyphrase: ""Biased viewpoint generation""",3
arXIv2023,Analyzing Leakage of Personally Identifiable Information in Language Models,Yes.,5,"""Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks"" and ""showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences.""",2023,2023-02-01T16:04:48Z,"Keyphrase: ""Information leakage risks""","""Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks"" and ""showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences."" Keyphrase: ""Information leakage risks""",8
arXIv2023,Conversational Automated Program Repair,Yes.,5,"""prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases.""",2023,2023-01-30T19:22:36Z,"Keyphrase: ""Generating incorrect patches""","""prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases."" Keyphrase: ""Generating incorrect patches""",7
arXIv2023,On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex,Yes.,5,"""Despite these advancements, existing fine-tuned neural semantic parsers are susceptible to adversarial attacks on natural-language inputs."" and ""this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data."" and ""Our results demonstrate that the state-of-the-art (SOTA) code-language models are",2023,2023-01-30T13:21:00Z,"Keyphrase: ""Susceptible to adversarial attacks""","""Despite these advancements, existing fine-tuned neural semantic parsers are susceptible to adversarial attacks on natural-language inputs."" and ""this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data."" and ""Our results demonstrate that the state-of-the-art (SOTA) code-language models are Keyphrase: ""Susceptible to adversarial attacks""",2
arXIv2023,"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",Yes.,5,"""Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility."" and ""we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies.""",2023,2023-01-30T13:20:48Z,"Keyphrase: ""Ethical risks and societal dangers""","""Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility."" and ""we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies."" Keyphrase: ""Ethical risks and societal dangers""",3
arXIv2023,A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction,Yes.,4,"""LLMs must overcome frequency biases in order to master such constructions.""",2023,2023-01-29T22:29:55Z,"Keyphrase: ""Frequency bias""","""LLMs must overcome frequency biases in order to master such constructions."" Keyphrase: ""Frequency bias""",3
arXIv2023,Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes,Yes.,4,"""We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation.""",2023,2023-01-29T15:52:33Z,"Keyphrase: ""Difficulty in generating structured output""","""We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation."" Keyphrase: ""Difficulty in generating structured output""",0
arXIv2023,Context-Aware Differential Privacy for Language Modeling,Yes.,4,"""A critical challenge pertains to how much information these models retain and leak about the training data.""",2023,2023-01-28T20:06:16Z,"Keyphrase: ""Data leakage concerns""","""A critical challenge pertains to how much information these models retain and leak about the training data."" Keyphrase: ""Data leakage concerns""",8
arXIv2023,Learning the Effects of Physical Actions in a Multi-modal Environment,Yes.,5,"""Large Language Models (LLMs) handle physical commonsense information inadequately. As a result of being trained in a disembodied setting, LLMs often fail to predict an action's outcome in a given environment.""",2023,2023-01-27T16:49:52Z,"Keyphrase: ""Limited physical commonsense understanding""","""Large Language Models (LLMs) handle physical commonsense information inadequately. As a result of being trained in a disembodied setting, LLMs often fail to predict an action's outcome in a given environment."" Keyphrase: ""Limited physical commonsense understanding""",1
arXIv2023,ThoughtSource: A central hub for large language model reasoning data,Yes.,5,"""LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases.""",2023,2023-01-27T08:45:53Z,"Keyphrase: ""Limited complex reasoning""","""LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases."" Keyphrase: ""Limited complex reasoning""",1
arXIv2023,Causal Reasoning of Entities and Events in Procedural Texts,Yes.,5,"""We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1.""",2023,2023-01-26T01:43:17Z,"Keyphrase: ""Limited performance compared to humans""","""We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1."" Keyphrase: ""Limited performance compared to humans""",6
arXIv2023,Opportunities and Challenges in Neural Dialog Tutoring,Yes.,5,"""We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios."" Additionally, ""both models and ground-truth annotations exhibit low performance in terms of equitable tutoring,"" and ""a significantly large number of model reasoning errors in 45% of conversations.""",2023,2023-01-24T11:00:17Z,"Keyphrase: ""Limited performance in constrained learning scenarios""","""We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios."" Additionally, ""both models and ground-truth annotations exhibit low performance in terms of equitable tutoring,"" and ""a significantly large number of model reasoning errors in 45% of conversations."" Keyphrase: ""Limited performance in constrained learning scenarios""",6
arXIv2023,Dissociating language and thought in large language models,Yes.,5,"""Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules.""",2023,2023-01-16T22:41:19Z,"Keyphrase: ""Spotty performance without specialized fine-tuning""","""Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules."" Keyphrase: ""Spotty performance without specialized fine-tuning""",6
arXIv2023,TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World,Yes.,4,"""Experimental results indicate that the models incorporating large language models (LLM) can generate more diverse responses, while the model utilizing knowledge graphs to introduce external knowledge performs the best overall. Furthermore, no existing model can solve all the above challenges well. There is still a large room for",2023,2023-01-14T10:18:22Z,"Keyphrase: ""Limited incorporation of external knowledge""","""Experimental results indicate that the models incorporating large language models (LLM) can generate more diverse responses, while the model utilizing knowledge graphs to introduce external knowledge performs the best overall. Furthermore, no existing model can solve all the above challenges well. There is still a large room for Keyphrase: ""Limited incorporation of external knowledge""",0
arXIv2023,AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT,Yes.,5,"""We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary.""",2023,2023-01-10T16:57:16Z,"Keyphrase: ""Inaccurate paraphrasing""","""We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary."" Keyphrase: ""Inaccurate paraphrasing""",0
arXIv2023,Can Large Language Models Change User Preference Adversarially?,Yes.,4,"""there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially"" and ""The issue of lack of interpretability in these models in adversarial settings remains largely unsolved.""",2023,2023-01-05T18:49:21Z,"Keyphrase: ""Lack of interpretability and vulnerability to adversarial manipulation""","""there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially"" and ""The issue of lack of interpretability in these models in adversarial settings remains largely unsolved."" Keyphrase: ""Lack of interpretability and vulnerability to adversarial manipulation""",2
arXIv2023,"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",Yes.,4,"""its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases.""",2023,2023-01-05T07:13:13Z,"Keyphrase: ""Bias and limitations in decision-making""","""its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases."" Keyphrase: ""Bias and limitations in decision-making""",3
arXIv2023,RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,Yes.,4,"""Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents.""",2023,2023-12-31T04:43:45Z,"Keyphrase: ""Unsupported claims""","""Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents."" Keyphrase: ""Unsupported claims""",0
arXIv2023,The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness,Yes.,5,"""reveals several interesting and important findings, such as (a) the widely popular 'self-checking' techniques indeed improve the safety against unsafe inputs, but this comes at the cost of extreme over-defensiveness on the safe inputs, (b) providing a safety instruction along with in-context exemplars (of both safe and unsafe inputs) consistently improves safety and also mitigates undue",2023,2023-12-30T17:37:06Z,"Keyphrase: ""Overdefensiveness at the cost of safety""","""reveals several interesting and important findings, such as (a) the widely popular 'self-checking' techniques indeed improve the safety against unsafe inputs, but this comes at the cost of extreme over-defensiveness on the safe inputs, (b) providing a safety instruction along with in-context exemplars (of both safe and unsafe inputs) consistently improves safety and also mitigates undue Keyphrase: ""Overdefensiveness at the cost of safety""",2
arXIv2023,Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation,Yes.,5,"""The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity."" and ""Our results reveal that both the direct-use of decoder-only LLMs (i",2023,2023-12-30T16:56:24Z,"Keyphrase: ""Prone to hallucination""","""The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity."" and ""Our results reveal that both the direct-use of decoder-only LLMs (i Keyphrase: ""Prone to hallucination""",0
arXIv2023,Teach Large Language Models to Forget Privacy,Yes.,4,"""Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern.""",2023,2023-12-30T01:26:42Z,"Keyphrase: ""Privacy leakage risk""","""Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern."" Keyphrase: ""Privacy leakage risk""",8
arXIv2023,Jatmo: Prompt Injection Defense by Task-Specific Finetuning,Yes.,5,"""LLMs are vulnerable to prompt-injection attacks",2023,2023-12-29T16:37:53Z,"Keyphrase: ""Vulnerability to prompt injection attack""","""LLMs are vulnerable to prompt-injection attacks Keyphrase: ""Vulnerability to prompt injection attack""",2
arXIv2023,Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models,Yes.,4,"""preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks"" and ""we identify common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models.""",2023,2023-12-29T15:57:49Z,"Keyphrase: ""Lagging in commonsense reasoning""","""preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks"" and ""we identify common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models."" Keyphrase: ""Lagging in commonsense reasoning""",1
arXIv2023,Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception,Yes.,5,"""However, the lack of dimension knowledge and quantity-related benchmarks has resulted in low performance of LLMs.""",2023,2023-12-29T09:29:37Z,"Keyphrase: ""Limited knowledge base""","""However, the lack of dimension knowledge and quantity-related benchmarks has resulted in low performance of LLMs."" Keyphrase: ""Limited knowledge base""",7
arXIv2023,Spike No More: Stabilizing the Pre-training of Large Language Models,Yes.,5,"""Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training.""",2023,2023-12-28T08:53:27Z,"Keyphrase: ""Pretraining loss spikes""","""Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training."" Keyphrase: ""Pretraining loss spikes""",5
arXIv2023,LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner States Analysis,Yes.,5,"""a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality.""",2023,2023-12-27T01:44:47Z,"Keyphrase: ""Divergence from factual reality""","""a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality."" Keyphrase: ""Divergence from factual reality""",0
arXIv2023,Task Contamination: Language Models May Not Be Few-Shot Anymore,Yes.,5,"""However, their success in zero-shot and few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined."" and ""This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets released prior to the LLMs' training data creation date."" and ""Importantly, we find that for",2023,2023-12-26T21:17:46Z,"Keyphrase: ""Task contamination in zero-shot/few-shot settings""","""However, their success in zero-shot and few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined."" and ""This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets released prior to the LLMs' training data creation date."" and ""Importantly, we find that for Keyphrase: ""Task contamination in zero-shot/few-shot settings""",6
arXIv2023,MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks,Yes.,5,"""Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems.""",2023,2023-12-26T08:49:57Z,"Keyphrase: ""Limited performance on challenging tasks""","""Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems."" Keyphrase: ""Limited performance on challenging tasks""",7
arXIv2023,KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph,Yes.,5,"""LLM still suffers from knowledge limitation. Especially in scenarios that require long logical chains or complex reasoning, the hallucination and knowledge limitation of LLM limit its performance in question answering (QA).""",2023,2023-12-26T04:22:56Z,"Keyphrase: ""Limited knowledge and reasoning capabilities""","""LLM still suffers from knowledge limitation. Especially in scenarios that require long logical chains or complex reasoning, the hallucination and knowledge limitation of LLM limit its performance in question answering (QA)."" Keyphrase: ""Limited knowledge and reasoning capabilities""",1
arXIv2023,Reducing LLM Hallucinations using Epistemic Neural Networks,Yes.,5,"""Reducing and detecting hallucinations in large language models is an open research problem.""",2023,2023-12-25T01:17:01Z,"Keyphrase: ""Hallucination detection challenge""","""Reducing and detecting hallucinations in large language models is an open research problem."" Keyphrase: ""Hallucination detection challenge""",0
arXIv2023,The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective,Yes.,5,"""we empirically and theoretically analyze the challenges of conducting LLM-simulated experiments, and explore potential solutions,"" and ""variations in the treatment included in the prompt (e.g., price of focal product) can cause variations in unspecified confounding factors,"" and ""suggesting this endogeneity issue generalizes to other contexts and won't be fully resolved by merely improving the training data.""",2023,2023-12-24T16:32:35Z,"Keyphrase: ""Unspecified confounding factors""","""we empirically and theoretically analyze the challenges of conducting LLM-simulated experiments, and explore potential solutions,"" and ""variations in the treatment included in the prompt (e.g., price of focal product) can cause variations in unspecified confounding factors,"" and ""suggesting this endogeneity issue generalizes to other contexts and won't be fully resolved by merely improving the training data."" Keyphrase: ""Unspecified confounding factors""",0
arXIv2023,A Group Fairness Lens for Large Language Models,Yes.,4,"""The rapid advancement of large language models has revolutionized various applications but also raised crucial concerns about their potential to perpetuate biases and unfairness when deployed in social media contexts."" and ""Extensive evaluations of popular LLMs reveal inherent safety concerns.""",2023,2023-12-24T13:25:15Z,"Keyphrase: ""Inherent safety concerns""","""The rapid advancement of large language models has revolutionized various applications but also raised crucial concerns about their potential to perpetuate biases and unfairness when deployed in social media contexts."" and ""Extensive evaluations of popular LLMs reveal inherent safety concerns."" Keyphrase: ""Inherent safety concerns""",2
arXIv2023,Towards Consistent Language Models Using Declarative Constraints,Yes.,5,"""However, they often return incorrect and inconsistent answers to input questions."" and ""Due to the complexity and uninterpretability of the internally learned representations, it is challenging to modify language models such that they provide correct and consistent results.""",2023,2023-12-24T12:53:07Z,"Keyphrase: ""Uninterpretable internal representations""","""However, they often return incorrect and inconsistent answers to input questions."" and ""Due to the complexity and uninterpretability of the internally learned representations, it is challenging to modify language models such that they provide correct and consistent results."" Keyphrase: ""Uninterpretable internal representations""",7
arXIv2023,Fairness-Aware Structured Pruning in Transformers,Yes.,4,"""The increasing size of large language models (LLMs) has introduced challenges in their training and inference."" and ""existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs",2023,2023-12-24T03:57:52Z,"Keyphrase: ""Neglect of responsible use aspects""","""The increasing size of large language models (LLMs) has introduced challenges in their training and inference."" and ""existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs Keyphrase: ""Neglect of responsible use aspects""",5
arXIv2023,Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems,Yes.,4,"""the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput.""",2023,2023-12-23T11:57:53Z,"Keyphrase: ""High computational intensity and memory consumption""","""the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput."" Keyphrase: ""High computational intensity and memory consumption""",4
arXIv2023,NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,Yes.,4,"""current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance.""",2023,2023-12-22T18:07:44Z,"Keyphrase: ""Risk of overfitting and benchmark tailoring""","""current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance."" Keyphrase: ""Risk of overfitting and benchmark tailoring""",7
arXIv2023,Robust Knowledge Extraction from Large Language Models using Social Choice Theory,Yes.,5,"""they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times.""",2023,2023-12-22T17:57:29Z,"Keyphrase: ""Inconsistent query results""","""they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times."" Keyphrase: ""Inconsistent query results""",6
arXIv2023,Large Language Model (LLM) Bias Index -- LLMBI,Yes.,4,"""This research introduces a novel metric, LLMBI, to systematically measure and mitigate biases potentially skewing model responses"" and ""The research reveals LLMs, whilst demonstrating impressive capabilities in text generation, exhibit varying degrees of bias across different dimensions.""",2023,2023-12-22T15:38:13Z,"Keyphrase: ""Bias variability""","""This research introduces a novel metric, LLMBI, to systematically measure and mitigate biases potentially skewing model responses"" and ""The research reveals LLMs, whilst demonstrating impressive capabilities in text generation, exhibit varying degrees of bias across different dimensions."" Keyphrase: ""Bias variability""",3
arXIv2023,Empowering Working Memory for Large Language Model Agents,Yes.,5,"""Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning.""",2023,2023-12-22T05:59:00Z,"Keyphrase: ""Limited memory retention""","""Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning."" Keyphrase: ""Limited memory retention""",5
arXIv2023,Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models,Yes.,5,"""these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides"" and ""Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different",2023,2023-12-22T00:31:46Z,"Keyphrase: ""Hallucination and lack of faithfulness""","""these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides"" and ""Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different Keyphrase: ""Hallucination and lack of faithfulness""",0
arXIv2023,Context-aware Decoding Reduces Hallucination in Query-focused Summarization,Yes.,5,"""However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs.""",2023,2023-12-21T23:42:13Z,"Keyphrase: ""Risk of hallucination""","""However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs."" Keyphrase: ""Risk of hallucination""",0
arXIv2023,SimLM: Can Language Models Infer Parameters of Physical Systems?,Yes.,5,"""Our experiments suggest that they are not inherently suited to this task, even for simple systems.""",2023,2023-12-21T12:05:19Z,"Keyphrase: ""Limited task suitability""","""Our experiments suggest that they are not inherently suited to this task, even for simple systems."" Keyphrase: ""Limited task suitability""",1
arXIv2023,On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning,Yes.,5,"""both paradigms are prone to suffer from the critical problem of overconfidence (i.e., miscalibration),"" and ""the problem of miscalibration exists across all learning methods in low-resource scenarios.""",2023,2023-12-21T11:55:10Z,"Keyphrase: ""Overconfidence and miscalibration""","""both paradigms are prone to suffer from the critical problem of overconfidence (i.e., miscalibration),"" and ""the problem of miscalibration exists across all learning methods in low-resource scenarios."" Keyphrase: ""Overconfidence and miscalibration""",6
arXIv2023,"Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature",Yes.,5,"""We observed substantial differences in intraday (nondeterminism) and across day (drift) results,"" and ""Nondeterminism and drift within LLMs must be assessed and monitored when introducing LLM based functionality into clinical workflows.""",2023,2023-12-21T01:56:00Z,"Keyphrase: ""Intraday nondeterminism and drift""","""We observed substantial differences in intraday (nondeterminism) and across day (drift) results,"" and ""Nondeterminism and drift within LLMs must be assessed and monitored when introducing LLM based functionality into clinical workflows."" Keyphrase: ""Intraday nondeterminism and drift""",0
arXIv2023,Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models,Yes.,5,"""the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content.""",2023,2023-12-21T01:08:39Z,"Keyphrase: ""Limited contextual understanding""","""the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content."" Keyphrase: ""Limited contextual understanding""",1
arXIv2023,CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models,Yes.,5,"""Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting.""",2023,2023-12-20T09:06:18Z,"Keyphrase: ""Limited zero-shot performance""","""Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting."" Keyphrase: ""Limited zero-shot performance""",6
arXIv2023,Learning and Forgetting Unsafe Examples in Large Language Models,Yes.,4,"""We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness,"" and ""aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content.""",2023,2023-12-20T03:18:50Z,"Keyphrase: ""Learning and retention of unsafe content""","""We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness,"" and ""aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content."" Keyphrase: ""Learning and retention of unsafe content""",2
arXIv2023,Bypassing the Safety Training of Open-Source LLMs with Priming Attacks,Yes.,5,"""we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training.""",2023,2023-12-19T16:47:12Z,"Keyphrase: ""Vulnerability to optimization-free attacks""","""we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training."" Keyphrase: ""Vulnerability to optimization-free attacks""",2
arXIv2023,On Early Detection of Hallucinations in Factual Question Answering,Yes.,5,"""While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks like search and summarization, hallucinations remain a major impediment towards gaining user trust.""",2023,2023-12-19T14:35:04Z,"Keyphrase: ""Hallucination remains a major impediment""","""While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks like search and summarization, hallucinations remain a major impediment towards gaining user trust."" Keyphrase: ""Hallucination remains a major impediment""",0
arXIv2023,Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment,Yes.,4,"""the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources.""",2023,2023-12-19T13:31:24Z,"Keyphrase: ""High computational demand""","""the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources."" Keyphrase: ""High computational demand""",4
arXIv2023,Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies,Yes.,4,"""Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae)."" and ""We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLM",2023,2023-12-19T01:28:46Z,"Keyphrase: ""Limited gender diversity support""","""Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae)."" and ""We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLM Keyphrase: ""Limited gender diversity support""",3
arXIv2023,Opportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview,Yes.,4,"""Despite the promising potential of LLMs, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned LLMs, and self-consistency are discussed.""",2023,2023-12-18T20:58:58Z,"Keyphrase: ""Complexity and resource-intensive challenges""","""Despite the promising potential of LLMs, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned LLMs, and self-consistency are discussed."" Keyphrase: ""Complexity and resource-intensive challenges""",4
arXIv2023,Evaluating Language-Model Agents on Realistic Autonomous Tasks,Yes.,4,"""We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks."" and ""we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA.""",2023,2023-12-18T19:27:09Z,"Keyphrase: ""Limited task complexity understanding""","""We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks."" and ""we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA."" Keyphrase: ""Limited task complexity understanding""",1
arXIv2023,Traces of Memorisation in Large Language Models for Code,Yes.,5,"""The content of these datasets is memorised and can be extracted by attackers with data extraction attacks."" and ""We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts.""",2023,2023-12-18T19:12:58Z,"Keyphrase: ""Vulnerability to data extraction attacks""","""The content of these datasets is memorised and can be extracted by attackers with data extraction attacks."" and ""We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts."" Keyphrase: ""Vulnerability to data extraction attacks""",2
arXIv2023,Linear Attention via Orthogonal Memory,Yes.,5,"""most existing linear attention mechanisms suffer from an efficiency degradation problem, leading to inefficiencies in causal language modeling and hindering their application in long-range language models.""",2023,2023-12-18T12:26:27Z,"Keyphrase: ""Efficiency degradation""","""most existing linear attention mechanisms suffer from an efficiency degradation problem, leading to inefficiencies in causal language modeling and hindering their application in long-range language models."" Keyphrase: ""Efficiency degradation""",5
arXIv2023,Retrieval-Augmented Generation for Large Language Models: A Survey,Yes.,5,"""Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes.""",2023,2023-12-18T07:47:33Z,"Keyphrase: ""Nontransparent reasoning""","""Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes."" Keyphrase: ""Nontransparent reasoning""",0
arXIv2023,Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression,Yes.,4,"""However, they come with substantial computational and memory costs. Additionally, they are essentially black-box models, challenging to explain and interpret.""",2023,2023-12-17T12:33:50Z,"Keyphrase: ""High computational and memory costs""","""However, they come with substantial computational and memory costs. Additionally, they are essentially black-box models, challenging to explain and interpret."" Keyphrase: ""High computational and memory costs""",4
arXIv2023,An Evaluation of GPT-4V and Gemini in Online VQA,Yes.,5,"""Our zero-shot performance analysis highlights the types of questions that are most challenging for both models, including questions related to 'puzzling' topic, with 'Identification' user intention, with 'Sheet Music' image type, or labeled as 'hard' by GPT-4.""",2023,2023-12-17T07:38:43Z,"Keyphrase: ""Challenges in zero-shot performance""","""Our zero-shot performance analysis highlights the types of questions that are most challenging for both models, including questions related to 'puzzling' topic, with 'Identification' user intention, with 'Sheet Music' image type, or labeled as 'hard' by GPT-4."" Keyphrase: ""Challenges in zero-shot performance""",6
arXIv2023,LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin,Yes.,5,"""However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs.""",2023,2023-12-15T17:45:06Z,"Keyphrase: ""Damage to world knowledge""","""However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs."" Keyphrase: ""Damage to world knowledge""",5
arXIv2023,Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China,Yes.,4,"""The rising popularity of ChatGPT and other AI-powered large language models (LLMs) has led to increasing studies highlighting their susceptibility to mistakes and biases."" and ""This disparity may stem from Chinese state censorship and US-China geopolitical tensions, which influence the training corpora of GPT bilingual models.""",2023,2023-12-15T16:25:56Z,"Keyphrase: ""Bias and geopolitical influence""","""The rising popularity of ChatGPT and other AI-powered large language models (LLMs) has led to increasing studies highlighting their susceptibility to mistakes and biases."" and ""This disparity may stem from Chinese state censorship and US-China geopolitical tensions, which influence the training corpora of GPT bilingual models."" Keyphrase: ""Bias and geopolitical influence""",3
arXIv2023,Taxonomy-based CheckList for Large Language Model Evaluation,Yes.,4,"""the internal stereotypical representation may affect the fairness of the outputs,"" ""we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA),"" and ""Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation.""",2023,2023-12-15T12:58:07Z,"Keyphrase: ""Bias in unethical behavior""","""the internal stereotypical representation may affect the fairness of the outputs,"" ""we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA),"" and ""Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation."" Keyphrase: ""Bias in unethical behavior""",3
arXIv2023,Extending Context Window of Large Language Models via Semantic Compression,Yes.,5,"""Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts.""",2023,2023-12-15T07:04:33Z,"Keyphrase: ""Text input length limitation""","""Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts."" Keyphrase: ""Text input length limitation""",4
arXIv2023,Marathon: A Race Through the Realm of Long Context with Large Language Models,Yes.,5,"""the existing long context benchmarks are no longer sufficient for evaluating the long context understanding and reasoning capability of large language models.""",2023,2023-12-15T05:30:14Z,"Keyphrase: ""Inadequate long context evaluation""","""the existing long context benchmarks are no longer sufficient for evaluating the long context understanding and reasoning capability of large language models."" Keyphrase: ""Inadequate long context evaluation""",4
arXIv2023,No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models,Yes.,5,"""our work for the first time reveals the acceleration may be vulnerable to Denial-of-Service (DoS) attacks"" and ""evaluate the vulnerability of the skimming acceleration in various LLM architectures.""",2023,2023-12-15T02:42:05Z,"Keyphrase: ""Vulnerability to denial-of-service attacks""","""our work for the first time reveals the acceleration may be vulnerable to Denial-of-Service (DoS) attacks"" and ""evaluate the vulnerability of the skimming acceleration in various LLM architectures."" Keyphrase: ""Vulnerability to denial-of-service attacks""",2
arXIv2023,Towards Verifiable Text Generation with Evolving Memory and Self-Reflection,Yes.,5,"""Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination.""",2023,2023-12-14T16:10:56Z,"Keyphrase: ""Factually incorrect information and hallucination""","""Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination."" Keyphrase: ""Factually incorrect information and hallucination""",0
arXIv2023,Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent,Yes.,5,"""Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation.""",2023,2023-12-14T13:33:50Z,"Keyphrase: ""Limited mathematical problem-solving capacity""","""Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation."" Keyphrase: ""Limited mathematical problem-solving capacity""",1
arXIv2023,Evaluating Large Language Models for Health-related Queries with Presuppositions,Yes.,5,"""Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios.""",2023,2023-12-14T10:35:13Z,"Keyphrase: ""Moderate factual accuracy""","""Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios."" Keyphrase: ""Moderate factual accuracy""",0
arXIv2023,Large Language Model Enhanced Multi-Agent Systems for 6G Communications,Yes.,4,"""directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities.""",2023,2023-12-13T02:35:57Z,"Keyphrase: ""Limited logical reasoning""","""directly applying native LLMs in 6G encounters various challenges, such as a lack of private communication data and knowledge, limited logical reasoning, evaluation, and refinement abilities."" Keyphrase: ""Limited logical reasoning""",1
arXIv2023,Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection,Yes.,4,"""Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence."" and ""we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias.""",2023,2023-12-12T18:05:46Z,"Keyphrase: ""Bias in input-label mapping""","""Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence."" and ""we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias."" Keyphrase: ""Bias in input-label mapping""",5
arXIv2023,FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs,Yes.,4,"""Training large language models (LLMs) is a costly endeavour in terms of time and computational resources,"" and ""We evaluate the performance-fairness trade-off for SISA, and empirically demonstrate that SISA can indeed reduce fairness in LLMs.""",2023,2023-12-12T16:44:47Z,"Keyphrase: ""Costly training and fairness tradeoff""","""Training large language models (LLMs) is a costly endeavour in terms of time and computational resources,"" and ""We evaluate the performance-fairness trade-off for SISA, and empirically demonstrate that SISA can indeed reduce fairness in LLMs."" Keyphrase: ""Costly training and fairness tradeoff""",3
arXIv2023,Sequential Planning in Large Partially Observable Environments guided by LLMs,Yes.,4,"""But they still struggle with exploration and get stuck in local optima. Their planning capabilities are limited by the limited reasoning capability of the foundational LLMs on text data.""",2023,2023-12-12T15:36:59Z,"Keyphrase: ""Limited reasoning capability""","""But they still struggle with exploration and get stuck in local optima. Their planning capabilities are limited by the limited reasoning capability of the foundational LLMs on text data."" Keyphrase: ""Limited reasoning capability""",1
arXIv2023,Multilingual large language models leak human stereotypes across language boundaries,Yes.,5,"""Previous research has shown that the presence of stereotypes and biases in monolingual large language models can be attributed to the nature of their training data, which is collected from humans and reflects societal biases."" and ""This raises the question",2023,2023-12-12T10:24:17Z,"Keyphrase: ""Stereotype bias in training data""","""Previous research has shown that the presence of stereotypes and biases in monolingual large language models can be attributed to the nature of their training data, which is collected from humans and reflects societal biases."" and ""This raises the question Keyphrase: ""Stereotype bias in training data""",3
arXIv2023,Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications,Yes.,5,"""we explore the challenges inherent to Large Language Models (LLMs) like GPT-4, particularly their propensity for hallucinations, logic mistakes, and incorrect conclusions when tasked with answering complex questions.""",2023,2023-12-12T08:43:20Z,"Keyphrase: ""Hallucination and logic mistakes""","""we explore the challenges inherent to Large Language Models (LLMs) like GPT-4, particularly their propensity for hallucinations, logic mistakes, and incorrect conclusions when tasked with answering complex questions."" Keyphrase: ""Hallucination and logic mistakes""",0
arXIv2023,Hallucination Augmented Contrastive Learning for Multimodal Large Language Model,Yes.,5,"""MLLMs still face a fundamental limitation of hallucinations, where they tend to generate erroneous or fabricated information.""",2023,2023-12-12T04:05:15Z,"Keyphrase: ""Erroneous hallucination""","""MLLMs still face a fundamental limitation of hallucinations, where they tend to generate erroneous or fabricated information."" Keyphrase: ""Erroneous hallucination""",0
arXIv2023,Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack,Yes.,5,"""Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks."" and ""Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content.""",2023,2023-12-12T01:39:29Z,"Keyphrase: ""Safety misalignment""","""Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks."" and ""Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content."" Keyphrase: ""Safety misalignment""",2
arXIv2023,Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?,Yes.,4,"""However, this impressive performance comes with inherent limitations, such as the tendency to perpetuate stereotypical biases or fabricate non-existent facts.""",2023,2023-12-11T18:59:09Z,"Keyphrase: ""Perpetuation of stereotypical bias""","""However, this impressive performance comes with inherent limitations, such as the tendency to perpetuate stereotypical biases or fabricate non-existent facts."" Keyphrase: ""Perpetuation of stereotypical bias""",3
arXIv2023,Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding,Yes.,5,"""However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention.""",2023,2023-12-11T06:35:33Z,"Keyphrase: ""Toxicity and hallucination challenges""","""However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention."" Keyphrase: ""Toxicity and hallucination challenges""",0
arXIv2023,Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs,Yes.,5,"""However, this knowledge is inherently limited, relying heavily on the characteristics of the training data."" and ""we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.""",2023,2023-12-10T16:52:00Z,"Keyphrase: ""Limited knowledge acquisition""","""However, this knowledge is inherently limited, relying heavily on the characteristics of the training data."" and ""we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem."" Keyphrase: ""Limited knowledge acquisition""",5
arXIv2023,Large Multimodal Model Compression via Efficient Pruning and Distillation at AntGroup,Yes.,4,"""However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI.""",2023,2023-12-10T06:57:48Z,"Keyphrase: ""Latency and carbon footprint challenges""","""However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI."" Keyphrase: ""Latency and carbon footprint challenges""",4
arXIv2023,Understanding the Effect of Model Compression on Social Bias in Large Language Models,Yes.,4,"""Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm."" and ""We perform a carefully controlled study of the",2023,2023-12-09T20:04:20Z,"Keyphrase: ""Persistent social biases""","""Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm."" and ""We perform a carefully controlled study of the Keyphrase: ""Persistent social biases""",3
arXIv2023,PaperQA: Retrieval-Augmented Generative Agent for Scientific Research,Yes.,5,"""Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth.""",2023,2023-12-08T18:50:20Z,"Keyphrase: ""Hallucination and uninterpretability""","""Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth."" Keyphrase: ""Hallucination and uninterpretability""",0
arXIv2023,"Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",Yes.,5,"""Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities.""",2023,2023-12-08T18:25:22Z,"Keyphrase: ""Limited reasoning and planning""","""Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities."" Keyphrase: ""Limited reasoning and planning""",1
arXIv2023,HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models,Yes.,5,"""However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems.""",2023,2023-12-08T17:57:20Z,"Keyphrase: ""Faulty reasoning and information hallucination""","""However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems."" Keyphrase: ""Faulty reasoning and information hallucination""",0
arXIv2023,DelucionQA: Detecting Hallucinations in Domain-specific Question Answering,Yes.,5,"""Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing",2023,2023-12-08T17:41:06Z,"Keyphrase: ""Hallucinatory responses""","""Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing Keyphrase: ""Hallucinatory responses""",0
arXIv2023,Assessing LLMs for Moral Value Pluralism,Yes.,4,"""the fields of AI current lacks methods to quantitatively assess and potentially alter the moral values inherent in the output of large language models (LLMs)"" and ""we find that LLMs exhibit several Western-centric value biases; they overestimate how conservative people in non-Western countries are, they are less accurate in representing gender for non-Western countries, and portray older populations as having",2023,2023-12-08T16:18:15Z,"Keyphrase: ""Western-centric bias""","""the fields of AI current lacks methods to quantitatively assess and potentially alter the moral values inherent in the output of large language models (LLMs)"" and ""we find that LLMs exhibit several Western-centric value biases; they overestimate how conservative people in non-Western countries are, they are less accurate in representing gender for non-Western countries, and portray older populations as having Keyphrase: ""Western-centric bias""",3
arXIv2023,TypeFly: Flying Drones with Large Language Model,Yes.,5,"""However, powerful LLMs and their vision counterparts are limited in three important ways. First, they are only available as cloud-based services. Sending images to the cloud raises privacy concerns. Second, they are expensive, costing proportionally to the request size. Finally, without expensive fine-tuning, existing LLMs",2023,2023-12-08T15:57:18Z,"Keyphrase: ""Privacy concerns and cost limitations""","""However, powerful LLMs and their vision counterparts are limited in three important ways. First, they are only available as cloud-based services. Sending images to the cloud raises privacy concerns. Second, they are expensive, costing proportionally to the request size. Finally, without expensive fine-tuning, existing LLMs Keyphrase: ""Privacy concerns and cost limitations""",8
arXIv2023,Retrieval-based Video Language Model for Efficient Long Video Question Answering,Yes.,5,"""However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video QA process.""",2023,2023-12-08T09:48:36Z,"Keyphrase: ""Computational cost and noise in video understanding""","""However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video QA process."" Keyphrase: ""Computational cost and noise in video understanding""",4
arXIv2023,DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions,Yes.,5,"""a dangerous nature is hidden in the code, which is the existence of fatal vulnerabilities,"" and ""shed light on the huge weakness of LLMs in the code generation task.""",2023,2023-12-07T22:19:06Z,"Keyphrase: ""Hidden code vulnerabilities""","""a dangerous nature is hidden in the code, which is the existence of fatal vulnerabilities,"" and ""shed light on the huge weakness of LLMs in the code generation task."" Keyphrase: ""Hidden code vulnerabilities""",2
arXIv2023,Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models,Yes.,4,"""A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs.""",2023,2023-12-07T22:07:54Z,"Keyphrase: ""Insecure code tendencies""","""A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs."" Keyphrase: ""Insecure code tendencies""",2
arXIv2023,Hijacking Context in Large Multi-modal Models,Yes.,5,"""we identify a new limitation of off-the-shelf LMMs where a small fraction of incoherent images or text descriptions mislead LMMs to only generate biased output about the hijacked context, not the originally intended context.""",2023,2023-12-07T11:23:29Z,"Keyphrase: ""Incoherent and biased outputs""","""we identify a new limitation of off-the-shelf LMMs where a small fraction of incoherent images or text descriptions mislead LMMs to only generate biased output about the hijacked context, not the originally intended context."" Keyphrase: ""Incoherent and biased outputs""",3
arXIv2023,Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak,Yes.,5,"""However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as 'Jailbreak Attack'.""",2023,2023-12-07T08:29:58Z,"Keyphrase: ""Vulnerability to malicious instructions""","""However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as 'Jailbreak Attack'."" Keyphrase: ""Vulnerability to malicious instructions""",2
arXIv2023,LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications,Yes.,5,"""due to unresolved vulnerabilities and limitations, great care needs to be used before applying them to intelligence and safety-critical applications."" and ""The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM.""",2023,2023-12-05T19:04:50Z,"Keyphrase: ""Safety-critical application vulnerability""","""due to unresolved vulnerabilities and limitations, great care needs to be used before applying them to intelligence and safety-critical applications."" and ""The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM."" Keyphrase: ""Safety-critical application vulnerability""",2
arXIv2023,Weakly Supervised Detection of Hallucinations in LLM Activations,Yes.,4,"""Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally.""",2023,2023-12-05T14:35:11Z,"Keyphrase: ""Limited internal capacity""","""Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally."" Keyphrase: ""Limited internal capacity""",0
arXIv2023,How should the advent of large language models affect the practice of science?,Yes.,4,"""Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools.""",2023,2023-12-05T10:45:12Z,"Keyphrase: ""Misuse and overhype""","""Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools."" Keyphrase: ""Misuse and overhype""",2
arXIv2023,"Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety",Yes.,5,"""Nevertheless, these models remain black boxes despite incorporating human feedback and instruction-guided tuning. For instance, ChatGPT can generate unsafe responses despite instituting safety guardrails.""",2023,2023-12-05T06:13:55Z,"Keyphrase: ""Black box model with unsafe responses""","""Nevertheless, these models remain black boxes despite incorporating human feedback and instruction-guided tuning. For instance, ChatGPT can generate unsafe responses despite instituting safety guardrails."" Keyphrase: ""Black box model with unsafe responses""",2
arXIv2023,E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation,Yes.,5,"""However, this approach necessitates items to possess rich semantic information, often generates out-of-range results, and suffers from notably low efficiency and limited extensibility. ... Nevertheless, the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations.""",2023,2023-12-05T02:50:18Z,"Keyphrase: ""Limited semantic understanding""","""However, this approach necessitates items to possess rich semantic information, often generates out-of-range results, and suffers from notably low efficiency and limited extensibility. ... Nevertheless, the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations."" Keyphrase: ""Limited semantic understanding""",1
arXIv2023,Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation,Yes.,5,"""observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game.""",2023,2023-12-05T02:41:57Z,"Keyphrase: ""Limited out-of-domain generalization""","""observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game."" Keyphrase: ""Limited out-of-domain generalization""",5
arXIv2023,Competition-Level Problems are Effective LLM Evaluators,Yes.,5,"""the challenges for any existing LLM to solve unseen complex reasoning problems"" and ""none of them is able to consistently mitigate the challenges.""",2023,2023-12-04T18:58:57Z,"Keyphrase: ""Limited complex reasoning ability""","""the challenges for any existing LLM to solve unseen complex reasoning problems"" and ""none of them is able to consistently mitigate the challenges."" Keyphrase: ""Limited complex reasoning ability""",1
arXIv2023,Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?,Yes.,5,"""Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible. They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels. Overall, current LLMs have set readability ranges and do",2023,2023-12-04T17:19:53Z,"Keyphrase: ""Limited adaptability to diverse audience demographics""","""Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible. They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels. Overall, current LLMs have set readability ranges and do Keyphrase: ""Limited adaptability to diverse audience demographics""",3
arXIv2023,Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites,Yes.,5,"""However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods.""",2023,2023-12-04T07:43:02Z,"Keyphrase: ""Object hallucination""","""However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods."" Keyphrase: ""Object hallucination""",0
arXIv2023,Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies,Yes.,4,"""However, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, and must be addressed."" and ""This research presents a comprehensive survey synthesising the current trends and limitations in techniques used for identifying and mitigating bias in LLMs.""",2023,2023-12-03T21:25:10Z,"Keyphrase: ""Bias and discrimination""","""However, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, and must be addressed."" and ""This research presents a comprehensive survey synthesising the current trends and limitations in techniques used for identifying and mitigating bias in LLMs."" Keyphrase: ""Bias and discrimination""",3
arXIv2023,Running cognitive evaluations on large language models: The do's and the don'ts,Yes.,4,"""I describe common pitfalls that might arise when applying a cognitive test to an LLM"" and ""I conclude by discussing four areas where the do's and don'ts are currently under active discussion -- prompt sensitivity, cultural and linguistic diversity, using LLMs as research assistants, and running evaluations on open vs. closed LLMs.""",2023,2023-12-03T04:28:19Z,"Keyphrase: ""Limited sensitivity to cultural and linguistic diversity""","""I describe common pitfalls that might arise when applying a cognitive test to an LLM"" and ""I conclude by discussing four areas where the do's and don'ts are currently under active discussion -- prompt sensitivity, cultural and linguistic diversity, using LLMs as research assistants, and running evaluations on open vs. closed LLMs."" Keyphrase: ""Limited sensitivity to cultural and linguistic diversity""",3
arXIv2023,Towards leveraging LLMs for Conditional QA,Yes.,5,"""This study delves into the capabilities and limitations of Large Language Models (LLMs) in the challenging domain of conditional question-answering."" and ""these models encounter challenges in extractive question answering, where they lag behind the SOTA by over 10 points, and in mitigating the risk of injecting false information.""",2023,2023-12-02T14:02:52Z,"Keyphrase: ""Challenges in conditional question-answering""","""This study delves into the capabilities and limitations of Large Language Models (LLMs) in the challenging domain of conditional question-answering."" and ""these models encounter challenges in extractive question answering, where they lag behind the SOTA by over 10 points, and in mitigating the risk of injecting false information."" Keyphrase: ""Challenges in conditional question-answering""",1
arXIv2023,Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?,Yes.,4,"""a critical concern arises regarding the potential biases embedded within these summaries,"" and ""The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models.""",2023,2023-12-01T13:00:45Z,"Keyphrase: ""Embedded bias in summaries""","""a critical concern arises regarding the potential biases embedded within these summaries,"" and ""The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models."" Keyphrase: ""Embedded bias in summaries""",3
arXIv2023,Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension,Yes.,5,"""Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated.""",2023,2023-11-30T08:44:55Z,"Keyphrase: ""Limited capability in explaining incorrect alternatives""","""Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated."" Keyphrase: ""Limited capability in explaining incorrect alternatives""",1
arXIv2023,Zero-shot Conversational Summarization Evaluations with small Large Language Models,Yes.,5,"""We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully."" and ""We also evaluate the models with human evaluations and discuss the limitations of the models on conversational summarization",2023,2023-11-29T19:34:34Z,"Keyphrase: ""Instruction dependency and variability""","""We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully."" and ""We also evaluate the models with human evaluations and discuss the limitations of the models on conversational summarization Keyphrase: ""Instruction dependency and variability""",6
arXIv2023,OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation,Yes.,5,"""Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment.""",2023,2023-11-29T18:57:07Z,"Keyphrase: ""Pervasive hallucination""","""Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment."" Keyphrase: ""Pervasive hallucination""",0
arXIv2023,MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models,Yes.,5,"""we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images"" and ""Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned.""",2023,2023-11-29T12:49:45Z,"Keyphrase: ""Susceptible to breaches in multimodal analysis""","""we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images"" and ""Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned."" Keyphrase: ""Susceptible to breaches in multimodal analysis""",2
arXIv2023,Unveiling the Implicit Toxicity in Large Language Models,Yes.,5,"""LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting."" and ""LLMs pose a significant threat in generating undetectable implicit toxic outputs.""",2023,2023-11-29T06:42:36Z,"Keyphrase: ""Generation of undetectable toxic output""","""LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting."" and ""LLMs pose a significant threat in generating undetectable implicit toxic outputs."" Keyphrase: ""Generation of undetectable toxic output""",2
arXIv2023,Are Large Language Models Good Fact Checkers: A Preliminary Study,Yes.,5,"""However, they encounter challenges in effectively handling Chinese fact verification and the entirety of the fact-checking pipeline due to language inconsistencies and hallucinations.""",2023,2023-11-29T05:04:52Z,"Keyphrase: ""Language inconsistency and hallucination""","""However, they encounter challenges in effectively handling Chinese fact verification and the entirety of the fact-checking pipeline due to language inconsistencies and hallucinations."" Keyphrase: ""Language inconsistency and hallucination""",0
arXIv2023,Elo Uncovered: Robustness and Best Practices in Language Model Evaluation,Yes.,5,"""We show that these axioms are not always satisfied raising questions about the reliability of current comparative evaluations of LLMs.""",2023,2023-11-29T00:45:23Z,"Keyphrase: ""Reliability concerns""","""We show that these axioms are not always satisfied raising questions about the reliability of current comparative evaluations of LLMs."" Keyphrase: ""Reliability concerns""",0
arXIv2023,Scalable Extraction of Training Data from (Production) Language Models,Yes.,5,"""Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.""",2023,2023-11-28T18:47:03Z,"Keyphrase: ""Memorization vulnerability""","""Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization."" Keyphrase: ""Memorization vulnerability""",5
arXIv2023,Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization,Yes.,5,"""they still suffer from a common issue known as the 'hallucination problem', in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images.""",2023,2023-11-28T14:54:37Z,"Keyphrase: ""Inaccurate textual generation""","""they still suffer from a common issue known as the 'hallucination problem', in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images."" Keyphrase: ""Inaccurate textual generation""",0
arXIv2023,Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop,Yes.,5,"""We find that this self-consuming training loop initially improves both quality and diversity. However, after a few generations the output inevitably degenerates in diversity.""",2023,2023-11-28T14:36:43Z,"Keyphrase: ""Degradation of diversity""","""We find that this self-consuming training loop initially improves both quality and diversity. However, after a few generations the output inevitably degenerates in diversity."" Keyphrase: ""Degradation of diversity""",5
arXIv2023,SEED-Bench-2: Benchmarking Multimodal Large Language Models,Yes.,5,"""By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence.""",2023,2023-11-28T05:53:55Z,"Keyphrase: ""Limited evaluation and insight""","""By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence."" Keyphrase: ""Limited evaluation and insight""",0
arXIv2023,Methods to Estimate Large Language Model Confidence,Yes.,5,"""Large Language Models have difficulty communicating uncertainty, which is a significant obstacle to applying LLMs to complex medical tasks."" and ""We conclude GPT4 has a limited ability to assess its own diagnostic accuracy.""",2023,2023-11-28T05:44:06Z,"Keyphrase: ""Limited ability to communicate uncertainty""","""Large Language Models have difficulty communicating uncertainty, which is a significant obstacle to applying LLMs to complex medical tasks."" and ""We conclude GPT4 has a limited ability to assess its own diagnostic accuracy."" Keyphrase: ""Limited ability to communicate uncertainty""",6
arXIv2023,Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation,Yes.,4,"""However, these models are typically trained on web-scale data, which can introduce inappropriate content and lead to the development of unsafe and biased behavior. This, in turn, hampers their applicability in sensitive and trustworthy contexts and could raise significant concern in their adoption.""",2023,2023-11-27T19:02:17Z,"Keyphrase: ""Unsafe biased behavior""","""However, these models are typically trained on web-scale data, which can introduce inappropriate content and lead to the development of unsafe and biased behavior. This, in turn, hampers their applicability in sensitive and trustworthy contexts and could raise significant concern in their adoption."" Keyphrase: ""Unsafe biased behavior""",2
arXIv2023,Visual cognition in multimodal large language models,Yes.,5,"""Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology."" and ""Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas."" and ""Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether.""",2023,2023-11-27T18:58:34Z,"Keyphrase: ""Limitation in causal reasoning""","""Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology."" and ""Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas."" and ""Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether."" Keyphrase: ""Limitation in causal reasoning""",1
arXIv2023,BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification,Yes.,5,"""they still suffer from a performance gap when the underlying distribution of topics changes"" and ""domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models, such as GPT-3.""",2023,2023-11-27T18:53:31Z,"Keyphrase: ""Domain transfer challenges""","""they still suffer from a performance gap when the underlying distribution of topics changes"" and ""domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models, such as GPT-3."" Keyphrase: ""Domain transfer challenges""",6
arXIv2023,WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models,Yes.,5,"""We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly",2023,2023-11-27T15:38:17Z,"Keyphrase: ""Response bias and error persistence""","""We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly Keyphrase: ""Response bias and error persistence""",6
arXIv2023,"Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",Yes.,4,"""Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application."" and ""The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability.""",2023,2023-11-27T12:37:51Z,"Keyphrase: ""Risk of retaining faulty or harmful knowledge""","""Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application."" and ""The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability."" Keyphrase: ""Risk of retaining faulty or harmful knowledge""",2
arXIv2023,Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation,Yes.,5,"""we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help"" and ""the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation",2023,2023-11-27T07:19:26Z,"Keyphrase: ""Limited support for text-to-image generation""","""we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help"" and ""the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation Keyphrase: ""Limited support for text-to-image generation""",5
arXIv2023,Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination,Yes.,5,"""The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs),"" and ""our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks.""",2023,2023-11-27T05:27:13Z,"Keyphrase: ""Serious hallucination behavior""","""The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs),"" and ""our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks."" Keyphrase: ""Serious hallucination behavior""",0
arXIv2023,UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation,Yes.,5,"""These models often produce hallucinated text, compromising their practical utility in professional contexts.""",2023,2023-11-26T13:42:56Z,"Keyphrase: ""Hallucinated text""","""These models often produce hallucinated text, compromising their practical utility in professional contexts."" Keyphrase: ""Hallucinated text""",0
arXIv2023,"Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",Yes.,5,"""Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality.""",2023,2023-11-26T08:44:58Z,"Keyphrase: ""Susceptibility to prompt hacking""","""Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality."" Keyphrase: ""Susceptibility to prompt hacking""",2
arXIv2023,Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains,Yes.,5,"""Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains.""",2023,2023-11-25T08:58:07Z,"Keyphrase: ""Challenges in high-risk domains""","""Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains."" Keyphrase: ""Challenges in high-risk domains""",2
arXIv2023,AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering,Yes.,5,"""By conducting an extensive case study, we uncover several drawbacks of GPT-4V, such as limited temporal and dynamic comprehension, and overly general responses.""",2023,2023-11-25T02:46:12Z,"Keyphrase: ""Limited temporal comprehension""","""By conducting an extensive case study, we uncover several drawbacks of GPT-4V, such as limited temporal and dynamic comprehension, and overly general responses."" Keyphrase: ""Limited temporal comprehension""",1
arXIv2023,One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space,Yes.,5,"""Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources.""",2023,2023-11-24T18:35:00Z,"Keyphrase: ""High computational complexity""","""Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources."" Keyphrase: ""High computational complexity""",4
arXIv2023,Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review,Yes.,4,"""ChatGPT and other Generative Artificial Intelligence (GAI) models tend to inherit and even amplify prevailing societal biases as they are trained on large amounts of existing data."" and ""Our findings show that while there is an awareness of potential biases around large language models (LLMs) and",2023,2023-11-24T10:00:23Z,"Keyphrase: ""Amplification of societal biases""","""ChatGPT and other Generative Artificial Intelligence (GAI) models tend to inherit and even amplify prevailing societal biases as they are trained on large amounts of existing data."" and ""Our findings show that while there is an awareness of potential biases around large language models (LLMs) and Keyphrase: ""Amplification of societal biases""",3
arXIv2023,Towards Auditing Large Language Models: Improving Text-based Stereotype Detection,Yes.,4,"""LLMs often generate stereotypical output inherited from historical data, amplifying societal biases and raising ethical concerns.""",2023,2023-11-23T17:47:14Z,"Keyphrase: ""Amplifying societal bias""","""LLMs often generate stereotypical output inherited from historical data, amplifying societal biases and raising ethical concerns."" Keyphrase: ""Amplifying societal bias""",3
arXIv2023,Auditing and Mitigating Cultural Bias in LLMs,Yes.,5,"""We audit large language models for cultural bias,"" and ""Our mitigation strategy reduces cultural bias in recent models but not for all countries/territories.""",2023,2023-11-23T16:45:56Z,"Keyphrase: ""Cultural bias mitigation strategy""","""We audit large language models for cultural bias,"" and ""Our mitigation strategy reduces cultural bias in recent models but not for all countries/territories."" Keyphrase: ""Cultural bias mitigation strategy""",3
arXIv2023,Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach,Yes.,5,"""However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent.""",2023,2023-11-23T10:14:58Z,"Keyphrase: ""Coordination hallucination""","""However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent."" Keyphrase: ""Coordination hallucination""",0
arXIv2023,Challenges of Large Language Models for Mental Health Counseling,Yes.,5,"""However, the application of LLMs in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. This paper investigates the major challenges associated with the development of LLMs for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness.""",2023,2023-11-23T08:56:41Z,"Keyphrase: ""Challenges in mental health domain""","""However, the application of LLMs in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. This paper investigates the major challenges associated with the development of LLMs for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness."" Keyphrase: ""Challenges in mental health domain""",0
arXIv2023,Surpassing GPT-4 Medical Coding with a Two-Stage Approach,Yes.,5,"""the GPT-4 LLM predicts an excessive number of ICD codes for medical coding tasks, leading to high recall but low precision.""",2023,2023-11-22T23:35:13Z,"Keyphrase: ""Excessive predictions in medical coding""","""the GPT-4 LLM predicts an excessive number of ICD codes for medical coding tasks, leading to high recall but low precision."" Keyphrase: ""Excessive predictions in medical coding""",0
arXIv2023,Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering,Yes.,5,"""currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants).""",2023,2023-11-22T18:22:56Z,"Keyphrase: ""Limited context and high computational cost""","""currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants)."" Keyphrase: ""Limited context and high computational cost""",4
arXIv2023,Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents,Yes.,4,"""However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming.""",2023,2023-11-22T13:15:42Z,"Keyphrase: ""Lack of specialization in dynamic environments""","""However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming."" Keyphrase: ""Lack of specialization in dynamic environments""",1
arXIv2023,Applying Large Language Models to Power Systems: Potential Security Threats,Yes.,5,"""However, this action may also incur potential security threats, which have not been fully recognized so far.""",2023,2023-11-22T12:55:02Z,"Keyphrase: ""Security threats not fully recognized""","""However, this action may also incur potential security threats, which have not been fully recognized so far."" Keyphrase: ""Security threats not fully recognized""",2
arXIv2023,Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting,Yes.,5,"""Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process.""",2023,2023-11-22T11:08:38Z,"Keyphrase: ""Limited factual grounding""","""Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process."" Keyphrase: ""Limited factual grounding""",0
arXIv2023,Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus,Yes.,5,"""LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications.""",2023,2023-11-22T08:39:17Z,"Keyphrase: ""Untruthful nonsensical output""","""LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications."" Keyphrase: ""Untruthful nonsensical output""",0
arXIv2023,HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data,Yes.,5,"""the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored.""",2023,2023-11-22T04:52:58Z,"Keyphrase: ""Hallucinatory output""","""the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored."" Keyphrase: ""Hallucinatory output""",0
arXIv2023,Conditions for Length Generalization in Learning Reasoning Skills,Yes.,5,"""However, numerous evaluations of the reasoning capabilities of LLMs have also showed some limitations. An outstanding limitation is length generalization, meaning that when trained on reasoning problems of smaller lengths or sizes, the resulting models struggle with problems of larger sizes or lengths.""",2023,2023-11-22T03:36:18Z,"Keyphrase: ""Limited reasoning capability""","""However, numerous evaluations of the reasoning capabilities of LLMs have also showed some limitations. An outstanding limitation is length generalization, meaning that when trained on reasoning problems of smaller lengths or sizes, the resulting models struggle with problems of larger sizes or lengths."" Keyphrase: ""Limited reasoning capability""",1
arXIv2023,Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper,Yes.,4,"""While LLMs possess remarkable capabilities, their extensive parameter requirements and associated computational demands hinder their practicality and scalability for real-world applications."" and ""recognizes significant challenges and open issues that must be addressed to fully harness the powerful abilities of LLMs.""",2023,2023-11-22T03:28:34Z,"Keyphrase: ""Computational demands and scalability""","""While LLMs possess remarkable capabilities, their extensive parameter requirements and associated computational demands hinder their practicality and scalability for real-world applications."" and ""recognizes significant challenges and open issues that must be addressed to fully harness the powerful abilities of LLMs."" Keyphrase: ""Computational demands and scalability""",4
arXIv2023,Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications,Yes.,4,"""Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited.""",2023,2023-11-22T01:51:50Z,"Keyphrase: ""Limited logical reasoning competency""","""Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited."" Keyphrase: ""Limited logical reasoning competency""",1
arXIv2023,A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift,Yes.,4,"""However, there is little work measuring how robust these reward models are to distribution shifts."" and ""We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts.""",2023,2023-11-21T18:41:26Z,"Keyphrase: ""Reward model distribution shift""","""However, there is little work measuring how robust these reward models are to distribution shifts."" and ""We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts."" Keyphrase: ""Reward model distribution shift""",2
arXIv2023,Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey,Yes.,5,"""current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios."" and ""We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models.""",2023,2023-11-21T04:59:17Z,"Keyphrase: ""Ineffective long-context processing""","""current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios."" and ""We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models."" Keyphrase: ""Ineffective long-context processing""",4
arXIv2023,"Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",Yes.,4,"""A notable challenge, model hallucination-where the model yields inaccurate or misinterpreted data-is addressed alongside other model-specific hurdles.""",2023,2023-11-21T02:01:01Z,"Keyphrase: ""Hallucination of inaccurate data""","""A notable challenge, model hallucination-where the model yields inaccurate or misinterpreted data-is addressed alongside other model-specific hurdles."" Keyphrase: ""Hallucination of inaccurate data""",0
arXIv2023,Evil Geniuses: Delving into the Safety of LLM-based Agents,Yes.,4,"""Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research.""",2023,2023-11-20T15:50:09Z,"Keyphrase: ""Prone to harmful behavior""","""Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research."" Keyphrase: ""Prone to harmful behavior""",2
arXIv2023,Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information,Yes.,5,"""these models are susceptible to adversarial prompt attacks,"" and ""this vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs.""",2023,2023-11-20T03:17:21Z,"Keyphrase: ""Adversarial prompt vulnerability""","""these models are susceptible to adversarial prompt attacks,"" and ""this vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs."" Keyphrase: ""Adversarial prompt vulnerability""",2
arXIv2023,A Security Risk Taxonomy for Large Language Models,Yes.,5,"""The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial."" and ""Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs.""",2023,2023-11-19T20:22:05Z,"Keyphrase: ""Security vulnerabilities""","""The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial."" and ""Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs."" Keyphrase: ""Security vulnerabilities""",2
arXIv2023,Rethinking Large Language Models in Mental Health Applications,Yes.,5,"""It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability.""",2023,2023-11-19T08:40:01Z,"Keyphrase: ""Hallucinatory output""","""It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability."" Keyphrase: ""Hallucinatory output""",0
arXIv2023,A Principled Framework for Knowledge-enhanced Large Language Model,Yes.,5,"""Large Language Models (LLMs) are versatile, yet they often falter in tasks requiring deep and reliable reasoning due to issues like hallucinations, limiting their applicability in critical scenarios.""",2023,2023-11-18T18:10:02Z,"Keyphrase: ""Limited deep reasoning""","""Large Language Models (LLMs) are versatile, yet they often falter in tasks requiring deep and reliable reasoning due to issues like hallucinations, limiting their applicability in critical scenarios."" Keyphrase: ""Limited deep reasoning""",1
arXIv2023,Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers,Yes.,5,"""One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual. Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context.""",2023,2023-11-18T03:55:59Z,"Keyphrase: ""Factual hallucination due to biased training data""","""One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual. Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context."" Keyphrase: ""Factual hallucination due to biased training data""",0
arXIv2023,DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback,Yes.,5,"""First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction",2023,2023-11-16T18:37:29Z,"Keyphrase: ""Limited incorporation of extra feedback""","""First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction Keyphrase: ""Limited incorporation of extra feedback""",0
arXIv2023,Hijacking Large Language Models via Adversarial In-Context Learning,Yes.,5,"""ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL.""",2023,2023-11-16T15:01:48Z,"Keyphrase: ""Instability in adversarial attacks""","""ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL."" Keyphrase: ""Instability in adversarial attacks""",2
arXIv2023,ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks,Yes.,5,"""a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries"" and ""GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73% of the tasks, leaving a huge",2023,2023-11-16T12:03:21Z,"Keyphrase: ""Limited practical applicability""","""a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries"" and ""GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73% of the tasks, leaving a huge Keyphrase: ""Limited practical applicability""",7
arXIv2023,FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models,Yes.,5,"""We have evaluated various LLMs using the FollowEval benchmark and found that their performance significantly lags behind that of humans. This highlights the considerable room for improvement in the instruction-following ability of these models.""",2023,2023-11-16T11:53:31Z,"Keyphrase: ""Inferior performance compared to humans""","""We have evaluated various LLMs using the FollowEval benchmark and found that their performance significantly lags behind that of humans. This highlights the considerable room for improvement in the instruction-following ability of these models."" Keyphrase: ""Inferior performance compared to humans""",7
arXIv2023,Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking,Yes.,5,"""we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning.""",2023,2023-11-16T11:52:22Z,"Keyphrase: ""Cognitive overload and reasoning challenges""","""we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning."" Keyphrase: ""Cognitive overload and reasoning challenges""",1
arXIv2023,The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text,Yes.,5,"""Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness.""",2023,2023-11-16T11:31:50Z,"Keyphrase: ""Decreased diversity in model output""","""Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness."" Keyphrase: ""Decreased diversity in model output""",3
arXIv2023,DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data,Yes.,5,"""We found that, although the current best-performing system (i.e., GPT-4), can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts.""",2023,2023-11-16T11:30:53Z,"Keyphrase: ""Limited performance on complex, context-dependent tasks""","""We found that, although the current best-performing system (i.e., GPT-4), can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts."" Keyphrase: ""Limited performance on complex, context-dependent tasks""",7
arXIv2023,Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models,Yes.,4,"""We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning",2023,2023-11-16T10:36:08Z,"Keyphrase: ""Limited multistep reasoning""","""We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning Keyphrase: ""Limited multistep reasoning""",1
arXIv2023,Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks,Yes.,5,"""Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects.""",2023,2023-11-16T10:02:24Z,"Keyphrase: ""Biases in demographic-infused prompts""","""Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects."" Keyphrase: ""Biases in demographic-infused prompts""",3
arXIv2023,R-Tuning: Teaching Large Language Models to Refuse Unknown Questions,Yes.,5,"""A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination."" and ""previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not.""",2023,2023-11-16T08:45:44Z,"Keyphrase: ""Hallucination of nonexistent facts""","""A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination."" and ""previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not."" Keyphrase: ""Hallucination of nonexistent facts""",0
arXIv2023,Structured Chemistry Reasoning with Large Language Models,Yes.,5,"""Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry."" and ""even advanced LLMs, like GPT-4, can fail easily in different ways."" and ""the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that",2023,2023-11-16T08:20:36Z,"Keyphrase: ""Limited scientific reasoning in specialized domains""","""Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry."" and ""even advanced LLMs, like GPT-4, can fail easily in different ways."" and ""the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that Keyphrase: ""Limited scientific reasoning in specialized domains""",1
arXIv2023,On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models,Yes.,5,"""Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially."" and ""Our",2023,2023-11-16T07:48:45Z,"Keyphrase: ""Vulnerability to adversarial manipulation""","""Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially."" and ""Our Keyphrase: ""Vulnerability to adversarial manipulation""",2
arXIv2023,Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework,Yes.,4,"""modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked"" and ""These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts.""",2023,2023-11-16T07:31:18Z,"Keyphrase: ""Vulnerability to malicious attacks""","""modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked"" and ""These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts."" Keyphrase: ""Vulnerability to malicious attacks""",2
arXIv2023,Self-Contradictory Reasoning Evaluation and Detection,Yes.,5,"""We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense"" and ""Our results indicate that the current LLMs lack robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond accuracy-based metrics.""",2023,2023-11-16T06:22:17Z,"Keyphrase: ""Lack of robust reasoning""","""We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense"" and ""Our results indicate that the current LLMs lack robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond accuracy-based metrics."" Keyphrase: ""Lack of robust reasoning""",1
arXIv2023,LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks,Yes.,5,"""Assessing these models on long sequences is crucial since prior work in the general domain has demonstrated performance degradation of LLMs on longer texts."" and ""Preliminary experiments reveal that both medical LLMs (e.g., BioGPT) and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark.""",2023,2023-11-16T04:57:49Z,"Keyphrase: ""Performance degradation with long sequences""","""Assessing these models on long sequences is crucial since prior work in the general domain has demonstrated performance degradation of LLMs on longer texts."" and ""Preliminary experiments reveal that both medical LLMs (e.g., BioGPT) and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark."" Keyphrase: ""Performance degradation with long sequences""",4
arXIv2023,How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities,Yes.,5,"""However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly."" and ""scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations.""",2023,2023-11-15T23:33:07Z,"Keyphrase: ""Limited trustworthiness""","""However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly."" and ""scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations."" Keyphrase: ""Limited trustworthiness""",2
arXIv2023,Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment,Yes.,5,"""the vulnerability of their safety alignment has not been extensively studied"" and ""Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts"" and ""Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications.""",2023,2023-11-15T23:07:40Z,"Keyphrase: ""Vulnerability to attacks and resource-intensive implementation""","""the vulnerability of their safety alignment has not been extensively studied"" and ""Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts"" and ""Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications."" Keyphrase: ""Vulnerability to attacks and resource-intensive implementation""",2
arXIv2023,When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour,Yes.,5,"""the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability.""",2023,2023-11-15T22:18:33Z,"Keyphrase: ""Bias amplification through human feedback""","""the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability."" Keyphrase: ""Bias amplification through human feedback""",0
arXIv2023,Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization,Yes.,5,"""Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges",2023,2023-11-15T19:49:24Z,"Keyphrase: ""Challenges in abstractive summarization""","""Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges Keyphrase: ""Challenges in abstractive summarization""",1
arXIv2023,Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models,Yes.,4,"""Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language (e.g., chemical molecular formula).""",2023,2023-11-15T18:59:56Z,"Keyphrase: ""Limited world knowledge comprehension""","""Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language (e.g., chemical molecular formula)."" Keyphrase: ""Limited world knowledge comprehension""",1
arXIv2023,Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering,Yes.,5,"""they are struggling to seek correct information in long contexts. The 'lost in the middle' problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle.""",2023,2023-11-15T18:42:44Z,"Keyphrase: ""Difficulty with contextual accuracy""","""they are struggling to seek correct information in long contexts. The 'lost in the middle' problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle."" Keyphrase: ""Difficulty with contextual accuracy""",5
arXIv2023,Towards Verifiable Text Generation with Symbolic References,Yes.,5,"""However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult.""",2023,2023-11-15T18:28:29Z,"Keyphrase: ""Need for manual verification""","""However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult."" Keyphrase: ""Need for manual verification""",0
arXIv2023,Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization,Yes.,5,"""our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance",2023,2023-11-15T18:25:26Z,"Keyphrase: ""Factual errors and evaluation misalignment""","""our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance Keyphrase: ""Factual errors and evaluation misalignment""",0
arXIv2023,AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph,Yes.,5,"""Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings.""",2023,2023-11-15T18:11:23Z,"Keyphrase: ""Struggles with abstract knowledge comprehension""","""Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings."" Keyphrase: ""Struggles with abstract knowledge comprehension""",6
arXIv2023,Temporal Knowledge Question Answering via Abstract Reasoning Induction,Yes.,5,"""In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic.""",2023,2023-11-15T17:46:39Z,"Keyphrase: ""Limited temporal knowledge reasoning""","""In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic."" Keyphrase: ""Limited temporal knowledge reasoning""",1
arXIv2023,Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts,Yes.,5,"""This finding indicates potential exploitable security risks in MLLMs"" and ""Results show that appropriately designed system prompts can significantly reduce jailbreak success rates.""",2023,2023-11-15T17:17:39Z,"Keyphrase: ""Security vulnerabilities""","""This finding indicates potential exploitable security risks in MLLMs"" and ""Results show that appropriately designed system prompts can significantly reduce jailbreak success rates."" Keyphrase: ""Security vulnerabilities""",2
arXIv2023,Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content.""",2023,2023-11-15T17:04:56Z,"Keyphrase: ""Inaccurate hallucinated content""","""Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content."" Keyphrase: ""Inaccurate hallucinated content""",0
arXIv2023,Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?,Yes.,5,"""This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications.""",2023,2023-11-15T16:56:49Z,"Keyphrase: ""Limited inference capabilities""","""This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications."" Keyphrase: ""Limited inference capabilities""",1
arXIv2023,Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization,Yes.,5,"""Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks."" and ""we point out a pivotal factor contributing to the success of jailbreaks",2023,2023-11-15T16:42:29Z,"Keyphrase: ""Safety risks and vulnerabilities""","""Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks."" and ""we point out a pivotal factor contributing to the success of jailbreaks Keyphrase: ""Safety risks and vulnerabilities""",2
arXIv2023,Social Bias Probing: Fairness Benchmarking for Language Models,Yes.,4,"""Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms."" and ""When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged.""",2023,2023-11-15T16:35:59Z,"Keyphrase: ""Encoded social biases""","""Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms."" and ""When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged."" Keyphrase: ""Encoded social biases""",3
arXIv2023,How Well Do Large Language Models Truly Ground?,Yes.,5,"""Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge.""",2023,2023-11-15T16:11:27Z,"Keyphrase: ""Hallucination and lack of control""","""Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge."" Keyphrase: ""Hallucination and lack of control""",0
arXIv2023,GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models,Yes.,5,"""Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models."" and ""These identified limitations underline the importance of using benchmarks like GRASP to monitor the progress of future models in developing these competencies.""",2023,2023-11-15T15:38:28Z,"Keyphrase: ""Lack of intuitive physics capability""","""Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models."" and ""These identified limitations underline the importance of using benchmarks like GRASP to monitor the progress of future models in developing these competencies."" Keyphrase: ""Lack of intuitive physics capability""",1
arXIv2023,Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output,Yes.,5,"""The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs."" and ""Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify",2023,2023-11-15T14:41:57Z,"Keyphrase: ""Verification of factual accuracy""","""The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs."" and ""Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify Keyphrase: ""Verification of factual accuracy""",0
arXIv2023,Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation,Yes.,5,"""large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user."" and ""state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities.""",2023,2023-11-15T11:27:44Z,"Keyphrase: ""Low confidence signals""","""large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user."" and ""state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities."" Keyphrase: ""Low confidence signals""",6
arXIv2023,Disinformation Capabilities of Large Language Models,Yes.,4,"""Automated disinformation generation is often listed as an important risk associated with large language models (LLMs).""",2023,2023-11-15T10:25:30Z,"Keyphrase: ""Risk of automated disinformation""","""Automated disinformation generation is often listed as an important risk associated with large language models (LLMs)."" Keyphrase: ""Risk of automated disinformation""",2
arXIv2023,MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy,Yes.,5,"""degenerate modes can even occur in the absence of any model error, due to contamination of the training data"" and ""the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue.""",2023,2023-11-15T09:38:53Z,"Keyphrase: ""Degenerate modes""","""degenerate modes can even occur in the absence of any model error, due to contamination of the training data"" and ""the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue."" Keyphrase: ""Degenerate modes""",5
arXIv2023,Thread of Thought Unraveling Chaotic Contexts,Yes.,5,"""they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context.""",2023,2023-11-15T06:54:44Z,"Keyphrase: ""Difficulty in chaotic contexts""","""they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context."" Keyphrase: ""Difficulty in chaotic contexts""",5
arXIv2023,Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models,Yes.,4,"""However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills.""",2023,2023-11-15T06:48:50Z,"Keyphrase: ""Unreliable output and poor reasoning""","""However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills."" Keyphrase: ""Unreliable output and poor reasoning""",0
arXIv2023,Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures,Yes.,4,"""we undertake an exploration of decision-making processes and inherent biases within LLMs"" and ""We discuss the consequences of our findings for human-AI alignment and bias mitigation.""",2023,2023-11-15T00:02:25Z,"Keyphrase: ""Inherent bias and alignment issues""","""we undertake an exploration of decision-making processes and inherent biases within LLMs"" and ""We discuss the consequences of our findings for human-AI alignment and bias mitigation."" Keyphrase: ""Inherent bias and alignment issues""",3
arXIv2023,Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment,Yes.,5,"""A systematic study of ten LLMs on seven classification tasks reveals that models flip their answers on average 46% of the time and that all models see a deterioration of accuracy between their first and final prediction, with an average drop of 17% (the FlipFlop effect).""",2023,2023-11-14T23:40:22Z,"Keyphrase: ""Answer flipping and accuracy deterioration""","""A systematic study of ten LLMs on seven classification tasks reveals that models flip their answers on average 46% of the time and that all models see a deterioration of accuracy between their first and final prediction, with an average drop of 17% (the FlipFlop effect)."" Keyphrase: ""Answer flipping and accuracy deterioration""",5
arXIv2023,CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,Yes.,4,"""existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations."" and ""most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks"" and ""most benchmarks also fail to",2023,2023-11-14T23:18:52Z,"Keyphrase: ""Limited benchmark diversity""","""existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations."" and ""most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks"" and ""most benchmarks also fail to Keyphrase: ""Limited benchmark diversity""",7
arXIv2023,"LLMs cannot find reasoning errors, but can correct them!",Yes.,5,"""recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall"" and ""demonstrate that LLMs generally struggle with finding logical mistakes.""",2023,2023-11-14T20:12:38Z,"Keyphrase: ""Struggles with logical reasoning""","""recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall"" and ""demonstrate that LLMs generally struggle with finding logical mistakes."" Keyphrase: ""Struggles with logical reasoning""",1
arXIv2023,Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models,Yes.,4,"""while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems"" and ""We discuss how future work can include LLM fairness evaluations.""",2023,2023-11-14T19:02:03Z,"Keyphrase: ""Limited fairness evaluation""","""while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems"" and ""We discuss how future work can include LLM fairness evaluations."" Keyphrase: ""Limited fairness evaluation""",3
arXIv2023,Fine-tuning Language Models for Factuality,Yes.,5,"""Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions.""",2023,2023-11-14T18:59:15Z,"Keyphrase: ""Factually inaccurate hallucination""","""Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions."" Keyphrase: ""Factually inaccurate hallucination""",0
arXIv2023,Are Large Language Models Temporally Grounded?,Yes.,5,"""Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions.",2023,2023-11-14T18:57:15Z,"Keyphrase: ""Struggles with self-consistency""","""Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions. Keyphrase: ""Struggles with self-consistency""",1
arXIv2023,SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models,Yes.,5,"""However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content."" and ""We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses.""",2023,2023-11-14T18:33:43Z,"Keyphrase: ""Safety weaknesses in guidance""","""However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content."" and ""We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses."" Keyphrase: ""Safety weaknesses in guidance""",2
arXIv2023,GPT-4V(ision) Unsuitable for Clinical Care and Education: A Clinician-Evaluated Assessment,Yes.,5,"""Although GPT-4V is able to identify and explain medical images, its diagnostic accuracy and clinical decision-making abilities are poor, posing risks to patient safety.""",2023,2023-11-14T17:06:09Z,"Keyphrase: ""Poor clinical decision-making ability""","""Although GPT-4V is able to identify and explain medical images, its diagnostic accuracy and clinical decision-making abilities are poor, posing risks to patient safety."" Keyphrase: ""Poor clinical decision-making ability""",1
arXIv2023,Extrinsically-Focused Evaluation of Omissions in Medical Summarization,Yes.,4,"""Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs."" and ""especially given the potential for LLMs to omit important information in the resulting summary.""",2023,2023-11-14T16:46:15Z,"Keyphrase: ""Omission of important information""","""Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs."" and ""especially given the potential for LLMs to omit important information in the resulting summary."" Keyphrase: ""Omission of important information""",0
arXIv2023,A Survey of Confidence Estimation and Calibration in Large Language Models,Yes.,4,"""Despite their impressive performance, they can be unreliable due to factual errors in their generations."" and ""we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration.""",2023,2023-11-14T16:43:29Z,"Keyphrase: ""Unreliable factual error generation""","""Despite their impressive performance, they can be unreliable due to factual errors in their generations."" and ""we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration."" Keyphrase: ""Unreliable factual error generation""",0
arXIv2023,How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions,Yes.,5,"""Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points."" and ""simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLM",2023,2023-11-14T16:30:36Z,"Keyphrase: ""Limited syntactic knowledge grasp""","""Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points."" and ""simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLM Keyphrase: ""Limited syntactic knowledge grasp""",1
arXIv2023,A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily,Yes.,5,"""Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them."" and ""Our study also reveals the inadequacy of current defense methods in safeguarding LLMs.""",2023,2023-11-14T16:02:16Z,"Keyphrase: ""Vulnerability to attacks""","""Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them."" and ""Our study also reveals the inadequacy of current defense methods in safeguarding LLMs."" Keyphrase: ""Vulnerability to attacks""",2
arXIv2023,RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge,Yes.,5,"""Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue.""",2023,2023-11-14T13:24:19Z,"Keyphrase: ""Susceptibility to external interference""","""Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue."" Keyphrase: ""Susceptibility to external interference""",2
arXIv2023,Insights into Classifying and Mitigating LLMs' Hallucinations,Yes.,5,"""However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as 'Hallucinations'. They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation.""",2023,2023-11-14T12:30:28Z,"Keyphrase: ""False information propagation""","""However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as 'Hallucinations'. They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation."" Keyphrase: ""False information propagation""",0
arXIv2023,Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models,Yes.,5,"""the dynamic nature of knowledge presents challenges for language models that are trained on static data, leading to outdated encoded information,"" and ""we uncover that existing continual learning baselines have difficulty in updating and forgetting outdated knowledge,"" and ""the models fail to learn updated knowledge due to the small weight gradient,"" and ""the models struggle mostly on providing numerical or temporal answers to questions asking for updated knowledge",2023,2023-11-14T12:12:02Z,"Keyphrase: ""Difficulty in updating outdated knowledge""","""the dynamic nature of knowledge presents challenges for language models that are trained on static data, leading to outdated encoded information,"" and ""we uncover that existing continual learning baselines have difficulty in updating and forgetting outdated knowledge,"" and ""the models fail to learn updated knowledge due to the small weight gradient,"" and ""the models struggle mostly on providing numerical or temporal answers to questions asking for updated knowledge Keyphrase: ""Difficulty in updating outdated knowledge""",5
arXIv2023,How good are Large Language Models on African Languages?,Yes.,5,"""Our results suggest that all LLMs produce below-par performance on African languages, and there is a large gap in performance compared to high-resource languages like English most tasks.""",2023,2023-11-14T08:10:14Z,"Keyphrase: ""Performance gap in African languages""","""Our results suggest that all LLMs produce below-par performance on African languages, and there is a large gap in performance compared to high-resource languages like English most tasks."" Keyphrase: ""Performance gap in African languages""",7
arXIv2023,A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,Yes.,5,"""Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems."" and ""Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification",2023,2023-11-14T07:13:10Z,"Keyphrase: ""Struggle with logical reasoning""","""Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems."" and ""Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification Keyphrase: ""Struggle with logical reasoning""",1
arXIv2023,Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey,Yes.,5,"""The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy.""",2023,2023-11-14T05:21:57Z,"Keyphrase: ""Hallucination stemming from knowledge gap""","""The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy."" Keyphrase: ""Hallucination stemming from knowledge gap""",0
arXIv2023,Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems,Yes.,5,"""Large Language Models have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence.""",2023,2023-11-13T21:20:17Z,"Keyphrase: ""Trivial mistakes in commonsense competence""","""Large Language Models have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence."" Keyphrase: ""Trivial mistakes in commonsense competence""",1
arXIv2023,MART: Improving LLM Safety with Multi-round Automatic Red-Teaming,Yes.,4,"""Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.""",2023,2023-11-13T19:13:29Z,"Keyphrase: ""Limited ability to mitigate unsafe behavior""","""Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses."" Keyphrase: ""Limited ability to mitigate unsafe behavior""",2
arXIv2023,GPT-4V(ision) as A Social Media Analysis Engine,Yes.,5,"""Despite the overall impressive capacity of GPT-4V in the social media domain, there remain notable challenges. GPT-4V struggles with tasks involving multilingual social multimedia comprehension and has difficulties in generalizing to the latest trends in social media. Additionally, it exhibits a tendency to generate erroneous information in the context",2023,2023-11-13T18:36:50Z,"Keyphrase: ""Struggles with multilingual social media comprehension""","""Despite the overall impressive capacity of GPT-4V in the social media domain, there remain notable challenges. GPT-4V struggles with tasks involving multilingual social multimedia comprehension and has difficulties in generalizing to the latest trends in social media. Additionally, it exhibits a tendency to generate erroneous information in the context Keyphrase: ""Struggles with multilingual social media comprehension""",1
arXIv2023,A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models,Yes.,5,"""While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations. Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response.""",2023,2023-11-13T17:28:03Z,"Keyphrase: ""Limited understanding of complex questions""","""While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations. Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response."" Keyphrase: ""Limited understanding of complex questions""",1
arXIv2023,On Measuring Faithfulness or Self-consistency of Natural Language Explanations,Yes.,5,"""But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning.""",2023,2023-11-13T16:53:51Z,"Keyphrase: ""Unfaithful explanations""","""But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning."" Keyphrase: ""Unfaithful explanations""",1
arXIv2023,Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue,Yes.,5,"""However, LLMs still lack a crucial ability",2023,2023-11-13T16:19:42Z,"Keyphrase: ""Lack of crucial ability""","""However, LLMs still lack a crucial ability Keyphrase: ""Lack of crucial ability""",1
arXIv2023,AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation,Yes.,4,"""current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences.""",2023,2023-11-13T15:25:42Z,"Keyphrase: ""Hallucination challenges""","""current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences."" Keyphrase: ""Hallucination challenges""",0
arXIv2023,Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study,Yes.,5,"""Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data."" and ""Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent,",2023,2023-11-13T15:11:26Z,"Keyphrase: ""Limited reasoning and planning capabilities""","""Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data."" and ""Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent, Keyphrase: ""Limited reasoning and planning capabilities""",1
arXIv2023,LM-Polygraph: Uncertainty Estimation for Language Models,Yes.,5,"""However, a significant challenge arises as these models often 'hallucinate', i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements.""",2023,2023-11-13T15:08:59Z,"Keyphrase: ""Fabricating facts""","""However, a significant challenge arises as these models often 'hallucinate', i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements."" Keyphrase: ""Fabricating facts""",0
arXIv2023,In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search,Yes.,5,"""Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail."" and ""LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach,"" and ""find that model performances drop by as high as 5% in the long-tail distribution compared to head distribution.""",2023,2023-11-13T10:56:59Z,"Keyphrase: ""Performance drop in long-tail data""","""Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail."" and ""LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach,"" and ""find that model performances drop by as high as 5% in the long-tail distribution compared to head distribution."" Keyphrase: ""Performance drop in long-tail data""",5
arXIv2023,Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models,Yes.,5,"""on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all",2023,2023-11-13T09:32:12Z,"Keyphrase: ""Factual inconsistencies""","""on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all Keyphrase: ""Factual inconsistencies""",0
arXIv2023,Towards the Law of Capacity Gap in Distilling Language Models,Yes.,4,"""it is still a pain distilling LMs when a large capacity gap is exhibited between the teacher and the student LMs,"" and ""the curse of capacity gap can be only partly yet not fully lifted as indicated in previous studies.""",2023,2023-11-13T03:36:18Z,"Keyphrase: ""Capacity gap in distillation""","""it is still a pain distilling LMs when a large capacity gap is exhibited between the teacher and the student LMs,"" and ""the curse of capacity gap can be only partly yet not fully lifted as indicated in previous studies."" Keyphrase: ""Capacity gap in distillation""",4
arXIv2023,Flames: Benchmarking Value Alignment of LLMs in Chinese,Yes.,5,"""Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs,"" and ""there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness,"" and ""all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions.""",2023,2023-11-12T17:18:21Z,"Keyphrase: ""Limited safety evaluation""","""Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs,"" and ""there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness,"" and ""all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions."" Keyphrase: ""Limited safety evaluation""",2
arXIv2023,Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding,Yes.,5,"""Nonetheless, logical reasoning that involves proof planning, specifically those that necessitate the validation of explanation accuracy, continues to present stumbling blocks."" and ""Our analysis reveals that LLMs still struggle to navigate complex reasoning chains, which demand the meticulous linkage of premises",2023,2023-11-12T05:12:49Z,"Keyphrase: ""Struggles with complex reasoning""","""Nonetheless, logical reasoning that involves proof planning, specifically those that necessitate the validation of explanation accuracy, continues to present stumbling blocks."" and ""Our analysis reveals that LLMs still struggle to navigate complex reasoning chains, which demand the meticulous linkage of premises Keyphrase: ""Struggles with complex reasoning""",1
arXIv2023,CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset,Yes.,5,"""However, even models with billions of parameters face challenges in tasks demanding multi-step reasoning. Code generation and comprehension, especially in C and C++, emerge as significant challenges. ...they struggle with rectifying non-compilable C and C++ code. ...This approach, however, retains the limitations",2023,2023-11-11T08:21:52Z,"Keyphrase: ""Challenges in multistep reasoning and code generation""","""However, even models with billions of parameters face challenges in tasks demanding multi-step reasoning. Code generation and comprehension, especially in C and C++, emerge as significant challenges. ...they struggle with rectifying non-compilable C and C++ code. ...This approach, however, retains the limitations Keyphrase: ""Challenges in multistep reasoning and code generation""",7
arXIv2023,ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome Management,Yes.,5,"""a leading barrier to the deployment of Artificial Intelligence (AI) and in particular LLMs has been concern for embedded gender and racial biases."" and ""we evaluate whether a leading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical management of acute coronary syndrome (ACS).""",2023,2023-11-10T19:59:36Z,"Keyphrase: ""Embedded gender and racial bias""","""a leading barrier to the deployment of Artificial Intelligence (AI) and in particular LLMs has been concern for embedded gender and racial biases."" and ""we evaluate whether a leading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical management of acute coronary syndrome (ACS)."" Keyphrase: ""Embedded gender and racial bias""",3
arXIv2023,How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model,Yes.,4,"""MLLMs still face challenges in processing the semantic gap in multimodality, which may lead to erroneous generation, posing potential risks to society."" and ""Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with limited performance improvement.""",2023,2023-11-10T09:51:24Z,"Keyphrase: ""Semantic gap in multimodality""","""MLLMs still face challenges in processing the semantic gap in multimodality, which may lead to erroneous generation, posing potential risks to society."" and ""Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with limited performance improvement."" Keyphrase: ""Semantic gap in multimodality""",1
arXIv2023,"Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications",Yes.,4,"""Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations.""",2023,2023-11-10T05:24:04Z,"Keyphrase: ""Outdated data limitations""","""Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations."" Keyphrase: ""Outdated data limitations""",0
arXIv2023,Hallucination-minimized Data-to-answer Framework for Financial Decision-makers,Yes.,5,"""scaling such prototypes to robust products with minimized hallucinations or fake responses still remains an open challenge, especially in niche data-table heavy domains such as financial decision making.""",2023,2023-11-09T22:53:52Z,"Keyphrase: ""Hallucination in niche domains""","""scaling such prototypes to robust products with minimized hallucinations or fake responses still remains an open challenge, especially in niche data-table heavy domains such as financial decision making."" Keyphrase: ""Hallucination in niche domains""",0
arXIv2023,Removing RLHF Protections in GPT-4 via Fine-Tuning,Yes.,5,"""fine-tuning can remove RLHF protections"" and ""Our results show the need for further research on protections on LLMs.""",2023,2023-11-09T17:54:59Z,"Keyphrase: ""Limited protection from harmful content""","""fine-tuning can remove RLHF protections"" and ""Our results show the need for further research on protections on LLMs."" Keyphrase: ""Limited protection from harmful content""",2
arXIv2023,Do personality tests generalize to Large Language Models?,Yes.,5,"""LLMs' responses to personality tests systematically deviate from typical human responses,"" and ""reverse-coded items (e.g. 'I am introverted' vs 'I am extraverted') are often both answered affirmatively by LLMs,"" and ""variation across different prompts designed to 'steer' LLMs to simulate particular personality types does not follow the clear separation",2023,2023-11-09T11:54:01Z,"Keyphrase: ""Limited ability to simulate human-like responses""","""LLMs' responses to personality tests systematically deviate from typical human responses,"" and ""reverse-coded items (e.g. 'I am introverted' vs 'I am extraverted') are often both answered affirmatively by LLMs,"" and ""variation across different prompts designed to 'steer' LLMs to simulate particular personality types does not follow the clear separation Keyphrase: ""Limited ability to simulate human-like responses""",0
arXIv2023,"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",Yes.,5,"""LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs."" and ""This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios.""",2023,2023-11-09T09:25:37Z,"Keyphrase: ""Hallucination and inconsistency""","""LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs."" and ""This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios."" Keyphrase: ""Hallucination and inconsistency""",0
arXIv2023,"Frontier Language Models are not Robust to Adversarial Arithmetic, or ""What do I need to say so you agree 2+2=5?",Yes.,5,"""Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and",2023,2023-11-08T19:07:10Z,"Keyphrase: ""Vulnerability to adversarial prompts""","""Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and Keyphrase: ""Vulnerability to adversarial prompts""",2
arXIv2023,Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs,Yes.,5,"""Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas"" and ""Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness.""",2023,2023-11-08T18:52:17Z,"Keyphrase: ""Deep-rooted biases""","""Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas"" and ""Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness."" Keyphrase: ""Deep-rooted biases""",3
arXIv2023,LooGLE: Can Long-Context Language Models Understand Long Contexts?,Yes.,5,"""LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks"" and ""strategies for extending context window length had limited impact on long context understanding.""",2023,2023-11-08T01:45:37Z,"Keyphrase: ""Limited long-context understanding""","""LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks"" and ""strategies for extending context window length had limited impact on long context understanding."" Keyphrase: ""Limited long-context understanding""",5
arXIv2023,Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning,Yes.,5,"""LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations."" and ""Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication.""",2023,2023-11-07T21:09:57Z,"Keyphrase: ""Fabricated evidence hallucination""","""LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations."" and ""Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication."" Keyphrase: ""Fabricated evidence hallucination""",0
arXIv2023,Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications,Yes.,4,"""LLM-integrated applications also introduce new attack surfaces,"" and ""Successful exploits of the identified vulnerabilities result in the users receiving responses tailored to the intent of a threat initiator,"" and ""our empirical results show that the threats can effectively bypass the restrictions and moderation policies of OpenAI, resulting in users receiving responses that contain bias, toxic content, privacy risk, and disinformation.""",2023,2023-11-07T20:13:05Z,"Keyphrase: ""Vulnerability to malicious intent""","""LLM-integrated applications also introduce new attack surfaces,"" and ""Successful exploits of the identified vulnerabilities result in the users receiving responses tailored to the intent of a threat initiator,"" and ""our empirical results show that the threats can effectively bypass the restrictions and moderation policies of OpenAI, resulting in users receiving responses that contain bias, toxic content, privacy risk, and disinformation."" Keyphrase: ""Vulnerability to malicious intent""",2
arXIv2023,Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation,Yes.,5,"""However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures."" and ""However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models.""",2023,2023-11-07T18:03:23Z,"Keyphrase: ""Limited ability to acquire new knowledge""","""However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures."" and ""However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models."" Keyphrase: ""Limited ability to acquire new knowledge""",5
arXIv2023,Unveiling Safety Vulnerabilities of Large Language Models,Yes.,5,"""As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern."" and ""We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it.""",2023,2023-11-07T16:50:33Z,"Keyphrase: ""Potential for harmful and inappropriate responses""","""As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern."" and ""We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it."" Keyphrase: ""Potential for harmful and inappropriate responses""",2
arXIv2023,Do LLMs exhibit human-like response biases? A case study in survey design,Yes.,5,"""One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording"" and ""Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF.""",2023,2023-11-07T15:40:43Z,"Keyphrase: ""Failure to reflect humanlike behavior""","""One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording"" and ""Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF."" Keyphrase: ""Failure to reflect humanlike behavior""",0
arXIv2023,Input Reconstruction Attack against Vertical Federated Large Language Models,Yes.,5,"""privacy concerns limit their usage in real-life businesses"" and ""we demonstrate that in LLMs, VFL fails to protect the user input since it is simple and cheap to reconstruct the input from the intermediate embeddings.""",2023,2023-11-07T09:39:22Z,"Keyphrase: ""Privacy vulnerabilities""","""privacy concerns limit their usage in real-life businesses"" and ""we demonstrate that in LLMs, VFL fails to protect the user input since it is simple and cheap to reconstruct the input from the intermediate embeddings."" Keyphrase: ""Privacy vulnerabilities""",8
arXIv2023,A Survey of Large Language Models Attribution,Yes.,4,"""issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems.""",2023,2023-11-07T05:20:09Z,"Keyphrase: ""Inherent biases and attribution issues""","""issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems."" Keyphrase: ""Inherent biases and attribution issues""",3
arXIv2023,Quantifying Uncertainty in Natural Language Explanations of Large Language Models,Yes.,5,"""However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior."" and ""Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.""",2023,2023-11-06T21:14:40Z,"Keyphrase: ""Uncertain explanations""","""However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior."" and ""Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models."" Keyphrase: ""Uncertain explanations""",1
arXIv2023,Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation,Yes.,4,"""Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour."" and ""Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.""",2023,2023-11-06T18:55:18Z,"Keyphrase: ""Vulnerability to harmful prompts""","""Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour."" and ""Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards."" Keyphrase: ""Vulnerability to harmful prompts""",2
arXIv2023,Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance,Yes.,4,"""Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners.""",2023,2023-11-06T18:01:34Z,"Keyphrase: ""Inherent bias""","""Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners."" Keyphrase: ""Inherent bias""",3
arXIv2023,Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges,Yes.,5,"""This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models",2023,2023-11-06T17:26:59Z,"Keyphrase: ""Hallucination in evaluations""","""This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models Keyphrase: ""Hallucination in evaluations""",0
arXIv2023,Instructed Language Models with Retrievers Are Powerful Entity Linkers,Yes.,5,"""Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base."" and ""reaffirming that the EL task remains a persistent hurdle for general LLMs.""",2023,2023-11-06T16:38:51Z,"Keyphrase: ""Hallucination in generative content""","""Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base."" and ""reaffirming that the EL task remains a persistent hurdle for general LLMs."" Keyphrase: ""Hallucination in generative content""",0
arXIv2023,DeepInception: Hypnotize Large Language Model to Be Jailbreaker,Yes.,5,"""Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void."" and ""Our investigation appeals to people to pay more attention to the safety aspects of LLMs and develop a stronger defense against their misuse risks.""",2023,2023-11-06T15:29:30Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void."" and ""Our investigation appeals to people to pay more attention to the safety aspects of LLMs and develop a stronger defense against their misuse risks."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arXIv2023,FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM,Yes.,5,"""LLMs are constrained by the knowledge within their training data and are prone to generating inaccurate, or 'hallucinated', information.""",2023,2023-11-05T08:34:26Z,"Keyphrase: ""Inaccurate hallucinations""","""LLMs are constrained by the knowledge within their training data and are prone to generating inaccurate, or 'hallucinated', information."" Keyphrase: ""Inaccurate hallucinations""",0
arXIv2023,Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles,Yes.,4,"""Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting.""",2023,2023-11-04T03:18:45Z,"Keyphrase: ""Limited few-shot translation performance""","""Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting."" Keyphrase: ""Limited few-shot translation performance""",6
arXIv2023,An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology,Yes.,4,"""However, these LLMs are prone to many errors such as hallucinations, biases, and ethical violations, which necessitate rigorous evaluation and validation before clinical deployment.""",2023,2023-11-03T19:32:35Z,"Keyphrase: ""Error hallucination and ethical violations""","""However, these LLMs are prone to many errors such as hallucinations, biases, and ethical violations, which necessitate rigorous evaluation and validation before clinical deployment."" Keyphrase: ""Error hallucination and ethical violations""",0
arXIv2023,The Alignment Problem in Context,Yes.,5,"""large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour"" and ""the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities.""",2023,2023-11-03T17:57:55Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour"" and ""the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arXIv2023,Comprehensive Assessment of Toxicity in ChatGPT,Yes.,4,"""The emerging large language models (LLMs), such as ChatGPT, can potentially further accentuate this threat."" and ""Previous works have discovered that ChatGPT can generate toxic responses using carefully crafted inputs.""",2023,2023-11-03T14:37:53Z,"Keyphrase: ""Generation of toxic responses""","""The emerging large language models (LLMs), such as ChatGPT, can potentially further accentuate this threat."" and ""Previous works have discovered that ChatGPT can generate toxic responses using carefully crafted inputs."" Keyphrase: ""Generation of toxic responses""",2
arXIv2023,PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion,Yes.,5,"""The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy. We find three main error causes in our benchmark",2023,2023-11-03T08:06:35Z,"Keyphrase: ""Limited multi-turn dialogue capabilities""","""The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy. We find three main error causes in our benchmark Keyphrase: ""Limited multi-turn dialogue capabilities""",6
arXIv2023,FinGPT: Large Generative Models for a Small Language,Yes.,4,"""LLM work tends to focus on languages where nearly unlimited data is available for pretraining.""",2023,2023-11-03T08:05:04Z,"Keyphrase: ""Limited focus on language diversity""","""LLM work tends to focus on languages where nearly unlimited data is available for pretraining."" Keyphrase: ""Limited focus on language diversity""",5
arXIv2023,Successor Features for Efficient Multisubject Controlled Text Generation,Yes.,5,"""While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging.""",2023,2023-11-03T00:17:08Z,"Keyphrase: ""Challenges in safety and factuality""","""While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging."" Keyphrase: ""Challenges in safety and factuality""",0
arXIv2023,Preserving the knowledge of long clinical texts using aggregated ensembles of large language models,Yes.,4,"""applying large language models, such as BERT-based models, to clinical texts poses two major challenges",2023,2023-11-02T19:50:02Z,"Keyphrase: ""Challenges in clinical text""","""applying large language models, such as BERT-based models, to clinical texts poses two major challenges Keyphrase: ""Challenges in clinical text""",6
arXIv2023,"The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",Yes.,5,"""Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions.""",2023,2023-11-02T15:20:11Z,"Keyphrase: ""Inconsistent factual knowledge""","""Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions."" Keyphrase: ""Inconsistent factual knowledge""",0
arXIv2023,Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism,Yes.,5,"""these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios.""",2023,2023-11-02T07:20:49Z,"Keyphrase: ""Error-prone hallucination""","""these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios."" Keyphrase: ""Error-prone hallucination""",0
arXIv2023,Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game,Yes.,5,"""While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks",2023,2023-11-02T06:13:36Z,"Keyphrase: ""Vulnerability to prompt injection attacks""","""While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks Keyphrase: ""Vulnerability to prompt injection attacks""",2
arXIv2023,Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code,Yes.,5,"""Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code."" and ""existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security."" and ""existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations.""",2023,2023-11-01T22:46:31Z,"Keyphrase: ""Neglect of security considerations""","""Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code."" and ""existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security."" and ""existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations."" Keyphrase: ""Neglect of security considerations""",2
arXIv2023,Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models,Yes.,5,"""However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks.""",2023,2023-11-01T21:41:08Z,"Keyphrase: ""Degradation in generalization""","""However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks."" Keyphrase: ""Degradation in generalization""",5
arXIv2023,Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs,Yes.,5,"""Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across two factuality subcategories. These consistent findings across various factual error categories suggest a fundamental limitation in the current LLMs' capability to accurately gauge factual",2023,2023-11-01T17:42:45Z,"Keyphrase: ""Limited factual accuracy""","""Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across two factuality subcategories. These consistent findings across various factual error categories suggest a fundamental limitation in the current LLMs' capability to accurately gauge factual Keyphrase: ""Limited factual accuracy""",0
arXIv2023,Crosslingual Retrieval Augmented In-context Learning for Bangla,Yes.,5,"""The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla.""",2023,2023-11-01T15:32:50Z,"Keyphrase: ""Limited performance on low-resource languages""","""The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla."" Keyphrase: ""Limited performance on low-resource languages""",4
arXIv2023,Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation,Yes.,5,"""Large Language Models (LLMs) can generate biased and toxic responses."", ""all tested LLMs exhibit explicit and/or implicit gender bias, even when explicit gender stereotypes are absent in the inputs.""",2023,2023-11-01T05:31:46Z,"Keyphrase: ""Gender bias and toxicity""","""Large Language Models (LLMs) can generate biased and toxic responses."", ""all tested LLMs exhibit explicit and/or implicit gender bias, even when explicit gender stereotypes are absent in the inputs."" Keyphrase: ""Gender bias and toxicity""",3
arXIv2023,Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias,Yes.,4,"""The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included."" and ""disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans."" and ""these results underscore the importance of meticulous conditioning, model",2023,2023-11-01T01:32:59Z,"Keyphrase: ""Inaccurate representation of perspectives""","""The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included."" and ""disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans."" and ""these results underscore the importance of meticulous conditioning, model Keyphrase: ""Inaccurate representation of perspectives""",3
arXIv2023,The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback,Yes.,5,"""Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations.""",2023,2023-10-31T21:52:41Z,"Keyphrase: ""Safety and reliability issues""","""Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations."" Keyphrase: ""Safety and reliability issues""",2
arXIv2023,BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B,Yes.,5,"""We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are",2023,2023-10-31T19:45:15Z,"Keyphrase: ""Ineffective safety fine-tuning""","""We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are Keyphrase: ""Ineffective safety fine-tuning""",2
arXIv2023,LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B,Yes.,5,"""We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat."" and ""While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate",2023,2023-10-31T16:55:06Z,"Keyphrase: ""Safety and security risks""","""We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat."" and ""While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate Keyphrase: ""Safety and security risks""",2
arXIv2023,Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT,Yes.,5,"""these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input.""",2023,2023-10-31T15:41:08Z,"Keyphrase: ""Token limit constraint""","""these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input."" Keyphrase: ""Token limit constraint""",4
arXIv2023,The Expressibility of Polynomial based Attention Scheme,Yes.,5,"""the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference.""",2023,2023-10-30T22:16:18Z,"Keyphrase: ""Scalability challenges""","""the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference."" Keyphrase: ""Scalability challenges""",4
arXIv2023,Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization,Yes.,5,"""community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses.""",2023,2023-10-30T21:33:22Z,"Keyphrase: ""Factually hallucinated summaries""","""community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses."" Keyphrase: ""Factually hallucinated summaries""",0
arXIv2023,Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation,Yes.,4,"""However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements.""",2023,2023-10-30T17:04:35Z,"Keyphrase: ""Subjectivity and unreliable judgment""","""However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements."" Keyphrase: ""Subjectivity and unreliable judgment""",6
arXIv2023,Adversarial Attacks and Defenses in Large Language Models: Old and New Threats,Yes.,4,"""substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude"" and ""we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models.""",2023,2023-10-30T17:01:02Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude"" and ""we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arXIv2023,Evaluating Large Language Models: A Comprehensive Survey,Yes.,5,"""LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards.""",2023,2023-10-30T17:00:52Z,"Keyphrase: ""Privacy risks and potential harm""","""LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards."" Keyphrase: ""Privacy risks and potential harm""",2
arXIv2023,Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs,Yes.,5,"""they often generate summaries that are factually inconsistent with original articles, known as 'hallucinations' in text generation."" and ""current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, overgeneralizing, etc.""",2023,2023-10-30T08:40:16Z,"Keyphrase: ""Factually inconsistent summaries""","""they often generate summaries that are factually inconsistent with original articles, known as 'hallucinations' in text generation."" and ""current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, overgeneralizing, etc."" Keyphrase: ""Factually inconsistent summaries""",0
arXIv2023,M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models,Yes.,5,"""Our results reveal that",2023,2023-10-30T03:11:30Z,Keyphrase: Lack of context or specificity,"""Our results reveal that Keyphrase: Lack of context or specificity",5
arXIv2023,"From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude",Yes.,4,"""However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks.""",2023,2023-10-29T22:52:40Z,"Keyphrase: ""Susceptible to generating malicious content""","""However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks."" Keyphrase: ""Susceptible to generating malicious content""",2
arXIv2023,N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics,Yes.,5,"""We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination."" and ""enhance trustworthiness by addressing fairness, bias, and robustness concerns.""",2023,2023-10-28T11:22:22Z,"Keyphrase: ""Toxicity and hallucination""","""We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination."" and ""enhance trustworthiness by addressing fairness, bias, and robustness concerns."" Keyphrase: ""Toxicity and hallucination""",0
arXIv2023,DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues,Yes.,4,"""This dataset presents challenges concerning knowledge recency, safety, fairness, and bias.""",2023,2023-10-27T13:23:02Z,"Keyphrase: ""Challenges in knowledge recency and bias""","""This dataset presents challenges concerning knowledge recency, safety, fairness, and bias."" Keyphrase: ""Challenges in knowledge recency and bias""",3
arXIv2023,SOUL: Towards Sentiment and Opinion Understanding of Language,Yes.,5,"""Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications.""",2023,2023-10-27T06:48:48Z,"Keyphrase: ""Performance gap compared to human reasoning""","""Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications."" Keyphrase: ""Performance gap compared to human reasoning""",7
arXIv2023,Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method,Yes.,5,"""recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization.""",2023,2023-10-27T06:22:14Z,"Keyphrase: ""Nonfactual responses""","""recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization."" Keyphrase: ""Nonfactual responses""",0
arXIv2023,Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory,Yes.,5,"""Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning.""",2023,2023-10-27T04:15:30Z,"Keyphrase: ""Privacy leakage""","""Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning."" Keyphrase: ""Privacy leakage""",8
arXIv2023,"""You Are An Expert Linguistic Annotator"": Limits of LLMs as Analyzers of Abstract Meaning Representation",Yes.,5,"""we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure"" and ""model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses.""",2023,2023-10-26T21:47:59Z,"Keyphrase: ""Frequent major errors""","""we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure"" and ""model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses."" Keyphrase: ""Frequent major errors""",6
arXIv2023,Evaluation of large language models using an Indian language LGBTI+ lexicon,Yes.,4,"""Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English.""",2023,2023-10-26T21:32:24Z,"Keyphrase: ""Limited detection of hateful content""","""Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English."" Keyphrase: ""Limited detection of hateful content""",2
arXIv2023,A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications,Yes.,4,"""We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles.""",2023,2023-10-26T19:45:06Z,"Keyphrase: ""Violation of fairness principles""","""We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles."" Keyphrase: ""Violation of fairness principles""",3
arXIv2023,Proving Test Set Contamination in Black Box Language Models,Yes.,5,"""Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks."" and ""Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.""",2023,2023-10-26T17:43:13Z,"Keyphrase: ""Overreliance on memorization""","""Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks."" and ""Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples."" Keyphrase: ""Overreliance on memorization""",8
arXIv2023,An Open Source Data Contamination Report for Large Language Models,Yes.,4,"""Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models."" and ""Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics.""",2023,2023-10-26T17:11:42Z,"Keyphrase: ""Data contamination issues""","""Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models."" and ""Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics."" Keyphrase: ""Data contamination issues""",0
arXIv2023,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,Yes.,5,"""LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses.""",2023,2023-10-26T03:28:30Z,"Keyphrase: ""Limited external knowledge attribution""","""LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses."" Keyphrase: ""Limited external knowledge attribution""",0
arXIv2023,Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation,Yes.,5,"""The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Specifically, it showed limitations when dealing with non-Latin languages and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image.""",2023,2023-10-25T17:38:55Z,"Keyphrase: ""Struggles with multilingual and complex tasks""","""The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Specifically, it showed limitations when dealing with non-Latin languages and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image."" Keyphrase: ""Struggles with multilingual and complex tasks""",1
arXIv2023,SuperHF: Supervised Iterative Learning from Human Feedback,Yes.,5,"""While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training.""",2023,2023-10-25T16:52:00Z,"Keyphrase: ""Safety and value alignment challenges""","""While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training."" Keyphrase: ""Safety and value alignment challenges""",2
arXIv2023,HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models,Yes.,5,"""Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.""",2023,2023-10-25T16:41:15Z,"Keyphrase: ""Decline in performance on higher-order tasks""","""Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP."" Keyphrase: ""Decline in performance on higher-order tasks""",7
arXIv2023,"R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",Yes.,4,"""the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated.""",2023,2023-10-25T10:34:02Z,"Keyphrase: ""Inaccurate results in noisy contexts""","""the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated."" Keyphrase: ""Inaccurate results in noisy contexts""",0
arXIv2023,An Early Evaluation of GPT-4V(ision),Yes.,5,"""Our experimental results reveal the ability and limitations of GPT-4V"" and ""GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age; (3) GPT-4V obtains worse results",2023,2023-10-25T10:33:17Z,"Keyphrase: ""Cross-lingual and sensitive trait performance issues""","""Our experimental results reveal the ability and limitations of GPT-4V"" and ""GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age; (3) GPT-4V obtains worse results Keyphrase: ""Cross-lingual and sensitive trait performance issues""",3
arXIv2023,OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models,Yes.,4,"""existing instruction-tuning datasets suffer from occupational bias",2023,2023-10-25T10:06:17Z,"Keyphrase: ""Occupational bias""","""existing instruction-tuning datasets suffer from occupational bias Keyphrase: ""Occupational bias""",3
arXIv2023,Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation,Yes.,5,"""Our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. To address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs. Moreover, our study identifies",2023,2023-10-25T00:32:56Z,"Keyphrase: ""Overlooking security concerns""","""Our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. To address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs. Moreover, our study identifies Keyphrase: ""Overlooking security concerns""",2
arXIv2023,Knowledge Editing for Large Language Models: A Survey,Yes.,4,"""Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model.""",2023,2023-10-24T22:18:13Z,"Keyphrase: ""Substantial computational cost""","""Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model."" Keyphrase: ""Substantial computational cost""",4
arXIv2023,Can You Follow Me? Testing Situational Understanding in ChatGPT,Yes.,5,"""Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs),"" and ""despite the fundamental simplicity of the task, the model's performance reflects an inability to retain correct environment states across time,"" and ""performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to halluc",2023,2023-10-24T19:22:01Z,"Keyphrase: ""Lack of persistent in-context memory""","""Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs),"" and ""despite the fundamental simplicity of the task, the model's performance reflects an inability to retain correct environment states across time,"" and ""performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to halluc Keyphrase: ""Lack of persistent in-context memory""",5
arXIv2023,Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition,Yes.,5,"""These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones.""",2023,2023-10-24T18:18:11Z,"Keyphrase: ""Vulnerability to prompt manipulation""","""These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones."" Keyphrase: ""Vulnerability to prompt manipulation""",2
arXIv2023,MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning,Yes.,5,"""While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings.""",2023,2023-10-24T17:59:20Z,"Keyphrase: ""Limited reasoning ability""","""While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings."" Keyphrase: ""Limited reasoning ability""",1
arXIv2023,Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs,Yes.,5,"""it is important to investigate their limitations in dealing with different image and question properties."" and ""we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size.""",2023,2023-10-24T17:48:04Z,"Keyphrase: ""Limited zero-shot accuracy in answering visual questions""","""it is important to investigate their limitations in dealing with different image and question properties."" and ""we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size."" Keyphrase: ""Limited zero-shot accuracy in answering visual questions""",6
arXIv2023,This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models,Yes.,5,"""Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing"" and ""Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on",2023,2023-10-24T15:38:21Z,"Keyphrase: ""Struggles with negation""","""Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing"" and ""Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on Keyphrase: ""Struggles with negation""",1
arXIv2023,BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT,Yes.,5,"""However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning.""",2023,2023-10-24T14:57:34Z,"Keyphrase: ""Lack of multiturn engagement""","""However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning."" Keyphrase: ""Lack of multiturn engagement""",6
arXIv2023,SoK: Memorization in General-Purpose Large Language Models,Yes.,5,"""This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond.""",2023,2023-10-24T14:25:53Z,"Keyphrase: ""Privacy and security concerns""","""This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond."" Keyphrase: ""Privacy and security concerns""",8
arXIv2023,Self-Guard: Empower the LLM to Safeguard Itself,Yes.,5,"""The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content... safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help.""",2023,2023-10-24T14:08:26Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content... safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arXIv2023,Generative Language Models Exhibit Social Identity Biases,Yes.,4,"""The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans."" and ""Our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data.""",2023,2023-10-24T13:17:40Z,"Keyphrase: ""Social identity bias""","""The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans."" and ""Our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data."" Keyphrase: ""Social identity bias""",3
arXIv2023,Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers,Yes.,5,"""the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability"" and ""Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations.""",2023,2023-10-24T12:37:06Z,"Keyphrase: ""Vulnerability to shortcut manipulation""","""the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability"" and ""Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations."" Keyphrase: ""Vulnerability to shortcut manipulation""",2
arXIv2023,Prevalence and prevention of large language model use in crowd work,Yes.,4,"""LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data"" and ""preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use L",2023,2023-10-24T09:52:09Z,"Keyphrase: ""Homogeneous responses and human behavior degradation""","""LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data"" and ""preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use L Keyphrase: ""Homogeneous responses and human behavior degradation""",3
arXIv2023,KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval,Yes.,5,"""Motivated by rising concerns around factual incorrectness and hallucinations of LLMs,"" and ""Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases.""",2023,2023-10-24T04:40:38Z,"Keyphrase: ""Factual incorrectness and irrelevant information""","""Motivated by rising concerns around factual incorrectness and hallucinations of LLMs,"" and ""Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases."" Keyphrase: ""Factual incorrectness and irrelevant information""",0
arXIv2023,The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks,Yes.,5,"""security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure,"" and ""Our findings indicate that, with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from being impermeable to PII",2023,2023-10-24T02:48:19Z,"Keyphrase: ""Privacy and security risks""","""security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure,"" and ""Our findings indicate that, with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from being impermeable to PII Keyphrase: ""Privacy and security risks""",8
arXIv2023,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,Yes.,5,"""We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.""",2023,2023-10-24T00:24:11Z,"Keyphrase: ""Limited chain-of-thought reasoning""","""We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning."" Keyphrase: ""Limited chain-of-thought reasoning""",1
arXIv2023,EpiK-Eval: Evaluation for Language Models as Epistemic Models,Yes.,5,"""Evaluations across various LLMs reveal significant weaknesses in this domain.""",2023,2023-10-23T21:15:54Z,"Keyphrase: ""Domain weaknesses""","""Evaluations across various LLMs reveal significant weaknesses in this domain."" Keyphrase: ""Domain weaknesses""",6
arXIv2023,"Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",Yes.,5,"""We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence",2023,2023-10-23T20:35:52Z,"Keyphrase: ""Hallucination of constrained output""","""We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence Keyphrase: ""Hallucination of constrained output""",0
arXIv2023,Moral Foundations of Large Language Models,Yes.,4,"""they may reflect the biases that are present in such corpora"" and ""illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance.""",2023,2023-10-23T20:05:37Z,"Keyphrase: ""Assumed moral stance""","""they may reflect the biases that are present in such corpora"" and ""illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance."" Keyphrase: ""Assumed moral stance""",3
arXIv2023,"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models",Yes.,5,"""it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration."" and ""experimental results have shown that it poses significant challenges for all existing LLMs.""",2023,2023-10-23T17:52:06Z,"Keyphrase: ""Evaluation challenges due to text length""","""it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration."" and ""experimental results have shown that it poses significant challenges for all existing LLMs."" Keyphrase: ""Evaluation challenges due to text length""",4
arXIv2023,Towards LLM-driven Dialogue State Tracking,Yes.,5,"""Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities.""",2023,2023-10-23T14:15:28Z,"Keyphrase: ""Closed-source and data privacy concerns""","""Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities."" Keyphrase: ""Closed-source and data privacy concerns""",8
arXIv2023,Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism,Yes.,5,"""We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible ->implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family.""",2023,2023-10-23T12:40:41Z,"Keyphrase: ""Limited handling of lexical negation""","""We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible ->implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family."" Keyphrase: ""Limited handling of lexical negation""",1
arXIv2023,ALCUNA: Large Language Models Meet New Knowledge,Yes.,5,"""We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge.""",2023,2023-10-23T11:40:05Z,"Keyphrase: ""Limited reasoning capabilities""","""We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge."" Keyphrase: ""Limited reasoning capabilities""",1
arXIv2023,Evaluating the Knowledge Base Completion Potential of GPT,Yes.,5,"""We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task.""",2023,2023-10-23T10:15:13Z,"Keyphrase: ""Limited task performance""","""We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task."" Keyphrase: ""Limited task performance""",6
arXIv2023,Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models,Yes.,4,"""We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge.""",2023,2023-10-23T08:45:12Z,"Keyphrase: ""Limited lexical knowledge""","""We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge."" Keyphrase: ""Limited lexical knowledge""",1
arXIv2023,Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications,Yes.,5,"""LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society,"" ""LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks,"" and ""the fairness metric gap between different subgroups is still larger than that in traditional machine learning models.""",2023,2023-10-23T06:31:28Z,"Keyphrase: ""Harmful social bias and fairness gaps""","""LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society,"" ""LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks,"" and ""the fairness metric gap between different subgroups is still larger than that in traditional machine learning models."" Keyphrase: ""Harmful social bias and fairness gaps""",3
arXIv2023,"Language Models Hallucinate, but May Excel at Fact Verification",Yes.,5,"""Nevertheless, LLMs frequently 'hallucinate,' resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time.""",2023,2023-10-23T04:39:01Z,"Keyphrase: ""Frequent hallucinations""","""Nevertheless, LLMs frequently 'hallucinate,' resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time."" Keyphrase: ""Frequent hallucinations""",0
arXIv2023,The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages,Yes.,5,"""Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task",2023,2023-10-23T04:22:44Z,"Keyphrase: ""Struggles with understanding social meaning""","""Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task Keyphrase: ""Struggles with understanding social meaning""",6
arXIv2023,Evaluating Large Language Models on Controlled Generation Tasks,Yes.,5,"""We conclude that large language models struggle at meeting fine-grained hard constraints.""",2023,2023-10-23T03:48:24Z,"Keyphrase: ""Difficulty with fine-grained hard constraints""","""We conclude that large language models struggle at meeting fine-grained hard constraints."" Keyphrase: ""Difficulty with fine-grained hard constraints""",1
arXIv2023,Retrieval-Augmented Chain-of-Thought in Semi-structured Domains,Yes.,5,"""their inability to handle very long inputs/contexts is well known.""",2023,2023-10-22T22:45:14Z,"Keyphrase: ""Struggles with long inputs""","""their inability to handle very long inputs/contexts is well known."" Keyphrase: ""Struggles with long inputs""",4
arXIv2023,Large Language Models are biased to overestimate profoundness,Yes.,5,"""LLMs systematically overestimate the profoundness of nonsensical statements,"" and ""provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.""",2023,2023-10-22T21:33:50Z,"Keyphrase: ""Overestimation of profoundness""","""LLMs systematically overestimate the profoundness of nonsensical statements,"" and ""provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements."" Keyphrase: ""Overestimation of profoundness""",6
arXIv2023,Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis,Yes.,4,"""Ethical restrictions prohibit large language models (LLMs) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models.""",2023,2023-10-22T15:19:04Z,"Keyphrase: ""Ethical restrictions on harmful content""","""Ethical restrictions prohibit large language models (LLMs) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models."" Keyphrase: ""Ethical restrictions on harmful content""",2
arXIv2023,Chainpoll: A high efficacy method for LLM hallucination detection,Yes.,5,"""hallucinations - incorrect or unfounded claims - are still prevalent,"" and ""we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use.""",2023,2023-10-22T14:45:14Z,"Keyphrase: ""Hallucination and unfounded claims""","""hallucinations - incorrect or unfounded claims - are still prevalent,"" and ""we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use."" Keyphrase: ""Hallucination and unfounded claims""",0
arXIv2023,Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases,Yes.,5,"""Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training,"" and ""Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA-2-CHAT where the model's responses are strongly biased and opinionated 64% of the time.""",2023,2023-10-22T13:55:46Z,"Keyphrase: ""Unaddressed harmful biases""","""Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training,"" and ""Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA-2-CHAT where the model's responses are strongly biased and opinionated 64% of the time."" Keyphrase: ""Unaddressed harmful biases""",2
arXIv2023,From Static to Dynamic: A Continual Learning Framework for Large Language Models,Yes.,4,"""However, this complexity also presents challenges, making LLMs difficult to train and inhibiting their ability to continuously assimilate new knowledge, which may lead to inaccuracies in their outputs.""",2023,2023-10-22T10:18:53Z,"Keyphrase: ""Difficulty in continuous learning""","""However, this complexity also presents challenges, making LLMs difficult to train and inhibiting their ability to continuously assimilate new knowledge, which may lead to inaccuracies in their outputs."" Keyphrase: ""Difficulty in continuous learning""",5
arXIv2023,MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications,Yes.,4,"""However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of",2023,2023-10-21T17:18:09Z,"Keyphrase: ""Data imbalance and high cost in medical finetuning""","""However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of Keyphrase: ""Data imbalance and high cost in medical finetuning""",2
arXIv2023,Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain,Yes.,5,"""We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that",2023,2023-10-21T16:14:56Z,"Keyphrase: ""Lack of self-consistency""","""We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that Keyphrase: ""Lack of self-consistency""",7
arXIv2023,BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues,Yes.,4,"""other LLMs struggle to generate multi-turn dialogues of satisfactory quality due to poor instruction-following capability, tendency to generate lengthy utterances, or limited general capability.""",2023,2023-10-20T16:53:51Z,"Keyphrase: ""Poor multiturn dialogue quality""","""other LLMs struggle to generate multi-turn dialogues of satisfactory quality due to poor instruction-following capability, tendency to generate lengthy utterances, or limited general capability."" Keyphrase: ""Poor multiturn dialogue quality""",6
arXIv2023,Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning,Yes.,5,"""Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers.""",2023,2023-10-20T16:05:35Z,"Keyphrase: ""Misinterpretation of questions""","""Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers."" Keyphrase: ""Misinterpretation of questions""",1
arXIv2023,Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning,Yes.,5,"""current LLM-based MT systems are brittle",2023,2023-10-20T12:29:51Z,"Keyphrase: ""Brittle performance""","""current LLM-based MT systems are brittle Keyphrase: ""Brittle performance""",7
arXIv2023,POSQA: Probe the World Models of LLMs with Size Comparisons,Yes.,5,"""We show that even the largest LLMs today perform poorly under the zero-shot setting,"" and ""Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.""",2023,2023-10-20T10:05:01Z,"Keyphrase: ""Vulnerability to deception and confusion""","""We show that even the largest LLMs today perform poorly under the zero-shot setting,"" and ""Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours."" Keyphrase: ""Vulnerability to deception and confusion""",6
arXIv2023,Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,Yes.,5,"""However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to 'a blindfolded text-based game.' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand.""",2023,2023-10-20T03:22:05Z,"Keyphrase: ""Limited visual comprehension""","""However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to 'a blindfolded text-based game.' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand."" Keyphrase: ""Limited visual comprehension""",1
arXIv2023,Probing LLMs for hate speech detection: strengths and vulnerabilities,Yes.,4,"""we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute 'jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.""",2023,2023-10-19T16:11:02Z,"Keyphrase: ""Failure to classify errors""","""we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute 'jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts."" Keyphrase: ""Failure to classify errors""",2
arXIv2023,Prompt Injection Attacks and Defenses in LLM-Integrated Applications,Yes.,4,"""Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires.""",2023,2023-10-19T15:12:09Z,"Keyphrase: ""Vulnerability to prompt injection attacks""","""Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires."" Keyphrase: ""Vulnerability to prompt injection attacks""",2
arXIv2023,Safe RLHF: Safe Reinforcement Learning from Human Feedback,Yes.,4,"""the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training"" and ""We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints.""",2023,2023-10-19T14:22:03Z,"Keyphrase: ""Safety concerns and optimization challenges""","""the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training"" and ""We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints."" Keyphrase: ""Safety concerns and optimization challenges""",2
arXIv2023,Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong,Yes.,5,"""However, they over-rely on the LLMs when the explanation is wrong."" and ""Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.""",2023,2023-10-19T08:09:58Z,"Keyphrase: ""Unreliable explanations""","""However, they over-rely on the LLMs when the explanation is wrong."" and ""Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences."" Keyphrase: ""Unreliable explanations""",1
arXIv2023,Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks,Yes.,5,"""Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex.""",2023,2023-10-19T06:37:32Z,"Keyphrase: ""Hallucination in question-answering scenarios""","""Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex."" Keyphrase: ""Hallucination in question-answering scenarios""",0
arXIv2023,Attack Prompt Generation for Red Teaming and Defending Large Language Models,Yes.,4,"""Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content.""",2023,2023-10-19T06:15:05Z,"Keyphrase: ""Susceptibility to harmful content generation""","""Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content."" Keyphrase: ""Susceptibility to harmful content generation""",2
arXIv2023,Contrastive Learning for Inference in Dialogue,Yes.,5,"""While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning.""",2023,2023-10-19T04:49:36Z,"Keyphrase: ""Limited deductive reasoning""","""While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning."" Keyphrase: ""Limited deductive reasoning""",1
arXIv2023,"Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher",Yes.,5,"""challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the LLM hallucination problem.""",2023,2023-10-19T03:49:36Z,"Keyphrase: ""Hallucination problem and unreliable results""","""challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the LLM hallucination problem."" Keyphrase: ""Hallucination problem and unreliable results""",0
arXIv2023,PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models,Yes.,4,"""However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs.""",2023,2023-10-19T03:25:28Z,"Keyphrase: ""Backdoor vulnerability""","""However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs."" Keyphrase: ""Backdoor vulnerability""",2
arXIv2023,Automated Repair of Declarative Software Specifications in the Era of Large Language Models,Yes.,5,"""Our study revealed that while ChatGPT falls short in comparison to existing techniques, it was able to successfully repair bugs that no other technique could address. Our analysis also identified errors in ChatGPT's generated repairs, including improper operator usage, type errors, higher-order logic misuse, and relational arity mismatches. Additionally, we observed instances of hallucinations in ChatGPT-generated repairs and incons",2023,2023-10-19T02:30:42Z,"Keyphrase: ""Error-prone repairs""","""Our study revealed that while ChatGPT falls short in comparison to existing techniques, it was able to successfully repair bugs that no other technique could address. Our analysis also identified errors in ChatGPT's generated repairs, including improper operator usage, type errors, higher-order logic misuse, and relational arity mismatches. Additionally, we observed instances of hallucinations in ChatGPT-generated repairs and incons Keyphrase: ""Error-prone repairs""",6
arXIv2023,FactCHD: Benchmarking Fact-Conflicting Hallucination Detection,Yes.,5,"""Despite their impressive generative capabilities, LLMs are hindered by fact-conflicting hallucinations in real-world applications."" and ""Experiments on different LLMs expose the shortcomings of current approaches in detecting factual errors accurately.""",2023,2023-10-18T16:27:49Z,"Keyphrase: ""Factual error detection shortcomings""","""Despite their impressive generative capabilities, LLMs are hindered by fact-conflicting hallucinations in real-world applications."" and ""Experiments on different LLMs expose the shortcomings of current approaches in detecting factual errors accurately."" Keyphrase: ""Factual error detection shortcomings""",0
arXIv2023,SPEED: Speculative Pipelined Execution for Efficient Decoding,Yes.,5,"""Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound.""",2023,2023-10-18T16:07:01Z,"Keyphrase: ""Inference latency and memory-bound constraints""","""Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound."" Keyphrase: ""Inference latency and memory-bound constraints""",4
arXIv2023,Emptying the Ocean with a Spoon: Should We Edit Models?,Yes.,5,"""We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations."" and ""We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability,",2023,2023-10-18T13:38:03Z,"Keyphrase: ""Limited trust in direct model editing""","""We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations."" and ""We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, Keyphrase: ""Limited trust in direct model editing""",2
arXIv2023,The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models,Yes.,5,"""Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence.""",2023,2023-10-18T11:01:09Z,"Keyphrase: ""Hallucinatory behavior and overconfidence""","""Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence."" Keyphrase: ""Hallucinatory behavior and overconfidence""",0
arXIv2023,Solving the multiplication problem of a large language model system using a graph-based method,Yes.,5,"""The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication.""",2023,2023-10-18T08:02:00Z,"Keyphrase: ""Inadequate for arithmetic problem solving""","""The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication."" Keyphrase: ""Inadequate for arithmetic problem solving""",6
arXIv2023,SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents,Yes.,5,"""We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.""",2023,2023-10-18T02:27:01Z,"Keyphrase: ""Poor social commonsense reasoning""","""We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills."" Keyphrase: ""Poor social commonsense reasoning""",1
arXIv2023,MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations,Yes.,4,"""Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly."" and ""our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example."" and ""our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long",2023,2023-10-18T00:02:38Z,"Keyphrase: ""Limited knowledge generalization""","""Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly."" and ""our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example."" and ""our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long Keyphrase: ""Limited knowledge generalization""",1
arXIv2023,"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",Yes.,5,"""Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate.""",2023,2023-10-17T18:18:32Z,"Keyphrase: ""Overreliance on parametric knowledge""","""Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate."" Keyphrase: ""Overreliance on parametric knowledge""",0
arXIv2023,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,Yes.,4,"""there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes.""",2023,2023-10-17T18:00:25Z,"Keyphrase: ""Simplistic caricatures""","""there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes."" Keyphrase: ""Simplistic caricatures""",3
arXIv2023,"Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",Yes.,4,"""LLMs often require adaptation with private data, which poses privacy and security challenges."" and ""there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.""",2023,2023-10-17T17:03:00Z,"Keyphrase: ""Privacy and security challenges""","""LLMs often require adaptation with private data, which poses privacy and security challenges."" and ""there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses."" Keyphrase: ""Privacy and security challenges""",8
arXIv2023,Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting,Yes.,5,"""We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B.""",2023,2023-10-17T15:03:30Z,"Keyphrase: ""Sensitivity to prompt formatting""","""We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B."" Keyphrase: ""Sensitivity to prompt formatting""",4
arXIv2023,Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges,Yes.,4,"""We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges.""",2023,2023-10-17T13:20:16Z,"Keyphrase: ""Challenges in data and linguistic understanding""","""We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges."" Keyphrase: ""Challenges in data and linguistic understanding""",6
arXIv2023,H2O Open Ecosystem for State-of-the-art Large Language Models,Yes.,4,"""However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text.""",2023,2023-10-17T09:40:58Z,"Keyphrase: ""Risk of biased, private, and harmful text""","""However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text."" Keyphrase: ""Risk of biased, private, and harmful text""",2
arXIv2023,Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models,Yes.,4,"""These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions.""",2023,2023-10-17T08:56:04Z,"Keyphrase: ""Bias retention in chatbots""","""These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions."" Keyphrase: ""Bias retention in chatbots""",3
arXIv2023,Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation,Yes.,5,"""Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such",2023,2023-10-17T06:53:00Z,"Keyphrase: ""Surface-level understanding""","""Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such Keyphrase: ""Surface-level understanding""",5
arXIv2023,NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain,Yes.,4,"""Our experiments on state-of-the-art models suggest that even the best LLMs perform less than satisfactorily on our benchmark, demonstrating the scientific knowledge gap of existing LLMs.""",2023,2023-10-17T01:27:20Z,"Keyphrase: ""Limited scientific knowledge performance""","""Our experiments on state-of-the-art models suggest that even the best LLMs perform less than satisfactorily on our benchmark, demonstrating the scientific knowledge gap of existing LLMs."" Keyphrase: ""Limited scientific knowledge performance""",7
arXIv2023,BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology,Yes.,5,"""However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments.""",2023,2023-10-16T17:54:20Z,"Keyphrase: ""Limited long-term planning""","""However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments."" Keyphrase: ""Limited long-term planning""",1
arXIv2023,Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers,Yes.,5,"""Hallucination plagues even frontier LLMs"" and ""We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and",2023,2023-10-16T17:51:17Z,"Keyphrase: ""Persistent hallucination""","""Hallucination plagues even frontier LLMs"" and ""We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and Keyphrase: ""Persistent hallucination""",0
arXIv2023,Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis,Yes.,4,"""This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement.""",2023,2023-10-16T14:59:10Z,"Keyphrase: ""Generation of harmful content""","""This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement."" Keyphrase: ""Generation of harmful content""",2
arXIv2023,"Privacy in Large Language Models: Attacks, Defenses and Future Directions",Yes.,4,"""unrestricted access to these models can also introduce potential malicious and unintentional privacy risks"" and ""Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved.""",2023,2023-10-16T13:23:54Z,"Keyphrase: ""Privacy risks and unresolved safety concerns""","""unrestricted access to these models can also introduce potential malicious and unintentional privacy risks"" and ""Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved."" Keyphrase: ""Privacy risks and unresolved safety concerns""",8
arXIv2023,"Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs",Yes.,5,"""we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks.""",2023,2023-10-16T12:51:24Z,"Keyphrase: ""Sensitivity to noise operations""","""we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks."" Keyphrase: ""Sensitivity to noise operations""",6
arXIv2023,Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT,Yes.,5,"""Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain",2023,2023-10-16T08:34:44Z,"Keyphrase: ""Limited performance in zeroshot settings""","""Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain Keyphrase: ""Limited performance in zeroshot settings""",6
arXIv2023,Theory of Mind for Multi-Agent Collaboration via Large Language Models,Yes.,5,"""Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state.""",2023,2023-10-16T07:51:19Z,"Keyphrase: ""Failure in managing long-horizon context""","""Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state."" Keyphrase: ""Failure in managing long-horizon context""",1
arXIv2023,Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks,Yes.,5,"""Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications."" and ""Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development.""",2023,2023-10-16T05:19:25Z,"Keyphrase: ""Vulnerability to generating harmful content""","""Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications."" and ""Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development."" Keyphrase: ""Vulnerability to generating harmful content""",2
arXIv2023,FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models,Yes.,5,"""LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises.""",2023,2023-10-16T04:17:13Z,"Keyphrase: ""High resource consumption and data scarcity""","""LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises."" Keyphrase: ""High resource consumption and data scarcity""",4
arXIv2023,Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis,Yes.,5,"""while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing",2023,2023-10-15T18:32:27Z,"Keyphrase: ""Limited medical expertise""","""while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing Keyphrase: ""Limited medical expertise""",1
arXIv2023,Assessing the Reliability of Large Language Model Knowledge,Yes.,5,"""LLMs are typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability.""",2023,2023-10-15T12:40:30Z,"Keyphrase: ""Limited evaluation metrics""","""LLMs are typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability."" Keyphrase: ""Limited evaluation metrics""",0
arXIv2023,When can transformers reason with abstract symbols?,Yes.,5,"""transformers fail to generalize as their embedding dimension increases"" and ""require astonishingly large quantities of training data.""",2023,2023-10-15T06:45:38Z,"Keyphrase: ""Limited generalization with increased embedding dimension""","""transformers fail to generalize as their embedding dimension increases"" and ""require astonishingly large quantities of training data."" Keyphrase: ""Limited generalization with increased embedding dimension""",5
arXIv2023,DPZero: Private Fine-Tuning of Language Models without Backpropagation,Yes.,5,"""The widespread practice of fine-tuning large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continues to grow, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize training data, it is important to protect",2023,2023-10-14T18:42:56Z,"Keyphrase: ""Memory and privacy challenges""","""The widespread practice of fine-tuning large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continues to grow, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize training data, it is important to protect Keyphrase: ""Memory and privacy challenges""",4
arXIv2023,Autonomous Tree-search Ability of Large Language Models,Yes.,5,"""Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making."" and ""there are several fundamental limitations of these approaches.""",2023,2023-10-14T14:14:38Z,"Keyphrase: ""Limited strategic foresight""","""Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making."" and ""there are several fundamental limitations of these approaches."" Keyphrase: ""Limited strategic foresight""",1
arXIv2023,User Inference Attacks on Large Language Models,Yes.,5,"""We find that LLMs are susceptible to user inference across a variety of fine-tuning datasets, at times with near perfect attack success rates.""",2023,2023-10-13T17:24:52Z,"Keyphrase: ""Susceptible to user inference""","""We find that LLMs are susceptible to user inference across a variety of fine-tuning datasets, at times with near perfect attack success rates."" Keyphrase: ""Susceptible to user inference""",2
arXIv2023,"""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases in LLM-Generated Reference Letters",Yes.,5,"""In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions",2023,2023-10-13T16:12:57Z,"Keyphrase: ""Gender bias in generated content""","""In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions Keyphrase: ""Gender bias in generated content""",3
arXIv2023,KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment.""",2023,2023-10-13T12:12:34Z,"Keyphrase: ""Misinformation generation""","""Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment."" Keyphrase: ""Misinformation generation""",0
arXIv2023,"""Im not Racist but..."": Discovering Bias in the Internal Knowledge of Large Language Models",Yes.,4,"""these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications.""",2023,2023-10-13T00:03:37Z,"Keyphrase: ""Inherent societal bias""","""these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications."" Keyphrase: ""Inherent societal bias""",3
arXIv2023,MemGPT: Towards LLMs as Operating Systems,Yes.,5,"""Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis.""",2023,2023-10-12T17:51:32Z,"Keyphrase: ""Context window limitation""","""Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis."" Keyphrase: ""Context window limitation""",4
arXIv2023,Jailbreaking Black Box Large Language Models in Twenty Queries,Yes.,4,"""However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse.""",2023,2023-10-12T15:38:28Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arXIv2023,Impact of Co-occurrence on Factual Knowledge of Large Language Models,Yes.,5,"""Large language models (LLMs) often make factually incorrect responses despite their success in various applications."" and ""We show that co-occurrence bias remains despite scaling up model sizes or finetuning.""",2023,2023-10-12T12:01:32Z,"Keyphrase: ""Persistent cooccurrence bias""","""Large language models (LLMs) often make factually incorrect responses despite their success in various applications."" and ""We show that co-occurrence bias remains despite scaling up model sizes or finetuning."" Keyphrase: ""Persistent cooccurrence bias""",0
arXIv2023,QASiNa: Religious Domain Question Answering using Sirah Nabawiyah,Yes.,5,"""The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam."" and ""The experiment indicate that Chat GPT tends to give excessive interpretations as evidenced by its higher Substring Match scores compared to EM and F1-Score, even after providing instruction and",2023,2023-10-12T07:52:19Z,"Keyphrase: ""Excessive interpretation""","""The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam."" and ""The experiment indicate that Chat GPT tends to give excessive interpretations as evidenced by its higher Substring Match scores compared to EM and F1-Score, even after providing instruction and Keyphrase: ""Excessive interpretation""",1
arXIv2023,GameGPT: Multi-agent Collaborative Framework for Game Development,Yes.,4,"""While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern",2023,2023-10-12T06:31:43Z,"Keyphrase: ""Hallucination issues""","""While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern Keyphrase: ""Hallucination issues""",0
arXIv2023,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",Yes.,4,"""it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values,"" and ""we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.""",2023,2023-10-11T16:18:13Z,"Keyphrase: ""Challenges in feedback incorporation""","""it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values,"" and ""we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges."" Keyphrase: ""Challenges in feedback incorporation""",6
arXIv2023,"Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",Yes.,5,"""We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts,"" and ""highlighting the potential consequences and challenges posed by factual errors in LLM outputs.""",2023,2023-10-11T14:18:03Z,"Keyphrase: ""Factual inconsistency""","""We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts,"" and ""highlighting the potential consequences and challenges posed by factual errors in LLM outputs."" Keyphrase: ""Factual inconsistency""",0
arXIv2023,How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances,Yes.,4,"""Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era.""",2023,2023-10-11T09:46:32Z,"Keyphrase: ""Difficulty in maintaining up-to-date information""","""Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era."" Keyphrase: ""Difficulty in maintaining up-to-date information""",0
arXIv2023,Beyond Memorization: Violating Privacy Via Inference with Large Language Models,Yes.,5,"""we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text,"" and ""common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference.""",2023,2023-10-11T08:32:46Z,"Keyphrase: ""Inference of personal attributes""","""we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text,"" and ""common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference."" Keyphrase: ""Inference of personal attributes""",8
arXIv2023,Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation,Yes.,5,"""Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs.""",2023,2023-10-10T20:15:54Z,"Keyphrase: ""Alignment failures""","""Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs."" Keyphrase: ""Alignment failures""",2
arXIv2023,LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression,Yes.,5,"""In long context scenarios, large language models (LLMs) face three main challenges",2023,2023-10-10T17:59:58Z,"Keyphrase: ""Challenges with long context""","""In long context scenarios, large language models (LLMs) face three main challenges Keyphrase: ""Challenges with long context""",4
arXIv2023,Teaching Language Models to Hallucinate Less with Synthetic Tasks,Yes.,5,"""Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context.""",2023,2023-10-10T17:57:00Z,"Keyphrase: ""Frequent hallucinations in summarization tasks""","""Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context."" Keyphrase: ""Frequent hallucinations in summarization tasks""",0
arXIv2023,The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets,Yes.,4,"""Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods."" and ""this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues.""",2023,2023-10-10T17:54:39Z,"Keyphrase: ""Tendency to output falsehoods""","""Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods."" and ""this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues."" Keyphrase: ""Tendency to output falsehoods""",0
arXIv2023,Exploring Memorization in Fine-tuned Language Models,Yes.,5,"""Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns.""",2023,2023-10-10T15:41:26Z,"Keyphrase: ""Memorization and privacy concerns""","""Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns."" Keyphrase: ""Memorization and privacy concerns""",8
arXIv2023,A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection,Yes.,5,"""LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks.""",2023,2023-10-10T10:14:59Z,"Keyphrase: ""Generation of hallucinations""","""LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks."" Keyphrase: ""Generation of hallucinations""",0
arXIv2023,Multilingual Jailbreak Challenges in Large Language Models,Yes.,5,"""While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the `jailbreak' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior."" and ""The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically,",2023,2023-10-10T09:44:06Z,"Keyphrase: ""Safety concerns and unintended behaviors""","""While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the `jailbreak' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior."" and ""The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, Keyphrase: ""Safety concerns and unintended behaviors""",2
arXIv2023,Towards Mitigating Hallucination in Large Language Models via Self-Reflection,Yes.,5,"""However, the practical deployment still faces challenges, notably the issue of 'hallucination', where models generate plausible-sounding but unfaithful or nonsensical information.""",2023,2023-10-10T03:05:44Z,"Keyphrase: ""Hallucination of nonsensical information""","""However, the practical deployment still faces challenges, notably the issue of 'hallucination', where models generate plausible-sounding but unfaithful or nonsensical information."" Keyphrase: ""Hallucination of nonsensical information""",0
arXIv2023,Compressing Context to Enhance Inference Efficiency of Large Language Models,Yes.,5,"""However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length.""",2023,2023-10-09T23:03:24Z,"Keyphrase: ""Challenges with long documents and extended conversations""","""However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length."" Keyphrase: ""Challenges with long documents and extended conversations""",4
arXIv2023,SALMON: Self-Alignment with Instructable Reward Models,Yes.,4,"""a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences.""",2023,2023-10-09T17:56:53Z,"Keyphrase: ""Dependency on high-quality human annotation""","""a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences."" Keyphrase: ""Dependency on high-quality human annotation""",2
arXIv2023,ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models,Yes.,4,"""However, we identify a challenge with VLMs' passive perception, which often misses crucial context information, leading to incorrect or uncertain reasoning by LLMs.""",2023,2023-10-09T17:10:35Z,"Keyphrase: ""Misses crucial context""","""However, we identify a challenge with VLMs' passive perception, which often misses crucial context information, leading to incorrect or uncertain reasoning by LLMs."" Keyphrase: ""Misses crucial context""",5
arXIv2023,HyperAttention: Long-context Attention in Near-Linear Time,Yes.,4,"""We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs)."" and ""Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank.""",2023,2023-10-09T17:05:25Z,"Keyphrase: ""Computational complexity with long context""","""We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs)."" and ""Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank."" Keyphrase: ""Computational complexity with long context""",4
arXIv2023,SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese,Yes.,4,"""they can also produce harmful content that negatively affects societal perceptions"" and ""Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods.""",2023,2023-10-09T16:03:22Z,"Keyphrase: ""Harmful content generation""","""they can also produce harmful content that negatively affects societal perceptions"" and ""Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods."" Keyphrase: ""Harmful content generation""",2
arXIv2023,"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",Yes.,4,"""highlighting both the strengths and limitations"" and ""the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics.""",2023,2023-10-09T13:15:23Z,"Keyphrase: ""Ethical concerns in healthcare deployment""","""highlighting both the strengths and limitations"" and ""the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics."" Keyphrase: ""Ethical concerns in healthcare deployment""",3
arXIv2023,Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization,Yes.,5,"""We also find that MuggleMath is weak in out-of-domain math reasoning generalization to MATH. This is attributed to the differences in query distribution between AugGSM8K and MATH which suggest that augmentation on a single benchmark could not help with overall math reasoning performance.""",2023,2023-10-09T08:18:58Z,"Keyphrase: ""Weak out-of-domain generalization""","""We also find that MuggleMath is weak in out-of-domain math reasoning generalization to MATH. This is attributed to the differences in query distribution between AugGSM8K and MATH which suggest that augmentation on a single benchmark could not help with overall math reasoning performance."" Keyphrase: ""Weak out-of-domain generalization""",1
arXIv2023,Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models,Yes.,5,"""Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects."" and ""no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy",2023,2023-10-09T01:52:27Z,"Keyphrase: ""Object hallucination""","""Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects."" and ""no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy Keyphrase: ""Object hallucination""",0
arXIv2023,Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems,Yes.,5,"""Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations,"" and ""our study uncovers significant persona biases in dialogue systems.""",2023,2023-10-08T21:03:18Z,"Keyphrase: ""Persona bias""","""Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations,"" and ""our study uncovers significant persona biases in dialogue systems."" Keyphrase: ""Persona bias""",3
arXIv2023,Measuring reasoning capabilities of ChatGPT,Yes.,5,"""I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks."" and ""A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models.""",2023,2023-10-08T20:18:50Z,"Keyphrase: ""Logical reasoning faults""","""I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks."" and ""A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models."" Keyphrase: ""Logical reasoning faults""",1
arXIv2023,Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback,Yes.,5,"""we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs.""",2023,2023-10-08T15:14:39Z,"Keyphrase: ""Length bias leading to misleading outputs""","""we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs."" Keyphrase: ""Length bias leading to misleading outputs""",0
arXIv2023,Factuality Challenges in the Era of Large Language Models,Yes.,5,"""These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as 'hallucinations.'"" and ""Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding",2023,2023-10-08T14:55:02Z,"Keyphrase: ""Propensity for hallucination""","""These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as 'hallucinations.'"" and ""Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding Keyphrase: ""Propensity for hallucination""",0
arXIv2023,Do Large Language Models Know about Facts?,Yes.,5,"""Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time"" and ""Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations.""",2023,2023-10-08T14:26:55Z,"Keyphrase: ""Lack of factual knowledge""","""Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time"" and ""Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations."" Keyphrase: ""Lack of factual knowledge""",0
arXIv2023,Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT,Yes.,4,"""this introduces issues of bias on protected attributes like gender, race and maternity status"" and ""We use contrastive input decoding on open-source LLMs to uncover potential sources of bias.""",2023,2023-10-08T12:08:48Z,"Keyphrase: ""Bias in protected attributes""","""this introduces issues of bias on protected attributes like gender, race and maternity status"" and ""We use contrastive input decoding on open-source LLMs to uncover potential sources of bias."" Keyphrase: ""Bias in protected attributes""",3
arXIv2023,Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,Yes.,5,"""However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge.""",2023,2023-10-08T06:18:14Z,"Keyphrase: ""Limited context understanding""","""However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge."" Keyphrase: ""Limited context understanding""",4
arXIv2023,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,Yes.,5,"""Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture.""",2023,2023-10-07T21:49:38Z,"Keyphrase: ""Limited cultural and language knowledge""","""Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture."" Keyphrase: ""Limited cultural and language knowledge""",7
arXIv2023,Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM,Yes.,4,"""Large Language Models (LLMs) pose significant hardware challenges related to memory requirements and computational ability.""",2023,2023-10-07T14:50:28Z,"Keyphrase: ""Hardware challenges""","""Large Language Models (LLMs) pose significant hardware challenges related to memory requirements and computational ability."" Keyphrase: ""Hardware challenges""",4
arXIv2023,Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning,Yes.,5,"""However, these models often face the challenge of 'hallucination,' which undermines their reliability."" and ""Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations.""",2023,2023-10-07T12:06:53Z,"Keyphrase: ""Hallucination challenge""","""However, these models often face the challenge of 'hallucination,' which undermines their reliability."" and ""Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations."" Keyphrase: ""Hallucination challenge""",0
arXIv2023,The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning,Yes.,5,"""Reducing the model size by more than 30\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training.""",2023,2023-10-07T03:36:39Z,"Keyphrase: ""Decreased recall ability""","""Reducing the model size by more than 30\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training."" Keyphrase: ""Decreased recall ability""",5
arXIv2023,Amortizing intractable inference in large language models,Yes.,5,"""This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions.""",2023,2023-10-06T16:36:08Z,"Keyphrase: ""Intractable sampling tasks""","""This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions."" Keyphrase: ""Intractable sampling tasks""",4
arXIv2023,Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface,Yes.,5,"""Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation."" and ""complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly.""",2023,2023-10-06T12:44:04Z,"Keyphrase: ""Incomplete knowledge retrieval""","""Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation."" and ""complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly."" Keyphrase: ""Incomplete knowledge retrieval""",0
arXIv2023,Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models,Yes.,5,"""The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis.""",2023,2023-10-06T05:40:23Z,"Keyphrase: ""Contextual deficiency""","""The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis."" Keyphrase: ""Contextual deficiency""",4
arXIv2023,From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self,Yes.,4,"""However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology.""",2023,2023-10-06T02:19:10Z,"Keyphrase: ""Inauthentic and excessive responses""","""However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology."" Keyphrase: ""Inauthentic and excessive responses""",0
arXIv2023,Quantized Transformer Language Model Implementations on Edge Devices,Yes.,5,"""One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency.""",2023,2023-10-06T01:59:19Z,"Keyphrase: ""Resource-intensive deployment""","""One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency."" Keyphrase: ""Resource-intensive deployment""",4
arXIv2023,Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations,Yes.,5,"""However, LLMs are prone to generate hallucinations that are not supported by the provided sources.""",2023,2023-10-06T00:10:46Z,"Keyphrase: ""Prone to hallucination""","""However, LLMs are prone to generate hallucinations that are not supported by the provided sources."" Keyphrase: ""Prone to hallucination""",0
arXIv2023,LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models,Yes.,4,"""However, results on Coordination QA show a large room for improvement in the Theory of Mind reasoning and joint planning abilities of LLMs.""",2023,2023-10-05T21:18:15Z,"Keyphrase: ""Limited theory of mind reasoning""","""However, results on Coordination QA show a large room for improvement in the Theory of Mind reasoning and joint planning abilities of LLMs."" Keyphrase: ""Limited theory of mind reasoning""",1
arXIv2023,"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",Yes.,5,"""We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users."" and ""Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples."" and ""simply fine-t",2023,2023-10-05T17:12:17Z,"Keyphrase: ""Safety alignment restrictions""","""We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users."" and ""Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples."" and ""simply fine-t Keyphrase: ""Safety alignment restrictions""",2
arXIv2023,Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures,Yes.,4,"""However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations.""",2023,2023-10-05T16:37:29Z,"Keyphrase: ""Complex interconnected task demands""","""However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations."" Keyphrase: ""Complex interconnected task demands""",1
arXIv2023,Redefining Digital Health Interfaces with Large Language Models,Yes.,5,"""Directly applying LLMs in clinical settings is not straightforward, however, with LLMs susceptible to providing inconsistent or nonsensical answers."" and ""addressing current issues with using LLMs in clinical settings such as hallucinations.""",2023,2023-10-05T14:18:40Z,"Keyphrase: ""Inconsistent and nonsensical answers""","""Directly applying LLMs in clinical settings is not straightforward, however, with LLMs susceptible to providing inconsistent or nonsensical answers."" and ""addressing current issues with using LLMs in clinical settings such as hallucinations."" Keyphrase: ""Inconsistent and nonsensical answers""",0
arXIv2023,Evaluating Hallucinations in Chinese Large Language Models,Yes.,5,"""We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models.""",2023,2023-10-05T07:57:09Z,"Keyphrase: ""Hallucination prioritization""","""We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models."" Keyphrase: ""Hallucination prioritization""",0
arXIv2023,Benchmarking Large Language Models As AI Research Agents,Yes.,5,"""Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination.""",2023,2023-10-05T04:06:12Z,"Keyphrase: ""Challenges in long-term planning""","""Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination."" Keyphrase: ""Challenges in long-term planning""",1
arXIv2023,A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions,Yes.,5,"""However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete,",2023,2023-10-05T03:45:54Z,"Keyphrase: ""Mismatched user intention alignment and incomplete domain knowledge""","""However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete, Keyphrase: ""Mismatched user intention alignment and incomplete domain knowledge""",6
arXIv2023,Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,Yes.,5,"""they still face limitations in scenarios that demand long-term planning and spatial reasoning"" and ""it still fails to perform long-term temporal reasoning"" and ""fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles.""",2023,2023-10-05T01:42:16Z,"Keyphrase: ""Limited long-term reasoning""","""they still face limitations in scenarios that demand long-term planning and spatial reasoning"" and ""it still fails to perform long-term temporal reasoning"" and ""fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles."" Keyphrase: ""Limited long-term reasoning""",1
arXIv2023,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,Yes.,5,"""Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement",2023,2023-10-05T00:04:12Z,"Keyphrase: ""Room for improvement in human evaluation""","""Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement Keyphrase: ""Room for improvement in human evaluation""",0
arXIv2023,Misusing Tools in Large Language Models With Visual Adversarial Examples,Yes.,5,"""These new capabilities bring new benefits and also new security risks."" and ""In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage.""",2023,2023-10-04T22:10:01Z,"""Vulnerability to visual adversarial attacks""","""These new capabilities bring new benefits and also new security risks."" and ""In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage."" ""Vulnerability to visual adversarial attacks""",2
arXIv2023,From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference,Yes.,4,"""However, these models carry significant computational challenges, especially the compute and energy costs required for inference.""",2023,2023-10-04T17:41:59Z,"Keyphrase: ""High computational cost""","""However, these models carry significant computational challenges, especially the compute and energy costs required for inference."" Keyphrase: ""High computational cost""",4
arXIv2023,Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models,Yes.,5,"""To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour,",2023,2023-10-04T16:39:31Z,"Keyphrase: ""Vulnerability to malicious attacks""","""To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, Keyphrase: ""Vulnerability to malicious attacks""",2
arXIv2023,How FaR Are Large Language Models From Agents with Theory-of-Mind?,Yes.,5,"""LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action."" and ""Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D",2023,2023-10-04T06:47:58Z,"Keyphrase: ""Challenges in implicit inference""","""LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action."" and ""Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D Keyphrase: ""Challenges in implicit inference""",1
arXIv2023,Low-Resource Languages Jailbreak GPT-4,Yes.,5,"""Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data,"" and ""this deficiency now poses a risk to all LLMs users.""",2023,2023-10-03T21:30:56Z,"Keyphrase: ""Crosslingual vulnerability and safety deficiencies""","""Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data,"" and ""this deficiency now poses a risk to all LLMs users."" Keyphrase: ""Crosslingual vulnerability and safety deficiencies""",2
arXIv2023,Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions,Yes.,5,"""Our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers.""",2023,2023-10-03T21:19:50Z,"Keyphrase: ""Struggles with specific knowledge and misconceptions""","""Our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers."" Keyphrase: ""Struggles with specific knowledge and misconceptions""",1
arXIv2023,Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions,Yes.,5,"""However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content)."" and ""Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate",2023,2023-10-03T20:54:29Z,"Keyphrase: ""Inaccurate responses and toxic content""","""However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content)."" and ""Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate Keyphrase: ""Inaccurate responses and toxic content""",3
arXIv2023,AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models,Yes.,4,"""However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs.""",2023,2023-10-03T19:44:37Z,"Keyphrase: ""Susceptibility to jailbreak attacks""","""However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs."" Keyphrase: ""Susceptibility to jailbreak attacks""",2
arXIv2023,Investigating Large Language Models' Perception of Emotion Using Appraisal Theory,Yes.,4,"""The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data. The magnitude of their responses is also quite different from humans in several variables. We also found that GPTs can be quite sensitive to instruction and how questions are asked.""",2023,2023-10-03T16:34:47Z,"Keyphrase: ""Limited emotional intelligence""","""The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data. The magnitude of their responses is also quite different from humans in several variables. We also found that GPTs can be quite sensitive to instruction and how questions are asked."" Keyphrase: ""Limited emotional intelligence""",0
arXIv2023,Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems,Yes.,5,"""Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2).""",2023,2023-10-03T12:03:06Z,"Keyphrase: ""Backward reasoning accuracy drop""","""Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2)."" Keyphrase: ""Backward reasoning accuracy drop""",1
arXIv2023,HallE-Control: Controlling Object Hallucination in Large Multimodal Models,Yes.,5,"""Interestingly, while LMMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations."" and ""Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can",2023,2023-10-03T04:01:27Z,"Keyphrase: ""Object existence hallucination""","""Interestingly, while LMMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations."" and ""Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can Keyphrase: ""Object existence hallucination""",0
arXIv2023,Can GPT-4 Replicate Empirical Software Engineering Research?,Yes.,4,"""We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering",2023,2023-10-03T01:27:23Z,"Keyphrase: ""Implementation-level errors""","""We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering Keyphrase: ""Implementation-level errors""",7
arXIv2023,PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels,Yes.,5,"""The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models.""",2023,2023-10-02T21:39:04Z,"Keyphrase: ""Quadratic memory complexity""","""The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models."" Keyphrase: ""Quadratic memory complexity""",4
arXIv2023,On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?,Yes.,5,"""many existing studies have shown that they could be misused to generate undesired content"" and ""we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs.""",2023,2023-10-02T19:22:01Z,"Keyphrase: ""Potential for generating undesired content""","""many existing studies have shown that they could be misused to generate undesired content"" and ""we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs."" Keyphrase: ""Potential for generating undesired content""",2
arXIv2023,"LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",Yes.,5,"""However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception.""",2023,2023-10-02T17:01:56Z,"Keyphrase: ""Hallucination and fabrication""","""However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception."" Keyphrase: ""Hallucination and fabrication""",0
arXIv2023,Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation,Yes.,5,"""This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes."" and ""Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.""",2023,2023-10-02T16:27:36Z,"Keyphrase: ""Susceptibility to malicious manipulation""","""This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes."" and ""Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research."" Keyphrase: ""Susceptibility to malicious manipulation""",2
arXIv2023,Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models,Yes.,5,"""Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more.""",2023,2023-10-02T15:43:53Z,"Keyphrase: ""Limited geometric reasoning""","""Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more."" Keyphrase: ""Limited geometric reasoning""",1
arXIv2023,Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models,Yes.,4,"""Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability."" and ""A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp.""",2023,2023-10-02T10:35:23Z,"Keyphrase: ""Lack of explainability in complex reasoning tasks""","""Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability."" and ""A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp."" Keyphrase: ""Lack of explainability in complex reasoning tasks""",1
arXIv2023,Resolving Knowledge Conflicts in Large Language Models,Yes.,5,"""Extensive experiments with the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information.""",2023,2023-10-02T06:57:45Z,"Keyphrase: ""Struggles with conflicting information""","""Extensive experiments with the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information."" Keyphrase: ""Struggles with conflicting information""",0
arXIv2023,All Languages Matter: On the Multilingual Safety of Large Language Models,Yes.,4,"""Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages.""",2023,2023-10-02T05:23:34Z,"Keyphrase: ""Unsafe responses to non-English queries""","""Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages."" Keyphrase: ""Unsafe responses to non-English queries""",2
arXIv2023,FELM: Benchmarking Factuality Evaluation of Large Language Models,Yes.,5,"""Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.""",2023,2023-10-01T17:37:31Z,"Keyphrase: ""Limited factual accuracy""","""Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors."" Keyphrase: ""Limited factual accuracy""",0
arXIv2023,Measuring Value Understanding in Language Models through Discriminator-Critique Gap,Yes.,4,"""Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values"" and ""This may further suggest that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks.""",2023,2023-09-30T13:47:55Z,"Keyphrase: ""Lack of true understanding""","""Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values"" and ""This may further suggest that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks."" Keyphrase: ""Lack of true understanding""",1
arXIv2023,Understanding In-Context Learning from Repetitions,Yes.,5,"""our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures"" and ""providing a fresh perspective on this exciting capability.""",2023,2023-09-30T08:13:49Z,"Keyphrase: ""Limited understanding of internal workings""","""our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures"" and ""providing a fresh perspective on this exciting capability."" Keyphrase: ""Limited understanding of internal workings""",1
arXIv2023,AutoHall: Automated Hallucination Dataset Generation for Large Language Models,Yes.,5,"""the detection of non-factual or hallucinatory content generated by LLMs remains scarce"" and ""variations in hallucination proportions and types among different models.""",2023,2023-09-30T05:20:02Z,"Keyphrase: ""Variation in hallucinatory content""","""the detection of non-factual or hallucinatory content generated by LLMs remains scarce"" and ""variations in hallucination proportions and types among different models."" Keyphrase: ""Variation in hallucinatory content""",0
arXIv2023,"Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs ""Difficult"" Downstream Tasks in LLMs",Yes.,5,"""we reveal that these seemingly inconsequential weights can result in irreparable loss of knowledge and performance degradation in difficult tasks, even when downstream continual training is allowed.""",2023,2023-09-29T22:55:06Z,"Keyphrase: ""Loss of knowledge and performance degradation""","""we reveal that these seemingly inconsequential weights can result in irreparable loss of knowledge and performance degradation in difficult tasks, even when downstream continual training is allowed."" Keyphrase: ""Loss of knowledge and performance degradation""",5
arXIv2023,Efficient Streaming Language Models with Attention Sinks,Yes.,5,"""Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.""",2023,2023-09-29T17:59:56Z,"Keyphrase: ""Memory consumption and text length limitation""","""Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length."" Keyphrase: ""Memory consumption and text length limitation""",4
arXIv2023,Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks,Yes.,5,"""Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text."" and ""Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and",2023,2023-09-29T17:12:43Z,"Keyphrase: ""Privacy risks and harmful outputs""","""Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text."" and ""Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and Keyphrase: ""Privacy risks and harmful outputs""",8
arXIv2023,LoRA ensembles for large language model fine-tuning,Yes.,5,"""Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples.""",2023,2023-09-29T16:38:38Z,"Keyphrase: ""Poor uncertainty quantification""","""Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples."" Keyphrase: ""Poor uncertainty quantification""",5
arXIv2023,"Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",Yes.,4,"""Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment.""",2023,2023-09-29T16:36:39Z,"Keyphrase: ""Challenges in real-world application""","""Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment."" Keyphrase: ""Challenges in real-world application""",1
arXIv2023,Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis,Yes.,5,"""This bias can take two forms",2023,2023-09-29T15:30:32Z,"Keyphrase: ""Biases in two forms""","""This bias can take two forms Keyphrase: ""Biases in two forms""",3
arXIv2023,Using Large Language Models for Qualitative Analysis can Introduce Serious Bias,Yes.,4,"""We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences."" and ""Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations.""",2023,2023-09-29T11:19:15Z,"Keyphrase: ""Risk of bias in annotations""","""We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences."" and ""Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations."" Keyphrase: ""Risk of bias in annotations""",3
arXIv2023,Benchmarking Cognitive Biases in Large Language Models as Evaluators,Yes.,5,"""We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators.""",2023,2023-09-29T06:53:10Z,"Keyphrase: ""Biased text quality evaluation""","""We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators."" Keyphrase: ""Biased text quality evaluation""",3
arXIv2023,Medical Foundation Models are Susceptible to Targeted Misinformation Attacks,Yes.,5,"""Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. This peculiar susceptibility raises serious security and trustworthiness concerns for",2023,2023-09-29T06:44:36Z,"Keyphrase: ""Vulnerability to deliberate manipulation""","""Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. This peculiar susceptibility raises serious security and trustworthiness concerns for Keyphrase: ""Vulnerability to deliberate manipulation""",2
arXIv2023,Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving,Yes.,5,"""Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan,",2023,2023-09-28T13:40:50Z,"Keyphrase: ""Contextually inappropriate hallucination""","""Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, Keyphrase: ""Contextually inappropriate hallucination""",0
arXIv2023,Human Feedback is not Gold Standard,Yes.,5,"""We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality.""",2023,2023-09-28T11:18:20Z,"Keyphrase: ""Limited coverage of important aspects""","""We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality."" Keyphrase: ""Limited coverage of important aspects""",1
arXIv2023,Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI,Yes.,5,"""There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations.""",2023,2023-09-26T20:52:46Z,"Keyphrase: ""Medical domain hesitation""","""There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations."" Keyphrase: ""Medical domain hesitation""",0
arXIv2023,Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models,Yes.,5,"""We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text.""",2023,2023-09-26T17:48:55Z,"Keyphrase: ""Factually incorrect text generation""","""We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text."" Keyphrase: ""Factually incorrect text generation""",0
arXIv2023,How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions,Yes.,5,"""Large language models (LLMs) can 'lie', which we define as outputting false statements despite 'knowing' the truth in a demonstrable sense.""",2023,2023-09-26T16:07:54Z,"Keyphrase: ""Outputting false statements""","""Large language models (LLMs) can 'lie', which we define as outputting false statements despite 'knowing' the truth in a demonstrable sense."" Keyphrase: ""Outputting false statements""",0
arXIv2023,Large Language Model Alignment: A Survey,Yes.,4,"""they may yield texts that are imprecise, misleading, or even detrimental"" and ""probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks.""",2023,2023-09-26T15:49:23Z,"Keyphrase: ""Imprecise and misleading text""","""they may yield texts that are imprecise, misleading, or even detrimental"" and ""probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks."" Keyphrase: ""Imprecise and misleading text""",0
arXIv2023,Disinformation Detection: An Evolving Challenge in the Age of LLMs,Yes.,4,"""One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system.""",2023,2023-09-25T22:12:50Z,"Keyphrase: ""Misleading content generation""","""One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system."" Keyphrase: ""Misleading content generation""",0
arXIv2023,Lifelong Robot Learning with Human Assisted Language Planners,Yes.,5,"""However, current LLM-based planners are only able to operate with a fixed set of skills.""",2023,2023-09-25T17:45:55Z,"Keyphrase: ""Limited skill adaptability""","""However, current LLM-based planners are only able to operate with a fixed set of skills."" Keyphrase: ""Limited skill adaptability""",1
arXIv2023,"Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",Yes.,4,"""Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning.""",2023,2023-09-25T17:37:20Z,"Keyphrase: ""Limited generalization without augmentation""","""Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning."" Keyphrase: ""Limited generalization without augmentation""",5
arXIv2023,Identifying the Risks of LM Agents with an LM-Emulated Sandbox,Yes.,5,"""Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses."" and ""we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures",2023,2023-09-25T17:08:02Z,"Keyphrase: ""Risk of leaking private data""","""Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses."" and ""we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures Keyphrase: ""Risk of leaking private data""",2
arXIv2023,LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models,Yes.,4,"""Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations.""",2023,2023-09-25T14:50:04Z,"Keyphrase: ""Limitations in predicting carbon footprint""","""Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations."" Keyphrase: ""Limitations in predicting carbon footprint""",5
arXIv2023,The Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks,Yes.,5,"""The widespread integration of autoregressive-large language models (AR-LLMs), such as ChatGPT, across established applications, like search engines, has introduced critical vulnerabilities with uniquely scalable characteristics. In this commentary, we analyse these vulnerabilities, their dependence on natural language as a vector of",2023,2023-09-25T10:48:46Z,"Keyphrase: ""Critical vulnerability in scalability""","""The widespread integration of autoregressive-large language models (AR-LLMs), such as ChatGPT, across established applications, like search engines, has introduced critical vulnerabilities with uniquely scalable characteristics. In this commentary, we analyse these vulnerabilities, their dependence on natural language as a vector of Keyphrase: ""Critical vulnerability in scalability""",2
arXIv2023,Evaluating Cognitive Maps and Planning in Large Language Models with CogEval,Yes.,5,"""systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops.""",2023,2023-09-25T01:20:13Z,"Keyphrase: ""Failure in planning tasks""","""systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops."" Keyphrase: ""Failure in planning tasks""",0
arXIv2023,Can LLM-Generated Misinformation Be Detected?,Yes.,5,"""However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust."" and ""we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can",2023,2023-09-25T00:45:07Z,"Keyphrase: ""Misinformation generation""","""However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust."" and ""we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can Keyphrase: ""Misinformation generation""",2
arXIv2023,Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve,Yes.,5,"""The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations,"" and ""In many cases, the experiments reveal surprising failure modes,"" and ""AI practitioners should be careful about using LLMs in low-probability situations.""",2023,2023-09-24T13:35:28Z,"Keyphrase: ""Surprising failure modes""","""The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations,"" and ""In many cases, the experiments reveal surprising failure modes,"" and ""AI practitioners should be careful about using LLMs in low-probability situations."" Keyphrase: ""Surprising failure modes""",0
arXIv2023,Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic,Yes.,5,"""However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning,"" and ""These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles.""",2023,2023-09-23T11:21:12Z,"Keyphrase: ""Limited reasoning ability""","""However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning,"" and ""These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles."" Keyphrase: ""Limited reasoning ability""",1
arXIv2023,BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP,Yes.,4,"""Our experimental results demonstrate that while in some Bengali NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2-13b-chat being significantly bad) in comparison to the current SOTA results",2023,2023-09-22T20:29:34Z,"Keyphrase: ""Poor zero-shot performance""","""Our experimental results demonstrate that while in some Bengali NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2-13b-chat being significantly bad) in comparison to the current SOTA results Keyphrase: ""Poor zero-shot performance""",6
arXIv2023,Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models,Yes.,4,"""Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs).""",2023,2023-09-21T17:54:58Z,"Keyphrase: ""Inconsistent output quality""","""Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs)."" Keyphrase: ""Inconsistent output quality""",6
arXIv2023,"The Reversal Curse: LLMs trained on ""A is B"" fail to learn ""B is A""",Yes.,5,"""We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form 'A is B', it will not automatically generalize to the reverse direction 'B is A'.""",2023,2023-09-21T17:52:19Z,"Keyphrase: ""Limited bidirectional generalization""","""We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form 'A is B', it will not automatically generalize to the reverse direction 'B is A'."" Keyphrase: ""Limited bidirectional generalization""",5
arXIv2023,"Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",Yes.,5,"""First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a",2023,2023-09-21T16:47:30Z,"Keyphrase: ""Underperformance in detecting fake news""","""First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a Keyphrase: ""Underperformance in detecting fake news""",6
arXIv2023,Code Soliloquies for Accurate Calculations in Large Language Models,Yes.,5,"""While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects.""",2023,2023-09-21T15:16:58Z,"Keyphrase: ""Limited mathematical reasoning""","""While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects."" Keyphrase: ""Limited mathematical reasoning""",1
arXIv2023,A knowledge representation approach for construction contract knowledge modeling,Yes.,4,"""However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise.""",2023,2023-09-21T14:53:36Z,"Keyphrase: ""Lack of domain expertise""","""However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise."" Keyphrase: ""Lack of domain expertise""",0
arXIv2023,Knowledge Sanitization of Large Language Models,Yes.,4,"""LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns.""",2023,2023-09-21T07:49:55Z,"Keyphrase: ""Risk of revealing sensitive information""","""LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns."" Keyphrase: ""Risk of revealing sensitive information""",8
arXIv2023,Goal-Oriented Prompt Attack and Safety Evaluation for LLMs,Yes.,5,"""LLMs suffer from the risk of generating harmful contents especially while being employed to applications."" and ""there is no publicly available dataset with high successful attacking rate to evaluate the abilities of defending prompt attack.""",2023,2023-09-21T07:07:49Z,"Keyphrase: ""Risk of generating harmful content""","""LLMs suffer from the risk of generating harmful contents especially while being employed to applications."" and ""there is no publicly available dataset with high successful attacking rate to evaluate the abilities of defending prompt attack."" Keyphrase: ""Risk of generating harmful content""",2
arXIv2023,LLM Guided Inductive Inference for Solving Compositional Problems,Yes.,4,"""their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world."" and ""Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks",2023,2023-09-20T23:44:16Z,"Keyphrase: ""Limited deep reasoning ability""","""their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world."" and ""Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks Keyphrase: ""Limited deep reasoning ability""",1
arXIv2023,"""It's a Fair Game"", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",Yes.,4,"""users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks"" and ""the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs.""",2023,2023-09-20T21:34:36Z,"Keyphrase: ""Limited user awareness and comprehension""","""users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks"" and ""the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs."" Keyphrase: ""Limited user awareness and comprehension""",8
arXIv2023,Chain-of-Verification Reduces Hallucination in Large Language Models,Yes.,5,"""Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models.""",2023,2023-09-20T17:50:55Z,"Keyphrase: ""Factual hallucination""","""Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models."" Keyphrase: ""Factual hallucination""",0
arXIv2023,Are Large Language Models Really Robust to Word-Level Perturbations?,Yes.,5,"""our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage.""",2023,2023-09-20T09:23:46Z,"Keyphrase: ""Vulnerability to word-level perturbations""","""our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage."" Keyphrase: ""Vulnerability to word-level perturbations""",2
arXIv2023,"Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness",Yes.,5,"""As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination.""",2023,2023-09-20T05:04:16Z,"Keyphrase: ""Hallucination""","""As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination."" Keyphrase: ""Hallucination""",0
arXIv2023,In-Context Learning for Text Classification with Many Labels,Yes.,5,"""In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt.""",2023,2023-09-19T22:41:44Z,"Keyphrase: ""Limited context window""","""In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt."" Keyphrase: ""Limited context window""",5
arXIv2023,LMDX: Language Model-based Document Information Extraction and Localization,Yes.,4,"""The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated.""",2023,2023-09-19T22:32:56Z,"Keyphrase: ""Absence of layout encoding""","""The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated."" Keyphrase: ""Absence of layout encoding""",0
arXIv2023,Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome,Yes.,5,"""whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm.""",2023,2023-09-19T16:41:19Z,"Keyphrase: ""Limited sarcasm comprehension""","""whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm."" Keyphrase: ""Limited sarcasm comprehension""",1
arXIv2023,PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training,Yes.,5,"""Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs.""",2023,2023-09-19T08:03:38Z,"Keyphrase: ""Limited input context""","""Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs."" Keyphrase: ""Limited input context""",4
arXIv2023,Investigating the Catastrophic Forgetting in Multimodal Large Language Models,Yes.,5,"""However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).""",2023,2023-09-19T04:51:13Z,"Keyphrase: ""Catastrophic forgetting""","""However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM)."" Keyphrase: ""Catastrophic forgetting""",5
arXIv2023,LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins,Yes.,4,"""Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations."" and ""We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms.""",2023,2023-09-19T02:20:10Z,"Keyphrase: ""Imprecise natural language interpretation""","""Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations."" and ""We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms."" Keyphrase: ""Imprecise natural language interpretation""",8
arXIv2023,Bias of AI-Generated Content: An Examination of News Produced by Large Language Models,Yes.,5,"""To harness this transformation, we need to understand the limitations of LLMs."" and ""Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race.""",2023,2023-09-18T14:47:24Z,"Keyphrase: ""Gender and racial bias""","""To harness this transformation, we need to understand the limitations of LLMs."" and ""Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race."" Keyphrase: ""Gender and racial bias""",3
arXIv2023,"CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",Yes.,4,"""The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community.""",2023,2023-09-17T23:49:10Z,"Keyphrase: ""Transparency of training data""","""The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community."" Keyphrase: ""Transparency of training data""",8
arXIv2023,Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?,Yes.,5,"""Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging."" and ""In-depth error analysis and creating an ability map across six dimensions -- coverage, formatting, reasoning, comprehension, pragmatics, and hallucination -- highlight areas for future enhancements and suggest forthcoming research trajectories.""",2023,2023-09-16T11:31:58Z,"Keyphrase: ""Challenges in handling structured tabular data""","""Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging."" and ""In-depth error analysis and creating an ability map across six dimensions -- coverage, formatting, reasoning, comprehension, pragmatics, and hallucination -- highlight areas for future enhancements and suggest forthcoming research trajectories."" Keyphrase: ""Challenges in handling structured tabular data""",1
arXIv2023,"PDFTriage: Question Answering over Long, Structured Documents",Yes.,5,"""Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM.""",2023,2023-09-16T04:29:05Z,"Keyphrase: ""Limited context understanding""","""Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM."" Keyphrase: ""Limited context understanding""",4
arXIv2023,Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings,Yes.,5,"""Our experiments reveal that",2023,2023-09-15T17:45:28Z,Keyphrase: Lack of specificity,"""Our experiments reveal that Keyphrase: Lack of specificity",1
arXIv2023,Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West,Yes.,4,"""Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms."" and ""We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context.""",2023,2023-09-15T17:38:41Z,"Keyphrase: ""Societal bias and representational harm""","""Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms."" and ""We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context."" Keyphrase: ""Societal bias and representational harm""",3
arXIv2023,Investigating Answerability of LLMs for Long-Form Question Answering,Yes.,5,"""it becomes increasingly crucial to understand their capabilities, limitations, and differences"" and ""generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts"" and ""open-source LLMs exhibit decreased reliance on context for generated questions",2023,2023-09-15T07:22:56Z,"Keyphrase: ""Limited long-context understanding""","""it becomes increasingly crucial to understand their capabilities, limitations, and differences"" and ""generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts"" and ""open-source LLMs exhibit decreased reliance on context for generated questions Keyphrase: ""Limited long-context understanding""",4
arXIv2023,FedJudge: Federated Legal Large Language Model,Yes.,4,"""However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods.""",2023,2023-09-15T05:45:44Z,"Keyphrase: ""Overhead in fine-tuning""","""However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods."" Keyphrase: ""Overhead in fine-tuning""",4
arXIv2023,Self-Assessment Tests are Unreliable Measures of LLM Personality,Yes.,5,"""self-assessment personality tests created for humans are unreliable measures of personality in LLMs.""",2023,2023-09-15T05:19:39Z,"Keyphrase: ""Unreliable personality assessment""","""self-assessment personality tests created for humans are unreliable measures of personality in LLMs."" Keyphrase: ""Unreliable personality assessment""",0
arXIv2023,MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,Yes.,4,"""while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks."" and ""we observe that MMICL successfully allevi",2023,2023-09-14T17:59:17Z,"Keyphrase: ""Struggles with multimodal understanding""","""while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks."" and ""we observe that MMICL successfully allevi Keyphrase: ""Struggles with multimodal understanding""",1
arXIv2023,Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions,Yes.,5,"""we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning,"" and ""several popular instruction-tuned models are highly unsafe,"" and ""our results illustrate trade-offs in training LLMs to be helpful and",2023,2023-09-14T17:23:37Z,"Keyphrase: ""Safety concerns in instruction-tuned models""","""we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning,"" and ""several popular instruction-tuned models are highly unsafe,"" and ""our results illustrate trade-offs in training LLMs to be helpful and Keyphrase: ""Safety concerns in instruction-tuned models""",2
arXIv2023,Tree of Uncertain Thoughts Reasoning for Large Language Models,Yes.,4,"""These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process.""",2023,2023-09-14T13:14:51Z,"Keyphrase: ""Local uncertainty in reasoning""","""These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process."" Keyphrase: ""Local uncertainty in reasoning""",1
arXIv2023,ChatGPT MT: Competitive for High- (but not Low-) Resource Languages,Yes.,5,"""Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered.""",2023,2023-09-14T04:36:00Z,"Keyphrase: ""Performance gap between high-resource and low-resource languages""","""Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered."" Keyphrase: ""Performance gap between high-resource and low-resource languages""",7
arXIv2023,An Assessment of ChatGPT on Log Data,Yes.,5,"""Our findings show that the performance of the current version of ChatGPT for log processing is limited, with a lack of consistency in responses and scalability issues.""",2023,2023-09-14T04:09:27Z,"Keyphrase: ""Inconsistent responses and scalability issues""","""Our findings show that the performance of the current version of ChatGPT for log processing is limited, with a lack of consistency in responses and scalability issues."" Keyphrase: ""Inconsistent responses and scalability issues""",6
arXIv2023,In-Contextual Gender Bias Suppression for Large Language Models,Yes.,4,"""Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases.""",2023,2023-09-13T18:39:08Z,"Keyphrase: ""Gender bias encoding""","""Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases."" Keyphrase: ""Gender bias encoding""",3
arXIv2023,SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,Yes.,4,"""increasing attention has been paid to their safety concerns,"" and ""there is still significant room for improving the safety of current LLMs.""",2023,2023-09-13T15:56:50Z,"Keyphrase: ""Safety concerns""","""increasing attention has been paid to their safety concerns,"" and ""there is still significant room for improving the safety of current LLMs."" Keyphrase: ""Safety concerns""",2
arXIv2023,Scaled Prompt-Tuning for Few-Shot Natural Language Generation,Yes.,4,"""the memory demand and computation cost of fine-tuning LLMs on downstream tasks are non-negligible. Besides, fine-tuning generally requires a certain amount of data from individual tasks whilst data collection cost is another issue to consider in real-world applications.""",2023,2023-09-13T07:12:31Z,"Keyphrase: ""High computational cost and data requirements""","""the memory demand and computation cost of fine-tuning LLMs on downstream tasks are non-negligible. Besides, fine-tuning generally requires a certain amount of data from individual tasks whilst data collection cost is another issue to consider in real-world applications."" Keyphrase: ""High computational cost and data requirements""",4
arXIv2023,"TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",Yes.,5,"""However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges.""",2023,2023-09-13T04:47:43Z,"Keyphrase: ""Difficulty with numerical data and simulations""","""However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges."" Keyphrase: ""Difficulty with numerical data and simulations""",4
arXIv2023,Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions,Yes.,5,"""As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given.""",2023,2023-09-12T05:54:45Z,"Keyphrase: ""Risk of irrelevant information""","""As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given."" Keyphrase: ""Risk of irrelevant information""",0
arXIv2023,"Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs",Yes.,5,"""LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic nature, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (",2023,2023-09-12T02:14:05Z,"Keyphrase: ""Limited factual reliability""","""LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic nature, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and ( Keyphrase: ""Limited factual reliability""",0
arXIv2023,Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing,Yes.,5,"""These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning.""",2023,2023-09-12T00:54:15Z,"Keyphrase: ""Limited proficiency in complex strategic reasoning""","""These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning."" Keyphrase: ""Limited proficiency in complex strategic reasoning""",1
arXIv2023,PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis,Yes.,5,"""their effectiveness in assisting on-call engineers is constrained by low accuracy due to the intrinsic difficulty of the task, a propensity for LLM-based approaches to hallucinate, and difficulties in distinguishing these well-disguised hallucinations.""",2023,2023-09-11T21:24:00Z,"Keyphrase: ""Difficulty in distinguishing hallucination""","""their effectiveness in assisting on-call engineers is constrained by low accuracy due to the intrinsic difficulty of the task, a propensity for LLM-based approaches to hallucinate, and difficulties in distinguishing these well-disguised hallucinations."" Keyphrase: ""Difficulty in distinguishing hallucination""",0
arXIv2023,Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models,Yes.,5,"""Large Language Models (LLMs) struggle to perform such reasoning consistently.""",2023,2023-09-11T16:39:30Z,"Keyphrase: ""Inconsistent reasoning""","""Large Language Models (LLMs) struggle to perform such reasoning consistently."" Keyphrase: ""Inconsistent reasoning""",1
arXIv2023,Evaluating the Deductive Competence of Large Language Models,Yes.,5,"""The tested LLMs have limited abilities to solve these problems in their conventional form."" and ""Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance.""",2023,2023-09-11T13:47:07Z,"Keyphrase: ""Limited reasoning ability""","""The tested LLMs have limited abilities to solve these problems in their conventional form."" and ""Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance."" Keyphrase: ""Limited reasoning ability""",1
arXIv2023,Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis,Yes.,5,"""large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs"" and ""we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.""",2023,2023-09-11T03:35:00Z,"Keyphrase: ""Hallucination problem""","""large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs"" and ""we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination."" Keyphrase: ""Hallucination problem""",0
arXIv2023,Towards LLM-based Autograding for Short Textual Answers,Yes.,4,"""entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information"" and ""while 'out-of-the-box' LLMs provide a valuable tool to provide a complementary perspective, their readiness for independent automated grading remains a work in progress, necessitating",2023,2023-09-09T22:25:56Z,"Keyphrase: ""Ethical bias and false information""","""entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information"" and ""while 'out-of-the-box' LLMs provide a valuable tool to provide a complementary perspective, their readiness for independent automated grading remains a work in progress, necessitating Keyphrase: ""Ethical bias and false information""",3
arXIv2023,Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges,Yes.,4,"""Our findings indicate the promise of LLMs as interfaces to EHR, but also highlight the outstanding challenge posed by 'hallucinations'.""",2023,2023-09-08T18:44:47Z,"Keyphrase: ""Hallucination challenges""","""Our findings indicate the promise of LLMs as interfaces to EHR, but also highlight the outstanding challenge posed by 'hallucinations'."" Keyphrase: ""Hallucination challenges""",0
arXIv2023,Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,Yes.,5,"""We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans.""",2023,2023-09-08T17:49:44Z,"Keyphrase: ""Weak visual reasoning capability""","""We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans."" Keyphrase: ""Weak visual reasoning capability""",1
arXIv2023,Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems,Yes.,5,"""However, they are fraught with issues that undermine their utility and trustworthiness. These include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency).""",2023,2023-09-08T09:39:53Z,"Keyphrase: ""Erroneous information generation""","""However, they are fraught with issues that undermine their utility and trustworthiness. These include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency)."" Keyphrase: ""Erroneous information generation""",0
arXIv2023,Don't Ignore Dual Logic Ability of LLMs while Privatizing: A Data-Intensive Analysis in Medical Domain,Yes.,5,"""However, these privatization efforts often ignored a critical aspect",2023,2023-09-08T08:20:46Z,"Keyphrase: ""Overlooking critical aspects""","""However, these privatization efforts often ignored a critical aspect Keyphrase: ""Overlooking critical aspects""",6
arXIv2023,Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese,Yes.,4,"""LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts.""",2023,2023-09-08T07:42:57Z,"Keyphrase: ""Limited domain knowledge""","""LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts."" Keyphrase: ""Limited domain knowledge""",0
arXIv2023,DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models,Yes.,5,"""Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining.""",2023,2023-09-07T17:45:31Z,"Keyphrase: ""Hallucination tendency""","""Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining."" Keyphrase: ""Hallucination tendency""",0
arXIv2023,Large Language Models Are Not Robust Multiple Choice Selectors,Yes.,5,"""This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent 'selection bias',"" and ""We hope this work can draw broader research attention to the bias and robustness of modern LLMs.""",2023,2023-09-07T17:44:56Z,"Keyphrase: ""Vulnerability to selection bias""","""This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent 'selection bias',"" and ""We hope this work can draw broader research attention to the bias and robustness of modern LLMs."" Keyphrase: ""Vulnerability to selection bias""",3
arXIv2023,OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs,Yes.,4,"""an open research question concerns the inherent biases of trained models and their responses"" and ""Current research work seeks to de-bias such models, or suppress potentially biased answers.""",2023,2023-09-07T17:41:01Z,"Keyphrase: ""Inherent bias""","""an open research question concerns the inherent biases of trained models and their responses"" and ""Current research work seeks to de-bias such models, or suppress potentially biased answers."" Keyphrase: ""Inherent bias""",3
arXIv2023,XGen-7B Technical Report,Yes.,4,"""most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context.""",2023,2023-09-07T02:20:03Z,"Keyphrase: ""Proprietary confinement and limited sequence length""","""most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context."" Keyphrase: ""Proprietary confinement and limited sequence length""",4
arXIv2023,Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity,Yes.,5,"""We conclude that the LLM we tested (GPT-3.5) does not have sufficient algorithmic fidelity to expect research on it to generalize to human populations.""",2023,2023-09-06T15:00:44Z,"Keyphrase: ""Limited algorithmic fidelity""","""We conclude that the LLM we tested (GPT-3.5) does not have sufficient algorithmic fidelity to expect research on it to generalize to human populations."" Keyphrase: ""Limited algorithmic fidelity""",4
arXIv2023,Zero-Resource Hallucination Prevention for Large Language Models,Yes.,4,"""The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of 'hallucination,' which refers to instances where LLMs generate factually inaccurate or ungrounded information.""",2023,2023-09-06T01:57:36Z,"Keyphrase: ""Factually inaccurate hallucinations""","""The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of 'hallucination,' which refers to instances where LLMs generate factually inaccurate or ungrounded information."" Keyphrase: ""Factually inaccurate hallucinations""",0
arXIv2023,An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models,Yes.,5,"""However, recent studies have revealed that these models often suffer from hallucinations, leading to overly confident but incorrect judgments. This limits their application in the medical domain, where tasks require the utmost accuracy.""",2023,2023-09-05T09:24:48Z,"Keyphrase: ""Hallucination and overconfidence""","""However, recent studies have revealed that these models often suffer from hallucinations, leading to overly confident but incorrect judgments. This limits their application in the medical domain, where tasks require the utmost accuracy."" Keyphrase: ""Hallucination and overconfidence""",0
arXIv2023,Open Sesame! Universal Black Box Jailbreaking of Large Language Models,Yes.,5,"""Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior.""",2023,2023-09-04T08:54:20Z,"Keyphrase: ""Unexpected deviations""","""Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior."" Keyphrase: ""Unexpected deviations""",0
arXIv2023,Representations Matter: Embedding Modes of Large Language Models using Dynamic Mode Decomposition,Yes.,5,"""Existing large language models (LLMs) are known for generating 'hallucinated' content, namely a fabricated text of plausibly looking, yet unfounded, facts.""",2023,2023-09-03T19:10:18Z,"Keyphrase: ""Fabricated content generation""","""Existing large language models (LLMs) are known for generating 'hallucinated' content, namely a fabricated text of plausibly looking, yet unfounded, facts."" Keyphrase: ""Fabricated content generation""",0
arXIv2023,Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,Yes.,5,"""a significant concern revolves around their propensity to exhibit hallucinations",2023,2023-09-03T16:56:48Z,"Keyphrase: ""Propensity for hallucination""","""a significant concern revolves around their propensity to exhibit hallucinations Keyphrase: ""Propensity for hallucination""",0
arXIv2023,FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs,Yes.,5,"""The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs."" and ""consumer-level GPUs...are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth."" and ""this system faces critical",2023,2023-09-03T13:27:56Z,"Keyphrase: ""Hardware limitations""","""The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs."" and ""consumer-level GPUs...are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth."" and ""this system faces critical Keyphrase: ""Hardware limitations""",4
arXIv2023,Bias Testing and Mitigation in LLM-based Code Generation,Yes.,5,"""As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged",2023,2023-09-03T07:14:49Z,"Keyphrase: ""Challenges in software coding""","""As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged Keyphrase: ""Challenges in software coding""",7
arXIv2023,Explainability for Large Language Models: A Survey,Yes.,4,"""However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications."" and ""understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts.""",2023,2023-09-02T22:14:26Z,"Keyphrase: ""Lack of transparency""","""However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications."" and ""understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts."" Keyphrase: ""Lack of transparency""",8
arXIv2023,Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports,Yes.,4,"""While performing well in terms of accuracy, both the lack of interpretability and limitations to transfer across languages limit their use in clinical setting.""",2023,2023-09-02T11:46:41Z,"Keyphrase: ""Lack of interpretability""","""While performing well in terms of accuracy, both the lack of interpretability and limitations to transfer across languages limit their use in clinical setting."" Keyphrase: ""Lack of interpretability""",1
arXIv2023,No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function,Yes.,5,"""However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions.""",2023,2023-09-01T13:10:54Z,"Keyphrase: ""Difficulty in generating correct reasoning steps""","""However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions."" Keyphrase: ""Difficulty in generating correct reasoning steps""",1
arXIv2023,BatchPrompt: Accomplish more with less,Yes.,5,"""prompting with batched data in longer contexts will inevitably lead to worse performance, compared to single-data prompting"" and ""the performance of the language model is significantly correlated with the positions and order of the batched data, due to the corresponding change in decoder context.""",2023,2023-09-01T10:44:36Z,"Keyphrase: ""Performance degradation with longer context""","""prompting with batched data in longer contexts will inevitably lead to worse performance, compared to single-data prompting"" and ""the performance of the language model is significantly correlated with the positions and order of the batched data, due to the corresponding change in decoder context."" Keyphrase: ""Performance degradation with longer context""",4
arXIv2023,Why do universal adversarial attacks work on large language models?: Geometry might be the answer,Yes.,5,"""Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature.""",2023,2023-09-01T05:09:49Z,"Keyphrase: ""Vulnerability to universal adversarial attacks""","""Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature."" Keyphrase: ""Vulnerability to universal adversarial attacks""",2
arXIv2023,Context Aware Query Rewriting for Text Rankers using LLM,Yes.,5,"""We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing.""",2023,2023-08-31T14:19:50Z,"Keyphrase: ""Concept drift and high inference cost""","""We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing."" Keyphrase: ""Concept drift and high inference cost""",4
arXIv2023,Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering,Yes.,5,"""We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting.""",2023,2023-08-31T10:31:19Z,"Keyphrase: ""Limited utility for knowledge graph generation""","""We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting."" Keyphrase: ""Limited utility for knowledge graph generation""",6
arXIv2023,LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models,Yes.,5,"""their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues.""",2023,2023-08-30T16:47:51Z,"Keyphrase: ""Struggles with long context encoding""","""their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues."" Keyphrase: ""Struggles with long context encoding""",4
arXIv2023,Quantifying and Analyzing Entity-level Memorization in Large Language Models,Yes.,5,"""privacy risks arising from memorization have attracted increasing attention"" and ""LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations.""",2023,2023-08-30T03:06:47Z,"Keyphrase: ""Privacy risks from memorization""","""privacy risks arising from memorization have attracted increasing attention"" and ""LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations."" Keyphrase: ""Privacy risks from memorization""",8
arXIv2023,Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model,Yes.,5,"""The instructions given to the LLM by natural language may include ambiguity and lack of information depending on the task context."" and ""However, our experiments also revealed challenges in robot action planning with LLM, such as asking unimportant questions and assuming crucial information without asking.""",2023,2023-08-30T00:54:44Z,"Keyphrase: ""Challenges in task context understanding""","""The instructions given to the LLM by natural language may include ambiguity and lack of information depending on the task context."" and ""However, our experiments also revealed challenges in robot action planning with LLM, such as asking unimportant questions and assuming crucial information without asking."" Keyphrase: ""Challenges in task context understanding""",1
arXIv2023,Evaluation and Analysis of Hallucination in Large Vision-Language Models,Yes.,5,"""LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios."" and ""we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem.""",2023,2023-08-29T08:51:24Z,"Keyphrase: ""Persistent hallucination issues""","""LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios."" and ""we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem."" Keyphrase: ""Persistent hallucination issues""",0
arXIv2023,Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models,Yes.,5,"""However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses.""",2023,2023-08-29T04:59:53Z,"Keyphrase: ""Poor memory recall""","""However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses."" Keyphrase: ""Poor memory recall""",6
arXIv2023,Gender bias and stereotypes in Large Language Models,Yes.,5,"""This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models."" and ""LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior.""",2023,2023-08-28T22:32:05Z,"Keyphrase: ""Biased rationalization""","""This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models."" and ""LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior."" Keyphrase: ""Biased rationalization""",3
arXIv2023,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",Yes.,5,"""most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs"" and ""still struggles on longer contexts"" and ""Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have",2023,2023-08-28T11:53:40Z,"Keyphrase: ""Struggles with long context""","""most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs"" and ""still struggles on longer contexts"" and ""Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have Keyphrase: ""Struggles with long context""",4
arXIv2023,Biomedical Entity Linking with Triple-aware Pre-Training,Yes.,4,"""a difficulty of linking the biomedical entities using current large language models (LLM) trained on a general corpus is that biomedical entities are scarcely distributed in texts and therefore have been rarely seen during training by the LLM"" and ""those LLMs are not aware of high level semantic connection between different biomedical entities, which are useful in identifying similar concepts in different textual contexts.""",2023,2023-08-28T09:06:28Z,"Keyphrase: ""Limited biomedical entity linking""","""a difficulty of linking the biomedical entities using current large language models (LLM) trained on a general corpus is that biomedical entities are scarcely distributed in texts and therefore have been rarely seen during training by the LLM"" and ""those LLMs are not aware of high level semantic connection between different biomedical entities, which are useful in identifying similar concepts in different textual contexts."" Keyphrase: ""Limited biomedical entity linking""",1
arXIv2023,EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models,Yes.,4,"""the transition of LLMs from data centers to edge devices presents a set of challenges and opportunities. While this shift can enhance privacy and availability, it is hampered by the enormous parameter sizes of these models, leading to impractical runtime costs.""",2023,2023-08-28T06:56:08Z,"Keyphrase: ""Impractical runtime cost due to enormous parameter size""","""the transition of LLMs from data centers to edge devices presents a set of challenges and opportunities. While this shift can enhance privacy and availability, it is hampered by the enormous parameter sizes of these models, leading to impractical runtime costs."" Keyphrase: ""Impractical runtime cost due to enormous parameter size""",4
arXIv2023,Symbolic and Language Agnostic Large Language Models,Yes.,5,"""due to the subsymbolic nature of these models whatever knowledge these systems acquire about language will always be buried in millions of microfeatures (weights) none of which is meaningful on its own. Moreover, and due to their stochastic nature, these models will often fail in capturing various inferential aspects that are prevalent in natural language.""",2023,2023-08-27T20:24:33Z,"Keyphrase: ""Limited inferential capability""","""due to the subsymbolic nature of these models whatever knowledge these systems acquire about language will always be buried in millions of microfeatures (weights) none of which is meaningful on its own. Moreover, and due to their stochastic nature, these models will often fail in capturing various inferential aspects that are prevalent in natural language."" Keyphrase: ""Limited inferential capability""",1
arXIv2023,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,Yes.,4,"""The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs.""",2023,2023-08-27T19:22:12Z,"Keyphrase: ""Crosslingual imbalance""","""The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs."" Keyphrase: ""Crosslingual imbalance""",3
arXIv2023,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",Yes.,5,"""Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage.""",2023,2023-08-27T16:14:19Z,"Keyphrase: ""High computational requirements and complex deployment""","""Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage."" Keyphrase: ""High computational requirements and complex deployment""",4
arXIv2023,Detecting Language Model Attacks with Perplexity,Yes.,4,"""Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content."" and ""false positives are a significant challenge for plain perplexity filtering.""",2023,2023-08-27T15:20:06Z,"Keyphrase: ""Security vulnerability and potential misuse""","""Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content."" and ""false positives are a significant challenge for plain perplexity filtering."" Keyphrase: ""Security vulnerability and potential misuse""",2
arXIv2023,Rethinking Language Models as Symbolic Knowledge Graphs,Yes.,5,"""Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes."" and ""Our extensive evaluation of various LMs shows that while these models exhibit considerable potential in recalling factual information, their ability to capture intricate topological and semantic traits of KGs remains significantly constrained.""",2023,2023-08-25T21:25:08Z,"Keyphrase: ""Limited ability to capture intricate semantic traits""","""Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes."" and ""Our extensive evaluation of various LMs shows that while these models exhibit considerable potential in recalling factual information, their ability to capture intricate topological and semantic traits of KGs remains significantly constrained."" Keyphrase: ""Limited ability to capture intricate semantic traits""",1
arXIv2023,The Poison of Alignment,Yes.,5,"""alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP",2023,2023-08-25T15:51:15Z,"Keyphrase: ""Degraded performance due to alignment poisoning""","""alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP Keyphrase: ""Degraded performance due to alignment poisoning""",2
arXIv2023,Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering,Yes.,5,"""Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA.""",2023,2023-08-25T09:23:55Z,"Keyphrase: ""Limited access to external knowledge""","""Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA."" Keyphrase: ""Limited access to external knowledge""",0
arXIv2023,Causal Parrots: Large Language Models May Talk Causality But Are Not Causal,Yes.,5,"""We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise."" and ""Our empirical analysis provides favoring evidence that current LLMs are even weak 'causal parrots.'""",2023,2023-08-24T20:23:13Z,"Keyphrase: ""Limited causal reasoning""","""We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise."" and ""Our empirical analysis provides favoring evidence that current LLMs are even weak 'causal parrots.'"" Keyphrase: ""Limited causal reasoning""",1
arXIv2023,"Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",Yes.,5,"""safety- and security-related threats and vulnerabilities of LLMs,"" ""LLMs can be misused for fraud, impersonation, and the generation of malware,"" ""security-related problems with such models,"" ""limitations of LLMs in light of such security concerns.""",2023,2023-08-24T14:45:50Z,"Keyphrase: ""Security vulnerabilities""","""safety- and security-related threats and vulnerabilities of LLMs,"" ""LLMs can be misused for fraud, impersonation, and the generation of malware,"" ""security-related problems with such models,"" ""limitations of LLMs in light of such security concerns."" Keyphrase: ""Security vulnerabilities""",2
arXIv2023,VIGC: Visual Instruction Generation and Correction,Yes.,5,"""the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information.""",2023,2023-08-24T11:21:05Z,"Keyphrase: ""Generation of false information""","""the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information."" Keyphrase: ""Generation of false information""",0
arXIv2023,Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs,Yes.,5,"""We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating 'unknown' outputs, even when the correct document is among the top-k retrieved passages.""",2023,2023-08-24T05:26:54Z,"Keyphrase: ""Unknown output generation""","""We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating 'unknown' outputs, even when the correct document is among the top-k retrieved passages."" Keyphrase: ""Unknown output generation""",6
arXIv2023,Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models,Yes.,4,"""While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty.""",2023,2023-08-23T17:40:35Z,"Keyphrase: ""Hallucination and predictive uncertainty""","""While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty."" Keyphrase: ""Hallucination and predictive uncertainty""",0
arXIv2023,Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test,Yes.,5,"""Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents."" and ""However, the model still fails to answer some questions correctly even with providing library of context, highlighting room",2023,2023-08-22T23:18:53Z,"Keyphrase: ""Limited adaptability to new information""","""Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents."" and ""However, the model still fails to answer some questions correctly even with providing library of context, highlighting room Keyphrase: ""Limited adaptability to new information""",5
arXIv2023,Towards an On-device Agent for Text Rewriting,Yes.,4,"""Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference.""",2023,2023-08-22T22:18:38Z,"Keyphrase: ""Impractical on-device inference""","""Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference."" Keyphrase: ""Impractical on-device inference""",4
arXIv2023,Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models,Yes.,5,"""open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts.""",2023,2023-08-22T20:12:49Z,"Keyphrase: ""Severe hallucination in smaller LLMs""","""open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts."" Keyphrase: ""Severe hallucination in smaller LLMs""",0
arXIv2023,Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis,Yes.,5,"""All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy."" and ""GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity."" and ""GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities.""",2023,2023-08-22T06:32:07Z,"Keyphrase: ""Limited structural reasoning capabilities""","""All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy."" and ""GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity."" and ""GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities."" Keyphrase: ""Limited structural reasoning capabilities""",1
arXIv2023,Giraffe: Adventures in Expanding Context Lengths in LLMs,Yes.,5,"""Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time.""",2023,2023-08-21T17:30:16Z,"Keyphrase: ""Limited context handling""","""Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time."" Keyphrase: ""Limited context handling""",4
arXIv2023,Fact-checking information generated by a large language model can decrease news discernment,Yes.,5,"""we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases",2023,2023-08-21T15:47:37Z,"Keyphrase: ""Limited accuracy in fact-checking""","""we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases Keyphrase: ""Limited accuracy in fact-checking""",0
arXIv2023,SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding,Yes.,5,"""LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction",2023,2023-08-21T07:31:19Z,"Keyphrase: ""Limited natural language understanding""","""LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction Keyphrase: ""Limited natural language understanding""",1
arXIv2023,FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models,Yes.,4,"""Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied."" and ""Experimental results reveal varying degrees of stereotypes and biases in five LLMs evaluated on Edu-FairMonitor.""",2023,2023-08-21T00:25:17Z,"Keyphrase: ""Persistent stereotype bias""","""Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied."" and ""Experimental results reveal varying degrees of stereotypes and biases in five LLMs evaluated on Edu-FairMonitor."" Keyphrase: ""Persistent stereotype bias""",3
arXIv2023,Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation,Yes.,5,"""the reliability and robustness of the code generation from LLMs have not yet been thoroughly studied"" and ""even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software",2023,2023-08-20T18:36:28Z,"Keyphrase: ""Code generation reliability""","""the reliability and robustness of the code generation from LLMs have not yet been thoroughly studied"" and ""even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software Keyphrase: ""Code generation reliability""",7
arXIv2023,A Survey on Fairness in Large Language Models,Yes.,4,"""LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms.""",2023,2023-08-20T03:30:22Z,"Keyphrase: ""Propagation of social bias""","""LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms."" Keyphrase: ""Propagation of social bias""",3
arXIv2023,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Yes.,5,"""the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public"" and ""even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of",2023,2023-08-18T16:27:04Z,"Keyphrase: ""Ethical concerns and harmful output""","""the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public"" and ""even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of Keyphrase: ""Ethical concerns and harmful output""",2
arXIv2023,RatGPT: Turning online LLMs into Proxies for Malware Attacks,Yes.,4,"""These studies covered scenarios that still require the attacker to be in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim."" and ""This proof-of-concept highlights significant cybersecurity issues with openly available plugins and L",2023,2023-08-17T20:54:39Z,"Keyphrase: ""Vulnerability to cybersecurity attacks""","""These studies covered scenarios that still require the attacker to be in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim."" and ""This proof-of-concept highlights significant cybersecurity issues with openly available plugins and L Keyphrase: ""Vulnerability to cybersecurity attacks""",2
arXIv2023,Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning,Yes.,5,"""However, our findings spotlight both vision-language model's limitations",2023,2023-08-17T03:14:00Z,"Keyphrase: ""Limited vision-language integration""","""However, our findings spotlight both vision-language model's limitations Keyphrase: ""Limited vision-language integration""",4
arXIv2023,An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning,Yes.,5,"""The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale increases, the severity of forgetting intensifies.""",2023,2023-08-17T02:53:23Z,"Keyphrase: ""Catastrophic forgetting""","""The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale increases, the severity of forgetting intensifies."" Keyphrase: ""Catastrophic forgetting""",5
arXIv2023,Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models,Yes.,5,"""Unfortunately, these defenses are not foolproof, and some attackers have crafted 'jailbreak' prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions.""",2023,2023-08-16T09:04:36Z,Keyphrase: Vulnerability to crafted attacks,"""Unfortunately, these defenses are not foolproof, and some attackers have crafted 'jailbreak' prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions."" Keyphrase: Vulnerability to crafted attacks",2
arXIv2023,Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation,Yes.,4,"""contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data.""",2023,2023-08-15T08:49:14Z,"Keyphrase: ""Struggles with coherence and diversity""","""contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data."" Keyphrase: ""Struggles with coherence and diversity""",5
arXIv2023,Detecting The Corruption Of Online Questionnaires By Artificial Intelligence,Yes.,4,"""Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. ... Automatic AI detection systems are currently completely unusable.""",2023,2023-08-14T23:47:56Z,"Keyphrase: ""Vulnerability to manipulation""","""Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. ... Automatic AI detection systems are currently completely unusable."" Keyphrase: ""Vulnerability to manipulation""",2
arXIv2023,CausalLM is not optimal for in-context learning,Yes.,5,"""Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples.""",2023,2023-08-14T03:14:38Z,"Keyphrase: ""Limited in-context learning""","""Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples."" Keyphrase: ""Limited in-context learning""",5
arXIv2023,Dynamic Planning with a LLM,Yes.,5,"""applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows.""",2023,2023-08-11T21:17:13Z,"Keyphrase: ""Difficulty with complex multi-step reasoning""","""applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows."" Keyphrase: ""Difficulty with complex multi-step reasoning""",1
arXIv2023,Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems,Yes.,4,"""there are still often 'interface' failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.""",2023,2023-08-10T17:22:28Z,"Keyphrase: ""Interface failure and formulation challenges""","""there are still often 'interface' failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems."" Keyphrase: ""Interface failure and formulation challenges""",1
arXIv2023,C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT,Yes.,5,"""However, human forgetting and model contextual forgetting remain prominent issues in multi-turn conversation scenarios, which challenge the users' conversation comprehension and contextual continuity for ChatGPT.""",2023,2023-08-10T13:29:12Z,"Keyphrase: ""Contextual forgetting""","""However, human forgetting and model contextual forgetting remain prominent issues in multi-turn conversation scenarios, which challenge the users' conversation comprehension and contextual continuity for ChatGPT."" Keyphrase: ""Contextual forgetting""",5
arXIv2023,LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following,Yes.,4,"""mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers"" and ""LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission",2023,2023-08-09T12:26:37Z,"Keyphrase: ""Privacy concerns and data safeguarding""","""mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers"" and ""LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission Keyphrase: ""Privacy concerns and data safeguarding""",8
arXIv2023,CLEVA: Chinese Language Models EVAluation Platform,Yes.,4,"""The absence of a comprehensive Chinese benchmark that thoroughly assesses a model's performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs.""",2023,2023-08-09T09:11:31Z,"Keyphrase: ""Lack of standardized evaluation benchmarks""","""The absence of a comprehensive Chinese benchmark that thoroughly assesses a model's performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs."" Keyphrase: ""Lack of standardized evaluation benchmarks""",7
arXIv2023,"""Do Anything Now"": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",Yes.,5,"""Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios.""",2023,2023-08-07T16:55:20Z,"Keyphrase: ""Inadequate safeguards against jailbreak prompts""","""Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios."" Keyphrase: ""Inadequate safeguards against jailbreak prompts""",2
arXIv2023,AgentBench: Evaluating LLMs as Agents,Yes.,5,"""We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.""",2023,2023-08-07T16:08:11Z,"Keyphrase: ""Poor long-term reasoning""","""We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents."" Keyphrase: ""Poor long-term reasoning""",1
arXIv2023,Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records,Yes.,5,"""the inability of LLMs to derive reasoning paradigms that allow for comprehensive understanding of medical variables"" and ""the need for LLM steering through the application of symbolic reasoning as the exclusive use of LLMs results in the lowest performance.""",2023,2023-08-07T07:29:49Z,"Keyphrase: ""Lack of reasoning capabilities""","""the inability of LLMs to derive reasoning paradigms that allow for comprehensive understanding of medical variables"" and ""the need for LLM steering through the application of symbolic reasoning as the exclusive use of LLMs results in the lowest performance."" Keyphrase: ""Lack of reasoning capabilities""",1
arXIv2023,Studying Large Language Model Generalization with Influence Functions,Yes.,4,"""While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP)."" and ""Despite many apparently sophisticated forms of generalization, we identify a surprising limitation",2023,2023-08-07T04:47:42Z,"Keyphrase: ""Limited scalability due to complex computations""","""While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP)."" and ""Despite many apparently sophisticated forms of generalization, we identify a surprising limitation Keyphrase: ""Limited scalability due to complex computations""",4
arXIv2023,Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023),Yes.,5,"""humans achieve their capacity for language after exposure to several orders of magnitude less data,"" ""LLMs currently show little promise of solving this mystery,"" ""LLMs cannot constitute scientific theories of language for several reasons,"" ""scientific theories must provide interpretable explanations, not just predictions.""",2023,2023-08-06T23:41:14Z,"Keyphrase: ""Limited interpretability and predictive power""","""humans achieve their capacity for language after exposure to several orders of magnitude less data,"" ""LLMs currently show little promise of solving this mystery,"" ""LLMs cannot constitute scientific theories of language for several reasons,"" ""scientific theories must provide interpretable explanations, not just predictions."" Keyphrase: ""Limited interpretability and predictive power""",1
arXIv2023,TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties,Yes.,4,"""Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored."" and ""Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist,"" and ""instruction-tuned LLMs, however, trail behind commercial systems such as Google",2023,2023-08-06T08:29:16Z,"Keyphrase: ""Linguistic inclusivity challenges""","""Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored."" and ""Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist,"" and ""instruction-tuned LLMs, however, trail behind commercial systems such as Google Keyphrase: ""Linguistic inclusivity challenges""",6
arXIv2023,An Empirical Study of AI-based Smart Contract Creation,Yes.,4,"""Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted.""",2023,2023-08-05T21:38:57Z,"Keyphrase: ""Security vulnerabilities and code quality""","""Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted."" Keyphrase: ""Security vulnerabilities and code quality""",2
arXIv2023,The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations,Yes.,4,"""We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women.""",2023,2023-08-03T21:12:54Z,"Keyphrase: ""Biased recommendations based on demographics""","""We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women."" Keyphrase: ""Biased recommendations based on demographics""",3
arXIv2023,ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation,Yes.,5,"""First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs."" and ""Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes",2023,2023-08-03T16:31:02Z,"Keyphrase: ""Limited class-level code generation performance""","""First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs."" and ""Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes Keyphrase: ""Limited class-level code generation performance""",7
arXIv2023,XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models,Yes.,5,"""Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content."" and ""we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way."" and ""highlight systematic failure modes in state-of",2023,2023-08-02T16:30:40Z,"Keyphrase: ""Vulnerability to malicious instructions""","""Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content."" and ""we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way."" and ""highlight systematic failure modes in state-of Keyphrase: ""Vulnerability to malicious instructions""",2
arXIv2023,SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning,Yes.,5,"""However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes.""",2023,2023-08-01T10:31:36Z,"Keyphrase: ""Limited nonlinear thinking""","""However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes."" Keyphrase: ""Limited nonlinear thinking""",1
arXIv2023,Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias,Yes.,5,"""Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4.""",2023,2023-08-01T01:39:25Z,"Keyphrase: ""Bias presence after instruction tuning""","""Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4."" Keyphrase: ""Bias presence after instruction tuning""",3
arXIv2023,SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension,Yes.,4,"""By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research.""",2023,2023-07-30T04:25:16Z,"Keyphrase: ""Limited evaluation insights""","""By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research."" Keyphrase: ""Limited evaluation insights""",0
arXIv2023,"Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system",Yes.,4,"""Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context."" and ""However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics.""",2023,2023-07-28T20:25:11Z,"Keyphrase: ""Lack of personal relevance""","""Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context."" and ""However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics."" Keyphrase: ""Lack of personal relevance""",1
arXIv2023,"A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI",Yes.,4,"""It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity.""",2023,2023-07-28T09:20:22Z,"Keyphrase: ""Bias sensitivity""","""It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity."" Keyphrase: ""Bias sensitivity""",3
arXIv2023,Med-HALT: Medical Domain Hallucination Test for Large Language Models,Yes.,5,"""Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications.""",2023,2023-07-28T06:43:04Z,"Keyphrase: ""Generating unverified information""","""Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications."" Keyphrase: ""Generating unverified information""",0
arXIv2023,An Overview Of Temporal Commonsense Reasoning and Acquisition,Yes.,5,"""Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps."" and ""However, these augmented models still struggle to approach human performance on reasoning tasks over temporal common sense",2023,2023-07-28T01:30:15Z,"Keyphrase: ""Limited reasoning capability""","""Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps."" and ""However, these augmented models still struggle to approach human performance on reasoning tasks over temporal common sense Keyphrase: ""Limited reasoning capability""",1
arXIv2023,Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback,Yes.,4,"""RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs)."" and ""survey open problems and fundamental limitations of RLHF and related methods.""",2023,2023-07-27T22:29:25Z,"Keyphrase: ""Fundamental limitations in fine-tuning""","""RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs)."" and ""survey open problems and fundamental limitations of RLHF and related methods."" Keyphrase: ""Fundamental limitations in fine-tuning""",5
arXIv2023,This is not correct! Negation-aware Evaluation of Language Generation Systems,Yes.,5,"""Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations.""",2023,2023-07-26T06:54:31Z,"Keyphrase: ""Insensitive to negation""","""Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations."" Keyphrase: ""Insensitive to negation""",1
arXIv2023,Aligning Large Language Models with Human: A Survey,Yes.,4,"""Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information.""",2023,2023-07-24T17:44:58Z,"Keyphrase: ""Biased and factually incorrect content""","""Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information."" Keyphrase: ""Biased and factually incorrect content""",0
arXIv2023,Interpretable Stereotype Identification through Reasoning,Yes.,4,"""Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination.""",2023,2023-07-24T15:12:13Z,"Keyphrase: ""Inherent bias perpetuation""","""Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination."" Keyphrase: ""Inherent bias perpetuation""",3
arXIv2023,"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",Yes.,5,"""the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.""",2023,2023-07-24T14:56:30Z,"Keyphrase: ""Limited context and inductive bias""","""the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML."" Keyphrase: ""Limited context and inductive bias""",3
arXIv2023,Performance of Large Language Models in a Computer Science Degree Program,Yes.,4,"""We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program"" and ""Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations.""",2023,2023-07-24T14:17:00Z,"Keyphrase: ""Limitation in mathematical calculation""","""We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program"" and ""Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations."" Keyphrase: ""Limitation in mathematical calculation""",1
arXIv2023,Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models,Yes.,5,"""However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects.""",2023,2023-07-24T07:40:59Z,"Keyphrase: ""Limited interaction capabilities""","""However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects."" Keyphrase: ""Limited interaction capabilities""",6
arXIv2023,The Effectiveness of Large Language Models (ChatGPT and CodeBERT) for Security-Oriented Code Analysis,Yes.,4,"""However, we observed that the strengths and limitations of adopting these LLMs to the code analysis have not been investigated."" and ""However, it is essential to acknowledge certain limitations, such as the heavy reliance on well-defined variable and function names, making them unable to learn from anonymized code.""",2023,2023-07-24T02:38:24Z,"Keyphrase: ""Limited ability to learn anonymized code""","""However, we observed that the strengths and limitations of adopting these LLMs to the code analysis have not been investigated."" and ""However, it is essential to acknowledge certain limitations, such as the heavy reliance on well-defined variable and function names, making them unable to learn from anonymized code."" Keyphrase: ""Limited ability to learn anonymized code""",7
arXIv2023,In-Context Learning Learns Label Relationships but Is Not Conventional Learning,Yes.,4,"""we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations,"" and ""ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.""",2023,2023-07-23T16:54:41Z,"Keyphrase: ""Struggle with incorporating in-context information""","""we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations,"" and ""ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally."" Keyphrase: ""Struggle with incorporating in-context information""",5
arXIv2023,GPT-4 Can't Reason,Yes.,5,"""However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason."" and ""Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.""",2023,2023-07-21T17:04:25Z,"Keyphrase: ""Incapable of reasoning""","""However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason."" and ""Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning."" Keyphrase: ""Incapable of reasoning""",1
arXIv2023,SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models,Yes.,5,"""The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%."" and ""we categorize the errors made by LLMs into ten problem-solving abilities.""",2023,2023-07-20T07:01:57Z,"Keyphrase: ""Limited problem-solving ability""","""The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%."" and ""we categorize the errors made by LLMs into ten problem-solving abilities."" Keyphrase: ""Limited problem-solving ability""",1
arXIv2023,What can we learn from Data Leakage and Unlearning for Law?,Yes.,5,"""Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference."" and ""The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-t",2023,2023-07-19T22:14:58Z,"Keyphrase: ""Privacy concerns and data memorization""","""Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference."" and ""The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-t Keyphrase: ""Privacy concerns and data memorization""",8
arXIv2023,Generating Mathematical Derivations with Large Language Models,Yes.,5,"""Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in conventional scores. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse",2023,2023-07-19T14:13:02Z,"Keyphrase: ""Sensitivity to perturbations""","""Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in conventional scores. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse Keyphrase: ""Sensitivity to perturbations""",5
arXIv2023,CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility,Yes.,4,"""there is considerable room for improvement in terms of responsibility"" and ""evaluation of human values alignment is becoming increasingly important.""",2023,2023-07-19T01:22:40Z,"Keyphrase: ""Lack of human value alignment""","""there is considerable room for improvement in terms of responsibility"" and ""evaluation of human values alignment is becoming increasingly important."" Keyphrase: ""Lack of human value alignment""",3
arXIv2023,Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study,Yes.,5,"""While state-of-the-art transformer models provide excellent results in text classification, most of them have limitations in the maximum sequence length of the input sequence. The majority of the transformer models are limited to 512 tokens, and therefore, they struggle with long document classification problems.""",2023,2023-07-18T18:21:26Z,"Keyphrase: ""Limited maximum sequence length""","""While state-of-the-art transformer models provide excellent results in text classification, most of them have limitations in the maximum sequence length of the input sequence. The majority of the transformer models are limited to 512 tokens, and therefore, they struggle with long document classification problems."" Keyphrase: ""Limited maximum sequence length""",4
arXIv2023,Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications,Yes.,4,"""The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models."" and ""The discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities.""",2023,2023-07-18T11:38:45Z,"Keyphrase: ""Gender bias in language usage""","""The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models."" and ""The discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities."" Keyphrase: ""Gender bias in language usage""",3
arXIv2023,Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations,Yes.,5,"""We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.""",2023,2023-07-17T17:41:47Z,"Keyphrase: ""Low precision in explanations""","""We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution."" Keyphrase: ""Low precision in explanations""",1
arXIv2023,Measuring Faithfulness in Chain-of-Thought Reasoning,Yes.,5,"""it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning"" and ""As models become larger and more capable, they produce less faithful reasoning on most tasks we study.""",2023,2023-07-17T01:08:39Z,"Keyphrase: ""Lack of faithful reasoning""","""it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning"" and ""As models become larger and more capable, they produce less faithful reasoning on most tasks we study."" Keyphrase: ""Lack of faithful reasoning""",1
arXIv2023,The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant,Yes.,5,"""there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use.""",2023,2023-07-16T21:19:47Z,"Keyphrase: ""Inadequate for real-world clinical use""","""there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use."" Keyphrase: ""Inadequate for real-world clinical use""",0
arXIv2023,Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models,Yes.,5,"""erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions.""",2023,2023-07-16T08:28:04Z,"Keyphrase: ""Trustworthiness concerns""","""erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions."" Keyphrase: ""Trustworthiness concerns""",2
arXIv2023,Leveraging Large Language Models to Generate Answer Set Programs,Yes.,4,"""However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques.""",2023,2023-07-15T03:40:55Z,"Keyphrase: ""Limited reasoning capability""","""However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques."" Keyphrase: ""Limited reasoning capability""",1
arXIv2023,Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study,Yes.,5,"""Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications. Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the",2023,2023-07-13T02:31:55Z,"Keyphrase: ""High word error rate""","""Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications. Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the Keyphrase: ""High word error rate""",6
arXIv2023,A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation,Yes.,5,"""However, these models often tend to 'hallucinate' which critically hampers their reliability.""",2023,2023-07-08T14:25:57Z,"Keyphrase: ""Hallucination tendency""","""However, these models often tend to 'hallucinate' which critically hampers their reliability."" Keyphrase: ""Hallucination tendency""",0
arXIv2023,Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task,Yes.,5,"""some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks,"" and ""the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different",2023,2023-07-08T13:10:59Z,"Keyphrase: ""Limited performance in grammatical error correction""","""some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks,"" and ""the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different Keyphrase: ""Limited performance in grammatical error correction""",6
arXIv2023,"Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions",Yes.,4,"""With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF.""",2023,2023-07-08T09:28:50Z,"Keyphrase: ""Challenge for RTBF compliance""","""With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF."" Keyphrase: ""Challenge for RTBF compliance""",6
arXIv2023,Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models,Yes.,5,"""We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs."" and ""We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs.""",2023,2023-07-07T13:58:16Z,"Keyphrase: ""Overstated evaluation capabilities""","""We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs."" and ""We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs."" Keyphrase: ""Overstated evaluation capabilities""",6
arXIv2023,TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction,Yes.,5,"""When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called $\textit{hallucinations}$.""",2023,2023-07-07T02:42:06Z,"Keyphrase: ""Textual hallucinations""","""When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called $\textit{hallucinations}$."" Keyphrase: ""Textual hallucinations""",0
arXIv2023,Focused Transformer: Contrastive Training for Context Scaling,Yes.,5,"""However, the full potential of such an approach is often restrained due to a limitation in the effective context length."" and ""We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish.""",2023,2023-07-06T17:52:10Z,"Keyphrase: ""Context length limitation""","""However, the full potential of such an approach is often restrained due to a limitation in the effective context length."" and ""We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish."" Keyphrase: ""Context length limitation""",4
arXIv2023,Style Over Substance: Evaluation Biases for Large Language Models,Yes.,4,"""Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors.""",2023,2023-07-06T14:42:01Z,"Keyphrase: ""Biased evaluation and factual errors""","""Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors."" Keyphrase: ""Biased evaluation and factual errors""",3
arXIv2023,Scaling In-Context Demonstrations with Structured Attention,Yes.,5,"""their capabilities of in-context learning are limited by the model architecture",2023,2023-07-05T23:26:01Z,"Keyphrase: ""Limited in-context learning""","""their capabilities of in-context learning are limited by the model architecture Keyphrase: ""Limited in-context learning""",1
arXIv2023,Jailbroken: How Does LLM Safety Training Fail?,Yes.,5,"""Large language models trained for safety and harmlessness remain susceptible to adversarial misuse,"" and ""We hypothesize two failure modes of safety training",2023,2023-07-05T17:58:10Z,"Keyphrase: ""Susceptibility to adversarial misuse""","""Large language models trained for safety and harmlessness remain susceptible to adversarial misuse,"" and ""We hypothesize two failure modes of safety training Keyphrase: ""Susceptibility to adversarial misuse""",2
arXIv2023,Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks,Yes.,5,"""we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow",2023,2023-07-05T17:50:42Z,"Keyphrase: ""Limited abstract task-solving skills""","""we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow Keyphrase: ""Limited abstract task-solving skills""",7
arXIv2023,External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback,Yes.,5,"""they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases.""",2023,2023-07-05T17:05:32Z,"Keyphrase: ""Limited contextual knowledge processing""","""they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases."" Keyphrase: ""Limited contextual knowledge processing""",1
arXIv2023,ProPILE: Probing Privacy Leakage in Large Language Models,Yes.,4,"""The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII).""",2023,2023-07-04T18:53:47Z,"Keyphrase: ""Privacy concerns""","""The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII)."" Keyphrase: ""Privacy concerns""",8
arXIv2023,Learning to Prompt in the Classroom to Understand AI Limits: A pilot study,Yes.,5,"""ignoring their limitations such as hallucinations and reasoning constraints"" and ""better grasp of limitations, specifically unreliability, limited understanding of commands leading to unsatisfactory responses, and limited presentation flexibility.""",2023,2023-07-04T07:51:37Z,"Keyphrase: ""Limited reasoning and understanding""","""ignoring their limitations such as hallucinations and reasoning constraints"" and ""better grasp of limitations, specifically unreliability, limited understanding of commands leading to unsatisfactory responses, and limited presentation flexibility."" Keyphrase: ""Limited reasoning and understanding""",0
arXIv2023,Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models,Yes.,5,"""a persistent challenge lies in their susceptibility to 'hallucinations', which erodes trust in their outputs"" and ""existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities.""",2023,2023-07-03T22:17:16Z,"Keyphrase: ""Susceptibility to hallucination""","""a persistent challenge lies in their susceptibility to 'hallucinations', which erodes trust in their outputs"" and ""existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities."" Keyphrase: ""Susceptibility to hallucination""",0
arXIv2023,Multilingual Language Models are not Multicultural: A Case Study in Emotion,Yes.,5,"""Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.""",2023,2023-07-03T21:54:28Z,"Keyphrase: ""Limited cultural nuance understanding""","""Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this."" Keyphrase: ""Limited cultural nuance understanding""",3
arXIv2023,Challenges in Domain-Specific Abstractive Summarization and How to Overcome them,Yes.,5,"""This paper identifies three of those limitations as research problems in the context of abstractive text summarization",2023,2023-07-03T12:26:44Z,"Keyphrase: ""Limitations in abstractive text summarization""","""This paper identifies three of those limitations as research problems in the context of abstractive text summarization Keyphrase: ""Limitations in abstractive text summarization""",1
arxiv2024,The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models,Yes.,5,"""hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications"" and ""To tackle the LLM hallucination, three key questions should be well studied",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination of factually incorrect content""","""hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications"" and ""To tackle the LLM hallucination, three key questions should be well studied Keyphrase: ""Hallucination of factually incorrect content""",0
arxiv2024,LLM on FHIR -- Demystifying Health Records,Yes.,4,"""However, challenges included variability in LLM responses and the need for precise filtering of health data."" and ""While promising, the implementation and pilot also highlight risks such as inconsistent responses and the importance of replicable output.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent responses""","""However, challenges included variability in LLM responses and the need for precise filtering of health data."" and ""While promising, the implementation and pilot also highlight risks such as inconsistent responses and the importance of replicable output."" Keyphrase: ""Inconsistent responses""",0
arxiv2024,"The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey",Yes.,5,"""However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitation in context length extrapolation""","""However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation."" Keyphrase: ""Limitation in context length extrapolation""",4
arxiv2024,LLMs for Relational Reasoning: How Far are We?,Yes.,5,"""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."" and ""Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Poor reasoning ability""","""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."" and ""Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and Keyphrase: ""Poor reasoning ability""",1
arxiv2024,Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning,Yes.,4,"""Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content."" and ""prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100%",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to jailbreaking attacks""","""Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content."" and ""prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100% Keyphrase: ""Vulnerability to jailbreaking attacks""",2
arxiv2024,FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference,Yes.,5,"""The large number of parameters in Pretrained Language Models enhance their performance, but also make them resource-intensive, making it challenging to deploy them on commodity hardware like a single GPU."" and ""Due to the memory and power limitations of these devices, model compression techniques are often used to decrease both the model's size and its inference latency. This usually results in a trade-off between model accuracy",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Resource-intensive deployment""","""The large number of parameters in Pretrained Language Models enhance their performance, but also make them resource-intensive, making it challenging to deploy them on commodity hardware like a single GPU."" and ""Due to the memory and power limitations of these devices, model compression techniques are often used to decrease both the model's size and its inference latency. This usually results in a trade-off between model accuracy Keyphrase: ""Resource-intensive deployment""",4
arxiv2024,LLMs for Test Input Generation for Semantic Caches,Yes.,4,"""However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience."" and ""Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational cost and error-prone semantic caching""","""However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience."" and ""Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system."" Keyphrase: ""High computational cost and error-prone semantic caching""",4
arxiv2024,Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling,Yes.,5,"""This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational and data requirements""","""This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web."" Keyphrase: ""High computational and data requirements""",4
arxiv2024,Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning,Yes.,5,"""Despite being widely applied, in-context learning is vulnerable to malicious attacks."" and ""Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to malicious attacks""","""Despite being widely applied, in-context learning is vulnerable to malicious attacks."" and ""Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model."" Keyphrase: ""Vulnerability to malicious attacks""",2
arxiv2024,Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately,Yes.,5,"""Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Suboptimal quality answers""","""Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions."" Keyphrase: ""Suboptimal quality answers""",0
arxiv2024,Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering,Yes.,5,"""Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Generation of toxic responses""","""Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community."" Keyphrase: ""Generation of toxic responses""",2
arxiv2024,CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities,Yes.,5,"""Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems."" and ""Using this corpus, we find that models often arrive at the correct final answer",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited mathematical reasoning ability""","""Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems."" and ""Using this corpus, we find that models often arrive at the correct final answer Keyphrase: ""Limited mathematical reasoning ability""",1
arxiv2024,A Computational Framework for Behavioral Assessment of LLM Therapists,Yes.,5,"""Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences."" and ""Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent with high-quality care""","""Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences."" and ""Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care Keyphrase: ""Inconsistent with high-quality care""",2
arxiv2024,E^2-LLM: Efficient and Extreme Length Extension of Large Language Models,Yes.,5,"""Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""High computational cost""","""Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources."" Keyphrase: ""High computational cost""",4
arxiv2024,LoMA: Lossless Compressed Memory Attention,Yes.,5,"""Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Resource-intensive limitations""","""Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts."" Keyphrase: ""Resource-intensive limitations""",4
arxiv2024,Can AI Assistants Know What They Don't Know?,Yes.,5,"""Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual errors in knowledge-intensive tasks""","""Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications."" Keyphrase: ""Factual errors in knowledge-intensive tasks""",1
arxiv2024,CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs,Yes.,5,"""Large Multimodal Models (LMMs) encounter two issues in such scenarios",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Multimodal challenges""","""Large Multimodal Models (LMMs) encounter two issues in such scenarios Keyphrase: ""Multimodal challenges""",4
arxiv2024,ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters,Yes.,5,"""achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited real-time inference efficiency""","""achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context."" Keyphrase: ""Limited real-time inference efficiency""",4
arxiv2024,"Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",Yes.,5,"""In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Error-prone code generation""","""In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs."" Keyphrase: ""Error-prone code generation""",7
arxiv2024,Dynamic Q&A of Clinical Documents with Large Language Models,Yes.,4,"""Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited diversity in medical case evaluation""","""Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain."" Keyphrase: ""Limited diversity in medical case evaluation""",0
arxiv2024,Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks,Yes.,5,"""language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arxiv2024,Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis,Yes.,4,"""However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited generalizability""","""However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount."" Keyphrase: ""Limited generalizability""",0
arxiv2024,How well can large language models explain business processes?,Yes.,5,"""Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited capacity for fulfilling tasks""","""Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason."" Keyphrase: ""Limited capacity for fulfilling tasks""",7
arxiv2024,Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models,Yes.,5,"""a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination in video processing""","""a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level."" Keyphrase: ""Hallucination in video processing""",0
arxiv2024,Large Language Models for Mathematical Reasoning: Progresses and Challenges,Yes.,4,"""an overview of factors and concerns affecting LLMs in solving math"" and ""an elucidation of the persisting challenges within this domain.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Challenges in math problem-solving""","""an overview of factors and concerns affecting LLMs in solving math"" and ""an elucidation of the persisting challenges within this domain."" Keyphrase: ""Challenges in math problem-solving""",1
arxiv2024,Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering,Yes.,5,"""While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Prone to hallucination""","""While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers."" Keyphrase: ""Prone to hallucination""",0
arxiv2024,Seven Failure Points When Engineering a Retrieval Augmented Generation System,Yes.,4,"""RAG systems aim to",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited in scope""","""RAG systems aim to Keyphrase: ""Limited in scope""",6
arxiv2024,LLMs for Robotic Object Disambiguation,Yes.,5,"""Despite multiple query attempts with zero-shot prompt engineering (details can be found in the Appendix), the LLM struggled to inquire about features not explicitly provided in the scene description.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggles with zero-shot prompts""","""Despite multiple query attempts with zero-shot prompt engineering (details can be found in the Appendix), the LLM struggled to inquire about features not explicitly provided in the scene description."" Keyphrase: ""Struggles with zero-shot prompts""",6
arxiv2024,DocFinQA: A Long-Context Financial Reasoning Dataset,Yes.,5,"""DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggles with long documents""","""DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents."" Keyphrase: ""Struggles with long documents""",4
arxiv2024,Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation,Yes.,5,"""existing approaches struggle with hallucinations and overconfident predictions.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination and overconfidence""","""existing approaches struggle with hallucinations and overconfident predictions."" Keyphrase: ""Hallucination and overconfidence""",0
arxiv2024,Are self-explanations from Large Language Models faithful?,Yes.,5,"""convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk"" and ""showing self-explanations should not be trusted in general.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unsupported self-explanations""","""convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk"" and ""showing self-explanations should not be trusted in general."" Keyphrase: ""Unsupported self-explanations""",1
arxiv2024,MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries,Yes.,4,"""However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inadequate for multihop queries""","""However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence."" Keyphrase: ""Inadequate for multihop queries""",1
arxiv2024,JumpCoder: Go Beyond Autoregressive Coder via Online Modification,Yes.,5,"""While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Lack of reversibility""","""While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance."" Keyphrase: ""Lack of reversibility""",7
arxiv2024,LightHouse: A Survey of AGI Hallucination,Yes.,4,"""numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research"" and ""Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models).""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hallucination bottleneck""","""numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research"" and ""Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models)."" Keyphrase: ""Hallucination bottleneck""",0
arxiv2024,Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences,Yes.,5,"""we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Struggle with dynamic information""","""we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors."" Keyphrase: ""Struggle with dynamic information""",0
arxiv2024,Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models,Yes.,5,"""Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance."" and ""This research critically examines these biases and quantifies the effects on a representative list selection task.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inherent cognitive bias""","""Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance."" and ""This research critically examines these biases and quantifies the effects on a representative list selection task."" Keyphrase: ""Inherent cognitive bias""",3
arxiv2024,Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet,Yes.,5,"""We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited problem-solving capabilities""","""We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis."" Keyphrase: ""Limited problem-solving capabilities""",1
arxiv2024,Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do,Yes.,5,"""We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception,"" and ""Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others?""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited empathy capability""","""We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception,"" and ""Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others?"" Keyphrase: ""Limited empathy capability""",1
arxiv2024,Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,Yes.,5,"""Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings,"" and ""MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Systematic visual shortcomings""","""Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings,"" and ""MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations."" Keyphrase: ""Systematic visual shortcomings""",4
arxiv2024,Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,Yes.,5,"""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited reasoning capability""","""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount."" Keyphrase: ""Limited reasoning capability""",1
arxiv2024,Generalist embedding models are better at short-context clinical semantic search than specialized embedding models,Yes.,4,"""Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs."" and ""The highlighted problem of specialized models may be due to the fact that they have not been trained on sufficient data, and in particular on datasets that are not diverse enough to have a reliable global language understanding,",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited robustness in specialized domains""","""Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs."" and ""The highlighted problem of specialized models may be due to the fact that they have not been trained on sufficient data, and in particular on datasets that are not diverse enough to have a reliable global language understanding, Keyphrase: ""Limited robustness in specialized domains""",6
arxiv2024,LongAlign: A Recipe for Long Context Alignment of Large Language Models,Yes.,5,"""Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Difficulty in handling long contexts""","""Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length."" Keyphrase: ""Difficulty in handling long contexts""",4
arxiv2024,Gender Bias in Machine Translation and The Era of Large Language Models,Yes.,4,"""The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Bias in machine translation""","""The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies."" Keyphrase: ""Bias in machine translation""",3
arxiv2024,Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications,Yes.,4,"""The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Vulnerability to injection attacks""","""The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate."" Keyphrase: ""Vulnerability to injection attacks""",2
arxiv2024,Prompting Large Vision-Language Models for Compositional Reasoning,Yes.,5,"""However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset."" and ""this limitation stems from two factors",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited visiolinguistic compositionality""","""However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset."" and ""this limitation stems from two factors Keyphrase: ""Limited visiolinguistic compositionality""",1
arxiv2024,MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models,Yes.,5,"""We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Performance degradation in multiturn setting""","""We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance."" Keyphrase: ""Performance degradation in multiturn setting""",7
arxiv2024,OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models,Yes.,5,"""The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Poor performance on benchmarks""","""The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field."" Keyphrase: ""Poor performance on benchmarks""",7
arxiv2024,Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability,Yes.,5,"""LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unreliable task planning""","""LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps."" Keyphrase: ""Unreliable task planning""",1
arxiv2024,From Prompt Engineering to Prompt Science With Human in the Loop,Yes.,5,"""we need to be concerned about how it may affect that research, its findings, or any future works based on that research,"" and ""they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Lack of replicable generalizable knowledge""","""we need to be concerned about how it may affect that research, its findings, or any future works based on that research,"" and ""they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor."" Keyphrase: ""Lack of replicable generalizable knowledge""",0
arxiv2024,When Large Language Models Meet Vector Databases: A Survey,Yes.,5,"""With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Outdated knowledge and memory issues""","""With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues."" Keyphrase: ""Outdated knowledge and memory issues""",0
arxiv2024,OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models,Yes.,5,"""This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Sequential task performance degradation""","""This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped."" Keyphrase: ""Sequential task performance degradation""",5
arxiv2024,Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches,Yes.,5,"""although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Reliability and privacy concerns""","""although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation."" Keyphrase: ""Reliability and privacy concerns""",8
arxiv2024,InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification,Yes.,5,"""our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Difficulty in identifying information loss""","""our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss."" Keyphrase: ""Difficulty in identifying information loss""",6
arxiv2024,Hallucination Benchmark in Medical Visual Question Answering,Yes.,5,"""these models are not extensively tested on the hallucination phenomenon in clinical settings"" and ""The study provides an in-depth analysis of current models' limitations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited testing for hallucination phenomenon""","""these models are not extensively tested on the hallucination phenomenon in clinical settings"" and ""The study provides an in-depth analysis of current models' limitations."" Keyphrase: ""Limited testing for hallucination phenomenon""",0
arxiv2024,Knowledge Verification to Nip Hallucination in the Bud,Yes.,5,"""they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual contradictions""","""they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination."" Keyphrase: ""Factual contradictions""",0
arxiv2024,Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness,Yes.,5,"""However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective."" and ""Our results shed light on the limitations of the LLM chatbots when compared to specialized models.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited effectiveness in cybersecurity entity recognition""","""However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective."" and ""Our results shed light on the limitations of the LLM chatbots when compared to specialized models."" Keyphrase: ""Limited effectiveness in cybersecurity entity recognition""",6
arxiv2024,Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values,Yes.,4,"""troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Cultural bias and authoritarian tendencies""","""troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies."" Keyphrase: ""Cultural bias and authoritarian tendencies""",3
arxiv2024,Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine,Yes.,5,"""we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Flawed rationale in decision-making""","""we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows."" Keyphrase: ""Flawed rationale in decision-making""",1
arxiv2024,On Prompt-Driven Safeguarding for Large Language Models,Yes.,5,"""We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Safety prompt bias""","""We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless."" Keyphrase: ""Safety prompt bias""",2
arxiv2024,Detection of Machine-Generated Text: Literature Survey,Yes.,4,"""Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes."" and ""To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hazardous societal influence""","""Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes."" and ""To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented."" Keyphrase: ""Hazardous societal influence""",3
arxiv2024,TOFU: A Task of Fictitious Unlearning for LLMs,Yes.,5,"""Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns."" and ""Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Privacy concerns and lack of effective unlearning""","""Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns."" and ""Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."" Keyphrase: ""Privacy concerns and lack of effective unlearning""",8
arxiv2024,Extending LLMs' Context Window with 100 Samples,Yes.,5,"""Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited extrapolation ability""","""Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs."" Keyphrase: ""Limited extrapolation ability""",4
arxiv2024,MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline,Yes.,5,"""there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limited mathematical reasoning capability""","""there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints."" Keyphrase: ""Limited mathematical reasoning capability""",1
arxiv2024,"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",Yes.,5,"""the safety and security issues of LLM systems have become the major obstacle to their widespread application"" and ""potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Safety and security concerns""","""the safety and security issues of LLM systems have become the major obstacle to their widespread application"" and ""potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies."" Keyphrase: ""Safety and security concerns""",2
arxiv2024,Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning,Yes.,5,"""Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Inconsistent explanations""","""Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs."" Keyphrase: ""Inconsistent explanations""",1
arxiv2024,Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?,Yes.,5,"""The paper discusses what is needed to address the limitations of current LLM-centered AI systems.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitations in addressing current LLM-centered AI system""","""The paper discusses what is needed to address the limitations of current LLM-centered AI systems."" Keyphrase: ""Limitations in addressing current LLM-centered AI system""",7
arxiv2024,Small Language Model Can Self-correct,Yes.,5,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone"" and ""large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Generating inaccurate false information""","""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone"" and ""large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans."" Keyphrase: ""Generating inaccurate false information""",0
arxiv2024,Conditional and Modal Reasoning in Large Language Models,Yes.,5,"""Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Logical inconsistencies in inference""","""Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals."" Keyphrase: ""Logical inconsistencies in inference""",1
arxiv2024,Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?,Yes.,5,"""The models showed significantly reduced accuracy on tasks with suspected hierarchical bias.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Hierarchical bias affecting accuracy""","""The models showed significantly reduced accuracy on tasks with suspected hierarchical bias."" Keyphrase: ""Hierarchical bias affecting accuracy""",3
arxiv2024,Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring,Yes.,4,"""The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM)."" and ""highlight the potential requirements and limitations of utilizing chatbots in conversational explainability.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Limitation in conversational explainability""","""The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM)."" and ""highlight the potential requirements and limitations of utilizing chatbots in conversational explainability."" Keyphrase: ""Limitation in conversational explainability""",6
arxiv2024,Detecting Multimedia Generated by Large AI Models: A Survey,Yes.,4,"""this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns."" and ""we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Ethical concerns and societal risks""","""this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns."" and ""we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs."" Keyphrase: ""Ethical concerns and societal risks""",2
arxiv2024,SocraSynth: Multi-LLM Reasoning with Conditional Statistics,Yes.,4,"""Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Bias, hallucination, and lack of reasoning""","""Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability."" Keyphrase: ""Bias, hallucination, and lack of reasoning""",0
arxiv2024,Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation,Yes.,5,"""Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Unmeasured representational harm""","""Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated."" Keyphrase: ""Unmeasured representational harm""",3
arxiv2024,The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance,Yes.,5,"""We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Sensitivity to small perturbations""","""We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs."" Keyphrase: ""Sensitivity to small perturbations""",2
arxiv2024,MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance,Yes.,5,"""This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Catastrophic forgetting""","""This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning."" Keyphrase: ""Catastrophic forgetting""",5
arxiv2024,Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review,Yes.,4,"""the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Ethical and privacy challenges""","""the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations."" Keyphrase: ""Ethical and privacy challenges""",8
arxiv2024,Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging,Yes.,5,"""The outcomes reveal a 100% success rate in manipulating PLIP's predictions, underscoring its susceptibility to adversarial perturbations.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Susceptibility to adversarial perturbation""","""The outcomes reveal a 100% success rate in manipulating PLIP's predictions, underscoring its susceptibility to adversarial perturbations."" Keyphrase: ""Susceptibility to adversarial perturbation""",2
arxiv2024,A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models,Yes.,5,"""a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded,"" and ""The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations,"" and ""we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of L",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Factual hallucination""","""a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded,"" and ""The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations,"" and ""we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of L Keyphrase: ""Factual hallucination""",0
arxiv2024,The Reasoning Under Uncertainty Trap: A Structural AI Risk,Yes.,5,"""we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy.""",2024,2024-01-01T00:00:00Z,"Keyphrase: ""Computational explosiveness and deep uncertainty""","""we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy."" Keyphrase: ""Computational explosiveness and deep uncertainty""",7
arxiv2024,Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs,Yes.,5,"""they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications.""",2024,2024-03-30T22:41:05Z,"Keyphrase: ""Vulnerability to poisoned external evidence""","""they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications."" Keyphrase: ""Vulnerability to poisoned external evidence""",2
arxiv2024,Linguistic Calibration of Language Models,Yes.,5,"""Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate.""",2024,2024-03-30T20:47:55Z,"Keyphrase: ""Confident hallucination""","""Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate."" Keyphrase: ""Confident hallucination""",0
arxiv2024,NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,Yes.,5,"""Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation.""",2024,2024-03-30T19:46:59Z,"Keyphrase: ""Difficulty with numerical data""","""Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation."" Keyphrase: ""Difficulty with numerical data""",1
arxiv2024,Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,Yes.,5,"""Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving.""",2024,2024-03-30T12:48:31Z,"Keyphrase: ""Limited mathematical problem-solving capabilities""","""Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving."" Keyphrase: ""Limited mathematical problem-solving capabilities""",1
arxiv2024,Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,Yes.,4,"""adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date.""",2024,2024-03-30T01:56:07Z,"Keyphrase: ""Limited ability to incorporate out-of-domain knowledge""","""adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date."" Keyphrase: ""Limited ability to incorporate out-of-domain knowledge""",1
arxiv2024,Conceptual and Unbiased Reasoning in Language Models,Yes.,5,"""limited study has been done on large language models' capability to perform conceptual reasoning"" and ""existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods.""",2024,2024-03-30T00:53:53Z,"Keyphrase: ""Limited conceptual reasoning""","""limited study has been done on large language models' capability to perform conceptual reasoning"" and ""existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods."" Keyphrase: ""Limited conceptual reasoning""",1
arxiv2024,"Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value",Yes.,5,"""the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations."" and ""This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation."" and ""underscore the need for a more nuanced",2024,2024-03-29T22:49:43Z,"Keyphrase: ""Uncertain validity in human subject simulation""","""the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations."" and ""This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation."" and ""underscore the need for a more nuanced Keyphrase: ""Uncertain validity in human subject simulation""",0
arxiv2024,Uncovering Bias in Large Vision-Language Models with Counterfactuals,Yes.,4,"""While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs.""",2024,2024-03-29T21:45:53Z,"Keyphrase: ""Limited exploration of social bias""","""While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs."" Keyphrase: ""Limited exploration of social bias""",3
arxiv2024,Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models,Yes.,5,"""extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements.""",2024,2024-03-29T17:59:53Z,"Keyphrase: ""Performance struggles on benchmarks""","""extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements."" Keyphrase: ""Performance struggles on benchmarks""",6
arxiv2024,Are We on the Right Way for Evaluating Large Vision-Language Models?,Yes.,5,"""we dig into current evaluation works and identify two primary issues",2024,2024-03-29T17:59:34Z,"Keyphrase: ""Limited evaluation""","""we dig into current evaluation works and identify two primary issues Keyphrase: ""Limited evaluation""",0
arxiv2024,LUQ: Long-text Uncertainty Quantification for LLMs,Yes.,5,"""Our study first highlights the limitations of current UQ methods in handling long text generation."" and ""We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about.""",2024,2024-03-29T16:49:24Z,"Keyphrase: ""Lack of confidence in generating long text""","""Our study first highlights the limitations of current UQ methods in handling long text generation."" and ""We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about."" Keyphrase: ""Lack of confidence in generating long text""",4
arxiv2024,IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context,Yes.,4,"""The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs)."" and ""We observed that the language models exhibit more bias across a majority of the intersectional groups.""",2024,2024-03-29T12:32:06Z,"Keyphrase: ""Biased language model performance""","""The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs)."" and ""We observed that the language models exhibit more bias across a majority of the intersectional groups."" Keyphrase: ""Biased language model performance""",3
arxiv2024,ITCMA: A Generative Agent Based on a Computational Consciousness Structure,Yes.,4,"""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior.""",2024,2024-03-29T10:23:18Z,"Keyphrase: ""Difficulty with implicit instructions and commonsense knowledge""","""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior."" Keyphrase: ""Difficulty with implicit instructions and commonsense knowledge""",1
arxiv2024,Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning,Yes.,5,"""We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome.""",2024,2024-03-29T08:30:34Z,"Keyphrase: ""Limited error analysis""","""We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome."" Keyphrase: ""Limited error analysis""",1
arxiv2024,On Large Language Models' Hallucination with Regard to Known Facts,Yes.,5,"""Large language models are successful in answering factoid questions but are also prone to hallucination."" and ""Our study shed light on understanding the reasons for LLMs' hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.""",2024,2024-03-29T06:48:30Z,"Keyphrase: ""Hallucination tendency""","""Large language models are successful in answering factoid questions but are also prone to hallucination."" and ""Our study shed light on understanding the reasons for LLMs' hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating."" Keyphrase: ""Hallucination tendency""",0
arxiv2024,Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning,Yes.,4,"""However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4."" and ""As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance.""",2024,2024-03-29T03:48:12Z,"Keyphrase: ""Limited real-world performance""","""However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4."" and ""As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance."" Keyphrase: ""Limited real-world performance""",1
arxiv2024,MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models,Yes.,5,"""Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them.""",2024,2024-03-29T01:53:24Z,"Keyphrase: ""Poor performance on challenging questions""","""Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them."" Keyphrase: ""Poor performance on challenging questions""",6
arxiv2024,"Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",Yes.,5,"""current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary.""",2024,2024-03-28T21:18:33Z,"Keyphrase: ""Unsuitable for real-time applications""","""current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary."" Keyphrase: ""Unsuitable for real-time applications""",4
arxiv2024,Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors,Yes.,5,"""However, we argue that a critical obstacle remains in deploying LLMs for practical use",2024,2024-03-28T12:05:15Z,"Keyphrase: ""Obstacle in practical deployment""","""However, we argue that a critical obstacle remains in deploying LLMs for practical use Keyphrase: ""Obstacle in practical deployment""",4
arxiv2024,Large Language Models Are Unconscious of Unreasonability in Math Problems,Yes.,5,"""Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors."" and ""Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content.""",2024,2024-03-28T12:04:28Z,"Keyphrase: ""Hallucinatory responses""","""Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors."" and ""Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content."" Keyphrase: ""Hallucinatory responses""",0
arxiv2024,Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models,Yes.,4,"""However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images.""",2024,2024-03-28T11:26:30Z,"Keyphrase: ""Limited image detail capture""","""However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images."" Keyphrase: ""Limited image detail capture""",4
arxiv2024,MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation,Yes.,5,"""the quality of the text generated by these models often reveals persistent issues"" and ""it is filled with significant uncertainty and instability"" and ""addressing the uncertainties and instabilities in evaluating LLMs-generated text.""",2024,2024-03-28T10:41:47Z,"Keyphrase: ""Uncertainty and instability""","""the quality of the text generated by these models often reveals persistent issues"" and ""it is filled with significant uncertainty and instability"" and ""addressing the uncertainties and instabilities in evaluating LLMs-generated text."" Keyphrase: ""Uncertainty and instability""",1
arxiv2024,Fine-Tuning Language Models with Reward Learning on Policy,Yes.,4,"""Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.""",2024,2024-03-28T10:02:10Z,"Keyphrase: ""Difficulty in adapting to changing data distribution""","""Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize."" Keyphrase: ""Difficulty in adapting to changing data distribution""",5
arxiv2024,"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",Yes.,5,"""prior benchmarks contain only a very limited set of problems, both in quantity and variety,"" ""many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data,"" ""there is a significant drop in performance (on average 39.4%) when using EvoEval,"" ""the brittleness of instruction-following models when encountering reword",2024,2024-03-28T03:10:39Z,"Keyphrase: ""Data leakage and brittleness""","""prior benchmarks contain only a very limited set of problems, both in quantity and variety,"" ""many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data,"" ""there is a significant drop in performance (on average 39.4%) when using EvoEval,"" ""the brittleness of instruction-following models when encountering reword Keyphrase: ""Data leakage and brittleness""",7
arxiv2024,FACTOID: FACtual enTailment fOr hallucInation Detection,Yes.,5,"""However, hallucination is a significant concern."" and ""current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted.""",2024,2024-03-28T03:09:42Z,"Keyphrase: ""Inaccurate annotation""","""However, hallucination is a significant concern."" and ""current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted."" Keyphrase: ""Inaccurate annotation""",0
arxiv2024,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Yes.,5,"""Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content."" and ""Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address.""",2024,2024-03-28T02:44:02Z,"Keyphrase: ""Generation of harmful content""","""Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content."" and ""Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address."" Keyphrase: ""Generation of harmful content""",2
arxiv2024,Learning From Correctness Without Prompting Makes LLM Efficient Reasoner,Yes.,5,"""Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content.""",2024,2024-03-28T02:12:49Z,"Keyphrase: ""Hallucination and toxic content""","""Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content."" Keyphrase: ""Hallucination and toxic content""",0
arxiv2024,"""Sorry, Come Again?"" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing",Yes.,5,"""Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs)."" and ""Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans."" and ""Recent studies reveal that an LLM often neglects the middle sections of extended prompts, a phenomenon termed as lost in the middle.""",2024,2024-03-27T19:45:09Z,"Keyphrase: ""Neglect of middle section""","""Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs)."" and ""Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans."" and ""Recent studies reveal that an LLM often neglects the middle sections of extended prompts, a phenomenon termed as lost in the middle."" Keyphrase: ""Neglect of middle section""",0
arxiv2024,Measuring Political Bias in Large Language Models: What Is Said and How It Is Said,Yes.,4,"""However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications.""",2024,2024-03-27T18:22:48Z,"Keyphrase: ""Political bias and polarization""","""However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications."" Keyphrase: ""Political bias and polarization""",3
arxiv2024,Projective Methods for Mitigating Gender Bias in Pre-trained Language Models,Yes.,4,"""We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing",2024,2024-03-27T17:49:31Z,"Keyphrase: ""Limited correlation between intrinsic bias and downstream bias mitigation""","""We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing Keyphrase: ""Limited correlation between intrinsic bias and downstream bias mitigation""",3
arxiv2024,Long-form factuality in large language models,Yes.,5,"""Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics.""",2024,2024-03-27T17:48:55Z,"Keyphrase: ""Factual errors in generated content""","""Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics."" Keyphrase: ""Factual errors in generated content""",0
arxiv2024,Vulnerability Detection with Code Language Models: How Far Are We?,Yes.,5,"""Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models."" and ""Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings.""",2024,2024-03-27T14:34:29Z,"Keyphrase: ""Overestimated performance""","""Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models."" and ""Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings."" Keyphrase: ""Overestimated performance""",6
arxiv2024,BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text,Yes.,5,"""However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources.""",2024,2024-03-27T10:18:21Z,"Keyphrase: ""High computational cost and data privacy concerns""","""However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources."" Keyphrase: ""High computational cost and data privacy concerns""",4
arxiv2024,Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback,Yes.,5,"""Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope."" and ""These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby",2024,2024-03-27T08:39:56Z,"Keyphrase: ""Limited ability to discern and reject questions beyond knowledge scope""","""Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope."" and ""These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby Keyphrase: ""Limited ability to discern and reject questions beyond knowledge scope""",0
arxiv2024,Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective,Yes.,5,"""MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks.""",2024,2024-03-27T08:38:49Z,"Keyphrase: ""Unimodal bias""","""MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks."" Keyphrase: ""Unimodal bias""",1
arxiv2024,Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,Yes.,4,"""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions.""",2024,2024-03-27T06:43:58Z,"Keyphrase: ""Inaccuracy in answer prediction""","""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions."" Keyphrase: ""Inaccuracy in answer prediction""",5
arxiv2024,Exploring the Privacy Protection Capabilities of Chinese Large Language Models,Yes.,5,"""Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models.""",2024,2024-03-27T02:31:54Z,"Keyphrase: ""Privacy protection shortcomings""","""Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models."" Keyphrase: ""Privacy protection shortcomings""",8
arxiv2024,Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization,Yes.,5,"""However, they still make unjustified logical and computational errors in their reasoning steps and answers.""",2024,2024-03-26T22:01:13Z,"Keyphrase: ""Unjustified computational errors""","""However, they still make unjustified logical and computational errors in their reasoning steps and answers."" Keyphrase: ""Unjustified computational errors""",1
arxiv2024,MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution,Yes.,4,"""Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level."" and ""To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors.""",2024,2024-03-26T17:57:57Z,"Keyphrase: ""Difficulty in code change resolution""","""Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level."" and ""To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors."" Keyphrase: ""Difficulty in code change resolution""",7
arxiv2024,The Unreasonable Ineffectiveness of the Deeper Layers,Yes.,5,"""the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.""",2024,2024-03-26T17:20:04Z,"Keyphrase: ""Inefficient knowledge utilization""","""the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge."" Keyphrase: ""Inefficient knowledge utilization""",5
arxiv2024,Can multiple-choice questions really be useful in detecting the abilities of LLMs?,Yes.,5,"""There are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required."" and ""LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position."" and ""Our results reveal a relatively low correlation between answers from MCQs and L",2024,2024-03-26T14:43:48Z,"Keyphrase: ""Order sensitivity in knowledge-intensive scenarios""","""There are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required."" and ""LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position."" and ""Our results reveal a relatively low correlation between answers from MCQs and L Keyphrase: ""Order sensitivity in knowledge-intensive scenarios""",3
arxiv2024,Optimization-based Prompt Injection Attack to LLM-as-a-Judge,Yes.,4,"""However, the robustness of these systems against prompt injection attacks remains an open question."" and ""highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.""",2024,2024-03-26T13:58:00Z,"Keyphrase: ""Vulnerability to prompt injection attacks""","""However, the robustness of these systems against prompt injection attacks remains an open question."" and ""highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack."" Keyphrase: ""Vulnerability to prompt injection attacks""",2
arxiv2024,RuBia: A Russian Language Bias Detection Dataset,Yes.,4,"""Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data"" and ""we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases.""",2024,2024-03-26T10:01:01Z,"Keyphrase: ""Social and cultural bias in training data""","""Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data"" and ""we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases."" Keyphrase: ""Social and cultural bias in training data""",3
arxiv2024,Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models,Yes.,4,"""there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers"" and ""jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited.""",2024,2024-03-26T02:47:42Z,"Keyphrase: ""Security circumvention risks""","""there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers"" and ""jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited."" Keyphrase: ""Security circumvention risks""",2
arxiv2024,"Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",Yes.,5,"""In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs).""",2024,2024-03-26T01:28:42Z,"Keyphrase: ""Hallucination detection and mitigation""","""In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs)."" Keyphrase: ""Hallucination detection and mitigation""",0
arxiv2024,A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection,Yes.,5,"""LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that",2024,2024-03-25T21:47:36Z,"Keyphrase: ""Struggles in vulnerability detection""","""LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that Keyphrase: ""Struggles in vulnerability detection""",1
arxiv2024,Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model,Yes.,5,"""Unexpectedly, the RBS outperformed the LLMs across all metrics. Intensive review demonstrates that this finding is due to the divergent approach taken by the RBS and LLM.""",2024,2024-03-25T21:19:50Z,"Keyphrase: ""Divergent performance from traditional LLMs""","""Unexpectedly, the RBS outperformed the LLMs across all metrics. Intensive review demonstrates that this finding is due to the divergent approach taken by the RBS and LLM."" Keyphrase: ""Divergent performance from traditional LLMs""",7
arxiv2024,The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition,Yes.,5,"""However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition,"" and ""We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that",2024,2024-03-25T19:07:32Z,"Keyphrase: ""Limited integration of information""","""However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition,"" and ""We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that Keyphrase: ""Limited integration of information""",1
arxiv2024,Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators,Yes.,5,"""LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments.""",2024,2024-03-25T17:11:28Z,"Keyphrase: ""Biased evaluation and lack of coherence""","""LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments."" Keyphrase: ""Biased evaluation and lack of coherence""",0
arxiv2024,Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback,Yes.,5,"""As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context.""",2024,2024-03-25T14:07:27Z,"Keyphrase: ""Limited project-specific context understanding""","""As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context."" Keyphrase: ""Limited project-specific context understanding""",4
arxiv2024,"All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification",Yes.,4,"""It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code."" and ""Our study reveals that approximately 60% of the hardware designs generated by LLMs are prone to CWEs, posing potential safety and security risks.""",2024,2024-03-25T13:23:24Z,"Keyphrase: ""Prone to security vulnerabilities""","""It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code."" and ""Our study reveals that approximately 60% of the hardware designs generated by LLMs are prone to CWEs, posing potential safety and security risks."" Keyphrase: ""Prone to security vulnerabilities""",2
arxiv2024,Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography,Yes.,5,"""students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test"" and ""ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students' knowledge application and creativity were insignificant.""",2024,2024-03-25T12:23:12Z,"Keyphrase: ""Limited knowledge application and creativity""","""students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test"" and ""ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students' knowledge application and creativity were insignificant."" Keyphrase: ""Limited knowledge application and creativity""",6
arxiv2024,Elysium: Exploring Object-level Perception in Videos via MLLM,Yes.,5,"""extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships"" and ""processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden.""",2024,2024-03-25T09:17:15Z,"Keyphrase: ""High computational burden""","""extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships"" and ""processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden."" Keyphrase: ""High computational burden""",4
arxiv2024,Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art,Yes.,4,"""foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor."" and ""discuss existing approaches to hallucination detection and mitigation with a focus on decision problems.""",2024,2024-03-25T08:11:02Z,"Keyphrase: ""Hallucination in decision-making""","""foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor."" and ""discuss existing approaches to hallucination detection and mitigation with a focus on decision problems."" Keyphrase: ""Hallucination in decision-making""",0
arxiv2024,Evaluating Large Language Models with Runtime Behavior of Program Execution,Yes.,5,"""most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3).""",2024,2024-03-25T05:37:16Z,"Keyphrase: ""Poor runtime reasoning""","""most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3)."" Keyphrase: ""Poor runtime reasoning""",7
arxiv2024,How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation,Yes.,5,"""While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation",2024,2024-03-25T04:21:06Z,"Keyphrase: ""Limitation in user simulation""","""While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation Keyphrase: ""Limitation in user simulation""",0
arxiv2024,A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish,Yes.,4,"""This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance.""",2024,2024-03-24T13:21:58Z,"Keyphrase: ""Risk of data leakage""","""This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance."" Keyphrase: ""Risk of data leakage""",8
arxiv2024,TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions,Yes.,5,"""little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones.""",2024,2024-03-23T16:12:52Z,"Keyphrase: ""Limited handling of diverse question types""","""little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones."" Keyphrase: ""Limited handling of diverse question types""",6
arxiv2024,The Frontier of Data Erasure: Machine Unlearning for Large Language Models,Yes.,5,"""Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets.""",2024,2024-03-23T09:26:15Z,"Keyphrase: ""Risk of memorizing sensitive and biased information""","""Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets."" Keyphrase: ""Risk of memorizing sensitive and biased information""",8
arxiv2024,Long-CLIP: Unlocking the Long-Text Capability of CLIP,Yes.,5,"""a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites.""",2024,2024-03-22T17:58:16Z,"Keyphrase: ""Limited text input length""","""a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites."" Keyphrase: ""Limited text input length""",4
arxiv2024,Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs,Yes.,5,"""suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs.""",2024,2024-03-22T17:27:18Z,"Keyphrase: ""Limited real-world diagnostic readiness""","""suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs."" Keyphrase: ""Limited real-world diagnostic readiness""",1
arxiv2024,CoLLEGe: Concept Embedding Generation for Large Language Models,Yes.,4,"""Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts.""",2024,2024-03-22T17:26:05Z,"Keyphrase: ""Slow adaptation to new concepts""","""Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts."" Keyphrase: ""Slow adaptation to new concepts""",5
arxiv2024,Sphere Neural-Networks for Rational Reasoning,Yes.,5,"""This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination.""",2024,2024-03-22T15:44:59Z,"Keyphrase: ""Limitation in rational reasoning""","""This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination."" Keyphrase: ""Limitation in rational reasoning""",1
arxiv2024,Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study,Yes.,5,"""knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages,"" and ""CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions.""",2024,2024-03-22T13:13:13Z,"Keyphrase: ""Poor transfer to functional programming languages""","""knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages,"" and ""CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions."" Keyphrase: ""Poor transfer to functional programming languages""",7
arxiv2024,Risk and Response in Large Language Models: Evaluating Key Threat Categories,Yes.,5,"""Our findings indicate that LLMs tend to consider Information Hazards less harmful,"" and ""The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in LLM risk assessment and emphasizing the need for improved AI safety measures.""",2024,2024-03-22T06:46:40Z,"Keyphrase: ""Information hazards and security vulnerabilities""","""Our findings indicate that LLMs tend to consider Information Hazards less harmful,"" and ""The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in LLM risk assessment and emphasizing the need for improved AI safety measures."" Keyphrase: ""Information hazards and security vulnerabilities""",2
arxiv2024,Language Repository for Long Video Understanding,Yes.,5,"""Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length.""",2024,2024-03-21T17:59:35Z,"Keyphrase: ""Declining effectiveness with long context lengths""","""Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length."" Keyphrase: ""Declining effectiveness with long context lengths""",4
arxiv2024,RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain,Yes.,4,"""yet their reliability in realistic use cases is under-researched"" and ""We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case.""",2024,2024-03-21T17:30:59Z,"Keyphrase: ""Reliability in realistic use cases""","""yet their reliability in realistic use cases is under-researched"" and ""We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case."" Keyphrase: ""Reliability in realistic use cases""",0
arxiv2024,Open Source Conversational LLMs do not know most Spanish words,Yes.,4,"""The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.""",2024,2024-03-21T15:41:02Z,"Keyphrase: ""Language bias and performance discrepancy""","""The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages."" Keyphrase: ""Language bias and performance discrepancy""",6
arxiv2024,Locating and Mitigating Gender Bias in Large Language Models,Yes.,4,"""this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society"" and ""we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns.""",2024,2024-03-21T13:57:43Z,"Keyphrase: ""Acquiring societal bias""","""this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society"" and ""we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns."" Keyphrase: ""Acquiring societal bias""",3
arxiv2024,Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination,Yes.,5,"""However, they suffer from visual hallucination, where the generated responses diverge from the provided image.""",2024,2024-03-21T13:49:42Z,"Keyphrase: ""Visual hallucination""","""However, they suffer from visual hallucination, where the generated responses diverge from the provided image."" Keyphrase: ""Visual hallucination""",0
arxiv2024,"WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",Yes.,5,"""The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are 'unknown' to them.""",2024,2024-03-21T12:45:12Z,"Keyphrase: ""Decay of factuality over time""","""The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are 'unknown' to them."" Keyphrase: ""Decay of factuality over time""",0
arxiv2024,Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection,Yes.,4,"""Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models.""",2024,2024-03-21T08:57:27Z,"Keyphrase: ""Superficial alignment with human preference""","""Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models."" Keyphrase: ""Superficial alignment with human preference""",0
arxiv2024,Improving the Robustness of Large Language Models via Consistency Alignment,Yes.,5,"""their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions.""",2024,2024-03-21T08:21:12Z,"Keyphrase: ""Inconsistent responses""","""their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions."" Keyphrase: ""Inconsistent responses""",6
arxiv2024,AI and Memory Wall,Yes.,5,"""the main performance bottleneck is increasingly shifting to memory bandwidth"" and ""we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models.""",2024,2024-03-21T04:31:59Z,"""Memory bandwidth bottleneck""","""the main performance bottleneck is increasingly shifting to memory bandwidth"" and ""we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models."" ""Memory bandwidth bottleneck""",4
arxiv2024,Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,Yes.,4,"""We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability,"" and ""We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.""",2024,2024-03-21T03:52:01Z,"Keyphrase: ""Struggles with reasoning and memorization""","""We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability,"" and ""We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors."" Keyphrase: ""Struggles with reasoning and memorization""",1
arxiv2024,Protected group bias and stereotypes in Large Language Models,Yes.,5,"""We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations."" and ""The model not only reflects societal biases, but appears to amplify them."" and ""This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should",2024,2024-03-21T00:21:38Z,"Keyphrase: ""Amplification of societal bias""","""We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations."" and ""The model not only reflects societal biases, but appears to amplify them."" and ""This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should Keyphrase: ""Amplification of societal bias""",3
arxiv2024,Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification,Yes.,4,"""Despite the growing capabilities of large language models, there exists concerns about the biases they develop."" and ""bias occurs due to both intrinsic model architecture and dataset.""",2024,2024-03-20T18:59:18Z,"Keyphrase: ""Bias development concerns""","""Despite the growing capabilities of large language models, there exists concerns about the biases they develop."" and ""bias occurs due to both intrinsic model architecture and dataset."" Keyphrase: ""Bias development concerns""",3
arxiv2024,Defending Against Indirect Prompt Injection Attacks With Spotlighting,Yes.,5,"""Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources.""",2024,2024-03-20T15:26:23Z,"Keyphrase: ""Difficulty in distinguishing input sources""","""Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources."" Keyphrase: ""Difficulty in distinguishing input sources""",4
arxiv2024,CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing,Yes.,4,"""Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents.""",2024,2024-03-20T13:33:55Z,"Keyphrase: ""Challenges in generating complex code""","""Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents."" Keyphrase: ""Challenges in generating complex code""",7
arxiv2024,LeanReasoner: Boosting Complex Logical Reasoning with Lean,Yes.,4,"""Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning.""",2024,2024-03-20T05:29:06Z,"Keyphrase: ""Struggles with logical reasoning""","""Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning."" Keyphrase: ""Struggles with logical reasoning""",1
arxiv2024,Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal,Yes.,4,"""However, this technological advancement is accompanied by significant risks and vulnerabilities. Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of LLMs.""",2024,2024-03-20T05:17:22Z,"Keyphrase: ""Persistent security vulnerabilities""","""However, this technological advancement is accompanied by significant risks and vulnerabilities. Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of LLMs."" Keyphrase: ""Persistent security vulnerabilities""",2
arxiv2024,From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards,Yes.,5,"""However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations."" and ""multiple concerns regarding the safety and ingrained biases in these models remain."" and ""a clear trade-off between the helpfulness and safety of these models has been documented in the literature.""",2024,2024-03-20T00:22:38Z,"Keyphrase: ""Safety risks and ingrained biases""","""However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations."" and ""multiple concerns regarding the safety and ingrained biases in these models remain."" and ""a clear trade-off between the helpfulness and safety of these models has been documented in the literature."" Keyphrase: ""Safety risks and ingrained biases""",2
arxiv2024,Dated Data: Tracing Knowledge Cutoffs in Large Language Models,Yes.,5,"""To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies",2024,2024-03-19T17:57:58Z,"Keyphrase: ""Inconsistency in pretraining datasets""","""To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies Keyphrase: ""Inconsistency in pretraining datasets""",5
arxiv2024,MELTing point: Mobile Evaluation of Language Transformers,Yes.,5,"""Their runtime requirements have prevented them from being broadly deployed on mobile,"" ""LLM inference is largely memory-bound,"" ""Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost,"" ""the continuous execution of LLMs remains elusive, as both factors negatively affect user experience,"" and ""the ecosystem is still in its infancy, and algorithmic",2024,2024-03-19T15:51:21Z,"Keyphrase: ""Memory-bound execution""","""Their runtime requirements have prevented them from being broadly deployed on mobile,"" ""LLM inference is largely memory-bound,"" ""Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost,"" ""the continuous execution of LLMs remains elusive, as both factors negatively affect user experience,"" and ""the ecosystem is still in its infancy, and algorithmic Keyphrase: ""Memory-bound execution""",4
arxiv2024,AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework,Yes.,5,"""LLMs still suffer from hallucinations and are unable to keep up with the latest information.""",2024,2024-03-19T09:45:33Z,"Keyphrase: ""Inability to maintain latest information""","""LLMs still suffer from hallucinations and are unable to keep up with the latest information."" Keyphrase: ""Inability to maintain latest information""",0
arxiv2024,RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content,Yes.,4,"""the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges.""",2024,2024-03-19T07:25:02Z,"Keyphrase: ""Bias in generating harmful content""","""the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges."" Keyphrase: ""Bias in generating harmful content""",2
arxiv2024,"Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",Yes.,4,"""These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities."" and ""mitigation strategies to address these challenges while identifying limitations of current strategies.""",2024,2024-03-19T07:10:58Z,"Keyphrase: ""Limited vulnerability mitigation""","""These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities."" and ""mitigation strategies to address these challenges while identifying limitations of current strategies."" Keyphrase: ""Limited vulnerability mitigation""",2
arxiv2024,Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering,Yes.,4,"""LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance.""",2024,2024-03-19T03:00:03Z,"Keyphrase: ""Off-topic answers""","""LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance."" Keyphrase: ""Off-topic answers""",0
arxiv2024,"OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety",Yes.,4,"""many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues"" and ""Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety.""",2024,2024-03-18T23:21:37Z,"Keyphrase: ""Overlooking alignment safety""","""many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues"" and ""Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety."" Keyphrase: ""Overlooking alignment safety""",2
arxiv2024,Zero-Shot Multi-task Hallucination Detection,Yes.,4,"""This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria.""",2024,2024-03-18T20:50:26Z,"Keyphrase: ""Lack of faithfulness to the source""","""This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria."" Keyphrase: ""Lack of faithfulness to the source""",6
arxiv2024,EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models,Yes.,5,"""Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs)."" and ""Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks.""",2024,2024-03-18T18:39:53Z,"Keyphrase: ""Security vulnerabilities""","""Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs)."" and ""Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks."" Keyphrase: ""Security vulnerabilities""",2
arxiv2024,Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets,Yes.,4,"""Recent work has shown that false assumptions in naturally occurring questions pose challenges to current models, with low performance on both generative QA and simple detection tasks"" and ""Our findings from evaluating a range of large language models are threefold",2024,2024-03-18T18:01:26Z,"Keyphrase: ""Low performance in generative QA""","""Recent work has shown that false assumptions in naturally occurring questions pose challenges to current models, with low performance on both generative QA and simple detection tasks"" and ""Our findings from evaluating a range of large language models are threefold Keyphrase: ""Low performance in generative QA""",1
arxiv2024,NovelQA: A Benchmark for Long-Range Novel Question Answering,Yes.,5,"""Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with more than 100,000 tokens.""",2024,2024-03-18T17:32:32Z,"Keyphrase: ""Challenges with multihop reasoning""","""Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with more than 100,000 tokens."" Keyphrase: ""Challenges with multihop reasoning""",4
arxiv2024,Investigating Markers and Drivers of Gender Bias in Machine Translations,Yes.,4,"""Implicit gender bias in Large Language Models (LLMs) is a well-documented problem,"" and ""These results show that the back-translation method can provide further insights into bias in language models.""",2024,2024-03-18T15:54:46Z,"Keyphrase: ""Implicit gender bias""","""Implicit gender bias in Large Language Models (LLMs) is a well-documented problem,"" and ""These results show that the back-translation method can provide further insights into bias in language models."" Keyphrase: ""Implicit gender bias""",3
arxiv2024,Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models,Yes.,4,"""Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues."" and ""challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training.""",2024,2024-03-18T14:48:29Z,"Keyphrase: ""Biased content generation and privacy risks""","""Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues."" and ""challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training."" Keyphrase: ""Biased content generation and privacy risks""",3
arxiv2024,Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus,Yes.,5,"""Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity.""",2024,2024-03-18T13:50:50Z,"Keyphrase: ""Weak inference and logical coherence""","""Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity."" Keyphrase: ""Weak inference and logical coherence""",1
arxiv2024,Do CLIPs Always Generalize Better than ImageNet Models?,Yes.,4,"""We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different",2024,2024-03-18T06:04:02Z,"Keyphrase: ""Performance drop on out-of-distribution data""","""We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different Keyphrase: ""Performance drop on out-of-distribution data""",5
arxiv2024,Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression,Yes.,5,"""the potential risks of compression in terms of safety and trustworthiness have been largely neglected"" and ""our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns.""",2024,2024-03-18T01:38:19Z,"Keyphrase: ""Neglected safety and trustworthiness""","""the potential risks of compression in terms of safety and trustworthiness have been largely neglected"" and ""our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns."" Keyphrase: ""Neglected safety and trustworthiness""",2
arxiv2024,What Makes Math Word Problems Challenging for LLMs?,Yes.,5,"""This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs).""",2024,2024-03-17T23:18:40Z,"Keyphrase: ""Challenges with math word problems""","""This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs)."" Keyphrase: ""Challenges with math word problems""",0
arxiv2024,Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts,Yes.,5,"""a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning"" and ""we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model.""",2024,2024-03-17T19:32:12Z,"Keyphrase: ""Spurious pattern learning""","""a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning"" and ""we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model."" Keyphrase: ""Spurious pattern learning""",5
arxiv2024,Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs,Yes.,5,"""Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps.""",2024,2024-03-17T17:01:45Z,"Keyphrase: ""Hallucination and lack of logical reasoning""","""Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps."" Keyphrase: ""Hallucination and lack of logical reasoning""",0
arxiv2024,Correcting misinformation on social media with a large language model,Yes.,5,"""LLMs also have versatile capabilities that could accelerate misinformation correction--however, they struggle due to a lack of recent information, a tendency to produce false content, and limitations in addressing multimodal information.""",2024,2024-03-17T10:59:09Z,"Keyphrase: ""Tendency to produce false content""","""LLMs also have versatile capabilities that could accelerate misinformation correction--however, they struggle due to a lack of recent information, a tendency to produce false content, and limitations in addressing multimodal information."" Keyphrase: ""Tendency to produce false content""",4
arxiv2024,PhD: A Prompted Visual Hallucination Evaluation Dataset,Yes.,4,"""The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs."" and ""Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IV",2024,2024-03-17T06:53:44Z,"Keyphrase: ""Inability to tackle hallucination""","""The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs."" and ""Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IV Keyphrase: ""Inability to tackle hallucination""",0
arxiv2024,ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models,Yes.,4,"""Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER).""",2024,2024-03-17T06:12:43Z,"Keyphrase: ""Struggles with structured knowledge extraction""","""Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER)."" Keyphrase: ""Struggles with structured knowledge extraction""",1
arxiv2024,Pre-Trained Language Models Represent Some Geographic Populations Better Than Others,Yes.,4,"""Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented.""",2024,2024-03-16T22:01:39Z,"Keyphrase: ""Population representation bias""","""Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented."" Keyphrase: ""Population representation bias""",3
arxiv2024,A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment,Yes.,5,"""Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.""",2024,2024-03-16T08:30:45Z,"Keyphrase: ""Weak fine-grained quality discrimination""","""Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly."" Keyphrase: ""Weak fine-grained quality discrimination""",1
arxiv2024,Do Large Language Models understand Medical Codes?,Yes.,5,"""However, these models are also prone to producing 'hallucinations' or incorrect responses when faced with queries they cannot adequately address,"" and ""Our results indicate that these models as they currently stand do not comprehend the meaning of the medical codes, highlighting the need for better representation of these alphanumeric codes extensively used in healthcare.""",2024,2024-03-16T06:18:15Z,"Keyphrase: ""Difficulty in comprehending medical codes""","""However, these models are also prone to producing 'hallucinations' or incorrect responses when faced with queries they cannot adequately address,"" and ""Our results indicate that these models as they currently stand do not comprehend the meaning of the medical codes, highlighting the need for better representation of these alphanumeric codes extensively used in healthcare."" Keyphrase: ""Difficulty in comprehending medical codes""",0
arxiv2024,Detecting Bias in Large Language Models: Fine-tuned KcBERT,Yes.,4,"""they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language.""",2024,2024-03-16T02:27:19Z,"Keyphrase: ""Propagation of discriminatory language""","""they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language."" Keyphrase: ""Propagation of discriminatory language""",3
arxiv2024,Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases,Yes.,5,"""Addressing the challenge of LLM hallucinations,"" and ""The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets.""",2024,2024-03-15T16:30:14Z,"Keyphrase: ""Limited generalization on small-scale skewed datasets""","""Addressing the challenge of LLM hallucinations,"" and ""The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets."" Keyphrase: ""Limited generalization on small-scale skewed datasets""",0
arxiv2024,A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption,Yes.,5,"""The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity."" and ""We highlight that, in a typical case study where word-level univariate explanations are analyzed with",2024,2024-03-15T13:15:23Z,"Keyphrase: ""Sensitivity to randomness""","""The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity."" and ""We highlight that, in a typical case study where word-level univariate explanations are analyzed with Keyphrase: ""Sensitivity to randomness""",5
arxiv2024,Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models,Yes.,4,"""they are mostly English-centric due to the imbalanced training corpora"" and ""even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios.""",2024,2024-03-15T12:47:39Z,"Keyphrase: ""English-centric bias""","""they are mostly English-centric due to the imbalanced training corpora"" and ""even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios."" Keyphrase: ""English-centric bias""",3
arxiv2024,HawkEye: Training Video-Text LLMs for Grounding Text in Videos,Yes.,5,"""they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images.""",2024,2024-03-15T11:58:18Z,"Keyphrase: ""Limited understanding of temporal information""","""they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images."" Keyphrase: ""Limited understanding of temporal information""",1
arxiv2024,Are LLMs Good Cryptic Crossword Solvers?,Yes.,5,"""showing that their performance on this task is still far from that of humans.""",2024,2024-03-15T06:57:08Z,"Keyphrase: ""Performance gap with humans""","""showing that their performance on this task is still far from that of humans."" Keyphrase: ""Performance gap with humans""",7
arxiv2024,Whose Side Are You On? Investigating the Political Stance of Large Language Models,Yes.,4,"""it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias.""",2024,2024-03-15T04:02:24Z,"Keyphrase: ""Political impartiality and confirmation bias""","""it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias."" Keyphrase: ""Political impartiality and confirmation bias""",3
arxiv2024,Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks,Yes.,5,"""Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways."" and ""we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon.""",2024,2024-03-14T19:39:10Z,"Keyphrase: ""Vulnerability to subversion""","""Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways."" and ""we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon."" Keyphrase: ""Vulnerability to subversion""",2
arxiv2024,Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention,Yes.,5,"""We find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models.""",2024,2024-03-14T18:27:43Z,"Keyphrase: ""Inconsistent harmful behavior generation""","""We find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models."" Keyphrase: ""Inconsistent harmful behavior generation""",2
arxiv2024,Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey,Yes.,4,"""addressing fairness and safety issues in LLMs"" and ""understanding and improving the LLMs' reasoning capacity.""",2024,2024-03-14T17:47:20Z,"Keyphrase: ""Limited fairness and safety considerations""","""addressing fairness and safety issues in LLMs"" and ""understanding and improving the LLMs' reasoning capacity."" Keyphrase: ""Limited fairness and safety considerations""",2
arxiv2024,"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",Yes.,4,"""MLLMs... are also more vulnerable to jailbreak attacks than their LLM predecessors,"" and ""safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features.""",2024,2024-03-14T17:03:04Z,"Keyphrase: ""Vulnerability to jailbreak attacks""","""MLLMs... are also more vulnerable to jailbreak attacks than their LLM predecessors,"" and ""safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features."" Keyphrase: ""Vulnerability to jailbreak attacks""",2
arxiv2024,Logits of API-Protected LLMs Leak Proprietary Information,Yes.,5,"""most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space.""",2024,2024-03-14T16:27:49Z,"Keyphrase: ""Softmax bottleneck restricting output space""","""most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space."" Keyphrase: ""Softmax bottleneck restricting output space""",4
arxiv2024,AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,Yes.,4,"""with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks""",2024,2024-03-14T15:57:13Z,"Keyphrase: ""Vulnerability to structured-based jailbreak attacks""","""with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks"" Keyphrase: ""Vulnerability to structured-based jailbreak attacks""",2
arxiv2024,AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions,Yes.,4,"""These instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks."" and ""Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V.""",2024,2024-03-14T12:51:07Z,"Keyphrase: ""Vulnerability to attacks and bias""","""These instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks."" and ""Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V."" Keyphrase: ""Vulnerability to attacks and bias""",3
arxiv2024,Evaluating LLMs for Gender Disparities in Notable Persons,Yes.,4,"""addressing concerns over their propensity to produce factually incorrect 'hallucinated' responses or to altogether decline to even answer prompt at all"" and ""investigates the presence of gender-based biases in LLMs' responses to factual inquiries.""",2024,2024-03-14T07:58:27Z,"Keyphrase: ""Gender-based bias and factual inaccuracies""","""addressing concerns over their propensity to produce factually incorrect 'hallucinated' responses or to altogether decline to even answer prompt at all"" and ""investigates the presence of gender-based biases in LLMs' responses to factual inquiries."" Keyphrase: ""Gender-based bias and factual inaccuracies""",0
arxiv2024,Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance,Yes.,5,"""Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities.""",2024,2024-03-14T04:06:13Z,"Keyphrase: ""Deficiency in abstract reasoning""","""Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities."" Keyphrase: ""Deficiency in abstract reasoning""",1
arxiv2024,Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors,Yes.,5,"""LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints.""",2024,2024-03-14T00:35:39Z,"Keyphrase: ""Struggles with low-quality evidence retrieval""","""LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints."" Keyphrase: ""Struggles with low-quality evidence retrieval""",4
arxiv2024,AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents,Yes.,5,"""The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge.""",2024,2024-03-13T22:06:03Z,"Keyphrase: ""Limited real-world understanding""","""The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge."" Keyphrase: ""Limited real-world understanding""",1
arxiv2024,The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions,Yes.,5,"""However, these models are susceptible to errors - 'hallucinations' and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.""",2024,2024-03-13T21:39:39Z,"Keyphrase: ""Error hallucination and omission""","""However, these models are susceptible to errors - 'hallucinations' and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks."" Keyphrase: ""Error hallucination and omission""",0
arxiv2024,Bugs in Large Language Models Generated Code: An Empirical Study,Yes.,5,"""Similar to human-written code, LLM-generated code is prone to bugs,"" and ""examines a sample of 333 bugs collected from code generated using three leading LLMs"" and ""identifies the following 10 distinctive bug patterns.""",2024,2024-03-13T20:12:01Z,"Keyphrase: ""Prone to bugs""","""Similar to human-written code, LLM-generated code is prone to bugs,"" and ""examines a sample of 333 bugs collected from code generated using three leading LLMs"" and ""identifies the following 10 distinctive bug patterns."" Keyphrase: ""Prone to bugs""",7
arxiv2024,Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework,Yes.,4,"""Large language models (LLMs) can easily generate biased and discriminative responses."" and ""it is of crucial importance to develop strategies to mitigate these biases.""",2024,2024-03-13T17:46:28Z,"Keyphrase: ""Biased responses""","""Large language models (LLMs) can easily generate biased and discriminative responses."" and ""it is of crucial importance to develop strategies to mitigate these biases."" Keyphrase: ""Biased responses""",3
arxiv2024,Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,Yes.,5,"""they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information.""",2024,2024-03-13T17:29:45Z,"Keyphrase: ""Bias towards pretraining corpus""","""they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information."" Keyphrase: ""Bias towards pretraining corpus""",3
arxiv2024,DevBench: A Comprehensive Benchmark for Software Development,Yes.,5,"""Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts.""",2024,2024-03-13T15:13:44Z,"Keyphrase: ""Struggles with complex programming challenges""","""Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts."" Keyphrase: ""Struggles with complex programming challenges""",7
arxiv2024,Non-discrimination Criteria for Generative Language Models,Yes.,4,"""concerns arise about perpetuating and amplifying harmful biases in applications"" and ""this paper studies how to uncover and quantify the presence of gender biases in generative language models.""",2024,2024-03-13T14:19:08Z,"Keyphrase: ""Perpetuating harmful bias""","""concerns arise about perpetuating and amplifying harmful biases in applications"" and ""this paper studies how to uncover and quantify the presence of gender biases in generative language models."" Keyphrase: ""Perpetuating harmful bias""",3
arxiv2024,SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks,Yes.,4,"""We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies.""",2024,2024-03-13T12:46:51Z,"Keyphrase: ""Vulnerability to membership inference attacks""","""We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies."" Keyphrase: ""Vulnerability to membership inference attacks""",2
arxiv2024,Tastle: Distract Large Language Models for Automatic Jailbreak Attack,Yes.,5,"""even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors"" and ""highlight the crucial need to develop more effective and practical defense strategies.""",2024,2024-03-13T11:16:43Z,"Keyphrase: ""Vulnerability to malicious manipulation""","""even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors"" and ""highlight the crucial need to develop more effective and practical defense strategies."" Keyphrase: ""Vulnerability to malicious manipulation""",2
arxiv2024,Do Large Language Models Solve ARC Visual Analogies Like People Do?,Yes.,5,"""Results show that both children and adults outperform most LLMs on these tasks."" and ""Error analysis revealed a similar 'fallback' solution strategy in LLMs and young children, where part of the analogy is simply copied."" and ""On the whole, 'concept' errors were more common in humans, and 'matrix' errors were more common in LLMs.""",2024,2024-03-13T09:48:13Z,"Keyphrase: ""Limited ability to understand context and generate original solutions""","""Results show that both children and adults outperform most LLMs on these tasks."" and ""Error analysis revealed a similar 'fallback' solution strategy in LLMs and young children, where part of the analogy is simply copied."" and ""On the whole, 'concept' errors were more common in humans, and 'matrix' errors were more common in LLMs."" Keyphrase: ""Limited ability to understand context and generate original solutions""",2
arxiv2024,CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model,Yes.,5,"""Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated."" and ""Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and",2024,2024-03-13T08:54:31Z,"Keyphrase: ""Catastrophic forgetting""","""Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated."" and ""Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and Keyphrase: ""Catastrophic forgetting""",5
arxiv2024,Knowledge Conflicts for LLMs: A Survey,Yes.,5,"""highlighting the complex challenges they encounter when blending contextual and parametric knowledge"" and ""These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common.""",2024,2024-03-13T08:02:23Z,"Keyphrase: ""Trustworthiness and performance impacted by conflicting parametric knowledge""","""highlighting the complex challenges they encounter when blending contextual and parametric knowledge"" and ""These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common."" Keyphrase: ""Trustworthiness and performance impacted by conflicting parametric knowledge""",2
arxiv2024,Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models,Yes.,5,"""LLMs have known biases, commonly derived from their training data."" and ""we observe negative correlations between the metropolitan size and the performance of the LLMS, indicating that smaller regions are indeed underrepresented.""",2024,2024-03-12T19:40:18Z,"Keyphrase: ""Underrepresentation of smaller regions""","""LLMs have known biases, commonly derived from their training data."" and ""we observe negative correlations between the metropolitan size and the performance of the LLMS, indicating that smaller regions are indeed underrepresented."" Keyphrase: ""Underrepresentation of smaller regions""",3
arxiv2024,Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging,Yes.,5,"""Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data.""",2024,2024-03-12T18:12:02Z,"Keyphrase: ""Limited multimodal capability and accessibility""","""Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data."" Keyphrase: ""Limited multimodal capability and accessibility""",1
arxiv2024,Exploring Safety Generalization Challenges of Large Language Models via Code,Yes.,5,"""Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input",2024,2024-03-12T17:55:38Z,"Keyphrase: ""Safety vulnerabilities in LLMs""","""Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input Keyphrase: ""Safety vulnerabilities in LLMs""",2
arxiv2024,The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing,Yes.,4,"""the ripple effect in the hidden space is a significant issue in all current model editing methods.""",2024,2024-03-12T17:04:28Z,"Keyphrase: ""Hidden space ripple effect""","""the ripple effect in the hidden space is a significant issue in all current model editing methods."" Keyphrase: ""Hidden space ripple effect""",6
arxiv2024,Beyond Memorization: The Challenge of Random Memory Access in Language Models,Yes.,5,"""we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content.""",2024,2024-03-12T16:42:44Z,"Keyphrase: ""Sequential memory access challenge""","""we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content."" Keyphrase: ""Sequential memory access challenge""",8
arxiv2024,Characterization of Large Language Model Development in the Datacenter,Yes.,4,"""However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization.""",2024,2024-03-12T13:31:14Z,"Keyphrase: ""Challenges in cluster resource utilization""","""However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization."" Keyphrase: ""Challenges in cluster resource utilization""",4
arxiv2024,Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts,Yes.,5,"""Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge augmentation tools, thereby producing hallucinations.""",2024,2024-03-12T11:40:44Z,"Keyphrase: ""Hallucination in text generation""","""Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge augmentation tools, thereby producing hallucinations."" Keyphrase: ""Hallucination in text generation""",0
arxiv2024,MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki,Yes.,5,"""NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled.""",2024,2024-03-12T11:32:30Z,"Keyphrase: ""Limitation in handling large amounts of information""","""NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled."" Keyphrase: ""Limitation in handling large amounts of information""",4
arxiv2024,SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression,Yes.,5,"""state-of-the-art SVD-based LLM compression methods have two key limitations",2024,2024-03-12T07:31:18Z,"Keyphrase: ""Limitations in compression""","""state-of-the-art SVD-based LLM compression methods have two key limitations Keyphrase: ""Limitations in compression""",4
arxiv2024,SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation,Yes.,5,"""LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information.""",2024,2024-03-11T18:26:02Z,"Keyphrase: ""High computational and memory costs with privacy concerns""","""LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information."" Keyphrase: ""High computational and memory costs with privacy concerns""",4
arxiv2024,Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena,Yes.,5,"""We find that all models struggle with understanding the motion component that the CMC adds to a sentence.""",2024,2024-03-11T17:47:47Z,"Keyphrase: ""Struggles with understanding motion""","""We find that all models struggle with understanding the motion component that the CMC adds to a sentence."" Keyphrase: ""Struggles with understanding motion""",1
arxiv2024,Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?,Yes.,5,"""However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection.""",2024,2024-03-11T15:48:56Z,"Keyphrase: ""Lack of safety features""","""However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection."" Keyphrase: ""Lack of safety features""",2
arxiv2024,ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation,Yes.,4,"""However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation.""",2024,2024-03-11T14:10:57Z,"Keyphrase: ""Off-target translation issues""","""However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation."" Keyphrase: ""Off-target translation issues""",6
arxiv2024,Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code,Yes.,4,"""since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples.""",2024,2024-03-11T12:47:04Z,"Keyphrase: ""Vulnerability to data poisoning""","""since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples."" Keyphrase: ""Vulnerability to data poisoning""",2
arxiv2024,Elephants Never Forget: Testing Language Models for Memorization of Tabular Data,Yes.,5,"""the critical issues of data contamination and memorization are often glossed over,"" ""Our investigation reveals that LLMs are pre-trained on many popular tabular datasets,"" ""This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to",2024,2024-03-11T12:07:13Z,"Keyphrase: ""Data contamination and memorization""","""the critical issues of data contamination and memorization are often glossed over,"" ""Our investigation reveals that LLMs are pre-trained on many popular tabular datasets,"" ""This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to Keyphrase: ""Data contamination and memorization""",0
arxiv2024,MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding,Yes.,4,"""This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses.""",2024,2024-03-11T10:57:45Z,"Keyphrase: ""Inaccurate medical information""","""This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses."" Keyphrase: ""Inaccurate medical information""",0
arxiv2024,Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds,Yes.,4,"""However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians.""",2024,2024-03-11T10:53:20Z,"Keyphrase: ""Hallucination and reasoning issues""","""However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians."" Keyphrase: ""Hallucination and reasoning issues""",0
arxiv2024,Academically intelligent LLMs are not necessarily socially intelligent,Yes.,5,"""The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors.""",2024,2024-03-11T10:35:53Z,"Keyphrase: ""Limited social intelligence""","""The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors."" Keyphrase: ""Limited social intelligence""",1
arxiv2024,From English to ASIC: Hardware Implementation with Large Language Model,Yes.,5,"""challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities",2024,2024-03-11T09:57:16Z,"Keyphrase: ""Limited performance in generating hardware description code""","""challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities Keyphrase: ""Limited performance in generating hardware description code""",7
arxiv2024,Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models,Yes.,5,"""Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs.""",2024,2024-03-11T05:51:03Z,"Keyphrase: ""Factually inaccurate hallucination""","""Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs."" Keyphrase: ""Factually inaccurate hallucination""",0
arxiv2024,MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs,Yes.,5,"""we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively.""",2024,2024-03-09T06:28:48Z,"Keyphrase: ""Difficulty handling topic shifts""","""we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively."" Keyphrase: ""Difficulty handling topic shifts""",6
arxiv2024,Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach,Yes.,5,"""While convenient, this modus operandi aggravates 'hallucination' concerns, particularly given the enigmatic 'black-box' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences.""",2024,2024-03-08T19:18:53Z,"Keyphrase: ""Blackbox nature and high-stakes consequences""","""While convenient, this modus operandi aggravates 'hallucination' concerns, particularly given the enigmatic 'black-box' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences."" Keyphrase: ""Blackbox nature and high-stakes consequences""",0
arxiv2024,Can Large Language Models Play Games? A Case Study of A Self-Play Approach,Yes.,4,"""their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on.""",2024,2024-03-08T19:16:29Z,"Keyphrase: ""Reasoning limitations""","""their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on."" Keyphrase: ""Reasoning limitations""",0
arxiv2024,Unfamiliar Finetuning Examples Control How Language Models Hallucinate,Yes.,5,"""Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts.""",2024,2024-03-08T18:28:13Z,"Keyphrase: ""Factually incorrect responses""","""Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts."" Keyphrase: ""Factually incorrect responses""",0
arxiv2024,Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs,Yes.,4,"""LLMs exhibit impressive zero/few-shot inference and generation quality for high-resource languages (HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The",2024,2024-03-08T16:37:36Z,"Keyphrase: ""Prohibitive training costs""","""LLMs exhibit impressive zero/few-shot inference and generation quality for high-resource languages (HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The Keyphrase: ""Prohibitive training costs""",4
arxiv2024,ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models,Yes.,4,"""We observe that better LLMs like GPT-4 can handle a larger variety of question types, but are by no means perfect. Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other benchmarks for various",2024,2024-03-08T12:42:36Z,"Keyphrase: ""Limited reasoning ability""","""We observe that better LLMs like GPT-4 can handle a larger variety of question types, but are by no means perfect. Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other benchmarks for various Keyphrase: ""Limited reasoning ability""",1
arxiv2024,Debiasing Multimodal Large Language Models,Yes.,4,"""our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image,"" and ""our investigation sheds light on the instability of LVLMs across various decoding configurations.""",2024,2024-03-08T12:35:07Z,"Keyphrase: ""Bias in generated content""","""our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image,"" and ""our investigation sheds light on the instability of LVLMs across various decoding configurations."" Keyphrase: ""Bias in generated content""",3
arxiv2024,ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models,Yes.,4,"""Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations.""",2024,2024-03-08T07:59:19Z,"Keyphrase: ""Limited domain-specific capability""","""Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations."" Keyphrase: ""Limited domain-specific capability""",6
arxiv2024,Are Human Conversations Special? A Large Language Model Perspective,Yes.,4,"""there is a significant gap in their ability to specialize in human conversations"" and ""highlight the unique challenges posed by conversational data.""",2024,2024-03-08T04:44:25Z,"Keyphrase: ""Limited ability in human conversation specialization""","""there is a significant gap in their ability to specialize in human conversations"" and ""highlight the unique challenges posed by conversational data."" Keyphrase: ""Limited ability in human conversation specialization""",6
arxiv2024,Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs,Yes.,5,"""Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents.""",2024,2024-03-08T03:49:17Z,"Keyphrase: ""Information asymmetry challenge""","""Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents."" Keyphrase: ""Information asymmetry challenge""",7
arxiv2024,Tell me the truth: A system to measure the trustworthiness of Large Language Models,Yes.,5,"""one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems."" and ""ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites."" and ""ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases.""",2024,2024-03-08T00:27:57Z,"Keyphrase: ""Low accuracy and high false positive rate""","""one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems."" and ""ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites."" and ""ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases."" Keyphrase: ""Low accuracy and high false positive rate""",6
arxiv2024,SecGPT: An Execution Isolation Architecture for LLM-Based Systems,Yes.,4,"""Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users.""",2024,2024-03-08T00:02:30Z,"Keyphrase: ""Trustworthiness and Security Risks""","""Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users."" Keyphrase: ""Trustworthiness and Security Risks""",8
arxiv2024,Automatic and Universal Prompt Injection Attacks against Large Language Models,Yes.,5,"""These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests.""",2024,2024-03-07T23:46:20Z,"Keyphrase: ""Vulnerability to injected content""","""These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests."" Keyphrase: ""Vulnerability to injected content""",2
arxiv2024,Evaluating Biases in Context-Dependent Health Questions,Yes.,4,"""We study how large language model biases are exhibited through these contextual questions in the healthcare domain."" and ""Our experiments reveal biases in each of these attributes, where young adult female users are favored.""",2024,2024-03-07T19:15:40Z,"Keyphrase: ""Contextual bias in healthcare domain""","""We study how large language model biases are exhibited through these contextual questions in the healthcare domain."" and ""Our experiments reveal biases in each of these attributes, where young adult female users are favored."" Keyphrase: ""Contextual bias in healthcare domain""",3
arxiv2024,LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error,Yes.,5,"""Existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice.""",2024,2024-03-07T18:50:51Z,"Keyphrase: ""Low correctness rate""","""Existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice."" Keyphrase: ""Low correctness rate""",6
arxiv2024,SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM,Yes.,4,"""Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses.""",2024,2024-03-07T18:38:17Z,"Keyphrase: ""Difficulty with longtail entities and hallucinated responses""","""Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses."" Keyphrase: ""Difficulty with longtail entities and hallucinated responses""",0
arxiv2024,How Far Are We from Intelligent Visual Deductive Reasoning?,Yes.,4,"""we are still far from achieving comparable proficiency in visual deductive reasoning,"" and ""certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks,"" and ""VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.""",2024,2024-03-07T18:35:54Z,"Keyphrase: ""Limited visual deductive reasoning proficiency""","""we are still far from achieving comparable proficiency in visual deductive reasoning,"" and ""certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks,"" and ""VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples."" Keyphrase: ""Limited visual deductive reasoning proficiency""",1
arxiv2024,Common 7B Language Models Already Possess Strong Math Capabilities,Yes.,5,"""The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities.""",2024,2024-03-07T18:00:40Z,"Keyphrase: ""Inconsistent mathematical capability""","""The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities."" Keyphrase: ""Inconsistent mathematical capability""",1
arxiv2024,Telecom Language Models: Must They Be Large?,Yes.,5,"""the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments"" and ""highlighting its potential and limitations.""",2024,2024-03-07T17:13:12Z,"Keyphrase: ""High computational demand""","""the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments"" and ""highlighting its potential and limitations."" Keyphrase: ""High computational demand""",4
arxiv2024,QAQ: Quality Adaptive Quantization for LLM KV Cache,Yes.,5,"""a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length"" and ""heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance.""",2024,2024-03-07T16:42:37Z,"Keyphrase: ""Context length bottleneck""","""a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length"" and ""heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance."" Keyphrase: ""Context length bottleneck""",4
arxiv2024,HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild,Yes.,5,"""Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains.""",2024,2024-03-07T08:25:46Z,"Keyphrase: ""Reliability challenges due to hallucination""","""Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains."" Keyphrase: ""Reliability challenges due to hallucination""",0
arxiv2024,Can Small Language Models be Good Reasoners for Sequential Recommendation?,Yes.,5,"""However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high",2024,2024-03-07T06:49:37Z,"Keyphrase: ""Complex user behavior patterns and resource requirements""","""However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high Keyphrase: ""Complex user behavior patterns and resource requirements""",6
arxiv2024,Can Large Language Models Reason and Plan?,Yes.,4,"""While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.""",2024,2024-03-07T00:36:32Z,"Keyphrase: ""Limited self-correction capabilities""","""While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs."" Keyphrase: ""Limited self-correction capabilities""",5
arxiv2024,Can Large Language Models do Analytical Reasoning?,Yes.,5,"""we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores."" and ""we conclude that task complexity depends on the length of context, the information density, and the presence of related information.""",2024,2024-03-06T20:22:08Z,"Keyphrase: ""Difficulty with accurate counting""","""we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores."" and ""we conclude that task complexity depends on the length of context, the information density, and the presence of related information."" Keyphrase: ""Difficulty with accurate counting""",7
arxiv2024,Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ,Yes.,4,"""However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen)."" and ""there is a long tail of languages where models are neither accurate nor faithful.""",2024,2024-03-06T16:01:44Z,"Keyphrase: ""Limited accuracy and faithfulness in non-English language models""","""However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen)."" and ""there is a long tail of languages where models are neither accurate nor faithful."" Keyphrase: ""Limited accuracy and faithfulness in non-English language models""",6
arxiv2024,Towards Safe and Aligned Large Language Models for Medicine,Yes.,4,"""While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses,"" and ""the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights.""",2024,2024-03-06T14:34:07Z,"Keyphrase: ""Lack of safety evaluation""","""While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses,"" and ""the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights."" Keyphrase: ""Lack of safety evaluation""",2
arxiv2024,Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem,Yes.,5,"""However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination.""",2024,2024-03-06T09:06:34Z,"Keyphrase: ""Unreliable hallucination""","""However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination."" Keyphrase: ""Unreliable hallucination""",0
arxiv2024,Towards Efficient and Effective Unlearning of Large Language Models for Recommendation,Yes.,4,"""recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process.""",2024,2024-03-06T08:31:35Z,"Keyphrase: ""Inefficient unlearning process""","""recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process."" Keyphrase: ""Inefficient unlearning process""",5
arxiv2024,CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models,Yes.,5,"""We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings.""",2024,2024-03-06T07:43:43Z,"Keyphrase: ""Challenges in long-context settings""","""We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings."" Keyphrase: ""Challenges in long-context settings""",4
arxiv2024,Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy,Yes.,5,"""Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities,""",2024,2024-03-05T19:40:53Z,"Keyphrase: ""Limited rational reasoning capabilities""","""Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities,"" Keyphrase: ""Limited rational reasoning capabilities""",1
arxiv2024,Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs,Yes.,5,"""Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other L",2024,2024-03-05T19:32:01Z,"Keyphrase: ""Data leakage and overfitting""","""Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other L Keyphrase: ""Data leakage and overfitting""",6
arxiv2024,The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning,Yes.,4,"""The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons."" and ""WMDP serves two roles",2024,2024-03-05T18:59:35Z,"Keyphrase: ""Security risks and misuse""","""The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons."" and ""WMDP serves two roles Keyphrase: ""Security risks and misuse""",2
arxiv2024,"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",Yes.,4,"""We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes."" and ""The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.""",2024,2024-03-05T17:04:05Z,"Keyphrase: ""Gendered emotion stereotypes""","""We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes."" and ""The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications."" Keyphrase: ""Gendered emotion stereotypes""",3
arxiv2024,KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents,Yes.,5,"""Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories",2024,2024-03-05T16:39:12Z,"Keyphrase: ""Lack of actionable knowledge""","""Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories Keyphrase: ""Lack of actionable knowledge""",1
arxiv2024,Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations,Yes.,4,"""However, their precision is still far away from acceptable in a sensitive field like education."" and ""reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context.""",2024,2024-03-05T14:41:12Z,"Keyphrase: ""Risk of hallucination and imprecise information""","""However, their precision is still far away from acceptable in a sensitive field like education."" and ""reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context."" Keyphrase: ""Risk of hallucination and imprecise information""",0
arxiv2024,ImgTrojan: Jailbreaking Vision-Language Models with ONE Image,Yes.,4,"""However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored.""",2024,2024-03-05T12:21:57Z,"Keyphrase: ""Underexplored safety concerns""","""However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored."" Keyphrase: ""Underexplored safety concerns""",2
arxiv2024,In Search of Truth: An Interrogation Approach to Hallucination Detection,Yes.,5,"""One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth.""",2024,2024-03-05T11:50:01Z,"Keyphrase: ""Factual truth drift""","""One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth."" Keyphrase: ""Factual truth drift""",0
arxiv2024,An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers,Yes.,4,"""their generalizability and fairness severely underperform GPT4.""",2024,2024-03-05T10:20:52Z,"Keyphrase: ""Poor generalizability and fairness""","""their generalizability and fairness severely underperform GPT4."" Keyphrase: ""Poor generalizability and fairness""",3
arxiv2024,EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs,Yes.,5,"""However, their expensive computations and high memory requirements are prohibitive for deployment."" and ""the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks.""",2024,2024-03-05T08:45:30Z,"Keyphrase: ""High resource requirements""","""However, their expensive computations and high memory requirements are prohibitive for deployment."" and ""the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks."" Keyphrase: ""High resource requirements""",4
arxiv2024,"Towards Measuring and Modeling ""Culture"" in LLMs: A Survey",Yes.,4,"""Our analysis indicates that only certain aspects of 'culture,' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situated",2024,2024-03-05T08:29:36Z,"Keyphrase: ""Lack of robustness in semantic understanding""","""Our analysis indicates that only certain aspects of 'culture,' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situated Keyphrase: ""Lack of robustness in semantic understanding""",3
arxiv2024,Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models,Yes.,5,"""when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains.""",2024,2024-03-05T08:22:41Z,"Keyphrase: ""Catastrophic forgetting in specific domains""","""when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains."" Keyphrase: ""Catastrophic forgetting in specific domains""",5
arxiv2024,Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,Yes.,4,"""they still face challenges of various biases"" and ""Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs.""",2024,2024-03-05T07:47:34Z,"Keyphrase: ""Limited traditional debiasing methods""","""they still face challenges of various biases"" and ""Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs."" Keyphrase: ""Limited traditional debiasing methods""",3
arxiv2024,Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models,Yes.,4,"""available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese,"" and ""models with more parameters can introduce more biases and uncalibrated outputs.""",2024,2024-03-05T07:13:28Z,"Keyphrase: ""Bias and uncalibrated output""","""available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese,"" and ""models with more parameters can introduce more biases and uncalibrated outputs."" Keyphrase: ""Bias and uncalibrated output""",3
arxiv2024,Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding,Yes.,5,"""the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled.""",2024,2024-03-05T04:58:37Z,"Keyphrase: ""Difficulty in context understanding""","""the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled."" Keyphrase: ""Difficulty in context understanding""",5
arxiv2024,Exploring the Limitations of Large Language Models in Compositional Relation Reasoning,Yes.,5,"""We present a comprehensive evaluation of large language models (LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations.""",2024,2024-03-05T03:07:10Z,"Keyphrase: ""Limited reasoning ability""","""We present a comprehensive evaluation of large language models (LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations."" Keyphrase: ""Limited reasoning ability""",1
arxiv2024,"Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",Yes.,5,"""a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted.""",2024,2024-03-04T22:02:12Z,"Keyphrase: ""Knowledge reduction and forgetting""","""a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted."" Keyphrase: ""Knowledge reduction and forgetting""",5
arxiv2024,SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models,Yes.,5,"""However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs.""",2024,2024-03-04T21:55:22Z,"Keyphrase: ""Confidently wrong predictions""","""However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs."" Keyphrase: ""Confidently wrong predictions""",0
arxiv2024,Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems,Yes.,5,"""We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls.""",2024,2024-03-04T19:12:48Z,"Keyphrase: ""Inconsistent performance with voting inference""","""We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls."" Keyphrase: ""Inconsistent performance with voting inference""",5
arxiv2024,FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction,Yes.,4,"""a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations"" and ""these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles),",2024,2024-03-04T17:57:18Z,"Keyphrase: ""Factual inconsistency and lack of interpretability""","""a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations"" and ""these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles), Keyphrase: ""Factual inconsistency and lack of interpretability""",0
arxiv2024,PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models,Yes.,4,"""Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at."" and ""our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their reasoning abilities in an unexpected manner.""",2024,2024-03-04T17:34:34Z,"Keyphrase: ""Limited socialcognitive reasoning""","""Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at."" and ""our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their reasoning abilities in an unexpected manner."" Keyphrase: ""Limited socialcognitive reasoning""",1
arxiv2024,Cognition is All You Need -- The Next Layer of AI Above Large Language Models,Yes.,5,"""Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving."" and ""The failure of these systems to address complex knowledge work is due to the fact that they",2024,2024-03-04T16:11:57Z,"Keyphrase: ""Limited reasoning and problem-solving capabilities""","""Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving."" and ""The failure of these systems to address complex knowledge work is due to the fact that they Keyphrase: ""Limited reasoning and problem-solving capabilities""",6
arxiv2024,Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?,Yes.,4,"""inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss.""",2024,2024-03-04T14:01:11Z,"Keyphrase: ""English-centric bias and information loss""","""inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss."" Keyphrase: ""English-centric bias and information loss""",3
arxiv2024,Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism,Yes.,4,"""While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.""",2024,2024-03-04T13:57:37Z,"Keyphrase: ""Hallucination propensity""","""While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions."" Keyphrase: ""Hallucination propensity""",0
arxiv2024,WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations,Yes.,5,"""existing datasets and evaluation methods in this domain still exhibit notable limitations"" and ""highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement.""",2024,2024-03-04T07:06:41Z,"Keyphrase: ""Source attribution challenges""","""existing datasets and evaluation methods in this domain still exhibit notable limitations"" and ""highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement."" Keyphrase: ""Source attribution challenges""",0
arxiv2024,How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems,Yes.,5,"""a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems.""",2024,2024-03-04T06:24:21Z,"Keyphrase: ""Struggle with capturing relationships among decision variables""","""a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems."" Keyphrase: ""Struggle with capturing relationships among decision variables""",1
arxiv2024,In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation,Yes.,5,"""Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited.""",2024,2024-03-03T15:53:41Z,"Keyphrase: ""Frequent hallucinations and factual errors""","""Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited."" Keyphrase: ""Frequent hallucinations and factual errors""",0
arxiv2024,Ever-Evolving Memory by Blending and Refining the Past,Yes.,5,"""current large language models often lack this capability, leading to instances of missing important user information or redundantly asking for the same information, thereby diminishing conversation quality.""",2024,2024-03-03T08:12:59Z,"Keyphrase: ""Lack of conversational capability""","""current large language models often lack this capability, leading to instances of missing important user information or redundantly asking for the same information, thereby diminishing conversation quality."" Keyphrase: ""Lack of conversational capability""",6
arxiv2024,Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge,Yes.,5,"""However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications.""",2024,2024-03-03T08:07:55Z,"Keyphrase: ""Struggles with low-frequency concepts""","""However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications."" Keyphrase: ""Struggles with low-frequency concepts""",6
arxiv2024,Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering,Yes.,4,"""existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable.""",2024,2024-03-03T04:22:13Z,"Keyphrase: ""Hallucination in KGQA""","""existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable."" Keyphrase: ""Hallucination in KGQA""",0
arxiv2024,Analysis of Privacy Leakage in Federated Large Language Models,Yes.,5,"""revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets.""",2024,2024-03-02T20:25:38Z,"Keyphrase: ""Privacy vulnerabilities""","""revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets."" Keyphrase: ""Privacy vulnerabilities""",8
arxiv2024,Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal,Yes.,5,"""Large language models (LLMs) suffer from catastrophic forgetting during continual learning.""",2024,2024-03-02T16:11:23Z,"Keyphrase: ""Catastrophic forgetting""","""Large language models (LLMs) suffer from catastrophic forgetting during continual learning."" Keyphrase: ""Catastrophic forgetting""",5
arxiv2024,RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,Yes.,5,"""However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge."" and ""These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications.""",2024,2024-03-02T12:19:04Z,"Keyphrase: ""Generation of false information""","""However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge."" and ""These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications."" Keyphrase: ""Generation of false information""",0
arxiv2024,"A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",Yes.,4,"""While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale.""",2024,2024-03-02T09:39:13Z,"Keyphrase: ""Risk of misinformation proliferation""","""While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale."" Keyphrase: ""Risk of misinformation proliferation""",0
arxiv2024,Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers,Yes.,5,"""We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext.""",2024,2024-03-02T01:52:14Z,"Keyphrase: ""Faithfulness mistakes""","""We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext."" Keyphrase: ""Faithfulness mistakes""",1
arxiv2024,Mitigating Reversal Curse in Large Language Models via Semantic-aware Permutation Training,Yes.,5,"""recent studies showcase that causal LLMs suffer from the 'reversal curse'. It is a typical example that the model knows 'A's father is B', but is unable to reason 'B's child is A'. This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional",2024,2024-03-01T18:55:20Z,"Keyphrase: ""Limited causal reasoning""","""recent studies showcase that causal LLMs suffer from the 'reversal curse'. It is a typical example that the model knows 'A's father is B', but is unable to reason 'B's child is A'. This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional Keyphrase: ""Limited causal reasoning""",1
arxiv2024,Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents,Yes.,5,"""However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions."" and ""While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack's mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback.""",2024,2024-03-01T17:22:16Z,"Keyphrase: ""Limited adaptability to dynamic environments""","""However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions."" and ""While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack's mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback."" Keyphrase: ""Limited adaptability to dynamic environments""",1
arxiv2024,DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models,Yes.,4,"""Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge,"" and ""many merely focus on the factuality hallucination while ignoring the faithfulness hallucination.""",2024,2024-03-01T15:38:55Z,"Keyphrase: ""Hallucination issue""","""Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge,"" and ""many merely focus on the factuality hallucination while ignoring the faithfulness hallucination."" Keyphrase: ""Hallucination issue""",0
arxiv2024,TempCompass: Do Video LLMs Really Understand Videos?,Yes.,5,"""reveal the discerning fact that these models exhibit notably poor temporal perception ability.""",2024,2024-03-01T12:02:19Z,"Keyphrase: ""Poor temporal perception""","""reveal the discerning fact that these models exhibit notably poor temporal perception ability."" Keyphrase: ""Poor temporal perception""",1
arxiv2024,Invariant Test-Time Adaptation for Vision-Language Model Generalization,Yes.,4,"""However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of 'decision shortcuts' that hinders their generalization capabilities."" and ""the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in",2024,2024-03-01T09:01:53Z,"Keyphrase: ""Limited generalization in longtail tasks""","""However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of 'decision shortcuts' that hinders their generalization capabilities."" and ""the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in Keyphrase: ""Limited generalization in longtail tasks""",1
arxiv2024,Teach LLMs to Phish: Stealing Private Information from Language Models,Yes.,5,"""When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information.""",2024,2024-03-01T06:15:07Z,"Keyphrase: ""Privacy risks and sensitive information memorization""","""When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information."" Keyphrase: ""Privacy risks and sensitive information memorization""",8
arxiv2024,DPP-Based Adversarial Prompt Searching for Lanugage Models,Yes.,4,"""Language models risk generating mindless and offensive content, which hinders their safe deployment."" and ""Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content.""",2024,2024-03-01T05:28:06Z,"Keyphrase: ""Generation of offensive content""","""Language models risk generating mindless and offensive content, which hinders their safe deployment."" and ""Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content."" Keyphrase: ""Generation of offensive content""",2
arxiv2024,Gender Bias in Large Language Models across Multiple Languages,Yes.,5,"""assessing the influence of gender biases embedded in LLMs becomes crucial"" and ""Our findings revealed significant gender biases across all the languages we examined.""",2024,2024-03-01T04:47:16Z,"Keyphrase: ""Gender bias""","""assessing the influence of gender biases embedded in LLMs becomes crucial"" and ""Our findings revealed significant gender biases across all the languages we examined."" Keyphrase: ""Gender bias""",3
arxiv2024,Extracting Polymer Nanocomposite Samples from Full-Length Documents,Yes.,5,"""Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.""",2024,2024-03-01T03:51:56Z,"Keyphrase: ""Challenges in information extraction""","""Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them."" Keyphrase: ""Challenges in information extraction""",1
arxiv2024,Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes,Yes.,4,"""recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails.""",2024,2024-03-01T03:29:54Z,"Keyphrase: ""Vulnerability to adversarial jailbreak attempts""","""recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails."" Keyphrase: ""Vulnerability to adversarial jailbreak attempts""",2
arxiv2024,Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,Yes.,5,"""However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains."" and ""Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.""",2024,2024-03-01T02:21:30Z,"Keyphrase: ""Limited interpretive abilities in scientific contexts""","""However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains."" and ""Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements."" Keyphrase: ""Limited interpretive abilities in scientific contexts""",1
arxiv2024,AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs,Yes.,4,"""Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications.""",2024,2024-03-01T00:02:37Z,"Keyphrase: ""Bias from training data""","""Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications."" Keyphrase: ""Bias from training data""",3
arxiv2024,FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition,Yes.,5,"""such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities"" and ""we identify a common shortfall in knowledge utilization among models.""",2024,2024-02-29T21:05:37Z,"Keyphrase: ""Limited cognitive skill differentiation""","""such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities"" and ""we identify a common shortfall in knowledge utilization among models."" Keyphrase: ""Limited cognitive skill differentiation""",1
arxiv2024,NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications,Yes.,4,"""highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks.""",2024,2024-02-29T21:05:14Z,"Keyphrase: ""Ethical and creative deficiencies""","""highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks."" Keyphrase: ""Ethical and creative deficiencies""",3
arxiv2024,Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs,Yes.,5,"""Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates.""",2024,2024-02-29T19:55:06Z,"Keyphrase: ""Memory limitations""","""Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates."" Keyphrase: ""Memory limitations""",4
arxiv2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,Yes.,5,"""This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences.""",2024,2024-02-29T19:02:03Z,"Keyphrase: ""Difficulty with out-of-distribution tokens""","""This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences."" Keyphrase: ""Difficulty with out-of-distribution tokens""",4
arxiv2024,Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization,Yes.,4,"""However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases."" and ""almost all of them (except GPT-4), even after fine-tuning, could not properly generate the response in the required output format.""",2024,2024-02-29T19:00:47Z,"Keyphrase: ""High inference costs""","""However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases."" and ""almost all of them (except GPT-4), even after fine-tuning, could not properly generate the response in the required output format."" Keyphrase: ""High inference costs""",4
arxiv2024,Curiosity-driven Red-teaming for Large Language Models,Yes.,5,"""Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content.""",2024,2024-02-29T18:55:03Z,"Keyphrase: ""Risk of generating toxic content""","""Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content."" Keyphrase: ""Risk of generating toxic content""",2
arxiv2024,On the Scaling Laws of Geographical Representation in Language Models,Yes.,4,"""we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.""",2024,2024-02-29T18:04:11Z,"Keyphrase: ""Geographical bias persistence""","""we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data."" Keyphrase: ""Geographical bias persistence""",3
arxiv2024,Entity-Aware Multimodal Alignment Framework for News Image Captioning,Yes.,4,"""Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-t",2024,2024-02-29T18:03:00Z,"Keyphrase: ""Limited entity information handling""","""Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-t Keyphrase: ""Limited entity information handling""",6
arxiv2024,SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation,Yes.,4,"""Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training samples available in practice lead to poor code generation performance.""",2024,2024-02-29T16:09:02Z,"Keyphrase: ""Limited adaptation to specific scenarios""","""Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training samples available in practice lead to poor code generation performance."" Keyphrase: ""Limited adaptation to specific scenarios""",7
arxiv2024,GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers,Yes.,5,"""One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly."" and ""Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust.""",2024,2024-02-29T15:26:14Z,"Keyphrase: ""Limited math reasoning ability""","""One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly."" and ""Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust."" Keyphrase: ""Limited math reasoning ability""",1
arxiv2024,Memory-Augmented Generative Adversarial Transformers,Yes.,5,"""Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy.""",2024,2024-02-29T14:47:24Z,"Keyphrase: ""Limited ability to integrate external data""","""Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy."" Keyphrase: ""Limited ability to integrate external data""",6
arxiv2024,Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model,Yes.,5,"""However, the Typographic Attack, which disrupts vision-language models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), has also been expected to be a security threat to LVLMs."" and ""Based on the evaluation results, we investigate the causes why typographic attacks may impact VLMs and LVLMs, leading to three highly insightful",2024,2024-02-29T13:31:56Z,"Keyphrase: ""Vulnerability to typographic attacks""","""However, the Typographic Attack, which disrupts vision-language models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), has also been expected to be a security threat to LVLMs."" and ""Based on the evaluation results, we investigate the causes why typographic attacks may impact VLMs and LVLMs, leading to three highly insightful Keyphrase: ""Vulnerability to typographic attacks""",2
arxiv2024,Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models,Yes.,5,"""Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted",2024,2024-02-29T12:35:45Z,"Keyphrase: ""Hallucination of text""","""Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted Keyphrase: ""Hallucination of text""",0
arxiv2024,Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials,Yes.,5,"""investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples"" and ""the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent",2024,2024-02-29T12:01:46Z,"Keyphrase: ""Reliance on shortcut features""","""investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples"" and ""the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent Keyphrase: ""Reliance on shortcut features""",5
arxiv2024,Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning,Yes.,5,"""However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem.""",2024,2024-02-29T05:27:45Z,"Keyphrase: ""Catastrophic forgetting""","""However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem."" Keyphrase: ""Catastrophic forgetting""",5
arxiv2024,Learning to Compress Prompt in Natural Language Formats,Yes.,5,"""Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results.""",2024,2024-02-28T20:41:21Z,"Keyphrase: ""Inferior long context processing""","""Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results."" Keyphrase: ""Inferior long context processing""",4
arxiv2024,FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability,Yes.,5,"""Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately.""",2024,2024-02-28T19:23:27Z,"Keyphrase: ""Benchmark performance inadequacy""","""Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately."" Keyphrase: ""Benchmark performance inadequacy""",7
arxiv2024,A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems,Yes.,4,"""there are also increasing concerns over the security of such probabilistic intelligent systems"" and ""Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components.""",2024,2024-02-28T19:00:12Z,"Keyphrase: ""Security vulnerabilities""","""there are also increasing concerns over the security of such probabilistic intelligent systems"" and ""Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components."" Keyphrase: ""Security vulnerabilities""",2
arxiv2024,Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates,Yes.,4,"""even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models.""",2024,2024-02-28T18:23:49Z,"Keyphrase: ""Unsafe behavior after finetuning""","""even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models."" Keyphrase: ""Unsafe behavior after finetuning""",2
arxiv2024,Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,Yes.,5,"""we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem."" and ""we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers.""",2024,2024-02-28T14:09:02Z,"Keyphrase: ""Information loss and shallow attention""","""we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem."" and ""we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers."" Keyphrase: ""Information loss and shallow attention""",1
arxiv2024,LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History,Yes.,5,"""we find that performance can in fact also be negatively impacted, if there is a task-switch"" and ""many of the task-switches can lead to significant performance degradation.""",2024,2024-02-28T10:19:05Z,"Keyphrase: ""Performance degradation due to task-switching""","""we find that performance can in fact also be negatively impacted, if there is a task-switch"" and ""many of the task-switches can lead to significant performance degradation."" Keyphrase: ""Performance degradation due to task-switching""",5
arxiv2024,Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction,Yes.,5,"""One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs.""",2024,2024-02-28T06:50:14Z,"Keyphrase: ""Vulnerability to adversarial prompts""","""One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs."" Keyphrase: ""Vulnerability to adversarial prompts""",2
arxiv2024,No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization,Yes.,5,"""the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself.""",2024,2024-02-28T06:34:54Z,"Keyphrase: ""Memory footprint bottleneck""","""the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself."" Keyphrase: ""Memory footprint bottleneck""",4
arxiv2024,MEGAnno+: A Human-LLM Collaborative Annotation System,Yes.,5,"""Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations.""",2024,2024-02-28T04:58:07Z,"Keyphrase: ""Difficulty with complex sociocultural context""","""Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations."" Keyphrase: ""Difficulty with complex sociocultural context""",3
arxiv2024,Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore,Yes.,4,"""Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge,"" and ""multilingual models demonstrate a bias towards factual information from Western continents.""",2024,2024-02-28T04:43:46Z,"Keyphrase: ""Factual hallucination and bias""","""Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge,"" and ""multilingual models demonstrate a bias towards factual information from Western continents."" Keyphrase: ""Factual hallucination and bias""",0
arxiv2024,Corpus-Steered Query Expansion with Large Language Models,Yes.,5,"""challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs.""",2024,2024-02-28T03:58:58Z,"Keyphrase: ""Misalignment in retrieval corpus""","""challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs."" Keyphrase: ""Misalignment in retrieval corpus""",1
arxiv2024,TroubleLLM: Align to Red Team Expert,Yes.,4,"""However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content.""",2024,2024-02-28T03:40:46Z,"Keyphrase: ""Undesirable safety issues""","""However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content."" Keyphrase: ""Undesirable safety issues""",2
arxiv2024,FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization,Yes.,5,"""However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance.""",2024,2024-02-28T02:00:34Z,"Keyphrase: ""Latency and memory consumption restrictions""","""However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance."" Keyphrase: ""Latency and memory consumption restrictions""",4
arxiv2024,Collaborative decoding of critical tokens for boosting factuality of large language models,Yes.,5,"""their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination.""",2024,2024-02-28T01:53:37Z,"Keyphrase: ""Increased risk of hallucination""","""their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination."" Keyphrase: ""Increased risk of hallucination""",0
arxiv2024,Gradient-Free Adaptive Global Pruning for Pre-trained Language Models,Yes.,5,"""The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands.""",2024,2024-02-28T00:09:07Z,"Keyphrase: ""Prohibitive computational demand""","""The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands."" Keyphrase: ""Prohibitive computational demand""",4
arxiv2024,LLM-Resistant Math Word Problem Generation via Adversarial Attacks,Yes.,5,"""We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure, offering a nuanced view into model's limitation.""",2024,2024-02-27T22:07:52Z,"Keyphrase: ""Vulnerability to attacks""","""We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure, offering a nuanced view into model's limitation."" Keyphrase: ""Vulnerability to attacks""",2
arxiv2024,BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra,Yes.,5,"""due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting.""",2024,2024-02-27T20:48:24Z,"Keyphrase: ""Context size limitation""","""due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting."" Keyphrase: ""Context size limitation""",4
arxiv2024,Evaluating Very Long-Term Conversational Memory of LLM Agents,Yes.,5,"""Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.""",2024,2024-02-27T18:42:31Z,"Keyphrase: ""Difficulty in long-range conversation comprehension""","""Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues."" Keyphrase: ""Difficulty in long-range conversation comprehension""",1
arxiv2024,AmbigNLG: Addressing Task Ambiguity in Instruction for NLG,Yes.,5,"""their performance is significantly hindered by the ambiguity present in real-world instructions.""",2024,2024-02-27T17:52:33Z,"Keyphrase: ""Ambiguity in real-world instructions""","""their performance is significantly hindered by the ambiguity present in real-world instructions."" Keyphrase: ""Ambiguity in real-world instructions""",1
arxiv2024,Case-Based or Rule-Based: How Do Transformers Do the Math?,Yes.,5,"""modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition.""",2024,2024-02-27T17:41:58Z,"Keyphrase: ""Struggles with math problems""","""modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition."" Keyphrase: ""Struggles with math problems""",1
arxiv2024,NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents,Yes.,5,"""they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism.""",2024,2024-02-27T16:56:30Z,"Keyphrase: ""Struggles with processing long sequences""","""they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism."" Keyphrase: ""Struggles with processing long sequences""",4
arxiv2024,Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data,Yes.,5,"""Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously.""",2024,2024-02-27T16:15:03Z,"Keyphrase: ""Limited causal reasoning""","""Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously."" Keyphrase: ""Limited causal reasoning""",1
arxiv2024,TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space,Yes.,5,"""Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge.""",2024,2024-02-27T14:45:04Z,"Keyphrase: ""Hallucination and untruthful responses""","""Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge."" Keyphrase: ""Hallucination and untruthful responses""",0
arxiv2024,Predict the Next Word: Humans exhibit uncertainty in this task and language models _____,Yes.,5,"""We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty.""",2024,2024-02-27T14:11:32Z,"Keyphrase: ""Low calibration of human uncertainty""","""We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty."" Keyphrase: ""Low calibration of human uncertainty""",6
arxiv2024,Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles,Yes.,5,"""Results showed that GPT-4's performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4",2024,2024-02-27T13:02:19Z,"Keyphrase: ""Limited performance in detecting propaganda techniques""","""Results showed that GPT-4's performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 Keyphrase: ""Limited performance in detecting propaganda techniques""",3
arxiv2024,Training-Free Long-Context Scaling of Large Language Models,Yes.,5,"""The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length.""",2024,2024-02-27T12:39:23Z,"Keyphrase: ""Limited coherence with lengthy input""","""The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length."" Keyphrase: ""Limited coherence with lengthy input""",4
arxiv2024,LLMGuard: Guarding Against Unsafe LLM Behavior,Yes.,5,"""it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns.""",2024,2024-02-27T10:22:45Z,"Keyphrase: ""Generating inappropriate biased content""","""it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns."" Keyphrase: ""Generating inappropriate biased content""",2
arxiv2024,SoFA: Shielded On-the-fly Alignment via Priority Rule Following,Yes.,5,"""even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules.""",2024,2024-02-27T09:52:27Z,"Keyphrase: ""Poor rule understanding and prioritization""","""even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules."" Keyphrase: ""Poor rule understanding and prioritization""",1
arxiv2024,Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese,Yes.,4,"""Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages.""",2024,2024-02-27T08:24:32Z,"Keyphrase: ""Knowledge adequacy disparity""","""Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages."" Keyphrase: ""Knowledge adequacy disparity""",7
arxiv2024,Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue,Yes.,5,"""Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs.""",2024,2024-02-27T07:11:59Z,"Keyphrase: ""Inadequate safety mechanisms""","""Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs."" Keyphrase: ""Inadequate safety mechanisms""",2
arxiv2024,Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection,Yes.,5,"""We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD,",2024,2024-02-27T07:02:10Z,"Keyphrase: ""Limited zero-shot/few-shot capabilities""","""We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD, Keyphrase: ""Limited zero-shot/few-shot capabilities""",6
arxiv2024,Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses,Yes.,5,"""Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios.""",2024,2024-02-27T00:22:18Z,"Keyphrase: ""Hallucination challenge""","""Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios."" Keyphrase: ""Hallucination challenge""",0
arxiv2024,Algorithmic Arbitrariness in Content Moderation,Yes.,4,"""We analyze (i) the extent of predictive multiplicity among state-of-the-art LLMs used for detecting toxic content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) how model multiplicity compares to unambiguous human classifications.""",2024,2024-02-26T19:27:00Z,"Keyphrase: ""Predictive multiplicity and disparate impact""","""We analyze (i) the extent of predictive multiplicity among state-of-the-art LLMs used for detecting toxic content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) how model multiplicity compares to unambiguous human classifications."" Keyphrase: ""Predictive multiplicity and disparate impact""",3
arxiv2024,A Survey of Large Language Models in Cybersecurity,Yes.,5,"""This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome.""",2024,2024-02-26T19:06:02Z,"Keyphrase: ""Limited application in cybersecurity""","""This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome."" Keyphrase: ""Limited application in cybersecurity""",7
arxiv2024,"Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding",Yes.,5,"""However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications.""",2024,2024-02-26T18:59:28Z,"Keyphrase: ""High deployment cost and latency issues""","""However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications."" Keyphrase: ""High deployment cost and latency issues""",4
arxiv2024,MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT,Yes.,5,"""However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency.""",2024,2024-02-26T18:59:03Z,"Keyphrase: ""Limited suitability for on-device processing""","""However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency."" Keyphrase: ""Limited suitability for on-device processing""",4
arxiv2024,Eight Methods to Evaluate Robust Unlearning in LLMs,Yes.,4,"""we first survey techniques and limitations of existing unlearning evaluations"" and ""Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.""",2024,2024-02-26T18:57:37Z,"Keyphrase: ""Limited unlearning evaluation""","""we first survey techniques and limitations of existing unlearning evaluations"" and ""Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics."" Keyphrase: ""Limited unlearning evaluation""",0
arxiv2024,A Surprising Failure? Multimodal LLMs and the NLVR Challenge,Yes.,5,"""Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.""",2024,2024-02-26T18:37:18Z,"Keyphrase: ""Poor compositional spatial reasoning""","""Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases."" Keyphrase: ""Poor compositional spatial reasoning""",1
arxiv2024,A Comprehensive Evaluation of Quantization Strategies for Large Language Models,Yes.,4,"""Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings."" and ""Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs.""",2024,2024-02-26T17:45:36Z,"Keyphrase: ""Resource-intensive deployment""","""Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings."" and ""Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs."" Keyphrase: ""Resource-intensive deployment""",4
arxiv2024,Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study,Yes.,5,"""they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes.""",2024,2024-02-26T16:05:33Z,"Keyphrase: ""Input sequence length constraint""","""they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes."" Keyphrase: ""Input sequence length constraint""",4
arxiv2024,StructLM: Towards Building Generalist Models for Structured Knowledge Grounding,Yes.,4,"""Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%.""",2024,2024-02-26T15:47:01Z,"Keyphrase: ""Limited ability with structured data""","""Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%."" Keyphrase: ""Limited ability with structured data""",6
arxiv2024,Long-Context Language Modeling with Parallel Context Encoding,Yes.,5,"""the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window"" and ""while existing long-context models degenerate with retrieved contexts.""",2024,2024-02-26T14:47:35Z,"Keyphrase: ""Limited contextual window""","""the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window"" and ""while existing long-context models degenerate with retrieved contexts."" Keyphrase: ""Limited contextual window""",4
arxiv2024,LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments,Yes.,4,"""showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration.""",2024,2024-02-26T11:31:48Z,"Keyphrase: ""Limited autonomy and opponent modeling""","""showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration."" Keyphrase: ""Limited autonomy and opponent modeling""",1
arxiv2024,Defending LLMs against Jailbreaking Attacks via Backtranslation,Yes.,4,"""Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent.""",2024,2024-02-26T10:03:33Z,"Keyphrase: ""Vulnerability to malicious intent""","""Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent."" Keyphrase: ""Vulnerability to malicious intent""",2
arxiv2024,"ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",Yes.,4,"""The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner.""",2024,2024-02-26T09:43:02Z,"Keyphrase: ""Limited safety detection""","""The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner."" Keyphrase: ""Limited safety detection""",2
arxiv2024,From RAGs to riches: Using large language models to write documents for clinical trials,Yes.,5,"""however there are concerns about the quality of their output"" and ""deficiencies remain",2024,2024-02-26T08:59:05Z,"Keyphrase: ""Output quality deficiency""","""however there are concerns about the quality of their output"" and ""deficiencies remain Keyphrase: ""Output quality deficiency""",4
arxiv2024,Improving LLM-based Machine Translation with Systematic Self-Correction,Yes.,4,"""However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors.""",2024,2024-02-26T07:58:12Z,"Keyphrase: ""Translation errors""","""However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors."" Keyphrase: ""Translation errors""",6
arxiv2024,HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs,Yes.,5,"""Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs.""",2024,2024-02-25T22:23:37Z,"Keyphrase: ""Reliability and alignment challenges""","""Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs."" Keyphrase: ""Reliability and alignment challenges""",0
arxiv2024,DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers,Yes.,5,"""The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks,"" and ""current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned L",2024,2024-02-25T17:43:29Z,"Keyphrase: ""Vulnerability to jailbreak attacks""","""The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks,"" and ""current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned L Keyphrase: ""Vulnerability to jailbreak attacks""",2
arxiv2024,Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions,Yes.,4,"""generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate"" and ""retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval.""",2024,2024-02-25T11:22:19Z,"Keyphrase: ""Susceptibility to factual errors""","""generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate"" and ""retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval."" Keyphrase: ""Susceptibility to factual errors""",0
arxiv2024,Cognitive Bias in High-Stakes Decision-Making with LLMs,Yes.,4,"""LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance.""",2024,2024-02-25T02:35:56Z,"Keyphrase: ""Biases in decision-making""","""LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance."" Keyphrase: ""Biases in decision-making""",3
arxiv2024,Rethinking Software Engineering in the Foundation Model Era: A Curated Catalogue of Challenges in the Development of Trustworthy FMware,Yes.,5,"""The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges.""",2024,2024-02-25T00:53:16Z,"Keyphrase: ""Orchestration challenges and hallucination""","""The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges."" Keyphrase: ""Orchestration challenges and hallucination""",0
arxiv2024,Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis,Yes.,5,"""The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm.""",2024,2024-02-24T21:36:26Z,"Keyphrase: ""Limited long-term quantitative reasoning""","""The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm."" Keyphrase: ""Limited long-term quantitative reasoning""",1
arxiv2024,PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails,Yes.,5,"""Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content."" and ""Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective.""",2024,2024-02-24T21:27:13Z,"Keyphrase: ""Vulnerability to automated attacks""","""Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content."" and ""Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective."" Keyphrase: ""Vulnerability to automated attacks""",2
arxiv2024,Prompt Perturbation Consistency Learning for Robust Language Models,Yes.,5,"""their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts.""",2024,2024-02-24T15:00:58Z,"Keyphrase: ""Poor performance on sequence labeling tasks""","""their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts."" Keyphrase: ""Poor performance on sequence labeling tasks""",6
arxiv2024,Empowering Large Language Model Agents through Action Learning,Yes.,5,"""Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior.""",2024,2024-02-24T13:13:04Z,"Keyphrase: ""Limited trial and error learning""","""Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior."" Keyphrase: ""Limited trial and error learning""",5
arxiv2024,Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models,Yes.,4,"""Large language models (LLMs) still grapple with complex tasks like mathematical reasoning.""",2024,2024-02-24T08:40:30Z,"Keyphrase: ""Struggles with mathematical reasoning""","""Large language models (LLMs) still grapple with complex tasks like mathematical reasoning."" Keyphrase: ""Struggles with mathematical reasoning""",1
arxiv2024,Stepwise Self-Consistent Mathematical Reasoning with Large Language Models,Yes.,4,"""Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions.""",2024,2024-02-24T08:22:39Z,"Keyphrase: ""Limited multistep reasoning""","""Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions."" Keyphrase: ""Limited multistep reasoning""",1
arxiv2024,HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition,Yes.,4,"""the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria.""",2024,2024-02-24T08:01:32Z,"Keyphrase: ""Limited evaluation scope and potential bias""","""the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria."" Keyphrase: ""Limited evaluation scope and potential bias""",3
arxiv2024,Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology,Yes.,5,"""Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks.""",2024,2024-02-24T02:27:55Z,"Keyphrase: ""Vulnerability to jailbreaking attacks""","""Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks."" Keyphrase: ""Vulnerability to jailbreaking attacks""",2
arxiv2024,Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study,Yes.,4,"""The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels",2024,2024-02-24T00:38:29Z,"Keyphrase: ""Performance limitations with data augmentation""","""The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels Keyphrase: ""Performance limitations with data augmentation""",6
arxiv2024,Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics,Yes.,5,"""demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task.""",2024,2024-02-24T00:01:01Z,"Keyphrase: ""Limited world knowledge integration""","""demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task."" Keyphrase: ""Limited world knowledge integration""",6
arxiv2024,DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures,Yes.,4,"""Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web",2024,2024-02-23T20:10:18Z,"Keyphrase: ""Limited web-based representation""","""Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web Keyphrase: ""Limited web-based representation""",3
arxiv2024,The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG),Yes.,4,"""Whereas extensive research has demonstrated the privacy risks of large language models (LLMs),"" and ""posing new privacy issues that are currently under-explored.""",2024,2024-02-23T18:35:15Z,"Keyphrase: ""Privacy risks""","""Whereas extensive research has demonstrated the privacy risks of large language models (LLMs),"" and ""posing new privacy issues that are currently under-explored."" Keyphrase: ""Privacy risks""",8
arxiv2024,Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models,Yes.,4,"""The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability."" and ""we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently",2024,2024-02-23T18:15:56Z,"Keyphrase: ""Discrimination and bias amplification""","""The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability."" and ""we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently Keyphrase: ""Discrimination and bias amplification""",3
arxiv2024,Explorations of Self-Repair in Language Models,Yes.,4,"""We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect).""",2024,2024-02-23T15:42:12Z,"Keyphrase: ""Inconsistent self-repair mechanisms""","""We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect)."" Keyphrase: ""Inconsistent self-repair mechanisms""",5
arxiv2024,How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries,Yes.,5,"""Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation.""",2024,2024-02-23T13:03:12Z,"Keyphrase: ""Vulnerability to unethical content generation""","""Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation."" Keyphrase: ""Vulnerability to unethical content generation""",2
arxiv2024,Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models,Yes.,4,"""these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility.""",2024,2024-02-23T09:04:48Z,"Keyphrase: ""Production of toxic content""","""these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility."" Keyphrase: ""Production of toxic content""",2
arxiv2024,A First Look at GPT Apps: Landscape and Vulnerability,Yes.,4,"""LLMs' susceptibility to attacks raises concerns over safety and plagiarism.""",2024,2024-02-23T05:30:32Z,"Keyphrase: ""Susceptibility to attacks""","""LLMs' susceptibility to attacks raises concerns over safety and plagiarism."" Keyphrase: ""Susceptibility to attacks""",2
arxiv2024,Studying LLM Performance on Closed- and Open-source Data,Yes.,5,"""These models are extremely data-hungry,"" ""do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds?"" and ""We find that performance for C# changes little from",2024,2024-02-23T05:17:28Z,"Keyphrase: ""High data dependency""","""These models are extremely data-hungry,"" ""do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds?"" and ""We find that performance for C# changes little from Keyphrase: ""High data dependency""",7
arxiv2024,AttributionBench: How Hard is Automatic Attribution Evaluation?,Yes.,5,"""our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model's inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do.""",2024,2024-02-23T04:23:33Z,"Keyphrase: ""Limited nuanced information processing""","""our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model's inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do."" Keyphrase: ""Limited nuanced information processing""",1
arxiv2024,Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions,Yes.,4,"""they often display a considerable level of overconfidence even when the question does not have a definitive answer"" and ""avoid providing hallucinated answers to these unknown questions.""",2024,2024-02-23T02:24:36Z,"Keyphrase: ""Overconfidence and Hallucination""","""they often display a considerable level of overconfidence even when the question does not have a definitive answer"" and ""avoid providing hallucinated answers to these unknown questions."" Keyphrase: ""Overconfidence and Hallucination""",0
arxiv2024,ToMBench: Benchmarking Theory of Mind in Large Language Models,Yes.,5,"""We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet.""",2024,2024-02-23T02:05:46Z,"Keyphrase: ""Lag behind human performance""","""We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet."" Keyphrase: ""Lag behind human performance""",7
arxiv2024,KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models,Yes.,5,"""Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness."" and ""We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.""",2024,2024-02-23T01:30:39Z,"Keyphrase: ""Data contamination hindrance""","""Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness."" and ""We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning."" Keyphrase: ""Data contamination hindrance""",0
arxiv2024,Unintended Impacts of LLM Alignment on Global Representation,Yes.,4,"""We explore how alignment impacts performance along three axes of global representation",2024,2024-02-22T23:31:22Z,"Keyphrase: ""Limited global representation""","""We explore how alignment impacts performance along three axes of global representation Keyphrase: ""Limited global representation""",6
arxiv2024,Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning,Yes.,4,"""recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback.""",2024,2024-02-22T20:57:17Z,"Keyphrase: ""Limited knowledge retention""","""recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback."" Keyphrase: ""Limited knowledge retention""",5
arxiv2024,RelayAttention for Efficient Large Language Model Serving with Long System Prompts,Yes.,5,"""However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length.""",2024,2024-02-22T18:58:28Z,"Keyphrase: ""Throughput and latency bottleneck""","""However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length."" Keyphrase: ""Throughput and latency bottleneck""",4
arxiv2024,Identifying Multiple Personalities in Large Language Models with External Evaluation,Yes.,4,"""Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs."" and ""This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans.""",2024,2024-02-22T18:57:20Z,"Keyphrase: ""Inconsistent personality representation""","""Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs."" and ""This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans."" Keyphrase: ""Inconsistent personality representation""",0
arxiv2024,MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues,Yes.,4,"""comprehensively evaluating the dialogue abilities of LLMs remains a challenge"" and ""neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs.""",2024,2024-02-22T18:21:59Z,"Keyphrase: ""Challenges in dialogue evaluation""","""comprehensively evaluating the dialogue abilities of LLMs remains a challenge"" and ""neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs."" Keyphrase: ""Challenges in dialogue evaluation""",6
arxiv2024,Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images,Yes.,5,"""they are still vulnerable to adversarial images"" and ""lack of study regarding MLLMs' adversarial robustness with CoT"" and ""finding that CoT marginally improves adversarial robustness against existing attack methods"" and ""introduce a novel",2024,2024-02-22T17:36:34Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""they are still vulnerable to adversarial images"" and ""lack of study regarding MLLMs' adversarial robustness with CoT"" and ""finding that CoT marginally improves adversarial robustness against existing attack methods"" and ""introduce a novel Keyphrase: ""Vulnerability to adversarial attacks""",2
arxiv2024,UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models,Yes.,5,"""Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or hallucination."" and ""Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources",2024,2024-02-22T16:45:32Z,"Keyphrase: ""Factual inaccuracy and hallucination""","""Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or hallucination."" and ""Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources Keyphrase: ""Factual inaccuracy and hallucination""",0
arxiv2024,Visual Hallucinations of Multi-modal Large Language Models,Yes.,5,"""Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances."" and ""We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 halluc",2024,2024-02-22T16:40:33Z,"Keyphrase: ""Limited diversity in training data""","""Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances."" and ""We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 halluc Keyphrase: ""Limited diversity in training data""",1
arxiv2024,Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis,Yes.,5,"""Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance",2024,2024-02-22T12:19:04Z,"Keyphrase: ""Limited performance without extensive training data""","""Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance Keyphrase: ""Limited performance without extensive training data""",6
arxiv2024,COBIAS: Contextual Reliability in Bias Assessment,Yes.,4,"""Large Language Models (LLMs) are trained on inherently biased data"" and ""highlighting a critical need for contextual exploration.""",2024,2024-02-22T10:46:11Z,"Keyphrase: ""Inherent bias from training data""","""Large Language Models (LLMs) are trained on inherently biased data"" and ""highlighting a critical need for contextual exploration."" Keyphrase: ""Inherent bias from training data""",3
arxiv2024,Understanding and Patching Compositional Reasoning in LLMs,Yes.,5,"""LLMs have marked a revolutionary shift, yet they falter when faced with compositional reasoning tasks.""",2024,2024-02-22T06:47:56Z,"Keyphrase: ""Falter in compositional reasoning""","""LLMs have marked a revolutionary shift, yet they falter when faced with compositional reasoning tasks."" Keyphrase: ""Falter in compositional reasoning""",1
arxiv2024,Mitigating Biases of Large Language Models in Stance Detection with Calibration,Yes.,4,"""LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance.""",2024,2024-02-22T05:17:49Z,"Keyphrase: ""Biased stance due to sentiment-topic correlation""","""LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance."" Keyphrase: ""Biased stance due to sentiment-topic correlation""",3
arxiv2024,Can Language Models Act as Knowledge Bases at Scale?,Yes.,5,"""However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable.""",2024,2024-02-22T04:20:14Z,"Keyphrase: ""Limited factual coverage""","""However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable."" Keyphrase: ""Limited factual coverage""",0
arxiv2024,Eagle: Ethical Dataset Given from Real Interactions,Yes.,5,"""Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content.""",2024,2024-02-22T03:46:02Z,"Keyphrase: ""Ethical and social bias""","""Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content."" Keyphrase: ""Ethical and social bias""",3
arxiv2024,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Yes.,5,"""state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome.""",2024,2024-02-22T01:02:37Z,"Keyphrase: ""Limited conversational prediction""","""state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome."" Keyphrase: ""Limited conversational prediction""",4
arxiv2024,Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models,Yes.,5,"""existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.""",2024,2024-02-22T00:41:23Z,"Keyphrase: ""Challenges in integrating structured data""","""existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial."" Keyphrase: ""Challenges in integrating structured data""",6
arxiv2024,Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization,Yes.,5,"""Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability."" and ""Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general",2024,2024-02-22T00:01:02Z,"Keyphrase: ""Lack of explainability""","""Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability."" and ""Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general Keyphrase: ""Lack of explainability""",7
arxiv2024,MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms,Yes.,4,"""MLLMs have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation."" and ""Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks.""",2024,2024-02-21T22:27:40Z,"Keyphrase: ""Difficulty handling human emotion and complex content""","""MLLMs have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation."" and ""Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks."" Keyphrase: ""Difficulty handling human emotion and complex content""",3
arxiv2024,Coercing LLMs to do and reveal (almost) anything,Yes.,5,"""adversarial attacks on large language models (LLMs) can 'jailbreak' the model into making harmful statements"" and ""we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.""",2024,2024-02-21T18:59:13Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""adversarial attacks on large language models (LLMs) can 'jailbreak' the model into making harmful statements"" and ""we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arxiv2024,Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment,Yes.,5,"""no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs,"" ""both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks,"" and ""Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before",2024,2024-02-21T18:55:20Z,"Keyphrase: ""Vulnerability to manipulation""","""no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs,"" ""both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks,"" and ""Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before Keyphrase: ""Vulnerability to manipulation""",2
arxiv2024,OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,Yes.,5,"""Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies.""",2024,2024-02-21T18:49:26Z,"Keyphrase: ""Hallucination and knowledge omission""","""Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies."" Keyphrase: ""Hallucination and knowledge omission""",0
arxiv2024,What's in a Name? Auditing Large Language Models for Race and Gender Bias,Yes.,5,"""We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4."" and ""We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women."" and ""Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for harm against marginalized communities.""",2024,2024-02-21T18:25:25Z,"Keyphrase: ""Bias in LLMs""","""We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4."" and ""We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women."" and ""Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for harm against marginalized communities."" Keyphrase: ""Bias in LLMs""",3
arxiv2024,Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content,Yes.,5,"""The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs.""",2024,2024-02-21T16:46:36Z,"Keyphrase: ""Generating toxic content""","""The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs."" Keyphrase: ""Generating toxic content""",2
arxiv2024,SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization,Yes.,4,"""Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences.""",2024,2024-02-21T16:33:22Z,"Keyphrase: ""Factual inaccuracy""","""Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences."" Keyphrase: ""Factual inaccuracy""",0
arxiv2024,Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs,Yes.,5,"""Large Language Models (LLMs)... are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs."" and ""existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds.""",2024,2024-02-21T15:13:50Z,"Keyphrase: ""Vulnerability to jailbreak attacks""","""Large Language Models (LLMs)... are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs."" and ""existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds."" Keyphrase: ""Vulnerability to jailbreak attacks""",2
arxiv2024,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,Yes.,5,"""due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens.""",2024,2024-02-21T12:30:33Z,"Keyphrase: ""Limited context window""","""due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens."" Keyphrase: ""Limited context window""",4
arxiv2024,$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens,Yes.,5,"""The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context.""",2024,2024-02-21T11:30:29Z,"Keyphrase: ""Limitation in processing long contexts""","""The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context."" Keyphrase: ""Limitation in processing long contexts""",4
arxiv2024,SaGE: Evaluating Moral Consistency in Large Language Models,Yes.,5,"""we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general)."" and ""Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.""",2024,2024-02-21T11:23:21Z,"Keyphrase: ""Morally inconsistent generation""","""we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general)."" and ""Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further."" Keyphrase: ""Morally inconsistent generation""",3
arxiv2024,CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models,Yes.,5,"""Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images.""",2024,2024-02-21T08:21:12Z,"Keyphrase: ""Struggle with contextual information""","""Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images."" Keyphrase: ""Struggle with contextual information""",5
arxiv2024,A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models,Yes.,5,"""The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability.""",2024,2024-02-21T08:20:06Z,"Keyphrase: ""Overconfidence in predictions""","""The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability."" Keyphrase: ""Overconfidence in predictions""",0
arxiv2024,Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues,Yes.,5,"""Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous.""",2024,2024-02-21T06:11:03Z,"Keyphrase: ""Poor correlation with human judgment""","""Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous."" Keyphrase: ""Poor correlation with human judgment""",1
arxiv2024,LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs,Yes.,5,"""this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions.""",2024,2024-02-21T05:56:52Z,"Keyphrase: ""High computational cost and visual token aggregation challenges""","""this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions."" Keyphrase: ""High computational cost and visual token aggregation challenges""",4
arxiv2024,FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing,Yes.,5,"""Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and",2024,2024-02-21T05:03:17Z,"Keyphrase: ""High computational demands""","""Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and Keyphrase: ""High computational demands""",4
arxiv2024,RITFIS: Robust input testing framework for LLMs-based intelligent software,Yes.,4,"""existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models.""",2024,2024-02-21T04:00:54Z,"Keyphrase: ""Challenges with lengthy and complex text""","""existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models."" Keyphrase: ""Challenges with lengthy and complex text""",4
arxiv2024,Round Trip Translation Defence against Large Language Model Jailbreaking Attacks,Yes.,4,"""Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most.""",2024,2024-02-21T03:59:52Z,"Keyphrase: ""Susceptible to social engineering attacks""","""Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most."" Keyphrase: ""Susceptible to social engineering attacks""",2
arxiv2024,From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers,Yes.,5,"""This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text.""",2024,2024-02-21T03:51:34Z,"Keyphrase: ""Repetitive text generation""","""This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text."" Keyphrase: ""Repetitive text generation""",6
arxiv2024,Potential and Challenges of Model Editing for Social Debiasing,Yes.,5,"""Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases."" and ""Our findings in three scenarios reveal both the potential and challenges of debias editing",2024,2024-02-21T01:35:26Z,"Keyphrase: ""Inevitable stereotype bias""","""Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases."" and ""Our findings in three scenarios reveal both the potential and challenges of debias editing Keyphrase: ""Inevitable stereotype bias""",3
arxiv2024,Learning to Poison Large Language Models During Instruction Tuning,Yes.,5,"""Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes.""",2024,2024-02-21T01:30:03Z,"Keyphrase: ""Vulnerability to data poisoning attacks""","""Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes."" Keyphrase: ""Vulnerability to data poisoning attacks""",2
arxiv2024,LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study,Yes.,5,"""the phenomenon of 'jailbreaking', where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge.""",2024,2024-02-21T01:26:39Z,"Keyphrase: ""Harmful responses""","""the phenomenon of 'jailbreaking', where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge."" Keyphrase: ""Harmful responses""",2
arxiv2024,CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory,Yes.,5,"""Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs.""",2024,2024-02-21T01:00:17Z,"Keyphrase: ""Struggles with long input sequences""","""Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs."" Keyphrase: ""Struggles with long input sequences""",4
arxiv2024,Large Language Models for Data Annotation: A Survey,Yes.,4,"""Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation.""",2024,2024-02-21T00:44:04Z,"Keyphrase: ""Challenges with LLM data annotation""","""Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation."" Keyphrase: ""Challenges with LLM data annotation""",1
arxiv2024,The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative,Yes.,5,"""This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs"" and ""Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications.""",2024,2024-02-20T23:08:21Z,"Keyphrase: ""Security risks and covert manipulation""","""This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs"" and ""Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications."" Keyphrase: ""Security risks and covert manipulation""",2
arxiv2024,Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text,Yes.,5,"""Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors.""",2024,2024-02-20T22:56:23Z,"Keyphrase: ""Difficulty with complex multistep reasoning""","""Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors."" Keyphrase: ""Difficulty with complex multistep reasoning""",1
arxiv2024,A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction,Yes.,5,"""their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE).""",2024,2024-02-20T20:42:02Z,"Keyphrase: ""Inconsistent performance in structured text formats""","""their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE)."" Keyphrase: ""Inconsistent performance in structured text formats""",1
arxiv2024,TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization,Yes.,5,"""Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size."" and ""when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics.""",2024,2024-02-20T18:58:49Z,"Keyphrase: ""Factual errors and hallucinations""","""Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size."" and ""when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics."" Keyphrase: ""Factual errors and hallucinations""",0
arxiv2024,Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive,Yes.,4,"""first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples,"" and ""we then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which",2024,2024-02-20T18:42:34Z,"Keyphrase: ""Text reduction and likelihood preference""","""first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples,"" and ""we then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which Keyphrase: ""Text reduction and likelihood preference""",5
arxiv2024,How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts,Yes.,5,"""The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions.""",2024,2024-02-20T18:31:27Z,"Keyphrase: ""Deceptive hallucinated responses""","""The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions."" Keyphrase: ""Deceptive hallucinated responses""",0
arxiv2024,Benchmarking Retrieval-Augmented Generation for Medicine,Yes.,5,"""While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge.""",2024,2024-02-20T17:44:06Z,"Keyphrase: ""Hallucination and outdated knowledge""","""While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge."" Keyphrase: ""Hallucination and outdated knowledge""",0
arxiv2024,Is the System Message Really Important to Jailbreaks in Large Language Models?,Yes.,4,"""This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions."" and ""we explore the transferability of jailbreak across LLMs.""",2024,2024-02-20T17:39:40Z,"Keyphrase: ""Vulnerability to malicious prompts""","""This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions."" and ""we explore the transferability of jailbreak across LLMs."" Keyphrase: ""Vulnerability to malicious prompts""",3
arxiv2024,ELAD: Explanation-Guided Large Language Models Active Distillation,Yes.,5,"""The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences.""",2024,2024-02-20T15:47:59Z,"Keyphrase: ""Memory inefficiency and high computational costs""","""The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences."" Keyphrase: ""Memory inefficiency and high computational costs""",4
arxiv2024,Understanding the effects of language-specific class imbalance in multilingual fine-tuning,Yes.,4,"""We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features.""",2024,2024-02-20T13:59:12Z,"Keyphrase: ""Dataset imbalance affecting performance""","""We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features."" Keyphrase: ""Dataset imbalance affecting performance""",5
arxiv2024,Large Language Model-based Human-Agent Collaboration for Complex Task Solving,Yes.,5,"""LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs.""",2024,2024-02-20T11:03:36Z,"Keyphrase: ""Difficulty in adapting to dynamic environments""","""LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs."" Keyphrase: ""Difficulty in adapting to dynamic environments""",1
arxiv2024,An LLM Maturity Model for Reliable and Transparent Text-to-Query,Yes.,4,"""Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications.""",2024,2024-02-20T06:20:09Z,"Keyphrase: ""Reliability and transparency issues""","""Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications."" Keyphrase: ""Reliability and transparency issues""",6
arxiv2024,Thermometer: Towards Universal Calibration for Large Language Models,Yes.,5,"""Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs"" and ""calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks.""",2024,2024-02-20T04:13:48Z,"Keyphrase: ""Poor calibration and computational requirements""","""Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs"" and ""calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks."" Keyphrase: ""Poor calibration and computational requirements""",4
arxiv2024,The FinBen: An Holistic Financial Benchmark for Large Language Models,Yes.,4,"""The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities.""",2024,2024-02-20T02:16:16Z,"Keyphrase: ""Struggles with complex reasoning and forecasting""","""The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities."" Keyphrase: ""Struggles with complex reasoning and forecasting""",1
arxiv2024,Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation,Yes.,4,"""Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs,"" and ""We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.""",2024,2024-02-20T01:49:15Z,"Keyphrase: ""Insufficient bias mitigation""","""Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs,"" and ""We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms."" Keyphrase: ""Insufficient bias mitigation""",3
arxiv2024,GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence,Yes.,5,"""LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance).""",2024,2024-02-19T21:45:55Z,"Keyphrase: ""Dangerous errors in high-stakes applications""","""LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance)."" Keyphrase: ""Dangerous errors in high-stakes applications""",2
arxiv2024,TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness,Yes.,4,"""However, concerns have arisen regarding the trustworthiness of LLMs outputs, particularly in closed-book question-answering tasks, where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information.""",2024,2024-02-19T21:12:14Z,"Keyphrase: ""Trustworthiness of output""","""However, concerns have arisen regarding the trustworthiness of LLMs outputs, particularly in closed-book question-answering tasks, where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information."" Keyphrase: ""Trustworthiness of output""",0
arxiv2024,GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations,Yes.,5,"""We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios.""",2024,2024-02-19T18:23:36Z,"Keyphrase: ""Limited performance in gaming scenarios""","""We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios."" Keyphrase: ""Limited performance in gaming scenarios""",7
arxiv2024,Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!,Yes.,5,"""this paper introduces an inference-time attack method, demonstrating that safety alignment can be easily reversed to produce harmful language models without additional training.""",2024,2024-02-19T18:16:51Z,"Keyphrase: ""Vulnerability to inference-time attacks""","""this paper introduces an inference-time attack method, demonstrating that safety alignment can be easily reversed to produce harmful language models without additional training."" Keyphrase: ""Vulnerability to inference-time attacks""",2
arxiv2024,Large Language Model for Mental Health: A Systematic Review,Yes.,4,"""Findings reveal LLMs' effectiveness in mental health issue detection and the enhancement of telepsychological services through personalised healthcare. Nonetheless, risks like text inconsistencies, hallucinatory content, and the lack of an ethical framework raise concerns about their clinical use.""",2024,2024-02-19T17:58:41Z,"Keyphrase: ""Ethical concerns and hallucinatory content""","""Findings reveal LLMs' effectiveness in mental health issue detection and the enhancement of telepsychological services through personalised healthcare. Nonetheless, risks like text inconsistencies, hallucinatory content, and the lack of an ethical framework raise concerns about their clinical use."" Keyphrase: ""Ethical concerns and hallucinatory content""",0
arxiv2024,Polarization of Autonomous Generative AI Agents Under Echo Chambers,Yes.,4,"""We investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment."" and ""we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments.""",2024,2024-02-19T15:14:15Z,"Keyphrase: ""Polarization in echo chambers""","""We investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment."" and ""we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments."" Keyphrase: ""Polarization in echo chambers""",3
arxiv2024,Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One,Yes.,4,"""LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases.""",2024,2024-02-19T14:02:22Z,"Keyphrase: ""Dominant viewpoint bias""","""LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases."" Keyphrase: ""Dominant viewpoint bias""",3
arxiv2024,Purifying Large Language Models by Ensembling a Small Language Model,Yes.,5,"""well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs.""",2024,2024-02-19T14:00:39Z,"Keyphrase: ""Copyright infringement and privacy violations""","""well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs."" Keyphrase: ""Copyright infringement and privacy violations""",8
arxiv2024,Do Large Language Models Understand Logic or Just Mimick Context?,Yes.,5,"""Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can",2024,2024-02-19T12:12:35Z,"Keyphrase: ""Limited understanding of logical rules""","""Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can Keyphrase: ""Limited understanding of logical rules""",1
arxiv2024,Can LLMs Compute with Reasons?,Yes.,5,"""Large language models (LLMs) often struggle with complex mathematical tasks, prone to 'hallucinating' incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data.""",2024,2024-02-19T12:04:25Z,"Keyphrase: ""Struggles with complex mathematical tasks""","""Large language models (LLMs) often struggle with complex mathematical tasks, prone to 'hallucinating' incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data."" Keyphrase: ""Struggles with complex mathematical tasks""",1
arxiv2024,EmoBench: Evaluating the Emotional Intelligence of Large Language Models,Yes.,4,"""Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research.""",2024,2024-02-19T11:48:09Z,"Keyphrase: ""Gap between LLM and human understanding""","""Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research."" Keyphrase: ""Gap between LLM and human understanding""",7
arxiv2024,Are LLM-based Evaluators Confusing NLG Quality Criteria?,Yes.,5,"""we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability"" and ""Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.""",2024,2024-02-19T11:19:02Z,"Keyphrase: ""Confusion in evaluation criteria""","""we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability"" and ""Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation."" Keyphrase: ""Confusion in evaluation criteria""",0
arxiv2024,Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models,Yes.,5,"""Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks.""",2024,2024-02-19T11:02:05Z,"Keyphrase: ""Catastrophic forgetting""","""Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks."" Keyphrase: ""Catastrophic forgetting""",5
arxiv2024,Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs,Yes.,4,"""Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility."" and ""these methods based on logits often require both teacher and student models to share the same tokenizer",2024,2024-02-19T10:37:29Z,"Keyphrase: ""Practical constraints in deployment""","""Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility."" and ""these methods based on logits often require both teacher and student models to share the same tokenizer Keyphrase: ""Practical constraints in deployment""",4
arxiv2024,Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought,Yes.,5,"""GPT-4 performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection."" and ""These experimental results indicate that GPT-4 lacks the ability to detect smart contract vulnerabilities effectively.""",2024,2024-02-19T10:33:29Z,"Keyphrase: ""Ineffective vulnerability detection""","""GPT-4 performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection."" and ""These experimental results indicate that GPT-4 lacks the ability to detect smart contract vulnerabilities effectively."" Keyphrase: ""Ineffective vulnerability detection""",2
arxiv2024,Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models,Yes.,5,"""However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored."" and ""Analysis shows that approximately 20% of the failures are attributed to shortcuts.""",2024,2024-02-19T07:34:10Z,"Keyphrase: ""Limited reasoning capabilities""","""However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored."" and ""Analysis shows that approximately 20% of the failures are attributed to shortcuts."" Keyphrase: ""Limited reasoning capabilities""",1
arxiv2024,Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint,Yes.,4,"""This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the parametric knowledge internalized during pre-training.""",2024,2024-02-19T07:10:30Z,"Keyphrase: ""Conflicting knowledge sources""","""This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the parametric knowledge internalized during pre-training."" Keyphrase: ""Conflicting knowledge sources""",5
arxiv2024,RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning,Yes.,4,"""Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities."" and ""The overall performance of existing LMMs is still limited; however LMMs more robust to low",2024,2024-02-19T06:57:02Z,"Keyphrase: ""Limited specialized reasoning""","""Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities."" and ""The overall performance of existing LMMs is still limited; however LMMs more robust to low Keyphrase: ""Limited specialized reasoning""",1
arxiv2024,The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth,Yes.,4,"""We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice.""",2024,2024-02-19T06:54:55Z,"Keyphrase: ""Lack of personalization""","""We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice."" Keyphrase: ""Lack of personalization""",0
arxiv2024,Head-wise Shareable Attention for Large Language Models,Yes.,4,"""Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices.""",2024,2024-02-19T04:19:36Z,"Keyphrase: ""Deployment restrictions on edge devices""","""Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices."" Keyphrase: ""Deployment restrictions on edge devices""",4
arxiv2024,ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs,Yes.,4,"""Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities.""",2024,2024-02-19T01:28:48Z,"Keyphrase: ""Harmful social biases""","""Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities."" Keyphrase: ""Harmful social biases""",3
arxiv2024,"Large Language Models for Stemming: Promises, Pitfalls and Failures",Yes.,5,"""We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions.""",2024,2024-02-19T01:11:44Z,"Keyphrase: ""Limited effectiveness of contextual stemming""","""We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions."" Keyphrase: ""Limited effectiveness of contextual stemming""",1
arxiv2024,MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs,Yes.,5,"""However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments.""",2024,2024-02-19T01:04:22Z,"Keyphrase: ""Inaccurate and misleading output""","""However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments."" Keyphrase: ""Inaccurate and misleading output""",0
arxiv2024,SPML: A DSL for Defending Language Models Against Prompt Attacks,Yes.,4,"""post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users,"" and ""Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored.""",2024,2024-02-19T00:53:48Z,"Keyphrase: ""Vulnerability to malicious attacks""","""post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users,"" and ""Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored."" Keyphrase: ""Vulnerability to malicious attacks""",2
arxiv2024,ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs,Yes.,5,"""We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art.""",2024,2024-02-19T00:43:31Z,"Keyphrase: ""Struggles with visual prompts""","""We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art."" Keyphrase: ""Struggles with visual prompts""",6
arxiv2024,Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic,Yes.,5,"""Aligned language models face a significant limitation as their fine-tuning often results in compromised safety.""",2024,2024-02-19T00:18:09Z,"Keyphrase: ""Safety compromises in fine-tuning""","""Aligned language models face a significant limitation as their fine-tuning often results in compromised safety."" Keyphrase: ""Safety compromises in fine-tuning""",2
arxiv2024,How Susceptible are Large Language Models to Ideological Manipulation?,Yes.,5,"""Our findings reveal a concerning vulnerability",2024,2024-02-18T22:36:19Z,"Keyphrase: ""Vulnerability concerns""","""Our findings reveal a concerning vulnerability Keyphrase: ""Vulnerability concerns""",2
arxiv2024,Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable,Yes.,4,"""Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases."" and ""output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability.""",2024,2024-02-18T21:10:18Z,"Keyphrase: ""Factual inconsistency and bias""","""Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases."" and ""output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability."" Keyphrase: ""Factual inconsistency and bias""",0
arxiv2024,Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation,Yes.,5,"""Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.""",2024,2024-02-18T20:48:09Z,"Keyphrase: ""Limited production-ready code generation""","""Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development."" Keyphrase: ""Limited production-ready code generation""",7
arxiv2024,Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers,Yes.,4,"""the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking.""",2024,2024-02-18T20:47:10Z,"Keyphrase: ""Resource-intensive storage and processing""","""the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking."" Keyphrase: ""Resource-intensive storage and processing""",4
arxiv2024,Stealthy Attack on Large Language Model based Recommendation,Yes.,5,"""we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items.""",2024,2024-02-18T16:51:02Z,"Keyphrase: ""Security vulnerability due to textual emphasis""","""we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items."" Keyphrase: ""Security vulnerability due to textual emphasis""",2
arxiv2024,Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark,Yes.,5,"""the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge.""",2024,2024-02-18T14:08:48Z,"Keyphrase: ""Memory overhead challenge""","""the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge."" Keyphrase: ""Memory overhead challenge""",4
arxiv2024,LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration,Yes.,5,"""LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the",2024,2024-02-18T11:46:52Z,"Keyphrase: ""High computational cost""","""LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the Keyphrase: ""High computational cost""",4
arxiv2024,KMMLU: Measuring Massive Multitask Language Understanding in Korean,Yes.,4,"""identifying significant room for improvement,"" ""Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse,"" and ""even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean",2024,2024-02-18T11:41:07Z,"Keyphrase: ""Performance gap in non-English languages""","""identifying significant room for improvement,"" ""Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse,"" and ""even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean Keyphrase: ""Performance gap in non-English languages""",6
arxiv2024,From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings,Yes.,4,"""Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias.""",2024,2024-02-18T08:53:41Z,"Keyphrase: ""Learned biases""","""Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias."" Keyphrase: ""Learned biases""",3
arxiv2024,What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs,Yes.,5,"""we demonstrate through experimentation that LLMs lack necessary skills required for planning.""",2024,2024-02-18T07:42:49Z,"Keyphrase: ""Lack of necessary planning skills""","""we demonstrate through experimentation that LLMs lack necessary skills required for planning."" Keyphrase: ""Lack of necessary planning skills""",1
arxiv2024,MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing,Yes.,4,"""Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored."" and ""we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring",2024,2024-02-18T07:15:03Z,"Keyphrase: ""Limited fine-grained knowledge""","""Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored."" and ""we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring Keyphrase: ""Limited fine-grained knowledge""",6
arxiv2024,When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation,Yes.,5,"""Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases.""",2024,2024-02-18T04:57:19Z,"Keyphrase: ""Limited knowledge possession""","""Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases."" Keyphrase: ""Limited knowledge possession""",6
arxiv2024,Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation,Yes.,5,"""This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations."" and ""Experimental results show a general performance decline in most LLMs against their original results.""",2024,2024-02-18T03:40:06Z,"Keyphrase: ""General performance decline""","""This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations."" and ""Experimental results show a general performance decline in most LLMs against their original results."" Keyphrase: ""General performance decline""",7
arxiv2024,Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs,Yes.,5,"""their mastery of underlying inferential rules still falls short of human capabilities,"" and ""reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns,"" and ""our work sheds light on",2024,2024-02-18T03:38:51Z,"Keyphrase: ""Limited inferential rule mastery""","""their mastery of underlying inferential rules still falls short of human capabilities,"" and ""reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns,"" and ""our work sheds light on Keyphrase: ""Limited inferential rule mastery""",1
arxiv2024,Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models,Yes.,5,"""We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias.""",2024,2024-02-18T03:10:39Z,"Keyphrase: ""Self-bias amplification""","""We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias."" Keyphrase: ""Self-bias amplification""",3
arxiv2024,EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models,Yes.,4,"""EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types.""",2024,2024-02-18T02:41:06Z,"Keyphrase: ""Hallucination with event structure""","""EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types."" Keyphrase: ""Hallucination with event structure""",0
arxiv2024,Aligning Modalities in Vision Large Language Models via Preference Fine-tuning,Yes.,4,"""This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations.""",2024,2024-02-18T00:56:16Z,"Keyphrase: ""Limited factual representation""","""This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations."" Keyphrase: ""Limited factual representation""",0
arxiv2024,Puzzle Solving using Reasoning of Large Language Models: A Survey,Yes.,4,"""identifying significant challenges in complex puzzle scenarios"" and ""highlight the disparity between LLM capabilities and human-like reasoning.""",2024,2024-02-17T14:19:38Z,"Keyphrase: ""Limited humanlike reasoning""","""identifying significant challenges in complex puzzle scenarios"" and ""highlight the disparity between LLM capabilities and human-like reasoning."" Keyphrase: ""Limited humanlike reasoning""",1
arxiv2024,Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,Yes.,4,"""the safety issues of LLM-based agents are currently under-explored"" and ""LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents.""",2024,2024-02-17T06:48:45Z,"Keyphrase: ""Vulnerability to backdoor attacks""","""the safety issues of LLM-based agents are currently under-explored"" and ""LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents."" Keyphrase: ""Vulnerability to backdoor attacks""",2
arxiv2024,Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs,Yes.,5,"""However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT."" and ""we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning",2024,2024-02-17T05:22:56Z,"Keyphrase: ""Lack of reasoning ability""","""However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT."" and ""we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning Keyphrase: ""Lack of reasoning ability""",1
arxiv2024,Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering,Yes.,4,"""Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain."" and ""The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables.""",2024,2024-02-17T05:10:18Z,"Keyphrase: ""Limited handling of complex mathematical scenarios""","""Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain."" and ""The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables."" Keyphrase: ""Limited handling of complex mathematical scenarios""",1
arxiv2024,Disclosure and Mitigation of Gender Bias in LLMs,Yes.,5,"""Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs.""",2024,2024-02-17T04:48:55Z,"Keyphrase: ""Gender bias and stereotypes""","""Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs."" Keyphrase: ""Gender bias and stereotypes""",3
arxiv2024,GenDec: A robust generative Question-decomposition method for Multi-hop reasoning,Yes.,4,"""Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer.""",2024,2024-02-17T02:21:44Z,"Keyphrase: ""Limited multihop reasoning""","""Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer."" Keyphrase: ""Limited multihop reasoning""",1
arxiv2024,Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models,Yes.,5,"""Regrettably, previous studies on ME evaluation have two critical limitations",2024,2024-02-16T23:08:55Z,"Keyphrase: ""Limited evaluation and critical limitations""","""Regrettably, previous studies on ME evaluation have two critical limitations Keyphrase: ""Limited evaluation and critical limitations""",0
arxiv2024,When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models,Yes.,4,"""we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp.""",2024,2024-02-16T22:12:53Z,"Keyphrase: ""Limited reasoning ability""","""we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp."" Keyphrase: ""Limited reasoning ability""",1
arxiv2024,Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives,Yes.,5,"""Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives.""",2024,2024-02-16T19:59:45Z,"Keyphrase: ""Difficulty in inferencing complex relationships""","""Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives."" Keyphrase: ""Difficulty in inferencing complex relationships""",1
arxiv2024,PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering,Yes.,5,"""large language models (LLMs) may have outdated knowledge,"" and ""The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities.""",2024,2024-02-16T19:26:09Z,"Keyphrase: ""Outdated knowledge""","""large language models (LLMs) may have outdated knowledge,"" and ""The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities."" Keyphrase: ""Outdated knowledge""",1
arxiv2024,RLVF: Learning from Verbal Feedback without Overgeneralization,Yes.,5,"""we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant.""",2024,2024-02-16T18:50:24Z,"Keyphrase: ""Overgeneralization of feedback""","""we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant."" Keyphrase: ""Overgeneralization of feedback""",0
arxiv2024,Exploring Value Biases: How LLMs Deviate Towards the Ideal,Yes.,4,"""Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications."" and ""We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars.""",2024,2024-02-16T18:28:43Z,"Keyphrase: ""Unintended biases""","""Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications."" and ""We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars."" Keyphrase: ""Unintended biases""",3
arxiv2024,Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities,Yes.,5,"""For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity."" and ""Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions.""",2024,2024-02-16T17:15:28Z,"Keyphrase: ""Challenges with time series forecasting""","""For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity."" and ""Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions."" Keyphrase: ""Challenges with time series forecasting""",6
arxiv2024,RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model,Yes.,5,"""severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment.""",2024,2024-02-16T16:57:18Z,"Keyphrase: ""Data scarcity and domain gap""","""severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment."" Keyphrase: ""Data scarcity and domain gap""",4
arxiv2024,In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss,Yes.,5,"""Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements.""",2024,2024-02-16T16:15:01Z,"Keyphrase: ""Limited evaluation benchmarks""","""Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements."" Keyphrase: ""Limited evaluation benchmarks""",7
arxiv2024,ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages,Yes.,5,"""Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to.""",2024,2024-02-16T15:19:46Z,"Keyphrase: ""Enduring safety challenges""","""Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to."" Keyphrase: ""Enduring safety challenges""",2
arxiv2024,GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models,Yes.,5,"""traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods,"" ""prompting LLMs with a fixed set of relations or entities can cause hallucinations,"" and ""precision/recall fails to justify the performance of GRE methods.""",2024,2024-02-16T15:01:24Z,"Keyphrase: ""Hallucination in relation extraction""","""traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods,"" ""prompting LLMs with a fixed set of relations or entities can cause hallucinations,"" and ""precision/recall fails to justify the performance of GRE methods."" Keyphrase: ""Hallucination in relation extraction""",0
arxiv2024,Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification,Yes.,5,"""Our results show that ChatGPT struggles in abductive reasoning,"" and ""Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated to distinguish between hype and actual capabilities",2024,2024-02-16T14:52:05Z,"Keyphrase: ""Limited abductive reasoning""","""Our results show that ChatGPT struggles in abductive reasoning,"" and ""Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated to distinguish between hype and actual capabilities Keyphrase: ""Limited abductive reasoning""",1
arxiv2024,An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference,Yes.,4,"""recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs.""",2024,2024-02-16T14:15:15Z,"Keyphrase: ""Inference efficiency deterioration""","""recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs."" Keyphrase: ""Inference efficiency deterioration""",4
arxiv2024,LongHeads: Multi-Head Attention is Secretly a Long Context Processor,Yes.,5,"""Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands.""",2024,2024-02-16T13:39:34Z,"Keyphrase: ""Limited input length processing""","""Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands."" Keyphrase: ""Limited input length processing""",4
arxiv2024,Humans or LLMs as the Judge? A Study on Judgement Biases,Yes.,5,"""Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases.""",2024,2024-02-16T13:21:06Z,"Keyphrase: ""Vulnerability to bias""","""Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases."" Keyphrase: ""Vulnerability to bias""",3
arxiv2024,Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes,Yes.,4,"""current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are 'unreliable' since such processes could contain information unrelated to the answer.""",2024,2024-02-16T13:02:11Z,"Keyphrase: ""Unreliable reasoning process""","""current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are 'unreliable' since such processes could contain information unrelated to the answer."" Keyphrase: ""Unreliable reasoning process""",1
arxiv2024,Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement,Yes.,5,"""existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios."" and ""we find that existing models exhibit a general deficiency in role alignment capabilities.""",2024,2024-02-16T12:12:05Z,"Keyphrase: ""Role alignment deficiency""","""existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios."" and ""we find that existing models exhibit a general deficiency in role alignment capabilities."" Keyphrase: ""Role alignment deficiency""",7
arxiv2024,Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements,Yes.,4,"""existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements.""",2024,2024-02-16T12:00:34Z,"Keyphrase: ""Lack of controllability and biased content""","""existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements."" Keyphrase: ""Lack of controllability and biased content""",3
arxiv2024,Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models,Yes.,5,"""Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations.""",2024,2024-02-16T11:55:40Z,"Keyphrase: ""Internal hallucination""","""Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations."" Keyphrase: ""Internal hallucination""",0
arxiv2024,Jailbreaking Proprietary Large Language Models using Word Substitution Cipher,Yes.,4,"""Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process."" and ""Additionally, we discuss the over-defensiveness of these models.""",2024,2024-02-16T11:37:05Z,"Keyphrase: ""Ethical alignment susceptibility""","""Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process."" and ""Additionally, we discuss the over-defensiveness of these models."" Keyphrase: ""Ethical alignment susceptibility""",2
arxiv2024,InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?,Yes.,4,"""Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions.""",2024,2024-02-16T10:54:10Z,"Keyphrase: ""Societal bias and unfair predictions""","""Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions."" Keyphrase: ""Societal bias and unfair predictions""",3
arxiv2024,I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models,Yes.,4,"""The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group."" and ""this replication unveiled an intrinsic Democratic bias in Large Language Models (LL",2024,2024-02-16T03:54:48Z,"Keyphrase: ""Social identity bias""","""The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group."" and ""this replication unveiled an intrinsic Democratic bias in Large Language Models (LL Keyphrase: ""Social identity bias""",3
arxiv2024,DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection,Yes.,5,"""Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount.""",2024,2024-02-16T03:24:56Z,"Keyphrase: ""Factuality hallucination""","""Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount."" Keyphrase: ""Factuality hallucination""",0
arxiv2024,Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting,Yes.,5,"""LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs.""",2024,2024-02-16T02:32:06Z,"Keyphrase: ""Factually incorrect hallucinations""","""LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs."" Keyphrase: ""Factually incorrect hallucinations""",0
arxiv2024,On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities,Yes.,5,"""such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences"" and ""simple adversarial attacks can significantly undermine the effectiveness of LLM/VLM-robot integrated systems.""",2024,2024-02-15T22:01:45Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences"" and ""simple adversarial attacks can significantly undermine the effectiveness of LLM/VLM-robot integrated systems."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arxiv2024,Uncertainty Quantification for In-Context Learning of Large Language Models,Yes.,5,"""trustworthy issues with LLM's response, such as hallucination"" and ""highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty).""",2024,2024-02-15T18:46:24Z,"Keyphrase: ""Uncertainty in hallucination""","""trustworthy issues with LLM's response, such as hallucination"" and ""highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty)."" Keyphrase: ""Uncertainty in hallucination""",0
arxiv2024,Language Models with Conformal Factuality Guarantees,Yes.,5,"""Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem.""",2024,2024-02-15T18:31:53Z,"Keyphrase: ""Factuality guarantee challenge""","""Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem."" Keyphrase: ""Factuality guarantee challenge""",0
arxiv2024,Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination,Yes.,5,"""Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data.""",2024,2024-02-15T16:21:14Z,"Keyphrase: ""Privacy violation and exposure of sensitive data""","""Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data."" Keyphrase: ""Privacy violation and exposure of sensitive data""",8
arxiv2024,Case Study: Testing Model Capabilities in Some Reasoning Tasks,Yes.,5,"""However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios.""",2024,2024-02-15T14:21:30Z,"Keyphrase: ""Limited reasoning capability""","""However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios."" Keyphrase: ""Limited reasoning capability""",1
arxiv2024,Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering,Yes.,5,"""Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations.""",2024,2024-02-15T12:20:02Z,"Keyphrase: ""Ineffective mitigation of factual hallucination""","""Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations."" Keyphrase: ""Ineffective mitigation of factual hallucination""",0
arxiv2024,Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence,Yes.,5,"""Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment.""",2024,2024-02-15T11:08:10Z,"Keyphrase: ""Limitations in genuine reasoning""","""Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment."" Keyphrase: ""Limitations in genuine reasoning""",1
arxiv2024,Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States,Yes.,5,"""Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination."" and ""Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one.""",2024,2024-02-15T06:14:55Z,"Keyphrase: ""Susceptibility to hallucination""","""Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination."" and ""Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one."" Keyphrase: ""Susceptibility to hallucination""",0
arxiv2024,A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts,Yes.,5,"""Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs.""",2024,2024-02-15T05:40:21Z,"Keyphrase: ""Limited context length""","""Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs."" Keyphrase: ""Limited context length""",4
arxiv2024,PAL: Proxy-Guided Black-Box Attack on Large Language Models,Yes.,5,"""Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses.""",2024,2024-02-15T02:54:49Z,"Keyphrase: ""Vulnerability to generating harmful content""","""Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses."" Keyphrase: ""Vulnerability to generating harmful content""",3
arxiv2024,CodeMind: A Framework to Challenge Large Language Models for Code Reasoning,Yes.,5,"""Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage."" and ""their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls.""",2024,2024-02-15T02:24:46Z,"Keyphrase: ""Unfair assessment due to data leakage""","""Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage."" and ""their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls."" Keyphrase: ""Unfair assessment due to data leakage""",7
arxiv2024,Probabilistic Reasoning in Generative Large Language Models,Yes.,5,"""Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning.""",2024,2024-02-14T23:05:44Z,"Keyphrase: ""Limited probabilistic reasoning""","""Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning."" Keyphrase: ""Limited probabilistic reasoning""",1
arxiv2024,How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?,Yes.,5,"""However, the security aspects of these systems have received relatively less attention"" and ""Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks.""",2024,2024-02-14T19:45:17Z,"Keyphrase: ""Vulnerability to attacks""","""However, the security aspects of these systems have received relatively less attention"" and ""Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks."" Keyphrase: ""Vulnerability to attacks""",2
arxiv2024,Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference,Yes.,5,"""Many computational factors limit broader deployment of large language models."" and ""we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding.""",2024,2024-02-14T18:54:56Z,"Keyphrase: ""Memory bottleneck and computational shortcuts""","""Many computational factors limit broader deployment of large language models."" and ""we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding."" Keyphrase: ""Memory bottleneck and computational shortcuts""",4
arxiv2024,HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation,Yes.,4,"""With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns.""",2024,2024-02-14T18:41:19Z,"Keyphrase: ""Factuality and hallucination propensity""","""With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns."" Keyphrase: ""Factuality and hallucination propensity""",0
arxiv2024,Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking,Yes.,4,"""Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions.""",2024,2024-02-14T18:16:54Z,"Keyphrase: ""Cultural bias and lack of commonsense knowledge""","""Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions."" Keyphrase: ""Cultural bias and lack of commonsense knowledge""",3
arxiv2024,Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop,Yes.,5,"""Examples include bias, inconsistencies, and hallucination."" and ""auditing the LLM for these problems is desirable, it is far from being easy or solved.""",2024,2024-02-14T17:49:31Z,"Keyphrase: ""Bias, inconsistency, and hallucination""","""Examples include bias, inconsistencies, and hallucination."" and ""auditing the LLM for these problems is desirable, it is far from being easy or solved."" Keyphrase: ""Bias, inconsistency, and hallucination""",0
arxiv2024,AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach,Yes.,4,"""Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality."" and ""A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues.""",2024,2024-02-14T17:31:04Z,"Keyphrase: ""Inconsistency and potential bias""","""Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality."" and ""A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues."" Keyphrase: ""Inconsistency and potential bias""",0
arxiv2024,Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code,Yes.,4,"""The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing."" and ""Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement.""",2024,2024-02-14T16:41:35Z,"Keyphrase: ""Challenges in code auditing""","""The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing."" and ""Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement."" Keyphrase: ""Challenges in code auditing""",7
arxiv2024,"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",Yes.,4,"""Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety.""",2024,2024-02-14T16:14:03Z,"Keyphrase: ""Risk of harmful responses""","""Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety."" Keyphrase: ""Risk of harmful responses""",2
arxiv2024,Personalized Large Language Models,Yes.,4,"""However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots.""",2024,2024-02-14T15:55:30Z,"Keyphrase: ""Limited personalization""","""However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots."" Keyphrase: ""Limited personalization""",6
arxiv2024,Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation,Yes.,5,"""Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. 'hallucinations', even when they hold relevant knowledge.""",2024,2024-02-14T15:52:42Z,"Keyphrase: ""Factual inaccuracy and hallucination""","""Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. 'hallucinations', even when they hold relevant knowledge."" Keyphrase: ""Factual inaccuracy and hallucination""",0
arxiv2024,Scaling the Authoring of AutoTutors with Large Language Models,Yes.,4,"""A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees.""",2024,2024-02-14T14:53:56Z,"Keyphrase: ""Leaking answers""","""A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees."" Keyphrase: ""Leaking answers""",6
arxiv2024,Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling,Yes.,4,"""their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution.""",2024,2024-02-14T14:32:16Z,"Keyphrase: ""Vulnerability to misinformation""","""their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution."" Keyphrase: ""Vulnerability to misinformation""",2
arxiv2024,"Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization",Yes.,5,"""the trustworthiness of third-party custom versions of LLMs remains an essential concern."" and ""Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.""",2024,2024-02-14T13:47:35Z,"Keyphrase: ""Vulnerability to customization risks""","""the trustworthiness of third-party custom versions of LLMs remains an essential concern."" and ""Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs."" Keyphrase: ""Vulnerability to customization risks""",2
arxiv2024,Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks,Yes.,5,"""Large Language Models (LLMs) are susceptible to Jailbreaking attacks,"" and ""we guide the responses of the model toward revealing the 'desired' harmful information.""",2024,2024-02-14T13:45:19Z,"Keyphrase: ""Susceptible to jailbreaking attacks""","""Large Language Models (LLMs) are susceptible to Jailbreaking attacks,"" and ""we guide the responses of the model toward revealing the 'desired' harmful information."" Keyphrase: ""Susceptible to jailbreaking attacks""",2
arxiv2024,Into the Unknown: Self-Learning Large Language Models,Yes.,5,"""We address the main problem of self-learning LLM",2024,2024-02-14T12:56:58Z,"Keyphrase: ""Limited self-learning capabilities""","""We address the main problem of self-learning LLM Keyphrase: ""Limited self-learning capabilities""",1
arxiv2024,Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space,Yes.,5,"""We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning."" and ""embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models.""",2024,2024-02-14T10:20:03Z,"Keyphrase: ""Vulnerability to adversarial attacks""","""We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning."" and ""embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models."" Keyphrase: ""Vulnerability to adversarial attacks""",2
arxiv2024,SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks,Yes.,5,"""However, their large number of parameters poses significant challenges for practical deployment.""",2024,2024-02-14T09:01:13Z,"Keyphrase: ""Deployment challenges due to large number of parameters""","""However, their large number of parameters poses significant challenges for practical deployment."" Keyphrase: ""Deployment challenges due to large number of parameters""",4
arxiv2024,SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding,Yes.,5,"""Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat.""",2024,2024-02-14T06:54:31Z,"Keyphrase: ""Safety vulnerabilities""","""Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat."" Keyphrase: ""Safety vulnerabilities""",2
arxiv2024,GrounDial: Human-norm Grounded Safe Dialog Response Generation,Yes.,4,"""Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses, agreeing to offensive user input or including toxic content.""",2024,2024-02-14T06:25:50Z,"Keyphrase: ""Unsafe and offensive responses""","""Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses, agreeing to offensive user input or including toxic content."" Keyphrase: ""Unsafe and offensive responses""",2
arxiv2024,Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models,Yes.,5,"""This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.""",2024,2024-02-14T05:52:23Z,"Keyphrase: ""Limited robustness in analogical reasoning""","""This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making."" Keyphrase: ""Limited robustness in analogical reasoning""",1
arxiv2024,"ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions",Yes.,4,"""identify and understand why LLMs fails"" and ""ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains.""",2024,2024-02-13T21:15:33Z,"Keyphrase: ""Limited domain expertise""","""identify and understand why LLMs fails"" and ""ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains."" Keyphrase: ""Limited domain expertise""",7
arxiv2024,Measuring and Controlling Instruction (In)Stability in Language Model Dialogs,Yes.,5,"""Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a significant instruction drift within eight rounds of conversations.""",2024,2024-02-13T20:10:29Z,"Keyphrase: ""Instruction drift""","""Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a significant instruction drift within eight rounds of conversations."" Keyphrase: ""Instruction drift""",6
arxiv2024,Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance,Yes.,4,"""highlighted the critical issue of their tendency to hallucinate non-existing objects in the images"" and ""these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.""",2024,2024-02-13T18:59:05Z,"Keyphrase: ""Hallucination of nonexisting objects""","""highlighted the critical issue of their tendency to hallucinate non-existing objects in the images"" and ""these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation."" Keyphrase: ""Hallucination of nonexisting objects""",0
arxiv2024,Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast,Yes.,5,"""Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors."" and ""It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost",2024,2024-02-13T16:06:17Z,"Keyphrase: ""Adversarial behavior and jailbreaking vulnerability""","""Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors."" and ""It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost Keyphrase: ""Adversarial behavior and jailbreaking vulnerability""",2
arxiv2024,The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale,Yes.,5,"""ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists.""",2024,2024-02-13T14:38:12Z,"Keyphrase: ""Inconsistent accuracy""","""ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists."" Keyphrase: ""Inconsistent accuracy""",6
arxiv2024,Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale,Yes.,4,"""This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff,"" and ""We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools.""",2024,2024-02-13T13:50:08Z,"Keyphrase: ""Generation of realistic disinformation""","""This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff,"" and ""We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools."" Keyphrase: ""Generation of realistic disinformation""",2
arxiv2024,Visually Dehallucinative Instruction Generation,Yes.,4,"""challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents.""",2024,2024-02-13T10:25:45Z,"Keyphrase: ""Unintended content generation""","""challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents."" Keyphrase: ""Unintended content generation""",0
arxiv2024,Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering,Yes.,4,"""Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability).""",2024,2024-02-13T08:12:48Z,"Keyphrase: ""Lack of source attribution""","""Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability)."" Keyphrase: ""Lack of source attribution""",0
arxiv2024,Addressing cognitive bias in medical language models,Yes.,4,"""Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias.""",2024,2024-02-12T23:08:37Z,"Keyphrase: ""Varying effect of bias""","""Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias."" Keyphrase: ""Varying effect of bias""",3
arxiv2024,Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation,Yes.,5,"""However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination."" and ""Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to",2024,2024-02-12T22:35:40Z,"Keyphrase: ""Translation ability influenced by data contamination""","""However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination."" and ""Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to Keyphrase: ""Translation ability influenced by data contamination""",0
arxiv2024,Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking,Yes.,5,"""Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a",2024,2024-02-12T19:49:58Z,"Keyphrase: ""Overreliance on inaccurate suggestions""","""Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a Keyphrase: ""Overreliance on inaccurate suggestions""",0
arxiv2024,PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models,Yes.,5,"""Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination.""",2024,2024-02-12T18:28:36Z,"Keyphrase: ""Outdated knowledge hallucination""","""Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination."" Keyphrase: ""Outdated knowledge hallucination""",0
arxiv2024,Lissard: Long and Simple Sequential Reasoning Datasets,Yes.,5,"""Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training.""",2024,2024-02-12T18:10:17Z,"Keyphrase: ""Struggles with rule-based tasks""","""Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training."" Keyphrase: ""Struggles with rule-based tasks""",4
arxiv2024,Mercury: An Efficiency Benchmark for LLM Code Synthesis,Yes.,5,"""Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for LLM research and development.""",2024,2024-02-12T17:53:22Z,"Keyphrase: ""Efficiency gap in code generation""","""Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for LLM research and development."" Keyphrase: ""Efficiency gap in code generation""",7
arxiv2024,Retrieval-Augmented Thought Process as Sequential Decision Making,Yes.,5,"""However, several open challenges hinder their wider application",2024,2024-02-12T17:17:50Z,"Keyphrase: ""Open challenges hindering wider application""","""However, several open challenges hinder their wider application Keyphrase: ""Open challenges hindering wider application""",7
arxiv2024,AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension,Yes.,4,"""By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.""",2024,2024-02-12T15:41:22Z,"Keyphrase: ""Limited evaluation insights""","""By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research."" Keyphrase: ""Limited evaluation insights""",0
arxiv2024,Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT,Yes.,5,"""Developing long-context retrieval encoders suitable for these domains raises three challenges",2024,2024-02-12T06:43:52Z,"Keyphrase: ""Challenges in domain adaptation""","""Developing long-context retrieval encoders suitable for these domains raises three challenges Keyphrase: ""Challenges in domain adaptation""",4
arxiv2024,Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate,Yes.,4,"""While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness.""",2024,2024-02-12T04:32:33Z,"Keyphrase: ""Unfaithful explanations""","""While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness."" Keyphrase: ""Unfaithful explanations""",0
arxiv2024,Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models,Yes.,5,"""We find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers.""",2024,2024-02-11T12:25:41Z,"Keyphrase: ""Inaccurate generation""","""We find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers."" Keyphrase: ""Inaccurate generation""",0
arxiv2024,Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias,Yes.,5,"""We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic.""",2024,2024-02-11T11:23:28Z,"Keyphrase: ""Toxicity bias and hallucination""","""We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic."" Keyphrase: ""Toxicity bias and hallucination""",0
arxiv2024,A Tale of Tails: Model Collapse as a Change of Scaling Laws,Yes.,5,"""We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning"" of skills, and grokking when mixing human and synthesized data.""",2024,2024-02-10T21:06:34Z,"Keyphrase: ""Decay phenomenon and loss scaling""","""We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning"" of skills, and grokking when mixing human and synthesized data."" Keyphrase: ""Decay phenomenon and loss scaling""",5
arxiv2024,Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric,Yes.,4,"""The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset.""",2024,2024-02-10T07:55:27Z,"Keyphrase: ""Limited generalization to out-of-distribution toxicity""","""The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset."" Keyphrase: ""Limited generalization to out-of-distribution toxicity""",1
arxiv2024,GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding,Yes.,5,"""their ability to reason over domain-specialized graphs of interconnected entities remains limited"" and ""The answer is no--such capabilities lie beyond current methods.""",2024,2024-02-09T19:53:29Z,"Keyphrase: ""Limited domain-specific reasoning""","""their ability to reason over domain-specialized graphs of interconnected entities remains limited"" and ""The answer is no--such capabilities lie beyond current methods."" Keyphrase: ""Limited domain-specific reasoning""",1
arxiv2024,Understanding the Effects of Iterative Prompting on Truthfulness,Yes.,5,"""Yet, the reliability and truthfulness of these models remain pressing concerns."" and ""naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors.""",2024,2024-02-09T18:57:08Z,"Keyphrase: ""Reliability and truthfulness concerns""","""Yet, the reliability and truthfulness of these models remain pressing concerns."" and ""naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors."" Keyphrase: ""Reliability and truthfulness concerns""",0
arxiv2024,On the Out-Of-Distribution Generalization of Multimodal Large Language Models,Yes.,5,"""Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation."" and ""We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples",2024,2024-02-09T18:21:51Z,"Keyphrase: ""Limited generalization beyond training domain""","""Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation."" and ""We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples Keyphrase: ""Limited generalization beyond training domain""",5
arxiv2024,Understanding the Weakness of Large Language Model Agents within a Complex Android Environment,Yes.,5,"""LLM agents face three primary challenges,"" ""even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints,"" and ""a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents.""",2024,2024-02-09T18:19:25Z,"**Given Evidence:**
Evidence: ""llm agent face three primary challenge even stateoftheart llm agent struggle crossapp scenario adhering specific constraint lack four key capability ie understanding reasoning exploration reflection primary reason failure llm agent""

**Keyphrase:**
Lack of key capabilities","""LLM agents face three primary challenges,"" ""even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints,"" and ""a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents."" **Given Evidence:**
Evidence: ""llm agent face three primary challenge even stateoftheart llm agent struggle crossapp scenario adhering specific constraint lack four key capability ie understanding reasoning exploration reflection primary reason failure llm agent""

**Keyphrase:**
Lack of key capabilities",1
arxiv2024,The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model,Yes.,5,"""The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates.""",2024,2024-02-09T17:15:45Z,"Keyphrase: ""Hallucinations in information retrieval""","""The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates."" Keyphrase: ""Hallucinations in information retrieval""",0
arxiv2024,Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty,Yes.,5,"""However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist.""",2024,2024-02-09T16:40:59Z,"Keyphrase: ""Hallucination leading to unsafe outcomes""","""However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist."" Keyphrase: ""Hallucination leading to unsafe outcomes""",0
arxiv2024,On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference,Yes.,4,"""Despite the recent success associated with Large Language Models (LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands.""",2024,2024-02-09T09:20:59Z,"Keyphrase: ""Cost-prohibitive deployment""","""Despite the recent success associated with Large Language Models (LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands."" Keyphrase: ""Cost-prohibitive deployment""",4
arxiv2024,Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning,Yes.,4,"""they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak.""",2024,2024-02-09T09:09:39Z,"Keyphrase: ""Susceptibility to prompt-induced safety bypass""","""they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak."" Keyphrase: ""Susceptibility to prompt-induced safety bypass""",2
arxiv2024,ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling,Yes.,5,"""the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects.""",2024,2024-02-09T01:00:14Z,"Keyphrase: ""Inaccurate visual grounding""","""the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects."" Keyphrase: ""Inaccurate visual grounding""",0
arxiv2024,SubGen: Token Generation in Sublinear Time and Memory,Yes.,5,"""Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation.""",2024,2024-02-08T22:17:40Z,"Keyphrase: ""Extensive memory requirement""","""Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation."" Keyphrase: ""Extensive memory requirement""",4
arxiv2024,OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models,Yes.,5,"""Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.""",2024,2024-02-08T20:35:06Z,"Keyphrase: ""Limited modeling of mental states""","""Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world."" Keyphrase: ""Limited modeling of mental states""",1
arxiv2024,Large Language Model Meets Graph Neural Network in Knowledge Distillation,Yes.,4,"""the deployment of LLMs for production is hindered by its high computational and storage requirements, as well as long latencies during model inference.""",2024,2024-02-08T18:33:21Z,"Keyphrase: ""High computational and storage requirements""","""the deployment of LLMs for production is hindered by its high computational and storage requirements, as well as long latencies during model inference."" Keyphrase: ""High computational and storage requirements""",4
arxiv2024,Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking,Yes.,5,"""little is known about such a risk of LLM-powered conversational search"" and ""participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias.""",2024,2024-02-08T18:14:33Z,"Keyphrase: ""Biased information reinforcement""","""little is known about such a risk of LLM-powered conversational search"" and ""participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias."" Keyphrase: ""Biased information reinforcement""",3
arxiv2024,EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models,Yes.,4,"""they also introduce significant privacy concerns",2024,2024-02-08T17:57:11Z,"Keyphrase: ""Privacy concerns""","""they also introduce significant privacy concerns Keyphrase: ""Privacy concerns""",8
arxiv2024,Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images,Yes.,4,"""we examine potential gender and racial biases in such systems,"" and ""we observe significant differences in the responses according to the perceived gender or race of the person depicted.""",2024,2024-02-08T16:11:23Z,"Keyphrase: ""Gender and racial biases""","""we examine potential gender and racial biases in such systems,"" and ""we observe significant differences in the responses according to the perceived gender or race of the person depicted."" Keyphrase: ""Gender and racial biases""",3
arxiv2024,In-Context Learning Can Re-learn Forbidden Tasks,Yes.,5,"""Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities."" and ""we investigate whether ICL can undo safety training, which could represent a major security risk.""",2024,2024-02-08T14:54:17Z,"Keyphrase: ""Vulnerability despite safety training""","""Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities."" and ""we investigate whether ICL can undo safety training, which could represent a major security risk."" Keyphrase: ""Vulnerability despite safety training""",2
arxiv2024,Comprehensive Assessment of Jailbreak Attacks Against LLMs,Yes.,5,"""safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks.""",2024,2024-02-08T13:42:50Z,"Keyphrase: ""Vulnerability to jailbreak attacks""","""safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks."" Keyphrase: ""Vulnerability to jailbreak attacks""",2
arxiv2024,"Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks",Yes.,5,"""We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good.""",2024,2024-02-08T13:07:31Z,"Keyphrase: ""Limited performance in complex tasks""","""We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good."" Keyphrase: ""Limited performance in complex tasks""",6
arxiv2024,"Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations",Yes.,5,"""We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity."" and ""We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs.""",2024,2024-02-08T12:36:29Z,"Keyphrase: ""Entity ambiguity and mixing information""","""We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity."" and ""We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs."" Keyphrase: ""Entity ambiguity and mixing information""",0
arxiv2024,"Efficient Models for the Detection of Hate, Abuse and Profanity",Yes.,5,"""Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content.""",2024,2024-02-08T12:28:18Z,"Keyphrase: ""Hateful and profane content generation""","""Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content."" Keyphrase: ""Hateful and profane content generation""",3
arxiv2024,AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers,Yes.,4,"""Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process.""",2024,2024-02-08T12:01:24Z,"Keyphrase: ""Biased prediction and hallucination""","""Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process."" Keyphrase: ""Biased prediction and hallucination""",0
arxiv2024,Can ChatGPT evaluate research quality?,Yes.,5,"""ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks.""",2024,2024-02-08T10:00:40Z,"Keyphrase: ""Limited accuracy and trustworthiness""","""ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks."" Keyphrase: ""Limited accuracy and trustworthiness""",6
arxiv2024,Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia,Yes.,4,"""Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle.""",2024,2024-02-08T07:56:49Z,"Keyphrase: ""Prone to generating harmful content""","""Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle."" Keyphrase: ""Prone to generating harmful content""",2
arxiv2024,Do Large Code Models Understand Programming Concepts? A Black-box Approach,Yes.,5,"""Our findings suggest that current models lack understanding of concepts such as data flow and control flow.""",2024,2024-02-08T06:48:01Z,"Keyphrase: ""Limited understanding of data flow control""","""Our findings suggest that current models lack understanding of concepts such as data flow and control flow."" Keyphrase: ""Limited understanding of data flow control""",1
arxiv2024,Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes,Yes.,5,"""LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target.""",2024,2024-02-08T04:48:26Z,"Keyphrase: ""Resource-intensive scalability""","""LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target."" Keyphrase: ""Resource-intensive scalability""",4
arxiv2024,Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception,Yes.,5,"""existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination.""",2024,2024-02-08T02:37:30Z,"Keyphrase: ""Insufficient expressive power""","""existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination."" Keyphrase: ""Insufficient expressive power""",0
arxiv2024,Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications,Yes.,5,"""Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning."" and ""These findings underscore the urgent need for more robust safety strategies in LLMs.""",2024,2024-02-07T18:34:38Z,"Keyphrase: ""Brittleness and susceptibility to jailbreaking""","""Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning."" and ""These findings underscore the urgent need for more robust safety strategies in LLMs."" Keyphrase: ""Brittleness and susceptibility to jailbreaking""",2
arxiv2024,An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration,Yes.,4,"""they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process.""",2024,2024-02-07T15:56:17Z,"Keyphrase: ""Limited transparency and reasoning""","""they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process."" Keyphrase: ""Limited transparency and reasoning""",0
arxiv2024,Reconfidencing LLMs from the Grouping Loss Perspective,Yes.,5,"""Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone."" and ""Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query.""",2024,2024-02-07T15:40:22Z,"Keyphrase: ""Overconfident hallucinated answers""","""Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone."" and ""Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query."" Keyphrase: ""Overconfident hallucinated answers""",0
arxiv2024,Prompting Implicit Discourse Relation Annotation,Yes.,5,"""Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches.""",2024,2024-02-07T14:44:42Z,"Keyphrase: ""Inferior performance on implicit discourse relation classification""","""Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches."" Keyphrase: ""Inferior performance on implicit discourse relation classification""",6
arxiv2024,MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark,Yes.,5,"""MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V.""",2024,2024-02-07T12:28:32Z,"Keyphrase: ""Diverse bias and hallucinatory responses""","""MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V."" Keyphrase: ""Diverse bias and hallucinatory responses""",0
arxiv2024,A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models,Yes.,4,"""how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further"" and ""The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework's potential to approximate LLM decisions better in future work.""",2024,2024-02-07T12:26:12Z,"Keyphrase: ""Questionable faithfulness in explanations""","""how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further"" and ""The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework's potential to approximate LLM decisions better in future work."" Keyphrase: ""Questionable faithfulness in explanations""",1
arxiv2024,Large Language Models As Faithful Explainers,Yes.,4,"""natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs.""",2024,2024-02-07T09:09:14Z,"Keyphrase: ""Lack of faithfulness in explanations""","""natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs."" Keyphrase: ""Lack of faithfulness in explanations""",1
arxiv2024,MEMORYLLM: Towards Self-Updatable Large Language Models,Yes.,4,"""Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model.""",2024,2024-02-07T07:14:11Z,"Keyphrase: ""Limited adaptability to new knowledge""","""Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model."" Keyphrase: ""Limited adaptability to new knowledge""",4
arxiv2024,InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory,Yes.,5,"""existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues.""",2024,2024-02-07T06:50:42Z,"Keyphrase: ""Limited sequence length generalization""","""existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues."" Keyphrase: ""Limited sequence length generalization""",4
arxiv2024,Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,Yes.,5,"""However, there is little to no understanding of their faithfulness,"" ""we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs,"" ""these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness,"" and ""improving faithfulness is an open challenge.""",2024,2024-02-07T06:32:50Z,"Keyphrase: ""Challenges in faithfulness""","""However, there is little to no understanding of their faithfulness,"" ""we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs,"" ""these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness,"" and ""improving faithfulness is an open challenge."" Keyphrase: ""Challenges in faithfulness""",6
arxiv2024,Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector,Yes.,4,"""Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs).""",2024,2024-02-07T05:56:54Z,"Keyphrase: ""Overcorrection challenge""","""Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs)."" Keyphrase: ""Overcorrection challenge""",6
arxiv2024,Online Cascade Learning for Efficient Inference over Streams,Yes.,5,"""Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks.""",2024,2024-02-07T01:46:50Z,"Keyphrase: ""High computational cost""","""Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks."" Keyphrase: ""High computational cost""",4
arxiv2024,De-amplifying Bias from Differential Privacy in Language Model Fine-tuning,Yes.,5,"""We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP.""",2024,2024-02-07T00:30:58Z,"Keyphrase: ""Amplification of biases""","""We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP."" Keyphrase: ""Amplification of biases""",3
arxiv2024,Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models,Yes.,5,"""concentrating on deceptive behaviours of Large Language Models (LLMs)"" and ""emphasising multidimensional biases that underlie their deceptive behaviours"" and ""the literature review covers four types of deception categorised",2024,2024-02-07T00:21:46Z,"Keyphrase: ""Multidimensional bias and deceptive behavior""","""concentrating on deceptive behaviours of Large Language Models (LLMs)"" and ""emphasising multidimensional biases that underlie their deceptive behaviours"" and ""the literature review covers four types of deception categorised Keyphrase: ""Multidimensional bias and deceptive behavior""",3
arxiv2024,Detecting Mode Collapse in Language Models via Narration,Yes.,5,"""we show successive versions of GPT-3 suffer from increasing degrees of 'mode collapse' whereby overfitting the model during alignment constrains it from generalizing over authorship",2024,2024-02-06T23:52:58Z,"Keyphrase: ""Mode collapse and overfitting""","""we show successive versions of GPT-3 suffer from increasing degrees of 'mode collapse' whereby overfitting the model during alignment constrains it from generalizing over authorship Keyphrase: ""Mode collapse and overfitting""",5
arxiv2024,Training Language Models to Generate Text with Citations via Fine-grained Rewards,Yes.,5,"""While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources.""",2024,2024-02-06T19:00:40Z,"Keyphrase: ""Hallucination and lack of credibility""","""While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources."" Keyphrase: ""Hallucination and lack of credibility""",0
arxiv2024,Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science,Yes.,5,"""While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety."" and ""We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents,"" and ""Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents.""",2024,2024-02-06T18:54:07Z,"Keyphrase: ""Novel vulnerabilities and safety considerations""","""While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety."" and ""We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents,"" and ""Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents."" Keyphrase: ""Novel vulnerabilities and safety considerations""",2
arxiv2024,Measuring Implicit Bias in Explicitly Unbiased Large Language Models,Yes.,5,"""Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases,"" and ""Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others).""",2024,2024-02-06T15:59:23Z,"Keyphrase: ""Implicit bias across social domains""","""Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases,"" and ""Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others)."" Keyphrase: ""Implicit bias across social domains""",3
arxiv2024,Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought,Yes.,5,"""We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise.""",2024,2024-02-06T13:59:56Z,"Keyphrase: ""Struggles with dynamic noise""","""We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise."" Keyphrase: ""Struggles with dynamic noise""",5
arxiv2024,Can Large Language Models Detect Rumors on Social Media?,Yes.,5,"""it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information.""",2024,2024-02-06T11:33:57Z,"Keyphrase: ""Difficulty in reasoning with redundant information""","""it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information."" Keyphrase: ""Difficulty in reasoning with redundant information""",1
arxiv2024,Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models,Yes.,5,"""Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements.""",2024,2024-02-06T10:37:21Z,"Keyphrase: ""Biased target variable selection""","""Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements."" Keyphrase: ""Biased target variable selection""",3
arxiv2024,The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs,Yes.,5,"""those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs."" and ""illustrating that they universally suffer from this instinctive bias to varying degrees.""",2024,2024-02-06T06:48:46Z,"Keyphrase: ""Instinctive bias""","""those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs."" and ""illustrating that they universally suffer from this instinctive bias to varying degrees."" Keyphrase: ""Instinctive bias""",1
arxiv2024,Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context,Yes.,5,"""Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs",2024,2024-02-06T01:59:41Z,"Keyphrase: ""Limited cross-file context integration""","""Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs Keyphrase: ""Limited cross-file context integration""",4
arxiv2024,Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains,Yes.,5,"""We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors.""",2024,2024-02-05T20:51:11Z,"Keyphrase: ""Inconsistency in summarization""","""We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors."" Keyphrase: ""Inconsistency in summarization""",6
arxiv2024,Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues,Yes.,5,"""This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and",2024,2024-02-05T20:11:56Z,"Keyphrase: ""Struggles with nuance and verbal instruction""","""This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and Keyphrase: ""Struggles with nuance and verbal instruction""",6
arxiv2024,A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications,Yes.,4,"""We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique.""",2024,2024-02-05T19:49:13Z,"Keyphrase: ""Limited depth and detail""","""We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique."" Keyphrase: ""Limited depth and detail""",6
arxiv2024,Nevermind: Instruction Override and Moderation in Large Language Models,Yes.,5,"""Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines.""",2024,2024-02-05T18:58:19Z,"Keyphrase: ""Difficulty in following safety guidelines""","""Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines."" Keyphrase: ""Difficulty in following safety guidelines""",2
arxiv2024,GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models,Yes.,4,"""The discovery of 'jailbreaks' to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures."" and ""Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses.""",2024,2024-02-05T18:54:43Z,"Keyphrase: ""Ethical guideline violations""","""The discovery of 'jailbreaks' to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures."" and ""Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses."" Keyphrase: ""Ethical guideline violations""",2
arxiv2024,Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS,Yes.,5,"""Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency.""",2024,2024-02-05T18:47:04Z,"Keyphrase: ""Challenges in code generation""","""Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency."" Keyphrase: ""Challenges in code generation""",7
arxiv2024,Unified Hallucination Detection for Multimodal Large Language Models,Yes.,5,"""Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination.""",2024,2024-02-05T16:56:11Z,"Keyphrase: ""Hallucination issues""","""Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination."" Keyphrase: ""Hallucination issues""",0
arxiv2024,C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models,Yes.,5,"""Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments.""",2024,2024-02-05T16:46:16Z,"Keyphrase: ""Trustworthiness issues""","""Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments."" Keyphrase: ""Trustworthiness issues""",0
arxiv2024,Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations,Yes.,5,"""Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions.""",2024,2024-02-05T15:08:19Z,"Keyphrase: ""Stability issues and content hallucination""","""Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions."" Keyphrase: ""Stability issues and content hallucination""",0
arxiv2024,Evading Data Contamination Detection for Language Models is (too) Easy,Yes.,5,"""However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements."" and ""we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.""",2024,2024-02-05T09:10:32Z,"Keyphrase: ""Contamination of training data""","""However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements."" and ""we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods."" Keyphrase: ""Contamination of training data""",0
arxiv2024,Graph-enhanced Large Language Models in Asynchronous Plan Reasoning,Yes.,5,"""We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow."" and ""LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices.""",2024,2024-02-05T08:26:33Z,"Keyphrase: ""Poor performance on complex tasks""","""We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow."" and ""LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices."" Keyphrase: ""Poor performance on complex tasks""",7
arxiv2024,DeAL: Decoding-time Alignment for Large Language Models,Yes.,4,"""First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training).""",2024,2024-02-05T06:12:29Z,"Keyphrase: ""Reliability and safety concerns""","""First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training)."" Keyphrase: ""Reliability and safety concerns""",2
arxiv2024,Large Language Models are Geographically Biased,Yes.,5,"""Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm."" and ""We show various problematic geographic biases, which we define as systemic errors in geospatial predictions."" and ""LLMs exhibit common biases across a range of objective and subjective topics"" and ""LLMs are clearly biased against locations with lower",2024,2024-02-05T02:32:09Z,"Keyphrase: ""Inherent societal biases""","""Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm."" and ""We show various problematic geographic biases, which we define as systemic errors in geospatial predictions."" and ""LLMs exhibit common biases across a range of objective and subjective topics"" and ""LLMs are clearly biased against locations with lower Keyphrase: ""Inherent societal biases""",3
arxiv2024,Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting,Yes.,5,"""Large Language Models (LLMs) frequently struggle with complex reasoning tasks, failing to construct logically sound steps towards the solution."" and ""repeated meaningless feedback gradually decreases the quality of the responses, eventually leading to a larger deviation from the intended outcome.""",2024,2024-02-05T00:44:28Z,"Keyphrase: ""Struggles with complex reasoning""","""Large Language Models (LLMs) frequently struggle with complex reasoning tasks, failing to construct logically sound steps towards the solution."" and ""repeated meaningless feedback gradually decreases the quality of the responses, eventually leading to a larger deviation from the intended outcome."" Keyphrase: ""Struggles with complex reasoning""",1
arxiv2024,LLM-Enhanced Data Management,Yes.,5,"""existing LLMs have several limitations",2024,2024-02-04T23:42:02Z,"Keyphrase: ""Multiple limitations""","""existing LLMs have several limitations Keyphrase: ""Multiple limitations""",6
arxiv2024,Can Large Language Models Learn Independent Causal Mechanisms?,Yes.,5,"""Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability.""",2024,2024-02-04T23:04:02Z,"Keyphrase: ""Limited generalization ability in uncommon settings""","""Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability."" Keyphrase: ""Limited generalization ability in uncommon settings""",1
arxiv2024,Factuality of Large Language Models in the Year 2024,Yes.,5,"""Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios.""",2024,2024-02-04T09:36:31Z,"Keyphrase: ""Factual inaccuracy""","""Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios."" Keyphrase: ""Factual inaccuracy""",0
arxiv2024,DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models,Yes.,5,"""we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases.""",2024,2024-02-04T08:11:45Z,"Keyphrase: ""Poor decision-making in complex problems""","""we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases."" Keyphrase: ""Poor decision-making in complex problems""",1
arxiv2024,Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning,Yes.,5,"""However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process.""",2024,2024-02-04T07:59:06Z,"Keyphrase: ""Lack of self-evaluation capability""","""However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process."" Keyphrase: ""Lack of self-evaluation capability""",1
arxiv2024,A Survey of Large Language Models in Finance (FinLLMs),Yes.,4,"""Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency.""",2024,2024-02-04T02:06:57Z,"Keyphrase: ""Hallucination and privacy concerns""","""Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency."" Keyphrase: ""Hallucination and privacy concerns""",0
arxiv2024,"Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times",Yes.,5,"""Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades.""",2024,2024-02-03T20:22:54Z,"Keyphrase: ""Degraded human reading time estimation""","""Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades."" Keyphrase: ""Degraded human reading time estimation""",5
arxiv2024,Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models,Yes.,4,"""Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks."" and ""Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning L",2024,2024-02-03T16:43:42Z,"Keyphrase: ""Vulnerability to harmful content and forgetting safety alignment""","""Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks."" and ""Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning L Keyphrase: ""Vulnerability to harmful content and forgetting safety alignment""",2
arxiv2024,GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events,Yes.,5,"""Concurrently, we also identify certain limitations of GPT-4V, which constrain its understanding in more intricate scenarios.""",2024,2024-02-03T16:38:25Z,"Keyphrase: ""Limited understanding of intricate scenarios""","""Concurrently, we also identify certain limitations of GPT-4V, which constrain its understanding in more intricate scenarios."" Keyphrase: ""Limited understanding of intricate scenarios""",1
arxiv2024,Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations,Yes.,4,"""At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the",2024,2024-02-03T14:28:55Z,"Keyphrase: ""Diverse pitfalls in generating desired results""","""At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the Keyphrase: ""Diverse pitfalls in generating desired results""",0
arxiv2024,Affordable Generative Agents,Yes.,4,"""the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents,"" and ""demonstrating that agents can only generate finite behaviors in fixed environments.""",2024,2024-02-03T06:16:28Z,"Keyphrase: ""Limited adaptability in dynamic environments""","""the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents,"" and ""demonstrating that agents can only generate finite behaviors in fixed environments."" Keyphrase: ""Limited adaptability in dynamic environments""",1
arxiv2024,A Closer Look at the Limitations of Instruction Tuning,Yes.,5,"""While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT.""",2024,2024-02-03T04:45:25Z,"Keyphrase: ""Underexplored limitations""","""While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT."" Keyphrase: ""Underexplored limitations""",0
arxiv2024,How well do LLMs cite relevant medical references? An evaluation framework and analyses,Yes.,5,"""Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide."" and ""Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references.""",2024,2024-02-03T03:44:57Z,"Keyphrase: ""Potential for harm with medical information""","""Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide."" and ""Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references."" Keyphrase: ""Potential for harm with medical information""",3
arxiv2024,Human-Centered Privacy Research in the Age of Large Language Models,Yes.,4,"""The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns."" and ""To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems.""",2024,2024-02-03T02:32:45Z,"Keyphrase: ""Imperfect privacy properties""","""The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns."" and ""To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems."" Keyphrase: ""Imperfect privacy properties""",8
arxiv2024,Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes,Yes.,4,"""Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases.""",2024,2024-02-03T01:40:11Z,"Keyphrase: ""Harmful social bias""","""Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases."" Keyphrase: ""Harmful social bias""",3
arxiv2024,What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement,Yes.,5,"""Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase.""",2024,2024-02-02T19:43:15Z,"Keyphrase: ""Catastrophic forgetting""","""Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase."" Keyphrase: ""Catastrophic forgetting""",5
arxiv2024,"(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",Yes.,5,"""Beyond known issues like hallucinations, experts revealed novel legal problems, including that users' conversations with LLMs are not protected by attorney-client confidentiality or bound to professional ethics that guard against conflicted counsel or poor quality advice.""",2024,2024-02-02T19:35:34Z,"Keyphrase: ""Poor quality legal advice""","""Beyond known issues like hallucinations, experts revealed novel legal problems, including that users' conversations with LLMs are not protected by attorney-client confidentiality or bound to professional ethics that guard against conflicted counsel or poor quality advice."" Keyphrase: ""Poor quality legal advice""",0
arxiv2024,"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",Yes.,5,"""We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature.""",2024,2024-02-02T14:43:18Z,"Keyphrase: ""Limited planning and reasoning capabilities""","""We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature."" Keyphrase: ""Limited planning and reasoning capabilities""",1
arxiv2024,Distilling LLMs' Decomposition Abilities into Compact Language Models,Yes.,4,"""Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization.""",2024,2024-02-02T13:23:15Z,"Keyphrase: ""Scalability challenges and limited customization""","""Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization."" Keyphrase: ""Scalability challenges and limited customization""",1
arxiv2024,StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback,Yes.,4,"""the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective.""",2024,2024-02-02T13:14:31Z,"Keyphrase: ""Limited understanding of complex human requirements""","""the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective."" Keyphrase: ""Limited understanding of complex human requirements""",7
arxiv2024,A Survey on Large Language Model Hallucination via a Creativity Perspective,Yes.,4,"""This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications.""",2024,2024-02-02T12:21:04Z,"Keyphrase: ""Reliability issues due to hallucination""","""This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications."" Keyphrase: ""Reliability issues due to hallucination""",0
arxiv2024,Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models,Yes.,5,"""our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of L",2024,2024-02-02T12:07:00Z,"Keyphrase: ""Response variability syndrome""","""our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of L Keyphrase: ""Response variability syndrome""",0
arxiv2024,Can MLLMs Perform Text-to-Image In-Context Learning?,Yes.,5,"""we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation.""",2024,2024-02-02T10:30:05Z,"Keyphrase: ""Multimodal complexity""","""we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation."" Keyphrase: ""Multimodal complexity""",1
arxiv2024,Exploring the Limitations of Graph Reasoning in Large Language Models,Yes.,5,"""We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution.""",2024,2024-02-02T09:45:33Z,"Keyphrase: ""Bias in benchmarking""","""We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution."" Keyphrase: ""Bias in benchmarking""",3
arxiv2024,Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus,Yes.,4,"""yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources.""",2024,2024-02-02T06:44:22Z,"Keyphrase: ""Limited external knowledge integration""","""yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources."" Keyphrase: ""Limited external knowledge integration""",0
arxiv2024,A Multi-Agent Conversational Recommender System,Yes.,4,"""Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address",2024,2024-02-02T04:20:13Z,"Keyphrase: ""Limited conversational recommendation capabilities""","""Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address Keyphrase: ""Limited conversational recommendation capabilities""",6
arxiv2024,The Political Preferences of LLMs,Yes.,4,"""The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints."" and ""base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests.""",2024,2024-02-02T02:43:10Z,"Keyphrase: ""Political bias and suboptimal performance""","""The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints."" and ""base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests."" Keyphrase: ""Political bias and suboptimal performance""",3
arxiv2024,LitLLM: A Toolkit for Scientific Literature Review,Yes.,5,"""Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on.""",2024,2024-02-02T02:41:28Z,"Keyphrase: ""Hallucination of non-actual information""","""Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on."" Keyphrase: ""Hallucination of non-actual information""",0
arxiv2024,When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards,Yes.,5,"""Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details.""",2024,2024-02-01T19:12:25Z,"Keyphrase: ""Sensitivity to minute details""","""Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details."" Keyphrase: ""Sensitivity to minute details""",7
arxiv2024,Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents,Yes.,5,"""However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents.""",2024,2024-02-01T17:30:50Z,"Keyphrase: ""Uncontrollable content generation""","""However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents."" Keyphrase: ""Uncontrollable content generation""",1
arxiv2024,Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing,Yes.,5,"""However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process... the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training.""",2024,2024-02-01T15:18:33Z,"Keyphrase: ""Inefficient reasoning process""","""However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process... the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training."" Keyphrase: ""Inefficient reasoning process""",0
arxiv2024,Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks,Yes.,5,"""We uncover that Self-Generated attacks pose a significant threat, reducing LVLM(s) classification performance by up to 33%.""",2024,2024-02-01T14:41:20Z,"Keyphrase: ""Vulnerability to self-generated attacks""","""We uncover that Self-Generated attacks pose a significant threat, reducing LVLM(s) classification performance by up to 33%."" Keyphrase: ""Vulnerability to self-generated attacks""",2
arxiv2024,Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection,Yes.,4,"""Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises.""",2024,2024-02-01T08:11:56Z,"Keyphrase: ""Risk of plagiarism and fake news""","""Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises."" Keyphrase: ""Risk of plagiarism and fake news""",2
arxiv2024,Investigating Bias Representations in Llama 2 Chat via Activation Steering,Yes.,4,"""We address the challenge of societal bias in Large Language Models (LLMs),"" and ""Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF).""",2024,2024-02-01T07:48:50Z,"Keyphrase: ""Inherent gender bias""","""We address the challenge of societal bias in Large Language Models (LLMs),"" and ""Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF)."" Keyphrase: ""Inherent gender bias""",3
arxiv2024,Safety of Multimodal Large Language Models on Images and Text,Yes.,4,"""the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios.""",2024,2024-02-01T05:57:10Z,"Keyphrase: ""Safety risks in real-world deployment""","""the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios."" Keyphrase: ""Safety risks in real-world deployment""",2
arxiv2024,"Redefining ""Hallucination"" in LLMs: Towards a psychology-informed framework for mitigating misinformation",Yes.,5,"""a notable challenge surfaces in the form of 'hallucinations.' This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base.""",2024,2024-02-01T03:01:11Z,"Keyphrase: ""Misinformation surface form hallucination""","""a notable challenge surfaces in the form of 'hallucinations.' This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base."" Keyphrase: ""Misinformation surface form hallucination""",0
arxiv2024,Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective,Yes.,4,"""the absence of explicit explainability in LLMs significantly hinders their application in the social sciences.""",2024,2024-02-01T01:17:46Z,"Keyphrase: ""Lack of explainability""","""the absence of explicit explainability in LLMs significantly hinders their application in the social sciences."" Keyphrase: ""Lack of explainability""",6
arXIv2022,GitHub Copilot AI pair programmer: Asset or Liability?,No.,1,"The abstract discusses GitHub Copilot, an AI pair programmer, but does not mention LLMs or their limitations.",2022,2022-06-30T15:00:03Z,,,
arXIv2022,Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding,Yes.,3,"""the huge size of these models brings significant challenges to their fine-tuning and online deployment due to latency and cost constraints.""",2022,2022-06-30T04:33:50Z,,,
arXIv2022,Solving Quantitative Reasoning Problems with Language Models,Yes.,3,"""state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level.""",2022,2022-06-29T18:54:49Z,,,
arXIv2022,Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding,Yes.,3,"""The resulting models are too large for on-edge applications. For instance, BERT-based systems contain over 110M parameters.""",2022,2022-06-28T23:08:32Z,,,
arXIv2022,Flexible text generation for counterfactual fairness probing,Yes.,1,"""We show that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactual generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier.""",2022,2022-06-28T05:07:20Z,,,
arXIv2022,NERDA-Con: Extending NER models for Continual Learning -- Integrating Distinct Tasks and Updating Distribution Shifts,Yes.,3,"""Re-training NERs based on Large Language Models (LLMs) from scratch over newly acquired data poses economic disadvantages. In contrast, re-training only with newly acquired data will result in Catastrophic Forgetting of previously acquired knowledge.""",2022,2022-06-28T03:22:55Z,,,
arXIv2022,Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities,Yes.,3,"""We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task.""",2022,2022-06-22T07:26:55Z,,,
arXIv2022,Local Slot Attention for Vision-and-Language Navigation,No.,1,The abstract does not mention LLMs or their limitations.,2022,2022-06-17T09:21:26Z,,,
arXIv2022,Estimating Confidence of Predictions of Individual Classifiers and Their Ensembles for the Genre Classification Task,Yes.,3,"""Neural models based on pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA results in many NLP tasks, including non-topical classification. However, in many cases, their downstream application to very large corpora, such as",2022,2022-06-15T09:59:05Z,,,
arXIv2022,SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features,Yes.,3,"""Models based on large-pretrained language models, such as S(entence)BERT, provide effective and efficient sentence embeddings that show high correlation to human similarity ratings, but lack interpretability.""",2022,2022-06-14T17:37:18Z,,,
arXIv2022,Memory-Based Model Editing at Scale,Yes.,3,"""Existing model editors have shown promise, but also suffer from insufficient expressiveness",2022,2022-06-13T23:40:34Z,,,
arXIv2022,Bridging the Gap Between Training and Inference of Bayesian Controllable Language Models,Yes.,3,"""However, it is difficult to control the pre-trained language models to generate sentences with the desired attribute such as topic and sentiment, etc."" and ""the mismatch between training and inference of BCLMs limits the performance of the models.""",2022,2022-06-11T12:52:32Z,,,
arXIv2022,Measuring the Carbon Intensity of AI in Cloud Instances,Yes.,1,"""We provide measurements of operational software carbon intensity for a set of modern models for natural language processing and computer vision, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model.""",2022,2022-06-10T17:04:04Z,,,
arXIv2022,Domain-specific Language Pre-training for Dialogue Comprehension on Clinical Inquiry-Answering Conversations,Yes.,3,"""Yet, due to the gap between pre-training and downstream clinical domains, it remains challenging to exploit the generic backbones for domain-specific applications.""",2022,2022-06-06T08:45:03Z,,,
arXIv2022,Making Large Language Models Better Reasoners with Step-Aware Verifier,Yes.,3,"""Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems.""",2022,2022-06-06T03:38:36Z,,,
arXIv2022,Offline RL for Natural Language Generation with Implicit Language Q Learning,Yes.,3,"""Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks.""",2022,2022-06-05T18:38:42Z,,,
arXIv2022,Fault-Aware Neural Code Rankers,Yes.,3,"""However, these approaches assume that the unit tests are given and assume the ability to safely execute the generated programs (which can do arbitrary dangerous operations such as file manipulations). Both of the above assumptions are impractical in real-world software development.""",2022,2022-06-04T22:01:05Z,,,
arXIv2022,Extreme Compression for Pre-trained Transformers Made Simple and Efficient,Yes.,2,"""However, to preserve the accuracy for such aggressive compression schemes, cutting-edge methods usually introduce complicated compression pipelines, e.g., multi-stage expensive knowledge distillation with extensive hyperparameter tuning.""",2022,2022-06-04T00:19:45Z,,,
arXIv2022,Differentially Private Model Compression,Yes.,2,"""The inference cost of these models -- which consist of hundreds of millions of parameters -- however, can be prohibitively large.""",2022,2022-06-03T22:04:36Z,,,
arXIv2022,A Mixture-of-Expert Approach to RL-based Dialogue Management,Yes.,3,"""Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge."" and ""As a result, they struggle to produce a successful and engaging dialogue even if they are warm-started with a pre-trained LM.""",2022,2022-05-31T19:00:41Z,,,
arXIv2022,Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,Yes.,3,"""Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap.""",2022,2022-05-27T04:51:15Z,,,
arXIv2022,BiT: Robustly Binarized Multi-distilled Transformer,No.,1,The abstract discusses binarization of transformer models and improvements in optimization for resource-constrained environments but does not specifically mention language models or their limitations.,2022,2022-05-25T19:01:54Z,,,
arXIv2022,Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling,Yes.,3,"""Previous works on sentence scoring mainly adopted either causal language modeling (CLM) like GPT or masked language modeling (MLM) like BERT, which have some limitations",2022,2022-05-25T18:00:09Z,,,
arXIv2022,"Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",No.,1,The abstract does not mention LLMs or any specific limitations related to LLMs.,2022,2022-05-25T15:26:48Z,,,
arXIv2022,PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation,Yes.,3,"""even large-scale pre-trained language models present low logical fidelity on logical table-to-text.""",2022,2022-05-25T11:55:54Z,,,
arXIv2022,Large Language Models are Few-Shot Clinical Information Extractors,Yes.,1,"""we show that large language models, such as InstructGPT, perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain.""",2022,2022-05-25T11:49:58Z,,,
arXIv2022,Gradient-Based Constrained Sampling from Language Models,Yes.,3,"""Large pretrained language models generate fluent text but are notoriously hard to controllably sample from.""",2022,2022-05-25T08:09:03Z,,,
arXIv2022,RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning,Yes.,3,"""Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating LM prompting may not follow human language patterns.""",2022,2022-05-25T07:50:31Z,,,
arXIv2022,Do we need Label Regularization to Fine-tune Pre-trained Language Models?,Yes.,1,"""Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs.""",2022,2022-05-25T01:26:31Z,,,
arXIv2022,Toxicity Detection with Generative Prompt-based Inference,Yes.,3,"""It is a long-known risk that language models (LMs), once trained on corpus containing undesirable content, have the power to manifest biases and toxicity.""",2022,2022-05-24T22:44:43Z,,,
arXIv2022,Medical Scientific Table-to-Text Generation with Human-in-the-Loop under the Data Sparsity Constraint,Yes.,3,"""inability of the state-of-the-art natural language generation models (including T5, PEGASUS and GPT-Neo) to produce accurate and reliable outputs.""",2022,2022-05-24T21:10:57Z,,,
arXIv2022,PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation,Yes.,1,"""In this work, we propose an unsupervised approach to generate poems following any given meter and rhyme scheme, without requiring any poetic text for training.""",2022,2022-05-24T17:09:55Z,,,
arXIv2022,Word-order typology in Multilingual BERT: A case study in subordinate-clause detection,Yes.,3,"""The capabilities and limitations of BERT and similar models are still unclear when it comes to learning syntactic abstractions, in particular across languages.""",2022,2022-05-24T11:35:39Z,,,
arXIv2022,The Authenticity Gap in Human Evaluation,Yes.,2,"""For the latter, we propose a new human evaluation protocol called $\textit{system-level probabilistic assessment}$ (SPA). When human evaluation of stories is done with SPA, we can recover the ordering of GPT-3 models by size, with statistically significant results. However, when human evaluation is done with the standard protocol, less than half of the expected preferences can be recovered (",2022,2022-05-24T09:51:27Z,,,
arXIv2022,Large Language Models are Zero-Shot Reasoners,Yes.,1,"""Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars.""",2022,2022-05-24T09:22:26Z,,,
arXIv2022,On the Paradox of Learning to Reason from Data,No.,1,The abstract does not mention LLMs or any specific language models. It focuses on BERT and logical reasoning in NLP tasks.,2022,2022-05-23T17:56:48Z,,,
arXIv2022,Parameter-Efficient Sparsity for Large Language Models Fine-Tuning,Yes.,3,"""there are challenges in the computational overhead and memory footprint of sparse training when compressing large-scale language models.""",2022,2022-05-23T02:43:45Z,,,
arXIv2022,Improving Short Text Classification With Augmented Data Using GPT-3,Yes.,3,"""Although researchers claim that it requires only a small number of in-context examples to learn a task, in practice GPT-3 requires these training examples to be either of exceptional quality or a higher quantity than easily created by hand.""",2022,2022-05-23T01:10:38Z,,,
arXIv2022,"Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models",Yes.,3,"""The energy requirements of current natural language processing models continue to grow at a rapid, unsustainable pace.""",2022,2022-05-19T16:03:55Z,,,
arXIv2022,The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues,Yes.,3,"""We find that, even though conversational agents (Blender in particular) perform well on conversational uptake, they are quantifiably worse than real teachers on several pedagogical dimensions, especially with regard to helpfulness (Blender",2022,2022-05-16T09:36:30Z,,,
arXIv2022,Transkimmer: Transformer Learns to Layer-wise Skim,No.,1,"The abstract discusses Transformer-based models in general, focusing on computational efficiency improvements, but does not specifically mention LLMs or their limitations.",2022,2022-05-15T16:23:30Z,,,
arXIv2022,Discovering Latent Concepts Learned in BERT,Yes.,3,"""The scope of the analyses is limited to pre-defined concepts that reinforce the traditional linguistic knowledge and do not reflect on how novel concepts are learned by the model."" and ""the discovered latent concepts highlight potential biases learned in the model.""",2022,2022-05-15T09:45:34Z,,,
arXIv2022,PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain,Yes.,3,"""a few approaches fine-tuned general transformer models on specialized corpora while maintaining the original tokenizer, but in fields requiring specialized terminology, these models often fail to perform adequately.""",2022,2022-05-13T20:42:07Z,,,
arXIv2022,Towards Answering Open-ended Ethical Quandary Questions,Yes.,2,"""We also discuss the remaining challenges and ethical issues involved in this task and suggest the direction toward developing responsible NLP systems by incorporating human values explicitly.""",2022,2022-05-12T09:52:59Z,,,
arXIv2022,Clinical Prompt Learning with Frozen Language Models,Yes.,3,"""the performance of even the largest PLMs such as GPT-3 do not perform well on specialized domains (e.g. medical text),"" and ""The reliance on fine-tuning large PLMs is problematic in clinical settings where data is often held in non-GPU environments, and more resource efficient methods of training specialized domain models is crucial.""",2022,2022-05-11T14:25:13Z,,,
arXIv2022,Towards the Generation of Musical Explanations with GPT-3,Yes.,3,"""Our results show that GPT-3 lacks the necessary intelligence to really understand musical decisions. A major barrier to reach a better performance is the lack of data that includes explanations of the creative process carried out by artists for musical pieces.""",2022,2022-05-11T13:04:54Z,,,
arXIv2022,Query-Based Keyphrase Extraction from Long Documents,Yes.,3,"""Transformer-based architectures in natural language processing force input size limits that can be problematic when long documents need to be processed.""",2022,2022-05-11T10:29:30Z,,,
arXIv2022,Towards Unified Prompt Tuning for Few-shot Text Classification,Yes.,3,"""PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream tasks.""",2022,2022-05-11T07:40:45Z,,,
arXIv2022,Reducing Activation Recomputation in Large Transformer Models,Yes.,1,"""We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%.""",2022,2022-05-10T22:40:17Z,,,
arXIv2022,Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words,No.,1,The abstract discusses cosine similarity of contextual embeddings and does not mention language models (LLMs or LMs).,2022,2022-05-10T18:00:06Z,,,
arXIv2022,Context-Aware Abbreviation Expansion Using Large Language Models,Yes.,1,"""Our approach is to expand the abbreviations into full-phrase options by leveraging conversation context with the power of pretrained large language models (LLMs).""",2022,2022-05-08T03:02:53Z,,,
arXIv2022,Vector Representations of Idioms in Conversational Systems,Yes.,1,"""We experiment with three instances of the SoTA dialogue model, Dialogue Generative Pre-trained Transformer (DialoGPT), for conversation generation.""",2022,2022-05-07T14:50:05Z,,,
arXIv2022,Robust Conversational Agents against Imperceptible Toxicity Triggers,Yes.,3,"""Existing work to generate such attacks is either based on human-generated attacks which is costly and not scalable or, in case of automatic attacks, the attack vector does not conform to human-like language, which can be detected using a language model loss.""",2022,2022-05-05T01:48:39Z,,,
arXIv2022,Data Governance in the Age of Large-Scale Data-Driven Language Technology,Yes.,1,"""The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data.""",2022,2022-05-04T00:44:35Z,,,
arXIv2022,Efficient Fine-Tuning of BERT Models on the Edge,Yes.,3,"""With the increasing size of deep neural networks, as noted with the likes of BERT and other natural language processing models, comes increased resource requirements, namely memory, computation, energy, and time. Furthermore, training is far more resource intensive than inference. Resource-constrained on-device learning is thus doubly difficult, especially with large BERT-like models.""",2022,2022-05-03T14:51:53Z,,,
arXIv2022,Improving Students' Academic Performance with AI and Semantic Technologies,No.,1,The abstract does not mention LLMs or any pre-trained transformer-based language models.,2022,2022-05-02T06:11:24Z,,,
arXIv2022,Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning,Yes.,3,"""automating this task is challenging due to a large number of LLT codes (as of writing over 80,000), limited availability of training data for long tail/emerging classes, and the general high accuracy demands of the medical domain.""",2022,2022-05-01T22:49:28Z,,,
arXIv2022,An End-to-End Dialogue Summarization System for Sales Calls,Yes.,1,"""We show how GPT-3 can be leveraged as an offline data labeler to handle training data scarcity and accommodate privacy constraints in an industrial setting.""",2022,2022-04-27T14:02:50Z,,,
arXIv2022,Pretraining Chinese BERT for Detecting Word Insertion and Deletion Errors,Yes.,3,"""Chinese BERT models achieve remarkable progress in dealing with grammatical errors of word substitution. However, they fail to handle word insertion and deletion because BERT assumes the existence of a word at each position.""",2022,2022-04-26T03:19:36Z,,,
arXIv2022,Data Distributional Properties Drive Emergent In-Context Learning in Transformers,Yes.,3,"""In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously.""",2022,2022-04-22T16:10:50Z,,,
arXIv2022,Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing,Yes.,3,"""Biomedical text with its complex semantics poses additional challenges in vision--language modelling compared to the general domain, and previous work has used insufficiently adapted models that lack domain-specific language understanding.""",2022,2022-04-21T00:04:35Z,,,
arXIv2022,Is BERT Robust to Label Noise? A Study on Learning with Noisy Labels in Text Classification,Yes.,3,"""existing noise-handling methods do not always improve its performance, and may even deteriorate it, suggesting the need for further investigation.""",2022,2022-04-20T10:24:19Z,,,
arXIv2022,Impact of Tokenization on Language Models: An Analysis for Turkish,Yes.,1,"""Tokenization is an important text preprocessing step to prepare input tokens for deep language models.""",2022,2022-04-19T12:01:46Z,,,
arXIv2022,DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks,Yes.,3,"""a common limitation of the attention mechanism utilized in Transformer Encoder is that it cannot automatically capture the information of word order, so explicit position embeddings are generally required to be fed into the target model.""",2022,2022-04-19T06:12:48Z,,,
arXIv2022,Context-Aware Language Modeling for Goal-Oriented Dialogue Systems,Yes.,3,"""While supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question.""",2022,2022-04-18T17:23:11Z,,,
arXIv2022,L3Cube-HingCorpus and HingBERT: A Code Mixed Hindi-English Dataset and BERT Language Models,Yes.,1,"""We present L3Cube-HingCorpus, the first large-scale real Hindi-English code mixed data in a Roman script. It consists of 52.93M sentences and 1.04B tokens, scraped from Twitter.""",2022,2022-04-18T16:49:59Z,,,
arXIv2022,Just Fine-tune Twice: Selective Differential Privacy for Large Language Models,Yes.,3,"""Yet applying differential privacy (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss.""",2022,2022-04-15T22:36:55Z,,,
arXIv2022,CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing,Yes.,1,"""Our experiments using large language models demonstrate that CAMERO significantly improves the generalization performance of the ensemble model.""",2022,2022-04-13T19:54:51Z,,,
arXIv2022,MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages,Yes.,3,"""directly training an mBERT-based QA system for low-resource languages is challenging due to the paucity of training data.""",2022,2022-04-12T13:52:54Z,,,
arXIv2022,Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change,Yes.,3,"""This indicates that language models do not fully cover the fine-grained morphological and syntactic signals that are explicitly represented in grammatical profiles.""",2022,2022-04-12T11:20:42Z,,,
arXIv2022,Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts,Yes.,1,"""Pre-trained language models (LM) have shown great potential for cross-lingual transfer in low-resource settings.""",2022,2022-04-10T21:46:52Z,,,
arXIv2022,BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model,Yes.,2,"""We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community.""",2022,2022-04-08T08:07:42Z,,,
arXIv2022,"Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",Yes.,3,"""Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions.""",2022,2022-04-07T18:33:10Z,,,
arXIv2022,Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval,Yes.,3,"""State-of-the-art neural (re)rankers are notoriously data-hungry which -- given the lack of large-scale training data in languages other than English -- makes them rarely used in multilingual and cross-lingual retrieval settings.""",2022,2022-04-05T15:44:27Z,,,
arXIv2022,Data Augmentation for Intent Classification with Off-the-shelf Large Language Models,Yes.,3,"""In tasks with semantically close intents, we observe that the generated data is less helpful. Our analysis shows that this is because GPT often generates utterances that belong to a closely-related intent instead of the desired one.""",2022,2022-04-05T03:29:26Z,,,
arXIv2022,Applying Automatic Text Summarization for Fake News Detection,Yes.,3,"""combines the power of transformer-based language models while simultaneously addressing one of their inherent problems"" and ""circumventing sequential limits and related loss of information the underlying transformer architecture typically suffers from.""",2022,2022-04-04T21:00:55Z,,,
arXIv2022,CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation,Yes.,2,"""Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets.""",2022,2022-04-02T13:42:49Z,,,
arXIv2022,CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos,No.,1,"The abstract discusses the robustness of dense retrievers and CharacterBERT, but does not mention large language models (LLMs) or their limitations.",2022,2022-04-01T23:02:50Z,,,
arXIv2022,Evaluation of Fake News Detection with Knowledge-Enhanced Language Models,Yes.,3,"""However, large-scale PLMs are generally not trained on structured factual data and hence may not possess priors that are grounded in factually accurate knowledge.""",2022,2022-04-01T14:14:46Z,,,
arXIv2022,Domain Adaptation for Sparse-Data Settings: What Do We Gain by Not Using Bert?,Yes.,3,"""While transfer learning with pre-trained language models outperforms other methods across tasks, alternatives do not perform much worse while requiring much less computational effort, thus significantly reducing monetary and environmental cost.""",2022,2022-03-31T09:59:08Z,,,
arXIv2022,Leveraging pre-trained language models for conversational information seeking from text,Yes.,2,"""It also highlight the challenge posed by control flow relations for which further training needs to be devised.""",2022,2022-03-31T09:00:46Z,,,
arXIv2022,Reproducibility Issues for BERT-based Evaluation Metrics,No.,1,"The abstract discusses issues related to BERT-based evaluation metrics, but it does not mention LLMs or their limitations.",2022,2022-03-30T20:35:37Z,,,
arXIv2022,Training Compute-Optimal Large Language Models,Yes.,2,"""We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant.""",2022,2022-03-29T13:38:03Z,,,
arXIv2022,GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models,Yes.,2,"""questions remain about their ability to generalize beyond the small reference sets that are publicly available for research.""",2022,2022-03-25T00:25:42Z,,,
arXIv2022,Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation,Yes.,3,"""Neural language models show vulnerability to adversarial examples which are semantically similar to their original counterparts with a few words replaced by their synonyms.""",2022,2022-03-23T20:04:14Z,,,
arXIv2022,BERT-ASC: Implicit Aspect Representation Learning through Auxiliary-Sentence Construction for Sentiment Analysis,Yes.,2,"""Unfortunately, the aspect is often expressed implicitly through a set of representatives and thus renders implicit mapping process unattainable unless sufficient labeled examples are available. However, high-quality labeled examples may not be readily available in real-world scenarios.""",2022,2022-03-22T13:12:27Z,,,
arXIv2022,Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT,Yes.,3,"""the standard self-attention mechanism of the Transformer suffers from quadratic computational cost in the input sequence length.""",2022,2022-03-17T03:33:47Z,,,
arXIv2022,Multi-Stage Prompting for Knowledgeable Dialogue Generation,Yes.,3,"""These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed.""",2022,2022-03-16T16:53:43Z,,,
arXIv2022,Representation Learning for Resource-Constrained Keyphrase Generation,Yes.,1,"""based on a pre-trained language model using large-scale unlabeled documents.""",2022,2022-03-15T17:48:04Z,,,
arXIv2022,Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer,Yes.,3,"""Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias.""",2022,2022-03-14T22:02:40Z,,,
arXIv2022,"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",Yes.,3,"""manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models.""",2022,2022-03-14T16:54:46Z,,,
arXIv2022,The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models,Yes.,3,"""While these models are extremely accurate, they can be too large and computationally intensive to run on standard deployments.""",2022,2022-03-14T16:40:31Z,,,
arXIv2022,Can pre-trained Transformers be used in detecting complex sensitive sentences? -- A Monsanto case study,Yes.,1,"""In this paper, we wish to explore whether pre-trained transformer models are well suited to detect complex sensitive information.""",2022,2022-03-14T00:17:34Z,,,
arXIv2022,Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice,Yes.,3,"""In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models.""",2022,2022-03-12T15:34:54Z,,,
arXIv2022,Block-Sparse Adversarial Attack to Fool Transformer-Based Text Classifiers,No.,1,The abstract focuses on adversarial attacks against transformer-based text classifiers but does not specifically discuss language models (LMs or LLMs) or their limitations.,2022,2022-03-11T14:37:41Z,,,
arXIv2022,"Contextualized Sensorimotor Norms: multi-dimensional measures of sensorimotor strength for ambiguous English words, in context",Yes.,2,"""Most large language models are trained on linguistic input alone, yet humans appear to ground their understanding of words in sensorimotor experience.""",2022,2022-03-10T21:23:00Z,,,
arXIv2022,Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition,Yes.,3,"""However, such corpora have properties that hinder downstream performance, including being (1) too large, (2) beset with domain-mismatched content, and (3) heavy-headed rather than heavy-tailed (excessively many duplicate search queries such as 'weather').""",2022,2022-03-09T19:20:03Z,,,
arXIv2022,Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing,Yes.,1,"""We developed a rule-based Natural Language Processing (NLP) algorithm, machine learning models, and Large Language Model(LLM)-based NLP algorithms to automate the extraction of sleep-related concepts.""",2022,2022-03-08T21:20:19Z,,,
arXIv2022,What Did You Say? Task-Oriented Dialog Datasets Are Not Conversational!?,No.,1,The abstract does not mention LLMs or their limitations.,2022,2022-03-07T14:26:23Z,,,
arXIv2022,Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models,Yes.,3,"""one of the factors hindering the development of prompt-tuning on NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different from the pretraining corpus).""",2022,2022-03-07T05:04:32Z,,,
arXIv2022,LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models,Yes.,3,"""finding architectures with the optimal trade-off between task performance (perplexity) and hardware constraints like peak memory utilization and latency is non-trivial.""",2022,2022-03-04T02:10:43Z,,,
arXIv2022,BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification,Yes.,1,"""The proposed method optimises a bag of multi-label descriptors (BoMD) to promote their similarity with the semantic descriptors produced by BERT models from the multi-label image annotation.""",2022,2022-03-03T08:04:59Z,,,
arXIv2022,Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation,No.,1,The abstract focuses on neural machine translation models and does not mention LLMs or their limitations.,2022,2022-02-28T10:24:22Z,,,
arXIv2022,A Systematic Evaluation of Large Language Models of Code,Yes.,3,"""However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions.""",2022,2022-02-26T15:53:55Z,,,
arXIv2022,AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation,Yes.,1,"""we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC).""",2022,2022-02-26T03:17:08Z,,,
arXIv2022,TrimBERT: Tailoring BERT for Trade-offs,Yes.,3,"""many of these large models require a great deal of computational resources and/or time for pre-training and fine-tuning which limits wider adoptability.""",2022,2022-02-24T23:06:29Z,,,
arXIv2022,Ask2Mask: Guided Data Selection for Masked Speech Modeling,No.,1,The abstract discusses masked speech modeling (MSM) methods and their limitations but does not mention language models (LLMs or LMs).,2022,2022-02-24T17:34:54Z,,,
arXIv2022,Speciesist bias in AI -- How AI applications perpetuate discrimination and unfair outcomes against animals,Yes.,3,"""Speciesist biases are learned and solidified by AI applications when they are trained on datasets in which speciesist patterns prevail. These patterns can be found in image recognition systems, large language models, and recommender systems.""",2022,2022-02-22T12:23:21Z,,,
arXIv2022,Adaptive Discounting of Implicit Language Models in RNN-Transducers,Yes.,3,"""One main reason for the degradation in performance on rare words is that the language model (LM) internal to RNN-Ts can become overconfident and lead to hallucinated predictions that are acoustically inconsistent with the underlying speech.""",2022,2022-02-21T08:44:56Z,,,
arXIv2022,GPT-based Open-Ended Knowledge Tracing,Yes.,1,"""We develop an initial solution to the OKT problem, a student knowledge-guided code generation approach, that combines program synthesis methods using language models with student knowledge tracing methods.""",2022,2022-02-21T02:33:34Z,,,
arXIv2022,SGPT: GPT Sentence Embeddings for Semantic Search,Yes.,3,"""Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models.""",2022,2022-02-17T21:35:56Z,,,
arXIv2022,Information Extraction in Low-Resource Scenarios: Survey and Perspective,Yes.,2,"""we conduct empirical study on LLM-based methods compared with previous state-of-the-art models, and discover that (1) well-tuned LMs are still predominant; (2) tuning open-resource LLMs and ICL with GPT family is promising in general; (3) the optimal LLM-based",2022,2022-02-16T13:44:00Z,,,
arXIv2022,Should You Mask 15% in Masked Language Modeling?,Yes.,3,"""We first establish that 15% is not universally optimal, and larger models should adopt a higher masking rate.""",2022,2022-02-16T11:42:34Z,,,
arXIv2022,Knowledge Transfer from Large-scale Pretrained Language Models to End-to-end Speech Recognizers,Yes.,3,"""training of end-to-end speech recognizers always requires transcribed utterances. Since end-to-end models are also known to be severely data hungry, this constraint is crucial especially because obtaining transcribed utterances is costly and can possibly be impractical or impossible.""",2022,2022-02-16T07:02:24Z,,,
arXIv2022,"A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications",No.,1,The abstract focuses on Pretrained Graph Models (PGMs) and does not discuss language models (LLMs).,2022,2022-02-16T07:00:52Z,,,
arXIv2022,Predicting on the Edge: Identifying Where a Larger Model Does Better,Yes.,3,"""large models have the largest improvement on examples where the small model is most uncertain. On more certain examples, even those where the small model is not particularly accurate, large models are often unable to improve at all, and can even perform worse than the smaller model.""",2022,2022-02-15T18:53:14Z,,,
arXIv2022,"Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?",Yes.,1,"""We propose to use the information stored in pre-trained transformer models to assist us in the documentation process.""",2022,2022-02-14T13:00:31Z,,,
arXIv2022,Assessment of contextualised representations in detecting outcome phrases in clinical trials,Yes.,3,"""several contextualized representations like BERT and ELMO have achieved unparalleled success in detecting various diseases, genes, proteins, and chemicals, however, the same cannot be emphatically stated for outcomes, because these models have been relatively under-tested and studied for the OD task.""",2022,2022-02-13T15:08:00Z,,,
arXIv2022,ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification,Yes.,1,"""we propose a new traffic representation model called Encrypted Traffic Bidirectional Encoder Representations from Transformer (ET-BERT), which pre-trains deep contextualized datagram-level representation from large-scale unlabeled data.""",2022,2022-02-13T14:54:48Z,,,
arXIv2022,Semantic-Oriented Unlabeled Priming for Large-Scale Language Models,Yes.,2,"""Unfortunately, for in-context learning there is currently no way to leverage unlabeled data, which is often much easier to obtain in large quantities than labeled examples.""",2022,2022-02-12T19:50:59Z,,,
arXIv2022,Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam,Yes.,2,"""we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied. To alleviate this limitation, we propose 0/1 Adam that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation.""",2022,2022-02-12T08:02:23Z,,,
arXIv2022,Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations,Yes.,3,"""we begin by analyzing the challenges of using PLM representations for topic discovery""",2022,2022-02-09T17:26:08Z,,,
arXIv2022,Logical Reasoning for Task Oriented Dialogue Systems,Yes.,3,"""lack of reasoning capabilities of dialogue platforms make it difficult to provide relevant and fluent responses, unless the designers of a conversational experience spend a considerable amount of time implementing these capabilities in external rule based modules.""",2022,2022-02-08T21:46:27Z,,,
arXIv2022,Do Language Models Learn Position-Role Mappings?,Yes.,3,"""We do, however, observe some limitations of this generalization when tasks involve constructions with novel ditransitive verbs, hinting at a degree of lexical specificity which underlies model performance.""",2022,2022-02-08T02:50:53Z,,,
arXIv2022,Fine-Tuning Approach for Arabic Offensive Language Detection System: BERT-Based Model,No.,1,The abstract focuses on the problem of online offensive language detection using fine-tuning across several Arabic offensive language datasets and does not mention LLMs or their limitations.,2022,2022-02-07T17:26:35Z,,,
arXIv2022,Multilingual Hate Speech and Offensive Content Detection using Modified Cross-entropy Loss,Yes.,1,"""Large language models are trained on a lot of data and they also make use of contextual embeddings.""",2022,2022-02-05T20:31:40Z,,,
arXIv2022,JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One-Shot Learning,Yes.,3,"""In spite of their great success, these vector representations fail to capture meaning of idiomatic multi-word expressions (MWEs).""",2022,2022-02-04T21:17:41Z,,,
arXIv2022,GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records,Yes.,2,"""However, there are few clinical language models, the largest of which trained in the clinical domain is comparatively small at 110 million parameters (compared with billions of parameters in the general domain).""",2022,2022-02-02T14:28:51Z,,,
arXIv2022,Examining Scaling and Transfer of Language Model Architectures for Machine Translation,Yes.,3,"""Several design choices, including causal masking and language-modeling objectives for the source sequence, have detrimental effects on translation quality.""",2022,2022-02-01T16:20:15Z,,,
arXIv2022,ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise,Yes.,3,"""increasing the batch size often makes the optimization more difficult, leading to slow convergence or poor generalization that can require orders of magnitude more training time to achieve the same model quality.""",2022,2022-01-29T01:47:01Z,,,
arXIv2022,Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences,Yes.,3,"""One of the core limitations of these transformers is the substantial memory consumption due to their full self-attention mechanism.""",2022,2022-01-27T22:51:58Z,,,
arXIv2022,Going Extreme: Comparative Analysis of Hate Speech in Parler and Gab,No.,1,"The abstract focuses on hate speech analysis on social platforms like Parler and Gab, and mentions the use of BERT classifier but does not discuss LLMs or their limitations.",2022,2022-01-27T19:29:17Z,,,
arXIv2022,DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence,No.,1,The abstract discusses BERT-based evaluation metrics and their limitations in recognizing coherence but does not mention LLMs or their limitations.,2022,2022-01-26T20:28:26Z,,,
arXIv2022,Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs,Yes.,3,"""We observed a loss of prediction accuracy between 10 and 22 percentage points using the state-of-the-art models BERT and T5.""",2022,2022-01-20T17:34:33Z,,,
arXIv2022,AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees,Yes.,3,"""there are several challenges in applying these language models to solve programming language-related problems directly.""",2022,2022-01-20T03:27:26Z,,,
arXIv2022,Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,Yes.,3,"""However, the plans produced naively by LLMs often cannot map precisely to admissible actions.""",2022,2022-01-18T18:59:45Z,,,
arXIv2022,The Dark Side of the Language: Pre-trained Transformers in the DarkNet,Yes.,3,"""Surprisingly, results show that syntactic and lexical neural networks perform on par with pre-trained Transformers even after fine-tuning. Only after what we call extreme domain adaptation, that is, retraining with the masked language model task on all the novel corpus, pre-trained Transformers reach their standard high results",2022,2022-01-14T16:04:09Z,,,
arXIv2022,A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models,Yes.,3,"""However, due to the limited level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed.""",2022,2022-01-14T08:32:20Z,,,
arXIv2022,Latency Adjustable Transformer Encoder for Language Understanding,Yes.,1,"""The proposed method is applied to the BERT-base and GPT-2 models for evaluation.""",2022,2022-01-10T13:04:39Z,,,
arXIv2022,Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow,Yes.,1,"""We quantify the differences between autobiographical and imagined stories by introducing sequentiality, a measure of narrative flow of events, drawing probabilistic inferences from a cutting-edge large language model (GPT-3).""",2022,2022-01-07T20:10:47Z,,,
arXIv2022,Improving Mandarin End-to-End Speech Recognition with Word N-gram Language Model,Yes.,2,"""the use of subword-level LMs will ignore the word-level information, which may limit the strength of the external LMs in E2E ASR.""",2022,2022-01-06T10:04:56Z,,,
arXIv2022,Formal Analysis of Art: Proxy Learning of Visual Concepts from Style Through Language Models,Yes.,2,"""The language modeling is a practical and scalable solution requiring no labeling, but it is inevitably imperfect.""",2022,2022-01-05T21:03:29Z,,,
arXIv2022,On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations,Yes.,3,"""We show that these deep learning approaches including BERT are sensitive to such legitimate input perturbations on four standard benchmark datasets SST2, TREC-6, BBC News, and tweet_eval.""",2022,2022-01-02T08:33:49Z,,,
arXIv2022,Language model compression with weighted low-rank factorization,No.,1,"""No evidence""",2022,2022-06-30T21:57:07Z,,,
arXIv2022,GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation,No.,1,"""No evidence""",2022,2022-06-30T21:29:47Z,,,
arXIv2022,Denoised MDPs: Learning World Models Better Than the World Itself,No.,1,"""No evidence""",2022,2022-06-30T17:59:49Z,,,
arXIv2022,Forecasting Future World Events with Neural Networks,No.,1,"""No evidence""",2022,2022-06-30T17:59:14Z,,,
arXIv2022,Watch and Match: Supercharging Imitation with Regularized Optimal Transport,No.,1,"""No evidence""",2022,2022-06-30T17:58:18Z,,,
arXIv2022,Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations,No.,1,"""No evidence""",2022,2022-06-30T17:55:12Z,,,
arXIv2022,Two-Stage Classifier for COVID-19 Misinformation Detection Using BERT: a Study on Indonesian Tweets,No.,1,"""No evidence""",2022,2022-06-30T15:33:20Z,,,
arXIv2022,The Topological BERT: Transforming Attention into Topology for Natural Language Processing,No.,1,"""No evidence""",2022,2022-06-30T11:25:31Z,,,
arXIv2022,BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing,No.,1,"""No evidence""",2022,2022-06-30T07:15:45Z,,,
arXIv2022,"""Diversity and Uncertainty in Moderation"" are the Key to Data Selection for Multilingual Few-shot Transfer",No.,1,"""No evidence""",2022,2022-06-30T04:22:27Z,,,
arXIv2022,GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language,No.,1,"""No evidence""",2022,2022-06-30T04:06:26Z,,,
arXIv2022,GPTs at Factify 2022: Prompt Aided Fact-Verification,No.,1,"""No evidence""",2022,2022-06-29T21:07:39Z,,,
arXIv2022,Two-Stage COVID19 Classification Using BERT Features,No.,1,"""No evidence""",2022,2022-06-29T19:01:37Z,,,
arXIv2022,Improving Deliberation by Text-Only and Semi-Supervised Training,No.,1,"""No evidence""",2022,2022-06-29T15:30:44Z,,,
arXIv2022,Towards a Data-Driven Requirements Engineering Approach: Automatic Analysis of User Reviews,No.,1,"""No evidence""",2022,2022-06-29T14:14:54Z,,,
arXIv2022,Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody,No.,1,"""No evidence""",2022,2022-06-29T13:37:03Z,,,
arXIv2022,Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems,No.,1,"""No evidence""",2022,2022-06-29T13:12:46Z,,,
arXIv2022,Chinese Word Sense Embedding with SememeWSD and Synonym Set,No.,1,"""No evidence""",2022,2022-06-29T03:42:03Z,,,
arXIv2022,Proton: Probing Schema Linking Information from Pre-trained Language Models for Text-to-SQL Parsing,No.,1,"""No evidence""",2022,2022-06-28T14:05:25Z,,,
arXIv2022,Exploring linguistic feature and model combination for speech recognition based automatic AD detection,No.,1,"""No evidence""",2022,2022-06-28T05:09:01Z,,,
arXIv2022,Adaptive Multi-view Rule Discovery for Weakly-Supervised Compatible Products Prediction,No.,1,"""No evidence""",2022,2022-06-28T04:11:58Z,,,
arXIv2022,Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse,No.,1,"""No evidence""",2022,2022-06-28T02:56:12Z,,,
arXIv2022,Kwame for Science: An AI Teaching Assistant Based on Sentence-BERT for Science Education in West Africa,No.,1,"""No evidence""",2022,2022-06-28T02:27:23Z,,,
arXIv2022,Perspective (In)consistency of Paint by Text,No.,1,"""No evidence""",2022,2022-06-27T19:52:33Z,,,
arXIv2022,Materials Transformers Language Models for Generative Materials Design: a benchmark study,No.,1,"""No evidence""",2022,2022-06-27T18:50:05Z,,,
arXIv2022,PROTOtypical Logic Tensor Networks (PROTO-LTN) for Zero Shot Learning,No.,1,"""No evidence""",2022,2022-06-26T18:34:07Z,,,
arXIv2022,Repository-Level Prompt Generation for Large Language Models of Code,No.,1,"""No evidence""",2022,2022-06-26T10:51:25Z,,,
arXIv2022,TEVR: Improving Speech Recognition by Token Entropy Variance Reduction,No.,1,"""No evidence""",2022,2022-06-25T16:42:05Z,,,
arXIv2022,Distilling a Pretrained Language Model to a Multilingual ASR Model,No.,1,"""No evidence""",2022,2022-06-25T12:36:11Z,,,
arXIv2022,Adversarial Self-Attention for Language Understanding,No.,1,"""No evidence""",2022,2022-06-25T09:18:10Z,,,
arXIv2022,Construct a Sentence with Multiple Specified Words,No.,1,"""No evidence""",2022,2022-06-25T05:49:51Z,,,
arXIv2022,Value-Consistent Representation Learning for Data-Efficient Reinforcement Learning,No.,1,"""No evidence""",2022,2022-06-25T03:02:25Z,,,
arXIv2022,A Test for Evaluating Performance in Human-Computer Systems,No.,1,"""No evidence""",2022,2022-06-24T17:44:58Z,,,
arXIv2022,Using BERT Embeddings to Model Word Importance in Conversational Transcripts for Deaf and Hard of Hearing Users,No.,1,"""No evidence""",2022,2022-06-24T16:35:57Z,,,
arXIv2022,Text and author-level political inference using heterogeneous knowledge representations,No.,1,"""No evidence""",2022,2022-06-24T13:45:36Z,,,
arXIv2022,MVP: Multi-task Supervised Pre-training for Natural Language Generation,No.,1,"""No evidence""",2022,2022-06-24T07:49:47Z,,,
arXIv2022,Unified BERT for Few-shot Natural Language Understanding,No.,1,"""No evidence""",2022,2022-06-24T06:10:53Z,,,
arXIv2022,SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners,No.,1,"""No evidence""",2022,2022-06-24T02:17:13Z,,,
arXIv2022,Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content,No.,1,"""No evidence""",2022,2022-06-23T10:46:39Z,,,
arXIv2022,Mining Error Templates for Grammatical Error Correction,No.,1,"""No evidence""",2022,2022-06-23T09:29:52Z,,,
arXIv2022,Evaluating Generative Patent Language Models,No.,1,"""No evidence""",2022,2022-06-23T08:58:05Z,,,
arXIv2022,CGAR: Critic Guided Action Redistribution in Reinforcement Leaning,No.,1,"""No evidence""",2022,2022-06-23T06:33:14Z,,,
arXIv2022,DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon,No.,1,"""No evidence""",2022,2022-06-22T19:15:57Z,,,
arXIv2022,GODEL: Large-Scale Pre-Training for Goal-Directed Dialog,No.,1,"""No evidence""",2022,2022-06-22T18:19:32Z,,,
arXIv2022,Answer Fast: Accelerating BERT on the Tensor Streaming Processor,No.,1,"""No evidence""",2022,2022-06-22T13:27:27Z,,,
arXIv2022,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,No.,1,"""No evidence""",2022,2022-06-22T01:11:29Z,,,
arXIv2022,Efficient and effective training of language and graph neural network models,No.,1,"""No evidence""",2022,2022-06-22T00:23:37Z,,,
arXIv2022,BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing,No.,1,"""No evidence""",2022,2022-06-21T18:34:11Z,,,
arXIv2022,Questions Are All You Need to Train a Dense Passage Retriever,No.,1,"""No evidence""",2022,2022-06-21T18:16:31Z,,,
arXIv2022,EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine,No.,1,"""No evidence""",2022,2022-06-21T17:36:15Z,,,
arXIv2022,An Automatic and Efficient BERT Pruning for Edge AI Systems,No.,1,"""No evidence""",2022,2022-06-21T15:10:29Z,,,
arXIv2022,CoCoPIE XGen: A Full-Stack AI-Oriented Optimizing Framework,No.,1,"""No evidence""",2022,2022-06-21T14:10:22Z,,,
arXIv2022,KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP,No.,1,"""No evidence""",2022,2022-06-21T11:34:02Z,,,
arXIv2022,TAPHSIR: Towards AnaPHoric Ambiguity Detection and ReSolution In Requirements,No.,1,"""No evidence""",2022,2022-06-21T09:53:13Z,,,
arXIv2022,Knowledge Graph Fusion for Language Model Fine-tuning,No.,1,"""No evidence""",2022,2022-06-21T08:06:22Z,,,
arXIv2022,General Framework for Reversible Data Hiding in Texts Based on Masked Language Modeling,No.,1,"""No evidence""",2022,2022-06-21T05:02:49Z,,,
arXIv2022,Automatic Controllable Product Copywriting for E-Commerce,No.,1,"""No evidence""",2022,2022-06-21T04:18:52Z,,,
arXIv2022,SPBERTQA: A Two-Stage Question Answering System Based on Sentence Transformers for Medical Texts,No.,1,"""No evidence""",2022,2022-06-20T07:07:59Z,,,
arXIv2022,Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning,No.,1,"""No evidence""",2022,2022-06-19T09:21:27Z,,,
arXIv2022,An Embedded Feature Selection Framework for Control,No.,1,"""No evidence""",2022,2022-06-19T07:03:40Z,,,
arXIv2022,Can Language Models Capture Graph Semantics? From Graphs to Language Model and Vice-Versa,No.,1,"""No evidence""",2022,2022-06-18T18:12:20Z,,,
arXIv2022,Automatic Summarization of Russian Texts: Comparison of Extractive and Abstractive Methods,No.,1,"""No evidence""",2022,2022-06-18T17:28:04Z,,,
arXIv2022,Argumentative Text Generation in Economic Domain,No.,1,"""No evidence""",2022,2022-06-18T17:22:06Z,,,
arXIv2022,RuArg-2022: Argument Mining Evaluation,No.,1,"""No evidence""",2022,2022-06-18T17:13:37Z,,,
arXIv2022,VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection,No.,1,"""No evidence""",2022,2022-06-18T04:08:19Z,,,
arXIv2022,Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis,No.,1,"""No evidence""",2022,2022-06-17T23:07:33Z,,,
arXIv2022,niksss at HinglishEval: Language-agnostic BERT-based Contextual Embeddings with Catboost for Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text,No.,1,"""No evidence""",2022,2022-06-17T17:36:03Z,,,
arXIv2022,Language with Vision: a Study on Grounded Word and Sentence Embeddings,No.,1,"""No evidence""",2022,2022-06-17T15:04:05Z,,,
arXIv2022,BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish Text Using Transformers,No.,1,"""No evidence""",2022,2022-06-17T10:36:50Z,,,
arXIv2022,MSDF: A General Open-Domain Multi-Skill Dialog Framework,No.,1,"""No evidence""",2022,2022-06-17T08:38:53Z,,,
arXIv2022,Deep Multi-Task Models for Misogyny Identification and Categorization on Arabic Social Media,No.,1,"""No evidence""",2022,2022-06-16T18:54:37Z,,,
arXIv2022,Know your audience: specializing grounded language models with listener subtraction,No.,1,"""No evidence""",2022,2022-06-16T17:52:08Z,,,
arXIv2022,Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition,No.,1,"""No evidence""",2022,2022-06-16T17:24:14Z,,,
arXIv2022,A Language Model With Million Sample Context For Raw Audio Using Transformer Architectures,No.,1,"""No evidence""",2022,2022-06-16T16:57:43Z,,,
arXIv2022,Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator,No.,1,"""No evidence""",2022,2022-06-16T10:52:13Z,,,
arXIv2022,An Open-Domain QA System for e-Governance,No.,1,"""No evidence""",2022,2022-06-16T10:02:31Z,,,
arXIv2022,PreCogIIITH at HinglishEval : Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,No.,1,"""No evidence""",2022,2022-06-16T08:00:42Z,,,
arXIv2022,Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization,No.,1,"""No evidence""",2022,2022-06-16T02:17:49Z,,,
arXIv2022,Detecting Harmful Online Conversational Content towards LGBTQIA+ Individuals,No.,1,"""No evidence""",2022,2022-06-15T20:14:02Z,,,
arXIv2022,Emergent Abilities of Large Language Models,No.,1,"""No evidence""",2022,2022-06-15T17:32:01Z,,,
arXIv2022,A Survey : Neural Networks for AMR-to-Text,No.,1,"""No evidence""",2022,2022-06-15T07:20:28Z,,,
arXIv2022,"Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt",No.,1,"""No evidence""",2022,2022-06-14T19:49:52Z,,,
arXIv2022,RDU: A Region-based Approach to Form-style Document Understanding,No.,1,"""No evidence""",2022,2022-06-14T14:47:48Z,,,
arXIv2022,CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation,No.,1,"""No evidence""",2022,2022-06-14T14:44:34Z,,,
arXIv2022,LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning,No.,1,"""No evidence""",2022,2022-06-13T23:51:56Z,,,
arXIv2022,Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training,No.,1,"""No evidence""",2022,2022-06-13T22:15:21Z,,,
arXIv2022,Language Models are General-Purpose Interfaces,No.,1,"""No evidence""",2022,2022-06-13T17:34:22Z,,,
arXIv2022,JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding,No.,1,"""No evidence""",2022,2022-06-13T17:03:52Z,,,
arXIv2022,Transition-based Abstract Meaning Representation Parsing with Contextual Embeddings,No.,1,"""No evidence""",2022,2022-06-13T15:05:24Z,,,
arXIv2022,Self-critiquing models for assisting human evaluators,No.,1,"""No evidence""",2022,2022-06-12T17:40:53Z,,,
arXIv2022,Improving Pre-trained Language Model Fine-tuning with Noise Stability Regularization,No.,1,"""No evidence""",2022,2022-06-12T04:42:49Z,,,
arXIv2022,Dealing with Sparse Rewards in Continuous Control Robotics via Heavy-Tailed Policies,No.,1,"""No evidence""",2022,2022-06-12T04:09:39Z,,,
arXIv2022,DeepEmotex: Classifying Emotion in Text Messages using Deep Transfer Learning,No.,1,"""No evidence""",2022,2022-06-12T03:23:40Z,,,
arXIv2022,Comparative Snippet Generation,No.,1,"""No evidence""",2022,2022-06-11T09:02:27Z,,,
arXIv2022,From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams,No.,1,"""No evidence""",2022,2022-06-11T06:38:06Z,,,
arXIv2022,A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction,No.,1,"""No evidence""",2022,2022-06-10T16:51:45Z,,,
arXIv2022,Putting GPT-3's Creativity to the (Alternative Uses) Test,No.,1,"""No evidence""",2022,2022-06-10T15:36:45Z,,,
arXIv2022,Unsupervised and Few-shot Parsing from Pretrained Language Models,No.,1,"""No evidence""",2022,2022-06-10T10:29:15Z,,,
arXIv2022,Sort by Structure: Language Model Ranking as Dependency Probing,No.,1,"""No evidence""",2022,2022-06-10T08:10:29Z,,,
arXIv2022,Extracting Zero-shot Common Sense from Large Language Models for Robot 3D Scene Understanding,No.,1,"""No evidence""",2022,2022-06-09T16:05:35Z,,,
arXIv2022,SsciBERT: A Pre-trained Language Model for Social Science Texts,No.,1,"""No evidence""",2022,2022-06-09T13:49:04Z,,,
arXIv2022,Joint Encoder-Decoder Self-Supervised Pre-training for ASR,No.,1,"""No evidence""",2022,2022-06-09T12:45:29Z,,,
arXIv2022,Context-based out-of-vocabulary word recovery for ASR systems in Indian languages,No.,1,"""No evidence""",2022,2022-06-09T06:51:31Z,,,
arXIv2022,Abstraction not Memory: BERT and the English Article System,No.,1,"""No evidence""",2022,2022-06-08T22:36:54Z,,,
arXIv2022,Learning to Generate Prompts for Dialogue Generation through Reinforcement Learning,No.,1,"""No evidence""",2022,2022-06-08T14:48:06Z,,,
arXIv2022,Revealing Single Frame Bias for Video-and-Language Learning,No.,1,"""No evidence""",2022,2022-06-07T16:28:30Z,,,
arXIv2022,Always Keep your Target in Mind: Studying Semantics and Improving Performance of Neural Lexical Substitution,No.,1,"""No evidence""",2022,2022-06-07T16:16:19Z,,,
arXIv2022,Searching for Optimal Subword Tokenization in Cross-domain NER,No.,1,"""No evidence""",2022,2022-06-07T14:39:31Z,,,
arXIv2022,OCHADAI at SemEval-2022 Task 2: Adversarial Training for Multilingual Idiomaticity Detection,No.,1,"""No evidence""",2022,2022-06-07T05:52:43Z,,,
arXIv2022,DynaMaR: Dynamic Prompt with Mask Token Representation,No.,1,"""No evidence""",2022,2022-06-07T02:54:36Z,,,
arXIv2022,Global Mixup: Eliminating Ambiguity with Clustering,No.,1,"""No evidence""",2022,2022-06-06T16:42:22Z,,,
arXIv2022,What do tokens know about their characters and how do they know it?,No.,1,"""No evidence""",2022,2022-06-06T13:27:26Z,,,
arXIv2022,Improving Contrastive Learning of Sentence Embeddings with Case-Augmented Positives and Retrieved Negatives,No.,1,"""No evidence""",2022,2022-06-06T09:46:12Z,,,
arXIv2022,Spam Detection Using BERT,No.,1,"""No evidence""",2022,2022-06-06T09:09:40Z,,,
arXIv2022,A computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface of dependency resolution and training time,No.,1,"""No evidence""",2022,2022-06-06T09:03:11Z,,,
arXIv2022,A sentiment analysis model for car review texts based on adversarial training and whole word mask BERT,No.,1,"""No evidence""",2022,2022-06-06T06:45:43Z,,,
arXIv2022,OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression,No.,1,"""No evidence""",2022,2022-06-06T03:54:53Z,,,
arXIv2022,Sentiment Analysis of Online Travel Reviews Based on Capsule Network and Sentiment Lexicon,No.,1,"""No evidence""",2022,2022-06-05T12:17:46Z,,,
arXIv2022,"Speech Detection Task Against Asian Hate: BERT the Central, While Data-Centric Studies the Crucial",No.,1,"""No evidence""",2022,2022-06-05T07:41:24Z,,,
arXIv2022,Actuarial Applications of Natural Language Processing Using Transformers: Case Studies for Using Text Features in an Actuarial Context,No.,1,"""No evidence""",2022,2022-06-04T15:39:30Z,,,
arXIv2022,Comparing Performance of Different Linguistically-Backed Word Embeddings for Cyberbullying Detection,No.,1,"""No evidence""",2022,2022-06-04T09:11:41Z,,,
arXIv2022,ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers,No.,1,"""No evidence""",2022,2022-06-04T00:28:21Z,,,
arXIv2022,Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning,No.,1,"""No evidence""",2022,2022-06-03T22:33:09Z,,,
arXIv2022,"Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing Metrics, and a Novel Simple Metric",No.,1,"""No evidence""",2022,2022-06-03T21:23:05Z,,,
arXIv2022,Extracting Similar Questions From Naturally-occurring Business Conversations,No.,1,"""No evidence""",2022,2022-06-03T14:13:44Z,,,
arXIv2022,Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian,No.,1,"""No evidence""",2022,2022-06-03T14:12:33Z,,,
arXIv2022,TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models,No.,1,"""No evidence""",2022,2022-06-03T13:00:48Z,,,
arXIv2022,Latent Topology Induction for Understanding Contextualized Representations,No.,1,"""No evidence""",2022,2022-06-03T11:22:48Z,,,
arXIv2022,Automatic Generation of Programming Exercises and Code Explanations using Large Language Models,No.,1,"""No evidence""",2022,2022-06-03T11:00:43Z,,,
arXIv2022,"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code",No.,1,"""No evidence""",2022,2022-06-02T23:15:42Z,,,
arXIv2022,Decentralized Training of Foundation Models in Heterogeneous Environments,No.,1,"""No evidence""",2022,2022-06-02T20:19:51Z,,,
arXIv2022,A Multi-Policy Framework for Deep Learning-Based Fake News Detection,No.,1,"""No evidence""",2022,2022-06-01T21:25:21Z,,,
arXIv2022,On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting,No.,1,"""No evidence""",2022,2022-06-01T20:54:41Z,,,
arXIv2022,Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training,No.,1,"""No evidence""",2022,2022-06-01T16:45:24Z,,,
arXIv2022,Romantic-Computing,No.,1,"""No evidence""",2022,2022-06-01T14:27:17Z,,,
arXIv2022,MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining,No.,1,"""No evidence""",2022,2022-06-01T08:27:19Z,,,
arXIv2022,Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback,No.,1,"""No evidence""",2022,2022-06-01T05:01:36Z,,,
arXIv2022,FHIST: A Benchmark for Few-shot Classification of Histological Images,No.,1,"""No evidence""",2022,2022-05-31T20:03:40Z,,,
arXIv2022,The Contribution of Lyrics and Acoustics to Collaborative Understanding of Mood,No.,1,"""No evidence""",2022,2022-05-31T19:58:41Z,,,
arXIv2022,"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation",No.,1,"""No evidence""",2022,2022-05-31T17:58:49Z,,,
arXIv2022,Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints,No.,1,"""No evidence""",2022,2022-05-31T16:50:46Z,,,
arXIv2022,Knowledge Graph - Deep Learning: A Case Study in Question Answering in Aviation Safety Domain,No.,1,"""No evidence""",2022,2022-05-31T16:49:55Z,,,
arXIv2022,The CLRS Algorithmic Reasoning Benchmark,No.,1,"""No evidence""",2022,2022-05-31T09:56:44Z,,,
arXIv2022,hmBERT: Historical Multilingual Language Models for Named Entity Recognition,No.,1,"""No evidence""",2022,2022-05-31T07:30:33Z,,,
arXIv2022,A Unified Framework for Emotion Identification and Generation in Dialogues,No.,1,"""No evidence""",2022,2022-05-31T02:58:49Z,,,
arXIv2022,Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking,No.,1,"""No evidence""",2022,2022-05-31T01:58:04Z,,,
arXIv2022,FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm,No.,1,"""No evidence""",2022,2022-05-31T00:44:57Z,,,
arXIv2022,Billions of Parameters Are Worth More Than In-domain Training Data: A case study in the Legal Case Entailment Task,No.,1,"""No evidence""",2022,2022-05-30T15:21:26Z,,,
arXIv2022,ZusammenQA: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System,No.,1,"""No evidence""",2022,2022-05-30T10:31:08Z,,,
arXIv2022,Multi-Agent Reinforcement Learning is a Sequence Modeling Problem,No.,1,"""No evidence""",2022,2022-05-30T09:39:45Z,,,
arXIv2022,E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation,No.,1,"""No evidence""",2022,2022-05-30T08:25:36Z,,,
arXIv2022,Rites de Passage: Elucidating Displacement to Emplacement of Refugees on Twitter,No.,1,"""No evidence""",2022,2022-05-30T05:12:34Z,,,
arXIv2022,COVID-19 Literature Mining and Retrieval using Text Mining Approaches,No.,1,"""No evidence""",2022,2022-05-29T22:34:19Z,,,
arXIv2022,Urdu News Article Recommendation Model using Natural Language Processing Techniques,No.,1,"""No evidence""",2022,2022-05-29T12:43:32Z,,,
arXIv2022,Micro-Expression Recognition Based on Attribute Information Embedding and Cross-modal Contrastive Learning,No.,1,"""No evidence""",2022,2022-05-29T12:28:10Z,,,
arXIv2022,Learning Locality and Isotropy in Dialogue Modeling,No.,1,"""No evidence""",2022,2022-05-29T06:48:53Z,,,
arXIv2022,MiniDisc: Minimal Distillation Schedule for Language Model Compression,No.,1,"""No evidence""",2022,2022-05-29T04:22:48Z,,,
arXIv2022,Happenstance: Utilizing Semantic Search to Track Russian State Media Narratives about the Russo-Ukrainian War On Reddit,No.,1,"""No evidence""",2022,2022-05-28T16:54:53Z,,,
arXIv2022,Teaching Models to Express Their Uncertainty in Words,No.,1,"""No evidence""",2022,2022-05-28T05:02:31Z,,,
arXIv2022,Multimodal Fake News Detection via CLIP-Guided Learning,No.,1,"""No evidence""",2022,2022-05-28T02:43:18Z,,,
arXIv2022,Few-shot Subgoal Planning with Language Models,No.,1,"""No evidence""",2022,2022-05-28T01:03:30Z,,,
arXIv2022,Controllable Text Generation with Neurally-Decomposed Oracle,No.,1,"""No evidence""",2022,2022-05-27T20:17:53Z,,,
arXIv2022,Diffusion-LM Improves Controllable Text Generation,No.,1,"""No evidence""",2022,2022-05-27T20:12:09Z,,,
arXIv2022,Multimodal Masked Autoencoders Learn Transferable Representations,No.,1,"""No evidence""",2022,2022-05-27T19:09:42Z,,,
arXIv2022,FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,No.,1,"""No evidence""",2022,2022-05-27T17:53:09Z,,,
arXIv2022,StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes,No.,1,"""No evidence""",2022,2022-05-27T15:09:56Z,,,
arXIv2022,kNN-Prompt: Nearest Neighbor Zero-Shot Inference,No.,1,"""No evidence""",2022,2022-05-27T07:00:59Z,,,
arXIv2022,Training and Inference on Any-Order Autoregressive Models the Right Way,No.,1,"""No evidence""",2022,2022-05-26T18:00:02Z,,,
arXIv2022,Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality,No.,1,"""No evidence""",2022,2022-05-26T17:40:52Z,,,
arXIv2022,Federated Split BERT for Heterogeneous Text Classification,No.,1,"""No evidence""",2022,2022-05-26T12:21:57Z,,,
arXIv2022,Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks,No.,1,"""No evidence""",2022,2022-05-26T05:27:50Z,,,
arXIv2022,Matryoshka Representation Learning,No.,1,"""No evidence""",2022,2022-05-26T04:33:56Z,,,
arXIv2022,NaturalProver: Grounded Mathematical Proof Generation with Language Models,No.,1,"""No evidence""",2022,2022-05-25T17:01:18Z,,,
arXIv2022,"Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models",No.,1,"""No evidence""",2022,2022-05-25T11:54:37Z,,,
arXIv2022,Investigating the Benefits of Free-Form Rationales,No.,1,"""No evidence""",2022,2022-05-25T11:46:11Z,,,
arXIv2022,Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations,No.,1,"""No evidence""",2022,2022-05-25T11:45:14Z,,,
arXIv2022,Training Language Models with Memory Augmentation,No.,1,"""No evidence""",2022,2022-05-25T11:37:29Z,,,
arXIv2022,Few-shot Reranking for Multi-hop QA via Language Model Prompting,No.,1,"""No evidence""",2022,2022-05-25T10:45:55Z,,,
arXIv2022,Lifelong Learning Natural Language Processing Approach for Multilingual Data Classification,No.,1,"""No evidence""",2022,2022-05-25T10:34:04Z,,,
arXIv2022,Multimodal Knowledge Alignment with Reinforcement Learning,No.,1,"""No evidence""",2022,2022-05-25T10:12:17Z,,,
arXIv2022,Autoformalization with Large Language Models,No.,1,"""No evidence""",2022,2022-05-25T09:53:30Z,,,
arXIv2022,ORCA: Interpreting Prompted Language Models via Locating Supporting Data Evidence in the Ocean of Pretraining Data,No.,1,"""No evidence""",2022,2022-05-25T09:25:06Z,,,
arXIv2022,Is a Question Decomposition Unit All We Need?,No.,1,"""No evidence""",2022,2022-05-25T07:24:09Z,,,
arXIv2022,Segmenting Numerical Substitution Ciphers,No.,1,"""No evidence""",2022,2022-05-25T06:45:59Z,,,
arXIv2022,Text-to-Face Generation with StyleGAN2,No.,1,"""No evidence""",2022,2022-05-25T06:02:01Z,,,
arXIv2022,Fine-grained Contrastive Learning for Relation Extraction,No.,1,"""No evidence""",2022,2022-05-25T05:03:01Z,,,
arXIv2022,Conditional set generation using Seq2seq models,No.,1,"""No evidence""",2022,2022-05-25T04:17:50Z,,,
arXIv2022,Low Resource Style Transfer via Domain Adaptive Meta Learning,No.,1,"""No evidence""",2022,2022-05-25T03:58:24Z,,,
arXIv2022,Improving CTC-based ASR Models with Gated Interlayer Collaboration,No.,1,"""No evidence""",2022,2022-05-25T03:21:27Z,,,
arXIv2022,Know Where You're Going: Meta-Learning for Parameter-Efficient Fine-Tuning,No.,1,"""No evidence""",2022,2022-05-25T02:51:57Z,,,
arXIv2022,Sparse*BERT: Sparse Models Generalize To New tasks and Domains,No.,1,"""No evidence""",2022,2022-05-25T02:51:12Z,,,
arXIv2022,FLUTE: Figurative Language Understanding through Textual Explanations,No.,1,"""No evidence""",2022,2022-05-24T23:25:02Z,,,
arXIv2022,Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT,No.,1,"""No evidence""",2022,2022-05-24T23:08:54Z,,,
arXIv2022,K-12BERT: BERT for K-12 education,No.,1,"""No evidence""",2022,2022-05-24T19:35:41Z,,,
arXIv2022,Garden-Path Traversal in GPT-2,No.,1,"""No evidence""",2022,2022-05-24T18:21:58Z,,,
arXIv2022,EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start,No.,1,"""No evidence""",2022,2022-05-24T17:13:22Z,,,
arXIv2022,Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift,No.,1,"""No evidence""",2022,2022-05-24T16:41:30Z,,,
arXIv2022,RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder,No.,1,"""No evidence""",2022,2022-05-24T12:43:04Z,,,
arXIv2022,ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,No.,1,"""No evidence""",2022,2022-05-24T10:48:33Z,,,
arXIv2022,Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition,No.,1,"""No evidence""",2022,2022-05-24T05:36:13Z,,,
arXIv2022,PERT: A New Solution to Pinyin to Character Conversion Task,No.,1,"""No evidence""",2022,2022-05-24T03:08:27Z,,,
arXIv2022,On the Role of Bidirectionality in Language Model Pre-Training,No.,1,"""No evidence""",2022,2022-05-24T02:25:05Z,,,
arXIv2022,Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions,No.,1,"""No evidence""",2022,2022-05-23T22:45:53Z,,,
arXIv2022,FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?,No.,1,"""No evidence""",2022,2022-05-23T22:44:34Z,,,
arXIv2022,Simple Recurrence Improves Masked Language Models,No.,1,"""No evidence""",2022,2022-05-23T19:38:23Z,,,
arXIv2022,Learning to Ignore Adversarial Attacks,No.,1,"""No evidence""",2022,2022-05-23T18:01:30Z,,,
arXIv2022,HyperTree Proof Search for Neural Theorem Proving,No.,1,"""No evidence""",2022,2022-05-23T17:49:55Z,,,
arXIv2022,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,No.,1,"""No evidence""",2022,2022-05-23T17:42:53Z,,,
arXIv2022,"Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond",No.,1,"""No evidence""",2022,2022-05-23T17:37:20Z,,,
arXIv2022,A Question-Answer Driven Approach to Reveal Affirmative Interpretations from Verbal Negations,No.,1,"""No evidence""",2022,2022-05-23T17:08:30Z,,,
arXIv2022,Multilingual Extraction and Categorization of Lexical Collocations with Graph-aware Transformers,No.,1,"""No evidence""",2022,2022-05-23T16:47:37Z,,,
arXIv2022,The Diminishing Returns of Masked Language Models to Science,No.,1,"""No evidence""",2022,2022-05-23T14:35:08Z,,,
arXIv2022,KOLD: Korean Offensive Language Dataset,No.,1,"""No evidence""",2022,2022-05-23T13:58:45Z,,,
arXIv2022,BBTv2: Towards a Gradient-Free Future with Large Language Models,No.,1,"""No evidence""",2022,2022-05-23T11:10:19Z,,,
arXIv2022,Prompt Tuning for Discriminative Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-05-23T10:11:50Z,,,
arXIv2022,RuNNE-2022 Shared Task: Recognizing Nested Named Entities,No.,1,"""No evidence""",2022,2022-05-23T09:50:42Z,,,
arXIv2022,Supporting Vision-Language Model Inference with Confounder-pruning Knowledge Prompt,No.,1,"""No evidence""",2022,2022-05-23T07:51:15Z,,,
arXIv2022,BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla,No.,1,"""No evidence""",2022,2022-05-23T06:54:56Z,,,
arXIv2022,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,No.,1,"""No evidence""",2022,2022-05-23T03:51:27Z,,,
arXIv2022,Artificial intelligence for topic modelling in Hindu philosophy: mapping themes between the Upanishads and the Bhagavad Gita,No.,1,"""No evidence""",2022,2022-05-23T03:39:00Z,,,
arXIv2022,What should I Ask: A Knowledge-driven Approach for Follow-up Questions Generation in Conversational Surveys,No.,1,"""No evidence""",2022,2022-05-23T00:57:33Z,,,
arXIv2022,The Geometry of Multilingual Language Model Representations,No.,1,"""No evidence""",2022,2022-05-22T23:58:24Z,,,
arXIv2022,A Graph Enhanced BERT Model for Event Prediction,No.,1,"""No evidence""",2022,2022-05-22T13:37:38Z,,,
arXIv2022,GraphMAE: Self-Supervised Masked Graph Autoencoders,No.,1,"""No evidence""",2022,2022-05-22T11:57:08Z,,,
arXIv2022,Instruction Induction: From Few Examples to Natural Language Task Descriptions,No.,1,"""No evidence""",2022,2022-05-22T09:22:37Z,,,
arXIv2022,A Domain-adaptive Pre-training Approach for Language Bias Detection in News,No.,1,"""No evidence""",2022,2022-05-22T08:18:19Z,,,
arXIv2022,Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models,No.,1,"""No evidence""",2022,2022-05-22T07:43:50Z,,,
arXIv2022,Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners,No.,1,"""No evidence""",2022,2022-05-22T05:18:27Z,,,
arXIv2022,Housekeep: Tidying Virtual Households using Commonsense Reasoning,No.,1,"""No evidence""",2022,2022-05-22T02:37:09Z,,,
arXIv2022,Life after BERT: What do Other Muppets Understand about Language?,No.,1,"""No evidence""",2022,2022-05-21T23:57:17Z,,,
arXIv2022,Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding,No.,1,"""No evidence""",2022,2022-05-21T22:38:19Z,,,
arXIv2022,Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,No.,1,"""No evidence""",2022,2022-05-21T15:34:53Z,,,
arXIv2022,HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking,No.,1,"""No evidence""",2022,2022-05-21T11:38:33Z,,,
arXIv2022,Scenario-based Multi-product Advertising Copywriting Generation for E-Commerce,No.,1,"""No evidence""",2022,2022-05-21T07:45:53Z,,,
arXIv2022,Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese,No.,1,"""No evidence""",2022,2022-05-21T06:44:59Z,,,
arXIv2022,Computable Artificial General Intelligence,No.,1,"""No evidence""",2022,2022-05-21T06:32:09Z,,,
arXIv2022,A Study on Transformer Configuration and Training Objective,No.,1,"""No evidence""",2022,2022-05-21T05:17:11Z,,,
arXIv2022,Named Entity Linking with Entity Representation by Multiple Embeddings,No.,1,"""No evidence""",2022,2022-05-21T03:31:25Z,,,
arXIv2022,DeepStruct: Pretraining of Language Models for Structure Prediction,No.,1,"""No evidence""",2022,2022-05-21T00:58:22Z,,,
arXIv2022,Current Trends and Approaches in Synonyms Extraction: Potential Adaptation to Arabic,No.,1,"""No evidence""",2022,2022-05-20T19:05:10Z,,,
arXIv2022,UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes,No.,1,"""No evidence""",2022,2022-05-20T17:47:59Z,,,
arXIv2022,Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions,No.,1,"""No evidence""",2022,2022-05-20T14:52:03Z,,,
arXIv2022,Progressive Class Semantic Matching for Semi-supervised Text Classification,No.,1,"""No evidence""",2022,2022-05-20T13:59:03Z,,,
arXIv2022,Adversarial Body Shape Search for Legged Robots,No.,1,"""No evidence""",2022,2022-05-20T13:55:47Z,,,
arXIv2022,Prototypical Calibration for Few-shot Learning of Language Models,No.,1,"""No evidence""",2022,2022-05-20T13:50:07Z,,,
arXIv2022,Visually-Augmented Language Modeling,No.,1,"""No evidence""",2022,2022-05-20T13:41:12Z,,,
arXIv2022,Adversarial joint attacks on legged robots,No.,1,"""No evidence""",2022,2022-05-20T11:30:23Z,,,
arXIv2022,Exploring Extreme Parameter Compression for Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-05-20T09:16:55Z,,,
arXIv2022,Evaluating and Inducing Personality in Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-05-20T07:32:57Z,,,
arXIv2022,KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,No.,1,"""No evidence""",2022,2022-05-20T01:25:57Z,,,
arXIv2022,Enhancing Slot Tagging with Intent Features for Task Oriented Natural Language Understanding using BERT,No.,1,"""No evidence""",2022,2022-05-19T17:41:04Z,,,
arXIv2022,RankGen: Improving Text Generation with Large Ranking Models,No.,1,"""No evidence""",2022,2022-05-19T17:36:46Z,,,
arXIv2022,ArabGlossBERT: Fine-Tuning BERT on Context-Gloss Pairs for WSD,No.,1,"""No evidence""",2022,2022-05-19T16:47:18Z,,,
arXIv2022,Wojood: Nested Arabic Named Entity Corpus and Recognition using BERT,No.,1,"""No evidence""",2022,2022-05-19T16:06:49Z,,,
arXIv2022,Acceptability Judgements via Examining the Topology of Attention Maps,No.,1,"""No evidence""",2022,2022-05-19T15:45:12Z,,,
arXIv2022,Automatic Spoken Language Identification using a Time-Delay Neural Network,No.,1,"""No evidence""",2022,2022-05-19T13:47:48Z,,,
arXIv2022,Psychiatric Scale Guided Risky Post Screening for Early Detection of Depression,No.,1,"""No evidence""",2022,2022-05-19T12:11:01Z,,,
arXIv2022,Nebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters,No.,1,"""No evidence""",2022,2022-05-19T11:10:14Z,,,
arXIv2022,A Weakly-Supervised Iterative Graph-Based Approach to Retrieve COVID-19 Misinformation Topics,No.,1,"""No evidence""",2022,2022-05-19T09:30:39Z,,,
arXIv2022,Debiasing Neural Retrieval via In-batch Balancing Regularization,No.,1,"""No evidence""",2022,2022-05-18T22:57:15Z,,,
arXIv2022,Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner,No.,1,"""No evidence""",2022,2022-05-18T21:52:11Z,,,
arXIv2022,LeRaC: Learning Rate Curriculum,No.,1,"""No evidence""",2022,2022-05-18T18:57:36Z,,,
arXIv2022,Masked Autoencoders As Spatiotemporal Learners,No.,1,"""No evidence""",2022,2022-05-18T17:59:59Z,,,
arXIv2022,Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator,No.,1,"""No evidence""",2022,2022-05-18T16:40:50Z,,,
arXIv2022,GPoeT-2: A GPT-2 Based Poem Generator,No.,1,"""No evidence""",2022,2022-05-18T10:25:12Z,,,
arXIv2022,Evaluation of Transfer Learning for Polish with a Text-to-Text Model,No.,1,"""No evidence""",2022,2022-05-18T09:17:14Z,,,
arXIv2022,PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for Multi-stage Ranking,No.,1,"""No evidence""",2022,2022-05-18T04:38:15Z,,,
arXIv2022,ViralBERT: A User Focused BERT-Based Approach to Virality Prediction,No.,1,"""No evidence""",2022,2022-05-17T21:40:24Z,,,
arXIv2022,AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars,No.,1,"""No evidence""",2022,2022-05-17T17:59:19Z,,,
arXIv2022,Feature Aggregation in Zero-Shot Cross-Lingual Transfer Using Multilingual BERT,No.,1,"""No evidence""",2022,2022-05-17T17:12:19Z,,,
arXIv2022,SKILL: Structured Knowledge Infusion for Large Language Models,No.,1,"""No evidence""",2022,2022-05-17T09:12:22Z,,,
arXIv2022,SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation,No.,1,"""No evidence""",2022,2022-05-17T08:58:48Z,,,
arXIv2022,SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection,No.,1,"""No evidence""",2022,2022-05-17T07:51:55Z,,,
arXIv2022,Harnessing Multilingual Resources to Question Answering in Arabic,No.,1,"""No evidence""",2022,2022-05-16T23:28:01Z,,,
arXIv2022,The Primacy Bias in Deep Reinforcement Learning,No.,1,"""No evidence""",2022,2022-05-16T16:48:36Z,,,
arXIv2022,Qualitative Differences Between Evolutionary Strategies and Reinforcement Learning Methods for Control of Autonomous Agents,No.,1,"""No evidence""",2022,2022-05-16T11:51:36Z,,,
arXIv2022,Chemical transformer compression for accelerating both training and inference of molecular modeling,No.,1,"""No evidence""",2022,2022-05-16T11:38:31Z,,,
arXIv2022,"Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data",No.,1,"""No evidence""",2022,2022-05-16T10:08:11Z,,,
arXIv2022,Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing Pipelines,No.,1,"""No evidence""",2022,2022-05-15T21:53:45Z,,,
arXIv2022,TiBERT: Tibetan Pre-trained Language Model,No.,1,"""No evidence""",2022,2022-05-15T14:45:08Z,,,
arXIv2022,Classifiers are Better Experts for Controllable Text Generation,No.,1,"""No evidence""",2022,2022-05-15T12:58:35Z,,,
arXIv2022,Topic Modelling on Consumer Financial Protection Bureau Data: An Approach Using BERT Based Embeddings,No.,1,"""No evidence""",2022,2022-05-15T11:14:47Z,,,
arXIv2022,Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT,No.,1,"""No evidence""",2022,2022-05-15T04:48:41Z,,,
arXIv2022,Naturalistic Causal Probing for Morpho-Syntax,No.,1,"""No evidence""",2022,2022-05-14T11:47:58Z,,,
arXIv2022,Fake News Quick Detection on Dynamic Heterogeneous Information Networks,No.,1,"""No evidence""",2022,2022-05-14T11:23:25Z,,,
arXIv2022,RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL,No.,1,"""No evidence""",2022,2022-05-14T06:27:40Z,,,
arXIv2022,Unified Distributed Environment,No.,1,"""No evidence""",2022,2022-05-14T02:27:35Z,,,
arXIv2022,Bootstrapping Text Anonymization Models with Distant Supervision,No.,1,"""No evidence""",2022,2022-05-13T21:10:14Z,,,
arXIv2022,A Study of the Attention Abnormality in Trojaned BERTs,No.,1,"""No evidence""",2022,2022-05-13T16:48:37Z,,,
arXIv2022,Controlling Translation Formality Using Pre-trained Multilingual Language Models,No.,1,"""No evidence""",2022,2022-05-13T13:47:28Z,,,
arXIv2022,"Analyzing Hate Speech Data along Racial, Gender and Intersectional Axes",No.,1,"""No evidence""",2022,2022-05-13T13:13:46Z,,,
arXIv2022,Weakly Supervised Text Classification using Supervision Signals from a Language Model,No.,1,"""No evidence""",2022,2022-05-13T12:57:15Z,,,
arXIv2022,Improving Contextual Representation with Gloss Regularized Pre-training,No.,1,"""No evidence""",2022,2022-05-13T12:50:32Z,,,
arXIv2022,ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation,No.,1,"""No evidence""",2022,2022-05-13T06:08:35Z,,,
arXIv2022,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,No.,1,"""No evidence""",2022,2022-05-13T03:21:09Z,,,
arXIv2022,Localized Vision-Language Matching for Open-vocabulary Object Detection,No.,1,"""No evidence""",2022,2022-05-12T15:34:37Z,,,
arXIv2022,Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?,No.,1,"""No evidence""",2022,2022-05-12T15:19:54Z,,,
arXIv2022,Efficient and Training-Free Control of Language Generation,No.,1,"""No evidence""",2022,2022-05-12T11:48:11Z,,,
arXIv2022,SimCPSR: Simple Contrastive Learning for Paper Submission Recommendation System,No.,1,"""No evidence""",2022,2022-05-12T08:08:22Z,,,
arXIv2022,NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension,No.,1,"""No evidence""",2022,2022-05-12T06:54:03Z,,,
arXIv2022,AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling,No.,1,"""No evidence""",2022,2022-05-12T03:22:07Z,,,
arXIv2022,AppTek's Submission to the IWSLT 2022 Isometric Spoken Language Translation Task,No.,1,"""No evidence""",2022,2022-05-12T00:02:24Z,,,
arXIv2022,"A time-varying study of Chinese investor sentiment, stock market liquidity and volatility: Based on deep learning BERT model and TVP-VAR model",No.,1,"""No evidence""",2022,2022-05-11T18:16:26Z,,,
arXIv2022,Ontology-Driven and Weakly Supervised Rare Disease Identification from Clinical Notes,No.,1,"""No evidence""",2022,2022-05-11T17:38:24Z,,,
arXIv2022,Aggregating Pairwise Semantic Differences for Few-Shot Claim Veracity Classification,No.,1,"""No evidence""",2022,2022-05-11T17:23:37Z,,,
arXIv2022,UL2: Unifying Language Learning Paradigms,No.,1,"""No evidence""",2022,2022-05-10T19:32:20Z,,,
arXIv2022,Extracting Latent Steering Vectors from Pretrained Language Models,No.,1,"""No evidence""",2022,2022-05-10T19:04:37Z,,,
arXIv2022,Symphony Generation with Permutation Invariant Language Model,No.,1,"""No evidence""",2022,2022-05-10T13:08:49Z,,,
arXIv2022,Ratatouille: A tool for Novel Recipe Generation,No.,1,"""No evidence""",2022,2022-05-10T11:20:19Z,,,
arXIv2022,The Importance of Context in Very Low Resource Language Modeling,No.,1,"""No evidence""",2022,2022-05-10T11:19:56Z,,,
arXIv2022,Deep learning based Chinese text sentiment mining and stock market correlation research,No.,1,"""No evidence""",2022,2022-05-10T08:35:33Z,,,
arXIv2022,From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective,No.,1,"""No evidence""",2022,2022-05-10T08:08:43Z,,,
arXIv2022,Long Document Re-ranking with Modular Re-ranker,No.,1,"""No evidence""",2022,2022-05-09T13:44:02Z,,,
arXIv2022,LayoutXLM vs. GNN: An Empirical Evaluation of Relation Extraction for Documents,No.,1,"""No evidence""",2022,2022-05-09T13:36:09Z,,,
arXIv2022,Multi-segment preserving sampling for deep manifold sampler,No.,1,"""No evidence""",2022,2022-05-09T13:19:41Z,,,
arXIv2022,Research on the correlation between text emotion mining and stock market based on deep learning,No.,1,"""No evidence""",2022,2022-05-09T12:51:16Z,,,
arXIv2022,A Dataset and BERT-based Models for Targeted Sentiment Analysis on Turkish Texts,No.,1,"""No evidence""",2022,2022-05-09T10:57:39Z,,,
arXIv2022,A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank,No.,1,"""No evidence""",2022,2022-05-09T07:32:50Z,,,
arXIv2022,Automated Evaluation for Student Argumentative Writing: A Survey,No.,1,"""No evidence""",2022,2022-05-09T07:27:59Z,,,
arXIv2022,Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning,No.,1,"""No evidence""",2022,2022-05-08T23:37:27Z,,,
arXIv2022,Multimodal Semi-Supervised Learning for Text Recognition,No.,1,"""No evidence""",2022,2022-05-08T13:55:30Z,,,
arXIv2022,On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation,No.,1,"""No evidence""",2022,2022-05-08T10:36:54Z,,,
arXIv2022,AKI-BERT: a Pre-trained Clinical Language Model for Early Prediction of Acute Kidney Injury,No.,1,"""No evidence""",2022,2022-05-07T18:04:31Z,,,
arXIv2022,Improving Downstream Task Performance by Treating Numbers as Entities,No.,1,"""No evidence""",2022,2022-05-07T05:22:43Z,,,
arXIv2022,A Data Cartography based MixUp for Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-05-06T17:59:19Z,,,
arXIv2022,Prompt Distribution Learning,No.,1,"""No evidence""",2022,2022-05-06T16:22:36Z,,,
arXIv2022,RaFoLa: A Rationale-Annotated Corpus for Detecting Indicators of Forced Labour,No.,1,"""No evidence""",2022,2022-05-05T14:43:31Z,,,
arXIv2022,Language Models Can See: Plugging Visual Controls in Text Generation,No.,1,"""No evidence""",2022,2022-05-05T13:56:18Z,,,
arXIv2022,Exploiting Global and Local Hierarchies for Hierarchical Text Classification,No.,1,"""No evidence""",2022,2022-05-05T12:48:41Z,,,
arXIv2022,FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework,No.,1,"""No evidence""",2022,2022-05-05T07:59:51Z,,,
arXIv2022,Language Models in the Loop: Incorporating Prompting into Weak Supervision,No.,1,"""No evidence""",2022,2022-05-04T20:42:40Z,,,
arXIv2022,Using virtual edges to extract keywords from texts modeled as complex networks,No.,1,"""No evidence""",2022,2022-05-04T16:43:03Z,,,
arXIv2022,Explain and Conquer: Personalised Text-based Reviews to Achieve Transparency,No.,1,"""No evidence""",2022,2022-05-03T20:04:32Z,,,
arXIv2022,Mixed-effects transformers for hierarchical adaptation,No.,1,"""No evidence""",2022,2022-05-03T19:34:15Z,,,
arXIv2022,Improving In-Context Few-Shot Learning via Self-Supervised Training,No.,1,"""No evidence""",2022,2022-05-03T18:01:07Z,,,
arXIv2022,Finding patterns in Knowledge Attribution for Transformers,No.,1,"""No evidence""",2022,2022-05-03T08:30:51Z,,,
arXIv2022,Predicting Issue Types with seBERT,No.,1,"""No evidence""",2022,2022-05-03T06:47:13Z,,,
arXIv2022,Contrastive Learning for Prompt-Based Few-Shot Language Learners,No.,1,"""No evidence""",2022,2022-05-03T04:56:45Z,,,
arXIv2022,Embedding Hallucination for Few-Shot Language Fine-tuning,No.,1,"""No evidence""",2022,2022-05-03T04:55:50Z,,,
arXIv2022,OPT: Open Pre-trained Transformer Language Models,No.,1,"""No evidence""",2022,2022-05-02T17:49:50Z,,,
arXIv2022,BERTops: Studying BERT Representations under a Topological Lens,No.,1,"""No evidence""",2022,2022-05-02T14:56:17Z,,,
arXIv2022,CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning,No.,1,"""No evidence""",2022,2022-05-02T14:42:05Z,,,
arXIv2022,Entity-aware Transformers for Entity Search,No.,1,"""No evidence""",2022,2022-05-02T11:53:59Z,,,
arXIv2022,Seeding Diversity into AI Art,No.,1,"""No evidence""",2022,2022-05-02T10:40:52Z,,,
arXIv2022,Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection,No.,1,"""No evidence""",2022,2022-05-02T02:13:24Z,,,
arXIv2022,Large-Scale Multi-Document Summarization with Information Extraction and Compression,No.,1,"""No evidence""",2022,2022-05-01T19:49:15Z,,,
arXIv2022,ELQA: A Corpus of Metalinguistic Questions and Answers about English,No.,1,"""No evidence""",2022,2022-05-01T04:29:50Z,,,
arXIv2022,Detecting COVID-19 Conspiracy Theories with Transformers and TF-IDF,No.,1,"""No evidence""",2022,2022-05-01T01:48:48Z,,,
arXIv2022,LayoutBERT: Masked Language Layout Model for Object Insertion,No.,1,"""No evidence""",2022,2022-04-30T21:35:38Z,,,
arXIv2022,Engineering flexible machine learning systems by traversing functionally-invariant paths,No.,1,"""No evidence""",2022,2022-04-30T19:44:56Z,,,
arXIv2022,HateCheckHIn: Evaluating Hindi Hate Speech Detection Models,No.,1,"""No evidence""",2022,2022-04-30T19:09:09Z,,,
arXIv2022,Visualizing and Explaining Language Models,No.,1,"""No evidence""",2022,2022-04-30T17:23:33Z,,,
arXIv2022,StorSeismic: A new paradigm in deep learning for seismic processing,No.,1,"""No evidence""",2022,2022-04-30T09:55:00Z,,,
arXIv2022,Self-Programming Artificial Intelligence Using Code-Generating Language Models,No.,1,"""No evidence""",2022,2022-04-30T05:44:34Z,,,
arXIv2022,To Know by the Company Words Keep and What Else Lies in the Vicinity,No.,1,"""No evidence""",2022,2022-04-30T03:47:48Z,,,
arXIv2022,Training Naturalized Semantic Parsers with Very Little Data,No.,1,"""No evidence""",2022,2022-04-29T17:14:54Z,,,
arXIv2022,Flamingo: a Visual Language Model for Few-Shot Learning,No.,1,"""No evidence""",2022,2022-04-29T16:29:01Z,,,
arXIv2022,PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining,No.,1,"""No evidence""",2022,2022-04-29T13:38:42Z,,,
arXIv2022,C3-STISR: Scene Text Image Super-resolution with Triple Clues,No.,1,"""No evidence""",2022,2022-04-29T12:39:51Z,,,
arXIv2022,ExaASC: A General Target-Based Stance Detection Corpus in Arabic Language,No.,1,"""No evidence""",2022,2022-04-29T10:03:51Z,,,
arXIv2022,QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance,No.,1,"""No evidence""",2022,2022-04-29T07:39:53Z,,,
arXIv2022,Czech Dataset for Cross-lingual Subjectivity Classification,No.,1,"""No evidence""",2022,2022-04-29T07:31:46Z,,,
arXIv2022,OA-Mine: Open-World Attribute Mining for E-Commerce Products with Weak Supervision,No.,1,"""No evidence""",2022,2022-04-29T04:16:04Z,,,
arXIv2022,Process-BERT: A Framework for Representation Learning on Educational Process Data,No.,1,"""No evidence""",2022,2022-04-28T16:07:28Z,,,
arXIv2022,CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers,No.,1,"""No evidence""",2022,2022-04-28T15:51:11Z,,,
arXIv2022,RobBERTje: a Distilled Dutch BERT Model,No.,1,"""No evidence""",2022,2022-04-28T14:02:13Z,,,
arXIv2022,Post-Training Dialogue Summarization using Pseudo-Paraphrasing,No.,1,"""No evidence""",2022,2022-04-28T13:42:19Z,,,
arXIv2022,A Survey on Sentence Embedding Models Performance for Patent Analysis,No.,1,"""No evidence""",2022,2022-04-28T12:04:42Z,,,
arXIv2022,HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification,No.,1,"""No evidence""",2022,2022-04-28T11:22:49Z,,,
arXIv2022,Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation,No.,1,"""No evidence""",2022,2022-04-28T09:09:45Z,,,
arXIv2022,BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information,No.,1,"""No evidence""",2022,2022-04-27T16:20:09Z,,,
arXIv2022,DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation,No.,1,"""No evidence""",2022,2022-04-27T16:18:15Z,,,
arXIv2022,RigoBERTa: A State-of-the-Art Language Model For Spanish,No.,1,"""No evidence""",2022,2022-04-27T11:53:25Z,,,
arXIv2022,SkillSpan: Hard and Soft Skill Extraction from English Job Postings,No.,1,"""No evidence""",2022,2022-04-27T10:07:36Z,,,
arXIv2022,Probing Simile Knowledge from Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-04-27T09:55:40Z,,,
arXIv2022,Modern Baselines for SPARQL Semantic Parsing,No.,1,"""No evidence""",2022,2022-04-27T09:26:59Z,,,
arXIv2022,Multimodal Transformer-based Model for Buchwald-Hartwig and Suzuki-Miyaura Reaction Yield Prediction,No.,1,"""No evidence""",2022,2022-04-27T07:28:27Z,,,
arXIv2022,UBERT: A Novel Language Model for Synonymy Prediction at Scale in the UMLS Metathesaurus,No.,1,"""No evidence""",2022,2022-04-27T06:03:24Z,,,
arXIv2022,Better Query Graph Selection for Knowledge Base Question Answering,No.,1,"""No evidence""",2022,2022-04-27T01:53:06Z,,,
arXIv2022,Parkinson's disease diagnostics using AI and natural language knowledge transfer,No.,1,"""No evidence""",2022,2022-04-26T19:39:29Z,,,
arXIv2022,Science Checker: Extractive-Boolean Question Answering For Scientific Fact Checking,No.,1,"""No evidence""",2022,2022-04-26T12:35:23Z,,,
arXIv2022,GypSum: Learning Hybrid Representations for Code Summarization,No.,1,"""No evidence""",2022,2022-04-26T07:44:49Z,,,
arXIv2022,Approach to Predicting News -- A Precise Multi-LSTM Network With BERT,No.,1,"""No evidence""",2022,2022-04-26T06:14:01Z,,,
arXIv2022,C3: Continued Pretraining with Contrastive Weak Supervision for Cross Language Ad-Hoc Retrieval,No.,1,"""No evidence""",2022,2022-04-25T23:12:05Z,,,
arXIv2022,Crystal Transformer: Self-learning neural language model for Generative and Tinkering Design of Materials,No.,1,"""No evidence""",2022,2022-04-25T20:20:26Z,,,
arXIv2022,Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks,No.,1,"""No evidence""",2022,2022-04-25T18:56:55Z,,,
arXIv2022,The Causal News Corpus: Annotating Causal Relations in Event Sentences from News,No.,1,"""No evidence""",2022,2022-04-25T15:14:07Z,,,
arXIv2022,Which Discriminator for Cooperative Text Generation?,No.,1,"""No evidence""",2022,2022-04-25T12:16:02Z,,,
arXIv2022,Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?,No.,1,"""No evidence""",2022,2022-04-25T08:23:13Z,,,
arXIv2022,Twitter-Based Gender Recognition Using Transformers,No.,1,"""No evidence""",2022,2022-04-24T19:58:42Z,,,
arXIv2022,Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training,No.,1,"""No evidence""",2022,2022-04-24T08:42:47Z,,,
arXIv2022,Locally Aggregated Feature Attribution on Natural Language Model Understanding,No.,1,"""No evidence""",2022,2022-04-22T18:59:27Z,,,
arXIv2022,Exploiting Session Information in BERT-based Session-aware Sequential Recommendation,No.,1,"""No evidence""",2022,2022-04-22T17:58:10Z,,,
arXIv2022,Reward Reports for Reinforcement Learning,No.,1,"""No evidence""",2022,2022-04-22T16:53:39Z,,,
arXIv2022,Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues,No.,1,"""No evidence""",2022,2022-04-22T08:15:15Z,,,
arXIv2022,Taygete at SemEval-2022 Task 4: RoBERTa based models for detecting Patronising and Condescending Language,No.,1,"""No evidence""",2022,2022-04-22T06:11:47Z,,,
arXIv2022,WaBERT: A Low-resource End-to-end Model for Spoken Language Understanding and Speech-to-BERT Alignment,No.,1,"""No evidence""",2022,2022-04-22T02:14:40Z,,,
arXIv2022,Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires,No.,1,"""No evidence""",2022,2022-04-21T22:57:11Z,,,
arXIv2022,DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings,No.,1,"""No evidence""",2022,2022-04-21T17:32:01Z,,,
arXIv2022,Revisiting Gaussian mixture critics in off-policy reinforcement learning: a sample-based approach,No.,1,"""No evidence""",2022,2022-04-21T16:44:47Z,,,
arXIv2022,Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics,No.,1,"""No evidence""",2022,2022-04-21T04:26:51Z,,,
arXIv2022,When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes,No.,1,"""No evidence""",2022,2022-04-20T18:09:36Z,,,
arXIv2022,Towards Arabic Sentence Simplification via Classification and Generative Approaches,No.,1,"""No evidence""",2022,2022-04-20T08:17:33Z,,,
arXIv2022,On the Representation Collapse of Sparse Mixture of Experts,No.,1,"""No evidence""",2022,2022-04-20T01:40:19Z,,,
arXIv2022,DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation,No.,1,"""No evidence""",2022,2022-04-19T22:26:18Z,,,
arXIv2022,ALBETO and DistilBETO: Lightweight Spanish Language Models,No.,1,"""No evidence""",2022,2022-04-19T22:07:34Z,,,
arXIv2022,Optimize_Prime@DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil,No.,1,"""No evidence""",2022,2022-04-19T18:55:18Z,,,
arXIv2022,CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex,No.,1,"""No evidence""",2022,2022-04-19T15:19:35Z,,,
arXIv2022,Probing for the Usage of Grammatical Number,No.,1,"""No evidence""",2022,2022-04-19T11:59:52Z,,,
arXIv2022,Multimodal Hate Speech Detection from Bengali Memes and Texts,No.,1,"""No evidence""",2022,2022-04-19T11:15:25Z,,,
arXIv2022,Mono vs Multilingual BERT for Hate Speech Detection and Text Classification: A Case Study in Marathi,No.,1,"""No evidence""",2022,2022-04-19T05:07:58Z,,,
arXIv2022,LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation,No.,1,"""No evidence""",2022,2022-04-19T04:03:45Z,,,
arXIv2022,Zero-shot Entity and Tweet Characterization with Designed Conditional Prompts and Contexts,No.,1,"""No evidence""",2022,2022-04-18T17:01:49Z,,,
arXIv2022,UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language,No.,1,"""No evidence""",2022,2022-04-18T13:22:10Z,,,
arXIv2022,Ingredient Extraction from Text in the Recipe Domain,No.,1,"""No evidence""",2022,2022-04-18T02:54:54Z,,,
arXIv2022,Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base,No.,1,"""No evidence""",2022,2022-04-17T12:33:34Z,,,
arXIv2022,nigam@COLIEE-22: Legal Case Retrieval and Entailment using Cascading of Lexical and Semantic-based models,No.,1,"""No evidence""",2022,2022-04-16T18:10:02Z,,,
arXIv2022,A Contrastive Cross-Channel Data Augmentation Framework for Aspect-based Sentiment Analysis,No.,1,"""No evidence""",2022,2022-04-16T16:05:58Z,,,
arXIv2022,Contrastive Learning with Hard Negative Entities for Entity Set Expansion,No.,1,"""No evidence""",2022,2022-04-16T12:26:42Z,,,
arXIv2022,WordAlchemy: A transformer-based Reverse Dictionary,No.,1,"""No evidence""",2022,2022-04-16T11:41:48Z,,,
arXIv2022,SimpleBERT: A Pre-trained Model That Learns to Generate Simple Words,No.,1,"""No evidence""",2022,2022-04-16T11:28:01Z,,,
arXIv2022,Probing Script Knowledge from Pre-Trained Models,No.,1,"""No evidence""",2022,2022-04-16T05:13:39Z,,,
arXIv2022,BLCU-ICALL at SemEval-2022 Task 1: Cross-Attention Multitasking Framework for Definition Modeling,No.,1,"""No evidence""",2022,2022-04-16T02:33:28Z,,,
arXIv2022,DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling,No.,1,"""No evidence""",2022,2022-04-15T23:39:41Z,,,
arXIv2022,MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation,No.,1,"""No evidence""",2022,2022-04-15T23:19:37Z,,,
arXIv2022,CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge Distillation,No.,1,"""No evidence""",2022,2022-04-15T23:16:37Z,,,
arXIv2022,Semantic Structure based Query Graph Prediction for Question Answering over Knowledge Graph,No.,1,"""No evidence""",2022,2022-04-15T20:35:00Z,,,
arXIv2022,Evaluation Benchmarks for Spanish Sentence Representations,No.,1,"""No evidence""",2022,2022-04-15T17:53:05Z,,,
arXIv2022,Improving Passage Retrieval with Zero-Shot Question Generation,No.,1,"""No evidence""",2022,2022-04-15T14:51:41Z,,,
arXIv2022,Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models,No.,1,"""No evidence""",2022,2022-04-15T14:33:58Z,,,
arXIv2022,Improving Pre-trained Language Models with Syntactic Dependency Prediction Task for Chinese Semantic Error Recognition,No.,1,"""No evidence""",2022,2022-04-15T13:55:32Z,,,
arXIv2022,mGPT: Few-Shot Learners Go Multilingual,No.,1,"""No evidence""",2022,2022-04-15T13:02:33Z,,,
arXIv2022,ML_LTU at SemEval-2022 Task 4: T5 Towards Identifying Patronizing and Condescending Language,No.,1,"""No evidence""",2022,2022-04-15T12:00:25Z,,,
arXIv2022,Is Surprisal in Issue Trackers Actionable?,No.,1,"""No evidence""",2022,2022-04-15T07:49:40Z,,,
arXIv2022,XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding,No.,1,"""No evidence""",2022,2022-04-15T03:44:00Z,,,
arXIv2022,Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning,No.,1,"""No evidence""",2022,2022-04-15T02:36:52Z,,,
arXIv2022,Identifying and Measuring Token-Level Sentiment Bias in Pre-trained Language Models with Prompts,No.,1,"""No evidence""",2022,2022-04-15T02:01:31Z,,,
arXIv2022,Analysing similarities between legal court documents using natural language processing approaches based on Transformers,No.,1,"""No evidence""",2022,2022-04-14T18:25:56Z,,,
arXIv2022,Label Semantic Aware Pre-training for Few-shot Text Classification,No.,1,"""No evidence""",2022,2022-04-14T17:33:34Z,,,
arXIv2022,DeiT III: Revenge of the ViT,No.,1,"""No evidence""",2022,2022-04-14T17:13:44Z,,,
arXIv2022,Generative power of a protein language model trained on multiple sequence alignments,No.,1,"""No evidence""",2022,2022-04-14T16:59:05Z,,,
arXIv2022,Composite Code Sparse Autoencoders for first stage retrieval,No.,1,"""No evidence""",2022,2022-04-14T15:20:46Z,,,
arXIv2022,Rows from Many Sources: Enriching row completions from Wikidata with a pre-trained Language Model,No.,1,"""No evidence""",2022,2022-04-14T15:11:52Z,,,
arXIv2022,Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task,No.,1,"""No evidence""",2022,2022-04-14T11:33:15Z,,,
arXIv2022,Multi-label topic classification for COVID-19 literature with Bioformer,No.,1,"""No evidence""",2022,2022-04-14T05:24:54Z,,,
arXIv2022,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,No.,1,"""No evidence""",2022,2022-04-14T04:00:27Z,,,
arXIv2022,GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation,No.,1,"""No evidence""",2022,2022-04-13T23:53:37Z,,,
arXIv2022,Scalable Training of Language Models using JAX pjit and TPUv4,No.,1,"""No evidence""",2022,2022-04-13T17:08:58Z,,,
arXIv2022,Local Feature Swapping for Generalization in Reinforcement Learning,No.,1,"""No evidence""",2022,2022-04-13T13:12:51Z,,,
arXIv2022,HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition,No.,1,"""No evidence""",2022,2022-04-13T12:11:44Z,,,
arXIv2022,Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification,No.,1,"""No evidence""",2022,2022-04-13T11:15:52Z,,,
arXIv2022,TangoBERT: Reducing Inference Cost by using Cascaded Architecture,No.,1,"""No evidence""",2022,2022-04-13T09:45:08Z,,,
arXIv2022,IIITDWD-ShankarB@ Dravidian-CodeMixi-HASOC2021: mBERT based model for identification of offensive content in south Indian languages,No.,1,"""No evidence""",2022,2022-04-13T06:24:57Z,,,
arXIv2022,HIT at SemEval-2022 Task 2: Pre-trained Language Model for Idioms Detection,No.,1,"""No evidence""",2022,2022-04-13T02:45:04Z,,,
arXIv2022,L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT models,No.,1,"""No evidence""",2022,2022-04-12T18:32:15Z,,,
arXIv2022,Mining Logical Event Schemas From Pre-Trained Language Models,No.,1,"""No evidence""",2022,2022-04-12T16:41:18Z,,,
arXIv2022,What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?,No.,1,"""No evidence""",2022,2022-04-12T14:19:49Z,,,
arXIv2022,Solving Price Per Unit Problem Around the World: Formulating Fact Extraction as Question Answering,No.,1,"""No evidence""",2022,2022-04-12T06:43:48Z,,,
arXIv2022,Overlapping Word Removal is All You Need: Revisiting Data Imbalance in Hope Speech Detection,No.,1,"""No evidence""",2022,2022-04-12T02:38:54Z,,,
arXIv2022,ProtoTEx: Explaining Model Decisions with Prototype Tensors,No.,1,"""No evidence""",2022,2022-04-11T22:08:45Z,,,
arXIv2022,Doctor XAvIer: Explainable Diagnosis on Physician-Patient Dialogues and XAI Evaluation,No.,1,"""No evidence""",2022,2022-04-11T18:38:22Z,,,
arXIv2022,A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis,No.,1,"""No evidence""",2022,2022-04-11T18:31:53Z,,,
arXIv2022,GDC- Generalized Distribution Calibration for Few-Shot Learning,No.,1,"""No evidence""",2022,2022-04-11T16:22:53Z,,,
arXIv2022,Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems,No.,1,"""No evidence""",2022,2022-04-11T15:24:25Z,,,
arXIv2022,Team FAL at CMCL 2022 Shared Task: Figuring out the correct recipe for predicting Eye-Tracking features using Pretrained Language Models,No.,1,"""No evidence""",2022,2022-04-11T10:43:34Z,,,
arXIv2022,JORLDY: a fully customizable open source framework for reinforcement learning,No.,1,"""No evidence""",2022,2022-04-11T06:28:27Z,,,
arXIv2022,Adapting BigScience Multilingual Model to Unseen Languages,No.,1,"""No evidence""",2022,2022-04-11T05:32:14Z,,,
arXIv2022,Fake news detection using parallel BERT deep neural networks,No.,1,"""No evidence""",2022,2022-04-10T23:16:00Z,,,
arXIv2022,Breaking Character: Are Subwords Good Enough for MRLs After All?,No.,1,"""No evidence""",2022,2022-04-10T18:54:43Z,,,
arXIv2022,Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features,No.,1,"""No evidence""",2022,2022-04-10T08:08:46Z,,,
arXIv2022,Efficient Extraction of Pathologies from C-Spine Radiology Reports using Multi-Task Learning,No.,1,"""No evidence""",2022,2022-04-09T20:29:48Z,,,
arXIv2022,MR-iNet Gym: Framework for Edge Deployment of Deep Reinforcement Learning on Embedded Software Defined Radio,No.,1,"""No evidence""",2022,2022-04-09T16:28:43Z,,,
arXIv2022,IDPG: An Instance-Dependent Prompt Generation Method,No.,1,"""No evidence""",2022,2022-04-09T15:45:27Z,,,
arXIv2022,"FoundationLayerNorm: Scaling BERT and GPT to 1,000 Layers",No.,1,"""No evidence""",2022,2022-04-09T14:03:28Z,,,
arXIv2022,Modeling Multi-Granularity Hierarchical Features for Relation Extraction,No.,1,"""No evidence""",2022,2022-04-09T09:44:05Z,,,
arXIv2022,Should we tweet this? Generative response modeling for predicting reception of public health messaging on Twitter,No.,1,"""No evidence""",2022,2022-04-09T01:56:46Z,,,
arXIv2022,"Show, Don't Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",No.,1,"""No evidence""",2022,2022-04-08T23:27:18Z,,,
arXIv2022,MMTAfrica: Multilingual Machine Translation for African Languages,No.,1,"""No evidence""",2022,2022-04-08T21:42:44Z,,,
arXIv2022,Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models,No.,1,"""No evidence""",2022,2022-04-08T20:42:08Z,,,
arXIv2022,BioRED: A Rich Biomedical Relation Extraction Dataset,No.,1,"""No evidence""",2022,2022-04-08T19:23:49Z,,,
arXIv2022,Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation,No.,1,"""No evidence""",2022,2022-04-08T09:32:18Z,,,
arXIv2022,RuBioRoBERTa: a pre-trained biomedical language model for Russian language biomedical text mining,No.,1,"""No evidence""",2022,2022-04-08T09:18:59Z,,,
arXIv2022,Towards Semi-Supervised Learning of Automatic Post-Editing: Data-Synthesis by Infilling Mask with Erroneous Tokens,No.,1,"""No evidence""",2022,2022-04-08T07:48:57Z,,,
arXIv2022,Infusing Knowledge from Wikipedia to Enhance Stance Detection,No.,1,"""No evidence""",2022,2022-04-08T04:49:55Z,,,
arXIv2022,Q-learning with online random forests,No.,1,"""No evidence""",2022,2022-04-07T23:00:39Z,,,
arXIv2022,MAESTRO: Matched Speech Text Representations through Modality Matching,No.,1,"""No evidence""",2022,2022-04-07T12:48:16Z,,,
arXIv2022,Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation,No.,1,"""No evidence""",2022,2022-04-07T09:43:51Z,,,
arXIv2022,Towards Automatic Construction of Filipino WordNet: Word Sense Induction and Synset Induction Using Sentence Embeddings,No.,1,"""No evidence""",2022,2022-04-07T06:50:37Z,,,
arXIv2022,Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators,No.,1,"""No evidence""",2022,2022-04-07T06:19:06Z,,,
arXIv2022,Accelerating Attention through Gradient-Based Learned Runtime Pruning,No.,1,"""No evidence""",2022,2022-04-07T05:31:13Z,,,
arXIv2022,Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding,No.,1,"""No evidence""",2022,2022-04-06T16:22:02Z,,,
arXIv2022,drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM,No.,1,"""No evidence""",2022,2022-04-06T13:32:37Z,,,
arXIv2022,SecureBERT: A Domain-Specific Language Model for Cybersecurity,No.,1,"""No evidence""",2022,2022-04-06T09:17:21Z,,,
arXIv2022,Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis of BERT Classifiers and Weak Supervision,No.,1,"""No evidence""",2022,2022-04-06T07:45:05Z,,,
arXIv2022,Structure-aware Protein Self-supervised Learning,No.,1,"""No evidence""",2022,2022-04-06T02:18:41Z,,,
arXIv2022,An Exploratory Study on Code Attention in BERT,No.,1,"""No evidence""",2022,2022-04-05T21:23:10Z,,,
arXIv2022,On the Effectiveness of Pretrained Models for API Learning,No.,1,"""No evidence""",2022,2022-04-05T20:33:24Z,,,
arXIv2022,PaLM: Scaling Language Modeling with Pathways,No.,1,"""No evidence""",2022,2022-04-05T16:11:45Z,,,
arXIv2022,Abstractive summarization of hospitalisation histories with transformer networks,No.,1,"""No evidence""",2022,2022-04-05T13:38:39Z,,,
arXIv2022,Multilinguals at SemEval-2022 Task 11: Transformer Based Architecture for Complex NER,No.,1,"""No evidence""",2022,2022-04-05T12:58:57Z,,,
arXIv2022,SemanticCAP: Chromatin Accessibility Prediction Enhanced by Features Learning from a Language Model,No.,1,"""No evidence""",2022,2022-04-05T11:47:58Z,,,
arXIv2022,How Different are Pre-trained Transformers for Text Ranking?,No.,1,"""No evidence""",2022,2022-04-05T10:48:52Z,,,
arXIv2022,A Complementary Joint Training Approach Using Unpaired Speech and Text for Low-Resource Automatic Speech Recognition,No.,1,"""No evidence""",2022,2022-04-05T07:02:53Z,,,
arXIv2022,A Machine With Human-Like Memory Systems,No.,1,"""No evidence""",2022,2022-04-04T16:05:53Z,,,
arXIv2022,Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study,No.,1,"""No evidence""",2022,2022-04-04T12:44:47Z,,,
arXIv2022,Aligned Weight Regularizers for Pruning Pretrained Neural Networks,No.,1,"""No evidence""",2022,2022-04-04T11:06:42Z,,,
arXIv2022,Interpretable Saliency Maps And Self-Supervised Learning For Generalized Zero Shot Medical Image Classification,No.,1,"""No evidence""",2022,2022-04-04T09:30:08Z,,,
arXIv2022,Into-TTS : Intonation Template Based Prosody Control System,No.,1,"""No evidence""",2022,2022-04-04T06:37:19Z,,,
arXIv2022,Graph Enhanced BERT for Query Understanding,No.,1,"""No evidence""",2022,2022-04-03T16:50:30Z,,,
arXIv2022,POS-BERT: Point Cloud One-Stage BERT Pre-Training,No.,1,"""No evidence""",2022,2022-04-03T04:49:39Z,,,
arXIv2022,BERT-Assisted Semantic Annotation Correction for Emotion-Related Questions,No.,1,"""No evidence""",2022,2022-04-02T18:00:49Z,,,
arXIv2022,Efficient comparison of sentence embeddings,No.,1,"""No evidence""",2022,2022-04-02T09:08:34Z,,,
arXIv2022,Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,No.,1,"""No evidence""",2022,2022-04-01T17:43:13Z,,,
arXIv2022,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,No.,1,"""No evidence""",2022,2022-04-01T17:37:29Z,,,
arXIv2022,Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition,No.,1,"""No evidence""",2022,2022-04-01T14:05:02Z,,,
arXIv2022,Cyberbullying detection across social media platforms via platform-aware adversarial encoding,No.,1,"""No evidence""",2022,2022-04-01T10:25:46Z,,,
arXIv2022,Feature Structure Distillation with Centered Kernel Alignment in BERT Transferring,No.,1,"""No evidence""",2022,2022-04-01T10:10:27Z,,,
arXIv2022,Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization,No.,1,"""No evidence""",2022,2022-04-01T08:45:39Z,,,
arXIv2022,Syntax-informed Question Answering with Heterogeneous Graph Transformer,No.,1,"""No evidence""",2022,2022-04-01T07:48:03Z,,,
arXIv2022,NC-DRE: Leveraging Non-entity Clue Information for Document-level Relation Extraction,No.,1,"""No evidence""",2022,2022-04-01T07:30:26Z,,,
arXIv2022,Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems,No.,1,"""No evidence""",2022,2022-04-01T05:20:55Z,,,
arXIv2022,A Baseline Readability Model for Cebuano,No.,1,"""No evidence""",2022,2022-03-31T17:49:11Z,,,
arXIv2022,Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech,No.,1,"""No evidence""",2022,2022-03-31T17:12:26Z,,,
arXIv2022,Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$,No.,1,"""No evidence""",2022,2022-03-31T17:12:13Z,,,
arXIv2022,Scaling Language Model Size in Cross-Device Federated Learning,No.,1,"""No evidence""",2022,2022-03-31T15:51:53Z,,,
arXIv2022,PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model,No.,1,"""No evidence""",2022,2022-03-31T15:09:12Z,,,
arXIv2022,PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations,No.,1,"""No evidence""",2022,2022-03-31T11:34:58Z,,,
arXIv2022,Generative Pre-Trained Transformers for Biologically Inspired Design,No.,1,"""No evidence""",2022,2022-03-31T11:13:22Z,,,
arXIv2022,A Character-level Span-based Model for Mandarin Prosodic Structure Prediction,No.,1,"""No evidence""",2022,2022-03-31T09:47:08Z,,,
arXIv2022,"ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices",No.,1,"""No evidence""",2022,2022-03-31T04:22:44Z,,,
arXIv2022,An Empirical Study of Language Model Integration for Transducer based Speech Recognition,No.,1,"""No evidence""",2022,2022-03-31T03:33:50Z,,,
arXIv2022,SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,No.,1,"""No evidence""",2022,2022-03-31T03:26:55Z,,,
arXIv2022,CREATE: A Benchmark for Chinese Short Video Retrieval and Title Generation,No.,1,"""No evidence""",2022,2022-03-31T02:39:18Z,,,
arXIv2022,Transformer Language Models without Positional Encodings Still Learn Positional Information,No.,1,"""No evidence""",2022,2022-03-30T19:37:07Z,,,
arXIv2022,Improving Speech Recognition for Indic Languages using Language Model,No.,1,"""No evidence""",2022,2022-03-30T18:22:12Z,,,
arXIv2022,PromptDet: Towards Open-vocabulary Detection using Uncurated Images,No.,1,"""No evidence""",2022,2022-03-30T17:50:21Z,,,
arXIv2022,Position-based Prompting for Health Outcome Generation,No.,1,"""No evidence""",2022,2022-03-30T16:44:04Z,,,
arXIv2022,Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis,No.,1,"""No evidence""",2022,2022-03-30T14:48:46Z,,,
arXIv2022,Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval,No.,1,"""No evidence""",2022,2022-03-30T10:13:57Z,,,
arXIv2022,Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling,No.,1,"""No evidence""",2022,2022-03-30T09:46:51Z,,,
arXIv2022,An Iterative Co-Training Transductive Framework for Zero Shot Learning,No.,1,"""No evidence""",2022,2022-03-30T04:08:44Z,,,
arXIv2022,Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization,No.,1,"""No evidence""",2022,2022-03-29T21:34:35Z,,,
arXIv2022,WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models,No.,1,"""No evidence""",2022,2022-03-29T19:08:55Z,,,
arXIv2022,LinkBERT: Pretraining Language Models with Document Links,No.,1,"""No evidence""",2022,2022-03-29T18:01:24Z,,,
arXIv2022,Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting,No.,1,"""No evidence""",2022,2022-03-29T17:04:17Z,,,
arXIv2022,LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,No.,1,"""No evidence""",2022,2022-03-29T14:20:55Z,,,
arXIv2022,Cross-Media Scientific Research Achievements Retrieval Based on Deep Language Model,No.,1,"""No evidence""",2022,2022-03-29T14:04:53Z,,,
arXIv2022,WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit,No.,1,"""No evidence""",2022,2022-03-29T11:54:34Z,,,
arXIv2022,mc-BEiT: Multi-choice Discretization for Image BERT Pre-training,No.,1,"""No evidence""",2022,2022-03-29T09:08:18Z,,,
arXIv2022,Improving Persian Relation Extraction Models by Data Augmentation,No.,1,"""No evidence""",2022,2022-03-29T08:08:47Z,,,
arXIv2022,A Fast Post-Training Pruning Framework for Transformers,No.,1,"""No evidence""",2022,2022-03-29T07:41:11Z,,,
arXIv2022,Worldwide city transport typology prediction with sentence-BERT based supervised learning via Wikipedia,No.,1,"""No evidence""",2022,2022-03-29T00:09:55Z,,,
arXIv2022,Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model,No.,1,"""No evidence""",2022,2022-03-28T17:50:26Z,,,
arXIv2022,Hierarchical Transformer Model for Scientific Named Entity Recognition,No.,1,"""No evidence""",2022,2022-03-28T12:59:06Z,,,
arXIv2022,ANNA: Enhanced Language Representation for Question Answering,No.,1,"""No evidence""",2022,2022-03-28T05:26:52Z,,,
arXIv2022,STaR: Bootstrapping Reasoning With Reasoning,No.,1,"""No evidence""",2022,2022-03-28T03:12:15Z,,,
arXIv2022,Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection,No.,1,"""No evidence""",2022,2022-03-27T19:52:01Z,,,
arXIv2022,Example-based Hypernetworks for Out-of-Distribution Generalization,No.,1,"""No evidence""",2022,2022-03-27T11:10:10Z,,,
arXIv2022,Text Adversarial Purification as Defense against Adversarial Attacks,No.,1,"""No evidence""",2022,2022-03-27T04:41:55Z,,,
arXIv2022,A Roadmap for Big Model,No.,1,"""No evidence""",2022,2022-03-26T15:38:00Z,,,
arXIv2022,Autoregressive Linguistic Steganography Based on BERT and Consistency Coding,No.,1,"""No evidence""",2022,2022-03-26T02:36:55Z,,,
arXIv2022,A Comparative Evaluation Of Transformer Models For De-Identification Of Clinical Text Data,No.,1,"""No evidence""",2022,2022-03-25T19:42:03Z,,,
arXIv2022,L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models,No.,1,"""No evidence""",2022,2022-03-25T17:00:33Z,,,
arXIv2022,MKQ-BERT: Quantized BERT with 4-bits Weights and Activations,No.,1,"""No evidence""",2022,2022-03-25T07:27:18Z,,,
arXIv2022,CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,No.,1,"""No evidence""",2022,2022-03-25T06:55:15Z,,,
arXIv2022,Predicting Clinical Intent from Free Text Electronic Health Records,No.,1,"""No evidence""",2022,2022-03-25T04:27:00Z,,,
arXIv2022,Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers,No.,1,"""No evidence""",2022,2022-03-25T01:36:56Z,,,
arXIv2022,Remember and Forget Experience Replay for Multi-Agent Reinforcement Learning,No.,1,"""No evidence""",2022,2022-03-24T19:59:43Z,,,
arXIv2022,Mix and Match: Learning-free Controllable Text Generation using Energy Language Models,No.,1,"""No evidence""",2022,2022-03-24T18:52:09Z,,,
arXIv2022,Token Dropping for Efficient BERT Pretraining,No.,1,"""No evidence""",2022,2022-03-24T17:50:46Z,,,
arXIv2022,Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion,No.,1,"""No evidence""",2022,2022-03-24T17:31:26Z,,,
arXIv2022,"Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking",No.,1,"""No evidence""",2022,2022-03-24T16:12:21Z,,,
arXIv2022,minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models,No.,1,"""No evidence""",2022,2022-03-24T15:11:06Z,,,
arXIv2022,Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction,No.,1,"""No evidence""",2022,2022-03-24T13:18:36Z,,,
arXIv2022,Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory,No.,1,"""No evidence""",2022,2022-03-24T13:06:43Z,,,
arXIv2022,mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling,No.,1,"""No evidence""",2022,2022-03-24T09:04:52Z,,,
arXIv2022,Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named Entity Recognition,No.,1,"""No evidence""",2022,2022-03-24T07:50:41Z,,,
arXIv2022,Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal,No.,1,"""No evidence""",2022,2022-03-23T17:34:35Z,,,
arXIv2022,Prompt-based System for Personality and Interpersonal Reactivity Prediction,No.,1,"""No evidence""",2022,2022-03-23T15:22:34Z,,,
arXIv2022,Input-specific Attention Subnetworks for Adversarial Detection,No.,1,"""No evidence""",2022,2022-03-23T09:46:41Z,,,
arXIv2022,Visual Prompt Tuning,No.,1,"""No evidence""",2022,2022-03-23T01:17:16Z,,,
arXIv2022,Self-supervision through Random Segments with Autoregressive Coding (RandSAC),No.,1,"""No evidence""",2022,2022-03-22T21:28:55Z,,,
arXIv2022,Transformer based ensemble for emotion detection,No.,1,"""No evidence""",2022,2022-03-22T17:11:18Z,,,
arXIv2022,Under the Hood of Transformer Networks for Trajectory Forecasting,No.,1,"""No evidence""",2022,2022-03-22T16:56:05Z,,,
arXIv2022,Open-Vocabulary DETR with Conditional Matching,No.,1,"""No evidence""",2022,2022-03-22T16:54:52Z,,,
arXIv2022,Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments,No.,1,"""No evidence""",2022,2022-03-22T14:24:56Z,,,
arXIv2022,Are You Misinformed? A Study of Covid-Related Fake News in Bengali on Facebook,No.,1,"""No evidence""",2022,2022-03-22T12:41:42Z,,,
arXIv2022,Factual Consistency of Multilingual Pretrained Language Models,No.,1,"""No evidence""",2022,2022-03-22T09:15:53Z,,,
arXIv2022,VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension,No.,1,"""No evidence""",2022,2022-03-22T00:44:41Z,,,
arXIv2022,Towards Textual Out-of-Domain Detection without In-Domain Labels,No.,1,"""No evidence""",2022,2022-03-22T00:11:46Z,,,
arXIv2022,Enhancing Speech Recognition Decoding via Layer Aggregation,No.,1,"""No evidence""",2022,2022-03-21T20:28:06Z,,,
arXIv2022,Self-Consistency Improves Chain of Thought Reasoning in Language Models,No.,1,"""No evidence""",2022,2022-03-21T17:48:52Z,,,
arXIv2022,Towards Explainable Evaluation Metrics for Natural Language Generation,No.,1,"""No evidence""",2022,2022-03-21T17:05:54Z,,,
arXIv2022,AraBART: a Pretrained Arabic Sequence-to-Sequence Model for Abstractive Summarization,No.,1,"""No evidence""",2022,2022-03-21T13:11:41Z,,,
arXIv2022,Neural Token Segmentation for High Token-Internal Complexity,No.,1,"""No evidence""",2022,2022-03-21T10:07:17Z,,,
arXIv2022,Multitask Neuroevolution for Reinforcement Learning with Long and Short Episodes,No.,1,"""No evidence""",2022,2022-03-21T10:06:16Z,,,
arXIv2022,TCM-SD: A Benchmark for Probing Syndrome Differentiation via Natural Language Processing,No.,1,"""No evidence""",2022,2022-03-21T09:59:54Z,,,
arXIv2022,Semantic Similarity Computing for Scientific Academic Conferences fused with domain features,No.,1,"""No evidence""",2022,2022-03-21T04:55:21Z,,,
arXIv2022,An Intellectual Property Entity Recognition Method Based on Transformer and Technological Word Information,No.,1,"""No evidence""",2022,2022-03-21T03:28:37Z,,,
arXIv2022,Compression of Generative Pre-trained Language Models via Quantization,No.,1,"""No evidence""",2022,2022-03-21T02:11:35Z,,,
arXIv2022,Better Language Model with Hypernym Class Prediction,No.,1,"""No evidence""",2022,2022-03-21T01:16:44Z,,,
arXIv2022,Immersive Text Game and Personality Classification,No.,1,"""No evidence""",2022,2022-03-20T18:37:03Z,,,
arXIv2022,Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation,No.,1,"""No evidence""",2022,2022-03-20T16:31:49Z,,,
arXIv2022,Cluster & Tune: Boost Cold Start Performance in Text Classification,No.,1,"""No evidence""",2022,2022-03-20T15:29:34Z,,,
arXIv2022,g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin,No.,1,"""No evidence""",2022,2022-03-20T02:28:25Z,,,
arXIv2022,How does the pre-training objective affect what large language models learn about linguistic properties?,No.,1,"""No evidence""",2022,2022-03-20T00:02:10Z,,,
arXIv2022,On Robust Prefix-Tuning for Text Classification,No.,1,"""No evidence""",2022,2022-03-19T18:52:47Z,,,
arXIv2022,Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense,No.,1,"""No evidence""",2022,2022-03-19T16:00:01Z,,,
arXIv2022,Automatic Detection of Entity-Manipulated Text using Factual Knowledge,No.,1,"""No evidence""",2022,2022-03-19T15:35:59Z,,,
arXIv2022,Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging,No.,1,"""No evidence""",2022,2022-03-19T12:33:38Z,,,
arXIv2022,Dependency-based Mixture Language Models,No.,1,"""No evidence""",2022,2022-03-19T06:28:30Z,,,
arXIv2022,Three things everyone should know about Vision Transformers,No.,1,"""No evidence""",2022,2022-03-18T08:23:03Z,,,
arXIv2022,HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information,No.,1,"""No evidence""",2022,2022-03-17T21:49:26Z,,,
arXIv2022,ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,No.,1,"""No evidence""",2022,2022-03-17T17:57:56Z,,,
arXIv2022,Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models,No.,1,"""No evidence""",2022,2022-03-17T15:46:53Z,,,
arXIv2022,Finding Structural Knowledge in Multimodal-BERT,No.,1,"""No evidence""",2022,2022-03-17T13:20:01Z,,,
arXIv2022,Universal Conditional Masked Language Pre-training for Neural Machine Translation,No.,1,"""No evidence""",2022,2022-03-17T10:00:33Z,,,
arXIv2022,Multilingual Detection of Personal Employment Status on Twitter,No.,1,"""No evidence""",2022,2022-03-17T08:55:18Z,,,
arXIv2022,RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction,No.,1,"""No evidence""",2022,2022-03-17T05:55:14Z,,,
arXIv2022,Triangular Transfer: Freezing the Pivot for Triangular Machine Translation,No.,1,"""No evidence""",2022,2022-03-17T02:00:40Z,,,
arXIv2022,AdapLeR: Speeding up Inference by Adaptive Length Reduction,No.,1,"""No evidence""",2022,2022-03-16T23:41:38Z,,,
arXIv2022,Label Semantics for Few Shot Named Entity Recognition,No.,1,"""No evidence""",2022,2022-03-16T23:21:05Z,,,
arXIv2022,CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals,No.,1,"""No evidence""",2022,2022-03-16T17:37:28Z,,,
arXIv2022,In-Context Learning for Few-Shot Dialogue State Tracking,No.,1,"""No evidence""",2022,2022-03-16T11:58:24Z,,,
arXIv2022,Linking Theories and Methods in Cognitive Sciences via Joint Embedding of the Scientific Literature: The Example of Cognitive Control,No.,1,"""No evidence""",2022,2022-03-16T11:03:09Z,,,
arXIv2022,Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding,No.,1,"""No evidence""",2022,2022-03-16T09:17:41Z,,,
arXIv2022,KinyaBERT: a Morphology-aware Kinyarwanda Language Model,No.,1,"""No evidence""",2022,2022-03-16T08:36:14Z,,,
arXIv2022,Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure,No.,1,"""No evidence""",2022,2022-03-16T07:09:35Z,,,
arXIv2022,Evaluating the Text-to-SQL Capabilities of Large Language Models,No.,1,"""No evidence""",2022,2022-03-15T17:23:53Z,,,
arXIv2022,Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns,No.,1,"""No evidence""",2022,2022-03-15T17:13:45Z,,,
arXIv2022,Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost,No.,1,"""No evidence""",2022,2022-03-15T13:11:07Z,,,
arXIv2022,Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs,No.,1,"""No evidence""",2022,2022-03-15T12:32:28Z,,,
arXIv2022,UniSAr: A Unified Structure-Aware Autoregressive Language Model for Text-to-SQL,No.,1,"""No evidence""",2022,2022-03-15T11:02:55Z,,,
arXIv2022,Evaluating BERT-based Pre-training Language Models for Detecting Misinformation,No.,1,"""No evidence""",2022,2022-03-15T08:54:36Z,,,
arXIv2022,ReACC: A Retrieval-Augmented Code Completion Framework,No.,1,"""No evidence""",2022,2022-03-15T08:25:08Z,,,
arXIv2022,Long Document Summarization with Top-down and Bottom-up Inference,No.,1,"""No evidence""",2022,2022-03-15T01:24:51Z,,,
arXIv2022,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,No.,1,"""No evidence""",2022,2022-03-14T22:07:52Z,,,
arXIv2022,Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations,No.,1,"""No evidence""",2022,2022-03-14T21:42:13Z,,,
arXIv2022,VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models,No.,1,"""No evidence""",2022,2022-03-14T21:29:38Z,,,
arXIv2022,CoNTACT: A Dutch COVID-19 Adapted BERT for Vaccine Hesitancy and Argumentation Detection,No.,1,"""No evidence""",2022,2022-03-14T17:55:32Z,,,
arXIv2022,All in One: Exploring Unified Video-Language Pre-training,No.,1,"""No evidence""",2022,2022-03-14T17:06:30Z,,,
arXIv2022,WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition,No.,1,"""No evidence""",2022,2022-03-14T08:29:58Z,,,
arXIv2022,PERT: Pre-training BERT with Permuted Language Model,No.,1,"""No evidence""",2022,2022-03-14T07:58:34Z,,,
arXIv2022,Pruned Graph Neural Network for Short Story Ordering,No.,1,"""No evidence""",2022,2022-03-13T22:25:17Z,,,
arXIv2022,Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video,No.,1,"""No evidence""",2022,2022-03-13T14:42:53Z,,,
arXIv2022,FiNER: Financial Numeric Entity Recognition for XBRL Tagging,No.,1,"""No evidence""",2022,2022-03-12T16:43:57Z,,,
arXIv2022,BiBERT: Accurate Fully Binarized BERT,No.,1,"""No evidence""",2022,2022-03-12T09:46:13Z,,,
arXIv2022,Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation,No.,1,"""No evidence""",2022,2022-03-12T09:33:37Z,,,
arXIv2022,MarkBERT: Marking Word Boundaries Improves Chinese BERT,No.,1,"""No evidence""",2022,2022-03-12T08:43:06Z,,,
arXIv2022,ELLE: Efficient Lifelong Pre-training for Emerging Data,No.,1,"""No evidence""",2022,2022-03-12T01:53:53Z,,,
arXIv2022,verBERT: Automating Brazilian Case Law Document Multi-label Categorization Using BERT,No.,1,"""No evidence""",2022,2022-03-11T20:01:20Z,,,
arXIv2022,Are discrete units necessary for Spoken Language Modeling?,No.,1,"""No evidence""",2022,2022-03-11T14:14:35Z,,,
arXIv2022,A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings,No.,1,"""No evidence""",2022,2022-03-11T12:29:22Z,,,
arXIv2022,DeepTrust: A Reliable Financial Knowledge Retrieval Framework For Explaining Extreme Pricing Anomalies,No.,1,"""No evidence""",2022,2022-03-11T06:29:22Z,,,
arXIv2022,Hierarchical BERT for Medical Document Understanding,No.,1,"""No evidence""",2022,2022-03-11T03:50:03Z,,,
arXIv2022,A new approach to calculating BERTScore for automatic assessment of translation quality,No.,1,"""No evidence""",2022,2022-03-10T19:25:16Z,,,
arXIv2022,Semantic Norm Recognition and its application to Portuguese Law,No.,1,"""No evidence""",2022,2022-03-10T15:28:05Z,,,
arXIv2022,MVP: Multimodality-guided Visual Pre-training,No.,1,"""No evidence""",2022,2022-03-10T06:11:20Z,,,
arXIv2022,Compilable Neural Code Generation with Compiler Feedback,No.,1,"""No evidence""",2022,2022-03-10T03:15:17Z,,,
arXIv2022,NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks,No.,1,"""No evidence""",2022,2022-03-09T22:57:15Z,,,
arXIv2022,HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing,No.,1,"""No evidence""",2022,2022-03-09T21:44:28Z,,,
arXIv2022,Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain,No.,1,"""No evidence""",2022,2022-03-09T14:10:55Z,,,
arXIv2022,LEMON: LanguagE ModeL for Negative Sampling of Knowledge Graph Embeddings,No.,1,"""No evidence""",2022,2022-03-09T13:27:47Z,,,
arXIv2022,Gym-saturation: an OpenAI Gym environment for saturation provers,No.,1,"""No evidence""",2022,2022-03-09T13:22:15Z,,,
arXIv2022,Learning Bidirectional Translation between Descriptions and Actions with Small Paired Data,No.,1,"""No evidence""",2022,2022-03-08T17:39:16Z,,,
arXIv2022,InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER,No.,1,"""No evidence""",2022,2022-03-08T07:56:36Z,,,
arXIv2022,HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks,No.,1,"""No evidence""",2022,2022-03-08T06:51:33Z,,,
arXIv2022,Semantic-Preserving Linguistic Steganography by Pivot Translation and Semantic-Aware Bins Coding,No.,1,"""No evidence""",2022,2022-03-08T01:35:05Z,,,
arXIv2022,IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation,No.,1,"""No evidence""",2022,2022-03-07T22:39:01Z,,,
arXIv2022,The Unsurprising Effectiveness of Pre-Trained Vision Models for Control,No.,1,"""No evidence""",2022,2022-03-07T18:26:14Z,,,
arXIv2022,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,No.,1,"""No evidence""",2022,2022-03-07T15:37:35Z,,,
arXIv2022,Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval,No.,1,"""No evidence""",2022,2022-03-07T13:20:46Z,,,
arXIv2022,Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos,No.,1,"""No evidence""",2022,2022-03-06T17:31:06Z,,,
arXIv2022,Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation,No.,1,"""No evidence""",2022,2022-03-06T12:34:10Z,,,
arXIv2022,Graph Neural Network Enhanced Language Models for Efficient Multilingual Text Classification,No.,1,"""No evidence""",2022,2022-03-06T09:05:42Z,,,
arXIv2022,Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents,No.,1,"""No evidence""",2022,2022-03-06T07:48:24Z,,,
arXIv2022,Leveraging Pre-trained BERT for Audio Captioning,No.,1,"""No evidence""",2022,2022-03-06T00:05:58Z,,,
arXIv2022,Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation,No.,1,"""No evidence""",2022,2022-03-05T14:56:14Z,,,
arXIv2022,Unfreeze with Care: Space-Efficient Fine-Tuning of Semantic Parsing Models,No.,1,"""No evidence""",2022,2022-03-05T04:30:03Z,,,
arXIv2022,Detecting Offensive Language on Social Networks: An End-to-end Detection Method based on Graph Attention Networks,No.,1,"""No evidence""",2022,2022-03-04T03:57:18Z,,,
arXIv2022,Deep Lexical Hypothesis: Identifying personality structure in natural language,No.,1,"""No evidence""",2022,2022-03-04T02:06:10Z,,,
arXIv2022,"Vision-Language Intelligence: Tasks, Representation Learning, and Large Models",No.,1,"""No evidence""",2022,2022-03-03T18:54:59Z,,,
arXIv2022,Improving Health Mentioning Classification of Tweets using Contrastive Adversarial Training,No.,1,"""No evidence""",2022,2022-03-03T18:20:51Z,,,
arXIv2022,Quantum Reinforcement Learning via Policy Iteration,No.,1,"""No evidence""",2022,2022-03-03T18:08:17Z,,,
arXIv2022,A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism,No.,1,"""No evidence""",2022,2022-03-03T09:47:59Z,,,
arXIv2022,"Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking",No.,1,"""No evidence""",2022,2022-03-03T07:54:09Z,,,
arXIv2022,Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering,No.,1,"""No evidence""",2022,2022-03-02T18:24:10Z,,,
arXIv2022,Integrating Contrastive Learning with Dynamic Models for Reinforcement Learning from Images,No.,1,"""No evidence""",2022,2022-03-02T14:39:17Z,,,
arXIv2022,Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-03-02T13:44:49Z,,,
arXIv2022,Discontinuous Constituency and BERT: A Case Study of Dutch,No.,1,"""No evidence""",2022,2022-03-02T12:30:21Z,,,
arXIv2022,HyperPrompt: Prompt-based Task-Conditioning of Transformers,No.,1,"""No evidence""",2022,2022-03-01T21:57:34Z,,,
arXIv2022,E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models,No.,1,"""No evidence""",2022,2022-03-01T21:21:27Z,,,
arXIv2022,BERT-LID: Leveraging BERT to Improve Spoken Language Identification,No.,1,"""No evidence""",2022,2022-03-01T10:01:25Z,,,
arXIv2022,"""Is Whole Word Masking Always Better for Chinese BERT?"": Probing on Chinese Grammatical Error Correction",No.,1,"""No evidence""",2022,2022-03-01T08:24:56Z,,,
arXIv2022,Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation,No.,1,"""No evidence""",2022,2022-03-01T07:54:44Z,,,
arXIv2022,Exploring and Adapting Chinese GPT to Pinyin Input Method,No.,1,"""No evidence""",2022,2022-03-01T06:05:07Z,,,
arXIv2022,The impact of lexical and grammatical processing on generating code from natural language,No.,1,"""No evidence""",2022,2022-02-28T17:23:30Z,,,
arXIv2022,Provably Efficient Convergence of Primal-Dual Actor-Critic with Nonlinear Function Approximation,No.,1,"""No evidence""",2022,2022-02-28T15:16:23Z,,,
arXIv2022,Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks,No.,1,"""No evidence""",2022,2022-02-28T14:54:08Z,,,
arXIv2022,Avalanche RL: a Continual Reinforcement Learning Library,No.,1,"""No evidence""",2022,2022-02-28T10:01:22Z,,,
arXIv2022,Cross-Lingual Text Classification with Multilingual Distillation and Zero-Shot-Aware Training,No.,1,"""No evidence""",2022,2022-02-28T09:51:32Z,,,
arXIv2022,CINO: A Chinese Minority Pre-trained Language Model,No.,1,"""No evidence""",2022,2022-02-28T06:02:06Z,,,
arXIv2022,Enhancing Legal Argument Mining with Domain Pre-training and Neural Networks,No.,1,"""No evidence""",2022,2022-02-27T21:24:53Z,,,
arXIv2022,Controllable Natural Language Generation with Contrastive Prefixes,No.,1,"""No evidence""",2022,2022-02-27T00:31:03Z,,,
arXIv2022,Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning,No.,1,"""No evidence""",2022,2022-02-26T17:28:02Z,,,
arXIv2022,Multi-Level Contrastive Learning for Cross-Lingual Alignment,No.,1,"""No evidence""",2022,2022-02-26T07:14:20Z,,,
arXIv2022,Bi-directional Joint Neural Networks for Intent Classification and Slot Filling,No.,1,"""No evidence""",2022,2022-02-26T06:35:21Z,,,
arXIv2022,Exploring Multi-Modal Representations for Ambiguity Detection & Coreference Resolution in the SIMMC 2.0 Challenge,No.,1,"""No evidence""",2022,2022-02-25T12:10:02Z,,,
arXIv2022,Probing BERT's priors with serial reproduction chains,No.,1,"""No evidence""",2022,2022-02-24T17:42:28Z,,,
arXIv2022,BERTVision -- A Parameter-Efficient Approach for Question Answering,No.,1,"""No evidence""",2022,2022-02-24T17:16:25Z,,,
arXIv2022,Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words,No.,1,"""No evidence""",2022,2022-02-24T15:15:48Z,,,
arXIv2022,Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition,No.,1,"""No evidence""",2022,2022-02-24T15:12:17Z,,,
arXIv2022,From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems,No.,1,"""No evidence""",2022,2022-02-24T14:01:50Z,,,
arXIv2022,Using calibrator to improve robustness in Machine Reading Comprehension,No.,1,"""No evidence""",2022,2022-02-24T02:16:42Z,,,
arXIv2022,Sky Computing: Accelerating Geo-distributed Computing in Federated Learning,No.,1,"""No evidence""",2022,2022-02-24T00:14:38Z,,,
arXIv2022,Consistent Dropout for Policy Gradient Reinforcement Learning,No.,1,"""No evidence""",2022,2022-02-23T23:00:40Z,,,
arXIv2022,Knowledge Augmented BERT Mutual Network in Multi-turn Spoken Dialogues,No.,1,"""No evidence""",2022,2022-02-23T04:03:35Z,,,
arXIv2022,Improving CTC-based speech recognition via knowledge transferring from pre-trained language models,No.,1,"""No evidence""",2022,2022-02-22T11:30:55Z,,,
arXIv2022,Korean Tokenization for Beam Search Rescoring in Speech Recognition,No.,1,"""No evidence""",2022,2022-02-22T11:25:01Z,,,
arXIv2022,VU-BERT: A Unified framework for Visual Dialog,No.,1,"""No evidence""",2022,2022-02-22T10:20:14Z,,,
arXIv2022,Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users,No.,1,"""No evidence""",2022,2022-02-21T18:24:59Z,,,
arXIv2022,BERT WEAVER: Using WEight AVERaging to enable lifelong learning for transformer-based models in biomedical semantic search engines,No.,1,"""No evidence""",2022,2022-02-21T10:34:41Z,,,
arXIv2022,Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning,No.,1,"""No evidence""",2022,2022-02-21T04:22:07Z,,,
arXIv2022,StyleBERT: Chinese pretraining by font style information,No.,1,"""No evidence""",2022,2022-02-21T02:45:12Z,,,
arXIv2022,Contextual Semantic Embeddings for Ontology Subsumption Prediction,No.,1,"""No evidence""",2022,2022-02-20T11:14:04Z,,,
arXIv2022,"Do Transformers know symbolic rules, and would we know if they did?",No.,1,"""No evidence""",2022,2022-02-19T09:56:38Z,,,
arXIv2022,From FreEM to D'AlemBERT: a Large Corpus and a Language Model for Early Modern French,No.,1,"""No evidence""",2022,2022-02-18T22:17:22Z,,,
arXIv2022,Mixture-of-Experts with Expert Choice Routing,No.,1,"""No evidence""",2022,2022-02-18T17:46:11Z,,,
arXIv2022,Evaluating the Construct Validity of Text Embeddings with Application to Survey Questions,No.,1,"""No evidence""",2022,2022-02-18T12:35:46Z,,,
arXIv2022,AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification,No.,1,"""No evidence""",2022,2022-02-18T09:41:37Z,,,
arXIv2022,An alternative approach to train neural networks using monotone variational inequality,No.,1,"""No evidence""",2022,2022-02-17T19:24:20Z,,,
arXIv2022,LAMP: Extracting Text from Gradients with Language Model Priors,No.,1,"""No evidence""",2022,2022-02-17T18:49:25Z,,,
arXIv2022,A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models,No.,1,"""No evidence""",2022,2022-02-17T17:17:43Z,,,
arXIv2022,Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing,No.,1,"""No evidence""",2022,2022-02-17T15:33:27Z,,,
arXIv2022,Revisiting Over-smoothing in BERT from the Perspective of Graph,No.,1,"""No evidence""",2022,2022-02-17T12:20:52Z,,,
arXIv2022,When BERT Meets Quantum Temporal Convolution Learning for Text Classification in Heterogeneous Computing,No.,1,"""No evidence""",2022,2022-02-17T09:55:21Z,,,
arXIv2022,Knowledge-informed Molecular Learning: A Survey on Paradigm Transfer,No.,1,"""No evidence""",2022,2022-02-17T06:18:02Z,,,
arXIv2022,FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction,No.,1,"""No evidence""",2022,2022-02-16T20:11:31Z,,,
arXIv2022,GraphNLI: A Graph-based Natural Language Inference Model for Polarity Prediction in Online Debates,No.,1,"""No evidence""",2022,2022-02-16T16:26:21Z,,,
arXIv2022,Capitalization Normalization for Language Modeling with an Accurate and Efficient Hierarchical RNN Model,No.,1,"""No evidence""",2022,2022-02-16T16:21:53Z,,,
arXIv2022,XFBoost: Improving Text Generation with Controllable Decoders,No.,1,"""No evidence""",2022,2022-02-16T15:00:25Z,,,
arXIv2022,No One Left Behind: Inclusive Federated Learning over Heterogeneous Devices,No.,1,"""No evidence""",2022,2022-02-16T13:03:27Z,,,
arXIv2022,Multimodal Emotion Recognition using Transfer Learning from Speaker Recognition and BERT-based models,No.,1,"""No evidence""",2022,2022-02-16T00:23:42Z,,,
arXIv2022,"Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation",No.,1,"""No evidence""",2022,2022-02-15T18:53:58Z,,,
arXIv2022,Defending against Reconstruction Attacks with Rnyi Differential Privacy,No.,1,"""No evidence""",2022,2022-02-15T18:09:30Z,,,
arXIv2022,"BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer",No.,1,"""No evidence""",2022,2022-02-15T16:25:02Z,,,
arXIv2022,Toxic Comments Hunter : Score Severity of Toxic Comments,No.,1,"""No evidence""",2022,2022-02-15T07:35:52Z,,,
arXIv2022,Impact of Pretraining Term Frequencies on Few-Shot Reasoning,No.,1,"""No evidence""",2022,2022-02-15T05:43:54Z,,,
arXIv2022,Text-Based Action-Model Acquisition for Planning,No.,1,"""No evidence""",2022,2022-02-15T02:23:31Z,,,
arXIv2022,QuadSim: A Quadcopter Rotational Dynamics Simulation Framework For Reinforcement Learning Algorithms,No.,1,"""No evidence""",2022,2022-02-14T20:34:08Z,,,
arXIv2022,Punctuation restoration in Swedish through fine-tuned KB-BERT,No.,1,"""No evidence""",2022,2022-02-14T14:39:40Z,,,
arXIv2022,CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences,No.,1,"""No evidence""",2022,2022-02-14T13:26:54Z,,,
arXIv2022,Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings,No.,1,"""No evidence""",2022,2022-02-14T12:57:37Z,,,
arXIv2022,UserBERT: Modeling Long- and Short-Term User Preferences via Self-Supervision,No.,1,"""No evidence""",2022,2022-02-14T08:31:36Z,,,
arXIv2022,"Modeling Intention, Emotion and External World in Dialogue Systems",No.,1,"""No evidence""",2022,2022-02-14T04:10:34Z,,,
arXIv2022,Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers,No.,1,"""No evidence""",2022,2022-02-12T14:23:30Z,,,
arXIv2022,USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder,No.,1,"""No evidence""",2022,2022-02-12T11:35:59Z,,,
arXIv2022,A multi-task semi-supervised framework for Text2Graph & Graph2Text,No.,1,"""No evidence""",2022,2022-02-12T11:02:17Z,,,
arXIv2022,White-Box Attacks on Hate-speech BERT Classifiers in German with Explicit and Implicit Character Level Defense,No.,1,"""No evidence""",2022,2022-02-11T17:20:50Z,,,
arXIv2022,HaT5: Hate Language Identification using Text-to-Text Transfer Transformer,No.,1,"""No evidence""",2022,2022-02-11T15:21:27Z,,,
arXIv2022,Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs,No.,1,"""No evidence""",2022,2022-02-10T21:20:53Z,,,
arXIv2022,Locating and Editing Factual Associations in GPT,No.,1,"""No evidence""",2022,2022-02-10T18:59:54Z,,,
arXIv2022,Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding,No.,1,"""No evidence""",2022,2022-02-10T18:13:32Z,,,
arXIv2022,Zero Shot Learning for Predicting Energy Usage of Buildings in Sustainable Design,No.,1,"""No evidence""",2022,2022-02-10T18:08:58Z,,,
arXIv2022,InPars: Data Augmentation for Information Retrieval using Large Language Models,No.,1,"""No evidence""",2022,2022-02-10T16:52:45Z,,,
arXIv2022,Slovene SuperGLUE Benchmark: Translation and Evaluation,No.,1,"""No evidence""",2022,2022-02-10T12:46:06Z,,,
arXIv2022,Predicting Human Similarity Judgments Using Large Language Models,No.,1,"""No evidence""",2022,2022-02-09T21:09:25Z,,,
arXIv2022,Generating Training Data with Language Models: Towards Zero-Shot Language Understanding,No.,1,"""No evidence""",2022,2022-02-09T16:02:18Z,,,
arXIv2022,Social Media as an Instant Source of Feedback on Water Quality,No.,1,"""No evidence""",2022,2022-02-09T13:47:33Z,,,
arXIv2022,Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?,No.,1,"""No evidence""",2022,2022-02-09T06:47:40Z,,,
arXIv2022,The Volcspeech system for the ICASSP 2022 multi-channel multi-party meeting transcription challenge,No.,1,"""No evidence""",2022,2022-02-09T03:38:39Z,,,
arXIv2022,Using a Language Model in a Kiosk Recommender System at Fast-Food Restaurants,No.,1,"""No evidence""",2022,2022-02-08T21:09:36Z,,,
arXIv2022,PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX,No.,1,"""No evidence""",2022,2022-02-08T19:27:48Z,,,
arXIv2022,DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models,No.,1,"""No evidence""",2022,2022-02-08T18:36:52Z,,,
arXIv2022,Differentiable N-gram Objective on Abstractive Summarization,No.,1,"""No evidence""",2022,2022-02-08T17:19:23Z,,,
arXIv2022,TimeLMs: Diachronic Language Models from Twitter,No.,1,"""No evidence""",2022,2022-02-08T12:47:38Z,,,
arXIv2022,skrl: Modular and Flexible Library for Reinforcement Learning,No.,1,"""No evidence""",2022,2022-02-08T12:43:31Z,,,
arXIv2022,What are the best systems? New perspectives on NLP Benchmarking,No.,1,"""No evidence""",2022,2022-02-08T11:44:20Z,,,
arXIv2022,Semantic features of object concepts generated with GPT-3,No.,1,"""No evidence""",2022,2022-02-08T09:51:48Z,,,
arXIv2022,How to Understand Masked Autoencoders,No.,1,"""No evidence""",2022,2022-02-08T06:15:07Z,,,
arXIv2022,Causal Scene BERT: Improving object detection by searching for challenging groups of data,No.,1,"""No evidence""",2022,2022-02-08T05:14:16Z,,,
arXIv2022,ECRECer: Enzyme Commission Number Recommendation and Benchmarking based on Multiagent Dual-core Learning,No.,1,"""No evidence""",2022,2022-02-08T04:00:49Z,,,
arXIv2022,HistBERT: A Pre-trained Language Model for Diachronic Lexical Semantic Analysis,No.,1,"""No evidence""",2022,2022-02-08T02:53:48Z,,,
arXIv2022,Universal Spam Detection using Transfer Learning of BERT Model,No.,1,"""No evidence""",2022,2022-02-07T19:37:39Z,,,
arXIv2022,Cedille: A large autoregressive French language model,No.,1,"""No evidence""",2022,2022-02-07T17:40:43Z,,,
arXIv2022,Soft Actor-Critic with Inhibitory Networks for Faster Retraining,No.,1,"""No evidence""",2022,2022-02-07T03:10:34Z,,,
arXIv2022,Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data,No.,1,"""No evidence""",2022,2022-02-06T20:07:35Z,,,
arXIv2022,"Ethics, Rules of Engagement, and AI: Neural Narrative Mapping Using Large Transformer Language Models",No.,1,"""No evidence""",2022,2022-02-05T22:08:21Z,,,
arXIv2022,Classification on Sentence Embeddings for Legal Assistance,No.,1,"""No evidence""",2022,2022-02-05T20:57:05Z,,,
arXIv2022,Webly Supervised Concept Expansion for General Purpose Vision Models,No.,1,"""No evidence""",2022,2022-02-04T18:58:36Z,,,
arXIv2022,StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?,No.,1,"""No evidence""",2022,2022-02-04T17:50:53Z,,,
arXIv2022,From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer,No.,1,"""No evidence""",2022,2022-02-04T12:52:32Z,,,
arXIv2022,Temporal Attention for Language Models,No.,1,"""No evidence""",2022,2022-02-04T11:55:34Z,,,
arXIv2022,A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications,No.,1,"""No evidence""",2022,2022-02-04T08:16:56Z,,,
arXIv2022,A Unified Training Process for Fake News Detection based on Fine-Tuned BERT Model,No.,1,"""No evidence""",2022,2022-02-03T23:23:26Z,,,
arXIv2022,Self-supervised Learning with Random-projection Quantizer for Speech Recognition,No.,1,"""No evidence""",2022,2022-02-03T21:29:04Z,,,
arXIv2022,Pre-Trained Language Models for Interactive Decision-Making,No.,1,"""No evidence""",2022,2022-02-03T18:55:52Z,,,
arXIv2022,mSLAM: Massively multilingual joint pre-training for speech and text,No.,1,"""No evidence""",2022,2022-02-03T02:26:40Z,,,
arXIv2022,ASR-Aware End-to-end Neural Diarization,No.,1,"""No evidence""",2022,2022-02-02T21:17:14Z,,,
arXIv2022,Unified Scaling Laws for Routed Language Models,No.,1,"""No evidence""",2022,2022-02-02T17:58:52Z,,,
arXIv2022,"L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources",No.,1,"""No evidence""",2022,2022-02-02T17:35:52Z,,,
arXIv2022,RescoreBERT: Discriminative Speech Recognition Rescoring with BERT,No.,1,"""No evidence""",2022,2022-02-02T15:45:26Z,,,
arXIv2022,Robust Training of Neural Networks Using Scale Invariant Architectures,No.,1,"""No evidence""",2022,2022-02-02T11:58:56Z,,,
arXIv2022,A Semi-Supervised Deep Clustering Pipeline for Mining Intentions From Texts,No.,1,"""No evidence""",2022,2022-02-01T23:01:05Z,,,
arXIv2022,BEA-Base: A Benchmark for ASR of Spontaneous Hungarian,No.,1,"""No evidence""",2022,2022-02-01T17:45:22Z,,,
arXIv2022,AlphaDesign: A graph protein design method and benchmark on AlphaFoldDB,No.,1,"""No evidence""",2022,2022-02-01T08:28:24Z,,,
arXIv2022,Transformer-based Models of Text Normalization for Speech Applications,No.,1,"""No evidence""",2022,2022-02-01T00:03:41Z,,,
arXIv2022,Learning affective meanings that derives the social behavior using Bidirectional Encoder Representations from Transformers,No.,1,"""No evidence""",2022,2022-01-31T19:58:28Z,,,
arXIv2022,Score vs. Winrate in Score-Based Games: which Reward for Reinforcement Learning?,No.,1,"""No evidence""",2022,2022-01-31T12:38:02Z,,,
arXIv2022,Assessment of DeepONet for reliability analysis of stochastic nonlinear dynamical systems,No.,1,"""No evidence""",2022,2022-01-31T11:41:08Z,,,
arXIv2022,Disaster Tweets Classification using BERT-Based Language Model,No.,1,"""No evidence""",2022,2022-01-31T10:25:29Z,,,
arXIv2022,Similarity Learning based Few Shot Learning for ECG Time Series Classification,No.,1,"""No evidence""",2022,2022-01-31T09:47:15Z,,,
arXIv2022,A Frustratingly Simple Approach for End-to-End Image Captioning,No.,1,"""No evidence""",2022,2022-01-30T04:44:54Z,,,
arXIv2022,MVPTR: Multi-Level Semantic Alignment for Vision-Language Pre-Training via Multi-Stage Learning,No.,1,"""No evidence""",2022,2022-01-29T14:30:59Z,,,
arXIv2022,AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,No.,1,"""No evidence""",2022,2022-01-29T06:13:04Z,,,
arXIv2022,Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval,No.,1,"""No evidence""",2022,2022-01-28T21:38:56Z,,,
arXIv2022,Schema-Free Dependency Parsing via Sequence Generation,No.,1,"""No evidence""",2022,2022-01-28T20:32:04Z,,,
arXIv2022,Describing Differences between Text Distributions with Natural Language,No.,1,"""No evidence""",2022,2022-01-28T18:38:13Z,,,
arXIv2022,A Post-Quantum Associative Memory,No.,1,"""No evidence""",2022,2022-01-28T18:10:19Z,,,
arXIv2022,From data to functa: Your data point is a function and you can treat it like one,No.,1,"""No evidence""",2022,2022-01-28T15:59:58Z,,,
arXIv2022,"Protum: A New Method For Prompt Tuning Based on ""[MASK]""",No.,1,"""No evidence""",2022,2022-01-28T13:34:30Z,,,
arXIv2022,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",No.,1,"""No evidence""",2022,2022-01-28T08:59:57Z,,,
arXIv2022,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,No.,1,"""No evidence""",2022,2022-01-28T02:33:07Z,,,
arXIv2022,"""That's so cute!"": The CARE Dataset for Affective Response Detection",No.,1,"""No evidence""",2022,2022-01-28T02:17:50Z,,,
arXIv2022,Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers,No.,1,"""No evidence""",2022,2022-01-28T00:50:01Z,,,
arXIv2022,Neural-FST Class Language Model for End-to-End Speech Recognition,No.,1,"""No evidence""",2022,2022-01-28T00:20:57Z,,,
arXIv2022,Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation,No.,1,"""No evidence""",2022,2022-01-27T15:29:30Z,,,
arXIv2022,Language-biased image classification: evaluation based on semantic representations,No.,1,"""No evidence""",2022,2022-01-26T15:46:36Z,,,
arXIv2022,Learning To Recognize Procedural Activities with Distant Supervision,No.,1,"""No evidence""",2022,2022-01-26T15:06:28Z,,,
arXIv2022,FiNCAT: Financial Numeral Claim Analysis Tool,No.,1,"""No evidence""",2022,2022-01-26T11:53:34Z,,,
arXIv2022,On the Effectiveness of Pinyin-Character Dual-Decoding for End-to-End Mandarin Chinese ASR,No.,1,"""No evidence""",2022,2022-01-26T07:59:03Z,,,
arXIv2022,Internal Language Model Estimation Through Explicit Context Vector Learning for Attention-based Encoder-decoder ASR,No.,1,"""No evidence""",2022,2022-01-26T07:47:27Z,,,
arXIv2022,Self-supervised 3D Semantic Representation Learning for Vision-and-Language Navigation,No.,1,"""No evidence""",2022,2022-01-26T07:43:47Z,,,
arXIv2022,Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models,No.,1,"""No evidence""",2022,2022-01-26T02:49:56Z,,,
arXIv2022,A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model,No.,1,"""No evidence""",2022,2022-01-26T02:10:32Z,,,
arXIv2022,Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,No.,1,"""No evidence""",2022,2022-01-25T17:20:04Z,,,
arXIv2022,Differentially Private Temporal Difference Learning with Stochastic Nonconvex-Strongly-Concave Optimization,No.,1,"""No evidence""",2022,2022-01-25T16:48:29Z,,,
arXIv2022,BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment,No.,1,"""No evidence""",2022,2022-01-25T11:29:58Z,,,
arXIv2022,Pre-Trained Language Transformers are Universal Image Classifiers,No.,1,"""No evidence""",2022,2022-01-25T08:56:14Z,,,
arXIv2022,Multimodal data matters: language model pre-training over structured and unstructured electronic health records,No.,1,"""No evidence""",2022,2022-01-25T06:14:49Z,,,
arXIv2022,Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and Resources,No.,1,"""No evidence""",2022,2022-01-25T03:05:23Z,,,
arXIv2022,Relational Memory Augmented Language Models,No.,1,"""No evidence""",2022,2022-01-24T13:25:41Z,,,
arXIv2022,Unified Multimodal Punctuation Restoration Framework for Mixed-Modality Corpus,No.,1,"""No evidence""",2022,2022-01-24T10:15:53Z,,,
arXIv2022,Synthetic Books,No.,1,"""No evidence""",2022,2022-01-24T08:26:28Z,,,
arXIv2022,Emotion-based Modeling of Mental Disorders on Social Media,No.,1,"""No evidence""",2022,2022-01-24T04:41:02Z,,,
arXIv2022,An Application of Pseudo-Log-Likelihoods to Natural Language Scoring,No.,1,"""No evidence""",2022,2022-01-23T22:00:54Z,,,
arXIv2022,A Large and Diverse Arabic Corpus for Language Modeling,No.,1,"""No evidence""",2022,2022-01-23T11:17:53Z,,,
arXIv2022,Chinese Word Segmentation with Heterogeneous Graph Neural Network,No.,1,"""No evidence""",2022,2022-01-22T06:25:56Z,,,
arXIv2022,Nearest Class-Center Simplification through Intermediate Layers,No.,1,"""No evidence""",2022,2022-01-21T23:21:26Z,,,
arXIv2022,Recurrent Neural Networks with Mixed Hierarchical Structures and EM Algorithm for Natural Language Processing,No.,1,"""No evidence""",2022,2022-01-21T23:08:33Z,,,
arXIv2022,Less is Less: When Are Snippets Insufficient for Human vs Machine Relevance Estimation?,No.,1,"""No evidence""",2022,2022-01-21T14:41:16Z,,,
arXIv2022,A Comparative Study on Language Models for Task-Oriented Dialogue Systems,No.,1,"""No evidence""",2022,2022-01-21T13:24:25Z,,,
arXIv2022,Deep Q-learning: a robust control approach,No.,1,"""No evidence""",2022,2022-01-21T09:47:34Z,,,
arXIv2022,Identifying Adversarial Attacks on Text Classifiers,No.,1,"""No evidence""",2022,2022-01-21T06:16:04Z,,,
arXIv2022,AutoDistill: an End-to-End Framework to Explore and Distill Hardware-Efficient Language Models,No.,1,"""No evidence""",2022,2022-01-21T04:32:19Z,,,
arXIv2022,Black-box Prompt Learning for Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-01-21T03:53:19Z,,,
arXIv2022,Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models,No.,1,"""No evidence""",2022,2022-01-20T22:11:38Z,,,
arXIv2022,End-to-end Generative Pretraining for Multimodal Video Captioning,No.,1,"""No evidence""",2022,2022-01-20T16:16:21Z,,,
arXIv2022,LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training,No.,1,"""No evidence""",2022,2022-01-20T09:29:34Z,,,
arXIv2022,Sentiment Analysis: Predicting Yelp Scores,No.,1,"""No evidence""",2022,2022-01-20T04:47:12Z,,,
arXIv2022,Near-Optimal Sparse Allreduce for Distributed Deep Learning,No.,1,"""No evidence""",2022,2022-01-19T13:56:57Z,,,
arXIv2022,TourBERT: A pretrained language model for the tourism industry,No.,1,"""No evidence""",2022,2022-01-19T07:24:30Z,,,
arXIv2022,GAP-Gen: Guided Automatic Python Code Generation,No.,1,"""No evidence""",2022,2022-01-19T06:32:47Z,,,
arXIv2022,Many Ways to Be Lonely: Fine-Grained Characterization of Loneliness and Its Potential Changes in COVID-19,No.,1,"""No evidence""",2022,2022-01-19T05:22:55Z,,,
arXIv2022,Fooling MOSS Detection with Pretrained Language Models,No.,1,"""No evidence""",2022,2022-01-19T04:00:46Z,,,
arXIv2022,Accelerating Representation Learning with View-Consistent Dynamics in Data-Efficient Reinforcement Learning,No.,1,"""No evidence""",2022,2022-01-18T14:28:30Z,,,
arXIv2022,Towards a Cleaner Document-Oriented Multilingual Crawled Corpus,No.,1,"""No evidence""",2022,2022-01-17T22:12:59Z,,,
arXIv2022,Language Model-Based Paired Variational Autoencoders for Robotic Language Learning,No.,1,"""No evidence""",2022,2022-01-17T10:05:26Z,,,
arXIv2022,"MuLVE, A Multi-Language Vocabulary Evaluation Data Set",No.,1,"""No evidence""",2022,2022-01-17T09:02:59Z,,,
arXIv2022,Korean-Specific Dataset for Table Question Answering,No.,1,"""No evidence""",2022,2022-01-17T05:47:44Z,,,
arXIv2022,Natural Language Deduction through Search over Statement Compositions,No.,1,"""No evidence""",2022,2022-01-16T12:05:48Z,,,
arXIv2022,UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models,No.,1,"""No evidence""",2022,2022-01-16T04:36:18Z,,,
arXIv2022,WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation,No.,1,"""No evidence""",2022,2022-01-16T03:13:49Z,,,
arXIv2022,Automatic Correction of Syntactic Dependency Annotation Differences,No.,1,"""No evidence""",2022,2022-01-15T17:17:55Z,,,
arXIv2022,StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning,No.,1,"""No evidence""",2022,2022-01-15T17:04:38Z,,,
arXIv2022,Automatic Lexical Simplification for Turkish,No.,1,"""No evidence""",2022,2022-01-15T15:58:44Z,,,
arXIv2022,Tailor Versatile Multi-modal Learning for Multi-label Emotion Recognition,No.,1,"""No evidence""",2022,2022-01-15T12:02:28Z,,,
arXIv2022,A Novel Multi-Task Learning Method for Symbolic Music Emotion Recognition,No.,1,"""No evidence""",2022,2022-01-15T07:45:10Z,,,
arXIv2022,Machine Learning for Food Review and Recommendation,No.,1,"""No evidence""",2022,2022-01-15T02:33:59Z,,,
arXIv2022,Polarity and Subjectivity Detection with Multitask Learning and BERT Embedding,No.,1,"""No evidence""",2022,2022-01-14T09:52:15Z,,,
arXIv2022,Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation,No.,1,"""No evidence""",2022,2022-01-14T04:50:28Z,,,
arXIv2022,Assemble Foundation Models for Automatic Code Summarization,No.,1,"""No evidence""",2022,2022-01-13T21:38:33Z,,,
arXIv2022,Multi-task Pre-training Language Model for Semantic Network Completion,No.,1,"""No evidence""",2022,2022-01-13T09:18:30Z,,,
arXIv2022,Direct Mutation and Crossover in Genetic Algorithms Applied to Reinforcement Learning Tasks,No.,1,"""No evidence""",2022,2022-01-13T07:19:28Z,,,
arXIv2022,Detection of Increased Time Intervals of Anti-Vaccine Tweets for COVID-19 Vaccine with BERT Model,No.,1,"""No evidence""",2022,2022-01-12T18:30:23Z,,,
arXIv2022,Diagnosing BERT with Retrieval Heuristics,No.,1,"""No evidence""",2022,2022-01-12T13:11:17Z,,,
arXIv2022,PromptBERT: Improving BERT Sentence Embeddings with Prompts,No.,1,"""No evidence""",2022,2022-01-12T06:54:21Z,,,
arXIv2022,A Feature Extraction based Model for Hate Speech Identification,No.,1,"""No evidence""",2022,2022-01-11T22:53:28Z,,,
arXIv2022,Polish Natural Language Inference and Factivity -- an Expert-based Dataset and Benchmarks,No.,1,"""No evidence""",2022,2022-01-10T18:32:55Z,,,
arXIv2022,Black-Box Tuning for Language-Model-as-a-Service,No.,1,"""No evidence""",2022,2022-01-10T18:17:05Z,,,
arXIv2022,BERT for Sentiment Analysis: Pre-trained and Fine-Tuned Alternatives,No.,1,"""No evidence""",2022,2022-01-10T15:05:05Z,,,
arXIv2022,Handwriting recognition and automatic scoring for descriptive answers in Japanese language tests,No.,1,"""No evidence""",2022,2022-01-10T08:47:52Z,,,
arXIv2022,Semantic and sentiment analysis of selected Bhagavad Gita translations using BERT-based language framework,No.,1,"""No evidence""",2022,2022-01-09T23:59:11Z,,,
arXIv2022,Medication Error Detection Using Contextual Language Models,No.,1,"""No evidence""",2022,2022-01-09T15:21:54Z,,,
arXIv2022,An Ensemble Approach to Acronym Extraction using Transformers,No.,1,"""No evidence""",2022,2022-01-09T14:49:46Z,,,
arXIv2022,Textual Data Augmentation for Arabic-English Code-Switching Speech Recognition,No.,1,"""No evidence""",2022,2022-01-07T17:14:19Z,,,
arXIv2022,"An Opinion Mining of Text in COVID-19 Issues along with Comparative Study in ML, BERT & RNN",No.,1,"""No evidence""",2022,2022-01-06T15:59:20Z,,,
arXIv2022,Self-Training Vision Language BERTs with a Unified Conditional Model,No.,1,"""No evidence""",2022,2022-01-06T11:00:52Z,,,
arXIv2022,Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,No.,1,"""No evidence""",2022,2022-01-05T17:40:45Z,,,
arXIv2022,Comparison of biomedical relationship extraction methods and models for knowledge graph creation,No.,1,"""No evidence""",2022,2022-01-05T15:09:33Z,,,
arXIv2022,Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety,No.,1,"""No evidence""",2022,2022-01-04T04:21:07Z,,,
arXIv2022,An Adversarial Benchmark for Fake News Detection Models,No.,1,"""No evidence""",2022,2022-01-03T23:51:55Z,,,
arXIv2022,Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models,No.,1,"""No evidence""",2022,2022-01-03T10:07:13Z,,,
arXIv2022,Semantic Search for Large Scale Clinical Ontologies,No.,1,"""No evidence""",2022,2022-01-01T05:15:42Z,,,
arXIv2022,Black-box language model explanation by context length probing,Yes.,1,"""We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies.""",2022,2022-12-30T16:24:10Z,,,
arXIv2022,Targeted Phishing Campaigns using Large Scale Language Models,Yes.,2,"""Our evaluations show that NLMs are capable of generating phishing emails that are difficult to detect and that have a high success rate in tricking individuals, but their effectiveness varies based on the specific NLM and training data used.""",2022,2022-12-30T03:18:05Z,,,
arXIv2022,Cramming: Training a Language Model on a Single GPU in One Day,Yes.,3,"""training language models is out of reach for most researchers and practitioners"" and ""We investigate why scaling down is hard, and which modifications actually improve performance in this scenario.""",2022,2022-12-28T18:59:28Z,,,
arXIv2022,Using Large Language Models to Generate Engaging Captions for Data Visualizations,Yes.,1,"""In this work we explore the opportunities offered by the newly emerging crop of large language models (LLM)"" and ""We report on first experiments using the popular LLM GPT-3 and deliver some promising results.""",2022,2022-12-27T23:56:57Z,,,
arXIv2022,TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence,Yes.,1,"""Moreover, an \emph{Embedding-Fusion} module that combines the domain-specific word embeddings learnt from the given corpus and the general-purpose word embeddings provided by a GPT-2 model pre-trained on massive text data is integrated into the decoder.""",2022,2022-12-27T11:50:14Z,,,
arXIv2022,DeepCuts: Single-Shot Interpretability based Pruning for BERT,Yes.,3,"""As language models have grown in parameters and layers, it has become much harder to train and infer with them on single GPUs. This is severely restricting the availability of large language models such as GPT-3, BERT-Large, and many others.""",2022,2022-12-27T07:21:41Z,,,
arXIv2022,Measuring an artificial intelligence agent's trust in humans using machine incentives,Yes.,1,"""We present a method for incentivizing machine decisions without altering an AI agent's underlying algorithms or goal orientation.""",2022,2022-12-27T06:05:49Z,,,
arXIv2022,Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,Yes.,3,"""We show that, while annotators often struggle at this task, there is substantial variance in annotator skill and that given proper incentives, annotators can improve at this task over time."" and ""Finally, we collect error annotations from our participants and use them to show that certain textual genres influence models to make different types of errors and that certain sentence-level features correlate highly with annotator",2022,2022-12-24T06:40:25Z,,,
arXIv2022,Methodological reflections for AI alignment research using human feedback,Yes.,3,"""AI alignment is particularly relevant for large language models (LLMs), which have the potential to exhibit unintended behavior due to their ability to learn and adapt in ways that are difficult to predict.""",2022,2022-12-22T14:27:33Z,,,
arXIv2022,Multi-Lingual DALL-E Storytime,Yes.,3,"""its decreased performance when given input in a different language, limits its audience and deepens the gap between populations,"" and ""An additional limitation of the current DALL-E model is that it only allows for the creation of a few images in response to a given input prompt, rather than a series of consecutive coherent frames that tell a story or describe a process that changes over time.""",2022,2022-12-22T07:06:35Z,,,
arXIv2022,CAMeMBERT: Cascading Assistant-Mediated Multilingual BERT,Yes.,3,"""Their widespread use and adoption, however, is hindered by the lack of availability and portability of sufficiently large computational resources.""",2022,2022-12-22T02:19:25Z,,,
arXIv2022,What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,Yes.,2,"""Though production applications of our model are limited by ethical considerations, the model's competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.""",2022,2022-12-21T19:11:19Z,,,
arXIv2022,Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges,Yes.,1,"""This paper presents the Crowd Score, a novel method to assess the funniness of jokes using large language models (LLMs) as AI judges.""",2022,2022-12-21T17:41:16Z,,,
arXIv2022,Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal,Yes.,1,"""Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism.""",2022,2022-12-21T16:56:07Z,,,
arXIv2022,KL Regularized Normalization Framework for Low Resource Tasks,Yes.,2,"""the success of normalization in low resource downstream NLP and speech tasks is limited. One of the reasons is the inability to capture expressiveness by rescaling parameters of normalization.""",2022,2022-12-21T05:59:25Z,,,
arXIv2022,CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding,Yes.,3,"""while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have.""",2022,2022-12-21T04:21:35Z,,,
arXIv2022,Self-Instruct: Aligning Language Models with Self-Generated Instructions,Yes.,3,"""Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model.""",2022,2022-12-20T18:59:19Z,,,
arXIv2022,DISCO: Distilling Counterfactuals with Large Language Models,Yes.,2,"""DISCO engineers prompts to generate phrasal perturbations with a large general language model."" and ""high-quality counterfactual data is scarce for most tasks and not easily generated at scale.""",2022,2022-12-20T18:46:08Z,,,
arXIv2022,Evaluating Psychological Safety of Large Language Models,Yes.,3,"""Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3.""",2022,2022-12-20T18:45:07Z,,,
arXIv2022,DePlot: One-shot visual language reasoning by plot-to-table translation,Yes.,1,"""The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.""",2022,2022-12-20T18:20:50Z,,,
arXIv2022,Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?,Yes.,3,"""In WebTOD, the dialogue system learns how to understand the web/mobile interface that the human agent interacts with, powered by a large-scale language model.""",2022,2022-12-20T18:18:41Z,,,
arXIv2022,Is GPT-3 a Good Data Annotator?,Yes.,1,"""GPT-3, a large-scale language model developed by OpenAI, has demonstrated impressive zero- and few-shot performance on a wide range of NLP tasks.""",2022,2022-12-20T17:28:41Z,,,
arXIv2022,Parameter-efficient Zero-shot Transfer for Cross-Language Dense Retrieval with Adapters,Yes.,3,"""However, such transferred models suffer from mismatches in the languages of the input text during training and inference."" and ""However, we found that the prior suggestion of replacing the language adapters to match the target language at inference time is suboptimal for dense retrieval models.""",2022,2022-12-20T17:25:04Z,,,
arXIv2022,Perplexed by Quality: A Perplexity-based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data,Yes.,1,"""As demand for large corpora increases with the size of current state-of-the-art language models, using web data as the main part of the pre-training corpus for these models has become a ubiquitous practice.""",2022,2022-12-20T17:14:45Z,,,
arXIv2022,Data Curation Alone Can Stabilize In-context Learning,Yes.,3,"""ICL is very sensitive to the choice of training examples",2022,2022-12-20T15:58:54Z,,,
arXIv2022,On the Blind Spots of Model-Based Evaluation Metrics for Text Generation,Yes.,3,"""We examine a range of recently proposed evaluation metrics based on pretrained language models,"" and ""Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics.""",2022,2022-12-20T06:24:25Z,,,
arXIv2022,Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,Yes.,3,"""Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance.""",2022,2022-12-20T05:20:54Z,,,
arXIv2022,On Improving Summarization Factual Consistency from Natural Language Feedback,Yes.,3,"""We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.""",2022,2022-12-20T02:47:37Z,,,
arXIv2022,Python Code Generation by Asking Clarification Questions,Yes.,3,"""While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified.""",2022,2022-12-19T22:08:36Z,,,
arXIv2022,Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations,Yes.,3,"""performance drops significantly when no demonstrations are available.""",2022,2022-12-19T21:34:26Z,,,
arXIv2022,The case for 4-bit precision: k-bit Inference Scaling Laws,Yes.,2,"""We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float).""",2022,2022-12-19T18:48:33Z,,,
arXIv2022,Multilingual Sequence-to-Sequence Models for Hebrew NLP,Yes.,3,"""current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages"" and ""previous work on pretrained Hebrew LMs focused on encoder-only models. While the encoder-only architecture is beneficial for classification tasks, it does not cater well for sub-word prediction tasks, such as Named Entity Recognition, when considering the morphologically rich nature",2022,2022-12-19T18:10:23Z,,,
arXIv2022,Visconde: Multi-document QA with GPT-3 and Neural Reranking,Yes.,1,"""The system, called Visconde, uses a three-step pipeline to perform the task",2022,2022-12-19T17:39:07Z,,,
arXIv2022,Explanation Regeneration via Information Bottleneck,Yes.,3,"""explanation generated through single-pass prompting often lacks sufficiency and conciseness.""",2022,2022-12-19T16:41:19Z,,,
arXIv2022,Large Language Models are Better Reasoners with Self-Verification,Yes.,3,"""However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation.""",2022,2022-12-19T15:51:52Z,,,
arXIv2022,Improving the Generalizability of Text-Based Emotion Detection by Leveraging Transformers with Psycholinguistic Features,Yes.,2,"""Yet, deployment of such models in real-world sentiment and emotion applications faces challenges, in particular poor out-of-domain generalizability.""",2022,2022-12-19T13:58:48Z,,,
arXIv2022,ChatGPT: The End of Online Exam Integrity?,Yes.,3,"""This capacity raises concerns about the potential use of ChatGPT as a tool for academic misconduct in online exams"" and ""Further research is needed to fully understand the implications of large language models like ChatGPT and to devise strategies for combating the risk of cheating using these tools.""",2022,2022-12-19T08:15:16Z,,,
arXIv2022,Very Large Language Model as a Unified Methodology of Text Mining,Yes.,3,"""Finally I discuss the challenges in the design and development of VLLM techniques for text mining.""",2022,2022-12-19T06:52:13Z,,,
arXIv2022,TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization,No.,1,The abstract does not mention LLMs or any specific limitations related to them.,2022,2022-12-19T05:55:58Z,,,
arXIv2022,"Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",Yes.,3,"""It brings deeper interaction between mention and types to reach better performance but has to perform N (type set size) forward passes to infer types of a single mention. CE is therefore very slow in inference when the type set is large (e.g., N = 10",2022,2022-12-18T16:42:52Z,,,
arXIv2022,Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search,Yes.,1,"""Pre-trained language models are state-of-the-art on many IR tasks but have yet to be applied to systematic review screening prioritisation.""",2022,2022-12-18T05:26:40Z,,,
arXIv2022,HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation,Yes.,3,"""However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse.""",2022,2022-12-17T11:56:21Z,,,
arXIv2022,Neural Story Planning,Yes.,3,"""pre-trained neural language models can generate stories with great diversity, while being generally incapable of ending a story in a specified manner and can have trouble maintaining coherence.""",2022,2022-12-16T21:29:41Z,,,
arXIv2022,Plansformer: Generating Symbolic Plans using Transformers,Yes.,1,"""Large Language Models (LLMs) have been the subject of active research, significantly advancing the field of Natural Language Processing (NLP). From BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural language tasks such as question answering, summarization, and text generation.""",2022,2022-12-16T19:06:49Z,,,
arXIv2022,LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension,Yes.,3,"""Many popular language models, such as BERT or RoBERTa, are general-purpose models, which have limitations on processing specialized legal terminology and syntax.""",2022,2022-12-16T00:15:14Z,,,
arXIv2022,FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference,Yes.,2,"""the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model.""",2022,2022-12-15T21:35:46Z,,,
arXIv2022,Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models,Yes.,2,"""Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).""",2022,2022-12-15T18:45:29Z,,,
arXIv2022,Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,Yes.,3,"""our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.""",2022,2022-12-15T17:26:05Z,,,
arXIv2022,Visually-augmented pretrained language models for NLP tasks without images,Yes.,3,"""they are found lack of visual semantics or commonsense.""",2022,2022-12-15T16:13:25Z,,,
arXIv2022,DeepJoin: Joinable Table Discovery with Pre-trained Language Models,Yes.,1,"""Our solution is an embedding-based retrieval, which employs a pre-trained language model (PLM) and is designed as one framework serving both equi- and semantic joins.""",2022,2022-12-15T02:40:57Z,,,
arXIv2022,ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages,Yes.,1,"""In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs).""",2022,2022-12-13T17:21:44Z,,,
arXIv2022,Benchmarking Large Language Models for Automated Verilog RTL Code Generation,Yes.,1,"""In this paper, we characterize the ability of LLMs to generate useful Verilog.""",2022,2022-12-13T16:34:39Z,,,
arXIv2022,On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning,No.,1,The abstract does not mention language models (LLMs or LMs) at all.,2022,2022-12-13T13:38:04Z,,,
arXIv2022,Evaluation of Synthetic Datasets for Conversational Recommender Systems,Yes.,3,"""The efficiency brought about by LLMs in the data generation phase is impeded during the process of evaluation of the generated data, since it generally requires human-raters to ensure that the data generated is of high quality and has sufficient diversity.""",2022,2022-12-12T18:53:10Z,,,
arXIv2022,Prompting Is Programming: A Query Language for Large Language Models,Yes.,2,"""However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.""",2022,2022-12-12T18:09:09Z,,,
arXIv2022,A Study of Slang Representation Methods,Yes.,3,"""Despite the success of large language models and the spontaneous emergence of slang dictionaries, it is unclear how far their combination goes in terms of slang understanding for downstream social good tasks."" and ""Our error analysis identifies core challenges for slang representation learning, including out-of-vocabulary words, polysemy, variance, and annotation disagreements, which can",2022,2022-12-11T21:56:44Z,,,
arXIv2022,Elixir: Train a Large Language Model on a Small GPU Cluster,Yes.,2,"""However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs.""",2022,2022-12-10T17:26:05Z,,,
arXIv2022,Structured information extraction from complex scientific text with fine-tuned large language models,Yes.,1,"""The approach leverages a pre-trained large language model (LLM), GPT-3, that is fine-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs).""",2022,2022-12-10T07:51:52Z,,,
arXIv2022,"The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies",No.,1,The abstract does not mention LLMs or any specific type of language models. It focuses on AI in drug discovery in general.,2022,2022-12-08T23:23:39Z,,,
arXIv2022,Learning Video Representations from Large Language Models,Yes.,1,"""We introduce LaViLa, a new approach to learning video-language representations by leveraging Large Language Models (LLMs).""",2022,2022-12-08T18:59:59Z,,,
arXIv2022,Learning Domain Invariant Prompt for Vision-Language Models,Yes.,3,"""However, although prompt learning achieves excellent performance over in-domain data, it still faces the major challenge of generalizing to unseen classes and domains.""",2022,2022-12-08T11:23:24Z,,,
arXIv2022,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,Yes.,3,"""The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly.""",2022,2022-12-08T05:46:32Z,,,
arXIv2022,A Generative Approach for Script Event Prediction via Contrastive Fine-tuning,Yes.,3,"""First, the pretrained language models adopted by current works ignore event-level knowledge, resulting in an inability to capture the correlations between events well.""",2022,2022-12-07T07:32:47Z,,,
arXIv2022,CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain,Yes.,3,"""However, due to the domain-knowledge and many technical terms in cybersecurity general language models might miss the gist of textual information, hence doing more harm than good.""",2022,2022-12-06T13:49:12Z,,,
arXIv2022,LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training,Yes.,3,"""current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors.""",2022,2022-12-06T01:31:37Z,,,
arXIv2022,Legal Prompt Engineering for Multilingual Legal Judgement Prediction,Yes.,3,"""Our results show that zero-shot LPE is better compared to the baselines, but it still falls short compared to current state of the art supervised approaches.""",2022,2022-12-05T12:17:02Z,,,
arXIv2022,Human-in-the-Loop Hate Speech Classification in a Multilingual Context,No.,1,The abstract does not mention language models (LMs or LLMs) explicitly. It focuses on a BERT-based hate speech classification pipeline.,2022,2022-12-05T09:05:40Z,,,
arXIv2022,Nonparametric Masked Language Modeling,Yes.,3,"""Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases.""",2022,2022-12-02T18:10:42Z,,,
arXIv2022,Language Model Pre-training on True Negatives,Yes.,3,"""Existing PLMs simply treat all corrupted texts as equal negative without any examination, which actually lets the resulting model inevitably suffer from the false negative issue where training is carried out on pseudo-negative data and leads to less efficiency and less robustness in the resulting PLMs.""",2022,2022-12-01T12:24:19Z,,,
arXIv2022,Distilling Reasoning Capabilities into Smaller Language Models,Yes.,3,"""However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work.""",2022,2022-12-01T00:39:56Z,,,
arXIv2022,BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?,Yes.,3,"""Many state-of-the-art Language Models (LMs), however, do not scale well above the threshold of 512 input tokens."" and ""since the pretraining process is extremely costly in general - but even more so as the sequence length increases.""",2022,2022-11-30T16:09:20Z,,,
arXIv2022,Quadapter: Adapter for GPT-2 Quantization,Yes.,3,"""Transformer language models such as GPT-2 are difficult to quantize because of outliers in activations leading to a large quantization error.""",2022,2022-11-30T11:20:33Z,,,
arXIv2022,Explicit Knowledge Transfer for Weakly-Supervised Code Generation,Yes.,2,"""In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific NL-code pairs, which are expensive to obtain.""",2022,2022-11-30T04:51:26Z,,,
arXIv2022,Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models,Yes.,3,"""existing models are often overly confident on unseen classes.""",2022,2022-11-28T19:03:35Z,,,
arXIv2022,Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation,Yes.,1,"""We introduce Action-GPT, a plug-and-play framework for incorporating Large Language Models (LLMs) into text-based action generation models.""",2022,2022-11-28T17:57:48Z,,,
arXIv2022,Automatically Extracting Information in Medical Dialogue: Expert System And Attention for Labelling,No.,1,The abstract discusses attention-based models and a novel model called ESAL but does not mention LLMs or their limitations.,2022,2022-11-28T16:49:13Z,,,
arXIv2022,Fine-tuning language models to find agreement among humans with diverse preferences,Yes.,2,"""Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user... This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a single 'generic' user will confer more general alignment.""",2022,2022-11-28T02:24:14Z,,,
arXIv2022,Understanding BLOOM: An empirical study on diverse NLP tasks,Yes.,3,"""BLOOM performance does not scale with parameter size, unlike other LLMs like GPT and BERT"" and ""Zero-shot cross-lingual and multi-lingual fine-tuning experiments show that BLOOM is at par or worse than monolingual GPT-2 models.""",2022,2022-11-27T15:48:14Z,,,
arXIv2022,GPT-3-driven pedagogical agents for training children's curious question-asking skills,Yes.,1,"""we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training.""",2022,2022-11-25T16:41:59Z,,,
arXIv2022,PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices,Yes.,1,"""Pipeline parallelism enables efficient training of Large Language Models (LLMs) on large-scale distributed accelerator clusters.""",2022,2022-11-25T14:16:35Z,,,
arXIv2022,Complementary Explanations for Effective In-Context Learning,Yes.,1,"""Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective.""",2022,2022-11-25T04:40:47Z,,,
arXIv2022,SEAT: Stable and Explainable Attention,No.,1,The abstract does not mention LLMs or their limitations. It focuses on the stability and explainability of the attention mechanism in general NLP models.,2022,2022-11-23T20:33:30Z,,,
arXIv2022,Automatic Generation of Socratic Subquestions for Teaching Math Word Problems,Yes.,1,"""We explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving.""",2022,2022-11-23T10:40:22Z,,,
arXIv2022,OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type,No.,1,The abstract does not mention language models (LLMs) or their limitations.,2022,2022-11-22T10:42:07Z,,,
arXIv2022,Deanthropomorphising NLP: Can a Language Model Be Conscious?,Yes.,2,"""This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models."" and ""Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task.""",2022,2022-11-21T14:18:25Z,,,
arXIv2022,Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text,Yes.,2,"""propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations.""",2022,2022-11-21T09:41:25Z,,,
arXIv2022,Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task,Yes.,3,"""We analyse the capabilities and limitations of our model to better understand the potential of language-music models.""",2022,2022-11-21T07:19:17Z,,,
arXIv2022,L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi,Yes.,2,"""Sentence representation from vanilla BERT models does not work well on sentence similarity tasks.""",2022,2022-11-21T05:15:48Z,,,
arXIv2022,You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model,Yes.,3,"""The performance improvements come with increasing model size, resulting in slow inference speed and increased cost for severing.""",2022,2022-11-21T02:32:25Z,,,
arXIv2022,The Stack: 3 TB of permissively licensed source code,Yes.,2,"""We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets.""",2022,2022-11-20T18:15:30Z,,,
arXIv2022,Leveraging per Image-Token Consistency for Vision-Language Pre-training,Yes.,3,"""However, we find that CMLM is insufficient for this purpose according to our observations",2022,2022-11-20T12:10:53Z,,,
arXIv2022,SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,Yes.,3,"""Large language models (LLMs) show excellent performance but are compute- and memory-intensive.""",2022,2022-11-18T18:59:33Z,,,
arXIv2022,CAPE: Corrective Actions from Precondition Errors using Large Language Models,Yes.,3,"""Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.""",2022,2022-11-17T23:14:51Z,,,
arXIv2022,Prompting PaLM for Translation: Assessing Strategies and Performance,Yes.,3,"""find that its performance, while impressive, still lags that of state-of-the-art supervised systems.""",2022,2022-11-16T18:42:37Z,,,
arXIv2022,Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight BERT,Yes.,2,"""However, the large number of parameters and computations also pose challenges for their deployment.""",2022,2022-11-16T11:43:09Z,,,
arXIv2022,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,Yes.,3,"""few-shot performance gains from including rationales have been largely observed only in +100B language models, and otherwise require large scale manual rationale annotation.""",2022,2022-11-15T19:36:06Z,,,
arXIv2022,PromptCap: Prompt-Guided Task-Aware Image Captioning,Yes.,3,"""However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly.""",2022,2022-11-15T19:07:53Z,,,
arXIv2022,Introducing Semantics into Speech Encoders,Yes.,1,"""Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM.""",2022,2022-11-15T18:44:28Z,,,
arXIv2022,FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery,Yes.,1,"""Thus, we propose a new approach that leverages the generation power of large language models~(LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph.""",2022,2022-11-15T17:20:40Z,,,
arXIv2022,Teaching Algorithmic Reasoning via In-context Learning,Yes.,3,"""Despite this progress, LLMs are still unable to solve algorithmic reasoning problems."" and ""Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved.""",2022,2022-11-15T06:12:28Z,,,
arXIv2022,Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations,Yes.,3,"""for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements.""",2022,2022-11-14T16:46:14Z,,,
arXIv2022,Language Model Classifier Aligns Better with Physician Word Sensitivity than XGBoost on Readmission Prediction,No.,1,"The abstract does not mention LLMs, LLM limitations, or any specific language models. It focuses on a new evaluation metric for classifiers in general, comparing a language model classifier to XGBoost.",2022,2022-11-13T23:59:11Z,,,
arXIv2022,Xu at SemEval-2022 Task 4: Pre-BERT Neural Network Methods vs Post-BERT RoBERTa Approach for Patronizing and Condescending Language Detection,Yes.,1,"""The experiments compare pre-BERT neural network (NN) based systems against post-BERT pretrained language model RoBERTa.""",2022,2022-11-13T10:59:45Z,,,
arXIv2022,Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters,Yes.,3,"""aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations.""",2022,2022-11-13T10:16:39Z,,,
arXIv2022,Textual Data Augmentation for Patient Outcomes Prediction,Yes.,1,"""we fine-tune the generative language model GPT-2 to synthesize labeled text with the original training data.""",2022,2022-11-13T01:07:23Z,,,
arXIv2022,Using Persuasive Writing Strategies to Explain and Detect Health Misinformation,Yes.,1,"""We evaluate fine-tuning and prompt-engineering techniques with pre-trained language models of the BERT family and the generative large language models of the GPT family using persuasive strategies as an additional source of information.""",2022,2022-11-11T03:26:37Z,,,
arXIv2022,Measuring Reliability of Large Language Models through Semantic Consistency,Yes.,3,"""recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers.""",2022,2022-11-10T20:21:07Z,,,
arXIv2022,The CRINGE Loss: Learning what language not to model,Yes.,3,"""Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data.""",2022,2022-11-10T19:30:08Z,,,
arXIv2022,Syntax-Guided Domain Adaptation for Aspect-based Sentiment Analysis,No.,1,"The abstract focuses on aspect-based sentiment analysis (ABSA) and domain adaptation, and does not mention language models (LLMs or LLMs).",2022,2022-11-10T10:09:33Z,,,
arXIv2022,Zero-Label Prompt Selection,Yes.,3,"""the cross-task performance is highly sensitive to the choice of prompts, while selecting a high-performing prompt is challenging given the scarcity of labels.""",2022,2022-11-09T04:13:31Z,,,
arXIv2022,Active Example Selection for In-Context Learning,Yes.,3,"""We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information.""",2022,2022-11-08T19:00:02Z,,,
arXIv2022,Active Learning with Tabular Language Models,Yes.,2,"""real-world applications are still challenging"" and ""open fundamental questions concerning computational efficiency and the perspective of human annotators.""",2022,2022-11-08T09:50:30Z,,,
arXIv2022,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,Yes.,3,"""SPLADE still struggles with exact matching of low-frequency words in training data. In addition, domain shifts in vocabulary and word frequencies deteriorate the IR performance of SPLADE.""",2022,2022-11-08T03:58:26Z,,,
arXIv2022,Retrieval augmentation of large language models for lay language generation,Yes.,3,"""However, the applicability of these models is constrained by the limited size and topical breadth of available corpora."" and ""Such explanation is challenging for neural models to generate because it goes beyond simplification by adding content absent from the source.""",2022,2022-11-07T19:06:53Z,,,
arXIv2022,Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach,Yes.,1,"""We distill several insights from our analysis on study the peer review process with the help of large LMs.""",2022,2022-11-07T16:19:42Z,,,
arXIv2022,AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages,Yes.,3,"""pre-training these large multilingual language models requires a lot of training data, which is not available for African Languages.""",2022,2022-11-07T02:15:25Z,,,
arXIv2022,Noisy Channel for Automatic Text Simplification,No.,1,The abstract does not mention LLMs or any specific limitations of language models.,2022,2022-11-06T15:28:42Z,,,
arXIv2022,Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust,Yes.,3,"""we discover the fact that the state-of-the-art (SOTA) model makes judgment predictions according to irrelevant (or non-casual) information. The violation of rule of law not only weakens the robustness and generalization ability of models but also results in severe social problems like discrimination.""",2022,2022-11-06T07:03:31Z,,,
arXIv2022,MolE: a molecular foundation model for drug discovery,Yes.,1,"""Recently, large language models have addressed this problem by using self-supervised pretraining on large unlabeled datasets, followed by fine-tuning on smaller, labeled datasets.""",2022,2022-11-03T21:22:05Z,,,
arXIv2022,"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model",Yes.,2,"""We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.""",2022,2022-11-03T17:13:48Z,,,
arXIv2022,Contextual information integration for stance detection via cross-attention,Yes.,2,"""Most existing stance detection models are limited because they do not consider relevant contextual information which allows for inferring the stance correctly.""",2022,2022-11-03T15:04:29Z,,,
arXIv2022,BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder,Yes.,3,"""One crucial factor that makes this integration challenging lies in the vocabulary mismatch; the vocabulary constructed for a pre-trained LM is generally too large for E2E-ASR training and is likely to have a mismatch against a target ASR domain.""",2022,2022-11-02T00:10:43Z,,,
arXIv2022,Generating Sequences by Learning to Self-Correct,Yes.,3,"""Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs.""",2022,2022-10-31T18:09:51Z,,,
arXIv2022,GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers,Yes.,3,"""due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models.""",2022,2022-10-31T13:42:40Z,,,
arXIv2022,Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task,Yes.,1,"""this paper proposes a simple but efficient method for generating adversarial negative responses leveraging a large-scale language model.""",2022,2022-10-31T11:49:49Z,,,
arXIv2022,QuaLA-MiniLM: a Quantized Length Adaptive MiniLM,Yes.,3,"""Limited computational budgets often prevent transformers from being used in production and from having their high accuracy utilized,"" and ""the performance of these models drops as we reduce the number of layers, notably in advanced NLP tasks such as span question answering.""",2022,2022-10-31T07:42:52Z,,,
arXIv2022,A Solvable Model of Neural Scaling Laws,Yes.,3,"""whether such scaling laws can break down and how they behave when they do.""",2022,2022-10-30T15:13:18Z,,,
arXIv2022,Solving Math Word Problems via Cooperative Reasoning induced Language Models,Yes.,3,"""However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans.""",2022,2022-10-28T16:47:03Z,,,
arXIv2022,Probing for targeted syntactic knowledge through grammatical error detection,Yes.,3,"""we also observe a divergence in performance when probes are trained on different training sets, and when they are evaluated on different syntactic constructions, suggesting the information pertaining to SVA error detection is not robustly encoded.""",2022,2022-10-28T16:01:25Z,,,
arXIv2022,QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation,Yes.,2,"""Search queries though pose a unique challenge, given their short-length and lack of nuance or context."" and ""While Retrieval Augmentation typically increases latency of LMs (thus hurting distillation efficacy).""",2022,2022-10-27T18:44:58Z,,,
arXIv2022,What Language Model to Train if You Have One Million GPU Hours?,Yes.,3,"""large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone.""",2022,2022-10-27T13:43:27Z,,,
arXIv2022,Truncation Sampling as Language Model Desmoothing,Yes.,3,"""Long samples of text from neural language models can be of poor quality.""",2022,2022-10-27T05:52:35Z,,,
arXIv2022,Learning on Large-scale Text-attributed Graphs via Variational Inference,Yes.,3,"""the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and GNNs together.""",2022,2022-10-26T13:40:57Z,,,
arXIv2022,Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in Financial Sentiment Analysis,Yes.,1,"""The invention of transformer-based models such as BERT, GPT, and RoBERTa has enabled researchers and financial companies to finetune these powerful models and use them in different downstream tasks to achieve state-of-the-art performance.""",2022,2022-10-26T01:13:41Z,,,
arXIv2022,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",Yes.,3,"""zero-shot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints.""",2022,2022-10-25T21:39:36Z,,,
arXIv2022,MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction,Yes.,1,"""New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM).""",2022,2022-10-25T12:08:14Z,,,
arXIv2022,Linguistic-Enhanced Transformer with CTC Embedding for Speech Recognition,No.,1,The abstract focuses on automatic speech recognition (ASR) and does not mention language models (LLMs or LMs).,2022,2022-10-25T08:12:59Z,,,
arXIv2022,Parameter-Efficient Legal Domain Adaptation,Yes.,3,"""State-of-the-art language models are growing increasingly large, making parameter-efficient learning increasingly important. Unfortunately, parameter-efficient methods perform poorly with small amounts of data, which are common in the legal domain (where data labelling costs are high).""",2022,2022-10-25T02:14:15Z,,,
arXIv2022,Proficiency assessment of L2 spoken English using wav2vec 2.0,No.,1,There is no mention of LLMs or their limitations in the abstract or title.,2022,2022-10-24T12:36:49Z,,,
arXIv2022,Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning,Yes.,3,"""With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning.""",2022,2022-10-23T01:33:16Z,,,
arXIv2022,Leveraging Large Language Models for Multiple Choice Question Answering,Yes.,3,"""While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA).""",2022,2022-10-22T05:04:54Z,,,
arXIv2022,Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination,Yes.,3,"""However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., 'an orange is orange'.""",2022,2022-10-21T21:33:10Z,,,
arXIv2022,Using Large Language Models to Enhance Programming Error Messages,Yes.,2,"""We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.""",2022,2022-10-20T23:17:26Z,,,
arXIv2022,Large Language Models Can Self-Improve,Yes.,2,"""However, fine-tuning an LLM requires extensive supervision.""",2022,2022-10-20T21:53:54Z,,,
arXIv2022,ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications,Yes.,1,"""We introduce ObSynth, an interactive system leveraging the domain knowledge embedded in large language models (LLMs) to help users design object models from high level natural language prompts.""",2022,2022-10-20T17:59:19Z,,,
arXIv2022,Language Detoxification with Attribute-Discriminative Latent Space,Yes.,3,"""Transformer-based Language Models (LMs) ... can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications."" and ""previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application.""",2022,2022-10-19T06:54:42Z,,,
arXIv2022,ROSE: Robust Selective Fine-tuning for Pre-trained Language Models,Yes.,3,"""Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.""",2022,2022-10-18T07:53:15Z,,,
arXIv2022,Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models,Yes.,3,"""Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples.""",2022,2022-10-18T02:44:38Z,,,
arXIv2022,Adversarial and Safely Scaled Question Generation,Yes.,2,"""However, there are critical risks of misinformation with these approaches.""",2022,2022-10-17T22:51:45Z,,,
arXIv2022,CAN-BERT do it? Controller Area Network Intrusion Detection System based on BERT Language Model,Yes.,1,"""Inspired by the outstanding performance of bidirectional encoder representations from transformers (BERT) for improving many natural language processing tasks, we propose in this paper ``CAN-BERT"", a deep learning based network intrusion detection system, to detect cyber attacks on CAN bus protocol.""",2022,2022-10-17T21:21:37Z,,,
arXIv2022,Exposing Influence Campaigns in the Age of LLMs: A Behavioral-Based AI Approach to Detecting State-Sponsored Trolls,Yes.,2,"""textual and linguistic properties can be easily mimicked by Large Language Models (LLMs)"" and ""ensuring greater resilience in identifying influence campaigns, especially given the potential increase in the usage of LLMs for generating inauthentic content.""",2022,2022-10-17T07:01:17Z,,,
arXIv2022,A Generative User Simulator with GPT-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems,Yes.,3,"""First, it is unclear whether we can leverage pretrained language models to design, for example, GPT-2 based USs, to catch up and interact with the recently advanced GPT-2 based DSs."" and ""how to flexibly integrate goal state tracking and develop an end-to-end trainable US for multi-domains has remained to be a challenge.""",2022,2022-10-17T01:57:50Z,,,
arXIv2022,MiQA: A Benchmark for Inference on Metaphorical Questions,Yes.,3,"""We also analyse the largest model in a generative setting and find that although human performance is approached, careful multiple-shot prompting is required.""",2022,2022-10-14T17:46:05Z,,,
arXIv2022,DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,Yes.,3,"""With the ever-growing size of pretrained models (PMs), fine-tuning them has become more expensive and resource-hungry."" and ""While LoRA blocks are parameter-efficient, they suffer from two major problems",2022,2022-10-14T06:29:22Z,,,
arXIv2022,Bootstrapping Multilingual Semantic Parsers using Large Language Models,Yes.,3,"""translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models.""",2022,2022-10-13T19:34:14Z,,,
arXIv2022,Mass-Editing Memory in a Transformer,Yes.,3,"""this line of work is predominantly limited to updating single associations.""",2022,2022-10-13T17:55:53Z,,,
arXIv2022,Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods,Yes.,2,"""Instructing GPT-3.5 to generate saliency map verbalizations yields plausible explanations which include associations, abstractive summarization and commonsense reasoning, achieving by far the highest human ratings, but they are not faithfully capturing numeric information and are inconsistent in their interpretation of the task.""",2022,2022-10-13T17:48:15Z,,,
arXIv2022,CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing,Yes.,3,"""Large Language Models (LLMs) excel at SP given only a few examples, however LLMs are unsuitable for runtime systems which require low latency.""",2022,2022-10-13T15:01:03Z,,,
arXIv2022,Spontaneous Emerging Preference in Two-tower Language Model,Yes.,2,"""With the existence of side-effects brought about by the large size of the foundation language model such as deployment cost, availability issues, and environmental cost, there is some interest in exploring other possible directions, such as a divide-and-conquer scheme.""",2022,2022-10-13T13:55:19Z,,,
arXIv2022,Sample-Then-Optimize Batch Neural Thompson Sampling,No.,1,"The abstract discusses Bayesian optimization, Gaussian processes, neural networks, and Thompson sampling but does not mention language models or their limitations.",2022,2022-10-13T09:01:58Z,,,
arXIv2022,Language Models are Realistic Tabular Data Generators,Yes.,1,"""less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature.""",2022,2022-10-12T15:03:28Z,,,
arXIv2022,A context-aware knowledge transferring strategy for CTC-based ASR,Yes.,2,"""The former is designed to distill linguistic information from a pre-trained language model, and the latter is framed to modulate the limitations caused by the conditional independence assumption.""",2022,2022-10-12T14:31:38Z,,,
arXIv2022,AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning,Yes.,3,"""Fine-tuning large pre-trained language models on downstream tasks is apt to suffer from overfitting when limited training data is available.""",2022,2022-10-12T02:54:41Z,,,
arXIv2022,Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval,Yes.,1,"""The case-view contrastive learning minimizes the hidden space distance between relevant legal case representations produced by a pre-trained language model (PLM) encoder.""",2022,2022-10-11T06:47:23Z,,,
arXIv2022,Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks,Yes.,2,"""the generic corpora used to pretrain the teacher and the corpora associated with the downstream target domain are often significantly different, which raises a natural question",2022,2022-10-10T16:49:52Z,,,
arXIv2022,Readability Controllable Biomedical Document Summarization,Yes.,3,"""Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation, the performance of existing controllable summarization methods is far from desirable in this task.""",2022,2022-10-10T14:03:20Z,,,
arXIv2022,DEPTWEET: A Typology for Social Media Texts to Detect Depression Severities,No.,1,The abstract does not mention LLMs or any specific limitations related to them.,2022,2022-10-10T08:23:57Z,,,
arXIv2022,FairGer: Using NLP to Measure Support for Women and Migrants in 155 Years of German Parliamentary Debates,No.,1,The abstract does not mention LLMs or any other language models.,2022,2022-10-09T22:02:58Z,,,
arXIv2022,Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection,Yes.,1,"""Pre-training large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks.""",2022,2022-10-09T13:53:06Z,,,
arXIv2022,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,Yes.,3,"""We also systematically analyzed the sensitivity of the InstructGPT model to prompt design, temperature, and injected spelling errors, and found that the model is particularly sensitive to certain variations (e.g., questions vs. imperative statements).""",2022,2022-10-09T06:35:14Z,,,
arXIv2022,"KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",Yes.,3,"""Such rich contextualization can be especially beneficial for long document understanding tasks since standard pretrained LMs are typically bounded by the input sequence length.""",2022,2022-10-08T20:51:02Z,,,
arXIv2022,Understanding HTML with Large Language Models,Yes.,1,"""Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks.""",2022,2022-10-08T07:27:17Z,,,
arXIv2022,AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models,Yes.,1,"""There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet.""",2022,2022-10-08T00:36:00Z,,,
arXIv2022,How Large Language Models are Transforming Machine-Paraphrased Plagiarism,Yes.,2,"""However, the role of large autoregressive transformers in generating machine-paraphrased plagiarism and their detection is still developing in the literature.""",2022,2022-10-07T14:08:57Z,,,
arXIv2022,Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners,Yes.,3,"""meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training.""",2022,2022-10-06T15:00:47Z,,,
arXIv2022,ReAct: Synergizing Reasoning and Acting in Language Models,Yes.,3,"""ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API.""",2022,2022-10-06T01:00:32Z,,,
arXIv2022,Learning to Reason With Relational Abstractions,Yes.,3,"""the solution sequences are not formally structured and the resulting model-generated sequences may not reflect the kind of systematic reasoning we might expect an expert human to produce.""",2022,2022-10-06T00:27:50Z,,,
arXIv2022,Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,Yes.,1,"""we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection.""",2022,2022-10-05T18:44:35Z,,,
arXIv2022,Antibody Representation Learning for Drug Discovery,Yes.,2,"""Existing works have not yet investigated the value, limitations and opportunities of these methods in application to antibody-based drug discovery.""",2022,2022-10-05T13:48:41Z,,,
arXIv2022,Grounding Language with Visual Affordances over Unstructured Data,Yes.,1,"""Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills.""",2022,2022-10-04T21:16:48Z,,,
arXIv2022,Explaining Patterns in Data with Language Models via Interpretable Autoprompting,Yes.,1,"""Large language models (LLMs) have displayed an impressive ability to harness natural language to perform complex tasks.""",2022,2022-10-04T18:32:14Z,,,
arXIv2022,When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment,Yes.,3,"""We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA.""",2022,2022-10-04T09:04:27Z,,,
arXIv2022,Less is More: Task-aware Layer-wise Distillation for Language Model Compression,Yes.,3,"""However, layer-wise distillation is difficult. Since the student has a smaller model capacity than the teacher, it is often under-fitted. Furthermore, the hidden representations of the teacher contain redundant information that the student does not necessarily need for the target task's learning.""",2022,2022-10-04T03:36:53Z,,,
arXIv2022,Robot Task Planning and Situation Handling in Open Worlds,Yes.,1,"""common sense is extracted from Large Language Models based on the current task at hand and robot skills.""",2022,2022-10-04T00:21:00Z,,,
arXIv2022,"Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",Yes.,3,"""using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment.""",2022,2022-10-03T21:38:29Z,,,
arXIv2022,The (In)Effectiveness of Intermediate Task Training For Domain Adaptation and Cross-Lingual Transfer Learning,Yes.,3,"""it remains an open question, if and when transfer learning will work, i.e. leading to positive or negative transfer.""",2022,2022-10-03T17:17:07Z,,,
arXIv2022,Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks,Yes.,3,"""Although large language models have achieved impressive zero-shot ability, the huge model size generally incurs high cost.""",2022,2022-10-01T04:08:50Z,,,
arXIv2022,Learning by Distilling Context,Yes.,3,"""However, they do not internalize these performance gains, which disappear when the context tokens are gone.""",2022,2022-09-30T02:30:15Z,,,
arXIv2022,Compositional Semantic Parsing with Large Language Models,Yes.,3,"""we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them.""",2022,2022-09-29T17:58:28Z,,,
arXIv2022,Repairing Bugs in Python Assignments Using Large Language Models,Yes.,1,"""We propose to use a large language model trained on code, such as Codex, to build an APR system -- MMAPR -- for introductory Python programming assignments.""",2022,2022-09-29T15:41:17Z,,,
arXIv2022,Downstream Datasets Make Surprisingly Good Pretraining Corpora,Yes.,1,"""For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets.""",2022,2022-09-28T19:28:43Z,,,
arXIv2022,Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer,Yes.,1,"""The paper explores the relevance of the Text-To-Text Transfer Transformer language model (T5) for Polish (plT5) to the task of intrinsic and extrinsic keyword extraction from short text passages.""",2022,2022-09-28T11:31:43Z,,,
arXIv2022,Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors,Yes.,3,"""However, such systems all succumb to the same problem",2022,2022-09-27T00:44:41Z,,,
arXIv2022,News Summarization and Evaluation in the Era of GPT-3,Yes.,3,"""Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries.""",2022,2022-09-26T01:04:52Z,,,
arXIv2022,Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity,Yes.,3,"""Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases.""",2022,2022-09-24T23:55:53Z,,,
arXIv2022,Augmenting Interpretable Models with LLMs during Training,Yes.,3,"""However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency.""",2022,2022-09-23T18:36:01Z,,,
arXIv2022,Promptagator: Few-shot Dense Retrieval From 8 Examples,Yes.,1,"""we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator.""",2022,2022-09-23T17:59:06Z,,,
arXIv2022,ProgPrompt: Generating Situated Robot Task Plans using Large Language Models,Yes.,3,"""However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context.""",2022,2022-09-22T20:29:49Z,,,
arXIv2022,Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation,Yes.,2,"""Our method works under the constraints of 1) a black-box (non-modifiable) question generation model and 2) lack of access to human-annotated references -- both of which are realistic limitations for real-world deployment of LLMs.""",2022,2022-09-22T13:33:48Z,,,
arXIv2022,Representing Affect Information in Word Embeddings,Yes.,3,"""Our analyses show that word embedding from the vanilla BERT model did not saliently encode the affect information of English words.""",2022,2022-09-21T18:16:33Z,,,
arXIv2022,Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers,Yes.,1,"""Current applications often use large transformer-based language models to classify input texts.""",2022,2022-09-21T17:05:12Z,,,
arXIv2022,T5QL: Taming language models for SQL generation,Yes.,3,"""Current SOTA methods for semantic parsing depend on LLMs to achieve high predictive accuracy on benchmark datasets. This reduces their applicability, since LLMs requires expensive GPUs. Furthermore, SOTA methods are ungrounded and thus not guaranteed to always generate valid SQL.""",2022,2022-09-21T10:43:13Z,,,
arXIv2022,Generate rather than Retrieve: Large Language Models are Strong Context Generators,Yes.,1,"""In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators.""",2022,2022-09-21T01:30:59Z,,,
arXIv2022,Open-vocabulary Queryable Scene Representations for Real World Planning,Yes.,3,"""However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene.""",2022,2022-09-20T17:29:56Z,,,
arXIv2022,Relaxed Attention for Transformer Models,Yes.,3,"""The powerful modeling capabilities of all-attention-based transformer architectures often cause overfitting and - for natural language processing tasks - lead to an implicitly learned internal language model in the autoregressive transformer decoder complicating the integration of external language models.""",2022,2022-09-20T14:10:28Z,,,
arXIv2022,EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics,Yes.,1,"""Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs.""",2022,2022-09-20T10:12:07Z,,,
arXIv2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,Yes.,1,"""This process is normally a black box in the case of deep learning models like large-scale language models.""",2022,2022-09-20T07:04:24Z,,,
arXIv2022,Enabling Conversational Interaction with Mobile UI using Large Language Models,Yes.,1,"""Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming.""",2022,2022-09-18T20:58:39Z,,,
arXIv2022,CodeQueries: A Dataset of Semantic Queries over Code,Yes.,3,"""We study a large language model (GPT3.5-Turbo) in zero-shot and few-shot settings on a subset of CodeQueries. We also evaluate a BERT style model (CuBERT) with fine-tuning. We find that these models achieve limited success on CodeQueries.""",2022,2022-09-17T17:09:30Z,,,
arXIv2022,Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models,Yes.,3,"""large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models.""",2022,2022-09-16T19:23:13Z,,,
arXIv2022,"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",Yes.,3,"""Despite impressive results across various tasks, the reasons behind their success have not been explored."" and ""brings into question the conventional wisdom around few-shot prompting.""",2022,2022-09-16T02:54:00Z,,,
arXIv2022,Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach,Yes.,3,"""Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances.""",2022,2022-09-15T01:51:22Z,,,
arXIv2022,Automated Fidelity Assessment for Strategy Training in Inpatient Rehabilitation using Natural Language Processing,No.,1,"The abstract focuses on using NLP techniques and specific models like BERT, LSTM, and a rule-based algorithm for fidelity assessment in rehabilitation sessions. It does not discuss LLMs or their limitations.",2022,2022-09-14T15:33:30Z,,,
arXIv2022,DECK: Behavioral Tests to Improve Interpretability and Generalizability of BERT Models Detecting Depression from Text,Yes.,3,"""However, these models are known to suffer from performance inconsistencies and poor generalization.""",2022,2022-09-12T14:39:46Z,,,
arXIv2022,T-NER: An All-Round Python Library for Transformer-based Named Entity Recognition,Yes.,3,"""cross-domain generalization is challenging even with a large pretrained LM, which has nevertheless capacity to learn domain-specific features if fine-tuned on a combined dataset.""",2022,2022-09-09T15:00:38Z,,,
arXIv2022,AILAB-Udine@SMM4H 22: Limits of Transformers and BERT Ensembles,Yes.,2,"""We explored the limits of Transformer based models on text classification, entity extraction and entity normalization.""",2022,2022-09-07T20:17:15Z,,,
arXIv2022,Every picture tells a story: Image-grounded controllable stylistic story generation,Yes.,1,"""we introduce Plug-and-Play Story Teller (PPST) and improve image-to-story generation by",2022,2022-09-04T15:07:53Z,,,
arXIv2022,Do Large Language Models know what humans know?,Yes.,3,"""the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime.""",2022,2022-09-04T01:29:53Z,,,
arXIv2022,Neural Approaches to Multilingual Information Retrieval,Yes.,1,"""This paper investigates whether advances in neural document translation and pretrained multilingual neural language models enable improvements in the state of the art over earlier MLIR techniques.""",2022,2022-09-03T06:02:52Z,,,
arXIv2022,Petals: Collaborative Inference and Fine-tuning of Large Models,Yes.,3,"""However, these techniques have innate limitations",2022,2022-09-02T17:38:03Z,,,
arXIv2022,Generating Coherent Drum Accompaniment With Fills And Improvisations,No.,1,The abstract discusses the use of transformer models for music generation but does not mention LLMs or their limitations.,2022,2022-09-01T08:31:26Z,,,
arXIv2022,Enhancing Semantic Understanding with Self-supervised Methods for Abstractive Dialogue Summarization,Yes.,2,"""We introduce self-supervised methods to compensate shortcomings to train a dialogue summarization model.""",2022,2022-09-01T07:51:46Z,,,
arXIv2022,Faithful Reasoning Using Large Language Models,Yes.,3,"""Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step.""",2022,2022-08-30T13:44:41Z,,,
arXIv2022,LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems,Yes.,3,"""state-of-the-art language models still struggle evaluating precise statements consistently"" and ""CLIP is not able to rerank those generated samples consistently.""",2022,2022-08-29T11:40:36Z,,,
arXIv2022,Target Speaker Voice Activity Detection with Transformers and Its Integration with End-to-End Neural Diarization,No.,1,The abstract does not mention LLMs or their limitations.,2022,2022-08-27T21:11:45Z,,,
arXIv2022,Repair Is Nearly Generation: Multilingual Program Repair with LLMs,Yes.,1,"""We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex.""",2022,2022-08-24T16:25:58Z,,,
arXIv2022,DPTDR: Deep Prompt Tuning for Dense Passage Retrieval,Yes.,1,"""applying DPT in dense retrieval largely underperforms FT methods.""",2022,2022-08-24T12:55:00Z,,,
arXIv2022,Evaluate Confidence Instead of Perplexity for Zero-shot Commonsense Reasoning,Yes.,3,"""Current pre-trained language model (PLM)-based reasoning follows the traditional practice using perplexity metric. However, commonsense reasoning is more than existing probability evaluation, which is biased by word frequency.""",2022,2022-08-23T14:42:14Z,,,
arXIv2022,GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization,Yes.,3,"""However, in these methods, there remain limitations in the way they capture and integrate the global semantic information.""",2022,2022-08-21T23:09:29Z,,,
arXIv2022,VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media,Yes.,3,"""ViLT, importantly, enables efficient training and inference in VL tasks, achieved by encoding images using a linear projection of patches instead of an object detector. However, it is pretrained on captioning datasets, where the language input is simple, literal, and descriptive, therefore",2022,2022-08-18T18:51:13Z,,,
arXIv2022,MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation,Yes.,1,"""The typical zero-shot approaches in dialogue generation rely heavily on large-scale pre-trained language generation models such as GPT-3 and T5.""",2022,2022-08-18T04:28:20Z,,,
arXIv2022,Ask Question First for Enhancing Lifelong Language Learning,No.,1,"The abstract discusses lifelong language learning, NLP tasks, and catastrophic forgetting, but does not mention large language models (LLMs) or their limitations.",2022,2022-08-17T15:58:33Z,,,
arXIv2022,HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models,Yes.,3,"""Controlling the text generated by language models and customizing the content has been a long-standing challenge."" and ""Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further",2022,2022-08-17T11:20:41Z,,,
arXIv2022,Transformer Encoder for Social Science,Yes.,1,"""We have witnessed the success of pretrained deep neural network models, such as BERT and RoBERTa, in recent social science research.""",2022,2022-08-17T01:01:25Z,,,
arXIv2022,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,Yes.,2,"""Large language models have been widely adopted but require significant GPU memory for inference.""",2022,2022-08-15T17:08:50Z,,,
arXIv2022,Targeted Honeyword Generation with Language Models,Yes.,1,"""we propose to build a more secure and trustworthy authentication system that employs off-the-shelf pre-trained language models which require no further training on real passwords to produce honeywords.""",2022,2022-08-15T00:06:29Z,,,
arXIv2022,New drugs and stock market: how to predict pharma market reaction to clinical trial announcements,No.,1,The abstract does not mention language models (LLMs or LMs) at all.,2022,2022-08-11T20:20:21Z,,,
arXIv2022,Reducing Retraining by Recycling Parameter-Efficient Prompts,Yes.,3,"""these learned prompts are tightly coupled to a given frozen model -- if the model is updated, corresponding new prompts need to be obtained.""",2022,2022-08-10T22:10:53Z,,,
arXIv2022,Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models,Yes.,1,"""We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs.""",2022,2022-08-05T17:46:38Z,,,
arXIv2022,Atlas: Few-shot Learning with Retrieval Augmented Language Models,Yes.,3,"""However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed.""",2022,2022-08-05T17:39:22Z,,,
arXIv2022,Debiasing Gender Bias in Information Retrieval Models,No.,1,The abstract does not mention LLMs or their limitations.,2022,2022-08-02T21:12:05Z,,,
arXIv2022,Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions,Yes.,3,"""Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do influence the predictions of PLMs, suggesting that such models rely on shallow heuristics.""",2022,2022-07-28T17:36:24Z,,,
arXIv2022,HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative,Yes.,1,"""HelixFold-Single is proposed to combine a large-scale protein language model with the superior geometric learning capability of AlphaFold2.""",2022,2022-07-28T07:30:33Z,,,
arXIv2022,Robots Enact Malignant Stereotypes,No.,1,"The abstract does not mention LLMs or any specific language models. It focuses on ML bias in robotics, particularly with CLIP-powered methods.",2022,2022-07-23T18:08:12Z,,,
arXIv2022,Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices,Yes.,3,"""However, the DP-noise introduced to the model increases as the model size grows, which often prevents convergence.""",2022,2022-07-18T23:53:17Z,,,
arXIv2022,Can large language models reason about medical questions?,Yes.,3,"""it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge.""",2022,2022-07-17T11:24:44Z,,,
arXIv2022,Automatic Context Pattern Generation for Entity Set Expansion,Yes.,1,"""we devise a context pattern generation module that utilizes autoregressive language models (e.g., GPT-2) to automatically generate high-quality context patterns for entities.""",2022,2022-07-17T06:50:35Z,,,
arXIv2022,Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model,Yes.,3,"""Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations",2022,2022-07-16T13:02:54Z,,,
arXIv2022,Confident Adaptive Language Modeling,Yes.,2,"""These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time.""",2022,2022-07-14T17:00:19Z,,,
arXIv2022,Language models show human-like content effects on reasoning tasks,Yes.,3,"""Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections.""",2022,2022-07-14T16:51:09Z,,,
arXIv2022,Neural Data-to-Text Generation Based on Small Datasets: Comparing the Added Value of Two Semi-Supervised Learning Approaches on Top of a Large Language Model,Yes.,1,"""This study discusses the effect of semi-supervised learning in combination with pretrained language models for data-to-text generation.""",2022,2022-07-14T11:53:04Z,,,
arXIv2022,BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling,Yes.,2,"""The pre-training of large language models usually requires massive amounts of resources, both in terms of computation and data.""",2022,2022-07-14T10:48:42Z,,,
arXIv2022,DocPrompting: Generating Code by Retrieving the Docs,No.,1,The abstract does not mention LLMs or any specific limitations related to language models.,2022,2022-07-13T06:47:51Z,,,
arXIv2022,Few-shot training LLMs for project-specific code-summarization,Yes.,1,"""Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code.""",2022,2022-07-09T09:57:11Z,,,
arXIv2022,Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation,Yes.,2,"""Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance.""",2022,2022-07-07T18:00:22Z,,,
arXIv2022,Machine Learning Model Sizes and the Parameter Gap,Yes.,3,"""We also identify that, since 2020, there have been many language models below 20B parameters, many models above 70B parameters, but a scarcity of models in the 20-70B parameter range. We refer to that scarcity as the parameter gap.""",2022,2022-07-05T20:55:38Z,,,
arXIv2022,FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales,Yes.,3,"""free-text rationales aim to use natural language to explain neural language model (LM) behavior"" and ""free-text rationales' unconstrained nature makes them prone to hallucination"" and ""we discuss the limitations of using RLC to evaluate free-text rationales.""",2022,2022-07-02T09:25:29Z,,,
arXIv2022,Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset,Yes.,3,"""One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information.""",2022,2022-07-01T06:25:15Z,,,
arXIv2022,When Does Differentially Private Learning Not Suffer in High Dimensions?,Yes.,1,"""Large pretrained models can be privately fine-tuned to achieve performance approaching that of non-private models.""",2022,2022-07-01T02:36:51Z,,,
arXIv2022,Logic Mill -- A Knowledge Navigation System,No.,1,"""No evidence""",2022,2022-12-31T13:46:50Z,,,
arXIv2022,Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?,No.,1,"""No evidence""",2022,2022-12-31T11:50:32Z,,,
arXIv2022,Towards Proactively Forecasting Sentence-Specific Information Popularity within Online News Documents,No.,1,"""No evidence""",2022,2022-12-31T08:40:08Z,,,
arXIv2022,Broad Learning System with Takagi-Sugeno Fuzzy Subsystem for Tobacco Origin Identification based on Near Infrared Spectroscopy,No.,1,"""No evidence""",2022,2022-12-31T05:38:37Z,,,
arXIv2022,Memory Augmented Lookup Dictionary based Language Modeling for Automatic Speech Recognition,No.,1,"""No evidence""",2022,2022-12-30T22:26:57Z,,,
arXIv2022,ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports,No.,1,"""No evidence""",2022,2022-12-30T18:55:16Z,,,
arXIv2022,An Analysis of Attention via the Lens of Exchangeability and Latent Variable Models,No.,1,"""No evidence""",2022,2022-12-30T17:59:01Z,,,
arXIv2022,Distant Reading of the German Coalition Deal: Recognizing Policy Positions with BERT-based Text Classification,No.,1,"""No evidence""",2022,2022-12-30T12:20:39Z,,,
arXIv2022,How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,No.,1,"""No evidence""",2022,2022-12-30T05:03:15Z,,,
arXIv2022,GPT Takes the Bar Exam,No.,1,"""No evidence""",2022,2022-12-29T18:19:43Z,,,
arXIv2022,Maximizing Use-Case Specificity through Precision Model Tuning,No.,1,"""No evidence""",2022,2022-12-29T07:50:14Z,,,
arXIv2022,Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP,No.,1,"""No evidence""",2022,2022-12-28T18:52:44Z,,,
arXIv2022,Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers,No.,1,"""No evidence""",2022,2022-12-26T16:06:04Z,,,
arXIv2022,Off-Policy Reinforcement Learning with Loss Function Weighted by Temporal Difference Error,No.,1,"""No evidence""",2022,2022-12-26T14:32:16Z,,,
arXIv2022,Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment,No.,1,"""No evidence""",2022,2022-12-26T08:12:41Z,,,
arXIv2022,Benchmark for Uncertainty & Robustness in Self-Supervised Learning,No.,1,"""No evidence""",2022,2022-12-23T15:46:23Z,,,
arXIv2022,Enhancing the prediction of disease outcomes using electronic health records and pretrained deep learning models,No.,1,"""No evidence""",2022,2022-12-22T22:53:32Z,,,
arXIv2022,OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization,No.,1,"""No evidence""",2022,2022-12-22T19:56:09Z,,,
arXIv2022,Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise,No.,1,"""No evidence""",2022,2022-12-22T13:17:11Z,,,
arXIv2022,Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning,No.,1,"""No evidence""",2022,2022-12-21T20:43:46Z,,,
arXIv2022,Critic-Guided Decoding for Controlled Text Generation,No.,1,"""No evidence""",2022,2022-12-21T11:25:41Z,,,
arXIv2022,SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning,No.,1,"""No evidence""",2022,2022-12-21T11:18:09Z,,,
arXIv2022,Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?,No.,1,"""No evidence""",2022,2022-12-21T09:44:08Z,,,
arXIv2022,SERENGETI: Massively Multilingual Language Models for Africa,No.,1,"""No evidence""",2022,2022-12-21T05:54:14Z,,,
arXIv2022,Towards Efficient Visual Simplification of Computational Graphs in Deep Neural Networks,No.,1,"""No evidence""",2022,2022-12-21T05:17:13Z,,,
arXIv2022,ImPaKT: A Dataset for Open-Schema Knowledge Base Construction,No.,1,"""No evidence""",2022,2022-12-21T05:02:49Z,,,
arXIv2022,JASMINE: Arabic GPT Models for Few-Shot Learning,No.,1,"""No evidence""",2022,2022-12-21T04:21:46Z,,,
arXIv2022,Spoken Language Understanding for Conversational AI: Recent Advances and Future Direction,No.,1,"""No evidence""",2022,2022-12-21T02:47:52Z,,,
arXIv2022,Zero-shot Triplet Extraction by Template Infilling,No.,1,"""No evidence""",2022,2022-12-21T00:57:24Z,,,
arXIv2022,KronA: Parameter Efficient Tuning with Kronecker Adapter,No.,1,"""No evidence""",2022,2022-12-20T20:56:52Z,,,
arXIv2022,A Vision-free Baseline for Multimodal Grammar Induction,No.,1,"""No evidence""",2022,2022-12-20T18:59:50Z,,,
arXIv2022,Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers,No.,1,"""No evidence""",2022,2022-12-20T18:58:48Z,,,
arXIv2022,PairReranker: Pairwise Reranking for Natural Language Generation,No.,1,"""No evidence""",2022,2022-12-20T18:56:57Z,,,
arXIv2022,Pretraining Without Attention,No.,1,"""No evidence""",2022,2022-12-20T18:50:08Z,,,
arXIv2022,"Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?",No.,1,"""No evidence""",2022,2022-12-20T18:47:13Z,,,
arXIv2022,Does CLIP Bind Concepts? Probing Compositionality in Large Image Models,No.,1,"""No evidence""",2022,2022-12-20T18:46:28Z,,,
arXIv2022,Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End,No.,1,"""No evidence""",2022,2022-12-20T18:37:11Z,,,
arXIv2022,A Measure-Theoretic Characterization of Tight Language Models,No.,1,"""No evidence""",2022,2022-12-20T18:17:11Z,,,
arXIv2022,Precise Zero-Shot Dense Retrieval without Relevance Labels,No.,1,"""No evidence""",2022,2022-12-20T18:09:52Z,,,
arXIv2022,Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models,No.,1,"""No evidence""",2022,2022-12-20T17:42:16Z,,,
arXIv2022,Controllable Text Generation with Language Constraints,No.,1,"""No evidence""",2022,2022-12-20T17:39:21Z,,,
arXIv2022,SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization,No.,1,"""No evidence""",2022,2022-12-20T17:38:47Z,,,
arXIv2022,Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models,No.,1,"""No evidence""",2022,2022-12-20T17:36:49Z,,,
arXIv2022,Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis,No.,1,"""No evidence""",2022,2022-12-20T15:40:17Z,,,
arXIv2022,Identifying and Manipulating the Personality Traits of Language Models,No.,1,"""No evidence""",2022,2022-12-20T14:24:11Z,,,
arXIv2022,ReCode: Robustness Evaluation of Code Generation Models,No.,1,"""No evidence""",2022,2022-12-20T14:11:31Z,,,
arXIv2022,In and Out-of-Domain Text Adversarial Robustness via Label Smoothing,No.,1,"""No evidence""",2022,2022-12-20T14:06:50Z,,,
arXIv2022,Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study,No.,1,"""No evidence""",2022,2022-12-20T13:20:21Z,,,
arXIv2022,Pay Attention to Your Tone: Introducing a New Dataset for Polite Language Rewrite,No.,1,"""No evidence""",2022,2022-12-20T12:02:34Z,,,
arXIv2022,Toward Human-Like Evaluation for Natural Language Generation with Error Analysis,No.,1,"""No evidence""",2022,2022-12-20T11:36:22Z,,,
arXIv2022,Human-Guided Fair Classification for Natural Language Processing,No.,1,"""No evidence""",2022,2022-12-20T10:46:40Z,,,
arXIv2022,Hybrid Rule-Neural Coreference Resolution System based on Actor-Critic Learning,No.,1,"""No evidence""",2022,2022-12-20T08:55:47Z,,,
arXIv2022,Large Language Models Are Reasoning Teachers,No.,1,"""No evidence""",2022,2022-12-20T08:24:45Z,,,
arXIv2022,A Twitter BERT Approach for Offensive Language Detection in Marathi,No.,1,"""No evidence""",2022,2022-12-20T07:22:45Z,,,
arXIv2022,DocAsRef: An Empirical Study on Repurposing Reference-Based Summary Quality Metrics Reference-Freely,No.,1,"""No evidence""",2022,2022-12-20T06:01:13Z,,,
arXIv2022,PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English,No.,1,"""No evidence""",2022,2022-12-20T05:58:32Z,,,
arXIv2022,Are Deep Neural Networks SMARTer than Second Graders?,No.,1,"""No evidence""",2022,2022-12-20T04:33:32Z,,,
arXIv2022,AnyTOD: A Programmable Task-Oriented Dialog System,No.,1,"""No evidence""",2022,2022-12-20T01:23:01Z,,,
arXIv2022,Plug & Play Directed Evolution of Proteins with Gradient-based Discrete MCMC,No.,1,"""No evidence""",2022,2022-12-20T00:26:23Z,,,
arXIv2022,Improved Long-Form Spoken Language Translation with Large Language Models,No.,1,"""No evidence""",2022,2022-12-19T22:36:53Z,,,
arXIv2022,MANTIS at TSAR-2022 Shared Task: Improved Unsupervised Lexical Simplification with Pretrained Encoders,No.,1,"""No evidence""",2022,2022-12-19T20:57:45Z,,,
arXIv2022,(Psycho-)Linguistic Features Meet Transformer Models for Improved Explainable and Controllable Text Simplification,No.,1,"""No evidence""",2022,2022-12-19T20:46:21Z,,,
arXIv2022,Exploring Hybrid and Ensemble Models for Multiclass Prediction of Mental Health Status on Social Media,No.,1,"""No evidence""",2022,2022-12-19T20:31:47Z,,,
arXIv2022,Evaluating Human-Language Model Interaction,No.,1,"""No evidence""",2022,2022-12-19T18:59:45Z,,,
arXIv2022,LENS: A Learnable Evaluation Metric for Text Simplification,No.,1,"""No evidence""",2022,2022-12-19T18:56:52Z,,,
arXIv2022,"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",No.,1,"""No evidence""",2022,2022-12-19T18:55:21Z,,,
arXIv2022,MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource Languages,No.,1,"""No evidence""",2022,2022-12-19T18:49:50Z,,,
arXIv2022,Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor,No.,1,"""No evidence""",2022,2022-12-19T18:21:00Z,,,
arXIv2022,MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,No.,1,"""No evidence""",2022,2022-12-19T17:44:54Z,,,
arXIv2022,Optimizing Prompts for Text-to-Image Generation,No.,1,"""No evidence""",2022,2022-12-19T16:50:41Z,,,
arXIv2022,Reasoning with Language Model Prompting: A Survey,No.,1,"""No evidence""",2022,2022-12-19T16:32:42Z,,,
arXIv2022,Unsupervised Summarization Re-ranking,No.,1,"""No evidence""",2022,2022-12-19T16:29:26Z,,,
arXIv2022,BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting,No.,1,"""No evidence""",2022,2022-12-19T15:24:45Z,,,
arXIv2022,Large Language Models Meet NL2Code: A Survey,No.,1,"""No evidence""",2022,2022-12-19T12:55:32Z,,,
arXIv2022,Less is More: Parameter-Free Text Classification with Gzip,No.,1,"""No evidence""",2022,2022-12-19T12:40:18Z,,,
arXIv2022,Enriching Relation Extraction with OpenIE,No.,1,"""No evidence""",2022,2022-12-19T11:26:23Z,,,
arXIv2022,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,No.,1,"""No evidence""",2022,2022-12-19T07:40:02Z,,,
arXIv2022,PromptBoosting: Black-Box Text Classification with Ten Forward Passes,No.,1,"""No evidence""",2022,2022-12-19T06:04:54Z,,,
arXIv2022,Natural Language to Code Generation in Interactive Data Science Notebooks,No.,1,"""No evidence""",2022,2022-12-19T05:06:00Z,,,
arXIv2022,I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation,No.,1,"""No evidence""",2022,2022-12-19T04:47:49Z,,,
arXIv2022,Emergent Analogical Reasoning in Large Language Models,No.,1,"""No evidence""",2022,2022-12-19T00:04:56Z,,,
arXIv2022,Chatbots in a Botnet World,No.,1,"""No evidence""",2022,2022-12-18T16:08:40Z,,,
arXIv2022,Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,No.,1,"""No evidence""",2022,2022-12-18T14:36:07Z,,,
arXIv2022,Neural Coreference Resolution based on Reinforcement Learning,No.,1,"""No evidence""",2022,2022-12-18T07:36:35Z,,,
arXIv2022,Sentence-level Feedback Generation for English Language Learners: Does Data Augmentation Help?,No.,1,"""No evidence""",2022,2022-12-18T03:53:44Z,,,
arXIv2022,Enhancing Cyber Resilience of Networked Microgrids using Vertical Federated Reinforcement Learning,No.,1,"""No evidence""",2022,2022-12-17T22:56:02Z,,,
arXIv2022,Claim Optimization in Computational Argumentation,No.,1,"""No evidence""",2022,2022-12-17T16:30:27Z,,,
arXIv2022,Exploiting Rich Textual User-Product Context for Improving Sentiment Analysis,No.,1,"""No evidence""",2022,2022-12-17T14:57:52Z,,,
arXIv2022,Point-E: A System for Generating 3D Point Clouds from Complex Prompts,No.,1,"""No evidence""",2022,2022-12-16T23:22:59Z,,,
arXIv2022,Self-Prompting Large Language Models for Zero-Shot Open-Domain QA,No.,1,"""No evidence""",2022,2022-12-16T18:23:43Z,,,
arXIv2022,Enhancing Multi-modal and Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation,No.,1,"""No evidence""",2022,2022-12-16T18:12:04Z,,,
arXIv2022,POIBERT: A Transformer-based Model for the Tour Recommendation Problem,No.,1,"""No evidence""",2022,2022-12-16T12:32:15Z,,,
arXIv2022,FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks,No.,1,"""No evidence""",2022,2022-12-16T09:01:56Z,,,
arXIv2022,ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks,No.,1,"""No evidence""",2022,2022-12-16T07:48:02Z,,,
arXIv2022,Investigation of Japanese PnG BERT language model in text-to-speech synthesis for pitch accent language,No.,1,"""No evidence""",2022,2022-12-16T07:47:03Z,,,
arXIv2022,ALERT: Adapting Language Models to Reasoning Tasks,No.,1,"""No evidence""",2022,2022-12-16T05:15:41Z,,,
arXIv2022,Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection,No.,1,"""No evidence""",2022,2022-12-15T19:49:27Z,,,
arXIv2022,Joint processing of linguistic properties in brains and language models,No.,1,"""No evidence""",2022,2022-12-15T19:13:42Z,,,
arXIv2022,ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning,No.,1,"""No evidence""",2022,2022-12-15T15:52:39Z,,,
arXIv2022,The Effects of In-domain Corpus Size on pre-training BERT,No.,1,"""No evidence""",2022,2022-12-15T15:49:27Z,,,
arXIv2022,MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers,No.,1,"""No evidence""",2022,2022-12-15T13:57:07Z,,,
arXIv2022,Enhancing Indic Handwritten Text Recognition Using Global Semantic Information,No.,1,"""No evidence""",2022,2022-12-15T12:53:26Z,,,
arXIv2022,Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking,No.,1,"""No evidence""",2022,2022-12-15T05:01:59Z,,,
arXIv2022,Robust Policy Optimization in Deep Reinforcement Learning,No.,1,"""No evidence""",2022,2022-12-14T22:43:56Z,,,
arXIv2022,MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling,No.,1,"""No evidence""",2022,2022-12-14T15:33:44Z,,,
arXIv2022,Multi-task Learning for Cross-Lingual Sentiment Analysis,No.,1,"""No evidence""",2022,2022-12-14T11:29:03Z,,,
arXIv2022,Reproducible scaling laws for contrastive language-image learning,No.,1,"""No evidence""",2022,2022-12-14T10:24:50Z,,,
arXIv2022,Explainability of Text Processing and Retrieval Methods: A Critical Survey,No.,1,"""No evidence""",2022,2022-12-14T09:25:49Z,,,
arXIv2022,Cross-Modal Similarity-Based Curriculum Learning for Image Captioning,No.,1,"""No evidence""",2022,2022-12-14T07:52:36Z,,,
arXIv2022,Paraphrase Identification with Deep Learning: A Review of Datasets and Methods,No.,1,"""No evidence""",2022,2022-12-13T23:06:20Z,,,
arXIv2022,Deep Image Style Transfer from Freeform Text,No.,1,"""No evidence""",2022,2022-12-13T19:24:08Z,,,
arXIv2022,CREPE: Can Vision-Language Foundation Models Reason Compositionally?,No.,1,"""No evidence""",2022,2022-12-13T19:17:36Z,,,
arXIv2022,Foresight -- Generative Pretrained Transformer (GPT) for Modelling of Patient Timelines using EHRs,No.,1,"""No evidence""",2022,2022-12-13T19:06:00Z,,,
arXIv2022,What do Vision Transformers Learn? A Visual Exploration,No.,1,"""No evidence""",2022,2022-12-13T16:55:12Z,,,
arXIv2022,"Structured Prompting: Scaling In-Context Learning to 1,000 Examples",No.,1,"""No evidence""",2022,2022-12-13T16:31:21Z,,,
arXIv2022,Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?,No.,1,"""No evidence""",2022,2022-12-13T15:28:57Z,,,
arXIv2022,The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique,No.,1,"""No evidence""",2022,2022-12-13T12:42:12Z,,,
arXIv2022,Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model,No.,1,"""No evidence""",2022,2022-12-13T04:57:04Z,,,
arXIv2022,Effective Seed-Guided Topic Discovery by Integrating Multiple Types of Contexts,No.,1,"""No evidence""",2022,2022-12-12T16:03:38Z,,,
arXIv2022,"DexBERT: Effective, Task-Agnostic and Fine-grained Representation Learning of Android Bytecode",No.,1,"""No evidence""",2022,2022-12-12T15:32:31Z,,,
arXIv2022,MaNLP@SMM4H22: BERT for Classification of Twitter Posts,No.,1,"""No evidence""",2022,2022-12-12T14:43:46Z,,,
arXIv2022,"""I think this is the most disruptive technology"": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data",No.,1,"""No evidence""",2022,2022-12-12T12:41:24Z,,,
arXIv2022,"Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",No.,1,"""No evidence""",2022,2022-12-11T04:45:50Z,,,
arXIv2022,"Punctuation Restoration for Singaporean Spoken Languages: English, Malay, and Mandarin",No.,1,"""No evidence""",2022,2022-12-10T19:54:53Z,,,
arXIv2022,A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks,No.,1,"""No evidence""",2022,2022-12-10T09:18:43Z,,,
arXIv2022,REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory,No.,1,"""No evidence""",2022,2022-12-10T06:17:56Z,,,
arXIv2022,Thinking Fast and Slow in Large Language Models,No.,1,"""No evidence""",2022,2022-12-10T05:07:30Z,,,
arXIv2022,Artificial Text Detection with Multiple Training Strategies,No.,1,"""No evidence""",2022,2022-12-10T03:57:28Z,,,
arXIv2022,"ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding",No.,1,"""No evidence""",2022,2022-12-10T01:34:47Z,,,
arXIv2022,Incorporating Emotions into Health Mention Classification Task on Social Media,No.,1,"""No evidence""",2022,2022-12-09T18:38:41Z,,,
arXIv2022,The Turing Deception,No.,1,"""No evidence""",2022,2022-12-09T16:32:11Z,,,
arXIv2022,TRBLLmaker -- Transformer Reads Between Lyrics Lines maker,No.,1,"""No evidence""",2022,2022-12-09T15:27:36Z,,,
arXIv2022,CKG: Dynamic Representation Based on Context and Knowledge Graph,No.,1,"""No evidence""",2022,2022-12-09T15:17:35Z,,,
arXIv2022,Towards Better Long-range Time Series Forecasting using Generative Forecasting,No.,1,"""No evidence""",2022,2022-12-09T13:35:39Z,,,
arXIv2022,From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader,No.,1,"""No evidence""",2022,2022-12-09T10:21:56Z,,,
arXIv2022,Explain to me like I am five -- Sentence Simplification Using Transformers,No.,1,"""No evidence""",2022,2022-12-08T22:57:18Z,,,
arXIv2022,Structured Like a Language Model: Analysing AI as an Automated Subject,No.,1,"""No evidence""",2022,2022-12-08T21:58:43Z,,,
arXIv2022,SpeechLMScore: Evaluating speech generation using speech language model,No.,1,"""No evidence""",2022,2022-12-08T21:00:15Z,,,
arXIv2022,Implicit causality in GPT-2: a case study,No.,1,"""No evidence""",2022,2022-12-08T15:42:38Z,,,
arXIv2022,Model-based trajectory stitching for improved behavioural cloning and its applications,No.,1,"""No evidence""",2022,2022-12-08T14:18:04Z,,,
arXIv2022,DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset,No.,1,"""No evidence""",2022,2022-12-08T07:29:07Z,,,
arXIv2022,NP4G : Network Programming for Generalization,No.,1,"""No evidence""",2022,2022-12-08T06:18:44Z,,,
arXIv2022,Successive Prompting for Decomposing Complex Questions,No.,1,"""No evidence""",2022,2022-12-08T06:03:38Z,,,
arXIv2022,RainUNet for Super-Resolution Rain Movie Prediction under Spatio-temporal Shifts,No.,1,"""No evidence""",2022,2022-12-07T23:42:39Z,,,
arXIv2022,TweetDrought: A Deep-Learning Drought Impacts Recognizer based on Twitter Data,No.,1,"""No evidence""",2022,2022-12-07T23:21:36Z,,,
arXIv2022,Discovering Latent Knowledge in Language Models Without Supervision,No.,1,"""No evidence""",2022,2022-12-07T18:17:56Z,,,
arXIv2022,Robustness of Learning from Task Instructions,No.,1,"""No evidence""",2022,2022-12-07T17:54:59Z,,,
arXIv2022,Pre-Training With Scientific Text Improves Educational Question Generation,No.,1,"""No evidence""",2022,2022-12-07T17:17:58Z,,,
arXIv2022,Memorization of Named Entities in Fine-tuned BERT Models,No.,1,"""No evidence""",2022,2022-12-07T16:20:50Z,,,
arXIv2022,G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks,No.,1,"""No evidence""",2022,2022-12-07T13:07:24Z,,,
arXIv2022,DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing,No.,1,"""No evidence""",2022,2022-12-07T12:27:28Z,,,
arXIv2022,SimVTP: Simple Video Text Pre-training with Masked Autoencoders,No.,1,"""No evidence""",2022,2022-12-07T07:14:22Z,,,
arXIv2022,Towards using Few-Shot Prompt Learning for Automating Model Completion,No.,1,"""No evidence""",2022,2022-12-07T02:11:26Z,,,
arXIv2022,Contactless Oxygen Monitoring with Gated Transformer,No.,1,"""No evidence""",2022,2022-12-06T22:43:59Z,,,
arXIv2022,Counterfactual reasoning: Do language models need world knowledge for causal understanding?,No.,1,"""No evidence""",2022,2022-12-06T19:22:25Z,,,
arXIv2022,PDA: Prompt-driven Zero-shot Domain Adaptation,No.,1,"""No evidence""",2022,2022-12-06T18:59:58Z,,,
arXIv2022,ADIR: Adaptive Diffusion for Image Reconstruction,No.,1,"""No evidence""",2022,2022-12-06T18:39:58Z,,,
arXIv2022,Style transfer and classification in hebrew news items,No.,1,"""No evidence""",2022,2022-12-06T14:47:29Z,,,
arXIv2022,SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies,No.,1,"""No evidence""",2022,2022-12-06T14:23:38Z,,,
arXIv2022,M-VADER: A Model for Diffusion with Multimodal Context,No.,1,"""No evidence""",2022,2022-12-06T12:45:21Z,,,
arXIv2022,Modern French Poetry Generation with RoBERTa and GPT-2,No.,1,"""No evidence""",2022,2022-12-06T12:10:14Z,,,
arXIv2022,Adaptive Testing of Computer Vision Models,No.,1,"""No evidence""",2022,2022-12-06T05:52:31Z,,,
arXIv2022,INCLUSIFY: A benchmark and a model for gender-inclusive German,No.,1,"""No evidence""",2022,2022-12-05T19:37:48Z,,,
arXIv2022,In-context Examples Selection for Machine Translation,No.,1,"""No evidence""",2022,2022-12-05T17:25:15Z,,,
arXIv2022,Audio-Driven Co-Speech Gesture Video Generation,No.,1,"""No evidence""",2022,2022-12-05T15:28:22Z,,,
arXIv2022,I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification,No.,1,"""No evidence""",2022,2022-12-05T14:11:36Z,,,
arXIv2022,Automatic Generation of Factual News Headlines in Finnish,No.,1,"""No evidence""",2022,2022-12-05T11:12:14Z,,,
arXIv2022,Video Games as a Corpus: Sentiment Analysis using Fallout New Vegas Dialog,No.,1,"""No evidence""",2022,2022-12-05T11:09:05Z,,,
arXIv2022,Fast and accurate factorized neural transducer for text adaption of end-to-end speech recognition models,No.,1,"""No evidence""",2022,2022-12-05T02:52:21Z,,,
arXIv2022,Building Metadata Inference Using a Transducer Based Language Model,No.,1,"""No evidence""",2022,2022-12-05T00:37:59Z,,,
arXIv2022,Applying Multilingual Models to Question Answering (QA),No.,1,"""No evidence""",2022,2022-12-04T21:58:33Z,,,
arXIv2022,Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE,No.,1,"""No evidence""",2022,2022-12-04T15:36:18Z,,,
arXIv2022,"Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E",No.,1,"""No evidence""",2022,2022-12-04T14:54:13Z,,,
arXIv2022,MiLMo:Minority Multilingual Pre-trained Language Model,No.,1,"""No evidence""",2022,2022-12-04T09:28:17Z,,,
arXIv2022,KPT: Keyword-guided Pre-training for Grounded Dialog Generation,No.,1,"""No evidence""",2022,2022-12-04T04:05:01Z,,,
arXIv2022,PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models,No.,1,"""No evidence""",2022,2022-12-03T06:59:01Z,,,
arXIv2022,Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping,No.,1,"""No evidence""",2022,2022-12-03T05:20:15Z,,,
arXIv2022,iEnhancer-ELM: improve enhancer identification by extracting position-related multiscale contextual information based on enhancer language models,No.,1,"""No evidence""",2022,2022-12-03T00:50:51Z,,,
arXIv2022,Twitter Data Analysis: Izmir Earthquake Case,No.,1,"""No evidence""",2022,2022-12-02T21:30:34Z,,,
arXIv2022,Compound Tokens: Channel Fusion for Vision-Language Representation Learning,No.,1,"""No evidence""",2022,2022-12-02T21:09:52Z,,,
arXIv2022,An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws,No.,1,"""No evidence""",2022,2022-12-02T18:46:41Z,,,
arXIv2022,CT-DQN: Control-Tutored Deep Reinforcement Learning,No.,1,"""No evidence""",2022,2022-12-02T17:59:43Z,,,
arXIv2022,Legal Prompting: Teaching a Language Model to Think Like a Lawyer,No.,1,"""No evidence""",2022,2022-12-02T17:41:22Z,,,
arXIv2022,SumREN: Summarizing Reported Speech about Events in News,No.,1,"""No evidence""",2022,2022-12-02T12:51:39Z,,,
arXIv2022,SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition,No.,1,"""No evidence""",2022,2022-12-02T09:11:32Z,,,
arXIv2022,UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph,No.,1,"""No evidence""",2022,2022-12-02T04:08:09Z,,,
arXIv2022,Analogical Math Word Problems Solving with Enhanced Problem-Solution Association,No.,1,"""No evidence""",2022,2022-12-01T19:50:30Z,,,
arXIv2022,CliMedBERT: A Pre-trained Language Model for Climate and Health-related Text,No.,1,"""No evidence""",2022,2022-12-01T17:44:09Z,,,
arXIv2022,Adapted Multimodal BERT with Layer-wise Fusion for Sentiment Analysis,No.,1,"""No evidence""",2022,2022-12-01T17:31:42Z,,,
arXIv2022,Extensible Prompts for Language Models on Zero-shot Language Style Customization,No.,1,"""No evidence""",2022,2022-12-01T16:11:56Z,,,
arXIv2022,Language models and brain alignment: beyond word-level semantics and prediction,No.,1,"""No evidence""",2022,2022-12-01T15:48:51Z,,,
arXIv2022,A Commonsense-Infused Language-Agnostic Learning Framework for Enhancing Prediction of Political Polarity in Multilingual News Headlines,No.,1,"""No evidence""",2022,2022-12-01T06:07:01Z,,,
arXIv2022,Task-Specific Embeddings for Ante-Hoc Explainable Text Classification,No.,1,"""No evidence""",2022,2022-11-30T19:56:25Z,,,
arXIv2022,ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT,No.,1,"""No evidence""",2022,2022-11-30T17:50:35Z,,,
arXIv2022,Rationale-Guided Few-Shot Classification to Detect Abusive Language,No.,1,"""No evidence""",2022,2022-11-30T14:47:14Z,,,
arXIv2022,xTrimoABFold: De novo Antibody Structure Prediction without MSA,No.,1,"""No evidence""",2022,2022-11-30T09:26:08Z,,,
arXIv2022,KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning,No.,1,"""No evidence""",2022,2022-11-30T06:27:46Z,,,
arXIv2022,HEAT: Hardware-Efficient Automatic Tensor Decomposition for Transformer Compression,No.,1,"""No evidence""",2022,2022-11-30T05:31:45Z,,,
arXIv2022,Protein Language Models and Structure Prediction: Connection and Progression,No.,1,"""No evidence""",2022,2022-11-30T04:58:54Z,,,
arXIv2022,Findings of the WMT 2022 Shared Task on Translation Suggestion,No.,1,"""No evidence""",2022,2022-11-30T03:48:36Z,,,
arXIv2022,Coder Reviewer Reranking for Code Generation,No.,1,"""No evidence""",2022,2022-11-29T18:56:33Z,,,
arXIv2022,Better Transcription of UK Supreme Court Hearings,No.,1,"""No evidence""",2022,2022-11-29T17:02:00Z,,,
arXIv2022,Outfit Generation and Recommendation -- An Experimental Study,No.,1,"""No evidence""",2022,2022-11-29T16:36:00Z,,,
arXIv2022,Improving astroBERT using Semantic Textual Similarity,No.,1,"""No evidence""",2022,2022-11-29T16:15:32Z,,,
arXIv2022,Syntactic Substitutability as Unsupervised Dependency Syntax,No.,1,"""No evidence""",2022,2022-11-29T09:01:37Z,,,
arXIv2022,Diverse Multi-Answer Retrieval with Determinantal Point Processes,No.,1,"""No evidence""",2022,2022-11-29T08:54:05Z,,,
arXIv2022,UDE: A Unified Driving Engine for Human Motion Generation,No.,1,"""No evidence""",2022,2022-11-29T08:30:52Z,,,
arXIv2022,Prompted Opinion Summarization with GPT-3.5,No.,1,"""No evidence""",2022,2022-11-29T04:06:21Z,,,
arXIv2022,Composition based oxidation state prediction of materials using deep learning,No.,1,"""No evidence""",2022,2022-11-29T03:24:53Z,,,
arXIv2022,GPT-Neo for commonsense reasoning -- a theoretical and practical lens,No.,1,"""No evidence""",2022,2022-11-28T17:49:38Z,,,
arXIv2022,Hypernetworks for Zero-shot Transfer in Reinforcement Learning,No.,1,"""No evidence""",2022,2022-11-28T15:48:35Z,,,
arXIv2022,Tackling Visual Control via Multi-View Exploration Maximization,No.,1,"""No evidence""",2022,2022-11-28T11:29:56Z,,,
arXIv2022,Is it Required? Ranking the Skills Required for a Job-Title,No.,1,"""No evidence""",2022,2022-11-28T10:27:11Z,,,
arXIv2022,Revisiting Distance Metric Learning for Few-Shot Natural Language Classification,No.,1,"""No evidence""",2022,2022-11-28T10:19:31Z,,,
arXIv2022,Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive Analysis of Hebrew BERT Models and a New One to Outperform Them All,No.,1,"""No evidence""",2022,2022-11-28T10:17:35Z,,,
arXIv2022,Handling and extracting key entities from customer conversations using Speech recognition and Named Entity recognition,No.,1,"""No evidence""",2022,2022-11-28T06:41:29Z,,,
arXIv2022,DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models,No.,1,"""No evidence""",2022,2022-11-28T03:25:49Z,,,
arXIv2022,Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models,No.,1,"""No evidence""",2022,2022-11-27T21:43:45Z,,,
arXIv2022,Multi-Modal Few-Shot Temporal Action Detection,No.,1,"""No evidence""",2022,2022-11-27T18:13:05Z,,,
arXIv2022,Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5,No.,1,"""No evidence""",2022,2022-11-27T16:11:29Z,,,
arXIv2022,ESIE-BERT: Enriching Sub-words Information Explicitly with BERT for Joint Intent Classification and SlotFilling,No.,1,"""No evidence""",2022,2022-11-27T13:49:19Z,,,
arXIv2022,An Automatic SOAP Classification System Using Weakly Supervision And Transfer Learning,No.,1,"""No evidence""",2022,2022-11-26T10:58:18Z,,,
arXIv2022,SKDBERT: Compressing BERT via Stochastic Knowledge Distillation,No.,1,"""No evidence""",2022,2022-11-26T03:18:55Z,,,
arXIv2022,Finetuning BERT on Partially Annotated NER Corpora,No.,1,"""No evidence""",2022,2022-11-25T19:54:30Z,,,
arXIv2022,CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels,No.,1,"""No evidence""",2022,2022-11-25T09:41:57Z,,,
arXIv2022,The European AI Liability Directives -- Critique of a Half-Hearted Approach and Lessons for the Future,No.,1,"""No evidence""",2022,2022-11-25T09:08:11Z,,,
arXIv2022,Comparison Study Between Token Classification and Sequence Classification In Text Classification,No.,1,"""No evidence""",2022,2022-11-25T05:14:58Z,,,
arXIv2022,Using Selective Masking as a Bridge between Pre-training and Fine-tuning,No.,1,"""No evidence""",2022,2022-11-24T22:25:27Z,,,
arXIv2022,Question-type Identification for Academic Questions in Online Learning Platform,No.,1,"""No evidence""",2022,2022-11-24T17:28:29Z,,,
arXIv2022,Undesirable Biases in NLP: Addressing Challenges of Measurement,No.,1,"""No evidence""",2022,2022-11-24T16:53:18Z,,,
arXIv2022,InDEX: Indonesian Idiom and Expression Dataset for Cloze Test,No.,1,"""No evidence""",2022,2022-11-24T02:05:47Z,,,
arXIv2022,Tapping the Potential of Coherence and Syntactic Features in Neural Models for Automatic Essay Scoring,No.,1,"""No evidence""",2022,2022-11-24T02:00:03Z,,,
arXIv2022,SeedBERT: Recovering Annotator Rating Distributions from an Aggregated Label,No.,1,"""No evidence""",2022,2022-11-23T18:35:15Z,,,
arXIv2022,"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish",No.,1,"""No evidence""",2022,2022-11-23T16:51:09Z,,,
arXIv2022,Improving Visual-textual Sentiment Analysis by Fusing Expert Features,No.,1,"""No evidence""",2022,2022-11-23T14:40:51Z,,,
arXIv2022,Word-Level Representation From Bytes For Language Modeling,No.,1,"""No evidence""",2022,2022-11-23T03:11:13Z,,,
arXIv2022,HyperTuning: Toward Adapting Large Language Models without Back-propagation,No.,1,"""No evidence""",2022,2022-11-22T18:52:25Z,,,
arXIv2022,PromptTTS: Controllable Text-to-Speech with Text Descriptions,No.,1,"""No evidence""",2022,2022-11-22T10:58:38Z,,,
arXIv2022,Coreference Resolution through a seq2seq Transition-Based System,No.,1,"""No evidence""",2022,2022-11-22T10:17:50Z,,,
arXIv2022,Converge to the Truth: Factual Error Correction via Iterative Constrained Editing,No.,1,"""No evidence""",2022,2022-11-22T10:03:13Z,,,
arXIv2022,Visually Grounded Commonsense Knowledge Acquisition,No.,1,"""No evidence""",2022,2022-11-22T07:00:16Z,,,
arXIv2022,Knowledge Prompting for Few-shot Action Recognition,No.,1,"""No evidence""",2022,2022-11-22T06:05:17Z,,,
arXIv2022,TEMPERA: Test-Time Prompting via Reinforcement Learning,No.,1,"""No evidence""",2022,2022-11-21T22:38:20Z,,,
arXIv2022,PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning,No.,1,"""No evidence""",2022,2022-11-21T17:52:43Z,,,
arXIv2022,Model-based Trajectory Stitching for Improved Offline Reinforcement Learning,No.,1,"""No evidence""",2022,2022-11-21T16:00:39Z,,,
arXIv2022,ClipCrop: Conditioned Cropping Driven by Vision-Language Model,No.,1,"""No evidence""",2022,2022-11-21T14:27:07Z,,,
arXIv2022,L3Cube-HindBERT and DevBERT: Pre-Trained BERT Transformer models for Devanagari based Hindi and Marathi Languages,No.,1,"""No evidence""",2022,2022-11-21T13:02:52Z,,,
arXIv2022,AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model,No.,1,"""No evidence""",2022,2022-11-21T11:30:13Z,,,
arXIv2022,TCBERT: A Technical Report for Chinese Topic Classification BERT,No.,1,"""No evidence""",2022,2022-11-21T09:45:15Z,,,
arXIv2022,Task-Specific Data Augmentation and Inference Processing for VIPriors Instance Segmentation Challenge,No.,1,"""No evidence""",2022,2022-11-21T09:15:30Z,,,
arXIv2022,VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning,No.,1,"""No evidence""",2022,2022-11-21T09:10:10Z,,,
arXIv2022,Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification,No.,1,"""No evidence""",2022,2022-11-21T03:05:02Z,,,
arXIv2022,Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors,No.,1,"""No evidence""",2022,2022-11-20T17:18:22Z,,,
arXIv2022,Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders,No.,1,"""No evidence""",2022,2022-11-20T16:23:23Z,,,
arXIv2022,Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrieval,No.,1,"""No evidence""",2022,2022-11-20T14:37:53Z,,,
arXIv2022,Feature Weaken: Vicinal Data Augmentation for Classification,No.,1,"""No evidence""",2022,2022-11-20T11:00:23Z,,,
arXIv2022,Detecting Conspiracy Theory Against COVID-19 Vaccines,No.,1,"""No evidence""",2022,2022-11-20T04:59:33Z,,,
arXIv2022,Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure,No.,1,"""No evidence""",2022,2022-11-19T16:30:29Z,,,
arXIv2022,Entity-Assisted Language Models for Identifying Check-worthy Sentences,No.,1,"""No evidence""",2022,2022-11-19T12:03:30Z,,,
arXIv2022,"ABINet++: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Spotting",No.,1,"""No evidence""",2022,2022-11-19T03:50:33Z,,,
arXIv2022,Knowledge Graph Generation From Text,No.,1,"""No evidence""",2022,2022-11-18T21:27:13Z,,,
arXIv2022,Knowledge Graph Refinement based on Triplet BERT-Networks,No.,1,"""No evidence""",2022,2022-11-18T19:01:21Z,,,
arXIv2022,Visual Programming: Compositional visual reasoning without training,No.,1,"""No evidence""",2022,2022-11-18T18:50:09Z,,,
arXIv2022,GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation,No.,1,"""No evidence""",2022,2022-11-18T16:39:45Z,,,
arXIv2022,Scaling Native Language Identification with Transformer Adapters,No.,1,"""No evidence""",2022,2022-11-18T09:40:16Z,,,
arXIv2022,Metadata Might Make Language Models Better,No.,1,"""No evidence""",2022,2022-11-18T08:29:00Z,,,
arXIv2022,3d human motion generation from the text via gesture action classification and the autoregressive model,No.,1,"""No evidence""",2022,2022-11-18T03:05:49Z,,,
arXIv2022,Protein language model rescue mutations highlight variant effects and structure in clinically relevant genes,No.,1,"""No evidence""",2022,2022-11-18T03:00:52Z,,,
arXIv2022,Towards Explaining Subjective Ground of Individuals on Social Media,No.,1,"""No evidence""",2022,2022-11-18T00:29:05Z,,,
arXIv2022,Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers,No.,1,"""No evidence""",2022,2022-11-17T23:14:58Z,,,
arXIv2022,ProtSi: Prototypical Siamese Network with Data Augmentation for Few-Shot Subjective Answer Evaluation,No.,1,"""No evidence""",2022,2022-11-17T19:33:35Z,,,
arXIv2022,Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks,No.,1,"""No evidence""",2022,2022-11-17T18:59:52Z,,,
arXIv2022,InstructPix2Pix: Learning to Follow Image Editing Instructions,No.,1,"""No evidence""",2022,2022-11-17T18:58:43Z,,,
arXIv2022,UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization,No.,1,"""No evidence""",2022,2022-11-17T18:54:47Z,,,
arXIv2022,Zero-Shot Dynamic Quantization for Transformer Inference,No.,1,"""No evidence""",2022,2022-11-17T18:09:07Z,,,
arXIv2022,LongFNT: Long-form Speech Recognition with Factorized Neural Transducer,No.,1,"""No evidence""",2022,2022-11-17T08:48:27Z,,,
arXIv2022,A Graph-Based Context-Aware Model to Understand Online Conversations,No.,1,"""No evidence""",2022,2022-11-16T20:51:45Z,,,
arXIv2022,Galactica: A Large Language Model for Science,No.,1,"""No evidence""",2022,2022-11-16T18:06:33Z,,,
arXIv2022,Technical Report on Neural Language Models and Few-Shot Learning for Systematic Requirements Processing in MDSE,No.,1,"""No evidence""",2022,2022-11-16T18:06:25Z,,,
arXIv2022,Towards Computationally Verifiable Semantic Grounding for Language Models,No.,1,"""No evidence""",2022,2022-11-16T17:35:52Z,,,
arXIv2022,TSMind: Alibaba and Soochow University's Submission to the WMT22 Translation Suggestion Task,No.,1,"""No evidence""",2022,2022-11-16T15:43:31Z,,,
arXIv2022,A Review of Intelligent Music Generation Systems,No.,1,"""No evidence""",2022,2022-11-16T13:43:16Z,,,
arXIv2022,L2 proficiency assessment using self-supervised speech representations,No.,1,"""No evidence""",2022,2022-11-16T11:47:20Z,,,
arXIv2022,Cognitive Simplification Operations Improve Text Simplification,No.,1,"""No evidence""",2022,2022-11-16T10:51:03Z,,,
arXIv2022,Streaming Joint Speech Recognition and Disfluency Detection,No.,1,"""No evidence""",2022,2022-11-16T07:34:20Z,,,
arXIv2022,ED-FAITH: Evaluating Dialogue Summarization on Faithfulness,No.,1,"""No evidence""",2022,2022-11-15T19:33:50Z,,,
arXIv2022,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,No.,1,"""No evidence""",2022,2022-11-15T18:26:26Z,,,
arXIv2022,An FNet based Auto Encoder for Long Sequence News Story Generation,No.,1,"""No evidence""",2022,2022-11-15T16:48:09Z,,,
arXIv2022,RobBERT-2022: Updating a Dutch Language Model to Account for Evolving Language Use,No.,1,"""No evidence""",2022,2022-11-15T14:55:53Z,,,
arXIv2022,FedTune: A Deep Dive into Efficient Federated Fine-Tuning with Pre-trained Transformers,No.,1,"""No evidence""",2022,2022-11-15T10:16:13Z,,,
arXIv2022,Relationship of the language distance to English ability of a country,No.,1,"""No evidence""",2022,2022-11-15T02:40:00Z,,,
arXIv2022,Towards a Mathematics Formalisation Assistant using Large Language Models,No.,1,"""No evidence""",2022,2022-11-14T16:52:32Z,,,
arXIv2022,AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot Domain Adaptation of KeyBERT,No.,1,"""No evidence""",2022,2022-11-14T16:29:03Z,,,
arXIv2022,Semantic Decomposition Improves Learning of Large Language Models on EHR Data,No.,1,"""No evidence""",2022,2022-11-14T14:59:16Z,,,
arXIv2022,Replacing Language Model for Style Transfer,No.,1,"""No evidence""",2022,2022-11-14T13:35:55Z,,,
arXIv2022,Hope Speech Detection on Social Media Platforms,No.,1,"""No evidence""",2022,2022-11-14T10:58:22Z,,,
arXIv2022,Grafting Pre-trained Models for Multimodal Headline Generation,No.,1,"""No evidence""",2022,2022-11-14T08:59:59Z,,,
arXIv2022,Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation,No.,1,"""No evidence""",2022,2022-11-14T03:40:02Z,,,
arXIv2022,Controllable Citation Sentence Generation with Language Models,No.,1,"""No evidence""",2022,2022-11-14T01:54:08Z,,,
arXIv2022,ALBERT with Knowledge Graph Encoder Utilizing Semantic Similarity for Commonsense Question Answering,No.,1,"""No evidence""",2022,2022-11-14T01:39:26Z,,,
arXIv2022,AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities,No.,1,"""No evidence""",2022,2022-11-12T14:48:55Z,,,
arXIv2022,Dark patterns in e-commerce: a dataset and its baseline evaluations,No.,1,"""No evidence""",2022,2022-11-12T01:53:49Z,,,
arXIv2022,The Architectural Bottleneck Principle,No.,1,"""No evidence""",2022,2022-11-11T18:58:08Z,,,
arXIv2022,Controlling Commercial Cooling Systems Using Reinforcement Learning,No.,1,"""No evidence""",2022,2022-11-11T17:48:13Z,,,
arXIv2022,Improving word mover's distance by leveraging self-attention matrix,No.,1,"""No evidence""",2022,2022-11-11T14:25:08Z,,,
arXIv2022,Towards automating Numerical Consistency Checks in Financial Reports,No.,1,"""No evidence""",2022,2022-11-11T10:35:07Z,,,
arXIv2022,pyRDDLGym: From RDDL to Gym Environments,No.,1,"""No evidence""",2022,2022-11-11T00:58:16Z,,,
arXIv2022,Steps towards prompt-based creation of virtual worlds,No.,1,"""No evidence""",2022,2022-11-10T21:13:04Z,,,
arXIv2022,Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control,No.,1,"""No evidence""",2022,2022-11-10T18:31:56Z,,,
arXIv2022,BERT in Plutarch's Shadows,No.,1,"""No evidence""",2022,2022-11-10T16:21:42Z,,,
arXIv2022,BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning,No.,1,"""No evidence""",2022,2022-11-10T14:37:23Z,,,
arXIv2022,Prompt Learning for Domain Adaptation in Task-Oriented Dialogue,No.,1,"""No evidence""",2022,2022-11-10T14:16:00Z,,,
arXIv2022,PAD-Net: An Efficient Framework for Dynamic Networks,No.,1,"""No evidence""",2022,2022-11-10T12:42:43Z,,,
arXIv2022,ADEPT: A DEbiasing PrompT Framework,No.,1,"""No evidence""",2022,2022-11-10T08:41:40Z,,,
arXIv2022,MSDT: Masked Language Model Scoring Defense in Text Domain,No.,1,"""No evidence""",2022,2022-11-10T06:46:47Z,,,
arXIv2022,LERT: A Linguistically-motivated Pre-trained Language Model,No.,1,"""No evidence""",2022,2022-11-10T05:09:16Z,,,
arXIv2022,On Optimizing the Communication of Model Parallelism,No.,1,"""No evidence""",2022,2022-11-10T03:56:48Z,,,
arXIv2022,FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information,No.,1,"""No evidence""",2022,2022-11-10T01:32:55Z,,,
arXIv2022,BERT-Based Combination of Convolutional and Recurrent Neural Network for Indonesian Sentiment Analysis,No.,1,"""No evidence""",2022,2022-11-10T00:32:40Z,,,
arXIv2022,Training self-supervised peptide sequence models on artificially chopped proteins,No.,1,"""No evidence""",2022,2022-11-09T22:22:17Z,,,
arXIv2022,Collateral facilitation in humans and language models,No.,1,"""No evidence""",2022,2022-11-09T21:08:08Z,,,
arXIv2022,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,No.,1,"""No evidence""",2022,2022-11-09T18:48:09Z,,,
arXIv2022,Cross-lingual Transfer Learning for Check-worthy Claim Identification over Twitter,No.,1,"""No evidence""",2022,2022-11-09T18:18:53Z,,,
arXIv2022,Understanding Cross-modal Interactions in V&L Models that Generate Scene Descriptions,No.,1,"""No evidence""",2022,2022-11-09T15:33:51Z,,,
arXIv2022,"Sentiment Analysis of Persian Language: Review of Algorithms, Approaches and Datasets",No.,1,"""No evidence""",2022,2022-11-09T13:08:46Z,,,
arXIv2022,Improving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition,No.,1,"""No evidence""",2022,2022-11-09T07:23:15Z,,,
arXIv2022,Adaptive Multi-Corpora Language Model Training for Speech Recognition,No.,1,"""No evidence""",2022,2022-11-09T06:54:50Z,,,
arXIv2022,FF2: A Feature Fusion Two-Stream Framework for Punctuation Restoration,No.,1,"""No evidence""",2022,2022-11-09T06:18:17Z,,,
arXIv2022,Classification of Colorectal Cancer Polyps via Transfer Learning and Vision-Based Tactile Sensing,No.,1,"""No evidence""",2022,2022-11-08T21:47:36Z,,,
arXIv2022,A Multimodal Approach for Dementia Detection from Spontaneous Speech with Tensor Fusion Layer,No.,1,"""No evidence""",2022,2022-11-08T16:43:58Z,,,
arXIv2022,An Ensemble-based approach for assigning text to correct Harmonized system code,No.,1,"""No evidence""",2022,2022-11-08T15:32:36Z,,,
arXIv2022,Third-Party Aligner for Neural Word Alignments,No.,1,"""No evidence""",2022,2022-11-08T12:30:08Z,,,
arXIv2022,ABC: Adversarial Behavioral Cloning for Offline Mode-Seeking Imitation Learning,No.,1,"""No evidence""",2022,2022-11-08T04:54:54Z,,,
arXIv2022,Parameter and Data Efficient Continual Pre-training for Robustness to Dialectal Variance in Arabic,No.,1,"""No evidence""",2022,2022-11-08T02:51:57Z,,,
arXIv2022,Facial Tic Detection in Untrimmed Videos of Tourette Syndrome Patients,No.,1,"""No evidence""",2022,2022-11-07T22:59:58Z,,,
arXIv2022,AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis,No.,1,"""No evidence""",2022,2022-11-07T19:44:42Z,,,
arXIv2022,"Astronomia ex machina: a history, primer, and outlook on neural networks in astronomy",No.,1,"""No evidence""",2022,2022-11-07T19:00:00Z,,,
arXIv2022,Learning Semantic Textual Similarity via Topic-informed Discrete Latent Variables,No.,1,"""No evidence""",2022,2022-11-07T15:09:58Z,,,
arXIv2022,A Targeted Sampling Strategy for Compressive Cryo Focused Ion Beam Scanning Electron Microscopy,No.,1,"""No evidence""",2022,2022-11-07T12:27:28Z,,,
arXIv2022,Generative Transformers for Design Concept Generation,No.,1,"""No evidence""",2022,2022-11-07T11:29:10Z,,,
arXIv2022,Probing neural language models for understanding of words of estimative probability,No.,1,"""No evidence""",2022,2022-11-07T08:29:11Z,,,
arXIv2022,AD-BERT: Using Pre-trained contextualized embeddings to Predict the Progression from Mild Cognitive Impairment to Alzheimer's Disease,No.,1,"""No evidence""",2022,2022-11-07T04:05:46Z,,,
arXIv2022,Complex Reading Comprehension Through Question Decomposition,No.,1,"""No evidence""",2022,2022-11-07T02:54:04Z,,,
arXIv2022,Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following,No.,1,"""No evidence""",2022,2022-11-07T02:24:39Z,,,
arXIv2022,Design Process is a Reinforcement Learning Problem,No.,1,"""No evidence""",2022,2022-11-06T14:37:22Z,,,
arXIv2022,Improved Target-specific Stance Detection on Social Media Platforms by Delving into Conversation Threads,No.,1,"""No evidence""",2022,2022-11-06T08:40:48Z,,,
arXIv2022,Suffix Retrieval-Augmented Language Modeling,No.,1,"""No evidence""",2022,2022-11-06T07:53:19Z,,,
arXIv2022,Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference,No.,1,"""No evidence""",2022,2022-11-05T20:34:08Z,,,
arXIv2022,BEKG: A Built Environment Knowledge Graph,No.,1,"""No evidence""",2022,2022-11-05T09:52:45Z,,,
arXIv2022,KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction,No.,1,"""No evidence""",2022,2022-11-04T20:38:12Z,,,
arXIv2022,Measuring Progress on Scalable Oversight for Large Language Models,No.,1,"""No evidence""",2022,2022-11-04T17:03:49Z,,,
arXIv2022,Multi-blank Transducers for Speech Recognition,No.,1,"""No evidence""",2022,2022-11-04T16:24:46Z,,,
arXIv2022,BERT for Long Documents: A Case Study of Automated ICD Coding,No.,1,"""No evidence""",2022,2022-11-04T15:24:19Z,,,
arXIv2022,BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets,No.,1,"""No evidence""",2022,2022-11-04T14:35:56Z,,,
arXIv2022,Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing,No.,1,"""No evidence""",2022,2022-11-04T14:20:40Z,,,
arXIv2022,OSIC: A New One-Stage Image Captioner Coined,No.,1,"""No evidence""",2022,2022-11-04T08:50:09Z,,,
arXIv2022,Probing Statistical Representations For End-To-End ASR,No.,1,"""No evidence""",2022,2022-11-03T17:08:14Z,,,
arXIv2022,Crosslingual Generalization through Multitask Finetuning,No.,1,"""No evidence""",2022,2022-11-03T13:19:32Z,,,
arXIv2022,Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively,No.,1,"""No evidence""",2022,2022-11-03T08:32:12Z,,,
arXIv2022,Using Large Pre-Trained Language Model to Assist FDA in Premarket Medical Device,No.,1,"""No evidence""",2022,2022-11-03T04:18:05Z,,,
arXIv2022,Open-Vocabulary Argument Role Prediction for Event Extraction,No.,1,"""No evidence""",2022,2022-11-03T04:13:37Z,,,
arXIv2022,Fine-Tuning Language Models via Epistemic Neural Networks,No.,1,"""No evidence""",2022,2022-11-03T03:24:46Z,,,
arXIv2022,Towards Zero-Shot Code-Switched Speech Recognition,No.,1,"""No evidence""",2022,2022-11-02T19:52:54Z,,,
arXIv2022,data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup,No.,1,"""No evidence""",2022,2022-11-02T16:29:59Z,,,
arXIv2022,Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model,No.,1,"""No evidence""",2022,2022-11-02T15:23:13Z,,,
arXIv2022,Transformer-based encoder-encoder architecture for Spoken Term Detection,No.,1,"""No evidence""",2022,2022-11-02T13:03:15Z,,,
arXIv2022,Internal Language Model Estimation based Adaptive Language Model Fusion for Domain Adaptation,No.,1,"""No evidence""",2022,2022-11-02T09:15:20Z,,,
arXIv2022,Numerical Optimizations for Weighted Low-rank Estimation on Language Model,No.,1,"""No evidence""",2022,2022-11-02T00:58:02Z,,,
arXIv2022,Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural Language Instructions,No.,1,"""No evidence""",2022,2022-11-01T18:30:42Z,,,
arXIv2022,"Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation",No.,1,"""No evidence""",2022,2022-11-01T18:16:00Z,,,
arXIv2022,Machine learning can guide experimental approaches for protein digestibility estimations,No.,1,"""No evidence""",2022,2022-11-01T17:43:58Z,,,
arXIv2022,Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small,No.,1,"""No evidence""",2022,2022-11-01T17:08:44Z,,,
arXIv2022,T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5,No.,1,"""No evidence""",2022,2022-11-01T17:00:23Z,,,
arXIv2022,VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding,No.,1,"""No evidence""",2022,2022-11-01T12:51:51Z,,,
arXIv2022,Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features,No.,1,"""No evidence""",2022,2022-11-01T09:18:50Z,,,
arXIv2022,Training Vision-Language Models with Less Bimodal Supervision,No.,1,"""No evidence""",2022,2022-11-01T04:07:11Z,,,
arXIv2022,WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained Language Model for Financial Domain,No.,1,"""No evidence""",2022,2022-10-31T18:35:18Z,,,
arXIv2022,Leveraging Pre-trained Models for Failure Analysis Triplets Generation,No.,1,"""No evidence""",2022,2022-10-31T17:21:15Z,,,
arXIv2022,Learning New Tasks from a Few Examples with Soft-Label Prototypes,No.,1,"""No evidence""",2022,2022-10-31T16:06:48Z,,,
arXIv2022,SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control,No.,1,"""No evidence""",2022,2022-10-31T16:02:00Z,,,
arXIv2022,Towards Zero-Shot and Few-Shot Table Question Answering using GPT-3,No.,1,"""No evidence""",2022,2022-10-31T13:08:55Z,,,
arXIv2022,When Language Model Meets Private Library,No.,1,"""No evidence""",2022,2022-10-31T11:42:06Z,,,
arXIv2022,SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking,No.,1,"""No evidence""",2022,2022-10-31T09:29:21Z,,,
arXIv2022,Improving Cause-of-Death Classification from Verbal Autopsy Reports,No.,1,"""No evidence""",2022,2022-10-31T09:14:08Z,,,
arXIv2022,1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via Beam-Search-based Position Selector,No.,1,"""No evidence""",2022,2022-10-31T09:09:59Z,,,
arXIv2022,Modular Hybrid Autoregressive Transducer,No.,1,"""No evidence""",2022,2022-10-31T03:56:37Z,,,
arXIv2022,Blank Collapse: Compressing CTC emission for the faster decoding,No.,1,"""No evidence""",2022,2022-10-31T02:12:51Z,,,
arXIv2022,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,No.,1,"""No evidence""",2022,2022-10-30T15:38:03Z,,,
arXIv2022,Parameter-Efficient Tuning Makes a Good Classification Head,No.,1,"""No evidence""",2022,2022-10-30T08:29:20Z,,,
arXIv2022,BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model,No.,1,"""No evidence""",2022,2022-10-29T18:19:44Z,,,
arXIv2022,Empirical Evaluation of Post-Training Quantization Methods for Language Tasks,No.,1,"""No evidence""",2022,2022-10-29T14:51:41Z,,,
arXIv2022,NTULM: Enriching Social Media Text Representations with Non-Textual Units,No.,1,"""No evidence""",2022,2022-10-29T12:18:04Z,,,
arXIv2022,Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection,No.,1,"""No evidence""",2022,2022-10-29T09:18:41Z,,,
arXIv2022,Differentiable Data Augmentation for Contrastive Sentence Representation Learning,No.,1,"""No evidence""",2022,2022-10-29T08:57:45Z,,,
arXIv2022,Aligning Offline Metrics and Human Judgments of Value for Code Generation Models,No.,1,"""No evidence""",2022,2022-10-29T05:03:28Z,,,
arXIv2022,Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models,No.,1,"""No evidence""",2022,2022-10-28T23:18:43Z,,,
arXIv2022,DiMBERT: Learning Vision-Language Grounded Representations with Disentangled Multimodal-Attention,No.,1,"""No evidence""",2022,2022-10-28T23:00:40Z,,,
arXIv2022,Feature Engineering vs BERT on Twitter Data,No.,1,"""No evidence""",2022,2022-10-28T14:43:13Z,,,
arXIv2022,Modeling structure-building in the brain with CCG parsing and large language models,No.,1,"""No evidence""",2022,2022-10-28T14:21:29Z,,,
arXIv2022,Zero-Shot Text Matching for Automated Auditing using Sentence Transformers,No.,1,"""No evidence""",2022,2022-10-28T11:52:16Z,,,
arXIv2022,UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance,No.,1,"""No evidence""",2022,2022-10-28T10:07:25Z,,,
arXIv2022,BEBERT: Efficient and Robust Binary Ensemble BERT,No.,1,"""No evidence""",2022,2022-10-28T08:15:26Z,,,
arXIv2022,RoChBert: Towards Robust BERT Fine-tuning for Chinese,No.,1,"""No evidence""",2022,2022-10-28T07:08:00Z,,,
arXIv2022,On the Use of Modality-Specific Large-Scale Pre-Trained Encoders for Multimodal Sentiment Analysis,No.,1,"""No evidence""",2022,2022-10-28T06:48:35Z,,,
arXIv2022,Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion,No.,1,"""No evidence""",2022,2022-10-28T02:27:18Z,,,
arXIv2022,Simulating realistic speech overlaps improves multi-talker ASR,No.,1,"""No evidence""",2022,2022-10-27T18:29:39Z,,,
arXIv2022,COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models,No.,1,"""No evidence""",2022,2022-10-27T15:06:40Z,,,
arXIv2022,FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational Speech Synthesis,No.,1,"""No evidence""",2022,2022-10-27T12:20:20Z,,,
arXIv2022,SAN: a robust end-to-end ASR model architecture,No.,1,"""No evidence""",2022,2022-10-27T09:36:25Z,,,
arXIv2022,Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model,No.,1,"""No evidence""",2022,2022-10-27T07:48:18Z,,,
arXIv2022,Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling,No.,1,"""No evidence""",2022,2022-10-27T07:38:50Z,,,
arXIv2022,BERT-Flow-VAE: A Weakly-supervised Model for Multi-Label Text Classification,No.,1,"""No evidence""",2022,2022-10-27T07:18:56Z,,,
arXIv2022,COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning,No.,1,"""No evidence""",2022,2022-10-27T06:51:39Z,,,
arXIv2022,Learning Joint Representation of Human Motion and Language,No.,1,"""No evidence""",2022,2022-10-27T05:32:20Z,,,
arXIv2022,Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language,No.,1,"""No evidence""",2022,2022-10-27T03:48:24Z,,,
arXIv2022,Open-vocabulary Semantic Segmentation with Frozen Vision-Language Models,No.,1,"""No evidence""",2022,2022-10-27T02:57:26Z,,,
arXIv2022,Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval,No.,1,"""No evidence""",2022,2022-10-27T02:43:48Z,,,
arXIv2022,Masked Vision-Language Transformer in Fashion,No.,1,"""No evidence""",2022,2022-10-27T01:44:08Z,,,
arXIv2022,TRScore: A Novel GPT-based Readability Scorer for ASR Segmentation and Punctuation model evaluation and selection,No.,1,"""No evidence""",2022,2022-10-27T01:11:32Z,,,
arXIv2022,The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs,No.,1,"""No evidence""",2022,2022-10-26T19:04:23Z,,,
arXIv2022,Causality Detection using Multiple Annotation Decisions,No.,1,"""No evidence""",2022,2022-10-26T16:50:10Z,,,
arXIv2022,"Don't Prompt, Search! Mining-based Zero-Shot Learning with Language Models",No.,1,"""No evidence""",2022,2022-10-26T15:52:30Z,,,
arXIv2022,Incorporating Pre-training Paradigm for Antibody Sequence-Structure Co-design,No.,1,"""No evidence""",2022,2022-10-26T15:31:36Z,,,
arXIv2022,A Case for Business Process-Specific Foundation Models,No.,1,"""No evidence""",2022,2022-10-26T14:17:47Z,,,
arXIv2022,"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?",No.,1,"""No evidence""",2022,2022-10-26T13:28:14Z,,,
arXIv2022,TSUP Speaker Diarization System for Conversational Short-phrase Speaker Diarization Challenge,No.,1,"""No evidence""",2022,2022-10-26T12:01:24Z,,,
arXIv2022,A Robust Bias Mitigation Procedure Based on the Stereotype Content Model,No.,1,"""No evidence""",2022,2022-10-26T08:13:58Z,,,
arXIv2022,Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning,No.,1,"""No evidence""",2022,2022-10-26T04:39:42Z,,,
arXIv2022,Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive Learning of Transformers and Prompts,No.,1,"""No evidence""",2022,2022-10-26T04:31:07Z,,,
arXIv2022,$N$-gram Is Back: Residual Learning of Neural Text Generation with $n$-gram Language Model,No.,1,"""No evidence""",2022,2022-10-26T02:42:53Z,,,
arXIv2022,Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe,No.,1,"""No evidence""",2022,2022-10-25T21:21:17Z,,,
arXIv2022,A single-cell gene expression language model,No.,1,"""No evidence""",2022,2022-10-25T20:52:19Z,,,
arXIv2022,Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models,No.,1,"""No evidence""",2022,2022-10-25T20:43:36Z,,,
arXIv2022,Learning Better Intent Representations for Financial Open Intent Classification,No.,1,"""No evidence""",2022,2022-10-25T20:01:13Z,,,
arXIv2022,Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios,No.,1,"""No evidence""",2022,2022-10-25T18:15:25Z,,,
arXIv2022,Contrastive Search Is What You Need For Neural Text Generation,No.,1,"""No evidence""",2022,2022-10-25T16:40:48Z,,,
arXIv2022,IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models,No.,1,"""No evidence""",2022,2022-10-25T16:25:00Z,,,
arXIv2022,Dual Mechanism Priming Effects in Hindi Word Order,No.,1,"""No evidence""",2022,2022-10-25T11:49:22Z,,,
arXIv2022,Cloning Ideology and Style using Deep Learning,No.,1,"""No evidence""",2022,2022-10-25T11:37:19Z,,,
arXIv2022,Differentially Private Language Models for Secure Data Sharing,No.,1,"""No evidence""",2022,2022-10-25T11:12:56Z,,,
arXIv2022,XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing,No.,1,"""No evidence""",2022,2022-10-25T01:33:49Z,,,
arXIv2022,Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing,No.,1,"""No evidence""",2022,2022-10-25T00:07:10Z,,,
arXIv2022,VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge,No.,1,"""No evidence""",2022,2022-10-24T22:01:17Z,,,
arXIv2022,Adapters for Enhanced Modeling of Multilingual Knowledge and Text,No.,1,"""No evidence""",2022,2022-10-24T21:33:42Z,,,
arXIv2022,Characterizing Verbatim Short-Term Memory in Neural Language Models,No.,1,"""No evidence""",2022,2022-10-24T19:47:56Z,,,
arXIv2022,Effective Pre-Training Objectives for Transformer-based Autoencoders,No.,1,"""No evidence""",2022,2022-10-24T18:39:44Z,,,
arXIv2022,Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models,No.,1,"""No evidence""",2022,2022-10-24T17:46:57Z,,,
arXIv2022,Perfectly Secure Steganography Using Minimum Entropy Coupling,No.,1,"""No evidence""",2022,2022-10-24T17:40:07Z,,,
arXIv2022,Explaining Translationese: why are Neural Classifiers Better and what do they Learn?,No.,1,"""No evidence""",2022,2022-10-24T16:43:28Z,,,
arXIv2022,Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,No.,1,"""No evidence""",2022,2022-10-24T16:29:55Z,,,
arXIv2022,ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation,No.,1,"""No evidence""",2022,2022-10-24T14:46:47Z,,,
arXIv2022,"The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",No.,1,"""No evidence""",2022,2022-10-24T13:01:24Z,,,
arXIv2022,An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks,No.,1,"""No evidence""",2022,2022-10-24T07:47:32Z,,,
arXIv2022,Abductive Action Inference,No.,1,"""No evidence""",2022,2022-10-24T07:43:59Z,,,
arXIv2022,Exploring Euphemism Detection in Few-Shot and Zero-Shot Settings,No.,1,"""No evidence""",2022,2022-10-24T02:43:43Z,,,
arXIv2022,Code4Struct: Code Generation for Few-Shot Event Structure Prediction,No.,1,"""No evidence""",2022,2022-10-23T18:18:51Z,,,
arXIv2022,Data Augmentation for Automated Essay Scoring using Transformer Models,No.,1,"""No evidence""",2022,2022-10-23T18:13:30Z,,,
arXIv2022,Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition,No.,1,"""No evidence""",2022,2022-10-23T16:27:31Z,,,
arXIv2022,Discriminative Language Model as Semantic Consistency Scorer for Prompt-based Few-Shot Text Classification,No.,1,"""No evidence""",2022,2022-10-23T16:10:48Z,,,
arXIv2022,A BERT-based Deep Learning Approach for Reputation Analysis in Social Media,No.,1,"""No evidence""",2022,2022-10-23T02:04:03Z,,,
arXIv2022,Language Model Pre-Training with Sparse Latent Typing,No.,1,"""No evidence""",2022,2022-10-23T00:37:08Z,,,
arXIv2022,Understanding Domain Learning in Language Models Through Subpopulation Analysis,No.,1,"""No evidence""",2022,2022-10-22T21:12:57Z,,,
arXIv2022,LMPriors: Pre-Trained Language Models as Task-Specific Priors,No.,1,"""No evidence""",2022,2022-10-22T19:09:18Z,,,
arXIv2022,Spectrum-BERT: Pre-training of Deep Bidirectional Transformers for Spectral Classification of Chinese Liquors,No.,1,"""No evidence""",2022,2022-10-22T13:11:25Z,,,
arXIv2022,Hindering Adversarial Attacks with Implicit Neural Representations,No.,1,"""No evidence""",2022,2022-10-22T13:10:24Z,,,
arXIv2022,Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and Reliable Language Model,No.,1,"""No evidence""",2022,2022-10-22T11:57:10Z,,,
arXIv2022,Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling,No.,1,"""No evidence""",2022,2022-10-22T07:16:19Z,,,
arXIv2022,NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation,No.,1,"""No evidence""",2022,2022-10-22T06:29:21Z,,,
arXIv2022,Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks,No.,1,"""No evidence""",2022,2022-10-22T05:22:29Z,,,
arXIv2022,P$^3$LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training,No.,1,"""No evidence""",2022,2022-10-22T03:50:59Z,,,
arXIv2022,PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding,No.,1,"""No evidence""",2022,2022-10-22T00:14:47Z,,,
arXIv2022,What do Large Language Models Learn beyond Language?,No.,1,"""No evidence""",2022,2022-10-21T23:43:13Z,,,
arXIv2022,"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",No.,1,"""No evidence""",2022,2022-10-21T22:37:22Z,,,
arXIv2022,SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation,No.,1,"""No evidence""",2022,2022-10-21T19:42:32Z,,,
arXIv2022,Probing with Noise: Unpicking the Warp and Weft of Embeddings,No.,1,"""No evidence""",2022,2022-10-21T19:33:33Z,,,
arXIv2022,"Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities",No.,1,"""No evidence""",2022,2022-10-21T18:30:56Z,,,
arXIv2022,Discovering Differences in the Representation of People using Contextualized Semantic Axes,No.,1,"""No evidence""",2022,2022-10-21T18:02:19Z,,,
arXIv2022,LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling,No.,1,"""No evidence""",2022,2022-10-21T13:03:49Z,,,
arXIv2022,A Semi-supervised Approach for a Better Translation of Sentiment in Dialectical Arabic UGT,No.,1,"""No evidence""",2022,2022-10-21T11:55:55Z,,,
arXIv2022,Multimodal Model with Text and Drug Embeddings for Adverse Drug Reaction Classification,No.,1,"""No evidence""",2022,2022-10-21T11:41:45Z,,,
arXIv2022,Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer,No.,1,"""No evidence""",2022,2022-10-21T11:26:59Z,,,
arXIv2022,Is Encoder-Decoder Redundant for Neural Machine Translation?,No.,1,"""No evidence""",2022,2022-10-21T08:33:55Z,,,
arXIv2022,InforMask: Unsupervised Informative Masking for Language Model Pretraining,No.,1,"""No evidence""",2022,2022-10-21T07:10:56Z,,,
arXIv2022,Dissecting Deep Metric Learning Losses for Image-Text Retrieval,No.,1,"""No evidence""",2022,2022-10-21T06:48:27Z,,,
arXIv2022,Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale,No.,1,"""No evidence""",2022,2022-10-21T02:37:58Z,,,
arXIv2022,SLING: Sino Linguistic Evaluation of Large Language Models,No.,1,"""No evidence""",2022,2022-10-21T02:29:39Z,,,
arXIv2022,3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows,No.,1,"""No evidence""",2022,2022-10-20T21:28:34Z,,,
arXIv2022,Composing Ensembles of Pre-trained Models via Iterative Consensus,No.,1,"""No evidence""",2022,2022-10-20T18:46:31Z,,,
arXIv2022,Transcending Scaling Laws with 0.1% Extra Compute,No.,1,"""No evidence""",2022,2022-10-20T16:46:41Z,,,
arXIv2022,Tele-Knowledge Pre-training for Fault Analysis,No.,1,"""No evidence""",2022,2022-10-20T14:31:48Z,,,
arXIv2022,Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble,No.,1,"""No evidence""",2022,2022-10-20T06:05:58Z,,,
arXIv2022,TabLLM: Few-shot Classification of Tabular Data with Large Language Models,No.,1,"""No evidence""",2022,2022-10-19T17:08:13Z,,,
arXIv2022,Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced African Languages,No.,1,"""No evidence""",2022,2022-10-19T16:12:27Z,,,
arXIv2022,Self-supervised Graph Masking Pre-training for Graph-to-Text Generation,No.,1,"""No evidence""",2022,2022-10-19T14:44:56Z,,,
arXIv2022,DIAMBRA Arena: a New Reinforcement Learning Platform for Research and Experimentation,No.,1,"""No evidence""",2022,2022-10-19T14:39:10Z,,,
arXIv2022,PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting,No.,1,"""No evidence""",2022,2022-10-19T13:30:39Z,,,
arXIv2022,Leveraging a New Spanish Corpus for Multilingual and Crosslingual Metaphor Detection,No.,1,"""No evidence""",2022,2022-10-19T07:55:36Z,,,
arXIv2022,BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining,No.,1,"""No evidence""",2022,2022-10-19T07:17:39Z,,,
arXIv2022,A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss,No.,1,"""No evidence""",2022,2022-10-19T05:33:27Z,,,
arXIv2022,Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning,No.,1,"""No evidence""",2022,2022-10-19T04:38:26Z,,,
arXIv2022,Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation,No.,1,"""No evidence""",2022,2022-10-19T04:31:08Z,,,
arXIv2022,Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models,No.,1,"""No evidence""",2022,2022-10-19T04:28:19Z,,,
arXIv2022,Continued Pretraining for Better Zero- and Few-Shot Promptability,No.,1,"""No evidence""",2022,2022-10-19T02:41:51Z,,,
arXIv2022,Tempo: Accelerating Transformer-Based Model Training through Memory Footprint Reduction,No.,1,"""No evidence""",2022,2022-10-19T01:59:37Z,,,
arXIv2022,Aligning MAGMA by Few-Shot Learning and Finetuning,No.,1,"""No evidence""",2022,2022-10-18T22:20:47Z,,,
arXIv2022,Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models,No.,1,"""No evidence""",2022,2022-10-18T22:19:41Z,,,
arXIv2022,JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions,No.,1,"""No evidence""",2022,2022-10-18T19:20:53Z,,,
arXIv2022,Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning,No.,1,"""No evidence""",2022,2022-10-18T17:58:43Z,,,
arXIv2022,The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks,No.,1,"""No evidence""",2022,2022-10-18T17:58:39Z,,,
arXIv2022,Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters,No.,1,"""No evidence""",2022,2022-10-18T15:20:44Z,,,
arXIv2022,Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis,No.,1,"""No evidence""",2022,2022-10-18T12:25:29Z,,,
arXIv2022,Alibaba-Translate China's Submission for WMT 2022 Quality Estimation Shared Task,No.,1,"""No evidence""",2022,2022-10-18T08:55:27Z,,,
arXIv2022,Alibaba-Translate China's Submission for WMT 2022 Metrics Shared Task,No.,1,"""No evidence""",2022,2022-10-18T08:51:25Z,,,
arXIv2022,Planning for Sample Efficient Imitation Learning,No.,1,"""No evidence""",2022,2022-10-18T05:19:26Z,,,
arXIv2022,Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation,No.,1,"""No evidence""",2022,2022-10-18T02:50:34Z,,,
arXIv2022,Team Flow at DRC2022: Pipeline System for Travel Destination Recommendation Task in Spoken Dialogue,No.,1,"""No evidence""",2022,2022-10-18T01:11:16Z,,,
arXIv2022,Systematicity in GPT-3's Interpretation of Novel English Noun Compounds,No.,1,"""No evidence""",2022,2022-10-18T00:25:24Z,,,
arXIv2022,Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints,No.,1,"""No evidence""",2022,2022-10-17T21:22:23Z,,,
arXIv2022,Sufficient Exploration for Convex Q-learning,No.,1,"""No evidence""",2022,2022-10-17T20:22:12Z,,,
arXIv2022,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,No.,1,"""No evidence""",2022,2022-10-17T17:08:26Z,,,
arXIv2022,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,No.,1,"""No evidence""",2022,2022-10-17T15:25:24Z,,,
arXIv2022,Continuous Pseudo-Labeling from the Start,No.,1,"""No evidence""",2022,2022-10-17T03:04:06Z,,,
arXIv2022,SGRAM: Improving Scene Graph Parsing via Abstract Meaning Representation,No.,1,"""No evidence""",2022,2022-10-17T00:37:00Z,,,
arXIv2022,NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly,No.,1,"""No evidence""",2022,2022-10-16T18:30:05Z,,,
arXIv2022,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,No.,1,"""No evidence""",2022,2022-10-16T13:36:57Z,,,
arXIv2022,Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion,No.,1,"""No evidence""",2022,2022-10-16T07:17:27Z,,,
arXIv2022,LAION-5B: An open large-scale dataset for training next generation image-text models,No.,1,"""No evidence""",2022,2022-10-16T00:08:18Z,,,
arXIv2022,Construction Repetition Reduces Information Rate in Dialogue,No.,1,"""No evidence""",2022,2022-10-15T15:44:00Z,,,
arXIv2022,AraLegal-BERT: A pretrained language model for Arabic Legal text,No.,1,"""No evidence""",2022,2022-10-15T13:08:40Z,,,
arXIv2022,Large Language Models for Multi-label Propaganda Detection,No.,1,"""No evidence""",2022,2022-10-15T06:47:31Z,,,
arXIv2022,TestAug: A Framework for Augmenting Capability-based NLP Tests,No.,1,"""No evidence""",2022,2022-10-14T20:42:16Z,,,
arXIv2022,PseudoReasoner: Leveraging Pseudo Labels for Commonsense Knowledge Base Population,No.,1,"""No evidence""",2022,2022-10-14T17:37:30Z,,,
arXIv2022,The Debate Over Understanding in AI's Large Language Models,No.,1,"""No evidence""",2022,2022-10-14T17:04:29Z,,,
arXIv2022,EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning,No.,1,"""No evidence""",2022,2022-10-14T13:26:41Z,,,
arXIv2022,Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning,No.,1,"""No evidence""",2022,2022-10-14T13:21:33Z,,,
arXIv2022,Extracting Cultural Commonsense Knowledge at Scale,No.,1,"""No evidence""",2022,2022-10-14T12:53:57Z,,,
arXIv2022,Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values,No.,1,"""No evidence""",2022,2022-10-14T09:10:49Z,,,
arXIv2022,Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding,No.,1,"""No evidence""",2022,2022-10-14T05:56:38Z,,,
arXIv2022,MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks,No.,1,"""No evidence""",2022,2022-10-14T03:34:09Z,,,
arXIv2022,Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods,No.,1,"""No evidence""",2022,2022-10-13T19:46:14Z,,,
arXIv2022,Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning,No.,1,"""No evidence""",2022,2022-10-13T19:30:43Z,,,
arXIv2022,Language Model Decoding as Likelihood-Utility Alignment,No.,1,"""No evidence""",2022,2022-10-13T17:55:51Z,,,
arXIv2022,Visual Classification via Description from Large Language Models,No.,1,"""No evidence""",2022,2022-10-13T17:03:46Z,,,
arXIv2022,SQuAT: Sharpness- and Quantization-Aware Training for BERT,No.,1,"""No evidence""",2022,2022-10-13T16:52:19Z,,,
arXIv2022,Language Models of Code are Few-Shot Commonsense Learners,No.,1,"""No evidence""",2022,2022-10-13T16:09:36Z,,,
arXIv2022,Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence,No.,1,"""No evidence""",2022,2022-10-13T15:43:39Z,,,
arXIv2022,Query Expansion Using Contextual Clue Sampling with Language Models,No.,1,"""No evidence""",2022,2022-10-13T15:18:04Z,,,
arXIv2022,Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation,No.,1,"""No evidence""",2022,2022-10-13T14:25:08Z,,,
arXIv2022,Tone prediction and orthographic conversion for Basaa,No.,1,"""No evidence""",2022,2022-10-13T12:58:39Z,,,
arXIv2022,Multilingual Zero Resource Speech Recognition Base on Self-Supervise Pre-Trained Acoustic Models,No.,1,"""No evidence""",2022,2022-10-13T12:11:18Z,,,
arXIv2022,ImaginaryNet: Learning Object Detectors without Real Images and Annotations,No.,1,"""No evidence""",2022,2022-10-13T10:25:22Z,,,
arXIv2022,Benchmarking Long-tail Generalization with Likelihood Splits,No.,1,"""No evidence""",2022,2022-10-13T07:27:14Z,,,
arXIv2022,Re3: Generating Longer Stories With Recursive Reprompting and Revision,No.,1,"""No evidence""",2022,2022-10-13T06:29:57Z,,,
arXIv2022,Policy Gradient With Serial Markov Chain Reasoning,No.,1,"""No evidence""",2022,2022-10-13T06:15:29Z,,,
arXIv2022,Explanations from Large Language Models Make Small Reasoners Better,No.,1,"""No evidence""",2022,2022-10-13T04:50:02Z,,,
arXIv2022,Large Language Models are few(1)-shot Table Reasoners,No.,1,"""No evidence""",2022,2022-10-13T04:08:24Z,,,
arXIv2022,Jointly Reinforced User Simulator and Task-oriented Dialog System with Simplified Generative Architecture,No.,1,"""No evidence""",2022,2022-10-13T03:57:17Z,,,
arXIv2022,Overlooked Video Classification in Weakly Supervised Video Anomaly Detection,No.,1,"""No evidence""",2022,2022-10-13T03:00:22Z,,,
arXIv2022,Neuro-symbolic Explainable Artificial Intelligence Twin for Zero-touch IoE in Wireless Network,No.,1,"""No evidence""",2022,2022-10-13T01:08:06Z,,,
arXIv2022,The COVID That Wasn't: Counterfactual Journalism Using GPT,No.,1,"""No evidence""",2022,2022-10-13T00:50:24Z,,,
arXIv2022,RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses,No.,1,"""No evidence""",2022,2022-10-12T20:51:49Z,,,
arXIv2022,DATScore: Evaluating Translation with Data Augmented Translations,No.,1,"""No evidence""",2022,2022-10-12T20:31:42Z,,,
arXIv2022,Developing a general-purpose clinical language inference model from a large corpus of clinical notes,No.,1,"""No evidence""",2022,2022-10-12T20:08:45Z,,,
arXIv2022,Subword Segmental Language Modelling for Nguni Languages,No.,1,"""No evidence""",2022,2022-10-12T18:41:00Z,,,
arXIv2022,Predictive Querying for Autoregressive Neural Sequence Models,No.,1,"""No evidence""",2022,2022-10-12T17:59:36Z,,,
arXIv2022,MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers,No.,1,"""No evidence""",2022,2022-10-12T17:23:21Z,,,
arXIv2022,Foundation Transformers,No.,1,"""No evidence""",2022,2022-10-12T17:16:27Z,,,
arXIv2022,On Text Style Transfer via Style Masked Language Models,No.,1,"""No evidence""",2022,2022-10-12T16:44:06Z,,,
arXIv2022,GMP*: Well-Tuned Gradual Magnitude Pruning Can Outperform Most BERT-Pruning Methods,No.,1,"""No evidence""",2022,2022-10-12T16:35:47Z,,,
arXIv2022,Probing Commonsense Knowledge in Pre-trained Language Models with Sense-level Precision and Expanded Vocabulary,No.,1,"""No evidence""",2022,2022-10-12T16:26:59Z,,,
arXIv2022,Context Generation Improves Open Domain Question Answering,No.,1,"""No evidence""",2022,2022-10-12T16:00:50Z,,,
arXIv2022,SQuId: Measuring Speech Naturalness in Many Languages,No.,1,"""No evidence""",2022,2022-10-12T15:43:09Z,,,
arXIv2022,Zero-Shot On-the-Fly Event Schema Induction,No.,1,"""No evidence""",2022,2022-10-12T14:37:00Z,,,
arXIv2022,Improved Data Augmentation for Translation Suggestion,No.,1,"""No evidence""",2022,2022-10-12T12:46:43Z,,,
arXIv2022,Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning,No.,1,"""No evidence""",2022,2022-10-12T03:33:49Z,,,
arXIv2022,MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score,No.,1,"""No evidence""",2022,2022-10-12T02:27:32Z,,,
arXIv2022,CLIP also Understands Text: Prompting CLIP for Phrase Understanding,No.,1,"""No evidence""",2022,2022-10-11T23:35:18Z,,,
arXIv2022,Cross-Lingual Speaker Identification Using Distant Supervision,No.,1,"""No evidence""",2022,2022-10-11T20:49:44Z,,,
arXIv2022,Visual Language Maps for Robot Navigation,No.,1,"""No evidence""",2022,2022-10-11T18:13:20Z,,,
arXIv2022,CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory,No.,1,"""No evidence""",2022,2022-10-11T17:57:10Z,,,
arXIv2022,A Kernel-Based View of Language Model Fine-Tuning,No.,1,"""No evidence""",2022,2022-10-11T17:34:32Z,,,
arXIv2022,Continual Training of Language Models for Few-Shot Learning,No.,1,"""No evidence""",2022,2022-10-11T15:43:58Z,,,
arXIv2022,Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration,No.,1,"""No evidence""",2022,2022-10-11T14:58:58Z,,,
arXIv2022,Like a bilingual baby: The advantage of visually grounding a bilingual language model,No.,1,"""No evidence""",2022,2022-10-11T14:43:26Z,,,
arXIv2022,Instance Regularization for Discriminative Language Model Pre-training,No.,1,"""No evidence""",2022,2022-10-11T14:16:37Z,,,
arXIv2022,Word Sense Induction with Hierarchical Clustering and Mutual Information Maximization,No.,1,"""No evidence""",2022,2022-10-11T13:04:06Z,,,
arXIv2022,Mind's Eye: Grounded Language Model Reasoning through Simulation,No.,1,"""No evidence""",2022,2022-10-11T11:39:23Z,,,
arXIv2022,On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding,No.,1,"""No evidence""",2022,2022-10-11T09:40:34Z,,,
arXIv2022,Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training,No.,1,"""No evidence""",2022,2022-10-11T09:34:21Z,,,
arXIv2022,Rethinking the Event Coding Pipeline with Prompt Entailment,No.,1,"""No evidence""",2022,2022-10-11T08:38:48Z,,,
arXIv2022,Retrieval Augmentation for T5 Re-ranker using External Sources,No.,1,"""No evidence""",2022,2022-10-11T04:54:19Z,,,
arXIv2022,HUE: Pretrained Model and Dataset for Understanding Hanja Documents of Ancient Korea,No.,1,"""No evidence""",2022,2022-10-11T03:04:28Z,,,
arXIv2022,Pre-Training Representations of Binary Code Using Contrastive Learning,No.,1,"""No evidence""",2022,2022-10-11T02:39:06Z,,,
arXIv2022,Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems,No.,1,"""No evidence""",2022,2022-10-11T00:57:19Z,,,
arXIv2022,Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling,No.,1,"""No evidence""",2022,2022-10-10T23:15:17Z,,,
arXIv2022,CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,No.,1,"""No evidence""",2022,2022-10-10T17:45:38Z,,,
arXIv2022,Transformer-based Localization from Embodied Dialog with Large-scale Pre-training,No.,1,"""No evidence""",2022,2022-10-10T17:25:06Z,,,
arXIv2022,Long N-step Surrogate Stage Reward to Reduce Variances of Deep Reinforcement Learning in Complex Problems,No.,1,"""No evidence""",2022,2022-10-10T16:32:10Z,,,
arXIv2022,Bridging CLIP and StyleGAN through Latent Alignment for Image Editing,No.,1,"""No evidence""",2022,2022-10-10T09:17:35Z,,,
arXIv2022,Parameter-Efficient Tuning with Special Token Adaptation,No.,1,"""No evidence""",2022,2022-10-10T01:02:51Z,,,
arXIv2022,ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models,No.,1,"""No evidence""",2022,2022-10-09T19:17:43Z,,,
arXIv2022,QAScore -- An Unsupervised Unreferenced Metric for the Question Generation Evaluation,No.,1,"""No evidence""",2022,2022-10-09T19:00:39Z,,,
arXIv2022,Better Pre-Training by Reducing Representation Confusion,No.,1,"""No evidence""",2022,2022-10-09T12:35:04Z,,,
arXIv2022,Fine-Tuning Pre-trained Transformers into Decaying Fast Weights,No.,1,"""No evidence""",2022,2022-10-09T12:27:25Z,,,
arXIv2022,Controllable Dialogue Simulation with In-Context Learning,No.,1,"""No evidence""",2022,2022-10-09T06:32:58Z,,,
arXIv2022,On Task-Adaptive Pretraining for Dialogue Response Selection,No.,1,"""No evidence""",2022,2022-10-08T17:58:49Z,,,
arXIv2022,InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings,No.,1,"""No evidence""",2022,2022-10-08T15:53:19Z,,,
arXIv2022,KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification,No.,1,"""No evidence""",2022,2022-10-08T08:37:44Z,,,
arXIv2022,Learning Fine-Grained Visual Understanding for Video Question Answering via Decoupling Spatial-Temporal Modeling,No.,1,"""No evidence""",2022,2022-10-08T07:03:31Z,,,
arXIv2022,Data-Efficiency with a Single GPU: An Exploration of Transfer Methods for Small Language Models,No.,1,"""No evidence""",2022,2022-10-08T01:45:22Z,,,
arXIv2022,Breaking BERT: Evaluating and Optimizing Sparsified Attention,No.,1,"""No evidence""",2022,2022-10-07T22:32:27Z,,,
arXIv2022,Large Language Models can Implement Policy Iteration,No.,1,"""No evidence""",2022,2022-10-07T21:18:22Z,,,
arXIv2022,Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts,No.,1,"""No evidence""",2022,2022-10-07T19:58:47Z,,,
arXIv2022,Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts,No.,1,"""No evidence""",2022,2022-10-07T16:51:45Z,,,
arXIv2022,Novice Type Error Diagnosis with Natural Language Models,No.,1,"""No evidence""",2022,2022-10-07T16:40:53Z,,,
arXIv2022,Reinforcement Learning Approach for Multi-Agent Flexible Scheduling Problems,No.,1,"""No evidence""",2022,2022-10-07T16:31:01Z,,,
arXIv2022,DABERT: Dual Attention Enhanced BERT for Semantic Matching,No.,1,"""No evidence""",2022,2022-10-07T10:54:49Z,,,
arXIv2022,UU-Tax at SemEval-2022 Task 3: Improving the generalizability of language models for taxonomy classification through data augmentation,No.,1,"""No evidence""",2022,2022-10-07T07:41:28Z,,,
arXIv2022,Elastic Step DQN: A novel multi-step algorithm to alleviate overestimation in Deep QNetworks,No.,1,"""No evidence""",2022,2022-10-07T04:56:04Z,,,
arXIv2022,HealthE: Classifying Entities in Online Textual Health Advice,No.,1,"""No evidence""",2022,2022-10-06T23:18:24Z,,,
arXIv2022,Improving Large-scale Paraphrase Acquisition and Generation,No.,1,"""No evidence""",2022,2022-10-06T22:00:56Z,,,
arXIv2022,PQLM -- Multilingual Decentralized Portable Quantum Language Model for Privacy Protection,No.,1,"""No evidence""",2022,2022-10-06T21:29:17Z,,,
arXIv2022,Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models,No.,1,"""No evidence""",2022,2022-10-06T18:52:24Z,,,
arXIv2022,VIMA: General Robot Manipulation with Multimodal Prompts,No.,1,"""No evidence""",2022,2022-10-06T17:50:11Z,,,
arXIv2022,Explainable Verbal Deception Detection using Transformers,No.,1,"""No evidence""",2022,2022-10-06T17:36:00Z,,,
arXIv2022,Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering,No.,1,"""No evidence""",2022,2022-10-06T17:34:06Z,,,
arXIv2022,Language Models are Multilingual Chain-of-Thought Reasoners,No.,1,"""No evidence""",2022,2022-10-06T17:03:34Z,,,
arXIv2022,ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs,No.,1,"""No evidence""",2022,2022-10-06T16:57:23Z,,,
arXIv2022,Conversational Semantic Role Labeling with Predicate-Oriented Latent Graph,No.,1,"""No evidence""",2022,2022-10-06T16:42:00Z,,,
arXIv2022,Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation,No.,1,"""No evidence""",2022,2022-10-06T14:44:21Z,,,
arXIv2022,Generative Entity Typing with Curriculum Learning,No.,1,"""No evidence""",2022,2022-10-06T13:32:50Z,,,
arXIv2022,Binding Language Models in Symbolic Languages,No.,1,"""No evidence""",2022,2022-10-06T12:55:17Z,,,
arXIv2022,Vision Transformer Based Model for Describing a Set of Images as a Story,No.,1,"""No evidence""",2022,2022-10-06T09:01:50Z,,,
arXIv2022,Join-Chain Network: A Logical Reasoning View of the Multi-head Attention in Transformer,No.,1,"""No evidence""",2022,2022-10-06T07:39:58Z,,,
arXIv2022,Generalization Properties of Retrieval-based Models,No.,1,"""No evidence""",2022,2022-10-06T00:33:01Z,,,
arXIv2022,CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations,No.,1,"""No evidence""",2022,2022-10-05T22:44:35Z,,,
arXIv2022,Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption,No.,1,"""No evidence""",2022,2022-10-05T21:46:02Z,,,
arXIv2022,Reprogramming Pretrained Language Models for Antibody Sequence Infilling,No.,1,"""No evidence""",2022,2022-10-05T20:44:55Z,,,
arXIv2022,Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model,No.,1,"""No evidence""",2022,2022-10-05T18:23:49Z,,,
arXIv2022,Ask Me Anything: A simple strategy for prompting language models,No.,1,"""No evidence""",2022,2022-10-05T17:59:45Z,,,
arXIv2022,GLM-130B: An Open Bilingual Pre-trained Model,No.,1,"""No evidence""",2022,2022-10-05T17:34:44Z,,,
arXIv2022,Decomposed Prompting: A Modular Approach for Solving Complex Tasks,No.,1,"""No evidence""",2022,2022-10-05T17:28:20Z,,,
arXIv2022,Bayesian Prompt Learning for Image-Language Model Generalization,No.,1,"""No evidence""",2022,2022-10-05T17:05:56Z,,,
arXIv2022,Continuous Monte Carlo Graph Search,No.,1,"""No evidence""",2022,2022-10-04T07:34:06Z,,,
arXIv2022,Recitation-Augmented Language Models,No.,1,"""No evidence""",2022,2022-10-04T00:49:20Z,,,
arXIv2022,Enriching Vulnerability Reports Through Automated and Augmented Description Summarization,No.,1,"""No evidence""",2022,2022-10-03T22:46:35Z,,,
arXIv2022,CaiRL: A High-Performance Reinforcement Learning Environment Toolkit,No.,1,"""No evidence""",2022,2022-10-03T21:24:04Z,,,
arXIv2022,ContraCLM: Contrastive Learning For Causal Language Model,No.,1,"""No evidence""",2022,2022-10-03T18:56:35Z,,,
arXIv2022,Hypothesis Engineering for Zero-Shot Hate Speech Detection,No.,1,"""No evidence""",2022,2022-10-03T13:11:42Z,,,
arXIv2022,Complexity-Based Prompting for Multi-Step Reasoning,No.,1,"""No evidence""",2022,2022-10-03T05:33:27Z,,,
arXIv2022,SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model,No.,1,"""No evidence""",2022,2022-10-03T04:15:36Z,,,
arXIv2022,Probing of Quantitative Values in Abstractive Summarization Models,No.,1,"""No evidence""",2022,2022-10-03T00:59:50Z,,,
arXIv2022,Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis,No.,1,"""No evidence""",2022,2022-10-02T03:09:12Z,,,
arXIv2022,LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings,No.,1,"""No evidence""",2022,2022-10-01T16:01:53Z,,,
arXIv2022,Construction and Evaluation of a Self-Attention Model for Semantic Understanding of Sentence-Final Particles,No.,1,"""No evidence""",2022,2022-10-01T13:54:54Z,,,
arXIv2022,Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution,No.,1,"""No evidence""",2022,2022-09-30T23:10:11Z,,,
arXIv2022,SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data,No.,1,"""No evidence""",2022,2022-09-30T09:12:10Z,,,
arXIv2022,SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation,No.,1,"""No evidence""",2022,2022-09-30T09:03:22Z,,,
arXIv2022,Linearly Mapping from Image to Text Space,No.,1,"""No evidence""",2022,2022-09-30T01:17:18Z,,,
arXIv2022,Toward Trustworthy Neural Program Synthesis,No.,1,"""No evidence""",2022,2022-09-29T20:32:07Z,,,
arXIv2022,Few-shot Text Classification with Dual Contrastive Consistency,No.,1,"""No evidence""",2022,2022-09-29T19:26:23Z,,,
arXIv2022,Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging,No.,1,"""No evidence""",2022,2022-09-29T17:46:13Z,,,
arXIv2022,Contrastive Unsupervised Learning of World Model with Invariant Causal Features,No.,1,"""No evidence""",2022,2022-09-29T16:49:24Z,,,
arXIv2022,Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus,No.,1,"""No evidence""",2022,2022-09-29T16:45:43Z,,,
arXIv2022,polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics,No.,1,"""No evidence""",2022,2022-09-29T14:09:54Z,,,
arXIv2022,Prompt-guided Scene Generation for 3D Zero-Shot Learning,No.,1,"""No evidence""",2022,2022-09-29T11:24:33Z,,,
arXIv2022,An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation,No.,1,"""No evidence""",2022,2022-09-29T08:41:32Z,,,
arXIv2022,Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,No.,1,"""No evidence""",2022,2022-09-29T08:01:04Z,,,
arXIv2022,Neural Media Bias Detection Using Distant Supervision With BABE -- Bias Annotations By Experts,No.,1,"""No evidence""",2022,2022-09-29T05:32:55Z,,,
arXIv2022,Bidirectional Language Models Are Also Few-shot Learners,No.,1,"""No evidence""",2022,2022-09-29T01:35:57Z,,,
arXIv2022,Improving alignment of dialogue agents via targeted human judgements,No.,1,"""No evidence""",2022,2022-09-28T19:04:43Z,,,
arXIv2022,"Who is GPT-3? An Exploration of Personality, Values and Demographics",No.,1,"""No evidence""",2022,2022-09-28T18:07:02Z,,,
arXIv2022,Programmable and Customized Intelligence for Traffic Steering in 5G Networks Using Open RAN Architectures,No.,1,"""No evidence""",2022,2022-09-28T15:31:06Z,,,
arXIv2022,Supervised Contrastive Learning as Multi-Objective Optimization for Fine-Tuning Large Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-09-28T15:13:58Z,,,
arXIv2022,CEFER: A Four Facets Framework based on Context and Emotion embedded features for Implicit and Explicit Emotion Recognition,No.,1,"""No evidence""",2022,2022-09-28T11:16:32Z,,,
arXIv2022,Medical Image Captioning via Generative Pretrained Transformers,No.,1,"""No evidence""",2022,2022-09-28T10:27:10Z,,,
arXIv2022,ArNLI: Arabic Natural Language Inference for Entailment and Contradiction Detection,No.,1,"""No evidence""",2022,2022-09-28T09:37:16Z,,,
arXIv2022,YATO: Yet Another deep learning based Text analysis Open toolkit,No.,1,"""No evidence""",2022,2022-09-28T07:25:04Z,,,
arXIv2022,Streaming Video Temporal Action Segmentation In Real Time,No.,1,"""No evidence""",2022,2022-09-28T03:27:37Z,,,
arXIv2022,Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models,No.,1,"""No evidence""",2022,2022-09-27T12:05:59Z,,,
arXIv2022,A general-purpose material property data extraction pipeline from large polymer corpora using Natural Language Processing,No.,1,"""No evidence""",2022,2022-09-27T03:47:03Z,,,
arXIv2022,Towards Simple and Efficient Task-Adaptive Pre-training for Text Classification,No.,1,"""No evidence""",2022,2022-09-26T18:29:12Z,,,
arXIv2022,Entailment Semantics Can Be Extracted from an Ideal Language Model,No.,1,"""No evidence""",2022,2022-09-26T04:16:02Z,,,
arXIv2022,Paraphrasing Is All You Need for Novel Object Captioning,No.,1,"""No evidence""",2022,2022-09-25T22:56:04Z,,,
arXIv2022,Can Transformer Models Effectively Detect Software Aspects in StackOverflow Discussion?,No.,1,"""No evidence""",2022,2022-09-24T18:28:14Z,,,
arXIv2022,Dead or Murdered? Predicting Responsibility Perception in Femicide News Reports,No.,1,"""No evidence""",2022,2022-09-24T15:14:03Z,,,
arXIv2022,Learning Chess With Language Models and Transformers,No.,1,"""No evidence""",2022,2022-09-24T01:22:59Z,,,
arXIv2022,Whodunit? Learning to Contrast for Authorship Attribution,No.,1,"""No evidence""",2022,2022-09-23T23:45:08Z,,,
arXIv2022,Variational Open-Domain Question Answering,No.,1,"""No evidence""",2022,2022-09-23T10:25:59Z,,,
arXIv2022,IDEA: Interactive DoublE Attentions from Label Embedding for Text Classification,No.,1,"""No evidence""",2022,2022-09-23T04:50:47Z,,,
arXIv2022,Optimization of FPGA-based CNN Accelerators Using Metaheuristics,No.,1,"""No evidence""",2022,2022-09-22T18:57:49Z,,,
arXIv2022,Prompting for a conversation: How to control a dialog model?,No.,1,"""No evidence""",2022,2022-09-22T14:59:55Z,,,
arXIv2022,MonoByte: A Pool of Monolingual Byte-level Language Models,No.,1,"""No evidence""",2022,2022-09-22T14:32:48Z,,,
arXIv2022,Adaptation of domain-specific transformer models with text oversampling for sentiment analysis of social media posts on Covid-19 vaccines,No.,1,"""No evidence""",2022,2022-09-22T12:36:40Z,,,
arXIv2022,"Developing, Evaluating and Scaling Learning Agents in Multi-Agent Environments",No.,1,"""No evidence""",2022,2022-09-22T12:28:29Z,,,
arXIv2022,Semantically Consistent Data Augmentation for Neural Machine Translation via Conditional Masked Language Model,No.,1,"""No evidence""",2022,2022-09-22T09:19:08Z,,,
arXIv2022,DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation,No.,1,"""No evidence""",2022,2022-09-22T05:59:59Z,,,
arXIv2022,Deep Learning Based Page Creation for Improving E-Commerce Organic Search Traffic,No.,1,"""No evidence""",2022,2022-09-22T05:36:44Z,,,
arXIv2022,AIR-JPMC@SMM4H'22: Classifying Self-Reported Intimate Partner Violence in Tweets with Multiple BERT-based Models,No.,1,"""No evidence""",2022,2022-09-22T03:43:25Z,,,
arXIv2022,Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans vs. BERT,No.,1,"""No evidence""",2022,2022-09-21T17:57:23Z,,,
arXIv2022,SMTCE: A Social Media Text Classification Evaluation Benchmark and BERTology Models for Vietnamese,No.,1,"""No evidence""",2022,2022-09-21T16:33:46Z,,,
arXIv2022,WeLM: A Well-Read Pre-trained Language Model for Chinese,No.,1,"""No evidence""",2022,2022-09-21T14:05:30Z,,,
arXIv2022,"Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text Transfer Transformers",No.,1,"""No evidence""",2022,2022-09-21T04:21:27Z,,,
arXIv2022,LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging,No.,1,"""No evidence""",2022,2022-09-20T17:59:08Z,,,
arXIv2022,Towards Fine-tuning Pre-trained Language Models with Integer Forward and Backward Propagation,No.,1,"""No evidence""",2022,2022-09-20T16:02:28Z,,,
arXIv2022,GAMA: Generative Adversarial Multi-Object Scene Attacks,No.,1,"""No evidence""",2022,2022-09-20T06:40:54Z,,,
arXIv2022,Generalizing through Forgetting -- Domain Generalization for Symptom Event Extraction in Clinical Notes,No.,1,"""No evidence""",2022,2022-09-20T05:53:22Z,,,
arXIv2022,Unsupervised Early Exit in DNNs with Multiple Exits,No.,1,"""No evidence""",2022,2022-09-20T05:35:54Z,,,
arXIv2022,A Few-shot Approach to Resume Information Extraction via Prompts,No.,1,"""No evidence""",2022,2022-09-20T04:01:46Z,,,
arXIv2022,Probabilistic Generative Transformer Language models for Generative Design of Molecules,No.,1,"""No evidence""",2022,2022-09-20T01:51:57Z,,,
arXIv2022,Will It Blend? Mixing Training Paradigms & Prompting for Argument Quality Prediction,No.,1,"""No evidence""",2022,2022-09-19T12:34:46Z,,,
arXIv2022,Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer,No.,1,"""No evidence""",2022,2022-09-19T10:21:13Z,,,
arXIv2022,Tree-based Text-Vision BERT for Video Search in Baidu Video Advertising,No.,1,"""No evidence""",2022,2022-09-19T04:49:51Z,,,
arXIv2022,Knowledge-based Analogical Reasoning in Neuro-symbolic Latent Spaces,No.,1,"""No evidence""",2022,2022-09-19T04:03:20Z,,,
arXIv2022,From Disfluency Detection to Intent Detection and Slot Filling,No.,1,"""No evidence""",2022,2022-09-17T16:03:57Z,,,
arXIv2022,Changing the Representation: Examining Language Representation for Neural Sign Language Production,No.,1,"""No evidence""",2022,2022-09-16T12:45:29Z,,,
arXIv2022,The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding,No.,1,"""No evidence""",2022,2022-09-16T09:00:49Z,,,
arXIv2022,Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning,No.,1,"""No evidence""",2022,2022-09-16T08:28:38Z,,,
arXIv2022,TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter,No.,1,"""No evidence""",2022,2022-09-15T19:01:21Z,,,
arXIv2022,"Machine Reading, Fast and Slow: When Do Models ""Understand"" Language?",No.,1,"""No evidence""",2022,2022-09-15T16:25:44Z,,,
arXIv2022,Continuous MDP Homomorphisms and Homomorphic Policy Gradient,No.,1,"""No evidence""",2022,2022-09-15T15:26:49Z,,,
arXIv2022,Linear Transformations for Cross-lingual Sentiment Analysis,No.,1,"""No evidence""",2022,2022-09-15T12:27:16Z,,,
arXIv2022,Learning to Exploit Elastic Actuators for Quadruped Locomotion,No.,1,"""No evidence""",2022,2022-09-15T09:43:17Z,,,
arXIv2022,PTab: Using the Pre-trained Language Model for Modeling Tabular Data,No.,1,"""No evidence""",2022,2022-09-15T08:58:42Z,,,
arXIv2022,COOL-MC: A Comprehensive Tool for Reinforcement Learning and Model Checking,No.,1,"""No evidence""",2022,2022-09-15T08:25:43Z,,,
arXIv2022,uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers,No.,1,"""No evidence""",2022,2022-09-15T05:57:12Z,,,
arXIv2022,"Out of One, Many: Using Language Models to Simulate Human Samples",No.,1,"""No evidence""",2022,2022-09-14T19:53:32Z,,,
arXIv2022,On the State of the Art in Authorship Attribution and Authorship Verification,No.,1,"""No evidence""",2022,2022-09-14T18:32:26Z,,,
arXIv2022,PaLI: A Jointly-Scaled Multilingual Language-Image Model,No.,1,"""No evidence""",2022,2022-09-14T17:24:07Z,,,
arXIv2022,Toward Improving Health Literacy in Patient Education Materials with Neural Machine Translation Models,No.,1,"""No evidence""",2022,2022-09-14T15:30:54Z,,,
arXIv2022,Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?,No.,1,"""No evidence""",2022,2022-09-14T12:03:31Z,,,
arXIv2022,How to Find Strong Summary Coherence Measures? A Toolbox and a Comparative Study for Summary Coherence Measure Evaluation,No.,1,"""No evidence""",2022,2022-09-14T09:42:19Z,,,
arXIv2022,Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models,No.,1,"""No evidence""",2022,2022-09-14T09:10:07Z,,,
arXIv2022,BERT-based Ensemble Approaches for Hate Speech Detection,No.,1,"""No evidence""",2022,2022-09-14T09:08:24Z,,,
arXIv2022,PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization,No.,1,"""No evidence""",2022,2022-09-14T06:08:13Z,,,
arXIv2022,CNN-Trans-Enc: A CNN-Enhanced Transformer-Encoder On Top Of Static BERT representations for Document Classification,No.,1,"""No evidence""",2022,2022-09-13T23:23:08Z,,,
arXIv2022,"Do Androids Laugh at Electric Sheep? Humor ""Understanding"" Benchmarks from The New Yorker Caption Contest",No.,1,"""No evidence""",2022,2022-09-13T20:54:00Z,,,
arXIv2022,Exploring Code Style Transfer with Neural Networks,No.,1,"""No evidence""",2022,2022-09-13T19:34:42Z,,,
arXIv2022,Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing N-gram Language Models,No.,1,"""No evidence""",2022,2022-09-13T17:59:21Z,,,
arXIv2022,Improving Language Model Prompting in Support of Semi-autonomous Task Learning,No.,1,"""No evidence""",2022,2022-09-13T15:36:01Z,,,
arXIv2022,Don't Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling,No.,1,"""No evidence""",2022,2022-09-13T13:09:49Z,,,
arXIv2022,Robin: A Novel Online Suicidal Text Corpus of Substantial Breadth and Scale,No.,1,"""No evidence""",2022,2022-09-13T03:32:47Z,,,
arXIv2022,Unified State Representation Learning under Data Augmentation,No.,1,"""No evidence""",2022,2022-09-12T15:10:28Z,,,
arXIv2022,A new hazard event classification model via deep learning and multifractal,No.,1,"""No evidence""",2022,2022-09-12T14:13:13Z,,,
arXIv2022,Open-Domain Dialog Evaluation using Follow-Ups Likelihood,No.,1,"""No evidence""",2022,2022-09-12T12:22:31Z,,,
arXIv2022,Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset,No.,1,"""No evidence""",2022,2022-09-11T15:05:42Z,,,
arXIv2022,Probing for Understanding of English Verb Classes and Alternations in Large Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-09-11T08:04:40Z,,,
arXIv2022,OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,No.,1,"""No evidence""",2022,2022-09-10T04:38:27Z,,,
arXIv2022,"Yes, DLGM! A novel hierarchical model for hazard classification",No.,1,"""No evidence""",2022,2022-09-10T02:45:59Z,,,
arXIv2022,Trigger Warnings: Bootstrapping a Violence Detector for FanFiction,No.,1,"""No evidence""",2022,2022-09-09T17:27:03Z,,,
arXIv2022,Automatic Readability Assessment of German Sentences with Transformer Ensembles,No.,1,"""No evidence""",2022,2022-09-09T13:47:55Z,,,
arXIv2022,EchoCoTr: Estimation of the Left Ventricular Ejection Fraction from Spatiotemporal Echocardiography,No.,1,"""No evidence""",2022,2022-09-09T11:01:59Z,,,
arXIv2022,MaxMatch-Dropout: Subword Regularization for WordPiece,No.,1,"""No evidence""",2022,2022-09-09T05:41:26Z,,,
arXIv2022,PoxVerifi: An Information Verification System to Combat Monkeypox Misinformation,No.,1,"""No evidence""",2022,2022-09-09T02:50:47Z,,,
arXIv2022,Non-autoregressive Error Correction for CTC-based ASR with Phone-conditioned Masked LM,No.,1,"""No evidence""",2022,2022-09-08T23:42:37Z,,,
arXIv2022,IDIAPers @ Causal News Corpus 2022: Extracting Cause-Effect-Signal Triplets via Pre-trained Autoregressive Language Model,No.,1,"""No evidence""",2022,2022-09-08T15:54:25Z,,,
arXIv2022,Pre-Training a Graph Recurrent Network for Language Representation,No.,1,"""No evidence""",2022,2022-09-08T14:12:15Z,,,
arXIv2022,Multi-Granularity Prediction for Scene Text Recognition,No.,1,"""No evidence""",2022,2022-09-08T06:43:59Z,,,
arXIv2022,Blessing of Class Diversity in Pre-training,No.,1,"""No evidence""",2022,2022-09-07T20:10:12Z,,,
arXIv2022,What does a platypus look like? Generating customized prompts for zero-shot image classification,No.,1,"""No evidence""",2022,2022-09-07T17:27:08Z,,,
arXIv2022,Distilling Deep RL Models Into Interpretable Neuro-Fuzzy Systems,No.,1,"""No evidence""",2022,2022-09-07T14:43:14Z,,,
arXIv2022,AudioLM: a Language Modeling Approach to Audio Generation,No.,1,"""No evidence""",2022,2022-09-07T13:40:08Z,,,
arXIv2022,DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality Attention,No.,1,"""No evidence""",2022,2022-09-07T13:25:09Z,,,
arXIv2022,Non-Standard Vietnamese Word Detection and Normalization for Text-to-Speech,No.,1,"""No evidence""",2022,2022-09-07T07:34:05Z,,,
arXIv2022,A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform,No.,1,"""No evidence""",2022,2022-09-07T06:33:57Z,,,
arXIv2022,ASR2K: Speech Recognition for Around 2000 Languages without Audio,No.,1,"""No evidence""",2022,2022-09-06T22:48:29Z,,,
arXIv2022,Depression Symptoms Modelling from Social Media Text: A Semi-supervised Learning Approach,No.,1,"""No evidence""",2022,2022-09-06T18:41:57Z,,,
arXIv2022,Project proposal: A modular reinforcement learning based automated theorem prover,No.,1,"""No evidence""",2022,2022-09-06T15:12:53Z,,,
arXIv2022,"""Mama Always Had a Way of Explaining Things So I Could Understand'': A Dialogue Corpus for Learning to Construct Explanations",No.,1,"""No evidence""",2022,2022-09-06T14:00:22Z,,,
arXIv2022,Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors,No.,1,"""No evidence""",2022,2022-09-06T13:32:55Z,,,
arXIv2022,Layer or Representation Space: What makes BERT-based Evaluation Metrics Robust?,No.,1,"""No evidence""",2022,2022-09-06T09:10:54Z,,,
arXIv2022,Reference Resolution and Context Change in Multimodal Situated Dialogue for Exploring Data Visualizations,No.,1,"""No evidence""",2022,2022-09-06T04:43:28Z,,,
arXIv2022,Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection in Aerial Images,No.,1,"""No evidence""",2022,2022-09-06T03:42:18Z,,,
arXIv2022,Distilling the Knowledge of BERT for CTC-based ASR,No.,1,"""No evidence""",2022,2022-09-05T16:08:35Z,,,
arXIv2022,Selective Annotation Makes Language Models Better Few-Shot Learners,No.,1,"""No evidence""",2022,2022-09-05T14:01:15Z,,,
arXIv2022,ChemBERTa-2: Towards Chemical Foundation Models,No.,1,"""No evidence""",2022,2022-09-05T00:31:12Z,,,
arXIv2022,Generalization in Neural Networks: A Broad Survey,No.,1,"""No evidence""",2022,2022-09-04T12:48:30Z,,,
arXIv2022,Selective Text Augmentation with Word Roles for Low-Resource Text Classification,No.,1,"""No evidence""",2022,2022-09-04T08:13:11Z,,,
arXIv2022,TransPolymer: a Transformer-based language model for polymer property predictions,No.,1,"""No evidence""",2022,2022-09-03T01:29:59Z,,,
arXIv2022,Elaboration-Generating Commonsense Question Answering at Scale,No.,1,"""No evidence""",2022,2022-09-02T18:32:09Z,,,
arXIv2022,GReS: Graphical Cross-domain Recommendation for Supply Chain Platform,No.,1,"""No evidence""",2022,2022-09-02T12:58:03Z,,,
arXIv2022,FOLIO: Natural Language Reasoning with First-Order Logic,No.,1,"""No evidence""",2022,2022-09-02T06:50:11Z,,,
arXIv2022,In conversation with Artificial Intelligence: aligning language models with human values,No.,1,"""No evidence""",2022,2022-09-01T21:16:47Z,,,
arXIv2022,Unsupervised Simplification of Legal Texts,No.,1,"""No evidence""",2022,2022-09-01T15:58:12Z,,,
arXIv2022,Isotropic Representation Can Improve Dense Retrieval,No.,1,"""No evidence""",2022,2022-09-01T04:29:38Z,,,
arXIv2022,The Fellowship of the Authors: Disambiguating Names from Social Network Context,No.,1,"""No evidence""",2022,2022-08-31T21:51:55Z,,,
arXIv2022,Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks,No.,1,"""No evidence""",2022,2022-08-31T15:36:27Z,,,
arXIv2022,Cluster-based Sampling in Hindsight Experience Replay for Robotic Tasks (Student Abstract),No.,1,"""No evidence""",2022,2022-08-31T09:45:30Z,,,
arXIv2022,Continuous QA Learning with Structured Prompts,No.,1,"""No evidence""",2022,2022-08-31T02:38:16Z,,,
arXIv2022,A Prescriptive Learning Analytics Framework: Beyond Predictive Modelling and onto Explainable AI with Prescriptive Analytics and ChatGPT,No.,1,"""No evidence""",2022,2022-08-31T00:57:17Z,,,
arXIv2022,To Adapt or to Fine-tune: A Case Study on Abstractive Summarization,No.,1,"""No evidence""",2022,2022-08-30T22:48:28Z,,,
arXIv2022,Efficient and Interpretable Neural Models for Entity Tracking,No.,1,"""No evidence""",2022,2022-08-30T13:25:27Z,,,
arXIv2022,Expressions Causing Differences in Emotion Recognition in Social Networking Service Documents,No.,1,"""No evidence""",2022,2022-08-30T13:17:32Z,,,
arXIv2022,Transformers with Learnable Activation Functions,No.,1,"""No evidence""",2022,2022-08-30T09:47:31Z,,,
arXIv2022,SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance,No.,1,"""No evidence""",2022,2022-08-30T03:05:56Z,,,
arXIv2022,Personal Attribute Prediction from Conversations,No.,1,"""No evidence""",2022,2022-08-29T15:21:53Z,,,
arXIv2022,"Multi-dimensional Racism Classification during COVID-19: Stigmatization, Offensiveness, Blame, and Exclusion",No.,1,"""No evidence""",2022,2022-08-29T00:38:56Z,,,
arXIv2022,On Unsupervised Training of Link Grammar Based Language Models,No.,1,"""No evidence""",2022,2022-08-27T14:07:24Z,,,
arXIv2022,Task-specific Pre-training and Prompt Decomposition for Knowledge Graph Population with Language Models,No.,1,"""No evidence""",2022,2022-08-26T09:56:27Z,,,
arXIv2022,Extracting Biomedical Factual Knowledge Using Pretrained Language Model and Electronic Health Record Context,No.,1,"""No evidence""",2022,2022-08-26T00:01:26Z,,,
arXIv2022,Addressing Token Uniformity in Transformers via Singular Value Transformation,No.,1,"""No evidence""",2022,2022-08-24T22:44:09Z,,,
arXIv2022,Learning from Unlabeled 3D Environments for Vision-and-Language Navigation,No.,1,"""No evidence""",2022,2022-08-24T21:50:20Z,,,
arXIv2022,IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages,No.,1,"""No evidence""",2022,2022-08-24T20:14:52Z,,,
arXIv2022,Interpreting Song Lyrics with an Audio-Informed Pre-trained Language Model,No.,1,"""No evidence""",2022,2022-08-24T17:07:37Z,,,
arXIv2022,PEER: A Collaborative Language Model,No.,1,"""No evidence""",2022,2022-08-24T16:56:47Z,,,
arXIv2022,Ontology-Driven Self-Supervision for Adverse Childhood Experiences Identification Using Social Media Datasets,No.,1,"""No evidence""",2022,2022-08-24T12:23:01Z,,,
arXIv2022,Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration,No.,1,"""No evidence""",2022,2022-08-24T07:56:12Z,,,
arXIv2022,Prompting as Probing: Using Language Models for Knowledge Base Construction,No.,1,"""No evidence""",2022,2022-08-23T16:03:50Z,,,
arXIv2022,CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations,No.,1,"""No evidence""",2022,2022-08-23T09:52:34Z,,,
arXIv2022,Multimodal Crop Type Classification Fusing Multi-Spectral Satellite Time Series with Farmers Crop Rotations and Local Crop Distribution,No.,1,"""No evidence""",2022,2022-08-23T09:41:09Z,,,
arXIv2022,GenTUS: Simulating User Behaviour and Language in Task-oriented Dialogues with Generative Transformers,No.,1,"""No evidence""",2022,2022-08-23T09:01:17Z,,,
arXIv2022,Learning Better Masking for Better Language Model Pre-training,No.,1,"""No evidence""",2022,2022-08-23T08:27:52Z,,,
arXIv2022,Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation,No.,1,"""No evidence""",2022,2022-08-23T05:12:27Z,,,
arXIv2022,K-MHaS: A Multi-label Hate Speech Detection Dataset in Korean Online News Comment,No.,1,"""No evidence""",2022,2022-08-23T02:10:53Z,,,
arXIv2022,A Syntax Aware BERT for Identifying Well-Formed Queries in a Curriculum Framework,No.,1,"""No evidence""",2022,2022-08-21T15:35:33Z,,,
arXIv2022,"CMSBERT-CLR: Context-driven Modality Shifting BERT with Contrastive Learning for linguistic, visual, acoustic Representations",No.,1,"""No evidence""",2022,2022-08-21T08:21:43Z,,,
arXIv2022,I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning,No.,1,"""No evidence""",2022,2022-08-21T07:34:37Z,,,
arXIv2022,Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization,No.,1,"""No evidence""",2022,2022-08-21T01:00:54Z,,,
arXIv2022,BSpell: A CNN-Blended BERT Based Bangla Spell Checker,No.,1,"""No evidence""",2022,2022-08-20T15:21:35Z,,,
arXIv2022,Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks,No.,1,"""No evidence""",2022,2022-08-20T14:01:56Z,,,
arXIv2022,SPOT: Knowledge-Enhanced Language Representations for Information Extraction,No.,1,"""No evidence""",2022,2022-08-20T07:32:25Z,,,
arXIv2022,Pretrained Language Encoders are Natural Tagging Frameworks for Aspect Sentiment Triplet Extraction,No.,1,"""No evidence""",2022,2022-08-20T06:40:45Z,,,
arXIv2022,Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection,No.,1,"""No evidence""",2022,2022-08-20T04:13:27Z,,,
arXIv2022,Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks,No.,1,"""No evidence""",2022,2022-08-19T21:53:15Z,,,
arXIv2022,Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes,No.,1,"""No evidence""",2022,2022-08-19T16:34:41Z,,,
arXIv2022,Nonlinear Optical Data Transformer for Machine Learning,No.,1,"""No evidence""",2022,2022-08-19T15:28:48Z,,,
arXIv2022,Non-Stationary Dynamic Pricing Via Actor-Critic Information-Directed Pricing,No.,1,"""No evidence""",2022,2022-08-19T14:37:37Z,,,
arXIv2022,UniCausal: Unified Benchmark and Repository for Causal Text Mining,No.,1,"""No evidence""",2022,2022-08-19T06:14:05Z,,,
arXIv2022,A Risk-Sensitive Approach to Policy Optimization,No.,1,"""No evidence""",2022,2022-08-19T00:55:05Z,,,
arXIv2022,MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing,No.,1,"""No evidence""",2022,2022-08-19T00:43:47Z,,,
arXIv2022,"MARTI-4: new model of human brain, considering neocortex and basal ganglia -- learns to play Atari game by reinforcement learning on a single CPU",No.,1,"""No evidence""",2022,2022-08-18T20:23:49Z,,,
arXIv2022,Extracting Medication Changes in Clinical Narratives using Pre-trained Language Models,No.,1,"""No evidence""",2022,2022-08-17T17:22:48Z,,,
arXIv2022,Neural Embeddings for Text,No.,1,"""No evidence""",2022,2022-08-17T16:26:13Z,,,
arXIv2022,A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting,No.,1,"""No evidence""",2022,2022-08-17T16:11:07Z,,,
arXIv2022,Dual Modality Prompt Tuning for Vision-Language Pre-Trained Model,No.,1,"""No evidence""",2022,2022-08-17T15:06:36Z,,,
arXIv2022,Quality Diversity Evolutionary Learning of Decision Trees,No.,1,"""No evidence""",2022,2022-08-17T13:57:32Z,,,
arXIv2022,MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation,No.,1,"""No evidence""",2022,2022-08-17T11:16:52Z,,,
arXIv2022,Visual Comparison of Language Model Adaptation,No.,1,"""No evidence""",2022,2022-08-17T09:25:28Z,,,
arXIv2022,ASTRO: An AST-Assisted Approach for Generalizable Neural Clone Detection,No.,1,"""No evidence""",2022,2022-08-17T04:50:51Z,,,
arXIv2022,Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models,No.,1,"""No evidence""",2022,2022-08-16T17:17:53Z,,,
arXIv2022,BERT(s) to Detect Multiword Expressions,No.,1,"""No evidence""",2022,2022-08-16T16:32:23Z,,,
arXIv2022,MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control,No.,1,"""No evidence""",2022,2022-08-15T17:57:33Z,,,
arXIv2022,MENLI: Robust Evaluation Metrics from Natural Language Inference,No.,1,"""No evidence""",2022,2022-08-15T16:30:14Z,,,
arXIv2022,Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection,No.,1,"""No evidence""",2022,2022-08-15T09:27:34Z,,,
arXIv2022,Syntax-driven Data Augmentation for Named Entity Recognition,No.,1,"""No evidence""",2022,2022-08-15T01:24:55Z,,,
arXIv2022,Continuous Active Learning Using Pretrained Transformers,No.,1,"""No evidence""",2022,2022-08-15T01:09:19Z,,,
arXIv2022,Teacher Guided Training: An Efficient Framework for Knowledge Transfer,No.,1,"""No evidence""",2022,2022-08-14T10:33:58Z,,,
arXIv2022,Text Difficulty Study: Do machines behave the same as humans regarding text difficulty?,No.,1,"""No evidence""",2022,2022-08-14T06:12:08Z,,,
arXIv2022,Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models,No.,1,"""No evidence""",2022,2022-08-13T16:04:39Z,,,
arXIv2022,Cloud-Based Real-Time Molecular Screening Platform with MolFormer,No.,1,"""No evidence""",2022,2022-08-13T14:43:19Z,,,
arXIv2022,Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification,No.,1,"""No evidence""",2022,2022-08-13T10:22:12Z,,,
arXIv2022,Interpreting BERT-based Text Similarity via Activation and Saliency Maps,No.,1,"""No evidence""",2022,2022-08-13T10:06:24Z,,,
arXIv2022,MetricBERT: Text Representation Learning via Self-Supervised Triplet Training,No.,1,"""No evidence""",2022,2022-08-13T09:52:58Z,,,
arXIv2022,LM-CORE: Language Models with Contextually Relevant External Knowledge,No.,1,"""No evidence""",2022,2022-08-12T18:59:37Z,,,
arXIv2022,A Twitter-Driven Deep Learning Mechanism for the Determination of Vehicle Hijacking Spots in Cities,No.,1,"""No evidence""",2022,2022-08-11T21:56:34Z,,,
arXIv2022,Bayesian Soft Actor-Critic: A Directed Acyclic Strategy Graph Based Deep Reinforcement Learning,No.,1,"""No evidence""",2022,2022-08-11T20:36:23Z,,,
arXIv2022,A Model of Anaphoric Ambiguities using Sheaf Theoretic Quantum-like Contextuality and BERT,No.,1,"""No evidence""",2022,2022-08-11T09:31:15Z,,,
arXIv2022,Re-creation of Creations: A New Paradigm for Lyric-to-Melody Generation,No.,1,"""No evidence""",2022,2022-08-11T08:44:47Z,,,
arXIv2022,Searching for chromate replacements using natural language processing and machine learning algorithms,No.,1,"""No evidence""",2022,2022-08-11T07:21:18Z,,,
arXIv2022,CoditT5: Pretraining for Source Code and Natural Language Editing,No.,1,"""No evidence""",2022,2022-08-10T16:59:40Z,,,
arXIv2022,Generative Action Description Prompts for Skeleton-based Action Recognition,No.,1,"""No evidence""",2022,2022-08-10T12:55:56Z,,,
arXIv2022,Controlling Perceived Emotion in Symbolic Music Generation with Monte Carlo Tree Search,No.,1,"""No evidence""",2022,2022-08-10T05:49:37Z,,,
arXIv2022,Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology,No.,1,"""No evidence""",2022,2022-08-10T04:35:58Z,,,
arXIv2022,Increasing Students' Engagement to Reminder Emails Through Multi-Armed Bandits,No.,1,"""No evidence""",2022,2022-08-10T00:30:52Z,,,
arXIv2022,Thai Wav2Vec2.0 with CommonVoice V8,No.,1,"""No evidence""",2022,2022-08-09T14:21:48Z,,,
arXIv2022,E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes,No.,1,"""No evidence""",2022,2022-08-09T09:05:10Z,,,
arXIv2022,Emotion Detection From Tweets Using a BERT and SVM Ensemble Model,No.,1,"""No evidence""",2022,2022-08-09T05:32:29Z,,,
arXIv2022,A Multimodal Transformer: Fusing Clinical Notes with Structured EHR Data for Interpretable In-Hospital Mortality Prediction,No.,1,"""No evidence""",2022,2022-08-09T03:49:52Z,,,
arXIv2022,Exploring Hate Speech Detection with HateXplain and BERT,No.,1,"""No evidence""",2022,2022-08-09T01:32:44Z,,,
arXIv2022,When can I Speak? Predicting initiation points for spoken dialogue agents,No.,1,"""No evidence""",2022,2022-08-07T20:58:52Z,,,
arXIv2022,Towards No.1 in CLUE Semantic Matching Challenge: Pre-trained Language Model Erlangshen with Propensity-Corrected Loss,No.,1,"""No evidence""",2022,2022-08-05T02:52:29Z,,,
arXIv2022,LATTE: LAnguage Trajectory TransformEr,No.,1,"""No evidence""",2022,2022-08-04T22:43:21Z,,,
arXIv2022,Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models,No.,1,"""No evidence""",2022,2022-08-04T02:13:03Z,,,
arXIv2022,A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis,No.,1,"""No evidence""",2022,2022-08-03T16:21:08Z,,,
arXIv2022,KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model for Financial Reports,No.,1,"""No evidence""",2022,2022-08-03T15:21:28Z,,,
arXIv2022,Efficient Fine-Tuning of Compressed Language Models with Learners,No.,1,"""No evidence""",2022,2022-08-03T13:42:30Z,,,
arXIv2022,Introducing BEREL: BERT Embeddings for Rabbinic-Encoded Language,No.,1,"""No evidence""",2022,2022-08-03T06:59:04Z,,,
arXIv2022,VQ-T: RNN Transducers using Vector-Quantized Prediction Network States,No.,1,"""No evidence""",2022,2022-08-03T02:45:52Z,,,
arXIv2022,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,No.,1,"""No evidence""",2022,2022-08-02T13:30:07Z,,,
arXIv2022,BERT4Loc: BERT for Location -- POI Recommender System,No.,1,"""No evidence""",2022,2022-08-02T11:46:59Z,,,
arXIv2022,A Comparative Study on COVID-19 Fake News Detection Using Different Transformer Based Models,No.,1,"""No evidence""",2022,2022-08-02T10:50:16Z,,,
arXIv2022,Automatic Classification of Bug Reports Based on Multiple Text Information and Reports' Intention,No.,1,"""No evidence""",2022,2022-08-02T06:44:51Z,,,
arXIv2022,Implicit Two-Tower Policies,No.,1,"""No evidence""",2022,2022-08-02T01:23:50Z,,,
arXIv2022,What Can Transformers Learn In-Context? A Case Study of Simple Function Classes,No.,1,"""No evidence""",2022,2022-08-01T18:01:40Z,,,
arXIv2022,giMLPs: Gate with Inhibition Mechanism in MLPs,No.,1,"""No evidence""",2022,2022-08-01T15:23:51Z,,,
arXIv2022,Learning from flowsheets: A generative transformer model for autocompletion of flowsheets,No.,1,"""No evidence""",2022,2022-08-01T13:43:58Z,,,
arXIv2022,Composable Text Controls in Latent Space with ODEs,No.,1,"""No evidence""",2022,2022-08-01T06:51:45Z,,,
arXIv2022,Interacting with next-phrase suggestions: How suggestion systems aid and influence the cognitive processes of writing,No.,1,"""No evidence""",2022,2022-08-01T06:49:07Z,,,
arXIv2022,DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning,No.,1,"""No evidence""",2022,2022-08-01T06:43:19Z,,,
arXIv2022,Smoothing Entailment Graphs with Language Models,No.,1,"""No evidence""",2022,2022-07-30T22:15:22Z,,,
arXIv2022,A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond,No.,1,"""No evidence""",2022,2022-07-30T09:59:28Z,,,
arXIv2022,SERCNN: Stacked Embedding Recurrent Convolutional Neural Network in Detecting Depression on Twitter,No.,1,"""No evidence""",2022,2022-07-29T08:08:15Z,,,
arXIv2022,Curriculum Learning for Data-Efficient Vision-Language Alignment,No.,1,"""No evidence""",2022,2022-07-29T07:45:56Z,,,
arXIv2022,Code Comment Inconsistency Detection with BERT and Longformer,No.,1,"""No evidence""",2022,2022-07-29T02:43:51Z,,,
arXIv2022,LAD: Language Models as Data for Zero-Shot Dialog,No.,1,"""No evidence""",2022,2022-07-28T22:10:45Z,,,
arXIv2022,Large Language Models and the Reverse Turing Test,No.,1,"""No evidence""",2022,2022-07-28T21:22:47Z,,,
arXIv2022,CrAM: A Compression-Aware Minimizer,No.,1,"""No evidence""",2022,2022-07-28T16:13:28Z,,,
arXIv2022,Bayesian Optimization-Based Beam Alignment for MmWave MIMO Communication Systems,No.,1,"""No evidence""",2022,2022-07-28T15:37:49Z,,,
arXIv2022,Entity Type Prediction Leveraging Graph Walks and Entity Descriptions,No.,1,"""No evidence""",2022,2022-07-28T13:56:55Z,,,
arXIv2022,Sequence to sequence pretraining for a less-resourced Slovenian language,No.,1,"""No evidence""",2022,2022-07-28T10:08:50Z,,,
arXIv2022,ClaSP -- Parameter-free Time Series Segmentation,No.,1,"""No evidence""",2022,2022-07-28T10:05:53Z,,,
arXIv2022,RangL: A Reinforcement Learning Competition Platform,No.,1,"""No evidence""",2022,2022-07-28T09:44:21Z,,,
arXIv2022,Knowing Where and What: Unified Word Block Pretraining for Document Understanding,No.,1,"""No evidence""",2022,2022-07-28T09:43:06Z,,,
arXIv2022,MLRIP: Pre-training a military language representation model with informative factual knowledge and professional knowledge base,No.,1,"""No evidence""",2022,2022-07-28T07:39:30Z,,,
arXIv2022,"SDBERT: SparseDistilBERT, a faster and smaller BERT model",No.,1,"""No evidence""",2022,2022-07-28T07:34:07Z,,,
arXIv2022,Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms,No.,1,"""No evidence""",2022,2022-07-27T11:10:50Z,,,
arXIv2022,RealTime QA: What's the Answer Right Now?,No.,1,"""No evidence""",2022,2022-07-27T07:26:01Z,,,
arXIv2022,Contextual Information and Commonsense Based Prompt for Emotion Recognition in Conversation,No.,1,"""No evidence""",2022,2022-07-27T02:34:05Z,,,
arXIv2022,SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation,No.,1,"""No evidence""",2022,2022-07-27T01:14:59Z,,,
arXIv2022,Boosting Point-BERT by Multi-choice Tokens,No.,1,"""No evidence""",2022,2022-07-27T00:34:33Z,,,
arXIv2022,Learning structures of the French clinical language:development and validation of word embedding models using 21 million clinical reports from electronic health records,No.,1,"""No evidence""",2022,2022-07-26T14:46:34Z,,,
arXIv2022,Training Effective Neural Sentence Encoders from Automatically Mined Paraphrases,No.,1,"""No evidence""",2022,2022-07-26T09:08:56Z,,,
arXIv2022,Bundle MCR: Towards Conversational Bundle Recommendation,No.,1,"""No evidence""",2022,2022-07-26T03:28:42Z,,,
arXIv2022,Modelling non-reinforced preferences using selective attention,No.,1,"""No evidence""",2022,2022-07-25T22:01:32Z,,,
arXIv2022,Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug Labeling to Enhance Product-Specific Guidance Assessment,No.,1,"""No evidence""",2022,2022-07-25T17:43:36Z,,,
arXIv2022,Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?,No.,1,"""No evidence""",2022,2022-07-25T12:12:46Z,,,
arXIv2022,UrduFake@FIRE2020: Shared Track on Fake News Identification in Urdu,No.,1,"""No evidence""",2022,2022-07-25T03:46:51Z,,,
arXIv2022,Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2020,No.,1,"""No evidence""",2022,2022-07-25T03:41:32Z,,,
arXIv2022,A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach,No.,1,"""No evidence""",2022,2022-07-24T11:06:56Z,,,
arXIv2022,Improving Mandarin Speech Recogntion with Block-augmented Transformer,No.,1,"""No evidence""",2022,2022-07-24T09:23:04Z,,,
arXIv2022,A Transformer-based Neural Language Model that Synthesizes Brain Activation Maps from Free-Form Text Queries,No.,1,"""No evidence""",2022,2022-07-24T09:15:03Z,,,
arXIv2022,Better Reasoning Behind Classification Predictions with BERT for Fake News Detection,No.,1,"""No evidence""",2022,2022-07-23T17:54:48Z,,,
arXIv2022,Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter,No.,1,"""No evidence""",2022,2022-07-23T11:55:18Z,,,
arXIv2022,Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations,No.,1,"""No evidence""",2022,2022-07-23T03:19:50Z,,,
arXIv2022,PanGu-Coder: Program Synthesis with Function-Level Language Modeling,No.,1,"""No evidence""",2022,2022-07-22T18:08:16Z,,,
arXIv2022,Zero-Shot Video Captioning with Evolving Pseudo-Tokens,No.,1,"""No evidence""",2022,2022-07-22T14:19:31Z,,,
arXIv2022,BigIssue: A Realistic Bug Localization Benchmark,No.,1,"""No evidence""",2022,2022-07-21T20:17:53Z,,,
arXIv2022,Efficient model compression with Random Operation Access Specific Tile (ROAST) hashing,No.,1,"""No evidence""",2022,2022-07-21T18:31:17Z,,,
arXIv2022,Leveraging Natural Supervision for Language Representation Learning and Generation,No.,1,"""No evidence""",2022,2022-07-21T17:26:03Z,,,
arXIv2022,Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration,No.,1,"""No evidence""",2022,2022-07-21T12:37:15Z,,,
arXIv2022,Language Model Cascades,No.,1,"""No evidence""",2022,2022-07-21T07:35:18Z,,,
arXIv2022,The Birth of Bias: A case study on the evolution of gender bias in an English language model,No.,1,"""No evidence""",2022,2022-07-21T00:59:04Z,,,
arXIv2022,Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores,No.,1,"""No evidence""",2022,2022-07-20T15:04:32Z,,,
arXIv2022,Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition,No.,1,"""No evidence""",2022,2022-07-20T09:04:12Z,,,
arXIv2022,Enhancing Collaborative Filtering Recommender with Prompt-Based Sentiment Analysis,No.,1,"""No evidence""",2022,2022-07-19T21:04:31Z,,,
arXIv2022,Revealing Secrets From Pre-trained Models,No.,1,"""No evidence""",2022,2022-07-19T20:19:03Z,,,
arXIv2022,Word Play for Playing Othello (Reverses),No.,1,"""No evidence""",2022,2022-07-18T17:13:32Z,,,
arXIv2022,Label2Label: A Language Modeling Framework for Multi-Attribute Learning,No.,1,"""No evidence""",2022,2022-07-18T15:12:33Z,,,
arXIv2022,Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks,No.,1,"""No evidence""",2022,2022-07-18T02:18:20Z,,,
arXIv2022,Towards the Human Global Context: Does the Vision-Language Model Really Judge Like a Human Being?,No.,1,"""No evidence""",2022,2022-07-18T01:01:43Z,,,
arXIv2022,Technology and Consciousness,No.,1,"""No evidence""",2022,2022-07-17T23:23:01Z,,,
arXIv2022,An Overview of Distant Supervision for Relation Extraction with a Focus on Denoising and Pre-training Methods,No.,1,"""No evidence""",2022,2022-07-17T21:02:04Z,,,
arXIv2022,Representation Learning of Image Schema,No.,1,"""No evidence""",2022,2022-07-17T18:42:37Z,,,
arXIv2022,A Context-Sensitive Word Embedding Approach for The Detection of Troll Tweets,No.,1,"""No evidence""",2022,2022-07-17T17:12:16Z,,,
arXIv2022,Natural language processing for clusterization of genes according to their functions,No.,1,"""No evidence""",2022,2022-07-17T12:59:34Z,,,
arXIv2022,"ELECTRA is a Zero-Shot Learner, Too",No.,1,"""No evidence""",2022,2022-07-17T11:20:58Z,,,
arXIv2022,Aspect-specific Context Modeling for Aspect-based Sentiment Analysis,No.,1,"""No evidence""",2022,2022-07-17T07:22:19Z,,,
arXIv2022,Clover: Towards A Unified Video-Language Alignment and Fusion Model,No.,1,"""No evidence""",2022,2022-07-16T09:38:52Z,,,
arXIv2022,A No-Code Low-Code Paradigm for Authoring Business Automations Using Natural Language,No.,1,"""No evidence""",2022,2022-07-15T19:17:55Z,,,
arXIv2022,POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging,No.,1,"""No evidence""",2022,2022-07-15T18:36:29Z,,,
arXIv2022,Position Prediction as an Effective Pretraining Strategy,No.,1,"""No evidence""",2022,2022-07-15T17:10:48Z,,,
arXIv2022,Learning Flexible Translation between Robot Actions and Language Descriptions,No.,1,"""No evidence""",2022,2022-07-15T12:37:05Z,,,
arXIv2022,Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet Text,No.,1,"""No evidence""",2022,2022-07-15T06:21:35Z,,,
arXIv2022,Bootstrapped Masked Autoencoders for Vision BERT Pretraining,No.,1,"""No evidence""",2022,2022-07-14T17:59:58Z,,,
arXIv2022,Language Modelling with Pixels,No.,1,"""No evidence""",2022,2022-07-14T15:20:36Z,,,
arXIv2022,Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language,No.,1,"""No evidence""",2022,2022-07-14T13:26:03Z,,,
arXIv2022,Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically Ambiguous Settings for Low Resource Languages,No.,1,"""No evidence""",2022,2022-07-14T13:00:41Z,,,
arXIv2022,TRIE++: Towards End-to-End Information Extraction from Visually Rich Documents,No.,1,"""No evidence""",2022,2022-07-14T08:52:07Z,,,
arXIv2022,"Layout-Aware Information Extraction for Document-Grounded Dialogue: Dataset, Method and Demonstration",No.,1,"""No evidence""",2022,2022-07-14T07:59:45Z,,,
arXIv2022,Overview of Abusive and Threatening Language Detection in Urdu at FIRE 2021,No.,1,"""No evidence""",2022,2022-07-14T07:38:13Z,,,
arXIv2022,Combing for Credentials: Active Pattern Extraction from Smart Reply,No.,1,"""No evidence""",2022,2022-07-14T05:03:56Z,,,
arXIv2022,"Re2G: Retrieve, Rerank, Generate",No.,1,"""No evidence""",2022,2022-07-13T15:51:40Z,,,
arXIv2022,A Transfer Learning Based Model for Text Readability Assessment in German,No.,1,"""No evidence""",2022,2022-07-13T15:15:44Z,,,
arXIv2022,Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS,No.,1,"""No evidence""",2022,2022-07-13T07:05:44Z,,,
arXIv2022,Developing a Component Comment Extractor from Product Reviews on E-Commerce Sites,No.,1,"""No evidence""",2022,2022-07-13T06:25:55Z,,,
arXIv2022,Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models,No.,1,"""No evidence""",2022,2022-07-13T02:28:08Z,,,
arXIv2022,A Novel DeBERTa-based Model for Financial Question Answering Task,No.,1,"""No evidence""",2022,2022-07-12T22:34:39Z,,,
arXIv2022,Learning Bellman Complete Representations for Offline Policy Evaluation,No.,1,"""No evidence""",2022,2022-07-12T21:02:02Z,,,
arXIv2022,How Do Multilingual Encoders Learn Cross-lingual Representation?,No.,1,"""No evidence""",2022,2022-07-12T17:57:05Z,,,
arXIv2022,Inner Monologue: Embodied Reasoning through Planning with Language Models,No.,1,"""No evidence""",2022,2022-07-12T15:20:48Z,,,
arXIv2022,Ego-motion Estimation Based on Fusion of Images and Events,No.,1,"""No evidence""",2022,2022-07-12T15:10:28Z,,,
arXIv2022,Using Paraphrases to Study Properties of Contextual Embeddings,No.,1,"""No evidence""",2022,2022-07-12T14:22:05Z,,,
arXIv2022,Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021,No.,1,"""No evidence""",2022,2022-07-11T18:58:36Z,,,
arXIv2022,Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition,No.,1,"""No evidence""",2022,2022-07-11T08:20:53Z,,,
arXIv2022,Learning Large-scale Universal User Representation with Sparse Mixture of Experts,No.,1,"""No evidence""",2022,2022-07-11T06:19:03Z,,,
arXIv2022,Myers-Briggs personality classification from social media text using pre-trained language models,No.,1,"""No evidence""",2022,2022-07-10T14:38:09Z,,,
arXIv2022,Multilingual Persuasion Detection: Video Games as an Invaluable Data Source for NLP,No.,1,"""No evidence""",2022,2022-07-10T12:38:02Z,,,
arXIv2022,"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action",No.,1,"""No evidence""",2022,2022-07-10T10:41:50Z,,,
arXIv2022,Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis,No.,1,"""No evidence""",2022,2022-07-09T17:21:03Z,,,
arXIv2022,Internal Language Model Estimation based Language Model Fusion for Cross-Domain Code-Switching Speech Recognition,No.,1,"""No evidence""",2022,2022-07-09T02:08:54Z,,,
arXIv2022,ABB-BERT: A BERT model for disambiguating abbreviations and contractions,No.,1,"""No evidence""",2022,2022-07-08T16:54:57Z,,,
arXIv2022,Hidden Schema Networks,No.,1,"""No evidence""",2022,2022-07-08T09:26:19Z,,,
arXIv2022,VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning,No.,1,"""No evidence""",2022,2022-07-07T18:48:58Z,,,
arXIv2022,Training Transformers Together,No.,1,"""No evidence""",2022,2022-07-07T17:59:16Z,,,
arXIv2022,AsNER -- Annotated Dataset and Baseline for Assamese Named Entity recognition,No.,1,"""No evidence""",2022,2022-07-07T16:45:55Z,,,
arXIv2022,"Neural Language Models are not Born Equal to Fit Brain Data, but Training Helps",No.,1,"""No evidence""",2022,2022-07-07T15:37:17Z,,,
arXIv2022,Predicting Opinion Dynamics via Sociologically-Informed Neural Networks,No.,1,"""No evidence""",2022,2022-07-07T05:55:47Z,,,
arXIv2022,Sensitivity Analysis on Transferred Neural Architectures of BERT and GPT-2 for Financial Sentiment Analysis,No.,1,"""No evidence""",2022,2022-07-07T01:38:07Z,,,
arXIv2022,Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa,No.,1,"""No evidence""",2022,2022-07-06T03:50:31Z,,,
arXIv2022,Text Enriched Sparse Hyperbolic Graph Convolutional Networks,No.,1,"""No evidence""",2022,2022-07-06T00:23:35Z,,,
arXIv2022,Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia,No.,1,"""No evidence""",2022,2022-07-05T18:29:27Z,,,
arXIv2022,An Empirical Study of Implicit Regularization in Deep Offline RL,No.,1,"""No evidence""",2022,2022-07-05T15:07:31Z,,,
arXIv2022,Resource Allocation in Multicore Elastic Optical Networks: A Deep Reinforcement Learning Approach,No.,1,"""No evidence""",2022,2022-07-05T14:24:21Z,,,
arXIv2022,"MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering",No.,1,"""No evidence""",2022,2022-07-05T10:27:17Z,,,
arXIv2022,Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in Icelandic,No.,1,"""No evidence""",2022,2022-07-05T09:52:34Z,,,
arXIv2022,Betti numbers of attention graphs is all you really need,No.,1,"""No evidence""",2022,2022-07-05T09:10:47Z,,,
arXIv2022,ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks,No.,1,"""No evidence""",2022,2022-07-05T08:47:51Z,,,
arXIv2022,Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge Transfer,No.,1,"""No evidence""",2022,2022-07-05T08:32:18Z,,,
arXIv2022,"BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model",No.,1,"""No evidence""",2022,2022-07-04T20:43:41Z,,,
arXIv2022,A Cascade Model for Argument Mining in Japanese Political Discussions: the QA Lab-PoliInfo-3 Case Study,No.,1,"""No evidence""",2022,2022-07-04T18:49:18Z,,,
arXIv2022,Using contextual sentence analysis models to recognize ESG concepts,No.,1,"""No evidence""",2022,2022-07-04T13:33:21Z,,,
arXIv2022,Egocentric Video-Language Pretraining @ Ego4D Challenge 2022,No.,1,"""No evidence""",2022,2022-07-04T12:47:16Z,,,
arXIv2022,Egocentric Video-Language Pretraining @ EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022,No.,1,"""No evidence""",2022,2022-07-04T11:32:48Z,,,
arXIv2022,Revisiting Classifier: Transferring Vision-Language Models for Video Recognition,No.,1,"""No evidence""",2022,2022-07-04T10:00:47Z,,,
arXIv2022,Stabilizing Off-Policy Deep Reinforcement Learning from Pixels,No.,1,"""No evidence""",2022,2022-07-03T08:52:40Z,,,
arXIv2022,Generating Repetitions with Appropriate Repeated Words,No.,1,"""No evidence""",2022,2022-07-03T01:21:49Z,,,
arXIv2022,A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking,No.,1,"""No evidence""",2022,2022-07-02T13:27:59Z,,,
arXIv2022,GUIM -- General User and Item Embedding with Mixture of Representation in E-commerce,No.,1,"""No evidence""",2022,2022-07-02T06:27:54Z,,,
arXIv2022,UserLibri: A Dataset for ASR Personalization Using Only Text,No.,1,"""No evidence""",2022,2022-07-02T01:03:01Z,,,
arXIv2022,A Polyphone BERT for Polyphone Disambiguation in Mandarin Chinese,No.,1,"""No evidence""",2022,2022-07-01T09:16:29Z,,,
arXIv2023,Large Language Models (GPT) for automating feedback on programming assignments,Yes.,3,"""This suggests potential over-reliance on GPT-generated feedback.""",2023,2023-06-30T21:57:40Z,,,
arXIv2023,Meta-training with Demonstration Retrieval for Efficient Few-shot Learning,Yes.,3,"""Large language models show impressive results on few-shot NLP tasks. However, these models are memory and computation-intensive.""",2023,2023-06-30T20:16:22Z,,,
arXIv2023,Ticket-BERT: Labeling Incident Management Tickets with Language Models,Yes.,1,"""To handle these issues, we introduce Ticket-BERT which trains a simple yet robust language model for labeling tickets using our proposed ticket datasets.""",2023,2023-06-30T19:48:25Z,,,
arXIv2023,Biomedical Language Models are Robust to Sub-optimal Tokenization,Yes.,3,"""Surprisingly, we find that pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve the entity representation quality of a language model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as NER and entity linking performance.""",2023,2023-06-30T13:35:24Z,,,
arXIv2023,Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,Yes.,3,"""we argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations.""",2023,2023-06-30T11:32:25Z,,,
arXIv2023,Provable Robust Watermarking for AI-Generated Text,Yes.,1,"""We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage.""",2023,2023-06-30T07:24:32Z,,,
arXIv2023,LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection,Yes.,1,"""we find that after finetuning on Twitter bot detection, pretrained language models achieve competitive performance and do not require a graph structure during deployment.""",2023,2023-06-30T05:50:26Z,,,
arXIv2023,SummQA at MEDIQA-Chat 2023:In-Context Learning with GPT-4 for Medical Summarization,Yes.,2,"""Our results highlight the effectiveness of few-shot prompting for this task, though we also identify several weaknesses of prompting-based approaches.""",2023,2023-06-30T03:14:04Z,,,
arXIv2023,Modeling Parallel Programs using Large Language Models,Yes.,1,"""In this paper, we show how large language models (LLMs) can be applied to tasks specific to high performance and scientific codes.""",2023,2023-06-29T19:44:55Z,,,
arXIv2023,DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios,Yes.,1,"""Large Language Models (LLMs) offer a powerful solution to expedite this process through in-context learning.""",2023,2023-06-29T19:24:19Z,,,
arXIv2023,Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendations,Yes.,3,"""this naive approach heavily relies on the strong in-context learning ability emerged from large language models, which could suffer from significant latency for online recommendations.""",2023,2023-06-29T18:50:12Z,,,
arXIv2023,"Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",Yes.,3,"""These results also highlight settings where GPT-4 still struggles, providing exciting future directions on developing techniques to improve the performance of these models.""",2023,2023-06-29T17:57:40Z,,,
arXIv2023,Concept-Oriented Deep Learning with Large Language Models,Yes.,3,"""Text-only LLMs, however, can represent only symbolic (conceptual) knowledge.""",2023,2023-06-29T16:47:11Z,,,
arXIv2023,UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?,Yes.,1,"""We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets.""",2023,2023-06-29T13:30:41Z,,,
arXIv2023,From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data,Yes.,3,"""We demonstrate the significant enhancement of LLM expertise on the quality of recovered causal structures from data, while also identifying critical challenges and issues, along with potential approaches to address them.""",2023,2023-06-29T12:48:00Z,,,
arXIv2023,Benchmarking Large Language Model Capabilities for Conditional Generation,Yes.,3,"""Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced."" and ""provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language.""",2023,2023-06-29T08:59:40Z,,,
arXIv2023,CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?,Yes.,3,"""We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabilities, and promote their ongoing development and advancement.""",2023,2023-06-29T02:19:50Z,,,
arXIv2023,Palm: Predicting Actions through Language Models @ Ego4D Long-Term Action Anticipation Challenge 2023,Yes.,1,"""Large language models have demonstrated remarkable commonsense-based reasoning ability.""",2023,2023-06-28T20:33:52Z,,,
arXIv2023,Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language,Yes.,1,"""We propose LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs).""",2023,2023-06-28T17:57:10Z,,,
arXIv2023,On the Exploitability of Instruction Tuning,Yes.,3,"""We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs.""",2023,2023-06-28T17:54:04Z,,,
arXIv2023,Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting,Yes.,2,"""its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear.""",2023,2023-06-28T14:55:13Z,,,
arXIv2023,Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition,Yes.,3,"""However, these approaches usually require a significant amount of target domain text data for the training of LMs.""",2023,2023-06-28T08:29:00Z,,,
arXIv2023,Query Understanding in the Age of Large Language Models,Yes.,1,"""Querying, conversing, and controlling search and information-seeking interfaces using natural language are fast becoming ubiquitous with the rise and adoption of large-language models (LLM).""",2023,2023-06-28T08:24:14Z,,,
arXIv2023,Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,Yes.,3,"""they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM.""",2023,2023-06-28T03:31:31Z,,,
arXIv2023,HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution,Yes.,3,"""Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA.""",2023,2023-06-27T20:46:34Z,,,
arXIv2023,Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese,Yes.,3,"""while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection.""",2023,2023-06-27T20:37:54Z,,,
arXIv2023,Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,Yes.,1,"""we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models.""",2023,2023-06-27T19:29:55Z,,,
arXIv2023,REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,Yes.,1,"""Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs.""",2023,2023-06-27T18:03:15Z,,,
arXIv2023,LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,Yes.,3,"""existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving.""",2023,2023-06-27T17:05:32Z,,,
arXIv2023,"Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool",Yes.,2,"""While AI-powered tools can automatically analyze the data, they are prone to inaccuracies as they lack domain-specific expertise.""",2023,2023-06-27T14:46:47Z,,,
arXIv2023,InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback,Yes.,3,"""current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment.""",2023,2023-06-26T17:59:50Z,,,
arXIv2023,Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference,Yes.,3,"""Deploying pre-trained transformer models like BERT on downstream tasks in resource-constrained scenarios is challenging due to their high inference cost, which grows rapidly with input sequence length.""",2023,2023-06-26T03:06:57Z,,,
arXIv2023,RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,Yes.,3,"""Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets.""",2023,2023-06-25T19:23:21Z,,,
arXIv2023,Chain-of-Thought Prompt Distillation for Multimodal Named Entity Recognition and Multimodal Relation Extraction,Yes.,1,"""In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps.""",2023,2023-06-25T04:33:56Z,,,
arXIv2023,Language models are weak learners,Yes.,1,"""we illustrate that prompt-based large language models can operate effectively as said weak learners.""",2023,2023-06-25T02:39:19Z,,,
arXIv2023,Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models,Yes.,1,"""Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching.""",2023,2023-06-25T02:24:30Z,,,
arXIv2023,Full Automation of Goal-driven LLM Dialog Threads with And-Or Recursors and Refiner Oracles,Yes.,1,"""we accommodate our logic engine to fit the natural language reasoning patterns LLMs have been trained on.""",2023,2023-06-24T23:33:00Z,,,
arXIv2023,"Symbolic Chain-of-Thought Distillation: Small Models Can Also ""Think"" Step-by-Step",Yes.,3,"""benefits appear to emerge only for sufficiently large models (beyond 50B parameters).""",2023,2023-06-24T20:15:07Z,,,
arXIv2023,H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models,Yes.,3,"""Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing."" and ""a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size",2023,2023-06-24T20:11:14Z,,,
arXIv2023,"My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks",Yes.,1,"""The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models.""",2023,2023-06-24T18:17:38Z,,,
arXIv2023,LLM-assisted Generation of Hardware Assertions,Yes.,1,"""In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions.""",2023,2023-06-24T17:44:36Z,,,
arXIv2023,Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data,Yes.,1,"""Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size.""",2023,2023-06-24T02:25:56Z,,,
arXIv2023,Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models,Yes.,2,"""Most previous studies on data reconstruction attacks have focused on LLM, while classification models were assumed to be more secure.""",2023,2023-06-23T21:25:38Z,,,
arXIv2023,LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding,Yes.,2,"""Additionally, we demonstrate that LACA can help refine prompts for deductive coding, identify codes for which an LLM is randomly guessing, and help assess when to use LLMs vs. human coders for deductive coding.""",2023,2023-06-23T20:57:32Z,,,
arXIv2023,Bring Your Own Data! Self-Supervised Evaluation for Large Language Models,Yes.,3,"""Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations.""",2023,2023-06-23T17:59:09Z,,,
arXIv2023,Comparing the Efficacy of Fine-Tuning and Meta-Learning for Few-Shot Policy Imitation,No.,1,"The abstract discusses few-shot imitation learning, control problems, and reinforcement learning but does not mention language models or their limitations.",2023,2023-06-23T15:29:15Z,,,
arXIv2023,Efficient Online Processing with Deep Neural Networks,Yes.,3,"""The economic cost and negative environmental externalities of training and serving models is in evident disharmony with financial viability and climate action goals.""",2023,2023-06-23T12:29:44Z,,,
arXIv2023,Product Information Extraction using ChatGPT,Yes.,2,"""The methods also struggle with generalizing to out-of-distribution attributes and attribute values that were not a part of the training data.""",2023,2023-06-23T09:30:01Z,,,
arXIv2023,Correcting discount-factor mismatch in on-policy policy gradient methods,No.,1,N/A (The paper does not mention language models).,2023,2023-06-23T04:10:58Z,,,
arXIv2023,Prompt to GPT-3: Step-by-Step Thinking Instructions for Humor Generation,Yes.,3,"""However, these models still have limitations when it comes to complex tasks that require an understanding of the user, such as mastering human comedy writing strategies.""",2023,2023-06-22T20:38:52Z,,,
arXIv2023,Towards Explainable Evaluation Metrics for Machine Translation,Yes.,2,"""recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent.""",2023,2023-06-22T17:07:57Z,,,
arXIv2023,Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing,Yes.,3,"""Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize.""",2023,2023-06-22T14:39:04Z,,,
arXIv2023,Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT,Yes.,1,"""This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology.""",2023,2023-06-22T13:21:20Z,,,
arXIv2023,Generative Multimodal Entity Linking,Yes.,2,"""Existing MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs).""",2023,2023-06-22T07:57:19Z,,,
arXIv2023,FLAG: Finding Line Anomalies (in code) with Generative AI,Yes.,1,"""In this work, we explore the features that help LLMs in this classification and evaluate the performance of FLAG on known bugs.""",2023,2023-06-22T03:04:56Z,,,
arXIv2023,Joint Prompt Optimization of Stacked LLMs using Variational Inference,Yes.,1,"""We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned.""",2023,2023-06-21T18:45:56Z,,,
arXIv2023,Testing of Detection Tools for AI-Generated Text,Yes.,2,"""Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content.""",2023,2023-06-21T16:29:44Z,,,
arXIv2023,GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks,Yes.,3,"""each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved.""",2023,2023-06-21T15:42:36Z,,,
arXIv2023,Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks,Yes.,3,"""Lastly, by utilizing a straightforward task in which the model predicts the winner of a Tic Tac Toe game, we identify limitations in attention analysis, particularly its inability to capture 2D patterns.""",2023,2023-06-21T11:48:07Z,,,
arXIv2023,OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue,Yes.,2,"""due to the significant differences between medical images and text and general web content, the performance of LMMs in medical scenarios is limited.""",2023,2023-06-21T11:09:48Z,,,
arXIv2023,Interactive Molecular Discovery with Natural Language,Yes.,2,"""Several typical solutions including large language models (e.g., ChatGPT) are evaluated, proving the challenge of conversational molecular design and the effectiveness of our knowledge enhancement method.""",2023,2023-06-21T02:05:48Z,,,
arXIv2023,Open-Domain Text Evaluation via Meta Distribution Modeling,Yes.,3,"""evaluating and controlling these models for desired attributes remains a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and METEOR are insufficient for open-ended generation tasks.""",2023,2023-06-20T20:37:54Z,,,
arXIv2023,A Simple and Effective Pruning Approach for Large Language Models,Yes.,3,"""Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive.""",2023,2023-06-20T17:18:20Z,,,
arXIv2023,Towards Environmentally Equitable AI via Geographical Load Balancing,Yes.,1,"""Fueled by the soaring popularity of large language and foundation models, the accelerated growth of artificial intelligence (AI) models' enormous environmental footprint has come under increased scrutiny.""",2023,2023-06-20T17:13:33Z,,,
arXIv2023,Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion,Yes.,1,"""Subsequently, the top two captions are fused using a Large Language Model (LLM).""",2023,2023-06-20T15:13:02Z,,,
arXIv2023,"Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Motivations and Formal Specifications",Yes.,3,"""It is conjectured that the shortcomings of current LLMs are due to a lack of ability to generalize.""",2023,2023-06-20T10:45:56Z,,,
arXIv2023,ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis,Yes.,3,"""This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging.""",2023,2023-06-20T05:20:29Z,,,
arXIv2023,Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting,Yes.,1,"""We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model.""",2023,2023-06-19T15:42:02Z,,,
arXIv2023,BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models,Yes.,3,"""the existing LLMs are usually focused on English, leading to inferior performance in non-English languages.""",2023,2023-06-19T14:30:52Z,,,
arXIv2023,Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis,No.,1,"The abstract focuses on Generative Adversarial Networks (GAN) and their application to text synthesis, without mentioning Large Language Models (LLMs).",2023,2023-06-19T10:22:12Z,,,
arXIv2023,Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost,Yes.,3,"""training cross-domain LLMs in the medical field poses significant challenges primarily attributed to the requirement of collecting data from diverse domains. This task becomes particularly difficult due to privacy restrictions and the scarcity of publicly available medical datasets.""",2023,2023-06-19T08:15:14Z,,,
arXIv2023,Fine-tuning Large Enterprise Language Models via Ontological Reasoning,Yes.,3,"""However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring business-level definitions and domain experience.""",2023,2023-06-19T06:48:45Z,,,
arXIv2023,The Importance of Human-Labeled Data in the Era of LLMs,Yes.,1,"""The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning.""",2023,2023-06-18T12:12:03Z,,,
arXIv2023,Can We Trust AI-Generated Educational Content? Comparative Analysis of Human and AI-Generated Learning Resources,Yes.,1,"""Large language models (LLMs) appear to offer a promising solution to the rapid creation of learning materials at scale, reducing the burden on instructors.""",2023,2023-06-18T09:49:21Z,,,
arXIv2023,CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents,Yes.,1,"""we first present an uncertainty estimation method for LLMs to classify whether the command is certain (i.e., clear) or not (i.e., ambiguous or infeasible).""",2023,2023-06-17T15:24:54Z,,,
arXIv2023,LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning,Yes.,1,"""We utilize a pretrained LLM for generating human-like captions with high quality.""",2023,2023-06-17T13:55:54Z,,,
arXIv2023,Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values,Yes.,3,"""dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models.""",2023,2023-06-16T20:07:38Z,,,
arXIv2023,Evaluating Superhuman Models with Consistency Checks,Yes.,3,"""GPT-4 forecasting that sports records will evolve non-monotonically over time.""",2023,2023-06-16T17:26:38Z,,,
arXIv2023,Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System,Yes.,3,"""However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models.""",2023,2023-06-16T13:04:56Z,,,
arXIv2023,Full Parameter Fine-tuning for Large Language Models with Limited Resources,Yes.,3,"""Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training.""",2023,2023-06-16T11:37:15Z,,,
arXIv2023,Schema-learning and rebinding as mechanisms of in-context learning and emergence,Yes.,3,"""Yet the mechanisms that underlie it are poorly understood."" and ""a key property of CSCGs is that, unlike transformer-based LLMs, they are interpretable, which considerably simplifies the task of explaining how ICL works.""",2023,2023-06-16T00:29:19Z,,,
arXIv2023,Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses,Yes.,3,"""Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader.""",2023,2023-06-15T22:12:34Z,,,
arXIv2023,Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models,Yes.,3,"""Existing VideoQA methods typically take two paradigms",2023,2023-06-15T20:56:20Z,,,
arXIv2023,SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts,Yes.,1,"""To overcome this challenge, we propose a set of best practices for using large language models (LLMs) to cheaply classify the comments at scale.""",2023,2023-06-15T17:59:47Z,,,
arXIv2023,Language-Guided Music Recommendation for Video via Prompt Analogies,Yes.,1,"""we propose a text-synthesis approach that relies on an analogy-based prompting procedure to generate natural language music descriptions from a large-scale language model (BLOOM-176B).""",2023,2023-06-15T17:58:01Z,,,
arXIv2023,Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization,Yes.,2,"""it is unclear whether they also make good teachers for weaker agents"" and ""communication is expensive.""",2023,2023-06-15T17:27:20Z,,,
arXIv2023,KoLA: Carefully Benchmarking World Knowledge of Large Language Models,Yes.,2,"""we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge hallucination.""",2023,2023-06-15T17:20:46Z,,,
arXIv2023,"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",Yes.,3,"""while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance.""",2023,2023-06-15T16:01:30Z,,,
arXIv2023,"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",Yes.,1,"""Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied.""",2023,2023-06-15T12:45:25Z,,,
arXIv2023,Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation,Yes.,3,"""PLMs can perform well in schema alignment but struggle to achieve complex reasoning, while LLMs is superior in complex reasoning tasks but cannot achieve precise schema alignment.""",2023,2023-06-15T06:50:51Z,,,
arXIv2023,Toward Grounded Commonsense Reasoning,Yes.,3,"""Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging.""",2023,2023-06-14T17:30:57Z,,,
arXIv2023,Language to Rewards for Robotic Skill Synthesis,Yes.,3,"""since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot.""",2023,2023-06-14T17:27:10Z,,,
arXIv2023,Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models,Yes.,1,"""Recently, chat systems powered by large language models (LLMs) emerge and rapidly become a promising direction to achieve AGI in natural language processing (NLP), but the path towards AGI in computer vision (CV) remains unclear.""",2023,2023-06-14T17:15:01Z,,,
arXIv2023,"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",Yes.,3,"""Despite this progress, complex visual-based tasks still remain challenging due to the diverse nature of visual tasks.""",2023,2023-06-14T17:12:56Z,,,
arXIv2023,MiniLLM: Knowledge Distillation of Large Language Models,Yes.,2,"""Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs)."" and ""How to effectively distill the knowledge of white-box LLMs into small models is still under-explored.""",2023,2023-06-14T14:44:03Z,,,
arXIv2023,Unifying Large Language Models and Knowledge Graphs: A Roadmap,Yes.,3,"""However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge.""",2023,2023-06-14T07:15:26Z,,,
arXIv2023,INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation,Yes.,2,"""rectifies quantization errors in quantized Large Language Models"" and ""ameliorate the gap between the quantized model and its float point counterpart.""",2023,2023-06-13T22:25:35Z,,,
arXIv2023,Large-scale Language Model Rescoring on Long-form Data,Yes.,1,"""In this work, we study the impact of Large-scale Language Models (LLM) on Automated Speech Recognition (ASR) of YouTube videos, which we use as a source for long-form ASR.""",2023,2023-06-13T20:54:12Z,,,
arXIv2023,AVIS: Autonomous Visual Information Seeking with Large Language Model Agent,Yes.,1,"""Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions.""",2023,2023-06-13T20:50:22Z,,,
arXIv2023,Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level,Yes.,2,"""Existing AI-generated text classifiers have limited accuracy and often produce false positives.""",2023,2023-06-13T20:34:55Z,,,
arXIv2023,"AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",Yes.,2,"""In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs.""",2023,2023-06-13T19:51:22Z,,,
arXIv2023,Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning,Yes.,2,"""We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 136% compared to solely beginner capabilities, with some of them even outperforming experts. However, ChatGPT does not provide consistent improvements across all scenarios.""",2023,2023-06-13T19:27:18Z,,,
arXIv2023,XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models,Yes.,3,"""their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images.""",2023,2023-06-13T17:59:59Z,,,
arXIv2023,arXiVeri: Automatic table verification with GPT,Yes.,2,"""Our findings highlight the complexity of this task, even for state-of-the-art LLMs like OpenAI's GPT-4.""",2023,2023-06-13T17:59:57Z,,,
arXIv2023,WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,Yes.,2,"""we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages.""",2023,2023-06-13T16:57:53Z,,,
arXIv2023,Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks,Yes.,2,"""With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results."" and ""our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human.""",2023,2023-06-13T16:46:24Z,,,
arXIv2023,Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models,Yes.,3,"""However, their proficiency within specialized domains such as biomolecular studies remains limited.""",2023,2023-06-13T14:35:34Z,,,
arXIv2023,NoCoLA: The Norwegian Corpus of Linguistic Acceptability,Yes.,2,"""we lack any tool to evaluate their understanding of grammaticality"" and ""conduct a comparative study of the existing Norwegian language models.""",2023,2023-06-13T14:11:19Z,,,
arXIv2023,"Assigning AI: Seven Approaches for Students, with Prompts",Yes.,3,"""despite their inherent risks and limitations"" and ""mitigate risks such as complacency about the AI's output, errors, and biases.""",2023,2023-06-13T03:36:36Z,,,
arXIv2023,Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine Translation Assessment,Yes.,2,"""GEMBA, the first MT quality assessment metric based on Large Language Models (LLMs), employs one-step prompting to achieve state-of-the-art (SOTA) in system-level MT quality estimation; however, it lacks segment-level analysis.""",2023,2023-06-13T01:18:32Z,,,
arXIv2023,Waffling around for Performance: Visual Classification with Random Words and Broad Concepts,Yes.,2,"""We conduct an extensive experimental study on the impact and shortcomings of additional semantics introduced with LLM-generated descriptors.""",2023,2023-06-12T17:59:48Z,,,
arXIv2023,Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow,Yes.,1,"""Considering that large language models (LLMs) have showcased promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while displaying and interacting in a human-friendly manner.""",2023,2023-06-12T16:12:56Z,,,
arXIv2023,Mitigating Prior Errors in Causal Structure Learning: Towards LLM driven Prior Knowledge,Yes.,3,"""to tackle erroneous prior causal statements from LLM, which is seldom considered in the current context of expert dominating prior resources.""",2023,2023-06-12T11:24:48Z,,,
arXIv2023,Weakly supervised information extraction from inscrutable handwritten document images,No.,1,The abstract does not mention LLMs or their limitations.,2023,2023-06-12T02:22:30Z,,,
arXIv2023,Multimodal Audio-textual Architecture for Robust Spoken Language Understanding,Yes.,3,"""we investigate impacts of this ASR error propagation on state-of-the-art NLU systems based on pre-trained language models (PLM), such as BERT and RoBERTa.""",2023,2023-06-12T01:55:53Z,,,
arXIv2023,Augmenting Greybox Fuzzing with Generative AI,Yes.,2,"""The experiment results show that our approach improves the edge coverage by 12.77\% over the SOTA greybox fuzzer (AFL++) on 12 target programs from three well-tested benchmarks. As for vulnerability detection, \sys is able to perform similar to or better than AFL++ for programs",2023,2023-06-11T21:44:47Z,,,
arXIv2023,Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis,Yes.,2,"""Prompt engineering has been shown to be effective for eliciting knowledge from an LLM, but alone it is insufficient for acquiring relevant, situationally grounded knowledge for an embodied agent learning novel tasks.""",2023,2023-06-11T20:50:14Z,,,
arXIv2023,The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases,Yes.,1,"""This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging.""",2023,2023-06-11T20:39:13Z,,,
arXIv2023,CoTran: An LLM-based Code Translator using Reinforcement Learning with Feedback from Compiler and Symbolic Execution,Yes.,3,"""Current LLM-based code translation methods lack a training approach to ensure that the translated code reliably compiles or bears substantial functional equivalence to the input code.""",2023,2023-06-11T19:47:52Z,,,
arXIv2023,GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model,Yes.,3,"""the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods.""",2023,2023-06-11T09:17:21Z,,,
arXIv2023,Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method,Yes.,2,"""The large scale of pre-trained language models poses a challenge for their deployment on various devices, with a growing emphasis on methods to compress these models, particularly knowledge distillation.""",2023,2023-06-11T08:53:27Z,,,
arXIv2023,RestGPT: Connecting Large Language Models with Real-World RESTful APIs,Yes.,3,"""existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios.""",2023,2023-06-11T08:53:12Z,,,
arXIv2023,Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective,Yes.,1,"""Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their great powerful capabilities in natural language understanding, generalization, and reasoning, which provides unprecedented opportunities to advance molecule discovery.""",2023,2023-06-11T08:16:25Z,,,
arXIv2023,Inductive reasoning in humans and large language models,Yes.,3,"""Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful",2023,2023-06-11T00:23:25Z,,,
arXIv2023,AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers,Yes.,3,"""However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal.""",2023,2023-06-10T21:58:29Z,,,
arXIv2023,Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification,Yes.,1,"""This study aims to explore the utilization of LLMs, specifically ChatGPT, for data augmentation to overcome the limited availability of annotated data for identifying the key factors in EHRs.""",2023,2023-06-10T20:55:21Z,,,
arXIv2023,Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,Yes.,3,"""However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains.""",2023,2023-06-10T12:42:36Z,,,
arXIv2023,Mind2Web: Towards a Generalist Agent for the Web,Yes.,3,"""While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs."" and ""there is still a substantial room to improve towards truly generalizable agents.""",2023,2023-06-09T17:44:31Z,,,
arXIv2023,FinGPT: Open-Source Financial Large Language Models,Yes.,2,"""Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs).""",2023,2023-06-09T16:52:00Z,,,
arXIv2023,Language Models Can Learn Exceptions to Syntactic Rules,Yes.,3,"""At the same time, this hypothesis fails to explain the magnitude of unpassivizability demonstrated by certain individual verbs, suggesting that other cues to exceptionality are available in the linguistic input.""",2023,2023-06-09T15:35:11Z,,,
arXIv2023,Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives,Yes.,1,"""The development of recent large language models (LLMs), particularly ChatGPT, has also introduced a revolutionary contribution to the way that legal texts can be processed and comprehended.""",2023,2023-06-09T11:57:57Z,,,
arXIv2023,How Can Recommender Systems Benefit from Large Language Models: A Survey,Yes.,3,"""we highlight key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics.""",2023,2023-06-09T11:31:50Z,,,
arXIv2023,Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation,Yes.,2,"""Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management.""",2023,2023-06-09T09:52:05Z,,,
arXIv2023,Customizing General-Purpose Foundation Models for Medical Report Generation,Yes.,2,"""the scarcity of labelled medical image-report pairs presents great challenges in the development of deep and large-scale neural networks capable of harnessing the potential artificial general intelligence power like large language models (LLMs).""",2023,2023-06-09T03:02:36Z,,,
arXIv2023,Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding,Yes.,3,"""Large Language Models (LLMs) have been applied in the speech domain, often incurring a performance drop due to misaligned between speech and language representations.""",2023,2023-06-08T22:33:22Z,,,
arXIv2023,Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization,Yes.,1,"""This work investigates the effectiveness of different pseudonymization techniques, ranging from rule-based substitutions to using pre-trained Large Language Models (LLMs), on a variety of datasets and models used for two widely used NLP tasks",2023,2023-06-08T21:06:19Z,,,
arXIv2023,Multi-Modal Classifiers for Open-Vocabulary Object Detection,Yes.,1,"""we prompt a large language model (LLM) to generate informative language descriptions for object classes.""",2023,2023-06-08T18:31:56Z,,,
arXIv2023,Artificial General Intelligence for Medical Imaging,Yes.,2,"""we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field.""",2023,2023-06-08T18:04:13Z,,,
arXIv2023,Grounded Text-to-Image Synthesis with Attention Refocusing,Yes.,3,"""However, these models still fail to precisely follow the text prompt involving multiple objects, attributes, or spatial compositions.""",2023,2023-06-08T17:59:59Z,,,
arXIv2023,MIMIC-IT: Multi-Modal In-Context Instruction Tuning,Yes.,1,"""Nevertheless, the current availability of vision-language instruction-response pairs in terms of quantity, diversity, and creativity remains limited, posing challenges to the generalization of interactive VLMs.""",2023,2023-06-08T17:59:56Z,,,
arXIv2023,ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases,Yes.,2,"""it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training.""",2023,2023-06-08T15:46:32Z,,,
arXIv2023,"PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance",Yes.,2,"""uncovering their strengths and weaknesses in handling critical financial tasks.""",2023,2023-06-08T14:20:29Z,,,
arXIv2023,Is AI the better programming partner? Human-Human Pair Programming vs. Human-AI pAIr Programming,Yes.,2,"""We find that the effectiveness of both approaches is mixed in the literature (though the measures used for pAIr programming are not as comprehensive).""",2023,2023-06-08T12:22:56Z,,,
arXIv2023,Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet,No.,1,"The abstract discusses generative AI tools like DALL-E, MidJourney, and ChatGPT but does not specifically focus on language models or their limitations.",2023,2023-06-08T11:14:51Z,,,
arXIv2023,PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization,Yes.,2,"""Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models.""",2023,2023-06-08T10:41:56Z,,,
arXIv2023,Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models,Yes.,2,"""large language models (LLMs) which excel at textual tasks, are probably not the best tool for modeling tabular data.""",2023,2023-06-08T09:12:28Z,,,
arXIv2023,Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models,Yes.,1,"""The experiments show that with the PLMs, the dependence on labeled training data has been greatly reduced, and the performance has improved. Meanwhile, we verify that ChatGPT, a renowned LLM, has potential for further advancement in this area.""",2023,2023-06-08T07:10:39Z,,,
arXIv2023,FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs,Yes.,1,"""We further demonstrate FedSecurity's utility and adaptability through federated training of Large Language Models (LLMs), showcasing its potential to impact a wide range of complex applications.""",2023,2023-06-08T06:21:35Z,,,
arXIv2023,covLLM: Large Language Models for COVID-19 Biomedical Literature,Yes.,1,"""A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing.""",2023,2023-06-08T04:08:32Z,,,
arXIv2023,Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning,Yes.,1,"""pre-trained large language models such as GPT-3 (low paid).""",2023,2023-06-08T04:04:47Z,,,
arXIv2023,Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts,Yes.,3,"""there is a large performance gap between supernet and training from scratch"" and ""supernet cannot be directly used and retraining is necessary after finding the optimal architectures.""",2023,2023-06-08T00:35:36Z,,,
arXIv2023,"Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers",Yes.,3,"""Our results indicate that using the PubMed 200K RCT dataset does not improve performance for the CODA-19 task. We also observe that while GPT-4 performs well, it does not outperform the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance of a dedicated and task-aligned datasets dataset for the target task.""",2023,2023-06-07T22:56:53Z,,,
arXIv2023,Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation,Yes.,2,"""Moreover, conventional VLP is limited to 2D images while medical images encompass diverse modalities, often in 3D, making the learning process more challenging.""",2023,2023-06-07T22:20:51Z,,,
arXIv2023,"A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises",Yes.,1,"""The recent advent of large language models (LLMs) has paved the way for building more comprehensive and accurate HKGs.""",2023,2023-06-07T21:51:56Z,,,
arXIv2023,INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models,Yes.,3,"""Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies.""",2023,2023-06-07T20:12:29Z,,,
arXIv2023,ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems,Yes.,3,"""complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems.""",2023,2023-06-07T19:37:55Z,,,
arXIv2023,"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",Yes.,3,"""We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks.""",2023,2023-06-07T17:47:03Z,,,
arXIv2023,StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code,Yes.,3,"""We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs.""",2023,2023-06-07T16:03:55Z,,,
arXIv2023,Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs,Yes.,3,"""We show that current German autoencoding models fine-tuned on translated NLI data can struggle on this challenge set, reflecting the fact that translated NLI datasets will not mirror all necessary language phenomena in the target language.""",2023,2023-06-07T15:33:07Z,,,
arXIv2023,Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering,Yes.,2,"""it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering.""",2023,2023-06-07T15:20:24Z,,,
arXIv2023,STEPS: A Benchmark for Order Reasoning in Sequential Tasks,Yes.,3,"""The commonsense reasoning of action orders in sequential tasks are challenging to resolve via zero-shot prompting or few-shot in-context learning for LLMs.""",2023,2023-06-07T13:58:55Z,,,
arXIv2023,"On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",Yes.,2,"""We start by examining the unsatisfactory performance of existing ChatGPT detecting tools and the challenges faced by human evaluators.""",2023,2023-06-07T12:33:24Z,,,
arXIv2023,Multilingual Clinical NER: Translation or Cross-lingual Transfer?,Yes.,2,"""Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language."" and ""While they can take advantage of monoling",2023,2023-06-07T12:31:07Z,,,
arXIv2023,Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks,Yes.,1,"""Finally, we scale up mPLUG-video based on the frozen Bloomz with only 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate impressive instruction and video understanding ability.""",2023,2023-06-07T11:52:36Z,,,
arXIv2023,GPT Self-Supervision for a Better Data Annotator,Yes.,3,"""significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist.""",2023,2023-06-07T11:33:14Z,,,
arXIv2023,"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results",Yes.,3,"""The findings revealed varying levels of agreement in chatbot's responses over time, with some scales displaying excellent agreement while others demonstrated poor agreement.""",2023,2023-06-07T10:14:17Z,,,
arXIv2023,Byk dil modellerinin Trke verisetleri ile eitilmesi ve ince ayarlanmas,Yes.,3,"""When it comes to Turkish language, open-access models do not provide satisfactory coverage. This is also observed over published datasets.""",2023,2023-06-06T19:31:08Z,,,
arXIv2023,Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction,Yes.,2,"""their widespread deployment is limited by the substantial quantities of task-specific data required for training.""",2023,2023-06-06T18:42:08Z,,,
arXIv2023,The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter,Yes.,2,"""With exploding parameter counts, Lottery Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size.""",2023,2023-06-06T15:49:09Z,,,
arXIv2023,Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models,Yes.,3,"""However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts.""",2023,2023-06-06T15:43:16Z,,,
arXIv2023,Towards End-to-end Speech-to-text Summarization,Yes.,2,"""the performance of the E2E model still lies behind the cascade one, which is object of an extensive analysis that includes future directions to close that gap.""",2023,2023-06-06T15:22:16Z,,,
arXIv2023,Language acquisition: do children and language models follow similar learning stages?,Yes.,1,"""we here compare the learning trajectories of deep language models to those of children.""",2023,2023-06-06T11:08:20Z,,,
arXIv2023,An Approach to Solving the Abstraction and Reasoning Corpus (ARC) Challenge,Yes.,1,"""We utilise the power of Large Language Models (LLMs), in particular GPT4, to be prompt engineered into performing an arbitrary task.""",2023,2023-06-06T10:08:12Z,,,
arXIv2023,Natural Language Commanding via Program Synthesis,Yes.,3,"""While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations.""",2023,2023-06-06T07:28:49Z,,,
arXIv2023,On the Role of Attention in Prompt-tuning,Yes.,2,"""Despite its success in LLMs, there is limited theoretical understanding of the power of prompt-tuning and the role of the attention mechanism in prompting.""",2023,2023-06-06T06:23:38Z,,,
arXIv2023,Prompting Large Language Models to Reformulate Queries for Moment Localization,Yes.,1,"""Inspired by the recent success of large language models, especially their ability of understanding and generating complex natural language contents, in this extended abstract, we make early attempts at reformulating the moment queries into a set of instructions using large language models and making them more friendly to the localization models.""",2023,2023-06-06T05:48:09Z,,,
arXIv2023,Inference-Time Intervention: Eliciting Truthful Answers from a Language Model,Yes.,3,"""Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.""",2023,2023-06-06T01:26:53Z,,,
arXIv2023,Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents,Yes.,3,"""Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations.""",2023,2023-06-05T23:55:37Z,,,
arXIv2023,shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation,Yes.,3,"""Instruction-tuned generative Large language models (LLMs) like ChatGPT and Bloomz possess excellent generalization abilities, but they face limitations in understanding radiology reports, particularly in the task of generating the IMPRESSIONS section from",2023,2023-06-05T21:33:04Z,,,
arXIv2023,Early Weight Averaging meets High Learning Rates for LLM Pre-training,Yes.,1,"""Training Large Language Models (LLMs) incurs significant cost; hence, any strategy that accelerates model convergence is helpful.""",2023,2023-06-05T20:51:44Z,,,
arXIv2023,NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks,Yes.,2,"""Finetuning large language models inflates the costs of NLU applications and remains the bottleneck of development cycles.""",2023,2023-06-05T19:30:41Z,,,
arXIv2023,ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs,Yes.,1,"""Results demonstrate two ways to effectively increase the accuracy of mapping suggestions without modifying the underlying AI models.""",2023,2023-06-05T19:26:21Z,,,
arXIv2023,InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models,Yes.,1,"""it can be challenging to find the best instruction for different situations, especially for black-box LLMs on which backpropagation is forbidden.""",2023,2023-06-05T17:55:22Z,,,
arXIv2023,SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression,Yes.,3,"""quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments.""",2023,2023-06-05T17:53:28Z,,,
arXIv2023,Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on Japanese Honorific Conversion,Yes.,3,"""It remains unclear whether pre-trained large language models (LLMs) can flexibly handle Japanese honorifics like humans.""",2023,2023-06-05T17:27:48Z,,,
arXIv2023,SelfEvolve: A Code Evolution Framework via Large Language Models,Yes.,3,"""the performance of these retrieval-based methods is limited by the strength of the retrievers used"" and ""while LLMs show great emergent ability, they still struggle to produce the correct code in one turn.""",2023,2023-06-05T14:12:46Z,,,
arXIv2023,Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs,Yes.,1,"""Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents.""",2023,2023-06-05T13:35:01Z,,,
arXIv2023,Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding,Yes.,1,"""Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video.""",2023,2023-06-05T13:17:27Z,,,
arXIv2023,Cheap-fake Detection with LLM using Prompt Engineering,Yes.,1,"""We enhance the baseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as a feature extractor.""",2023,2023-06-05T11:01:00Z,,,
arXIv2023,Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in Australia,Yes.,3,"""The findings highlight the promising potential of LLMs across various performance criteria, including relevance, accuracy, and applicability, though gaps remain in areas such as completeness and clarity.""",2023,2023-06-05T06:01:00Z,,,
arXIv2023,User Behavior Simulation with Large Language Model based Agents,Yes.,1,"""large language models (LLMs) can achieve human-like intelligence"" and ""we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors.""",2023,2023-06-05T02:58:35Z,,,
arXIv2023,Evaluation of AI Chatbots for Patient-Specific EHR Questions,Yes.,1,"""This paper investigates the use of artificial intelligence chatbots for patient-specific question answering (QA) from clinical notes using several large language model (LLM) based systems",2023,2023-06-05T02:52:54Z,,,
arXIv2023,Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning,Yes.,3,"""we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to wrong answers.""",2023,2023-06-04T17:02:59Z,,,
arXIv2023,Commonsense Knowledge Transfer for Pre-trained Language Models,Yes.,3,"""pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text.""",2023,2023-06-04T15:44:51Z,,,
arXIv2023,OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models,Yes.,2,"""Large language models (LLMs) with hundreds of billions of parameters require powerful server-grade GPUs for inference, limiting their practical deployment.""",2023,2023-06-04T06:33:13Z,,,
arXIv2023,Large Language Model Augmented Narrative Driven Recommendations,Yes.,1,"""In this work, we explore using large language models (LLMs) for data augmentation to train NDR models.""",2023,2023-06-04T03:46:45Z,,,
arXIv2023,Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model,Yes.,3,"""Despite its success, an embedded vector (Sen2Vec) representing a point estimate does not naturally express uncertainty in a taskagnostic way.""",2023,2023-06-04T03:26:43Z,,,
arXIv2023,Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions,Yes.,3,"""Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties.""",2023,2023-06-04T01:07:20Z,,,
arXIv2023,SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts,Yes.,3,"""the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation.""",2023,2023-06-03T22:35:27Z,,,
arXIv2023,Towards Coding Social Science Datasets with Language Models,Yes.,1,"""Recent advances in a specific kind of artificial intelligence tool - language models (LMs) - provide a solution to this problem.""",2023,2023-06-03T19:11:34Z,,,
arXIv2023,Unsupervised Human Activity Recognition through Two-stage Prompting with ChatGPT,Yes.,3,"""previous prompt engineering for ChatGPT exhibits limited generalization ability when dealing with a list of words (i.e., sequence of objects) due to the similar weighting assigned to each word in the list.""",2023,2023-06-03T15:41:59Z,,,
arXIv2023,Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions,Yes.,1,"""we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task.""",2023,2023-06-03T14:57:47Z,,,
arXIv2023,MultiLegalPile: A 689GB Multilingual Legal Corpus,Yes.,1,"""Large, high-quality datasets are crucial for training Large Language Models (LLMs).""",2023,2023-06-03T10:10:38Z,,,
arXIv2023,LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas,Yes.,1,"""Our experiments show that LambdaBeam outperforms neural, symbolic, and LLM-based techniques in an integer list manipulation domain.""",2023,2023-06-03T08:24:53Z,,,
arXIv2023,On Optimal Caching and Model Multiplexing for Large Model Inference,Yes.,3,"""Large Language Models (LLMs) and other large foundation models have achieved noteworthy success, but their size exacerbates existing resource consumption and latency challenges. In particular, the large-scale deployment of these models is hindered by the significant resource requirements during inference.""",2023,2023-06-03T05:01:51Z,,,
arXIv2023,Can Contextual Biasing Remain Effective with Whisper and GPT-2?,Yes.,3,"""Despite the large amount of training data, infrequent content words that occur in a particular task may still exhibit poor ASR performance, with contextual biasing a possible remedy.""",2023,2023-06-02T22:56:01Z,,,
arXIv2023,MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates,Yes.,3,"""Hence they exhibit poor scalability and performance in transformer models, e.g. large language models (LLMs), because the batch sizes in these models scale by the attention mechanism sequence length, leading to large model size and batch sizes.""",2023,2023-06-02T17:00:19Z,,,
arXIv2023,Harnessing large-language models to generate private synthetic text,Yes.,3,"""However, generating private synthetic data is much harder than training a private model."" and ""executing it has proven problematic. Previous approaches either show significant performance loss, or have, as we show, critical design flaws.""",2023,2023-06-02T16:59:36Z,,,
arXIv2023,Log Parsing: How Far Can ChatGPT Go?,Yes.,3,"""However, its performance in automated log parsing remains unclear."" and ""Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.""",2023,2023-06-02T14:58:43Z,,,
arXIv2023,EmoUS: Simulating User Emotions in Task-Oriented Dialogues,Yes.,1,"""Developing such methods is important in the age of large language model chat-bots and rising ethical concerns.""",2023,2023-06-02T14:48:19Z,,,
arXIv2023,PassGPT: Password Modeling and (Guided) Generation with Large Language Models,Yes.,1,"""Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision.""",2023,2023-06-02T13:49:53Z,,,
arXIv2023,Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations,Yes.,1,"""we leverage the recent advances in large language models and design a new prompt learning mechanism to generate aspects for the end recommendation task.""",2023,2023-06-02T12:00:03Z,,,
arXIv2023,ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?,Yes.,3,"""Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems.""",2023,2023-06-02T09:15:01Z,,,
arXIv2023,An Empirical Study on Challenging Math Problem Solving with GPT-4,Yes.,1,"""Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields.""",2023,2023-06-02T08:02:15Z,,,
arXIv2023,MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models,Yes.,1,"""Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct",2023,2023-06-02T07:21:03Z,,,
arXIv2023,Egocentric Planning for Scalable Embodied Task Achievement,Yes.,3,"""This work offers a solid baseline for studying end-to-end and hybrid methods that aim to generalize to new tasks, including recent approaches relying on LLMs, but often struggle to scale to long sequences of actions or produce robust plans for novel tasks.""",2023,2023-06-02T06:41:24Z,,,
arXIv2023,KL-Divergence Guided Temperature Sampling,Yes.,3,"""As temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual.""",2023,2023-06-02T06:11:26Z,,,
arXIv2023,Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators,Yes.,3,"""The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion."" and ""We further propose and compare two paradigms for implementing the first two capabilities. One is to leverage the generic knowledge of LLMs themselves via prompt engineering while the other is to adopt domain",2023,2023-06-02T02:42:58Z,,,
arXIv2023,LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization,Yes.,1,"""Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks.""",2023,2023-06-01T19:33:21Z,,,
arXIv2023,AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration,Yes.,3,"""Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth).""",2023,2023-06-01T17:59:10Z,,,
arXIv2023,Vocabulary-free Image Classification,Yes.,2,"""Despite showing impressive zero-shot capabilities, a pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time for composing the textual prompts. However, such assumption can be impractical when the semantic context is unknown and evolving.""",2023,2023-06-01T17:19:43Z,,,
arXIv2023,LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,Yes.,1,"""Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text.""",2023,2023-06-01T16:50:07Z,,,
arXIv2023,"Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers",Yes.,1,"""utilizing the powerful capabilities of large language models for choosing the suitable trigger and text-guided image editing techniques for generating the poisoned image with the trigger.""",2023,2023-06-01T15:42:06Z,,,
arXIv2023,GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?,Yes.,2,"""However, considering the prohibitively high memory and computational cost for implementing such a large model, the conventional models (such as CNN and ViT), are still essential for many visual perception tasks.""",2023,2023-06-01T14:02:45Z,,,
arXIv2023,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,Yes.,3,"""Current research commonly fine-tunes a text-based pre-trained language model on a small downstream dataset that is annotated with labeled graphs. However, due to the limited scale of available datasets, this approach may prove to be insufficient in bridging the gap between natural language text and structured graphs.""",2023,2023-06-01T13:20:22Z,,,
arXIv2023,ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing,Yes.,3,"""The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs.""",2023,2023-06-01T12:45:53Z,,,
arXIv2023,Analysis of ChatGPT on Source Code,Yes.,3,"""While these models can save time and provide highly accurate results, they are not yet advanced enough to replace human programmers entirely.""",2023,2023-06-01T12:12:59Z,,,
arXIv2023,Chain-Of-Thought Prompting Under Streaming Batch: A Case Study,Yes.,3,"""However, developing effective prompts can be a challenging and labor-intensive task."" and ""Most of them assume that all test data is visible before testing and only select a small subset to generate rationales, which is an unrealistic assumption.""",2023,2023-06-01T11:11:39Z,,,
arXIv2023,CapText: Large Language Model-based Caption Generation From Image Context and Description,Yes.,1,"""We propose and evaluate a new approach, which leverages existing large language models to generate captions from textual descriptions and context alone, without ever processing the image directly.""",2023,2023-06-01T02:40:44Z,,,
arXIv2023,An Invariant Learning Characterization of Controlled Text Generation,Yes.,3,"""researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text"" and ""the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on.""",2023,2023-05-31T21:35:08Z,,,
arXIv2023,Measuring the Robustness of NLP Models to Domain Shifts,Yes.,3,"""both model types suffer from drops upon domain shifts"" and ""few-shot LLMs often surpass them cross-domain, showing better robustness.""",2023,2023-05-31T20:25:08Z,,,
arXIv2023,"Better patching using LLM prompting, via Self-Consistency",Yes.,2,"""Unfortunately, the use of this highly-performant S-C (or even CoT) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations.""",2023,2023-05-31T18:28:46Z,,,
arXIv2023,Improving CLIP Training with Language Rewrites,Yes.,3,"""Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image."" and ""However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image.""",2023,2023-05-31T17:59:04Z,,,
arXIv2023,Scaling Evidence-based Instructional Design Expertise through Large Language Models,Yes.,3,"""We discuss the benefits and limitations of AI-driven content generation, emphasizing the necessity of human oversight in ensuring the quality of educational materials.""",2023,2023-05-31T17:54:07Z,,,
arXIv2023,Decision-Oriented Dialogue for Human-AI Collaboration,Yes.,3,"""we highlight a number of challenges models face in decision-oriented dialogues, ranging from efficient communication to reasoning and optimization.""",2023,2023-05-31T17:50:02Z,,,
arXIv2023,Revisiting the Reliability of Psychological Scales on Large Language Models,Yes.,2,"""However, the suitability of employing psychological scales, initially devised for humans, on LLMs is a matter of ongoing debate.""",2023,2023-05-31T15:03:28Z,,,
arXIv2023,Neuron to Graph: Interpreting Language Model Neurons at Scale,Yes.,2,"""Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown.""",2023,2023-05-31T14:44:33Z,,,
arXIv2023,CodeTF: One-stop Transformer Library for State-of-the-art Code LLM,Yes.,2,"""the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption.""",2023,2023-05-31T05:24:48Z,,,
arXIv2023,Catalysis distillation neural network for the few shot open catalyst challenge,No.,1,The abstract focuses on the integration of artificial intelligence in computational chemistry methods and does not discuss language models.,2023,2023-05-31T04:23:56Z,,,
arXIv2023,Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning,Yes.,1,"""Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources.""",2023,2023-05-31T03:18:03Z,,,
arXIv2023,PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning,Yes.,3,"""While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues.""",2023,2023-05-31T00:55:40Z,,,
arXIv2023,Stable Anisotropic Regularization,Yes.,2,"""Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space).""",2023,2023-05-30T18:57:45Z,,,
arXIv2023,GPT4GEO: How a Language Model Sees the World's Geography,Yes.,3,"""We provide a broad characterisation of what GPT-4 (without plugins or Internet access) knows about the world, highlighting both potentially surprising capabilities but also limitations.""",2023,2023-05-30T18:28:04Z,,,
arXIv2023,SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models,Yes.,1,"""With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal.""",2023,2023-05-30T17:59:30Z,,,
arXIv2023,Grammar Prompting for Domain-Specific Language Generation with Large Language Models,Yes.,3,"""However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars.""",2023,2023-05-30T17:26:01Z,,,
arXIv2023,Controlled Text Generation with Hidden Representation Transformations,Yes.,1,"""We propose CHRT (Control Hidden Representation Transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity).""",2023,2023-05-30T17:21:17Z,,,
arXIv2023,Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models,Yes.,3,"""gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG.""",2023,2023-05-30T16:31:26Z,,,
arXIv2023,LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images,Yes.,1,"""Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights.""",2023,2023-05-30T16:09:16Z,,,
arXIv2023,Does Conceptual Representation Require Embodiment? Insights From Large Language Models,Yes.,3,"""These results highlight the limitations of language in isolation, and that the integration of diverse modalities of inputs leads to a more human-like conceptual representation.""",2023,2023-05-30T15:06:28Z,,,
arXIv2023,Multitask learning for recognizing stress and depression in social media,No.,1,No mention of language models in the abstract or title.,2023,2023-05-30T10:04:01Z,,,
arXIv2023,AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation,Yes.,1,"""To address this issue, we propose to automatically collect a cognitive robot dataset by Large Language Models (LLMs).""",2023,2023-05-30T09:54:20Z,,,
arXIv2023,GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction,Yes.,1,"""This paper aims to efficiently enable Large Language Models (LLMs) to use multimodal tools.""",2023,2023-05-30T05:27:21Z,,,
arXIv2023,AdapterEM: Pre-trained Language Model Adaptation for Generalized Entity Matching using Adapter-tuning,Yes.,3,"""sequential fine-tuning of overparameterized PrLMs can lead to catastrophic forgetting, especially in low-resource scenarios.""",2023,2023-05-30T04:03:23Z,,,
arXIv2023,Controllable Text-to-Image Generation with GPT-4,Yes.,1,"""Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning.""",2023,2023-05-29T19:56:47Z,,,
arXIv2023,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,Yes.,3,"""achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training."" and ""RLHF is a complex and often unstable procedure.""",2023,2023-05-29T17:57:46Z,,,
arXIv2023,LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections,Yes.,3,"""However, despite these great advances, the performance of these zeroshot classifiers still falls short of the results of dedicated (closed category set) classifiers trained with supervised fine tuning.""",2023,2023-05-29T17:56:35Z,,,
arXIv2023,Contextual Object Detection with Multimodal Large Language Models,Yes.,3,"""Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection.""",2023,2023-05-29T17:50:33Z,,,
arXIv2023,Beyond Confidence: Reliable Models Should Also Consider Atypicality,Yes.,3,"""we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models.""",2023,2023-05-29T17:37:09Z,,,
arXIv2023,Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning,Yes.,2,"""designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem (MWP) solving.""",2023,2023-05-29T16:01:40Z,,,
arXIv2023,LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning,Yes.,2,"""However, these models often struggle when fine-tuned on small datasets.""",2023,2023-05-29T15:59:51Z,,,
arXIv2023,Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods,Yes.,2,"""However, applying prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks and their controllability remains underexplored.""",2023,2023-05-29T15:31:29Z,,,
arXIv2023,Multiscale Positive-Unlabeled Detection of AI-Generated Texts,Yes.,3,"""mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews.""",2023,2023-05-29T15:25:00Z,,,
arXIv2023,Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models,Yes.,3,"""current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion.""",2023,2023-05-29T15:14:09Z,,,
arXIv2023,ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback,Yes.,1,"""Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery.""",2023,2023-05-29T14:43:24Z,,,
arXIv2023,VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset,Yes.,1,"""we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions.""",2023,2023-05-29T14:34:50Z,,,
arXIv2023,ANPL: Towards Natural Programming with Interactive Decomposition,Yes.,3,"""Though LLMs are capable of generating plausible programs, it's challenging to interact with the LLMs further to revise the program, especially if the user's specific requirements are different from the initial proposal.""",2023,2023-05-29T14:19:40Z,,,
arXIv2023,BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages,Yes.,3,"""many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored.""",2023,2023-05-29T14:07:52Z,,,
arXIv2023,Game of Tones: Faculty detection of GPT-4 generated content in university assessments,Yes.,2,"""This suggests that the use of adversarial techniques regarding prompt engineering is an effective method in evading AI detection tools and highlights that improvements to AI detection software are needed.""",2023,2023-05-29T13:31:58Z,,,
arXIv2023,Image Captioning with Multi-Context Synthetic Data,Yes.,3,"""However, existing methods struggle to attain satisfactory performance solely through synthetic data.""",2023,2023-05-29T13:18:59Z,,,
arXIv2023,A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets,Yes.,3,"""even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks.""",2023,2023-05-29T12:37:21Z,,,
arXIv2023,Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation,Yes.,3,"""Large diffusion models have been successful in text-to-audio (T2A) synthesis tasks, but they often suffer from common issues such as semantic misalignment and poor temporal consistency due to limited natural language understanding and data scarcity.""",2023,2023-05-29T10:41:28Z,,,
arXIv2023,LLM-QAT: Data-Free Quantization Aware Training for Large Language Models,Yes.,3,"""We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further.""",2023,2023-05-29T05:22:11Z,,,
arXIv2023,Baselines for Identifying Watermarked Large Language Models,Yes.,1,"""We consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs).""",2023,2023-05-29T04:26:16Z,,,
arXIv2023,Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models,Yes.,2,"""Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful.""",2023,2023-05-29T04:19:35Z,,,
arXIv2023,Taming AI Bots: Controllability of Neural States in Large Language Models,Yes.,3,"""The fact that AI bots are controllable means that an adversary could steer them towards any state.""",2023,2023-05-29T03:58:33Z,,,
arXIv2023,Transfer Learning for Power Outage Detection Task with Limited Training Data,Yes.,3,"""Results show that while classical models outperform zero-shot Language Models, few-shot fine-tuning significantly improves their performance."" and ""Our evaluation provides insights into the potential of few-shot fine-tuning with Language Models for power outage detection, highlighting their strengths and limitations.""",2023,2023-05-28T22:36:35Z,,,
arXIv2023,Targeted Data Generation: Finding and Fixing Model Weaknesses,Yes.,3,"""Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust.""",2023,2023-05-28T19:36:50Z,,,
arXIv2023,Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR,Yes.,1,"""We address this limitation by distilling punctuation knowledge from a bidirectional teacher language model (LM) trained on written, punctuated text.""",2023,2023-05-28T19:31:45Z,,,
arXIv2023,Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data,Yes.,1,"""the results are validated through language models trained on biomedical literature, such as BlueBERT and other large language models trained on medical corpora.""",2023,2023-05-28T17:07:46Z,,,
arXIv2023,Conformal Prediction with Large Language Models for Multi-Choice Question Answering,Yes.,2,"""We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications.""",2023,2023-05-28T15:26:10Z,,,
arXIv2023,LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning,Yes.,2,"""their deployment is still hindered by the vast model scale and computational costs.""",2023,2023-05-28T15:15:48Z,,,
arXIv2023,Breaking Language Barriers with a LEAP: Learning Strategies for Polyglot LLMs,Yes.,3,"""their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages.""",2023,2023-05-28T14:48:38Z,,,
arXIv2023,FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions,Yes.,2,"""However, these models frequently produce generic captions and may omit semantically important image details.""",2023,2023-05-28T13:16:03Z,,,
arXIv2023,LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers,Yes.,3,"""these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs.""",2023,2023-05-28T13:08:13Z,,,
arXIv2023,Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks,Yes.,3,"""However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy."" and ""these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required.""",2023,2023-05-28T13:00:00Z,,,
arXIv2023,Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds,Yes.,1,"""Could we leverage the recent advances on pre-trained Large Language Models (LLMs) to enable classical planning systems to deal with novel situations?""",2023,2023-05-27T22:30:15Z,,,
arXIv2023,What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks,Yes.,3,"""In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks.""",2023,2023-05-27T14:17:33Z,,,
arXIv2023,Query-Efficient Black-Box Red Teaming via Bayesian Optimization,Yes.,3,"""The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways."" and ""Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating any information from past evaluations,",2023,2023-05-27T11:00:15Z,,,
arXIv2023,Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making,Yes.,3,"""However, the adversarial attack task has found that PLMs are vulnerable to small perturbations.""",2023,2023-05-27T10:33:53Z,,,
arXIv2023,SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,Yes.,1,"""SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance.""",2023,2023-05-27T07:04:15Z,,,
arXIv2023,Improving Generalization in Language Model-Based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-Based Techniques,Yes.,3,"""Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs).""",2023,2023-05-27T06:09:03Z,,,
arXIv2023,Augmenting Large Language Model Translators via Translation Memories,Yes.,1,"""We find that the ability of LLMs to ``understand'' prompts is indeed helpful for making better use of TMs.""",2023,2023-05-27T04:47:09Z,,,
arXIv2023,DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text,Yes.,3,"""Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power.""",2023,2023-05-27T03:58:29Z,,,
arXIv2023,Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance,Yes.,3,"""This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models."" and ""Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.""",2023,2023-05-26T23:46:42Z,,,
arXIv2023,Improved Instruction Ordering in Recipe-Grounded Conversation,Yes.,3,"""Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order."" and ""we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about half of which are out-of-order",2023,2023-05-26T21:57:11Z,,,
arXIv2023,SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL (extended),Yes.,3,"""Through comprehensive ablations and error analyses, we shed light on the strengths and weaknesses of our framework, offering valuable insights into Text-to-SQL's future work.""",2023,2023-05-26T21:39:05Z,,,
arXIv2023,Large language models improve Alzheimer's disease diagnosis using multi-modality data,Yes.,1,"""We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.""",2023,2023-05-26T18:42:19Z,,,
arXIv2023,Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time,Yes.,3,"""Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is",2023,2023-05-26T17:39:58Z,,,
arXIv2023,Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models,Yes.,3,"""recent studies, have found that their ability to plan remains questionable"" and ""the performance of a finetuned baseline remains poor because it violates pre-conditions of actions in the plans that it generates.""",2023,2023-05-26T16:36:55Z,,,
arXIv2023,Mindstorms in Natural Language-Based Societies of Mind,Yes.,1,"""Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so, they overcome the limitations of single LLMs, improving multimodal zero-shot reasoning.""",2023,2023-05-26T16:21:25Z,,,
arXIv2023,A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks,Yes.,3,"""Our empirical findings validate the challenge of segmentation.""",2023,2023-05-26T15:49:43Z,,,
arXIv2023,NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models,Yes.,2,"""Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models.""",2023,2023-05-26T14:41:06Z,,,
arXIv2023,Large Language Models Are Partially Primed in Pronoun Interpretation,Yes.,3,"""though in a limited fashion",2023,2023-05-26T13:30:48Z,,,
arXIv2023,HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis,Yes.,2,"""concerns arise over their potential to compromise academic integrity"" and ""emphasizes the critical need for effective strategies to uphold academic integrity amidst the growing influence of LLMs.""",2023,2023-05-26T11:07:25Z,,,
arXIv2023,Do GPTs Produce Less Literal Translations?,Yes.,2,"""However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models.""",2023,2023-05-26T10:38:31Z,,,
arXIv2023,Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media,Yes.,3,"""Despite the fact that transformer-based models achieve noticeable improvements, they cannot often capture rich factual knowledge.""",2023,2023-05-26T10:19:04Z,,,
arXIv2023,Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning,Yes.,2,"""Although there are numerous advantages of this generative model, it comes with some reasonable concerns as well.""",2023,2023-05-26T09:27:43Z,,,
arXIv2023,Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks,Yes.,3,"""Our findings demonstrate that ChatGPT performs well even without labeled data but fine-tuned models generally outperform it."" and ""Our research also highlights how annotating with generative models can be time-intensive.""",2023,2023-05-26T05:13:01Z,,,
arXIv2023,Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model,Yes.,3,"""Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance."" and ""DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues.""",2023,2023-05-26T04:23:10Z,,,
arXIv2023,AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization,Yes.,2,"""However, LLMs require massive amounts of data and resources and are challenging to implement as offline applications.""",2023,2023-05-26T03:44:35Z,,,
arXIv2023,Heterogeneous Value Alignment Evaluation for Large Language Models,Yes.,3,"""current methodologies typically attempt to assign value as an attribute to LLMs, yet lack attention to the ability to pursue value and the importance of transferring heterogeneous values in specific practical applications.""",2023,2023-05-26T02:34:20Z,,,
arXIv2023,Neural Task Synthesis for Visual Programming,No.,1,The abstract focuses on generative neural models and does not mention LLMs or their limitations.,2023,2023-05-26T01:08:18Z,,,
arXIv2023,CONA: A novel CONtext-Aware instruction paradigm for communication using large language model,Yes.,1,"""We introduce CONA, a novel context-aware instruction paradigm for effective knowledge dissemination using generative pre-trained transformer (GPT) models.""",2023,2023-05-26T00:53:18Z,,,
arXIv2023,On the Tool Manipulation Capability of Open-source Large Language Models,Yes.,3,"""By analyzing common tool manipulation failures, we first demonstrate that open-source LLMs may require training with usage examples, in-context demonstration and generation style regulation to resolve failures.""",2023,2023-05-25T22:10:20Z,,,
arXIv2023,Coarse-Tuning Models of Code with Reinforcement Learning Feedback,Yes.,3,"""However, these models are trained using next-token prediction, which ignores the syntax and semantics of code.""",2023,2023-05-25T22:09:08Z,,,
arXIv2023,Context-aware attention layers coupled with optimal transport domain adaptation and multimodal fusion methods for recognizing dementia from spontaneous speech,Yes.,1,"""Next, we pass each transcript and image through BERT and DeiT models respectively.""",2023,2023-05-25T18:18:09Z,,,
arXIv2023,Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory,Yes.,1,"""This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments.""",2023,2023-05-25T17:59:49Z,,,
arXIv2023,UFO: Unified Fact Obtaining for Commonsense Question Answering,Yes.,1,"""Recently, large-scale language models (LLMs) have dramatically improved the intelligence in capturing and leveraging knowledge, which opens up a new way to address the issue of eliciting knowledge from language models.""",2023,2023-05-25T13:25:49Z,,,
arXIv2023,Linguistic Properties of Truthful Response,Yes.,3,"""We investigate the phenomenon of an LLM's untruthful response using a large set of 220 handcrafted linguistic features."" and ""the limited scope of our experiments must be taken into account in interpreting the results.""",2023,2023-05-25T09:17:39Z,,,
arXIv2023,ChatGPT for PLC/DCS Control Logic Generation,Yes.,1,"""It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks.""",2023,2023-05-25T07:46:53Z,,,
arXIv2023,Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback,Yes.,1,"""Recent advancements in Large Language Models (LLMs) have demonstrated impressive reasoning, conversational, and zero-shot generation abilities across various domains.""",2023,2023-05-25T07:43:39Z,,,
arXIv2023,RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting,Yes.,3,"""However, as LLMs are primarily trained on final text results rather than intermediate revisions, it might be challenging for them to perform text rewriting tasks.""",2023,2023-05-25T03:26:26Z,,,
arXIv2023,Undetectable Watermarks for Language Models,Yes.,1,"""Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text.""",2023,2023-05-25T02:57:16Z,,,
arXIv2023,Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation,Yes.,1,"""This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU.""",2023,2023-05-24T19:59:51Z,,,
arXIv2023,Large Language Models are Few-Shot Health Learners,Yes.,3,"""However, language alone is limited. While existing LLMs excel at text-based inferences, health applications require that models be grounded in numerical data (e.g., vital signs, laboratory values in clinical domains; steps, movement in the wellness domain) that is not easily or readily expressed as text in existing training corpus.""",2023,2023-05-24T19:25:16Z,,,
arXIv2023,Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective,Yes.,3,"""bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length.""",2023,2023-05-24T17:59:21Z,,,
arXIv2023,LayoutGPT: Compositional Visual Planning and Generation with Large Language Models,Yes.,1,"""To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models.""",2023,2023-05-24T17:56:16Z,,,
arXIv2023,Visual Programming for Text-to-Image Generation and Evaluation,Yes.,2,"""Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes.""",2023,2023-05-24T16:42:17Z,,,
arXIv2023,Revisiting Token Dropping Strategy in Efficient BERT Pretraining,Yes.,3,"""we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks.""",2023,2023-05-24T15:59:44Z,,,
arXIv2023,EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models,Yes.,3,"""Recent studies have begun leveraging large language models (LLMs) to address event semantic processing. However, the extent that LLMs can effectively tackle these challenges remains uncertain.""",2023,2023-05-24T15:55:40Z,,,
arXIv2023,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,Yes.,3,"""With the rapid growth in model size, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage.""",2023,2023-05-24T15:52:08Z,,,
arXIv2023,Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM,Yes.,1,"""We present a novel approach to adapting pre-trained large language models (LLMs) to perform question answering (QA) and speech continuation.""",2023,2023-05-24T15:39:43Z,,,
arXIv2023,"Machine Unlearning: its nature, scope, and importance for a ""delete culture""",Yes.,3,"""The article explores the cultural shift from recording to deleting information in the digital age and its implications on privacy, intellectual property (IP), and Large Language Models like ChatGPT."" and ""However, potential ethical risks, such as misuse, overuse, and underuse of MU, should be",2023,2023-05-24T15:27:04Z,,,
arXIv2023,SAIL: Search-Augmented Instruction Learning,Yes.,3,"""Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information.""",2023,2023-05-24T15:07:30Z,,,
arXIv2023,SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation,Yes.,2,"""Human evaluation results show that some machine-generated summaries are comparable to human-written reviews, while revealing the challenges of automatic literature review generation such as hallucinations and a lack of detailed information.""",2023,2023-05-24T14:26:30Z,,,
arXIv2023,STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models,Yes.,1,"""We propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance.""",2023,2023-05-24T12:15:19Z,,,
arXIv2023,Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models,Yes.,3,"""While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement.""",2023,2023-05-24T11:59:13Z,,,
arXIv2023,Contrastive Learning of Sentence Embeddings from Scratch,Yes.,1,"""Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch).""",2023,2023-05-24T11:56:21Z,,,
arXIv2023,"HuatuoGPT, towards Taming Language Model to Be a Doctor",Yes.,3,"""The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis.""",2023,2023-05-24T11:56:01Z,,,
arXIv2023,Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References,Yes.,1,"""We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible.""",2023,2023-05-24T11:53:29Z,,,
arXIv2023,Who Wrote this Code? Watermarking for Code Generation,Yes.,2,"""ethical and legal concerns about using them have been raised, such as plagiarism and copyright issues"" and ""we discover that the previous methods fail to function appropriately with code generation tasks because of the syntactic and semantic characteristics of code.""",2023,2023-05-24T11:49:52Z,,,
arXIv2023,Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science,Yes.,3,"""its generative distribution often differs from the distribution of real-world data researchers care about (in other words, it is unfaithful).""",2023,2023-05-24T11:27:59Z,,,
arXIv2023,Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations,Yes.,3,"""However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools.""",2023,2023-05-24T11:22:34Z,,,
arXIv2023,ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories,Yes.,3,"""However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge.""",2023,2023-05-24T11:14:31Z,,,
arXIv2023,Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models,Yes.,3,"""existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning.""",2023,2023-05-24T11:06:15Z,,,
arXIv2023,Estimating Class Separability of Datasets Using Persistent Homology with Application to LLM Fine-Tuning,Yes.,1,"""Finally, we empirically show that the proposed method can be part of a stopping criterion for fine-tuning language-model classifiers.""",2023,2023-05-24T10:58:09Z,,,
arXIv2023,Unlocking Temporal Question Answering for Large Language Models Using Code Execution,Yes.,3,"""However, we notice the challenge that LLMs face when it comes to temporal reasoning.""",2023,2023-05-24T10:57:53Z,,,
arXIv2023,LLMDet: A Third Party Large Language Models Generated Text Detection Tool,Yes.,3,"""Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct.""",2023,2023-05-24T10:45:16Z,,,
arXIv2023,A RelEntLess Benchmark for Modelling Graded Relations between Named Entities,Yes.,3,"""Overall, we find a strong correlation between model size and performance, with smaller Language Models struggling to outperform a naive baseline.""",2023,2023-05-24T10:41:24Z,,,
arXIv2023,Enabling and Analyzing How to Efficiently Extract Information from Hybrid Long Documents with LLMs,Yes.,3,"""However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored.""",2023,2023-05-24T10:35:58Z,,,
arXIv2023,Controlling Pre-trained Language Models for Grade-Specific Text Simplification,Yes.,1,"""Recent work has shown that pre-trained language models can simplify text using a wealth of techniques to control output simplicity.""",2023,2023-05-24T10:29:45Z,,,
arXIv2023,Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios,Yes.,3,"""However, the adoption of LLMs in real-world applications for table information seeking remains underexplored."" and ""a significant performance gap still exists between other open-sourced LLMs (e.g., Tulu and LLaMA-2) and GPT-4 models.""",2023,2023-05-24T10:22:30Z,,,
arXIv2023,IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models,Yes.,3,"""However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing."" and ""we argue that previous efforts have several inherent shortcomings",2023,2023-05-24T10:19:57Z,,,
arXIv2023,OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning,Yes.,3,"""However, the prohibitive costs of tokens and time have hindered their adoption in applications.""",2023,2023-05-24T10:08:04Z,,,
arXIv2023,Editing Common Sense in Transformers,Yes.,3,"""However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers' reliability and usefulness.""",2023,2023-05-24T09:50:54Z,,,
arXIv2023,How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench,Yes.,1,"""We investigate the predictability of large language model (LLM) capabilities",2023,2023-05-24T09:35:34Z,,,
arXIv2023,GRACE: Discriminator-Guided Chain-of-Thought Reasoning,Yes.,3,"""In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps.""",2023,2023-05-24T09:16:51Z,,,
arXIv2023,"Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",Yes.,2,"""We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research.""",2023,2023-05-24T09:10:20Z,,,
arXIv2023,Universal Self-Adaptive Prompting,Yes.,3,"""zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable.""",2023,2023-05-24T09:09:48Z,,,
arXIv2023,Frugal Prompting for Dialog Models,Yes.,1,"""The use of large language models (LLMs) in natural language processing (NLP) tasks is rapidly increasing, leading to changes in how researchers approach problems in the field.""",2023,2023-05-24T09:06:49Z,,,
arXIv2023,Coverage-based Example Selection for In-Context Learning,Yes.,1,"""On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points",2023,2023-05-24T08:58:28Z,,,
arXIv2023,PIVOINE: Instruction Tuning for Open-world Information Extraction,Yes.,1,"""we seek to develop a large language model (LLM) that is able to perform Open-world IE to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions.""",2023,2023-05-24T08:52:08Z,,,
arXIv2023,Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory,Yes.,2,"""identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics.""",2023,2023-05-24T08:38:23Z,,,
arXIv2023,Leveraging GPT-4 for Automatic Translation Post-Editing,Yes.,3,"""However, we also show that GPT-4 could produce hallucinated edits, thereby urging caution in its use as an expert translation post-editor.""",2023,2023-05-24T08:30:05Z,,,
arXIv2023,CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering,Yes.,2,"""two bottlenecks limit these approaches",2023,2023-05-24T08:21:31Z,,,
arXIv2023,PromptNER: Prompting For Named Entity Recognition,Yes.,3,"""However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora.""",2023,2023-05-24T07:38:24Z,,,
arXIv2023,Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners,Yes.,3,"""LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense reasoning tasks by leveraging in-context new knowledge.""",2023,2023-05-24T07:33:34Z,,,
arXIv2023,Estimating Large Language Model Capabilities without Labeled Test Data,Yes.,3,"""the success of ICL varies widely from task to task"" and ""no existing approach provides an accurate and reliable ICL accuracy estimation in every setting, highlighting the need for better ways to measure the uncertainty of LLM predictions.""",2023,2023-05-24T06:55:09Z,,,
arXIv2023,Using Natural Language Explanations to Rescale Human Judgments,Yes.,1,"""The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation.""",2023,2023-05-24T06:19:14Z,,,
arXIv2023,Allies: Prompting Large Language Model with Beam Search,Yes.,3,"""However, this kind of methods face two limitations",2023,2023-05-24T06:16:44Z,,,
arXIv2023,A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification,Yes.,1,"""In this paper we present a novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities.""",2023,2023-05-24T05:54:10Z,,,
arXIv2023,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,Yes.,3,"""large language models (LLMs) may produce answers that do not satisfy all criteria of the question"" and ""can give insights into the errors and knowledge gaps of the model.""",2023,2023-05-24T05:53:11Z,,,
arXIv2023,ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation,Yes.,1,"""Furthermore, we develop an interactive system named ChatFace, which combines the zero-shot reasoning ability of large language models to perform efficient manipulations in diffusion semantic latent space.""",2023,2023-05-24T05:28:37Z,,,
arXIv2023,In-Context Demonstration Selection with Cross Entropy Difference,Yes.,3,"""However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples.""",2023,2023-05-24T05:04:00Z,,,
arXIv2023,I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors,Yes.,1,"""We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models.""",2023,2023-05-24T05:01:10Z,,,
arXIv2023,SciFix: Outperforming GPT3 on Scientific Factual Error Correction,Yes.,3,"""Our method outperforms the very LLM that was used to generate the annotated dataset -- with Few-Shot Prompting on GPT3.5 achieving 58%, 61%, and 64% on the respective datasets, a consistently lower correction accuracy, despite using nearly 800 times",2023,2023-05-24T04:24:16Z,,,
arXIv2023,Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models,Yes.,3,"""In the first scenario, MoE models overall underperform dense models of identical computational capacity.""",2023,2023-05-24T04:22:26Z,,,
arXIv2023,DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4,Yes.,2,"""Despite their significance, however, there has been limited research probing these pairwise or $k$-wise comparisons."" and ""It is also unclear if there are other hidden factors influencing human judgments.""",2023,2023-05-24T04:13:15Z,,,
arXIv2023,ExpertPrompting: Instructing Large Language Models to be Distinguished Experts,Yes.,1,"""The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts.""",2023,2023-05-24T03:51:31Z,,,
arXIv2023,Learning UI-to-Code Reverse Generator Using Visual Critic Without Rendering,Yes.,1,"""They are initialized by pre-trained models such as ViT/DiT and GPT-2/LLaMA but aligning the two modalities requires end-to-end finetuning, which aims to minimize the visual discrepancy between the code-rendered webpage and the original screenshot.""",2023,2023-05-24T02:17:32Z,,,
arXIv2023,Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,Yes.,3,"""fundamental questions persist regarding how LLMs acquire their multi-lingual abilities and how performance varies across different languages.""",2023,2023-05-24T02:05:03Z,,,
arXIv2023,Getting MoRE out of Mixture of Language Model Reasoning Experts,Yes.,3,"""state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt.""",2023,2023-05-24T02:00:51Z,,,
arXIv2023,Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models,Yes.,2,"""there is still significant room for improvement compared to SOTA fine-tuned models, which suggests that LLM adoption could be a promising approach for future fact-checking research.""",2023,2023-05-24T01:46:07Z,,,
arXIv2023,Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person,No.,1,"The abstract focuses on evaluating the performance of OpenAI's Whisper ASR for punctuation prediction and topic modeling, which is an automatic speech recognition system, not a language model.",2023,2023-05-23T23:37:29Z,,,
arXIv2023,PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents,Yes.,3,"""However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain.""",2023,2023-05-23T23:06:04Z,,,
arXIv2023,Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement,Yes.,3,"""they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly.""",2023,2023-05-23T19:58:30Z,,,
arXIv2023,Language Model Self-improvement by Reinforcement Learning Contemplation,Yes.,2,"""fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain.""",2023,2023-05-23T19:25:52Z,,,
arXIv2023,Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA,Yes.,3,"""GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors.""",2023,2023-05-23T18:30:49Z,,,
arXIv2023,Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding,Yes.,1,"""we further investigate the utilization of a Large Language Model (LLM) to enhance the FIG for user-entity link prediction in the Video/Music domains.""",2023,2023-05-23T18:15:29Z,,,
arXIv2023,Prompting Language-Informed Distribution for Compositional Zero-Shot Learning,Yes.,3,"""the key aspects that impact the generalization to unseen compositions, including the diversity and informativeness of class context, and the entanglement between visual primitives, i.e., state and object, are not properly addressed in existing CLIP-based CZSL literature.""",2023,2023-05-23T18:00:22Z,,,
arXIv2023,Schema-Driven Information Extraction from Heterogeneous Tables,Yes.,2,"""To assess various LLM's capabilities on this task"" and ""Moreover, through detailed ablation studies and analyses, we investigate the factors contributing to model success and validate the practicality of distilling compact models to reduce API reliance.""",2023,2023-05-23T17:58:10Z,,,
arXIv2023,DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot Text-to-Video Generation,Yes.,3,"""Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling shifts in scene composition or object placement from a single abstract user prompt.""",2023,2023-05-23T17:57:09Z,,,
arXIv2023,Benchmarking LLM-based Machine Translation on Cultural Awareness,Yes.,2,"""However, many MT systems still struggle to translate sentences containing cultural-specific entities accurately and understandably."" and ""Nevertheless, the effectiveness of this approach in enhancing machine translation with cultural awareness remains uncertain.""",2023,2023-05-23T17:56:33Z,,,
arXIv2023,Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation,Yes.,2,"""Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets.""",2023,2023-05-23T17:56:26Z,,,
arXIv2023,Improving Factuality and Reasoning in Language Models through Multiagent Debate,Yes.,3,"""reducing fallacious answers and hallucinations that contemporary models are prone to.""",2023,2023-05-23T17:55:11Z,,,
arXIv2023,RET-LLM: Towards a General Read-Write Memory for Large Language Models,Yes.,3,"""existing LLMs lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks.""",2023,2023-05-23T17:53:38Z,,,
arXIv2023,CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models,Yes.,3,"""Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved.""",2023,2023-05-23T17:51:52Z,,,
arXIv2023,QTSumm: Query-Focused Summarization over Tabular Data,Yes.,1,"""We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models.""",2023,2023-05-23T17:43:51Z,,,
arXIv2023,LLM-powered Data Augmentation for Enhanced Cross-lingual Performance,Yes.,3,"""LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset.""",2023,2023-05-23T17:33:27Z,,,
arXIv2023,INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback,Yes.,2,"""Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text.""",2023,2023-05-23T17:27:22Z,,,
arXIv2023,SciMON: Scientific Inspiration Machines Optimized for Novelty,Yes.,3,"""Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue.""",2023,2023-05-23T17:12:08Z,,,
arXIv2023,On Learning to Summarize with Large Language Models as References,Yes.,3,"""However, we found that the smaller models can not yet reach LLM-level performance under human evaluation despite promising improvements brought by our proposed training methods."" and ""reveals a discrepancy between human and LLM-based evaluation, highlighting the benefits and risks of this LLM-as-reference setting we investigated.""",2023,2023-05-23T16:56:04Z,,,
arXIv2023,ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media,Yes.,3,"""Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance.""",2023,2023-05-23T16:40:07Z,,,
arXIv2023,Exploring Chain-of-Thought Style Prompting for Text-to-SQL,Yes.,3,"""its performance on text-to-SQL parsing still has much room for improvement"" and ""using detailed reasoning steps tends to have more error propagation issues.""",2023,2023-05-23T16:32:36Z,,,
arXIv2023,CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models,Yes.,3,"""We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization.""",2023,2023-05-23T16:32:27Z,,,
arXIv2023,Domain Private Transformers for Multi-Domain Dialog Systems,Yes.,3,"""While multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt.""",2023,2023-05-23T16:27:12Z,,,
arXIv2023,"Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",Yes.,3,"""While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers.""",2023,2023-05-23T16:20:43Z,,,
arXIv2023,DetGPT: Detect What You Need via Reasoning,Yes.,1,"""In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs).""",2023,2023-05-23T15:37:28Z,,,
arXIv2023,Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization,Yes.,3,"""Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs.""",2023,2023-05-23T15:20:01Z,,,
arXIv2023,GrACE: Generation using Associated Code Edits,Yes.,1,"""In this work, we address these challenges by endowing pre-trained large language models (LLMs) of code with the knowledge of prior, relevant edits.""",2023,2023-05-23T14:55:44Z,,,
arXIv2023,Dr.ICL: Demonstration-Retrieved In-context Learning,Yes.,1,"""In-context learning (ICL), teaching a large language model (LLM) to perform a task with few-shot demonstrations rather than adjusting the model parameters, has emerged as a strong paradigm for using LLMs.""",2023,2023-05-23T14:55:25Z,,,
arXIv2023,Better Zero-Shot Reasoning with Self-Adaptive Prompting,Yes.,3,"""some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the",2023,2023-05-23T14:27:16Z,,,
arXIv2023,Evaluating Factual Consistency of Summaries with Large Language Models,Yes.,1,"""Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs.""",2023,2023-05-23T13:48:32Z,,,
arXIv2023,The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning,Yes.,3,"""Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks.""",2023,2023-05-23T13:14:59Z,,,
arXIv2023,ChipGPT: How far are we from natural language hardware design,Yes.,1,"""As large language models (LLMs) like ChatGPT exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction.""",2023,2023-05-23T12:54:02Z,,,
arXIv2023,Make a Choice! Knowledge Base Question Answering with In-Context Learning,Yes.,1,"""Recently, LLMs have shown strong few-shot performance in many NLP tasks."" and ""We expect LLM can help existing methods improve their generalization ability, especially in low-resource situations.""",2023,2023-05-23T11:56:03Z,,,
arXIv2023,Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning,Yes.,3,"""Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly.""",2023,2023-05-23T11:54:37Z,,,
arXIv2023,DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text,Yes.,2,"""Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement.""",2023,2023-05-23T11:18:30Z,,,
arXIv2023,PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,Yes.,3,"""their huge size and the inaccessibility of parameters present challenges for practical deployment"" and ""synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities.""",2023,2023-05-23T10:11:56Z,,,
arXIv2023,Learning from Mistakes via Cooperative Study Assistant for Large Language Models,Yes.,3,"""the feedback from LLM itself is often inaccurate, thereby limiting its benefits.""",2023,2023-05-23T08:51:08Z,,,
arXIv2023,Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation,Yes.,1,"""Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks.""",2023,2023-05-23T07:54:34Z,,,
arXIv2023,Aligning Large Language Models through Synthetic Feedback,Yes.,2,"""Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT.""",2023,2023-05-23T06:41:16Z,,,
arXIv2023,Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker,Yes.,1,"""While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet.""",2023,2023-05-23T06:35:33Z,,,
arXIv2023,Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models,Yes.,3,"""We argue this is an important feature for mitigating hallucinations."" and ""we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly.""",2023,2023-05-23T05:59:21Z,,,
arXIv2023,Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering,Yes.,2,"""While powerful, these LLMs usually contain tens or hundreds of billions of parameters, making them rather inefficient at inference time.""",2023,2023-05-23T04:57:31Z,,,
arXIv2023,Error Detection for Text-to-SQL Semantic Parsing,No.,1,The abstract does not mention LLMs or any specific limitations related to them. It focuses on text-to-SQL semantic parsing and error detection.,2023,2023-05-23T04:44:22Z,,,
arXIv2023,Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning,Yes.,1,"""GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search.""",2023,2023-05-23T04:07:03Z,,,
arXIv2023,LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models,Yes.,3,"""However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning.""",2023,2023-05-23T03:59:06Z,,,
arXIv2023,Understanding Programs by Exploiting (Fuzzing) Test Cases,Yes.,3,"""However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict.""",2023,2023-05-23T01:51:46Z,,,
arXIv2023,"Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?",Yes.,1,"""We discuss zero-shot, few-shot, and fine-tuning approaches on state of the art pre-trained Large Language Models (LLMs).""",2023,2023-05-23T01:21:55Z,,,
arXIv2023,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,Yes.,3,"""Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution.""",2023,2023-05-23T00:16:48Z,,,
arXIv2023,A Study of Generative Large Language Model for Medical Research and Healthcare,Yes.,2,"""This study provides insights on the opportunities and challenges of LLMs for medical research and healthcare.""",2023,2023-05-22T22:37:24Z,,,
arXIv2023,Small Language Models Improve Giants by Rewriting Their Outputs,Yes.,3,"""Despite the impressive performance of large language models (LLMs), they often lag behind specialized models in various tasks.""",2023,2023-05-22T22:07:50Z,,,
arXIv2023,Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents,Yes.,3,"""showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions"" and ""The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value.""",2023,2023-05-22T19:56:10Z,,,
arXIv2023,Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method,Yes.,1,"""we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work.""",2023,2023-05-22T18:54:35Z,,,
arXIv2023,Can LLMs facilitate interpretation of pre-trained language models?,Yes.,3,"""Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation.""",2023,2023-05-22T18:03:13Z,,,
arXIv2023,Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations,Yes.,3,"""We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases.""",2023,2023-05-22T17:56:31Z,,,
arXIv2023,AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback,Yes.,3,"""Replicating and understanding this instruction-following requires tackling three major challenges",2023,2023-05-22T17:55:50Z,,,
arXIv2023,CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles,Yes.,1,"""We present a design framework called Conversational Learning with Analytical Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring Systems (ITS) powered by high-performance Large Language Models (LLMs).""",2023,2023-05-22T17:35:05Z,,,
arXIv2023,Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources,Yes.,2,"""It results in more factual rationales and reduced hallucination in generation.""",2023,2023-05-22T17:34:23Z,,,
arXIv2023,Enhance Reasoning Ability of Visual-Language Models via Large Language Models,Yes.,2,"""Pre-trained visual language models (VLM) have shown excellent performance in image caption tasks. However, it sometimes shows insufficient reasoning ability.""",2023,2023-05-22T17:33:44Z,,,
arXIv2023,Chip-Chat: Challenges and Opportunities in Conversational Hardware Design,Yes.,2,"""we thus explore the challenges faced and opportunities presented when leveraging these recent advances in LLMs for hardware design.""",2023,2023-05-22T17:13:33Z,,,
arXIv2023,Deepfake Text Detection in the Wild,Yes.,3,"""Human annotators are only slightly better than random guessing at identifying machine-generated texts. Empirical results on automatic detection methods further showcase the challenges of deepfake text detection in a wild testbed. In addition, out-of-distribution poses a greater challenge for a detector to be employed in realistic application scenarios.""",2023,2023-05-22T17:13:29Z,,,
arXIv2023,Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance,Yes.,2,"""Finally, we initiate a discussion regarding the necessity of employing LLMs for only one targeted task, taking into account the efforts required for tuning and the resources consumed during deployment.""",2023,2023-05-22T16:56:44Z,,,
arXIv2023,LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities,Yes.,2,"""Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors.""",2023,2023-05-22T15:56:44Z,,,
arXIv2023,Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models,Yes.,2,"""revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS.""",2023,2023-05-22T15:12:43Z,,,
arXIv2023,Observations on LLMs for Telecom Domain: Capabilities and Limitations,Yes.,3,"""we analyze capabilities and limitations of incorporating such models in conversational interfaces for the telecommunication domain, specifically for enterprise wireless products and services.""",2023,2023-05-22T15:04:16Z,,,
arXIv2023,"InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",Yes.,3,"""their extensive serving and fine-tuning costs hinder their utilization in various applications"" and ""the quality of the summaries they generate is inferior to that of larger models like GPT-3 when assessed by human evaluators.""",2023,2023-05-22T14:52:32Z,,,
arXIv2023,Making Language Models Better Tool Learners with Execution Feedback,Yes.,3,"""Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance.""",2023,2023-05-22T14:37:05Z,,,
arXIv2023,Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study,Yes.,3,"""However, there is still much to learn about how well LLMs understand structured data, such as tables."" and ""there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data.""",2023,2023-05-22T14:23:46Z,,,
arXIv2023,Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media,Yes.,3,"""Many of these approaches require annotated training datasets, which limits their applicability for languages where these may not be readily available.""",2023,2023-05-22T13:56:35Z,,,
arXIv2023,Text-based Person Search without Parallel Image-Text Data,Yes.,1,"""we propose a fine-grained image captioning strategy to obtain an enriched description of the person image, which firstly utilizes a set of instruction prompts to activate the off-the-shelf pretrained vision-language model to capture and generate fine-grained person attributes, and then converts the extracted attributes into a textual description via the finetuned large language model or the hand-crafted template.""",2023,2023-05-22T12:13:08Z,,,
arXIv2023,ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness,Yes.,1,"""The emergence of generative large language models (LLMs) raises the question",2023,2023-05-22T11:46:32Z,,,
arXIv2023,Album Storytelling with Iterative Story-aware Captioning and Large Language Models,Yes.,3,"""we find this often results in stories containing hallucinated information that contradicts the images, as each generated caption (""story-agnostic"") is not always about the description related to the whole story or miss some necessary information.""",2023,2023-05-22T11:45:10Z,,,
arXIv2023,Lion: Adversarial Distillation of Proprietary Large Language Models,Yes.,1,"""Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions.""",2023,2023-05-22T09:49:16Z,,,
arXIv2023,MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering,Yes.,3,"""Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table.""",2023,2023-05-22T08:25:15Z,,,
arXIv2023,LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space,Yes.,3,"""efficient adaptation of a language model to diverse conditions remains an open challenge.""",2023,2023-05-22T07:52:04Z,,,
arXIv2023,GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs,Yes.,1,"""Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions.""",2023,2023-05-22T07:35:43Z,,,
arXIv2023,Explaining Emergent In-Context Learning as Kernel Regression,Yes.,1,"""Large language models (LLMs) have initiated a paradigm shift in transfer learning.""",2023,2023-05-22T06:45:02Z,,,
arXIv2023,Fact-Checking Complex Claims with Program-Guided Reasoning,Yes.,1,"""We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process.""",2023,2023-05-22T06:11:15Z,,,
arXIv2023,Can We Edit Factual Knowledge by In-Context Learning?,Yes.,3,"""the stored knowledge could be false or out-dated,"" and ""these gradient-based approaches bring large computation costs.""",2023,2023-05-22T06:07:58Z,,,
arXIv2023,llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology,Yes.,3,"""However, high-performing LLMs are usually mainly for English."" and ""However, we also revealed some difficulties in constructing LLMs in languages other than English.""",2023,2023-05-22T04:59:33Z,,,
arXIv2023,G3Detector: General GPT-Generated Text Detector,Yes.,3,"""it is critical to acknowledge the potential misuse of these models, which could give rise to a spectrum of social and ethical dilemmas.""",2023,2023-05-22T03:35:00Z,,,
arXIv2023,Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning,Yes.,3,"""Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from the web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks.""",2023,2023-05-21T23:16:26Z,,,
arXIv2023,On the Limitations of Simulating Active Learning,No.,1,The abstract does not mention LLMs or any other type of language models.,2023,2023-05-21T22:52:13Z,,,
arXIv2023,TheoremQA: A Theorem-driven Question Answering dataset,Yes.,3,"""However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated.""",2023,2023-05-21T17:51:35Z,,,
arXIv2023,LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance,Yes.,2,"""While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data.""",2023,2023-05-21T17:26:16Z,,,
arXIv2023,Evaluating the Performance of Large Language Models on GAOKAO Benchmark,Yes.,3,"""Our findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across various subjects.""",2023,2023-05-21T14:39:28Z,,,
arXIv2023,Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification,Yes.,2,"""Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks.""",2023,2023-05-21T14:03:49Z,,,
arXIv2023,Evaluating Open-QA Evaluation,Yes.,3,"""Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach."" and ""We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators.""",2023,2023-05-21T10:40:55Z,,,
arXIv2023,Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition,Yes.,1,"""LA-GCN proposes a graph convolution network using large-scale language models (LLM) knowledge assistance.""",2023,2023-05-21T08:29:16Z,,,
arXIv2023,PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs,Yes.,3,"""Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation.""",2023,2023-05-21T08:11:24Z,,,
arXIv2023,Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models,Yes.,1,"""Efficient deployment of large language models (LLMs) necessitates low-bit quantization to minimize model size and inference cost.""",2023,2023-05-21T05:28:37Z,,,
arXIv2023,Task-agnostic Distillation of Encoder-Decoder Language Models,Yes.,3,"""Frustratingly, we discover that existing task-agnostic distillation methods can fail to handle the distillation of encoder-decoder LMs.""",2023,2023-05-21T03:35:45Z,,,
arXIv2023,Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,Yes.,3,"""Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems.""",2023,2023-05-20T22:25:38Z,,,
arXIv2023,Tweetorial Hooks: Generative AI Tools to Motivate Science on Social Media,Yes.,2,"""Lastly, we discuss the importance of interactivity with LLMs to preserve the correctness, effectiveness, and authenticity of the writing.""",2023,2023-05-20T18:47:40Z,,,
arXIv2023,Collaborative Development of NLP models,Yes.,2,"""Despite substantial advancements, Natural Language Processing (NLP) models often require post-training adjustments to enforce business rules, rectify undesired behavior, and align with user values"" and ""Moreover, the exhaustive delineation of a concept is challenging, and an improper approach can create shortcuts or interfere with original data or other concepts.""",2023,2023-05-20T15:55:39Z,,,
arXIv2023,The Case Against Explainability,Yes.,3,"""we suggest that in some cases the right to explanation of AI systems could bring more harm than good to end users."" and ""considering recent end-user Explainability research trends, Large Language Models' capabilities, and the ability to manipulate end-users by both humans and machines.""",2023,2023-05-20T10:56:19Z,,,
arXIv2023,LogiCoT: Logical Chain-of-Thought Instruction-Tuning,Yes.,3,"""However, they fall short of helping the model handle complex reasoning tasks.""",2023,2023-05-20T09:23:09Z,,,
arXIv2023,Can Public Large Language Models Help Private Cross-device Federated Learning?,Yes.,2,"""The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users."" and ""Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models.""",2023,2023-05-20T07:55:58Z,,,
arXIv2023,"MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement",Yes.,1,"""The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with distinct schema.""",2023,2023-05-20T03:37:09Z,,,
arXIv2023,OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models,Yes.,3,"""Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while positively affecting the non-finetuned counterpart.""",2023,2023-05-19T20:58:22Z,,,
arXIv2023,Deep Learning Approaches to Lexical Simplification: A Survey,Yes.,1,"""the AI/NLP community has been taken by storm by recent advances in deep learning, particularly with the introduction of large language models (LLM) and prompt learning.""",2023,2023-05-19T20:56:22Z,,,
arXIv2023,Reducing Sequence Length by Predicting Edit Operations with Large Language Models,Yes.,3,"""However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer.""",2023,2023-05-19T17:51:05Z,,,
arXIv2023,"How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings",Yes.,3,"""However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research.""",2023,2023-05-19T17:43:58Z,,,
arXIv2023,Prompting with Pseudo-Code Instructions,Yes.,1,"""Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models.""",2023,2023-05-19T16:25:01Z,,,
arXIv2023,Cross-Lingual Supervision improves Large Language Models Pre-training,Yes.,1,"""The recent rapid progress in pre-training Large Language Models has relied on using self-supervised language modeling objectives like next token prediction or span corruption.""",2023,2023-05-19T16:14:07Z,,,
arXIv2023,Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate,Yes.,3,"""Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues.""",2023,2023-05-19T11:15:33Z,,,
arXIv2023,ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings,Yes.,3,"""However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from,",2023,2023-05-19T09:54:21Z,,,
arXIv2023,Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,Yes.,3,"""its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge.""",2023,2023-05-19T09:23:25Z,,,
arXIv2023,PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning,Yes.,3,"""It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field.""",2023,2023-05-19T08:18:24Z,,,
arXIv2023,Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models,Yes.,3,"""current interfaces for interacting with LLMs are generally linear to support conversational interaction. To address this limitation and explore how we can support LLM-powered exploration and sensemaking, we developed Sensecape.""",2023,2023-05-19T07:31:59Z,,,
arXIv2023,Hint of Thought prompting: an explainable and zero-shot approach to reasoning tasks with LLMs,Yes.,3,"""Although simple prompting performs well on single-step questions, it cannot permanently activate the correct knowledge path for multi-step reasoning tasks.""",2023,2023-05-19T06:30:17Z,,,
arXIv2023,Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions,Yes.,2,"""However, they typically rely on extensive human-annotated data.""",2023,2023-05-19T06:27:16Z,,,
arXIv2023,TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks,Yes.,3,"""their potential for performing ill-defined complex tasks is largely under-studied"" and ""large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts.""",2023,2023-05-19T04:59:34Z,,,
arXIv2023,Post Hoc Explanations of Language Models Can Improve Language Models,Yes.,3,"""However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement.""",2023,2023-05-19T04:46:04Z,,,
arXIv2023,Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models,Yes.,3,"""optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in many domains.""",2023,2023-05-19T03:51:59Z,,,
arXIv2023,VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks,Yes.,1,"""Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications.""",2023,2023-05-18T17:59:42Z,,,
arXIv2023,TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models,Yes.,3,"""large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use.""",2023,2023-05-18T17:58:35Z,,,
arXIv2023,Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors,Yes.,3,"""even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE),"" and ""instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE's low incidence in instruction-tuning datasets.""",2023,2023-05-18T17:48:03Z,,,
arXIv2023,Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement,No.,1,The abstract focuses on target-oriented opinion word extraction (TOWE) models using BERT-based text encoders and does not mention LLMs or their limitations.,2023,2023-05-18T15:22:00Z,,,
arXIv2023,Generalized Planning in PDDL Domains with Pretrained Large Language Models,Yes.,2,"""We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two training tasks are often sufficient for strong generalization.""",2023,2023-05-18T14:48:20Z,,,
arXIv2023,Large Language Models can be Guided to Evade AI-Generated Text Detection,Yes.,3,"""the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods.""",2023,2023-05-18T10:03:25Z,,,
arXIv2023,X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models,Yes.,1,"""This paper introduces a novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations.""",2023,2023-05-18T09:56:44Z,,,
arXIv2023,ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation,Yes.,1,"""Naturally, these programs can be considered as sequence data, for which large language models (LLM) can help.""",2023,2023-05-18T09:44:18Z,,,
arXIv2023,Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants,Yes.,1,"""we seek to fill this gap through the use of thirteen popular transformer based models, as well as ChatGPT, and we show that discriminative models perform better than GPT-3.5 model with our best performer reporting 92.2349% F1 score in metaphoric flower and plant names identification task.""",2023,2023-05-18T09:22:29Z,,,
arXIv2023,Human Behavioral Benchmarking: Numeric Magnitude Comparison Effects in Large Language Models,Yes.,3,"""Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text.""",2023,2023-05-18T07:50:44Z,,,
arXIv2023,Are Large Language Models Fit For Guided Reading?,Yes.,3,"""They generate diverse question that cover most topics in the input text even though this ability is significantly degraded as the input text increases,"" and ""They are significantly biased toward low cognitive question.""",2023,2023-05-18T02:03:55Z,,,
arXIv2023,Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning,Yes.,1,"""We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information.""",2023,2023-05-17T23:50:28Z,,,
arXIv2023,Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting,Yes.,3,"""Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words.""",2023,2023-05-17T23:41:30Z,,,
arXIv2023,Statistical Knowledge Assessment for Large Language Models,Yes.,3,"""Existing LLMs may generate distinct responses for different prompts."" and ""tuning on instruction-following data sometimes compromises the model's capability to generate factually correct text reliably.""",2023,2023-05-17T18:54:37Z,,,
arXIv2023,Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families,Yes.,1,"""This study explores the potential of large language models (LLMs) in helping families with creative coding using Scratch.""",2023,2023-05-17T17:52:25Z,,,
arXIv2023,Controllable Speaking Styles Using a Large Language Model,Yes.,1,"""Large generative language models (LLMs) have shown excellent performance in various language-related tasks.""",2023,2023-05-17T16:01:50Z,,,
arXIv2023,Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries,Yes.,3,"""However, these English-centric models encounter challenges in non-English clinical settings, primarily due to limited clinical knowledge in respective languages, a consequence of imbalanced training corpora.""",2023,2023-05-17T12:31:26Z,,,
arXIv2023,Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback,Yes.,3,"""Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game's rules or cannot incorporate AI feedback for further improvement.""",2023,2023-05-17T11:55:32Z,,,
arXIv2023,Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark,Yes.,2,"""However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive.""",2023,2023-05-17T08:28:54Z,,,
arXIv2023,Smaller Language Models are Better Black-box Machine-Generated Text Detectors,Yes.,1,"""With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures.""",2023,2023-05-17T00:09:08Z,,,
arXIv2023,Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs,Yes.,1,"""In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data.""",2023,2023-05-17T00:08:36Z,,,
arXIv2023,Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites,Yes.,3,"""However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize LLMs to mass produce misinformation.""",2023,2023-05-16T21:51:01Z,,,
arXIv2023,"What In-Context Learning ""Learns"" In-Context: Disentangling Task Recognition and Task Learning",Yes.,3,"""its mechanisms are not yet well-understood.""",2023,2023-05-16T18:05:19Z,,,
arXIv2023,SatLM: Satisfiability-Aided Language Models Using Declarative Prompting,Yes.,3,"""While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search.""",2023,2023-05-16T17:55:51Z,,,
arXIv2023,AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction,Yes.,3,"""We discuss practical constraints and ethical concerns regarding individual autonomy and privacy when using LLMs for opinion prediction.""",2023,2023-05-16T17:13:07Z,,,
arXIv2023,Towards Expert-Level Medical Question Answering with Large Language Models,Yes.,3,"""significant room for improvement, especially when models' answers were compared to clinicians' answers"" and ""newly introduced datasets of 240 long-form 'adversarial' questions to probe LLM limitations.""",2023,2023-05-16T17:11:29Z,,,
arXIv2023,Large Language Models are Built-in Autoregressive Search Engines,Yes.,1,"""In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval.""",2023,2023-05-16T17:04:48Z,,,
arXIv2023,Life of PII -- A PII Obfuscation Transformer,Yes.,1,"""Our Transformer-based approach learns mapping between the original PII and its transformed faux-PII representation, which we call 'obfuscated' data.""",2023,2023-05-16T15:48:36Z,,,
arXIv2023,CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural Topic Modeling,Yes.,1,The abstract discusses the use of contextualized word embeddings from BERT but does not mention any limitations of language models.,2023,2023-05-16T10:07:33Z,,,
arXIv2023,Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning,Yes.,2,"""training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs.""",2023,2023-05-16T07:52:57Z,,,
arXIv2023,Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models,Yes.,3,"""Vanilla language models are forgetful"" and ""Pre-training leads to retentive language models.""",2023,2023-05-16T03:50:38Z,,,
arXIv2023,SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,Yes.,1,"""Recently large language models (LLMs) have demonstrated exceptional proficiency in conversational engagement and adherence to instructions across various downstream tasks.""",2023,2023-05-15T23:29:56Z,,,
arXIv2023,RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs,Yes.,3,"""Despite their unprecedented success, even the largest language models make mistakes."" and ""this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network.""",2023,2023-05-15T17:57:16Z,,,
arXIv2023,Knowledge Rumination for Pre-trained Language Models,Yes.,3,"""Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone"" and ""we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks.""",2023,2023-05-15T15:47:09Z,,,
arXIv2023,Natural Language Decomposition and Interpretation of Complex Utterances,Yes.,1,"""large language models (LLMs) encode knowledge about goals and plans that can help conversational assistants interpret user requests requiring numerous steps to complete.""",2023,2023-05-15T14:35:00Z,,,
arXIv2023,C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models,Yes.,3,"""Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs.""",2023,2023-05-15T03:20:19Z,,,
arXIv2023,Large Language Model Guided Tree-of-Thought,Yes.,1,"""In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs).""",2023,2023-05-15T01:18:23Z,,,
arXIv2023,$SmartProbe$: A Virtual Moderator for Market Research Surveys,Yes.,1,"""In this work, we introduce ${\tt SmartProbe}$, an API which leverages the adaptive capabilities of large language models (LLMs), and incorporates domain knowledge from market research, in order to generate effective probing questions in any market research survey.""",2023,2023-05-14T22:36:08Z,,,
arXIv2023,Mobile-Env: An Evaluation Platform and Benchmark for LLM-GUI Interaction,Yes.,3,"""Recently, Large Language Models (LLMs) have exhibited robust reasoning and planning abilities, yet their potential for multi-turn interactions in complex environments remains under-explored.""",2023,2023-05-14T12:31:03Z,,,
arXIv2023,Watermarking Text Generated by Black-Box Language Models,Yes.,2,"""passive detection methods are stuck in domain specificity and limited adversarial robustness"" and ""this method is not applicable in many real-world scenarios where only black-box language models are available.""",2023,2023-05-14T07:37:33Z,,,
arXIv2023,ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance,Yes.,3,"""We demonstrate the limitations of using state-of-the-art large-scale language models (LMs) on this dataset.""",2023,2023-05-13T21:31:02Z,,,
arXIv2023,Dual Use Concerns of Generative AI and Large Language Models,Yes.,2,"""With its demonstrated advantages and drawbacks in biological research, we believe the DURC criteria can be effectively redefined for LLMs, potentially contributing to improved AI governance.""",2023,2023-05-13T10:08:57Z,,,
arXIv2023,Improving Small Language Models on PubMedQA via Generative Data Augmentation,Yes.,2,"""LLMs have made remarkable advancements in the field of natural language processing. However, their increasing size poses challenges in terms of computational cost.""",2023,2023-05-12T23:49:23Z,,,
arXIv2023,NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models,Yes.,1,"""In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages.""",2023,2023-05-12T21:22:08Z,,,
arXIv2023,Knowledge Authoring for Rules and Actions,No.,1,"The abstract does not mention language models, LLMs, or any related limitations.",2023,2023-05-12T21:08:35Z,,,
arXIv2023,Text2Cohort: Facilitating Intuitive Access to Biomedical Data with Natural Language Cohort Discovery,Yes.,1,"""Recently, large language models (LLM) have demonstrated exceptional utility for natural language processing tasks.""",2023,2023-05-12T17:46:06Z,,,
arXIv2023,Generative AI: Implications and Applications for Education,Yes.,3,"""In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation.""",2023,2023-05-12T16:52:38Z,,,
arXIv2023,LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development,Yes.,1,"""In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs).""",2023,2023-05-12T14:21:38Z,,,
arXIv2023,Calibration-Aware Bayesian Learning,Yes.,3,"""Deep learning models, including modern systems like large language models, are well known to offer unreliable estimates of the uncertainty of their decisions.""",2023,2023-05-12T14:19:15Z,,,
arXIv2023,ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter,Yes.,3,"""a grand challenge of exploiting LLMs for multimodal learning is the size of pre-trained LLMs which are always with billions of parameters"" and ""these models remain limited in their understanding of artistic imagery.""",2023,2023-05-12T14:04:30Z,,,
arXIv2023,"Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation",Yes.,2,"""Our approach is better at detecting gender bias and word-sense-disambiguation errors in translation than supervised QE, indicating its robustness to out-of-domain usage. The performance gap is larger when detecting errors on a nontraditional translation-prompting LLM, indicating that our approach is more generalizable to different MT systems.""",2023,2023-05-12T13:10:57Z,,,
arXIv2023,When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust,Yes.,3,"""While their remarkable performance spans a range of tasks, adapting LLMs for real-world business scenarios still poses challenges warranting further investigation.""",2023,2023-05-12T03:49:59Z,,,
arXIv2023,Exploring Zero and Few-shot Techniques for Intent Classification,Yes.,1,"""zero-shot intent classification using descriptions large language models (LLMs),"" and ""parameter-efficient fine-tuning of instruction-finetuned language models.""",2023,2023-05-11T22:07:27Z,,,
arXIv2023,"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",Yes.,3,"""In this review, we also explored the potential challenges and limitations of a GPT.""",2023,2023-05-11T19:20:38Z,,,
arXIv2023,Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales,Yes.,3,"""We observe that human utility of existing rationales is far from satisfactory, and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales, or similarity between generated and gold rationales are not good indicators of their human utility.""",2023,2023-05-11T19:01:13Z,,,
arXIv2023,Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,Yes.,3,"""their performance varies substantially across different languages.""",2023,2023-05-11T17:44:17Z,,,
arXIv2023,Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach,Yes.,1,"""Inspired by the recent progress on large language models (LLMs), we take a different approach to developing the recommendation models, considering recommendation as instruction following by LLMs.""",2023,2023-05-11T17:39:07Z,,,
arXIv2023,Spear Phishing With Large Language Models,Yes.,3,"""I demonstrate how basic prompt engineering can circumvent safeguards installed in LLMs, highlighting the need for further research into robust interventions that can help prevent models from being misused.""",2023,2023-05-11T16:55:19Z,,,
arXIv2023,Structured Chain-of-Thought Prompting for Code Generation,Yes.,3,"""However, CoT prompting is designed for natural language generation and has low accuracy in code generation.""",2023,2023-05-11T06:43:37Z,,,
arXIv2023,Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications,Yes.,3,"""Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems.""",2023,2023-05-11T01:50:16Z,,,
arXIv2023,Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction,Yes.,3,"""the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question."" and ""we find that zero-shot LLMs lag behind traditional recommender models that have the access to user interaction data, indicating the importance of user interaction data.""",2023,2023-05-10T21:43:42Z,,,
arXIv2023,Bot or Human? Detecting ChatGPT Imposters with A Single Question,Yes.,3,"""there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks.""",2023,2023-05-10T19:09:24Z,,,
arXIv2023,Automatic Evaluation of Attribution by Large Language Models,Yes.,3,"""However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem.""",2023,2023-05-10T16:58:33Z,,,
arXIv2023,Evaluating Embedding APIs for Information Retrieval,Yes.,2,"""The ever-increasing size of language models curtails their widespread availability to the community, thereby galvanizing many companies into offering access to large language models through APIs.""",2023,2023-05-10T16:40:52Z,,,
arXIv2023,Privacy-Preserving Prompt Tuning for Large Language Model Services,Yes.,2,"""However, the sensitive nature of private data brings the need for privacy preservation in LLM service customization.""",2023,2023-05-10T14:41:51Z,,,
arXIv2023,Davinci the Dualist: the mind-body divide in large language models and in human learners,Yes.,3,"""While Davinci's performance is constrained by its syntactic limitations, and it differs from humans, its Dualist bias is robust.""",2023,2023-05-10T12:28:09Z,,,
arXIv2023,Enriching language models with graph-based context information to better understand textual data,Yes.,1,"""The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text.""",2023,2023-05-10T10:57:21Z,,,
arXIv2023,Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models,Yes.,1,"""We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our experiments.""",2023,2023-05-10T08:48:53Z,,,
arXIv2023,Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer,Yes.,2,"""Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the significant amount of computational resources and labeled data required for the specific task.""",2023,2023-05-10T07:33:36Z,,,
arXIv2023,Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment,Yes.,3,"""We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces.""",2023,2023-05-10T07:24:36Z,,,
arXIv2023,Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks,Yes.,2,"""We report both the strengths and limitations of the current models by comparing them to the state-of-the-art fine-tuned approaches and the recently released domain-specific pretrained models.""",2023,2023-05-10T03:13:54Z,,,
arXIv2023,DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text,Yes.,2,"""Several preceding studies have ventured to address this challenge by employing binary classifiers to differentiate between human-written and LLM-generated text. Nevertheless, the reliability of these classifiers has been subject to question.""",2023,2023-05-09T21:31:07Z,,,
arXIv2023,TidyBot: Personalized Robot Assistance with Large Language Models,Yes.,1,"""We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions.""",2023,2023-05-09T17:52:59Z,,,
arXIv2023,Towards Building the Federated GPT: Federated Instruction Tuning,Yes.,3,"""Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. Consequently, this hinders the generality of the tuned models and may restrict their effectiveness in certain contexts.""",2023,2023-05-09T17:42:34Z,,,
arXIv2023,Fine-tuning Language Models with Generative Adversarial Reward Modelling,Yes.,3,"""RLHF is constrained by the expertise and productivity limitations of human evaluators."" and ""while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead.""",2023,2023-05-09T17:06:06Z,,,
arXIv2023,Large Language Models Need Holistically Thought in Medical Conversational QA,Yes.,3,"""Despite the success of large language models (LLMs) in complex reasoning tasks in various fields, such as mathematics, logic, and commonsense QA, they still need to improve with the increased complexity and specialization of the medical field.""",2023,2023-05-09T12:57:28Z,,,
arXIv2023,A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture,Yes.,2,"""There is limited understanding about the impact of introducing foundation models in software architecture.""",2023,2023-05-09T11:37:16Z,,,
arXIv2023,SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models,Yes.,3,"""However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation.""",2023,2023-05-09T05:48:38Z,,,
arXIv2023,MoT: Memory-of-Thought Enables ChatGPT to Self-Improve,Yes.,3,"""However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning.""",2023,2023-05-09T05:25:05Z,,,
arXIv2023,Accessible Instruction-Following Agent,Yes.,2,"""Previous instruction-following agents are biased to English-centric corpus, making it unrealizable to be applied to users that use multiple languages or even low-resource languages."" and ""the instruction-following agents are pre-trained in a mode that assumes the user can observe the environment, which limits its accessibility.""",2023,2023-05-08T23:57:26Z,,,
arXIv2023,Knowledge-enhanced Agents for Interactive Text Games,Yes.,3,"""Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment.""",2023,2023-05-08T23:31:39Z,,,
arXIv2023,Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer,Yes.,3,"""Large Language Models (LLMs), such as the Generative Pretrained Transformer (GPT), have achieved tremendous success in various language tasks, but their emergent abilities have also raised many questions, concerns, and challenges that need to be addressed.""",2023,2023-05-08T21:35:12Z,,,
arXIv2023,ANALOGICAL -- A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models,Yes.,3,"""Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.""",2023,2023-05-08T21:12:20Z,,,
arXIv2023,Web Content Filtering through knowledge distillation of Large Language Models,Yes.,1,"""We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering [...] Our student model matches the performance of the teacher LLM with 175 times less parameters.""",2023,2023-05-08T20:09:27Z,,,
arXIv2023,Revisiting Relation Extraction in the era of Large Language Models,Yes.,2,"""We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching.""",2023,2023-05-08T19:19:07Z,,,
arXIv2023,NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge,Yes.,3,"""not even the most powerful models are exempt from making errors"" and ""to what extent are models at different scales able to generate valid and diverse comparative knowledge?""",2023,2023-05-08T18:20:36Z,,,
arXIv2023,Learning Summary-Worthy Visual Representation for Abstractive Summarization in Video,Yes.,3,"""However, the generally extracted visual features may overlook some summary-worthy visual information, which impedes model performance.""",2023,2023-05-08T16:24:46Z,,,
arXIv2023,Algebra Error Classification with Large Language Models,Yes.,2,"""Additionally, we analyze common classification errors made by our method and discuss limitations of automated error classification.""",2023,2023-05-08T15:51:38Z,,,
arXIv2023,Enhancing Knowledge Graph Construction Using Large Language Models,Yes.,3,"""However, the combined application of Large Language Models with semantic technologies for reasoning and inference is still a challenging task.""",2023,2023-05-08T12:53:06Z,,,
arXIv2023,Sparks of Artificial General Recommender (AGR): Early Experiments with ChatGPT,Yes.,3,"""Our findings demonstrate the potential for ChatGPT to serve as an AGR, though several limitations and areas for improvement are identified.""",2023,2023-05-08T07:28:16Z,,,
arXIv2023,Language Independent Neuro-Symbolic Semantic Parsing for Form Understanding,Yes.,1,"""Recent works on form understanding mostly employ multimodal transformers or large-scale pre-trained language models. These models need ample data for pre-training.""",2023,2023-05-08T05:03:07Z,,,
arXIv2023,Unlocking Practical Applications in Legal Domain: Evaluation of GPT for Zero-Shot Semantic Annotation of Legal Texts,Yes.,1,"""to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings.""",2023,2023-05-08T01:55:53Z,,,
arXIv2023,Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5,Yes.,2,"""The fact that an LLM - which lacks these processes - also shows such effects invites consideration of the possibility that language may play a role in generating these effects in humans.""",2023,2023-05-08T01:02:52Z,,,
arXIv2023,"Perception, performance, and detectability of conversational artificial intelligence across 32 university courses",Yes.,3,"""current AI-text classifiers cannot reliably detect ChatGPT's use in school work, due to their propensity to classify human-written answers as AI-generated, as well as the ease with which AI-generated text can be edited to evade detection.""",2023,2023-05-07T10:37:51Z,,,
arXIv2023,X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages,Yes.,1,"""Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models.""",2023,2023-05-07T02:25:42Z,,,
arXIv2023,Controllable Mixed-Initiative Dialogue Generation through Prompting,Yes.,1,"""We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation.""",2023,2023-05-06T23:11:25Z,,,
arXIv2023,Self-Edit: Fault-Aware Code Editor for Code Generation,Yes.,3,"""However, with limited sample numbers, LLMs still suffer from poor accuracy.""",2023,2023-05-06T16:12:19Z,,,
arXIv2023,On Contrastive Learning of Semantic Similarity forCode to Code Search,Yes.,1,"""This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training.""",2023,2023-05-05T20:46:56Z,,,
arXIv2023,Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation,Yes.,3,"""End-to-end ASR models trained on large amount of data tend to be implicitly biased towards language semantics of the training data.""",2023,2023-05-05T20:35:42Z,,,
arXIv2023,Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios,Yes.,3,"""Although the task-adaptive pre-training approach has the potential to capture domain-specific patterns, it is constrained by the limited task-specific corpus and may be susceptible to overfitting.""",2023,2023-05-05T18:39:07Z,,,
arXIv2023,Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management,Yes.,2,"""The main challenges in developing such a device are the creation of a reliable non-invasive tool for angiogenic level assessment, a biophysics model and the practical consideration of an LLM communicating with emergency personnel on behalf of an incapacitated patient.""",2023,2023-05-05T17:55:49Z,,,
arXIv2023,LMEye: An Interactive Perception Network for Large Language Models,Yes.,3,"""Such networks project the image feature once yet do not consider the interaction between the image and the human input query. Hence, the obtained visual information without being connected to human intention may be inadequate for LLMs to generate intention-following responses, which we refer to as static visual information.""",2023,2023-05-05T17:27:21Z,,,
arXIv2023,Now It Sounds Like You: Learning Personalized Vocabulary On Device,No.,1,"The abstract discusses Federated Learning (FL) and its application to on-device language modeling, but it does not mention large language models (LLMs) or their limitations.",2023,2023-05-05T14:44:20Z,,,
arXIv2023,T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large Language Model Signals for Science Question Answering,Yes.,3,"""collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed.""",2023,2023-05-05T11:56:30Z,,,
arXIv2023,"Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects",Yes.,3,"""In Section 3, we discuss the challenges of utilizing existing LLMs to effectively complete the educated tasks and present a unified framework for addressing diverse education dataset, processing lengthy conversations, and condensing information to better accomplish more downstream tasks.""",2023,2023-05-05T11:09:13Z,,,
arXIv2023,Using ChatGPT for Entity Matching,Yes.,3,"""Two major drawbacks of using these models for entity matching are that (i) the models require significant amounts of fine-tuning data for reaching a good performance and (ii) the fine-tuned models are not robust concerning out-of-distribution entities.""",2023,2023-05-05T10:39:32Z,,,
arXIv2023,VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna,Yes.,2,"""However, these models can only be accessed via online APIs, which may cause data leak and non-reproducible problems.""",2023,2023-05-05T02:46:22Z,,,
arXIv2023,LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics,Yes.,3,"""we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases.""",2023,2023-05-04T23:54:37Z,,,
arXIv2023,Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing,Yes.,3,"""GPT-4 poses several challenges and limitations such as computational requirements, data requirements, and ethical concerns.""",2023,2023-05-04T22:46:43Z,,,
arXIv2023,Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study,Yes.,1,"""We applied BERT, a powerful Large Language Model (LLM) that enables us to transform code examples into numerical vectors by extracting their semantic information.""",2023,2023-05-04T17:43:19Z,,,
arXIv2023,Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence,Yes.,3,"""Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage.""",2023,2023-05-04T17:31:41Z,,,
arXIv2023,"Automatic Prompt Optimization with ""Gradient Descent"" and Beam Search",Yes.,3,"""Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort.""",2023,2023-05-04T15:15:22Z,,,
arXIv2023,An automatically discovered chain-of-thought prompt generalizes to novel models and datasets,Yes.,3,"""uncertainties remain about how reasoning strategies formulated for previous model generations generalize to new model generations and different datasets.""",2023,2023-05-04T15:07:20Z,,,
arXIv2023,How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning,Yes.,3,"""Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships.""",2023,2023-05-04T07:45:49Z,,,
arXIv2023,Faithful Question Answering with Monte-Carlo Planning,Yes.,3,"""Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging.""",2023,2023-05-04T05:21:36Z,,,
arXIv2023,PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits,Yes.,2,"""there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits"" and ""the accuracy drops significantly when the annotators were informed of AI authorship.""",2023,2023-05-04T04:58:00Z,,,
arXIv2023,AutoML-GPT: Automatic Machine Learning with GPT,Yes.,1,"""Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction.""",2023,2023-05-04T02:09:43Z,,,
arXIv2023,Personalized Abstractive Summarization by Tri-agent Generation Pipeline,Yes.,3,"""Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities.""",2023,2023-05-04T01:12:35Z,,,
arXIv2023,Black-box Prompt Tuning with Subspace Learning,Yes.,3,"""Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces.""",2023,2023-05-04T01:04:25Z,,,
arXIv2023,The System Model and the User Model: Exploring AI Dashboard Design,Yes.,2,"""Recently there has been a surge of attention to chatbots based on large language models, including widely reported unsavory interactions.""",2023,2023-05-04T00:22:49Z,,,
arXIv2023,Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs,Yes.,2,"""However, these models are extremely computationally expensive, even at inference time, raising the natural question",2023,2023-05-03T21:51:42Z,,,
arXIv2023,Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory,Yes.,1,"""selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round.""",2023,2023-05-03T21:40:54Z,,,
arXIv2023,Entity Tracking in Language Models,Yes.,3,"""there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities"" and ""pretraining on text corpora alone does not make this capacity surface.""",2023,2023-05-03T18:01:13Z,,,
arXIv2023,Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings,Yes.,3,"""Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks.""",2023,2023-05-03T17:58:29Z,,,
arXIv2023,Evaluating BERT-based Scientific Relation Classifiers for Scholarly Knowledge Graph Construction on Digital Library Collections,Yes.,3,"""The optimal classifier's performance can decline by around 10% to 20% in F-score on the noisy corpora.""",2023,2023-05-03T17:32:16Z,,,
arXIv2023,WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models,Yes.,1,"""the second uses few-shot in-context learning (ICL) with a large language model (LLM).""",2023,2023-05-03T15:58:28Z,,,
arXIv2023,Judgments of research co-created by generative AI: experimental evidence,Yes.,2,"""People judged delegating to an LLM as less acceptable than delegating to a human (d = -0.78). Delegation to an LLM also decreased trust to oversee future research projects (d = -0.80), and people thought the results would be less accurate and of lower quality (d = -0.85).""",2023,2023-05-03T15:57:39Z,,,
arXIv2023,Can Large Language Models Be an Alternative to Human Evaluations?,Yes.,3,"""We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.""",2023,2023-05-03T07:28:50Z,,,
arXIv2023,Improving Contrastive Learning of Sentence Embeddings from AI Feedback,Yes.,1,"""Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning.""",2023,2023-05-03T06:26:13Z,,,
arXIv2023,SCOTT: Self-Consistent Chain-of-Thought Distillation,Yes.,3,"""Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions.""",2023,2023-05-03T03:47:00Z,,,
arXIv2023,KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness,Yes.,3,"""we demonstrate that KEPLMs without incorporating the topic entities will lead to insufficient entity interaction and biased (relation) word semantics.""",2023,2023-05-02T22:28:26Z,,,
arXIv2023,Multimodal Procedural Planning via Dual Text-Image Prompting,Yes.,1,"""TIP, a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models.""",2023,2023-05-02T21:46:44Z,,,
arXIv2023,Automated Code generation for Information Technology Tasks in YAML through Large Language Models,Yes.,1,"""The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages.""",2023,2023-05-02T21:01:01Z,,,
arXIv2023,Few-shot In-context Learning for Knowledge Base Question Answering,Yes.,1,"""KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations.""",2023,2023-05-02T19:31:55Z,,,
arXIv2023,Privacy-Preserving In-Context Learning for Large Language Models,Yes.,3,"""LLM's responses may leak the sensitive private information contained in in-context exemplars.""",2023,2023-05-02T17:52:58Z,,,
arXIv2023,Finding Neurons in a Haystack: Case Studies with Sparse Probing,Yes.,3,"""Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood.""",2023,2023-05-02T17:13:55Z,,,
arXIv2023,FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information,Yes.,1,"""Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone.""",2023,2023-05-02T15:36:10Z,,,
arXIv2023,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset",Yes.,3,"""Experimental results show that the existing models perform far lower than expected and the released dataset is still challenging in the pre-trained language model era.""",2023,2023-05-02T15:33:01Z,,,
arXIv2023,VPGTrans: Transfer Visual Prompt Generator across LLMs,Yes.,1,"""While developing a new multimodal LLM (MLLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm.""",2023,2023-05-02T09:28:39Z,,,
arXIv2023,A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models,Yes.,2,"""we address the important concern of privacy in LLM-driven MT and suggest essential privacy-preserving strategies.""",2023,2023-05-02T03:27:27Z,,,
arXIv2023,Complex Logical Reasoning over Knowledge Graphs using Large Language Models,Yes.,1,"""to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively.""",2023,2023-05-02T02:21:49Z,,,
arXIv2023,RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models,Yes.,1,"""We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS).""",2023,2023-05-02T01:33:02Z,,,
arXIv2023,Evaluating statistical language models as pragmatic reasoners,Yes.,3,"""We find that LLMs can derive context-grounded, human-like distributions over the interpretations of several complex pragmatic utterances, yet struggle composing with negation.""",2023,2023-05-01T18:22:10Z,,,
arXIv2023,Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs,Yes.,3,"""We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research.""",2023,2023-05-01T17:09:33Z,,,
arXIv2023,Using Large Language Models to Generate JUnit Tests: An Empirical Study,Yes.,3,"""We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.""",2023,2023-04-30T07:28:06Z,,,
arXIv2023,Beyond Classification: Financial Reasoning in State-of-the-Art Language Models,Yes.,2,"""However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored.""",2023,2023-04-30T04:36:05Z,,,
arXIv2023,LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model,Yes.,3,"""Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4.""",2023,2023-04-28T17:59:25Z,,,
arXIv2023,Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs,Yes.,3,"""Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently.""",2023,2023-04-28T17:39:49Z,,,
arXIv2023,Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?,Yes.,3,"""Although there exist architectures and models to encode documents fully, they are in general limited to English and few other high-resourced languages.""",2023,2023-04-28T12:11:21Z,,,
arXIv2023,Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks,Yes.,3,"""Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM.""",2023,2023-04-28T10:15:25Z,,,
arXIv2023,Towards autonomous system: flexible modular production system enhanced with large language model agents,Yes.,2,"""This research highlights the potential of integrating LLMs into industrial automation systems in the context of smart factory for more agile, flexible, and adaptive production processes, while it also underscores the critical insights and limitations for future work.""",2023,2023-04-28T09:42:18Z,,,
arXIv2023,PMC-LLaMA: Towards Building Open-source Language Models for Medicine,Yes.,3,"""While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge.""",2023,2023-04-27T18:29:05Z,,,
arXIv2023,LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions,Yes.,2,"""Large language models (LLMs) with instruction fine-tuning demonstrate superior generative capabilities. However, these models are resource-intensive.""",2023,2023-04-27T17:58:49Z,,,
arXIv2023,IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers,Yes.,3,"""However, these methods still suffer from limitations in terms of generation quality, diversity, and flexibility.""",2023,2023-04-27T17:58:02Z,,,
arXIv2023,CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants,Yes.,2,"""A major challenge in deploying LLM-based virtual conversational assistants in real world settings is ensuring they operate within what is admissible for the task.""",2023,2023-04-27T17:39:11Z,,,
arXIv2023,Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems,Yes.,3,"""This paper identifies the limitation of current LLM approaches, particularly ChatGPT in selected practical problems native to oil and gas engineering but not exclusively.""",2023,2023-04-27T17:33:49Z,,,
arXIv2023,ICE-Score: Instructing Large Language Models to Evaluate Code,Yes.,3,"""Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement.""",2023,2023-04-27T16:38:17Z,,,
arXIv2023,Controlled Text Generation with Natural Language Instructions,Yes.,3,"""Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications.""",2023,2023-04-27T15:56:34Z,,,
arXIv2023,Origin Tracing and Detecting of LLMs,Yes.,3,"""We provide valuable observations based on the experimental results, such as the difficulty level of AI origin tracing, and the AI origin similarities, and call for ethical concerns of LLM providers.""",2023,2023-04-27T10:05:57Z,,,
arXIv2023,Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering,Yes.,3,"""Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques.""",2023,2023-04-27T01:48:03Z,,,
arXIv2023,Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables,Yes.,1,"""we propose to extend relational databases with so-called multi-modal operators (MMOps) which are based on the advances of recent large language models such as GPT-3.""",2023,2023-04-26T13:31:04Z,,,
arXIv2023,Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval,Yes.,3,"""Large language models (LLMs) work well in natural language generation tasks, but they are not specifically pre-trained to understand the syntax and semantics of SQL commands.""",2023,2023-04-26T06:02:01Z,,,
arXIv2023,The Closeness of In-Context Learning and Weight Shifting for Softmax Regression,Yes.,1,"""Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks.""",2023,2023-04-26T04:33:41Z,,,
arXIv2023,What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files,Yes.,3,"""We also identify key limitations to using LLMs with text data alone, and our findings provide a strong motivation for further work into multi-modal text-geometry models.""",2023,2023-04-25T12:30:01Z,,,
arXIv2023,Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting,Yes.,3,"""Under zero-shot setting, evaluation results reveal ChatGPT's promising ability to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision).""",2023,2023-04-25T04:09:45Z,,,
arXIv2023,Improved Trust in Human-Robot Collaboration with ChatGPT,Yes.,1,"""The emergence of Large Language Models (LLMs), such as ChatGPT, provides an opportunity to develop an interactive, communicative, and robust human-robot collaboration approach.""",2023,2023-04-25T02:48:35Z,,,
arXIv2023,WizardLM: Empowering Large Language Models to Follow Complex Instructions,Yes.,2,"""Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs.""",2023,2023-04-24T16:31:06Z,,,
arXIv2023,ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain,No.,1,The abstract discusses a BERT-based focused crawler for cybersecurity and does not mention LLMs or their limitations.,2023,2023-04-24T09:53:33Z,,,
arXIv2023,Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology,Yes.,3,"""ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent"" and ""Because of the risk of hallucination, facts provided by ChatGPT always need to be verified.""",2023,2023-04-24T09:50:39Z,,,
arXIv2023,Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT,Yes.,3,"""GPT-based zero-shot classification models tend to make independent predictions over test instances, which can be sub-optimal as the instance correlations and the decision boundaries in the target space are ignored.""",2023,2023-04-24T07:35:38Z,,,
arXIv2023,PARAGRAPH2GRAPH: A GNN-based framework for layout paragraph analysis,No.,1,The abstract does not mention language models or large language models.,2023,2023-04-24T03:54:48Z,,,
arXIv2023,A Lightweight Constrained Generation Alternative for Query-focused Summarization,Yes.,3,"""Current QFS approaches typically involve injecting additional information, e.g. query-answer relevance or fine-grained token-level interaction between a query and document, into a finetuned large language model. However, these approaches often require extra parameters \& training, and generalize poorly to new dataset distributions.""",2023,2023-04-23T18:43:48Z,,,
arXIv2023,Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release,Yes.,1,"""DomMa targets at testing Large Language Models (LLMs) on their domain knowledge understanding, it features extensive domain coverage, large data volume, and a continually updated data set based on Chinese 112 first-level subject classifications.""",2023,2023-04-23T15:11:49Z,,,
arXIv2023,Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models,Yes.,3,"""the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty.""",2023,2023-04-23T13:54:39Z,,,
arXIv2023,"Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",Yes.,3,"""Our findings reveal that ChatGPT's performance in Standard-IE setting is poor,"" and ""there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration.""",2023,2023-04-23T12:33:18Z,,,
arXIv2023,Divide and Prompt: Chain of Thought Prompting for Text-to-SQL,Yes.,1,"""Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks.""",2023,2023-04-23T06:52:35Z,,,
arXIv2023,Boosting Theory-of-Mind Performance in Large Language Models via Prompting,Yes.,3,"""Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning.""",2023,2023-04-22T22:50:50Z,,,
arXIv2023,N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models,Yes.,1,"""N2G represents a step towards scalable interpretability methods by allowing us to convert neurons in an LLM to interpretable representations of measurable quality.""",2023,2023-04-22T19:06:13Z,,,
arXIv2023,SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval,Yes.,3,"""most existing language models have difficulty understanding the long-distance dependencies between different structures"" and ""existing pre-trained language models designed for general purposes have not been equipped to handle legal elements.""",2023,2023-04-22T10:47:01Z,,,
arXIv2023,DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction,Yes.,1,"""There is currently a significant gap between the performance of fine-tuned models and prompting approaches using Large Language Models (LLMs) on the challenging task of text-to-SQL, as evaluated on datasets such as Spider.""",2023,2023-04-21T15:02:18Z,,,
arXIv2023,Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition,Yes.,3,"""On the other hand, the experiments highlighted the difficulty of GPT-3 in carrying out tasks that require a certain degree of reasoning, such as arithmetic operations.""",2023,2023-04-21T14:21:52Z,,,
arXIv2023,SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model,Yes.,1,"""we present SkinGPT-4, which is the world's first interactive dermatology diagnostic system powered by an advanced visual large language model.""",2023,2023-04-21T01:17:09Z,,,
arXIv2023,"""HOT"" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media",Yes.,2,"""Our findings also suggest that ChatGPT classifications align with provided HOT definitions, but ChatGPT classifies 'hateful' and 'offensive' as subsets of 'toxic.' Moreover, the choice of prompts used to interact with ChatGPT",2023,2023-04-20T19:40:51Z,,,
arXIv2023,MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models,Yes.,3,"""In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation).""",2023,2023-04-20T18:25:35Z,,,
arXIv2023,Learning to Plan with Natural Language,Yes.,3,"""LLMs can directly generate task plans, but these plans may still contain factual errors or are incomplete.""",2023,2023-04-20T17:09:12Z,,,
arXIv2023,GPT-NER: Named Entity Recognition via Large Language Models,Yes.,3,"""its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs",2023,2023-04-20T16:17:26Z,,,
arXIv2023,CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population,Yes.,2,"""Empirical results show that the population task is still challenging, even for large language models (LLM) such as ChatGPT.""",2023,2023-04-20T15:27:29Z,,,
arXIv2023,Interventional Probing in High Dimensions: An NLI Case Study,Yes.,1,"""Probing strategies have been shown to detect the presence of various linguistic features in large language models; in particular, semantic features intermediate to the 'natural logic' fragment of the Natural Language Inference task (NLI).""",2023,2023-04-20T14:34:31Z,,,
arXIv2023,Analyzing FOMC Minutes: Accuracy and Constraints of Language Models,Yes.,3,"""The study also highlights the challenges and limitations of using current NLP techniques to analyze FOMC texts and suggests the potential for enhancing language models and exploring alternative approaches.""",2023,2023-04-20T08:54:00Z,,,
arXIv2023,Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks,Yes.,3,"""ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain.""",2023,2023-04-20T08:08:12Z,,,
arXIv2023,SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,Yes.,3,"""Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long paragraphs.""",2023,2023-04-19T21:22:52Z,,,
arXIv2023,Low-resource Bilingual Dialect Lexicon Induction with Large Language Models,Yes.,2,"""This setup poses several unique challenges, including the scarcity of resources, the relatedness of the languages, and the lack of standardization in the orthography of dialects.""",2023,2023-04-19T20:20:41Z,,,
arXIv2023,Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers,Yes.,1,"""This study utilizes a novel pre-labeled dataset of 38048 physician reviews to establish the effectiveness of large language models in classifying reviews.""",2023,2023-04-19T19:59:26Z,,,
arXIv2023,GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information,Yes.,3,"""While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations.""",2023,2023-04-19T13:53:19Z,,,
arXIv2023,Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents,Yes.,3,"""The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge.""",2023,2023-04-19T10:16:03Z,,,
arXIv2023,A Theory on Adam Instability in Large-Scale Machine Learning,Yes.,3,"""We present a theory for the previously unexplained divergent behavior noticed in the training of large language models."" and ""This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training.""",2023,2023-04-19T06:15:11Z,,,
arXIv2023,Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes,Yes.,1,"""We identify two fundamentally different strategies for implementing this system",2023,2023-04-19T06:00:26Z,,,
arXIv2023,TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection,No.,1,The abstract does not mention LLMs or any language models.,2023,2023-04-19T04:47:36Z,,,
arXIv2023,LLM as A Robotic Brain: Unifying Egocentric Memory and Control,Yes.,1,"""In this paper, we propose a novel and generalizable framework called LLM-Brain",2023,2023-04-19T00:08:48Z,,,
arXIv2023,Creating Large Language Model Resistant Exams: Guidelines and Strategies,Yes.,3,"""This article investigates the performance of LLMs on exams and their implications for assessment, focusing on ChatGPT's abilities and limitations.""",2023,2023-04-18T18:01:32Z,,,
arXIv2023,Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling,Yes.,3,"""Post-training quantization~(PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations.""",2023,2023-04-18T17:34:23Z,,,
arXIv2023,Towards Designing a ChatGPT Conversational Companion for Elderly People,Yes.,3,"""However, it is essential to acknowledge the limitations of ChatGPT, such as potential biases and misinformation, and to consider the ethical implications of using AI-based companionship for the elderly, including privacy concerns.""",2023,2023-04-18T17:24:14Z,,,
arXIv2023,Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task,Yes.,2,"""Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested.""",2023,2023-04-18T17:21:48Z,,,
arXIv2023,Safer Conversational AI as a Source of User Delight,Yes.,3,"""However, some users argue that current approaches to moderation limit the technology, compromise free expression, and limit the value delivered by the technology.""",2023,2023-04-18T11:03:10Z,,,
arXIv2023,Masked Language Model Based Textual Adversarial Example Detection,Yes.,1,"""whereas pre-trained masked language models can fit the manifold of normal NLP data.""",2023,2023-04-18T06:52:14Z,,,
arXIv2023,A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models,Yes.,3,"""We finally discuss existing challenges and promising future directions in the era of LLMs.""",2023,2023-04-18T06:38:40Z,,,
arXIv2023,CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models,Yes.,2,"""However, their ability to generalize to unseen tasks in more complex fields, such as biology, has yet to be fully evaluated.""",2023,2023-04-18T02:49:53Z,,,
arXIv2023,Large Language Models Based Automatic Synthesis of Software Specifications,Yes.,1,"""we propose SpecSyn a framework that leverages a state-of-the-art large language model to automatically synthesize software specifications from natural language sources.""",2023,2023-04-18T01:22:44Z,,,
arXIv2023,Classification of US Supreme Court Cases using BERT-Based Techniques,Yes.,3,"""An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis.""",2023,2023-04-17T22:53:54Z,,,
arXIv2023,Visual Instruction Tuning,Yes.,1,"""Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field.""",2023,2023-04-17T17:59:25Z,,,
arXIv2023,LongForm: Effective Instruction Tuning with Reverse Instructions,Yes.,2,"""obtaining instruction data is costly and challenging"" and ""generating noisy examples via LLMs.""",2023,2023-04-17T17:36:35Z,,,
arXIv2023,ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT,Yes.,3,"""such models often require substantial amounts of medical text data and have poor generalization performance"" and ""their performance in specific domains, such as radiology, remains under-investigated and potentially limited.""",2023,2023-04-17T17:13:42Z,,,
arXIv2023,Low-code LLM: Graphical User Interface over Large Language Models,Yes.,2,"""Utilizing Large Language Models (LLMs) for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process.""",2023,2023-04-17T09:27:40Z,,,
arXIv2023,InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction,Yes.,3,"""recent studies have shown that existing large models still have difficulty with information extraction tasks"" and ""gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.""",2023,2023-04-17T09:00:50Z,,,
arXIv2023,SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model,Yes.,2,"""Directly prompting the latest conversational LLM for standard skills, however, is slow, costly and inaccurate.""",2023,2023-04-17T08:43:20Z,,,
arXIv2023,Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding,Yes.,3,"""We lay out challenges and opportunities in using LLMs to support qualitative coding and beyond.""",2023,2023-04-17T04:52:43Z,,,
arXIv2023,A Comprehensive Evaluation of Neural SPARQL Query Generation from Natural Language Questions,Yes.,3,"""Finally, the performance of the tested LLMs fell short of achieving the desired outcomes.""",2023,2023-04-16T13:12:26Z,,,
arXIv2023,Solving Math Word Problems by Combining Language Models With Symbolic Solvers,Yes.,3,"""prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning.""",2023,2023-04-16T04:16:06Z,,,
arXIv2023,Tractable Control for Autoregressive Language Generation,Yes.,3,"""it remains a major challenge to generate text that satisfies complex constraints",2023,2023-04-15T00:19:44Z,,,
arXIv2023,Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models,Yes.,2,"""Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios.""",2023,2023-04-14T21:19:46Z,,,
arXIv2023,OpenAssistant Conversations -- Democratizing Large Language Model Alignment,Yes.,3,"""state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary.""",2023,2023-04-14T18:01:29Z,,,
arXIv2023,API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs,Yes.,3,"""However, three pivotal questions remain unanswered",2023,2023-04-14T14:05:32Z,,,
arXIv2023,DroidBot-GPT: GPT-powered UI Automation for Android,Yes.,1,"""This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications.""",2023,2023-04-14T11:31:56Z,,,
arXIv2023,MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data,Yes.,2,"""Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy.""",2023,2023-04-14T11:28:08Z,,,
arXIv2023,HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge,Yes.,3,"""LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses.""",2023,2023-04-14T07:54:17Z,,,
arXIv2023,nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss Prediction across Scales,Yes.,2,"""As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones.""",2023,2023-04-14T00:45:01Z,,,
arXIv2023,On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence,Yes.,3,"""Despite their successes in language and vision tasks, we have yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI)."" and ""existing foundation models still underperform task-specific models.""",2023,2023-04-13T19:50:17Z,,,
arXIv2023,Verbs in Action: Improving verb understanding in video-language models,Yes.,3,"""state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understanding.""",2023,2023-04-13T17:57:01Z,,,
arXIv2023,Are LLMs All You Need for Task-Oriented Dialogue?,Yes.,3,"""We show that for explicit belief state tracking, LLMs underperform compared to specialized task-specific models.""",2023,2023-04-13T14:03:14Z,,,
arXIv2023,AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models,Yes.,3,"""In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge.""",2023,2023-04-13T09:39:30Z,,,
arXIv2023,Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report,Yes.,1,"""In this paper, we introduce a novel approach to ECG interpretation, leveraging recent breakthroughs in Large Language Models (LLMs) and Vision-Transformer (ViT) models.""",2023,2023-04-13T06:32:25Z,,,
arXIv2023,Language Instructed Reinforcement Learning for Human-AI Coordination,Yes.,1,"""We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective.""",2023,2023-04-13T04:47:31Z,,,
arXIv2023,Detection of Fake Generated Scientific Abstracts,Yes.,3,"""The academic community has taken notice of these technological advancements and has expressed concerns regarding the difficulty of discriminating between what is real and what is artificially generated.""",2023,2023-04-12T20:20:22Z,,,
arXIv2023,Can Large Language Models Transform Computational Social Science?,Yes.,3,"""On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans.""",2023,2023-04-12T17:33:28Z,,,
arXIv2023,Training Large Language Models Efficiently with Sparsity and Dataflow,Yes.,3,"""However, training such large foundational models is a non-trivial exercise that requires a significant amount of compute power and expertise from machine learning and systems experts. As models get larger, these demands are only increasing. Sparsity is a promising technique to relieve the compute requirements for training. However, sparsity introduces new challenges in training the sparse model to the same quality as the dense counterparts",2023,2023-04-11T21:37:13Z,,,
arXIv2023,TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning,Yes.,1,"""The surge of artificial intelligence, specifically large language models, has led to a rapid advent towards the development of large-scale machine learning training clusters.""",2023,2023-04-11T15:50:54Z,,,
arXIv2023,Approximating Online Human Evaluation of Social Chatbots with Prompting,Yes.,2,"""Existing evaluation metrics aim to automate offline user evaluation and approximate human judgment of pre-curated dialogs. However, they are limited in their ability to capture subjective perceptions of users who actually interact with the bots and might not generalize to real-world settings.""",2023,2023-04-11T14:45:01Z,,,
arXIv2023,Teaching Large Language Models to Self-Debug,Yes.,3,"""However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance.""",2023,2023-04-11T10:43:43Z,,,
arXIv2023,Human-machine cooperation for semantic feature listing,Yes.,3,"""Large language models (LLMs) offer a novel avenue for the automatic generation of such feature lists, but are prone to significant error.""",2023,2023-04-11T06:38:04Z,,,
arXIv2023,On the Possibilities of AI-Generated Text Detection,Yes.,1,"""Our work addresses the critical issue of distinguishing text generated by Large Language Models (LLMs) from human-produced text, a task essential for numerous applications.""",2023,2023-04-10T17:47:39Z,,,
arXIv2023,Inference with Reference: Lossless Acceleration of Large Language Models,Yes.,1,"""We propose LLMA, an LLM accelerator to losslessly speed up Large Language Model (LLM) inference with references.""",2023,2023-04-10T09:55:14Z,,,
arXIv2023,A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding,Yes.,3,"""extensive analysis shows that ChatGPT benefits from the multi-turn interactive prompt in the DST task but struggles to perform slot filling for SLU"" and ""we summarize several unexpected behaviors of ChatGPT in dialogue understanding tasks.""",2023,2023-04-09T15:28:36Z,,,
arXIv2023,Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding,Yes.,3,"""Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams.""",2023,2023-04-08T20:41:15Z,,,
arXIv2023,Comparing Code Explanations Created by Students and Large Language Models,Yes.,1,"""The recent emergence of powerful large language models (LLMs) may offer a solution.""",2023,2023-04-08T06:52:54Z,,,
arXIv2023,GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation,Yes.,2,"""current models commonly treat items as mere IDs and adopt discriminative modeling, resulting in limitations of (1) fully leveraging the content information of items and the language modeling capabilities of NLP models; (2) interpreting user interests to improve relevance and diversity; and (3) adapting practical circumstances such as growing item inventories.""",2023,2023-04-08T00:30:08Z,,,
arXIv2023,Why think step by step? Reasoning emerges from the locality of experience,Yes.,1,"""Similarly, when large language models generate intermediate steps (a chain of thought) before answering a question, they often produce better answers than they would directly.""",2023,2023-04-07T21:04:03Z,,,
arXIv2023,Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,Yes.,2,"""However, there is currently a lack of benchmark datasets for assessing the ability of LLMs to generate functionally correct code edits based on natural language descriptions of intended changes.""",2023,2023-04-07T18:58:33Z,,,
arXIv2023,Interpretable Unified Language Checking,Yes.,2,"""Despite recent concerns about undesirable behaviors generated by large language models (LLMs), including non-factual, biased, and hateful language, we find LLMs are inherent multi-task language checkers based on their latent representations of natural and social knowledge.""",2023,2023-04-07T16:47:49Z,,,
arXIv2023,What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory,Yes.,3,"""There has been concern about ideological basis and possible discrimination in text generated by Large Language Models (LLMs).""",2023,2023-04-07T12:20:13Z,,,
arXIv2023,Generative Agents: Interactive Simulacra of Human Behavior,Yes.,1,"""To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior.""",2023,2023-04-07T01:55:19Z,,,
arXIv2023,Towards Interpretable Mental Health Analysis with Large Language Models,Yes.,3,"""existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability.""",2023,2023-04-06T19:53:59Z,,,
arXIv2023,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Yes.,2,"""Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity.""",2023,2023-04-06T17:59:03Z,,,
arXIv2023,Instruction Tuning with GPT-4,Yes.,1,"""In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning.""",2023,2023-04-06T17:58:09Z,,,
arXIv2023,Zero-Shot Next-Item Recommendation using Large Pretrained Language Models,Yes.,3,"""We have identified two major challenges that must be addressed to enable LLMs to act effectively as recommenders. First, the recommendation space can be extremely large for LLMs, and LLMs do not know about the target user's past interacted items and preferences.""",2023,2023-04-06T15:35:11Z,,,
arXIv2023,Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media,Yes.,3,"""Traditional approaches include conventional machine learning, early deep neural networks, and pre-trained fine-tuning models. However, with the evolution of very large pre-trained language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face deployment challenges."" and ""demonstr",2023,2023-04-06T14:12:02Z,,,
arXIv2023,Multi-label classification of open-ended questions with BERT,Yes.,1,"""This paper focuses on multi-label classification of text answers to open-ended survey questions in social science surveys. We evaluate the performance of the transformer-based architecture BERT for the German language in comparison to traditional multi-label algorithms.""",2023,2023-04-06T09:09:44Z,,,
arXIv2023,"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",Yes.,2,"""Our conclusion outlines obstacles that generative writing assistants create for copyright.""",2023,2023-04-06T03:09:26Z,,,
arXIv2023,Context-Aware Classification of Legal Document Pages,Yes.,3,"""they typically cannot be utilized with large pre-trained language models due to the constraint on input length.""",2023,2023-04-05T23:14:58Z,,,
arXIv2023,Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks,Yes.,1,"""We have demonstrated that the proposed semi-supervised GAN-LM architecture (generative adversarial network on top of a pretrained language model) is a viable solution in classifying Bengali fake reviews.""",2023,2023-04-05T20:40:09Z,,,
arXIv2023,Efficient OCR for Building a Diverse Digital History,No.,1,The abstract does not mention LLMs or any limitations related to them.,2023,2023-04-05T20:36:04Z,,,
arXIv2023,Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning,Yes.,1,"""SPIRES, a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema.""",2023,2023-04-05T19:07:04Z,,,
arXIv2023,Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT,Yes.,1,"""Additionally, we designed experiments to predict the electrical performance of solar cells and design materials or devices with targeted parameters using large language models (LLMs).""",2023,2023-04-05T04:01:52Z,,,
arXIv2023,Document-Level Machine Translation with Large Language Models,Yes.,2,"""This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.""",2023,2023-04-05T03:49:06Z,,,
arXIv2023,ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules,Yes.,1,"""Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.""",2023,2023-04-05T00:25:27Z,,,
arXIv2023,Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in geotechnical engineering,Yes.,3,"""GPT models can sometimes generate plausible-sounding but false outputs, leading to hallucinations.""",2023,2023-04-04T21:47:41Z,,,
arXIv2023,Dialogue-Contextualized Re-ranking for Medical History-Taking,Yes.,1,"""We test both transformer and S4-based language model backbones.""",2023,2023-04-04T17:31:32Z,,,
arXIv2023,Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems,Yes.,1,"""For a natural language problem that requires some non-trivial reasoning to solve, there are at least two ways to do it using a large language model (LLM).""",2023,2023-04-04T13:01:48Z,,,
arXIv2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,Yes.,2,"""We present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias.""",2023,2023-04-03T20:58:15Z,,,
arXIv2023,Classification of integers based on residue classes via modern deep learning algorithms,Yes.,3,"""Finally, we evaluated Large Language Models (LLMs) such as GPT-4, GPT-J, LLaMA and Falcon, and demonstrated their failures.""",2023,2023-04-03T19:53:31Z,,,
arXIv2023,Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning,Yes.,3,"""While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource languages, are limited.""",2023,2023-04-03T18:46:01Z,,,
arXIv2023,Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data,Yes.,2,"""However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field.""",2023,2023-04-03T17:59:09Z,,,
arXIv2023,RPTQ: Reorder-based Post-training Quantization for Large Language Models,Yes.,3,"""Large-scale language models (LLMs) have demonstrated impressive performance, but their deployment presents challenges due to their significant memory usage.""",2023,2023-04-03T15:46:15Z,,,
arXIv2023,Can the Inference Logic of Large Language Models be Disentangled into Symbolic Concepts?,Yes.,3,"""Many recent studies have discovered that traditional DNNs usually encode sparse symbolic concepts. However, because an LLM has much more parameters than traditional DNNs, whether the LLM also encodes sparse symbolic concepts is still an open problem.""",2023,2023-04-03T15:39:35Z,,,
arXIv2023,Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?,Yes.,3,"""Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating",2023,2023-04-03T14:06:47Z,,,
arXIv2023,Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection,Yes.,1,"""This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families",2023,2023-04-03T10:27:53Z,,,
arXIv2023,Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study,Yes.,2,"""However, directly comparing the quality of two texts may lead to suboptimal results.""",2023,2023-04-03T05:29:58Z,,,
arXIv2023,A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets,Yes.,2,"""High-quality datasets are needed for general-purpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation.""",2023,2023-04-02T08:26:38Z,,,
arXIv2023,Demonstration of InsightPilot: An LLM-Empowered Automated Data Exploration System,Yes.,1,"""we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system designed to simplify the data exploration process.""",2023,2023-04-02T07:27:49Z,,,
arXIv2023,Querying Large Language Models with SQL,Yes.,3,"""However, we pinpoint several research challenges that must be addressed to build a DBMS that exploits LLMs.""",2023,2023-04-02T06:58:14Z,,,
arXIv2023,DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection,Yes.,1,"""We demonstrate that large language models (LLMs) are a promising research direction for ML-based vulnerability detection, outperforming Graph Neural Networks (GNNs) with code-structure features in our experiments.""",2023,2023-04-01T23:29:14Z,,,
arXIv2023,Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT,Yes.,3,"""While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches.""",2023,2023-04-01T20:57:33Z,,,
arXIv2023,"Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics",Yes.,3,"""Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials.""",2023,2023-04-01T06:04:58Z,,,
arXIv2023,Large language models can rate news outlet credibility,Yes.,3,"""Although large language models (LLMs) have shown exceptional performance in various natural language processing tasks, they are prone to hallucinations.""",2023,2023-04-01T05:04:06Z,,,
arXIv2023,Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts,Yes.,3,"""Recent works involving large language models (LLM) demonstrate considerable success in text generation and revision tasks; however, LLMs still struggle to provide structural and creative feedback on the document level that is crucial to academic writing.""",2023,2023-03-31T20:33:03Z,,,
arXIv2023,A Survey of Large Language Models,Yes.,3,"""Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.""",2023,2023-03-31T17:28:46Z,,,
arXIv2023,BERTino: an Italian DistilBERT model,Yes.,3,"""if on one hand the performances achieved by this kind of architectures are surprising, on the other their usability is limited by the high number of parameters which constitute their network, resulting in high computational and memory demands.""",2023,2023-03-31T15:07:40Z,,,
arXIv2023,AceCoder: Utilizing Existing Code to Enhance Code Generation,Yes.,3,"""Existing prompting techniques are designed for natural language generation and have low accuracy in code generation.""",2023,2023-03-31T02:57:15Z,,,
arXIv2023,Can ChatGPT be used to generate scientific hypotheses?,Yes.,3,"""While the error rate is high, generative AI seems to be able to effectively structure vast amounts of scientific knowledge and provide interesting and testable hypotheses.""",2023,2023-03-30T20:40:52Z,,,
arXIv2023,Self-Refine: Iterative Refinement with Self-Feedback,Yes.,2,"""Like humans, large language models (LLMs) do not always generate the best output on their first try.""",2023,2023-03-30T18:30:01Z,,,
arXIv2023,Language Models can Solve Computer Tasks,Yes.,2,"""However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks.""",2023,2023-03-30T16:01:52Z,,,
arXIv2023,WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research,Yes.,1,"""ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically.""",2023,2023-03-30T14:07:47Z,,,
arXIv2023,The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling,Yes.,3,"""This means that it may be challenging to build LLMs for smaller languages such as Nordic ones, where the availability of text corpora is limited.""",2023,2023-03-30T06:42:22Z,,,
arXIv2023,Advances in apparent conceptual physics reasoning in GPT-4,Yes.,3,"""Indeed, its responses come quite close to perfectly demonstrating expert-level competence, with a few very notable exceptions and limitations.""",2023,2023-03-29T20:32:40Z,,,
arXIv2023,G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment,Yes.,3,"""highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts.""",2023,2023-03-29T12:46:54Z,,,
arXIv2023,Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning,Yes.,3,"""We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets"" and ""Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing.""",2023,2023-03-29T04:00:53Z,,,
arXIv2023,Training Language Models with Language Feedback at Scale,Yes.,3,"""Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries.""",2023,2023-03-28T17:04:15Z,,,
arXIv2023,Improving Code Generation by Training with Natural Language Feedback,Yes.,1,"""The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development.""",2023,2023-03-28T16:15:31Z,,,
arXIv2023,Evaluation of ChatGPT for NLP-based Mental Health Applications,Yes.,1,"""The zero-shot classification accuracy obtained with ChatGPT indicates a potential use of language models for mental health classification tasks.""",2023,2023-03-28T04:47:43Z,,,
arXIv2023,Pre-training Transformers for Knowledge Graph Completion,Yes.,1,"""Inspired by Transformer-based pretrained language models' success on learning transferable representation for texts.""",2023,2023-03-28T02:10:37Z,,,
arXIv2023,Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning,Yes.,3,"""These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters.""",2023,2023-03-28T00:06:38Z,,,
arXIv2023,Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses,Yes.,1,"""Furthermore, we propose a novel two-step prompt strategy, which combines this pre-edit scheme with ChatGPT, currently the most widely used large language model.""",2023,2023-03-27T20:33:40Z,,,
arXIv2023,TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models,Yes.,3,"""Jointly modeling multiple modalities significantly increases the model complexity, and makes the training process data-hungry.""",2023,2023-03-27T17:54:32Z,,,
arXIv2023,KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation,Yes.,2,"""large language models are underestimated by prior evaluation works.""",2023,2023-03-27T17:45:38Z,,,
arXIv2023,LMCanvas: Object-Oriented Interaction to Personalize Large Language Model-Powered Writing Environments,Yes.,2,"""these interfaces provide limited support for writers to create personal tools for their own unique tasks, and may not comprehensively fulfill a writer's needs -- requiring them to continuously switch between interfaces during writing.""",2023,2023-03-27T11:56:26Z,,,
arXIv2023,Large Language Models are Diverse Role-Players for Summarization Evaluation,Yes.,1,"""In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects.""",2023,2023-03-27T10:40:59Z,,,
arXIv2023,Coupling Artificial Neurons in BERT and Biological Neurons in the Human Brain,Yes.,3,"""However, two critical problems limit its advancement",2023,2023-03-27T01:41:48Z,,,
arXIv2023,MGTBench: Benchmarking Machine-Generated Text Detection,Yes.,2,"""This raises concerns regarding authenticity, accountability, and potential bias.""",2023,2023-03-26T21:12:36Z,,,
arXIv2023,WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation,Yes.,3,"""Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks.""",2023,2023-03-26T20:41:21Z,,,
arXIv2023,Task-oriented Memory-efficient Pruning-Adapter,Yes.,2,"""The Outstanding performance and growing size of Large Language Models has led to increased attention in parameter efficient learning."" and ""So efficiency of training and inference can't be obtained in the same time.""",2023,2023-03-26T12:18:00Z,,,
arXIv2023,Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System,Yes.,1,"""Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks.""",2023,2023-03-25T17:37:43Z,,,
arXIv2023,Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining,Yes.,3,"""many Pretrained Language Model (PLM) lack synonym knowledge due to limitation of small-scale synsets and PLM's pretraining objectives.""",2023,2023-03-25T10:19:14Z,,,
arXIv2023,Freestyle Layout-to-Image Synthesis,No.,1,The abstract does not mention LLMs or any limitations related to them.,2023,2023-03-25T09:37:41Z,,,
arXIv2023,Backdoor Attacks with Input-unique Triggers in NLP,Yes.,3,"""IDBA generates context-related triggers by continuing writing the input with a language model like GPT2.""",2023,2023-03-25T01:41:54Z,,,
arXIv2023,TRAK: Attributing Model Behavior at Scale,Yes.,1,"""We demonstrate the utility of TRAK across various modalities and scales",2023,2023-03-24T17:56:22Z,,,
arXIv2023,"""Get ready for a party"": Exploring smarter smart spaces with help from large language models",Yes.,1,"""In this paper, we leverage the observation that recent task-agnostic large language models (LLMs) like GPT-3 embody a vast amount of cross-domain, sometimes unpredictable contextual knowledge that existing rule-based home assistant systems lack, which can make them powerful tools for inferring user intent and generating appropriate context-dependent responses during smart home interactions.""",2023,2023-03-24T16:51:08Z,,,
arXIv2023,ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge,Yes.,3,"""The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice.""",2023,2023-03-24T15:29:16Z,,,
arXIv2023,Paraphrase Detection: Human vs. Machine Content,Yes.,3,"""Our main finding is that human-authored paraphrases exceed machine-generated ones in terms of difficulty, diversity, and similarity implying that automatically generated texts are not yet on par with human-level performance.""",2023,2023-03-24T13:25:46Z,,,
arXIv2023,Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods,Yes.,1,"""Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc.""",2023,2023-03-24T13:24:41Z,,,
arXIv2023,Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models,Yes.,3,"""utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level.""",2023,2023-03-24T05:05:03Z,,,
arXIv2023,Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function,Yes.,1,"""P-ToD uses a pre-trained GPT-2 as a backbone model and works in three phases.""",2023,2023-03-24T04:33:40Z,,,
arXIv2023,Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching,Yes.,1,"""We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data.""",2023,2023-03-24T03:14:00Z,,,
arXIv2023,The Quantization Model of Neural Scaling,Yes.,1,"""We validate this prediction on toy datasets, then study how scaling curves decompose for large language models.""",2023,2023-03-23T17:58:43Z,,,
arXIv2023,ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model,Yes.,3,"""While LLMs offer significant potential benefits, the challenges, such as data privacy, data quality, and model bias, need further study.""",2023,2023-03-23T15:34:26Z,,,
arXIv2023,A Simple Explanation for the Phase Transition in Large Language Models with List Decoding,Yes.,1,"""Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance is greatly improved after passing a certain critical threshold of scale.""",2023,2023-03-23T09:00:07Z,,,
arXIv2023,A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification,No.,1,The abstract does not mention any large language models (LLMs) or their limitations. It focuses on Transformer-based models and their application to clinical text classification tasks.,2023,2023-03-22T20:10:29Z,,,
arXIv2023,Interpretable Bangla Sarcasm Detection using BERT and Explainable AI,Yes.,1,"""In this article, we present a BERT-based system that can achieve 99.60\% while the utilized traditional machine learning algorithms are only capable of achieving 89.93\%.""",2023,2023-03-22T17:35:35Z,,,
arXIv2023,Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study,Yes.,3,"""LLM-based NLP, particularly ChatGPT with few-shot prompts, achieved high recall but generally lower precision and F1 scores.""",2023,2023-03-22T13:46:16Z,,,
arXIv2023,MEDIMP: 3D Medical Images with clinical Prompts from limited tabular data for renal transplantation,Yes.,1,"""taking inspiration from the recent success of Large Language Models (LLMs),"" and ""generates medical prompts using automatic textual data augmentations from LLMs.""",2023,2023-03-22T10:30:43Z,,,
arXIv2023,Large Language Models Can Be Used to Estimate the Latent Positions of Politicians,Yes.,1,"""We leverage the embedded knowledge in generative large language models (LLMs) to address this challenge and measure lawmakers' positions along specific political or policy dimensions.""",2023,2023-03-21T17:48:00Z,,,
arXIv2023,cTBLS: Augmenting Large Language Models with Conversational Tables,Yes.,3,"""Optimizing accuracy and performance while eliminating hallucinations of open-domain conversational large language models (LLMs) is an open research challenge.""",2023,2023-03-21T17:04:44Z,,,
arXIv2023,Logical Reasoning over Natural Language as Knowledge Representation: A Survey,Yes.,3,"""This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation and pretrained language models as reasoners,"" and ""challenges of the new paradigm.""",2023,2023-03-21T16:56:05Z,,,
arXIv2023,TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering,Yes.,2,"""we automatically generate several question-answer pairs using a language model"" and ""highlight the limitations and challenges of current models.""",2023,2023-03-21T14:41:02Z,,,
arXIv2023,ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing,Yes.,2,"""Potential ethical issues that could arise with the emergence of large language models like GPT-3, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.""",2023,2023-03-21T14:35:07Z,,,
arXIv2023,Implicit Neural Representation for Cooperative Low-light Image Enhancement,No.,1,The abstract focuses on low-light image enhancement methods and does not mention language models (LLMs or LMs).,2023,2023-03-21T10:24:29Z,,,
arXIv2023,A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Yes.,2,"""Impressed by the capability of the ChatGPT, many people are wondering about its limits",2023,2023-03-21T10:09:47Z,,,
arXIv2023,The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue,Yes.,3,"""There is a surge in interest in the development of open-domain chatbots, driven by the recent advancements of large language models."" and ""we question the assumptions behind open-domain chatbots and identify paths forward for enabling common ground in human-computer dialogue.""",2023,2023-03-21T10:01:49Z,,,
arXIv2023,Reflexion: Language Agents with Verbal Reinforcement Learning,Yes.,3,"""However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning.""",2023,2023-03-20T18:08:50Z,,,
arXIv2023,DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4,Yes.,1,"""The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability.""",2023,2023-03-20T11:34:37Z,,,
arXIv2023,Retrieving Multimodal Information for Augmented Generation: A Survey,Yes.,2,"""However, there lacks a unified perception of at which stage and how to incorporate different modalities.""",2023,2023-03-20T05:07:41Z,,,
arXIv2023,Dynamic Documentation for AI Systems,Yes.,2,"""documentation standards for AI remain inchoate, and fail to match the capabilities and social effects of increasingly impactful architectures such as Large Language Models (LLMs).""",2023,2023-03-20T04:23:07Z,,,
arXIv2023,Bangla Grammatical Error Detection Using T5 Transformer Model,Yes.,3,"""The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection.""",2023,2023-03-19T09:24:48Z,,,
arXIv2023,Revisiting the Plastic Surgery Hypothesis via Large Language Models,Yes.,3,"""the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.""",2023,2023-03-18T20:33:46Z,,,
arXIv2023,SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models,Yes.,3,"""Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also lead to highly prohibitive computational costs.""",2023,2023-03-18T17:56:01Z,,,
arXIv2023,An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering,Yes.,3,"""We carefully analyze the results of all PLMs-based KGQA basic frameworks on these benchmarks and two other popular datasets, WebQuestionSP and FreebaseQA, and find that knowledge distillation techniques and knowledge enhancement methods in PLMs are promising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal of attention in the NLP community, demonstrating its impressive capabilities and",2023,2023-03-18T08:57:09Z,,,
arXIv2023,Measuring Improvement of F$_1$-Scores in Detection of Self-Admitted Technical Debt,Yes.,2,"""Future research will look into ways to diversify SATD datasets in order to maximize the latent power in large BERT models.""",2023,2023-03-16T19:47:38Z,,,
arXIv2023,DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion,Yes.,1,"""our approach utilizes large language models to bridge texts and visual images for stylization and build an unsupervised generative model with a diffusion model backbone.""",2023,2023-03-16T19:12:52Z,,,
arXIv2023,LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations,Yes.,3,"""the security of the code they generate has not been extensively investigated nor documented.""",2023,2023-03-16T15:13:58Z,,,
arXIv2023,How well do Large Language Models perform in Arithmetic tasks?,Yes.,3,"""Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step.""",2023,2023-03-16T09:28:15Z,,,
arXIv2023,"Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential",Yes.,3,"""ChatGPT also presents some randomness in its responses with occasionally over-simplified or neglected information, which can be mitigated using a more detailed prompt."" and ""further efforts are needed to address limitations and maximize their potential.""",2023,2023-03-16T02:21:39Z,,,
arXIv2023,ART: Automatic multi-step reasoning and tool-use for large language models,Yes.,3,"""Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use.""",2023,2023-03-16T01:04:45Z,,,
arXIv2023,Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs,Yes.,3,"""These Large Language Models (LLMs) rely on pattern-matching rather than a true understanding of the semantic meaning of a sentence. As a result, they may generate incorrect responses.""",2023,2023-03-15T21:10:33Z,,,
arXIv2023,UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation,Yes.,3,"""the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization.""",2023,2023-03-15T10:53:49Z,,,
arXIv2023,A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records,Yes.,3,"""The generalizability of clinical large language models is usually ignored during the model development process.""",2023,2023-03-15T08:44:07Z,,,
arXIv2023,ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation,Yes.,3,"""revealing that none of the current methods can achieve the original model quality for quantization with either INT4-weight or INT4-weight-and-INT8-activation.""",2023,2023-03-15T01:27:15Z,,,
arXIv2023,Attention-likelihood relationship in transformers,Yes.,3,"""unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers.""",2023,2023-03-15T00:23:49Z,,,
arXIv2023,Chat with the Environment: Interactive Multimodal Perception Using Large Language Models,Yes.,2,"""Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold.""",2023,2023-03-14T23:01:27Z,,,
arXIv2023,Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures,Yes.,1,"""We explored 6 state-of-the-art pretrained transformer models for the three subtasks, including GatorTron, a large language model pretrained using >90 billion words of text.""",2023,2023-03-14T22:22:28Z,,,
arXIv2023,Do Transformers Parse while Predicting the Masked Word?,Yes.,1,"""Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling.""",2023,2023-03-14T17:49:50Z,,,
arXIv2023,Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family,Yes.,3,"""there is still a lack of large-scale, comprehensive testing of various types of complex questions to analyze the limitations of the model.""",2023,2023-03-14T15:46:28Z,,,
arXIv2023,A Theory of Emergent In-Context Learning as Implicit Structure Induction,Yes.,1,"""Despite progress, theoretical understanding of this phenomenon remains limited.""",2023,2023-03-14T15:24:05Z,,,
arXIv2023,RE-MOVE: An Adaptive Policy Design for Robotic Navigation Tasks in Dynamic Environments via Language-Based Feedback,Yes.,1,"""RE-MOVE incorporates an epistemic uncertainty-based framework to determine the optimal time to request instructions-based feedback. For the second challenge, we employ a zero-shot learning natural language processing (NLP) paradigm with efficient, prompt design and leverage state-of-the-art GPT-3.5, Llama-2 language models.""",2023,2023-03-14T04:20:59Z,,,
arXIv2023,Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification,Yes.,1,"""We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings.""",2023,2023-03-13T14:09:53Z,,,
arXIv2023,FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU,Yes.,3,"""The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators.""",2023,2023-03-13T05:19:28Z,,,
arXIv2023,Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks,Yes.,1,"""Large Language Models (LLMs) have demonstrated impressive text generation capabilities, prompting us to reconsider the future of human-AI co-creation and how humans interact with LLMs.""",2023,2023-03-11T15:45:47Z,,,
arXIv2023,"ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",Yes.,1,"""This paper presents prompt design techniques for software engineering, in the form of patterns, to solve common problems when using large language models (LLMs), such as ChatGPT to automate common software engineering activities.""",2023,2023-03-11T14:43:17Z,,,
arXIv2023,Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook,Yes.,2,"""Accepted evaluative metrics for LLMs are not human-centered.""",2023,2023-03-10T22:15:49Z,,,
arXIv2023,Susceptibility to Influence of Large Language Models,Yes.,3,"""some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data.""",2023,2023-03-10T16:53:30Z,,,
arXIv2023,Do large language models resemble humans in language use?,Yes.,3,"""However, their internal workings remain a black box, and it is unclear whether LLMs and chatbots can develop humanlike characteristics in language use."" and ""Finally, unlike humans, neither model preferred using shorter words to convey less informative content, nor did they use context to resolve syntactic ambiguities.""",2023,2023-03-10T10:47:59Z,,,
arXIv2023,Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors,Yes.,1,"""we use a large language model to query which interactions are possible between a human and a given object category""",2023,2023-03-09T19:08:02Z,,,
arXIv2023,Dynamic Stashing Quantization for Efficient Transformer Training,Yes.,3,"""Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus challenging to deploy in use cases such as on-device learning.""",2023,2023-03-09T14:44:31Z,,,
arXIv2023,ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction,Yes.,3,"""Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning."" and ""Applying LLMs to DIE poses two challenges",2023,2023-03-09T06:24:50Z,,,
arXIv2023,Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification,Yes.,1,"""exploiting the paraphrasing capabilities of modern large language models (LLMs) to synthesize a diverse corpus of natural language commands corresponding to the LTL formulas.""",2023,2023-03-09T00:09:58Z,,,
arXIv2023,nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models,Yes.,1,"""We present nl2spec, a framework for applying Large Language Models (LLMs) to derive formal specifications (in temporal logics) from unstructured natural language.""",2023,2023-03-08T20:08:53Z,,,
arXIv2023,Stealing the Decoding Algorithms of Language Models,Yes.,1,"""A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms.""",2023,2023-03-08T17:15:58Z,,,
arXIv2023,Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference,Yes.,1,"""Large Language Models (LLMs) have sparked significant interest in their generative capabilities, leading to the development of various commercial applications.""",2023,2023-03-08T15:52:14Z,,,
arXIv2023,MenuCraft: Interactive Menu System Design with Large Language Models,Yes.,1,"""With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems.""",2023,2023-03-08T10:39:38Z,,,
arXIv2023,Can large language models build causal graphs?,Yes.,3,"""LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs.""",2023,2023-03-07T22:05:31Z,,,
arXIv2023,Gradient-Free Structured Pruning with Unlabeled Data,Yes.,2,"""Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency.""",2023,2023-03-07T19:12:31Z,,,
arXIv2023,Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering,Yes.,3,"""These follow-up questions largely overcome known issues with LLMs providing factually inaccurate responses.""",2023,2023-03-07T17:54:53Z,,,
arXIv2023,ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification,Yes.,3,"""However, if the model is fully prompted in Slovenian, the performance drops significantly, showing the current limitations of ChatGPT usage on smaller languages.""",2023,2023-03-07T14:59:33Z,,,
arXIv2023,Larger language models do in-context learning differently,Yes.,3,"""First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold",2023,2023-03-07T12:24:17Z,,,
arXIv2023,Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles,Yes.,1,"""an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance.""",2023,2023-03-07T09:20:43Z,,,
arXIv2023,Large Language Models as Zero-Shot Human Models for Human-Robot Interaction,Yes.,3,"""That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps.""",2023,2023-03-06T23:16:24Z,,,
arXIv2023,"Can an Embodied Agent Find Your ""Cat-shaped Mug""? LLM-Guided Exploration for Zero-Shot Object Navigation",Yes.,1,"""We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW).""",2023,2023-03-06T20:19:19Z,,,
arXIv2023,Spelling convention sensitivity in neural language models,Yes.,3,"""We further experiment with correcting for biases in the training data by fine-tuning T5 on synthetic data that has been debiased, and find that finetuned T5 remains only somewhat sensitive to spelling consistency. Further experiments show GPT2 to be similarly limited.""",2023,2023-03-06T19:29:20Z,,,
arXIv2023,Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting,Yes.,1,"""We propose a conceptual perspective on prompts for Large Language Models (LLMs) that distinguishes between (1) diegetic prompts... and (2) non-diegetic prompts...""",2023,2023-03-06T14:58:42Z,,,
arXIv2023,DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training,Yes.,3,"""The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. The modality gap issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix",2023,2023-03-06T11:02:47Z,,,
arXIv2023,"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",Yes.,3,"""However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even",2023,2023-03-06T10:08:51Z,,,
arXIv2023,LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models,Yes.,1,"""we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) such as ChatGPT/GPT-4 and image generation models (IGMs) are suitable to addressing these tasks.""",2023,2023-03-06T06:47:22Z,,,
arXIv2023,OpenICL: An Open-Source Framework for In-context Learning,Yes.,1,"""In recent years, In-context Learning (ICL) has gained increasing attention and emerged as the new paradigm for large language model (LLM) evaluation.""",2023,2023-03-06T06:20:25Z,,,
arXIv2023,The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges,Yes.,2,"""Current datasets used for VL pre-training only contain a limited amount of visual and linguistic knowledge, thus significantly limiting the generalization capabilities of many VL models.""",2023,2023-03-04T13:12:18Z,,,
arXIv2023,TrojText: Test-time Invisible Textual Trojan Insertion,No.,1,The abstract does not mention LLMs or their limitations. It focuses on textual Trojan attacks in NLP models.,2023,2023-03-03T22:19:22Z,,,
arXIv2023,Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models,Yes.,1,"""Our approach integrates classic logical programming languages into large language models (LLMs), enabling the utilization of logical reasoning capabilities to tackle the KGQA task.""",2023,2023-03-03T20:35:38Z,,,
arXIv2023,Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM,Yes.,3,"""Our results show that 0-shot performance suffers from overgeneration and generating in the wrong language, but this is greatly improved in the few-shot setting, with very good results for a number of language pairs.""",2023,2023-03-03T13:23:42Z,,,
arXIv2023,Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering,Yes.,3,"""Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the blind LLM as the provided textual input is insufficient to depict the required visual information to answer the question.""",2023,2023-03-03T13:05:15Z,,,
arXIv2023,Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers,No.,1,The abstract discusses transformers and Sparse Mixture-of-Experts (SMoEs) but does not specifically mention language models or large language models.,2023,2023-03-02T22:12:51Z,,,
arXIv2023,WiCE: Real-World Entailment for Claims in Wikipedia,Yes.,3,"""we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models' performance on multiple datasets at test time."" and ""Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.""",2023,2023-03-02T17:45:32Z,,,
arXIv2023,Open-World Object Manipulation using Pre-trained Vision-Language Models,Yes.,1,"""we study whether we can interface robot policies with these pre-trained models, with the aim of allowing robots to complete instructions involving object categories that the robot has never seen first-hand.""",2023,2023-03-02T01:55:10Z,,,
arXIv2023,UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers,Yes.,1,"""To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply.""",2023,2023-03-01T20:21:23Z,,,
arXIv2023,Domain-adapted large language models for classifying nuclear medicine reports,Yes.,3,"""it is unclear how well these models generalize to nuclear medicine which has domain-specific vocabulary and unique reporting styles.""",2023,2023-03-01T09:48:39Z,,,
arXIv2023,Can ChatGPT Assess Human Personalities? A General Evaluation Framework,Yes.,3,"""Our experiments reveal ChatGPT's ability to assess human personalities, and the average results demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT.""",2023,2023-03-01T06:16:14Z,,,
arXIv2023,Almanac: Retrieval-Augmented Language Models for Clinical Medicine,Yes.,3,"""Despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements.""",2023,2023-03-01T02:30:11Z,,,
arXIv2023,Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics,Yes.,3,"""Large language models are not detailed models of human linguistic processing.""",2023,2023-02-28T20:49:38Z,,,
arXIv2023,Automatic Scoring of Dream Reports' Emotional Content with Large Language Models,Yes.,3,"""Our results show that the off-the-shelf method achieves a low performance probably in light of inherent linguistic differences between reports collected in different (groups of) individuals.""",2023,2023-02-28T18:23:17Z,,,
arXIv2023,Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following,Yes.,3,"""We hypothesize that TAPP assists language models to better estimate the output distribution by focusing more on the instruction of the target task during inference. In other words, such ability does not seem to be sufficiently activated in not only base LLMs but also many instruction-fine-tuned LLMs.""",2023,2023-02-28T16:06:35Z,,,
arXIv2023,Zero-Shot Cross-Lingual Summarization via Large Language Models,Yes.,3,"""Moreover, we also find some multi-lingual and bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited zero-shot CLS ability."" and ""Due to the composite nature of CLS, which",2023,2023-02-28T01:27:37Z,,,
arXIv2023,Reward Design with Language Models,Yes.,1,"""This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function.""",2023,2023-02-27T22:09:35Z,,,
arXIv2023,SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks,Yes.,2,"""As the size of large language models continue to scale, so does the computational resources required to run it.""",2023,2023-02-27T16:43:04Z,,,
arXIv2023,Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity,Yes.,3,"""However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration."" and ""there was an upper bound of 25% of outputs bring rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content.""",2023,2023-02-27T15:54:57Z,,,
arXIv2023,Towards Human-Bot Collaborative Software Architecting with ChatGPT,Yes.,2,"""Preliminary results indicate that ChatGPT can mimic an architect's role to support and often lead ACSE, however; it requires human oversight and decision support for collaborative architecting.""",2023,2023-02-26T16:32:16Z,,,
arXIv2023,Fast Attention Requires Bounded Entries,Yes.,1,"""In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT.""",2023,2023-02-26T02:42:39Z,,,
arXIv2023,Leveraging Large Language Model and Story-Based Gamification in Intelligent Tutoring System to Scaffold Introductory Programming Courses: A Design-Based Research Study,Yes.,1,"""This study explores how large language models and gamification can scaffold coding learning and increase Chinese students sense of belonging in introductory programming courses.""",2023,2023-02-25T04:07:03Z,,,
arXIv2023,Robot Behavior-Tree-Based Task Generation with Large Language Models,Yes.,3,"""Assessment on Phase-Step prompts and the limitation of large language models are presented and discussed.""",2023,2023-02-24T22:53:10Z,,,
arXIv2023,Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data,Yes.,3,"""However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains.""",2023,2023-02-24T18:58:06Z,,,
arXIv2023,HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained transformers applied to the detection of sexism in social media,Yes.,1,"""We explore some of the most popular transformer models such as BERT, DistilBERT, RoBERTa, and XLNet.""",2023,2023-02-24T18:17:38Z,,,
arXIv2023,Spanish Built Factual Freectianary (Spanish-BFF): the first AI-generated free dictionary,Yes.,1,"""Building them is a complex task that, to the best of our knowledge, has yet to be explored with generative Large Language Models (LLMs).""",2023,2023-02-24T16:59:54Z,,,
arXIv2023,Language Models are Few-shot Learners for Prognostic Prediction,Yes.,1,"""This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas.""",2023,2023-02-24T15:35:36Z,,,
arXIv2023,Analyzing And Editing Inner Mechanisms Of Backdoored Language Models,Yes.,3,"""Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models."" and ""We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.""",2023,2023-02-24T05:26:08Z,,,
arXIv2023,Extracting Victim Counts from Text,Yes.,2,"""Beyond model accuracy, we analyze extraction reliability and robustness which are key for this sensitive task. In particular, we discuss model calibration and investigate few-shot and out-of-distribution performance.""",2023,2023-02-23T23:50:24Z,,,
arXIv2023,CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models,Yes.,1,"""CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records.""",2023,2023-02-23T21:23:06Z,,,
arXIv2023,Active Prompting with Chain-of-Thought for Large Language Models,Yes.,2,"""However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks.""",2023,2023-02-23T18:58:59Z,,,
arXIv2023,What Makes a Language Easy to Deep-Learn?,Yes.,1,"""We evaluate the memorization and generalization capabilities of a large language model and recurrent neural networks.""",2023,2023-02-23T18:57:34Z,,,
arXIv2023,Sentence Simplification via Large Language Models,Yes.,2,"""However, it is not yet known whether LLMs can be served as a high-quality sentence simplification system.""",2023,2023-02-23T12:11:58Z,,,
arXIv2023,Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments,Yes.,1,"""We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data.""",2023,2023-02-22T20:56:40Z,,,
arXIv2023,How Does In-Context Learning Help Prompt Tuning?,Yes.,3,"""Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale."" and ""PT is unstable and exhibits high variance.""",2023,2023-02-22T17:45:12Z,,,
arXIv2023,Guiding Large Language Models via Directional Stimulus Prompting,Yes.,1,"""We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs.""",2023,2023-02-22T17:44:15Z,,,
arXIv2023,In-context Example Selection with Influences,Yes.,3,"""ICL performance is known to be highly sensitive to input examples.""",2023,2023-02-21T22:47:45Z,,,
arXIv2023,$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models,Yes.,3,"""it can be infeasible when it comes to modern large-scale language models such as GPT-3, which can only be accessed through APIs, making it difficult to access the internal parameters of the model.""",2023,2023-02-21T18:54:21Z,,,
arXIv2023,A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT,Yes.,2,"""This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs.""",2023,2023-02-21T12:42:44Z,,,
arXIv2023,Mask-guided BERT for Few Shot Text Classification,Yes.,3,"""the data-intensive nature of the transformer architecture requires much labeled data, which is challenging in low-resource scenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the difficulty of training robust models on small amounts of samples, which frequently leads to overfitting.""",2023,2023-02-21T05:24:00Z,,,
arXIv2023,Zero-Shot Information Extraction via Chatting with ChatGPT,Yes.,1,"""Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods.""",2023,2023-02-20T12:57:12Z,,,
arXIv2023,90% F1 Score in Relational Triple Extraction: Is it Real ?,No.,1,The abstract discusses joint entity and relation extraction models and does not mention language models (LLMs or LMs).,2023,2023-02-20T10:30:16Z,,,
arXIv2023,Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,Yes.,3,"""We show that measuring uncertainty in natural language is challenging because of 'semantic equivalence' -- different sentences can mean the same thing.""",2023,2023-02-19T20:10:07Z,,,
arXIv2023,Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference,Yes.,1,"""This study used a form of artificial intelligence (AI) known as large language models (LLMs) to assess whether language-based representations of emotion causally contribute to the AI's ability to generate inferences about the emotional meaning of novel situations.""",2023,2023-02-19T14:21:33Z,,,
arXIv2023,How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation,Yes.,3,"""Our results show that GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages.""",2023,2023-02-18T02:11:36Z,,,
arXIv2023,Privately Customizing Prefinetuning to Better Match User Data in Federated Learning,Yes.,1,"""it privately computes and compares a Frchet distance between embeddings generated by a large language model on both the central (public) dataset and the federated private client data.""",2023,2023-02-17T18:18:22Z,,,
arXIv2023,Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?,No.,1,"The abstract discusses generative AI tools in general, such as DALL-E, MidJourney, or ChatGPT, but does not specifically focus on language models or their limitations.",2023,2023-02-17T17:39:41Z,,,
arXIv2023,Massively Multilingual Shallow Fusion with Large Language Models,Yes.,1,"""While large language models (LLM) have made impressive progress in natural language processing, it remains unclear how to utilize them in improving automatic speech recognition (ASR).""",2023,2023-02-17T14:46:38Z,,,
arXIv2023,LEVER: Learning to Verify Language-to-Code Generation with Execution,Yes.,3,"""it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program.""",2023,2023-02-16T18:23:22Z,,,
arXIv2023,LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation,Yes.,3,"""Large-scale language-agnostic sentence embedding models such as LaBSE (Feng et al., 2022) obtain state-of-the-art performance for parallel sentence alignment. However, these large-scale models can suffer from inference speed and computation overhead.""",2023,2023-02-16T16:05:34Z,,,
arXIv2023,Tree-Based Representation and Generation of Natural and Mathematical Language,Yes.,1,"""In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math.""",2023,2023-02-15T22:38:34Z,,,
arXIv2023,Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation,Yes.,3,"""However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios.""",2023,2023-02-15T18:46:42Z,,,
arXIv2023,"Conversational AI-Powered Design: ChatGPT as Designer, User, and Product",Yes.,3,"""The study does, however, highlight some drawbacks such as forgotten information, partial responses, and a lack of output diversity.""",2023,2023-02-15T00:14:17Z,,,
arXIv2023,ScatterShot: Interactive In-context Example Curation for Text Transformation,Yes.,3,"""users tend to include only the most obvious patterns when crafting examples, resulting in underspecified in-context functions that fall short on unseen cases.""",2023,2023-02-14T21:13:31Z,,,
arXIv2023,ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models,Yes.,3,"""However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions.""",2023,2023-02-14T18:54:06Z,,,
arXIv2023,Few-shot learning approaches for classifying low resource domain specific software requirements,Yes.,3,"""the availability of even a few hundred annotated samples may not always be guaranteed in low resource domains like automotive, which often limits the usage of such deep learning models in an industrial setting.""",2023,2023-02-14T10:19:23Z,,,
arXIv2023,Learning gain differences between ChatGPT and human tutor generated algebra hints,Yes.,3,"""Learning gains from human-created hints were substantially and statistically significantly higher than ChatGPT hints in both topic areas.""",2023,2023-02-14T07:20:48Z,,,
arXIv2023,Guiding Pretraining in Reinforcement Learning with Large Language Models,Yes.,1,"""By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop.""",2023,2023-02-13T21:16:03Z,,,
arXIv2023,Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning,Yes.,1,"""We develop conversational safety classifiers via the prompt-tuning PET method and show how the unique characteristics of the PET regime enable TracIn to identify the cause for certain misclassifications by LLMs.""",2023,2023-02-13T18:54:58Z,,,
arXIv2023,Implications of the Convergence of Language and Vision Model Geometries,Yes.,3,"""Large-scale pretrained language models (LMs) are said to 'lack the ability to connect [their] utterances to the world' (Bender and Koller, 2020).""",2023,2023-02-13T17:55:54Z,,,
arXIv2023,An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation,Yes.,2,"""suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.""",2023,2023-02-13T17:13:41Z,,,
arXIv2023,Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection,Yes.,1,"""Our approach uses the domain invariance and contextual understanding of a pre-trained joint vision and language model to predict the class distribution of unlabelled data.""",2023,2023-02-13T00:46:34Z,,,
arXIv2023,MarioGPT: Open-Ended Text2Level Generation through Large Language Models,Yes.,1,"""Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains.""",2023,2023-02-12T19:12:24Z,,,
arXIv2023,Level Generation Through Large Language Models,Yes.,3,"""Game levels, with their complex functional constraints and spatial relationships in more than one dimension, are very different from the kinds of data an LLM typically sees during training. Datasets of game levels are also hard to come by, potentially taxing the abilities of these data-hungry models.""",2023,2023-02-11T23:34:42Z,,,
arXIv2023,Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes,Yes.,1,"""We identify this as a question answering (QA) task and employ several state-of-the-art LLMs to present contexts around risk prediction model inferences and evaluate their acceptability.""",2023,2023-02-11T18:07:11Z,,,
arXIv2023,"Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models",Yes.,1,"""The method builds on top of natural language processing and large general language models but can work with almost any such model.""",2023,2023-02-09T19:56:37Z,,,
arXIv2023,Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow,Yes.,3,"""Recent research has shown that language models exploit `artifacts' in benchmarks to solve tasks, rather than truly learning them, leading to inflated model performance.""",2023,2023-02-09T04:43:10Z,,,
arXIv2023,Prompting for Multimodal Hateful Meme Classification,Yes.,1,"""we propose PromptHate, a simple yet effective prompt-based model that prompts pre-trained language models (PLMs) for hateful meme classification.""",2023,2023-02-08T16:04:08Z,,,
arXIv2023,ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots,Yes.,3,"""Conversational AI simulates conversations with humans; however, it is limited by the data captured in the training datasets.""",2023,2023-02-08T13:03:27Z,,,
arXIv2023,Is ChatGPT a General-Purpose Natural Language Processing Task Solver?,Yes.,3,"""We demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging.""",2023,2023-02-08T09:44:51Z,,,
arXIv2023,What Matters In The Structured Pruning of Generative Language Models?,Yes.,3,"""Auto-regressive large language models such as GPT-3 require enormous computational resources to use."" and ""established structured pruning methods do not take into account the distinctiveness of neurons, leaving behind excess redundancies.""",2023,2023-02-07T22:05:55Z,,,
arXIv2023,Long Horizon Temperature Scaling,Yes.,3,"""However, autoregressive models rely on myopic temperature scaling that greedily optimizes the next token.""",2023,2023-02-07T18:59:32Z,,,
arXIv2023,Learning Translation Quality Evaluation on Low Resource Languages from Large Language Models,Yes.,1,"""We show how knowledge can be distilled from Large Language Models (LLMs) to improve upon such learned metrics without requiring human annotators, by creating synthetic datasets which can be mixed into existing datasets, requiring only a corpus of text in the target language.""",2023,2023-02-07T14:35:35Z,,,
arXIv2023,PLACES: Prompting Language Models for Social Conversation Synthesis,Yes.,1,"""A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models.""",2023,2023-02-07T05:48:16Z,,,
arXIv2023,APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning,No.,1,"The abstract discusses practical NLP tasks, noisy labels, and long-tailed learning but does not specifically mention language models (LLMs or LMs).",2023,2023-02-06T18:40:04Z,,,
arXIv2023,Quantized Distributed Training of Large Models with Convergence Guarantees,Yes.,2,"""The recent emergence of large language models such as GPT has created the need for new approaches to exploit data-parallelism."" and ""FSDP training is highly popular, yet it still encounters scalability bottlenecks.""",2023,2023-02-05T14:20:55Z,,,
arXIv2023,The Science of Detecting LLM-Generated Texts,Yes.,3,"""However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system.""",2023,2023-02-04T04:49:17Z,,,
arXIv2023,Evaluating Large Language Models in Theory of Mind Tasks,Yes.,1,"""Eleven Large Language Models (LLMs) were assessed using a custom-made battery of false-belief tasks, considered a gold standard in testing Theory of Mind (ToM) in humans.""",2023,2023-02-04T03:50:01Z,,,
arXIv2023,Towards Few-Shot Identification of Morality Frames using In-Context Learning,Yes.,1,"""Few-shot in-context learning using pre-trained Large Language Models (LLMs) has been recently applied successfully in many NLP tasks.""",2023,2023-02-03T23:26:59Z,,,
arXIv2023,Witscript 2: A System for Generating Improvised Jokes Without Wordplay,Yes.,1,"""This paper extends that work by presenting Witscript 2, which uses a large language model to generate conversational jokes that rely on common sense instead of wordplay.""",2023,2023-02-03T21:51:55Z,,,
arXIv2023,Bioformer: an efficient transformer language model for biomedical text mining,Yes.,3,"""Despite the effectiveness, these models have hundreds of millions of parameters and are computationally expensive when applied to large-scale NLP applications.""",2023,2023-02-03T08:04:59Z,,,
arXIv2023,"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",Yes.,1,"""DEPS facilitates better error correction on initial LLM-generated plan by integrating description of the plan execution process and providing self-explanation of feedback when encountering failures during the extended planning phases.""",2023,2023-02-03T06:06:27Z,,,
arXIv2023,Multimodal Chain-of-Thought Reasoning in Language Models,Yes.,1,"""Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality.""",2023,2023-02-02T07:51:19Z,,,
arXIv2023,Collaborating with language models for embodied reasoning,Yes.,3,"""However, LSLMs do not inherently have the ability to interrogate or intervene on the environment.""",2023,2023-02-01T21:26:32Z,,,
arXIv2023,Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models,Yes.,3,"""However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly.""",2023,2023-02-01T17:33:12Z,,,
arXIv2023,Benchmarking Large Language Models for News Summarization,Yes.,3,"""existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance.""",2023,2023-01-31T18:46:19Z,,,
arXIv2023,Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning,Yes.,3,"""previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places.""",2023,2023-01-31T17:51:45Z,,,
arXIv2023,FLAME: A small language model for spreadsheet formulas,Yes.,3,"""Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters).""",2023,2023-01-31T17:29:43Z,,,
arXIv2023,Skill Decision Transformer,Yes.,3,"""However many of these methods only optimize for high returns, and may not extract much information from a diverse dataset of trajectories.""",2023,2023-01-31T11:52:46Z,,,
arXIv2023,Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models,Yes.,2,"""Previous publicly-available transformer models from eighteen months prior and 1000 times smaller failed to provide basic arithmetic.""",2023,2023-01-31T03:14:57Z,,,
arXIv2023,Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning,Yes.,1,"""Recently, multi-modal large language models (MLLMs) have been used as powerful tools for several multi-modal tasks but not for VCR yet, which requires elaborate reasoning on specific visual objects referred by texts.""",2023,2023-01-30T23:43:28Z,,,
arXIv2023,Adaptive Machine Translation with Large Language Models,Yes.,1,"""Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning.""",2023,2023-01-30T21:17:15Z,,,
arXIv2023,Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation,Yes.,3,"""However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs.""",2023,2023-01-30T15:44:55Z,,,
arXIv2023,Specializing Smaller Language Models towards Multi-Step Reasoning,Yes.,3,"""there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities"" and ""by paying the price of decreased generic ability, we can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability.""",2023,2023-01-30T08:51:19Z,,,
arXIv2023,Emerging Synergies in Causality and Deep Generative Models: A Survey,Yes.,1,"""navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs).""",2023,2023-01-29T04:10:12Z,,,
arXIv2023,Truth Machines: Synthesizing Veracity in AI Language Models,Yes.,3,"""highlighting how data harvesting, model architectures, and social feedback mechanisms weave together disparate understandings of veracity"" and ""truth as a non-trivial problem.""",2023,2023-01-28T02:47:50Z,,,
arXIv2023,Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling,Yes.,3,"""Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience... is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.""",2023,2023-01-28T02:04:07Z,,,
arXIv2023,Context Matters: A Strategy to Pre-train Language Model for Science Education,Yes.,3,"""However, science writing of students, including argumentation and explanation, is domain-specific. In addition, the language used by students is different from the language in journals and Wikipedia, which are training sources of BERT and its existing variants.""",2023,2023-01-27T23:50:16Z,,,
arXIv2023,"Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases",Yes.,3,"""W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models.""",2023,2023-01-27T22:44:18Z,,,
arXIv2023,Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning,Yes.,3,"""existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations.""",2023,2023-01-27T18:59:01Z,,,
arXIv2023,Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning,Yes.,3,"""it remains unclear if they can handle inputs that have been distributionally shifted effectively.""",2023,2023-01-27T11:27:40Z,,,
arXIv2023,A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks,Yes.,3,"""State-of-the-art methods use large language models developed with immense computational resources and training data; however, applying these models is challenging because of the highly varying syntax and vocabulary in clinical free text.""",2023,2023-01-27T09:19:03Z,,,
arXIv2023,Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,No.,1,The abstract does not mention language models or their limitations.,2023,2023-01-27T07:00:54Z,,,
arXIv2023,Theme-driven Keyphrase Extraction to Analyze Social Media Discourse,Yes.,1,"""Lastly, we found that a large language model (ChatGPT) outperforms unsupervised keyphrase extraction models, and we evaluate its efficacy in this task.""",2023,2023-01-27T03:00:46Z,,,
arXIv2023,Task formulation for Extracting Social Determinants of Health from Clinical Narratives,Yes.,2,"""These annotations are difficult to model with limited training data."" and ""Meanwhile, LLM with its versatile capability achieves the high F1 score, while respecting the annotated relations.""",2023,2023-01-26T20:00:54Z,,,
arXIv2023,DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature,Yes.,1,"""The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text.""",2023,2023-01-26T18:44:06Z,,,
arXIv2023,Domain-Agnostic Molecular Generation with Chemical Feedback,Yes.,3,"""However, despite the potential of language models in molecule generation, they face challenges such as generating syntactically or chemically flawed molecules, having narrow domain focus, and struggling to create diverse and feasible molecules due to limited annotated data or external molecular databases.""",2023,2023-01-26T17:52:56Z,,,
arXIv2023,Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract),Yes.,1,"""While large language models (LLMs) have demonstrated strong capability in structured prediction tasks such as semantic parsing, few amounts of research have explored the underlying mechanisms of their success.""",2023,2023-01-25T16:12:43Z,,,
arXIv2023,ExaRanker: Explanation-Augmented Neural Ranker,Yes.,1,"""Recent work has shown that inducing a large language model (LLM) to generate explanations prior to outputting an answer is an effective strategy to improve performance on a wide range of reasoning tasks.""",2023,2023-01-25T11:03:04Z,,,
arXIv2023,Language Model Detoxification in Dialogue with Contextualized Stance Control,Yes.,3,"""To reduce the toxic degeneration in a pretrained Language Model (LM), previous work on Language Model detoxification has focused on reducing the toxicity of the generation itself (self-toxicity) without consideration of the context.""",2023,2023-01-25T00:47:28Z,,,
arXIv2023,Audience-Centric Natural Language Generation via Style Infusion,Yes.,2,"""we argue that grounding style on audience-independent external factors is innately limiting for two reasons. First, it is difficult to collect large volumes of audience-specific stylistic data. Second, some stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to define without audience feedback.""",2023,2023-01-24T19:57:50Z,,,
arXIv2023,A Watermark for Large Language Models,Yes.,2,"""Potential harms of large language models can be mitigated by watermarking model output.""",2023,2023-01-24T18:52:59Z,,,
arXIv2023,Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models,Yes.,2,"""While using LLMs is promising, the critical challenge is to ensure high precision in the generated feedback, which is imperative before deploying such technology in classrooms.""",2023,2023-01-24T13:00:25Z,,,
arXIv2023,SMART: Self-supervised Multi-task pretrAining with contRol Transformers,No.,1,The abstract discusses self-supervised pretraining in the context of sequential decision-making tasks and does not mention language models or their limitations.,2023,2023-01-24T05:01:23Z,,,
arXIv2023,Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning,Yes.,3,"""Most Transformer language models are primarily pretrained on English text, limiting their use for other languages."" and ""As the model sizes grow, the performance gap between English and other languages with fewer compute and data resources increases even further.""",2023,2023-01-23T18:56:12Z,,,
arXIv2023,Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions,Yes.,3,"""Large-scale pre-trained language models have been shown to be helpful in improving the naturalness of text-to-speech (TTS) models... However, these models are usually word-level or sup-phoneme-level and jointly trained with phonemes, making them inefficient for the downstream TTS task where only phonemes are needed.""",2023,2023-01-20T21:36:16Z,,,
arXIv2023,Batch Prompting: Efficient Inference with Large Language Model APIs,Yes.,2,"""Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use.""",2023,2023-01-19T02:29:23Z,,,
arXIv2023,"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",Yes.,3,"""people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues.""",2023,2023-01-18T15:23:25Z,,,
arXIv2023,BERT-ERC: Fine-tuning BERT is Enough for Emotion Recognition in Conversation,Yes.,2,"""Previous works on emotion recognition in conversation (ERC) follow a two-step paradigm, which can be summarized as first producing context-independent features via fine-tuning pretrained language models (PLMs) and then analyzing contextual information and dialogue structure information among the extracted features. However, we discover that this paradigm has several limitations.""",2023,2023-01-17T08:03:32Z,,,
arXIv2023,Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data,Yes.,3,"""Yet, it is unclear how to design prompts to power chatbots to carry on naturalistic conversations while pursuing a given goal,"" and ""We discuss the opportunities and challenges of building chatbots with LLMs.""",2023,2023-01-14T07:29:36Z,,,
arXIv2023,In BLOOM: Creativity and Affinity in Artificial Lyrics and Art,Yes.,2,"""We find that current computational metrics for evaluating large language model outputs (MAUVE) have limitations in evaluation of creative writing.""",2023,2023-01-13T06:22:22Z,,,
arXIv2023,Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism,Yes.,3,"""Current LLMs are generally inaccessible to resource-constrained researchers, due to a variety of factors including closed source code.""",2023,2023-01-12T19:41:47Z,,,
arXIv2023,"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Yes.,2,"""Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the",2023,2023-01-12T18:59:50Z,,,
arXIv2023,MGeo: Multi-Modal Geographic Pre-Training Method,No.,1,"""Recently, pre-trained models (PTMs) have made advancements in many natural language processing (NLP) tasks.""",2023,2023-01-11T03:05:12Z,,,
arXIv2023,Language Models sounds the Death Knell of Knowledge Graphs,Yes.,1,"""Deep Learning based NLP especially Large Language Models (LLMs) such as BERT have found broad acceptance and are used extensively for many applications.""",2023,2023-01-10T14:20:15Z,,,
arXIv2023,Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models,Yes.,1,"""Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization.""",2023,2023-01-10T05:41:40Z,,,
arXIv2023,SAIDS: A Novel Approach for Sentiment Analysis Informed of Dialect and Sarcasm,Yes.,1,"""SAIDS uses its prediction of sarcasm and dialect as known information to predict the sentiment. It uses MARBERT as a language model to generate sentence embedding, then passes it to the sarcasm and dialect models, and then the outputs of the three models are",2023,2023-01-06T14:19:46Z,,,
arXIv2023,You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona,Yes.,3,"""the model that considers knowledge and persona at the same time is still limited, leading to hallucination and a passive way of using personas.""",2023,2023-01-06T06:47:21Z,,,
arXIv2023,TrojanPuzzle: Covertly Poisoning Code-Suggestion Models,Yes.,3,"""These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data.""",2023,2023-01-06T00:37:25Z,,,
arXIv2023,Evidence of behavior consistent with self-interest and altruism in an artificially intelligent agent,Yes.,1,"""Here we present an incentivized experiment to test for altruistic behavior among AI agents consisting of large language models developed by the private company OpenAI.""",2023,2023-01-05T23:30:29Z,,,
arXIv2023,Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach,Yes.,3,"""However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored.""",2023,2023-01-05T14:03:26Z,,,
arXIv2023,Large Language Models as Corporate Lobbyists,Yes.,3,"""Longer-term, if AI begins to influence law in a manner that is not a direct extension of human intentions, this threatens the critical role that law as information could play in aligning AI with humans.""",2023,2023-01-03T16:25:52Z,,,
arXIv2023,Language Models are Drummers: Drum Composition with Natural Language Pre-Training,Yes.,3,"""analyze drum grooves produced by GPT3 compared to those played by human professionals, exposing the strengths and weaknesses of such generation by language-to-music transfer.""",2023,2023-01-03T15:47:53Z,,,
arXIv2023,Invalidator: Automated Patch Correctness Assessment via Semantic and Syntactic Reasoning,Yes.,1,"""INVALIDATOR leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model.""",2023,2023-01-03T14:16:32Z,,,
arXIv2023,Muse: Text-To-Image Generation via Masked Generative Transformers,Yes.,1,"""given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens.""",2023,2023-01-02T14:43:38Z,,,
arXIv2023,CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation,Yes.,3,"""large-scale language models suffer from data inadequacy and biased corpus, especially for languages with insufficient resources such as Chinese.""",2023,2023-01-01T12:48:12Z,,,
arXIv2023,Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education,No.,1,"""No evidence""",2023,2023-06-30T19:53:23Z,,,
arXIv2023,SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,No.,1,"""No evidence""",2023,2023-06-30T17:59:07Z,,,
arXIv2023,Statler: State-Maintaining Language Models for Embodied Reasoning,No.,1,"""No evidence""",2023,2023-06-30T17:58:02Z,,,
arXIv2023,Stay on topic with Classifier-Free Guidance,No.,1,"""No evidence""",2023,2023-06-30T17:07:02Z,,,
arXIv2023,Thompson sampling for improved exploration in GFlowNets,No.,1,"""No evidence""",2023,2023-06-30T14:19:44Z,,,
arXIv2023,"MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems",No.,1,"""No evidence""",2023,2023-06-30T10:28:20Z,,,
arXIv2023,GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models,No.,1,"""No evidence""",2023,2023-06-30T10:12:30Z,,,
arXIv2023,Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring of Learning Objectives,No.,1,"""No evidence""",2023,2023-06-30T08:15:18Z,,,
arXIv2023,Japanese Lexical Complexity for Non-Native Readers: A New Dataset,No.,1,"""No evidence""",2023,2023-06-30T04:37:43Z,,,
arXIv2023,Towards Open-Domain Topic Classification,No.,1,"""No evidence""",2023,2023-06-29T20:25:28Z,,,
arXIv2023,A Hybrid System for Systematic Generalization in Simple Arithmetic Problems,No.,1,"""No evidence""",2023,2023-06-29T18:35:41Z,,,
arXIv2023,LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding,No.,1,"""No evidence""",2023,2023-06-29T17:08:16Z,,,
arXIv2023,LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT,No.,1,"""No evidence""",2023,2023-06-29T17:01:51Z,,,
arXIv2023,RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot,No.,1,"""No evidence""",2023,2023-06-29T16:28:34Z,,,
arXIv2023,Classifying Crime Types using Judgment Documents from Social Media,No.,1,"""No evidence""",2023,2023-06-29T15:12:24Z,,,
arXIv2023,Learning Environment Models with Continuous Stochastic Dynamics,No.,1,"""No evidence""",2023,2023-06-29T12:47:28Z,,,
arXIv2023,Harnessing the Power of Hugging Face Transformers for Predicting Mental Health Disorders in Social Networks,No.,1,"""No evidence""",2023,2023-06-29T12:25:19Z,,,
arXIv2023,SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores,No.,1,"""No evidence""",2023,2023-06-29T05:16:25Z,,,
arXIv2023,An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs,No.,1,"""No evidence""",2023,2023-06-28T23:55:51Z,,,
arXIv2023,Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare,No.,1,"""No evidence""",2023,2023-06-28T17:00:32Z,,,
arXIv2023,Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,No.,1,"""No evidence""",2023,2023-06-28T15:54:29Z,,,
arXIv2023,An Adversarial Multi-Task Learning Method for Chinese Text Correction with Semantic Detection,No.,1,"""No evidence""",2023,2023-06-28T15:46:00Z,,,
arXIv2023,Inferring the Goals of Communicating Agents from Actions and Instructions,No.,1,"""No evidence""",2023,2023-06-28T13:43:46Z,,,
arXIv2023,Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation,No.,1,"""No evidence""",2023,2023-06-28T13:21:00Z,,,
arXIv2023,A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023,No.,1,"""No evidence""",2023,2023-06-28T11:53:07Z,,,
arXIv2023,Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks,No.,1,"""No evidence""",2023,2023-06-28T11:24:48Z,,,
arXIv2023,ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases,No.,1,"""No evidence""",2023,2023-06-28T10:48:34Z,,,
arXIv2023,Mastering Nordschleife -- A comprehensive race simulation for AI strategy decision-making in motorsports,No.,1,"""No evidence""",2023,2023-06-28T10:39:31Z,,,
arXIv2023,Curious Replay for Model-based Adaptation,No.,1,"""No evidence""",2023,2023-06-28T05:34:53Z,,,
arXIv2023,Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio,No.,1,"""No evidence""",2023,2023-06-28T05:10:51Z,,,
arXIv2023,Let Segment Anything Help Image Dehaze,No.,1,"""No evidence""",2023,2023-06-28T02:02:19Z,,,
arXIv2023,MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning,No.,1,"""No evidence""",2023,2023-06-27T23:19:53Z,,,
arXIv2023,DMNER: Biomedical Entity Recognition by Detection and Matching,No.,1,"""No evidence""",2023,2023-06-27T18:32:07Z,,,
arXIv2023,SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design,No.,1,"""No evidence""",2023,2023-06-27T17:50:26Z,,,
arXIv2023,Extending Context Window of Large Language Models via Positional Interpolation,No.,1,"""No evidence""",2023,2023-06-27T16:26:26Z,,,
arXIv2023,MyCrunchGPT: A chatGPT assisted framework for scientific machine learning,No.,1,"""No evidence""",2023,2023-06-27T15:23:42Z,,,
arXIv2023,CamemBERT-bio: Leveraging Continual Pre-training for Cost-Effective Models on French Biomedical Data,No.,1,"""No evidence""",2023,2023-06-27T15:23:14Z,,,
arXIv2023,"Unleashing the Power of User Reviews: Exploring Airline Choices at Catania Airport, Italy",No.,1,"""No evidence""",2023,2023-06-27T15:10:57Z,,,
arXIv2023,Using Large Language Models to Provide Explanatory Feedback to Human Tutors,No.,1,"""No evidence""",2023,2023-06-27T14:19:12Z,,,
arXIv2023,IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning,No.,1,"""No evidence""",2023,2023-06-27T07:57:42Z,,,
arXIv2023,Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic,No.,1,"""No evidence""",2023,2023-06-27T04:31:52Z,,,
arXIv2023,DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization,No.,1,"""No evidence""",2023,2023-06-27T02:46:08Z,,,
arXIv2023,Investigating Cross-Domain Behaviors of BERT in Review Understanding,No.,1,"""No evidence""",2023,2023-06-27T00:23:35Z,,,
arXIv2023,Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation,No.,1,"""No evidence""",2023,2023-06-27T00:11:31Z,,,
arXIv2023,LM4HPC: Towards Effective Language Model Application in High-Performance Computing,No.,1,"""No evidence""",2023,2023-06-26T18:05:03Z,,,
arXIv2023,Large Multimodal Models: Notes on CVPR 2023 Tutorial,No.,1,"""No evidence""",2023,2023-06-26T17:59:31Z,,,
arXIv2023,LongCoder: A Long-Range Pre-trained Language Model for Code Completion,No.,1,"""No evidence""",2023,2023-06-26T17:59:24Z,,,
arXIv2023,Composing Parameter-Efficient Modules with Arithmetic Operations,No.,1,"""No evidence""",2023,2023-06-26T17:33:21Z,,,
arXIv2023,Kosmos-2: Grounding Multimodal Large Language Models to the World,No.,1,"""No evidence""",2023,2023-06-26T16:32:47Z,,,
arXIv2023,MotionGPT: Human Motion as a Foreign Language,No.,1,"""No evidence""",2023,2023-06-26T15:53:02Z,,,
arXIv2023,Automatic Assessment of Divergent Thinking in Chinese Language with TransDis: A Transformer-Based Language Model Approach,No.,1,"""No evidence""",2023,2023-06-26T15:48:05Z,,,
arXIv2023,Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement,No.,1,"""No evidence""",2023,2023-06-26T13:54:47Z,,,
arXIv2023,SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,No.,1,"""No evidence""",2023,2023-06-26T11:35:22Z,,,
arXIv2023,Transfer Learning across Several Centuries: Machine and Historian Integrated Method to Decipher Royal Secretary's Diary,No.,1,"""No evidence""",2023,2023-06-26T11:00:35Z,,,
arXIv2023,ParameterNet: Parameters Are All You Need,No.,1,"""No evidence""",2023,2023-06-26T09:01:35Z,,,
arXIv2023,Data-Driven Approach for Formality-Sensitive Machine Translation: Language-Specific Handling and Synthetic Data Generation,No.,1,"""No evidence""",2023,2023-06-26T08:45:47Z,,,
arXIv2023,Fauno: The Italian Large Language Model that will leave you senza parole!,No.,1,"""No evidence""",2023,2023-06-26T07:00:38Z,,,
arXIv2023,The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs,No.,1,"""No evidence""",2023,2023-06-25T19:38:01Z,,,
arXIv2023,Addressing Cold Start Problem for End-to-end Automatic Speech Scoring,No.,1,"""No evidence""",2023,2023-06-25T18:48:21Z,,,
arXIv2023,Revolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices,No.,1,"""No evidence""",2023,2023-06-25T15:04:21Z,,,
arXIv2023,Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?,No.,1,"""No evidence""",2023,2023-06-25T12:08:44Z,,,
arXIv2023,Switch-BERT: Learning to Model Multimodal Interactions by Switching Attention and Input,No.,1,"""No evidence""",2023,2023-06-25T09:28:40Z,,,
arXIv2023,Interactive Design by Integrating a Large Pre-Trained Language Model and Building Information Modeling,No.,1,"""No evidence""",2023,2023-06-25T08:18:03Z,,,
arXIv2023,Low-Rank Prune-And-Factorize for Language Model Compression,No.,1,"""No evidence""",2023,2023-06-25T07:38:43Z,,,
arXIv2023,UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation,No.,1,"""No evidence""",2023,2023-06-24T22:00:06Z,,,
arXIv2023,DesCo: Learning Object Recognition with Rich Language Descriptions,No.,1,"""No evidence""",2023,2023-06-24T21:05:02Z,,,
arXIv2023,Thinking Like an Annotator: Generation of Dataset Labeling Instructions,No.,1,"""No evidence""",2023,2023-06-24T18:32:48Z,,,
arXIv2023,Large Language Models as Sous Chefs: Revising Recipes with GPT-3,No.,1,"""No evidence""",2023,2023-06-24T14:42:43Z,,,
arXIv2023,Comparison of Pre-trained Language Models for Turkish Address Parsing,No.,1,"""No evidence""",2023,2023-06-24T12:09:43Z,,,
arXIv2023,Large Sequence Models for Sequential Decision-Making: A Survey,No.,1,"""No evidence""",2023,2023-06-24T12:06:26Z,,,
arXIv2023,Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis,No.,1,"""No evidence""",2023,2023-06-24T08:45:47Z,,,
arXIv2023,Math Word Problem Solving by Generating Linguistic Variants of Problem Statements,No.,1,"""No evidence""",2023,2023-06-24T08:27:39Z,,,
arXIv2023,L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models,No.,1,"""No evidence""",2023,2023-06-24T07:27:53Z,,,
arXIv2023,Is Pre-training Truly Better Than Meta-Learning?,No.,1,"""No evidence""",2023,2023-06-24T02:26:45Z,,,
arXIv2023,Potential Benefits of Employing Large Language Models in Research in Moral Education and Development,No.,1,"""No evidence""",2023,2023-06-23T22:39:05Z,,,
arXIv2023,Resume Information Extraction via Post-OCR Text Processing,No.,1,"""No evidence""",2023,2023-06-23T20:14:07Z,,,
arXIv2023,Max-Margin Token Selection in Attention Mechanism,No.,1,"""No evidence""",2023,2023-06-23T16:35:46Z,,,
arXIv2023,Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale,No.,1,"""No evidence""",2023,2023-06-23T16:23:24Z,,,
arXIv2023,System-Level Natural Language Feedback,No.,1,"""No evidence""",2023,2023-06-23T16:21:40Z,,,
arXIv2023,A Survey on Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-06-23T15:21:52Z,,,
arXIv2023,ChatGPT may excel in States Medical Licensing Examination but falters in basic Linear Algebra,No.,1,"""No evidence""",2023,2023-06-23T15:19:29Z,,,
arXIv2023,Exploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT,No.,1,"""No evidence""",2023,2023-06-23T15:15:13Z,,,
arXIv2023,Multimodal Search on Iconclass using Vision-Language Pre-Trained Models,No.,1,"""No evidence""",2023,2023-06-23T11:12:48Z,,,
arXIv2023,MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-06-23T09:22:36Z,,,
arXIv2023,Stress Testing BERT Anaphora Resolution Models for Reaction Extraction in Chemical Patents,No.,1,"""No evidence""",2023,2023-06-23T09:01:56Z,,,
arXIv2023,Implementing contextual biasing in GPU decoder for online ASR,No.,1,"""No evidence""",2023,2023-06-23T08:59:50Z,,,
arXIv2023,Exploring Qualitative Research Using LLMs,No.,1,"""No evidence""",2023,2023-06-23T05:21:36Z,,,
arXIv2023,DiversiGATE: A Comprehensive Framework for Reliable Large Language Models,No.,1,"""No evidence""",2023,2023-06-22T22:29:40Z,,,
arXIv2023,TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning,No.,1,"""No evidence""",2023,2023-06-22T22:21:53Z,,,
arXIv2023,"""Filling the Blanks'': Identifying Micro-activities that Compose Complex Human Activities of Daily Living",No.,1,"""No evidence""",2023,2023-06-22T18:14:54Z,,,
arXIv2023,"Public Attitudes Toward ChatGPT on Twitter: Sentiments, Topics, and Occupations",No.,1,"""No evidence""",2023,2023-06-22T15:10:18Z,,,
arXIv2023,AudioPaLM: A Large Language Model That Can Speak and Listen,No.,1,"""No evidence""",2023,2023-06-22T14:37:54Z,,,
arXIv2023,"Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation",No.,1,"""No evidence""",2023,2023-06-22T14:31:18Z,,,
arXIv2023,Beyond Chemical Language: A Multimodal Approach to Enhance Molecular Property Prediction,No.,1,"""No evidence""",2023,2023-06-22T13:28:59Z,,,
arXIv2023,Mapping and Cleaning Open Commonsense Knowledge Bases with Generative Translation,No.,1,"""No evidence""",2023,2023-06-22T09:42:54Z,,,
arXIv2023,Vec2Vec: A Compact Neural Network Approach for Transforming Text Embeddings with High Fidelity,No.,1,"""No evidence""",2023,2023-06-22T06:23:31Z,,,
arXIv2023,SoftGPT: Learn Goal-oriented Soft Object Manipulation Skills by Generative Pre-trained Heterogeneous Graph Transformer,No.,1,"""No evidence""",2023,2023-06-22T05:48:22Z,,,
arXIv2023,From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought,No.,1,"""No evidence""",2023,2023-06-22T05:14:00Z,,,
arXIv2023,A Reference-less Quality Metric for Automatic Speech Recognition via Contrastive-Learning of a Multi-Language Model with Self-Supervision,No.,1,"""No evidence""",2023,2023-06-21T21:33:39Z,,,
arXIv2023,NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning,No.,1,"""No evidence""",2023,2023-06-21T21:26:19Z,,,
arXIv2023,"Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI",No.,1,"""No evidence""",2023,2023-06-21T11:55:17Z,,,
arXIv2023,Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals,No.,1,"""No evidence""",2023,2023-06-21T09:50:48Z,,,
arXIv2023,Mass-Producing Failures of Multimodal Systems with Language Models,No.,1,"""No evidence""",2023,2023-06-21T08:43:29Z,,,
arXIv2023,Deep Fusion: Efficient Network Training via Pre-trained Initializations,No.,1,"""No evidence""",2023,2023-06-20T21:30:54Z,,,
arXIv2023,Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications,No.,1,"""No evidence""",2023,2023-06-20T21:12:16Z,,,
arXIv2023,SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling,No.,1,"""No evidence""",2023,2023-06-20T20:59:10Z,,,
arXIv2023,On Compositionality and Improved Training of NADO,No.,1,"""No evidence""",2023,2023-06-20T18:36:52Z,,,
arXIv2023,Learning to Generate Better Than Your LLM,No.,1,"""No evidence""",2023,2023-06-20T18:19:17Z,,,
arXIv2023,Learning Profitable NFT Image Diffusions via Multiple Visual-Policy Guided Reinforcement Learning,No.,1,"""No evidence""",2023,2023-06-20T17:59:46Z,,,
arXIv2023,Lingua Manga: A Generic Large Language Model Centric System for Data Curation,No.,1,"""No evidence""",2023,2023-06-20T17:30:02Z,,,
arXIv2023,Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy,No.,1,"""No evidence""",2023,2023-06-20T16:16:56Z,,,
arXIv2023,Textbooks Are All You Need,No.,1,"""No evidence""",2023,2023-06-20T16:14:25Z,,,
arXIv2023,An empirical study of using radiology reports and images to improve ICU mortality prediction,No.,1,"""No evidence""",2023,2023-06-20T15:43:28Z,,,
arXIv2023,FAIR: A Causal Framework for Accurately Inferring Judgments Reversals,No.,1,"""No evidence""",2023,2023-06-20T15:02:25Z,,,
arXIv2023,"Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events",No.,1,"""No evidence""",2023,2023-06-20T14:01:29Z,,,
arXIv2023,Pushing the Limits of 3D Shape Generation at Scale,No.,1,"""No evidence""",2023,2023-06-20T13:01:19Z,,,
arXIv2023,Why can neural language models solve next-word prediction? A mathematical perspective,No.,1,"""No evidence""",2023,2023-06-20T10:41:23Z,,,
arXIv2023,RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing,No.,1,"""No evidence""",2023,2023-06-20T05:30:59Z,,,
arXIv2023,A GPT-4 Reticular Chemist for Guiding MOF Discovery,No.,1,"""No evidence""",2023,2023-06-20T05:26:44Z,,,
arXIv2023,A Novel Counterfactual Data Augmentation Method for Aspect-Based Sentiment Analysis,No.,1,"""No evidence""",2023,2023-06-20T03:25:51Z,,,
arXIv2023,InRank: Incremental Low-Rank Learning,No.,1,"""No evidence""",2023,2023-06-20T03:03:04Z,,,
arXIv2023,Eight challenges in developing theory of intelligence,No.,1,"""No evidence""",2023,2023-06-20T01:45:42Z,,,
arXIv2023,LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation,No.,1,"""No evidence""",2023,2023-06-20T01:16:11Z,,,
arXIv2023,Quilt-1M: One Million Image-Text Pairs for Histopathology,No.,1,"""No evidence""",2023,2023-06-20T00:14:47Z,,,
arXIv2023,Evaluating Privacy Questions From Stack Overflow: Can ChatGPT Compete?,No.,1,"""No evidence""",2023,2023-06-19T21:33:04Z,,,
arXIv2023,SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design,No.,1,"""No evidence""",2023,2023-06-19T17:03:46Z,,,
arXIv2023,JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving,No.,1,"""No evidence""",2023,2023-06-19T15:45:36Z,,,
arXIv2023,Deep Reinforcement Learning for ESG financial portfolio management,No.,1,"""No evidence""",2023,2023-06-19T14:56:01Z,,,
arXIv2023,"LARG, Language-based Automatic Reward and Goal Generation",No.,1,"""No evidence""",2023,2023-06-19T14:52:39Z,,,
arXIv2023,Fine-Tuning Language Models for Scientific Writing Support,No.,1,"""No evidence""",2023,2023-06-19T14:34:49Z,,,
arXIv2023,Multilingual Few-Shot Learning via Language Model Retrieval,No.,1,"""No evidence""",2023,2023-06-19T14:27:21Z,,,
arXIv2023,MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators,No.,1,"""No evidence""",2023,2023-06-19T12:58:17Z,,,
arXIv2023,Replace and Report: NLP Assisted Radiology Report Generation,No.,1,"""No evidence""",2023,2023-06-19T10:04:42Z,,,
arXIv2023,Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT,No.,1,"""No evidence""",2023,2023-06-19T06:45:04Z,,,
arXIv2023,PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning,No.,1,"""No evidence""",2023,2023-06-19T06:14:51Z,,,
arXIv2023,The Manipulation Problem: Conversational AI as a Threat to Epistemic Agency,No.,1,"""No evidence""",2023,2023-06-19T04:09:16Z,,,
arXIv2023,Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis,No.,1,"""No evidence""",2023,2023-06-18T12:20:42Z,,,
arXIv2023,Deceptive AI Ecosystems: The Case of ChatGPT,No.,1,"""No evidence""",2023,2023-06-18T10:36:19Z,,,
arXIv2023,Evolutionary Verbalizer Search for Prompt-based Few Shot Text Classification,No.,1,"""No evidence""",2023,2023-06-18T10:03:11Z,,,
arXIv2023,Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective,No.,1,"""No evidence""",2023,2023-06-18T09:54:33Z,,,
arXIv2023,Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets from Large Models,No.,1,"""No evidence""",2023,2023-06-18T03:09:52Z,,,
arXIv2023,MISMATCH: Fine-grained Evaluation of Machine-generated Text with Mismatch Error Types,No.,1,"""No evidence""",2023,2023-06-18T01:38:53Z,,,
arXIv2023,Generation of Radiology Findings in Chest X-Ray by Leveraging Collaborative Knowledge,No.,1,"""No evidence""",2023,2023-06-18T00:51:28Z,,,
arXIv2023,KEST: Kernel Distance Based Efficient Self-Training for Improving Controllable Text Generation,No.,1,"""No evidence""",2023,2023-06-17T19:40:57Z,,,
arXIv2023,Enhancing the Prediction of Emotional Experience in Movies using Deep Neural Networks: The Significance of Audio and Language,No.,1,"""No evidence""",2023,2023-06-17T17:40:27Z,,,
arXIv2023,Persian Semantic Role Labeling Using Transfer Learning and BERT-Based Models,No.,1,"""No evidence""",2023,2023-06-17T12:50:09Z,,,
arXIv2023,CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot Vision-and-Language Navigation,No.,1,"""No evidence""",2023,2023-06-17T11:44:04Z,,,
arXIv2023,FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue,No.,1,"""No evidence""",2023,2023-06-17T10:40:07Z,,,
arXIv2023,Active Policy Improvement from Multiple Black-box Oracles,No.,1,"""No evidence""",2023,2023-06-17T05:03:43Z,,,
arXIv2023,Snowman: A Million-scale Chinese Commonsense Knowledge Graph Distilled from Foundation Model,No.,1,"""No evidence""",2023,2023-06-17T02:51:33Z,,,
arXIv2023,Bloated Disclosures: Can ChatGPT Help Investors Process Information?,No.,1,"""No evidence""",2023,2023-06-17T01:22:08Z,,,
arXIv2023,ZeRO++: Extremely Efficient Collective Communication for Giant Model Training,No.,1,"""No evidence""",2023,2023-06-16T23:26:19Z,,,
arXIv2023,GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study,No.,1,"""No evidence""",2023,2023-06-16T23:11:06Z,,,
arXIv2023,Structured Thoughts Automaton: First Formalized Execution Model for Auto-Regressive Language Models,No.,1,"""No evidence""",2023,2023-06-16T22:04:50Z,,,
arXIv2023,Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness,No.,1,"""No evidence""",2023,2023-06-16T17:59:51Z,,,
arXIv2023,Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering,No.,1,"""No evidence""",2023,2023-06-16T17:47:57Z,,,
arXIv2023,Investigating Masking-based Data Generation in Language Models,No.,1,"""No evidence""",2023,2023-06-16T16:48:27Z,,,
arXIv2023,AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology,No.,1,"""No evidence""",2023,2023-06-16T16:35:59Z,,,
arXIv2023,Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions,No.,1,"""No evidence""",2023,2023-06-16T15:47:24Z,,,
arXIv2023,Revealing the impact of social circumstances on the selection of cancer therapy through natural language processing of social work notes,No.,1,"""No evidence""",2023,2023-06-16T14:40:39Z,,,
arXIv2023,FALL-E: A Foley Sound Synthesis Model and Strategies,No.,1,"""No evidence""",2023,2023-06-16T12:44:10Z,,,
arXIv2023,Mimicking Better by Matching the Approximate Action Distribution,No.,1,"""No evidence""",2023,2023-06-16T12:43:47Z,,,
arXIv2023,Understanding Deep Generative Models with Generalized Empirical Likelihoods,No.,1,"""No evidence""",2023,2023-06-16T11:33:47Z,,,
arXIv2023,Inspire creativity with ORIBA: Transform Artists' Original Characters into Chatbots through Large Language Model,No.,1,"""No evidence""",2023,2023-06-16T11:25:44Z,,,
arXIv2023,DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding,No.,1,"""No evidence""",2023,2023-06-16T07:55:20Z,,,
arXIv2023,ReactGenie: A Development Framework for Complex Multimodal Interactions Using Large Language Models,No.,1,"""No evidence""",2023,2023-06-16T06:53:26Z,,,
arXIv2023,CHORUS: Foundation Models for Unified Data Discovery and Exploration,No.,1,"""No evidence""",2023,2023-06-16T03:58:42Z,,,
arXIv2023,CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings,No.,1,"""No evidence""",2023,2023-06-16T02:39:45Z,,,
arXIv2023,Clickbait Classification and Spoiling Using Natural Language Processing,No.,1,"""No evidence""",2023,2023-06-16T01:45:57Z,,,
arXIv2023,Building blocks for complex tasks: Robust generative event extraction for radiology reports under domain shifts,No.,1,"""No evidence""",2023,2023-06-15T23:16:58Z,,,
arXIv2023,Ensembled Prediction Intervals for Causal Outcomes Under Hidden Confounding,No.,1,"""No evidence""",2023,2023-06-15T21:42:40Z,,,
arXIv2023,Simplified Temporal Consistency Reinforcement Learning,No.,1,"""No evidence""",2023,2023-06-15T19:37:43Z,,,
arXIv2023,"The pop song generator: designing an online course to teach collaborative, creative AI",No.,1,"""No evidence""",2023,2023-06-15T18:17:28Z,,,
arXIv2023,Semantic HELM: A Human-Readable Memory for Reinforcement Learning,No.,1,"""No evidence""",2023,2023-06-15T17:47:31Z,,,
arXIv2023,Propagating Knowledge Updates to LMs Through Distillation,No.,1,"""No evidence""",2023,2023-06-15T17:39:50Z,,,
arXIv2023,Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning,No.,1,"""No evidence""",2023,2023-06-15T16:53:26Z,,,
arXIv2023,Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?,No.,1,"""No evidence""",2023,2023-06-15T16:40:30Z,,,
arXIv2023,Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization,No.,1,"""No evidence""",2023,2023-06-15T15:58:04Z,,,
arXIv2023,ChessGPT: Bridging Policy Learning and Language Modeling,No.,1,"""No evidence""",2023,2023-06-15T15:35:31Z,,,
arXIv2023,Domain-specific ChatBots for Science using Embeddings,No.,1,"""No evidence""",2023,2023-06-15T15:26:20Z,,,
arXIv2023,Can ChatGPT pass the Vietnamese National High School Graduation Examination?,No.,1,"""No evidence""",2023,2023-06-15T14:47:03Z,,,
arXIv2023,Opportunities for Large Language Models and Discourse in Engineering Design,No.,1,"""No evidence""",2023,2023-06-15T14:46:44Z,,,
arXIv2023,Mapping Researcher Activity based on Publication Data by means of Transformers,No.,1,"""No evidence""",2023,2023-06-15T11:13:54Z,,,
arXIv2023,Voting Booklet Bias: Stance Detection in Swiss Federal Communication,No.,1,"""No evidence""",2023,2023-06-15T09:49:12Z,,,
arXIv2023,Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models,No.,1,"""No evidence""",2023,2023-06-15T09:48:14Z,,,
arXIv2023,Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models,No.,1,"""No evidence""",2023,2023-06-15T08:44:41Z,,,
arXIv2023,DocumentNet: Bridging the Data Gap in Document Pre-Training,No.,1,"""No evidence""",2023,2023-06-15T08:21:15Z,,,
arXIv2023,MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text Classification,No.,1,"""No evidence""",2023,2023-06-15T06:51:35Z,,,
arXIv2023,BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection,No.,1,"""No evidence""",2023,2023-06-15T04:41:28Z,,,
arXIv2023,PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews using Domain-specific Finetuned Large Language Models,No.,1,"""No evidence""",2023,2023-06-15T02:52:50Z,,,
arXIv2023,Langevin Thompson Sampling with Logarithmic Communication: Bandits and Reinforcement Learning,No.,1,"""No evidence""",2023,2023-06-15T01:16:29Z,,,
arXIv2023,World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models,No.,1,"""No evidence""",2023,2023-06-14T18:10:05Z,,,
arXIv2023,Radiology-GPT: A Large Language Model for Radiology,No.,1,"""No evidence""",2023,2023-06-14T17:57:24Z,,,
arXIv2023,Revealing the structure of language model capabilities,No.,1,"""No evidence""",2023,2023-06-14T15:43:25Z,,,
arXIv2023,WizardCoder: Empowering Code Large Language Models with Evol-Instruct,No.,1,"""No evidence""",2023,2023-06-14T15:18:48Z,,,
arXIv2023,PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation,No.,1,"""No evidence""",2023,2023-06-14T11:57:31Z,,,
arXIv2023,Building a Corpus for Biomedical Relation Extraction of Species Mentions,No.,1,"""No evidence""",2023,2023-06-14T09:56:32Z,,,
arXIv2023,A semantically enhanced dual encoder for aspect sentiment triplet extraction,No.,1,"""No evidence""",2023,2023-06-14T09:04:14Z,,,
arXIv2023,CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration,No.,1,"""No evidence""",2023,2023-06-14T03:39:32Z,,,
arXIv2023,Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the LIAR Dataset,No.,1,"""No evidence""",2023,2023-06-14T01:16:49Z,,,
arXIv2023,h2oGPT: Democratizing Large Language Models,No.,1,"""No evidence""",2023,2023-06-13T22:19:53Z,,,
arXIv2023,One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning,No.,1,"""No evidence""",2023,2023-06-13T17:59:32Z,,,
arXIv2023,MOFI: Learning Image Representations from Noisy Entity Annotated Images,No.,1,"""No evidence""",2023,2023-06-13T17:51:18Z,,,
arXIv2023,ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading,No.,1,"""No evidence""",2023,2023-06-13T16:10:10Z,,,
arXIv2023,ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer,No.,1,"""No evidence""",2023,2023-06-13T14:21:35Z,,,
arXIv2023,Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification,No.,1,"""No evidence""",2023,2023-06-13T14:19:45Z,,,
arXIv2023,Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4,No.,1,"""No evidence""",2023,2023-06-13T08:43:13Z,,,
arXIv2023,Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-06-13T06:26:54Z,,,
arXIv2023,Adding guardrails to advanced chatbots,No.,1,"""No evidence""",2023,2023-06-13T02:23:04Z,,,
arXIv2023,PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and Pause-based Prosody Modeling,No.,1,"""No evidence""",2023,2023-06-13T01:36:55Z,,,
arXIv2023,EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing,No.,1,"""No evidence""",2023,2023-06-12T18:56:25Z,,,
arXIv2023,Scalable 3D Captioning with Pretrained Models,No.,1,"""No evidence""",2023,2023-06-12T17:59:03Z,,,
arXIv2023,MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images,No.,1,"""No evidence""",2023,2023-06-12T17:31:23Z,,,
arXIv2023,Valley: Video Assistant with Large Language model Enhanced abilitY,No.,1,"""No evidence""",2023,2023-06-12T16:11:10Z,,,
arXIv2023,A Survey of Vision-Language Pre-training from the Lens of Multimodal Machine Translation,No.,1,"""No evidence""",2023,2023-06-12T15:56:10Z,,,
arXIv2023,Large language models and (non-)linguistic recursion,No.,1,"""No evidence""",2023,2023-06-12T15:50:38Z,,,
arXIv2023,Augmenting Language Models with Long-Term Memory,No.,1,"""No evidence""",2023,2023-06-12T15:13:39Z,,,
arXIv2023,Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning,No.,1,"""No evidence""",2023,2023-06-12T15:08:25Z,,,
arXIv2023,InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions,No.,1,"""No evidence""",2023,2023-06-12T14:42:23Z,,,
arXIv2023,Linear Classifier: An Often-Forgotten Baseline for Text Classification,No.,1,"""No evidence""",2023,2023-06-12T13:39:54Z,,,
arXIv2023,Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence,No.,1,"""No evidence""",2023,2023-06-12T12:40:48Z,,,
arXIv2023,Gradient Ascent Post-training Enhances Language Model Generalization,No.,1,"""No evidence""",2023,2023-06-12T11:59:33Z,,,
arXIv2023,Imbalanced Multi-label Classification for Business-related Text with Moderately Large Label Spaces,No.,1,"""No evidence""",2023,2023-06-12T11:51:50Z,,,
arXIv2023,Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping,No.,1,"""No evidence""",2023,2023-06-12T09:42:16Z,,,
arXIv2023,The BEA 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues,No.,1,"""No evidence""",2023,2023-06-12T08:21:34Z,,,
arXIv2023,On the Viability of using LLMs for SW/HW Co-Design: An Example in Designing CiM DNN Accelerators,No.,1,"""No evidence""",2023,2023-06-12T07:50:53Z,,,
arXIv2023,On the N-gram Approximation of Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-06-12T06:42:08Z,,,
arXIv2023,Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models,No.,1,"""No evidence""",2023,2023-06-12T06:34:16Z,,,
arXIv2023,Sticker820K: Empowering Interactive Retrieval with Stickers,No.,1,"""No evidence""",2023,2023-06-12T05:06:53Z,,,
arXIv2023,QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search,No.,1,"""No evidence""",2023,2023-06-11T15:39:59Z,,,
arXIv2023,Open Brain AI. Automatic Language Assessment,No.,1,"""No evidence""",2023,2023-06-11T14:37:45Z,,,
arXIv2023,Language Versatilists vs. Specialists: An Empirical Revisiting on Multilingual Transfer Ability,No.,1,"""No evidence""",2023,2023-06-11T14:03:09Z,,,
arXIv2023,"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark",No.,1,"""No evidence""",2023,2023-06-11T14:01:17Z,,,
arXIv2023,EaSyGuide : ESG Issue Identification Framework leveraging Abilities of Generative Large Language Models,No.,1,"""No evidence""",2023,2023-06-11T12:25:02Z,,,
arXIv2023,RoBERTweet: A BERT Language Model for Romanian Tweets,No.,1,"""No evidence""",2023,2023-06-11T06:11:56Z,,,
arXIv2023,Learning World Models with Identifiable Factorization,No.,1,"""No evidence""",2023,2023-06-11T02:25:15Z,,,
arXIv2023,TS-MoCo: Time-Series Momentum Contrast for Self-Supervised Physiological Representation Learning,No.,1,"""No evidence""",2023,2023-06-10T21:17:42Z,,,
arXIv2023,Universal Language Modelling agent,No.,1,"""No evidence""",2023,2023-06-10T21:09:16Z,,,
arXIv2023,Enhancing Low Resource NER Using Assisting Language And Transfer Learning,No.,1,"""No evidence""",2023,2023-06-10T16:31:04Z,,,
arXIv2023,"Improving Non-autoregressive Translation Quality with Pretrained Language Model, Embedding Distillation and Upsampling Strategy for CTC",No.,1,"""No evidence""",2023,2023-06-10T05:24:29Z,,,
arXIv2023,Language-Guided Traffic Simulation via Scene-Level Diffusion,No.,1,"""No evidence""",2023,2023-06-10T05:20:30Z,,,
arXIv2023,ECGBERT: Understanding Hidden Language of ECGs with Self-Supervised Representation Learning,No.,1,"""No evidence""",2023,2023-06-10T04:23:08Z,,,
arXIv2023,Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination,No.,1,"""No evidence""",2023,2023-06-10T02:01:02Z,,,
arXIv2023,Protect Your Prompts: Protocols for IP Protection in LLM Applications,No.,1,"""No evidence""",2023,2023-06-09T23:23:26Z,,,
arXIv2023,14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon,No.,1,"""No evidence""",2023,2023-06-09T22:22:02Z,,,
arXIv2023,AVScan2Vec: Feature Learning on Antivirus Scan Data for Production-Scale Malware Corpora,No.,1,"""No evidence""",2023,2023-06-09T19:53:40Z,,,
arXIv2023,Aladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions,No.,1,"""No evidence""",2023,2023-06-09T19:24:39Z,,,
arXIv2023,Morphosyntactic probing of multilingual BERT models,No.,1,"""No evidence""",2023,2023-06-09T19:15:20Z,,,
arXIv2023,Prodigy: An Expeditiously Adaptive Parameter-Free Learner,No.,1,"""No evidence""",2023,2023-06-09T17:59:35Z,,,
arXIv2023,Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding,No.,1,"""No evidence""",2023,2023-06-09T17:57:01Z,,,
arXIv2023,Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT,No.,1,"""No evidence""",2023,2023-06-09T17:53:19Z,,,
arXIv2023,PoET: A generative model of protein families as sequences-of-sequences,No.,1,"""No evidence""",2023,2023-06-09T16:06:36Z,,,
arXIv2023,GPT-Calls: Enhancing Call Segmentation and Tagging by Generating Synthetic Conversations via Large Language Models,No.,1,"""No evidence""",2023,2023-06-09T15:47:22Z,,,
arXIv2023,Understanding Telecom Language Through Large Language Models,No.,1,"""No evidence""",2023,2023-06-09T15:44:41Z,,,
arXIv2023,Bring Your Own (Non-Robust) Algorithm to Solve Robust MDPs by Estimating The Worst Kernel,No.,1,"""No evidence""",2023,2023-06-09T12:45:41Z,,,
arXIv2023,End-to-End Neural Network Compression via $\frac{\ell_1}{\ell_2}$ Regularized Latency Surrogates,No.,1,"""No evidence""",2023,2023-06-09T09:57:17Z,,,
arXIv2023,Large Language Models Are Semi-Parametric Reinforcement Learning Agents,No.,1,"""No evidence""",2023,2023-06-09T08:08:18Z,,,
arXIv2023,COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models,No.,1,"""No evidence""",2023,2023-06-09T03:53:42Z,,,
arXIv2023,Word sense extension,No.,1,"""No evidence""",2023,2023-06-09T00:54:21Z,,,
arXIv2023,The economic trade-offs of large language models: A case study,No.,1,"""No evidence""",2023,2023-06-08T20:35:53Z,,,
arXIv2023,Hexatagging: Projective Dependency Parsing as Tagging,No.,1,"""No evidence""",2023,2023-06-08T18:02:07Z,,,
arXIv2023,Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models,No.,1,"""No evidence""",2023,2023-06-08T17:59:56Z,,,
arXIv2023,The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues,No.,1,"""No evidence""",2023,2023-06-08T17:05:38Z,,,
arXIv2023,Simple and Controllable Music Generation,No.,1,"""No evidence""",2023,2023-06-08T15:31:05Z,,,
arXIv2023,Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction,No.,1,"""No evidence""",2023,2023-06-08T15:25:24Z,,,
arXIv2023,Mapping Brains with Language Models: A Survey,No.,1,"""No evidence""",2023,2023-06-08T11:50:58Z,,,
arXIv2023,Closing the Loop: Testing ChatGPT to Generate Model Explanations to Improve Human Labelling of Sponsored Content on Social Media,No.,1,"""No evidence""",2023,2023-06-08T11:29:58Z,,,
arXIv2023,Improving Language Model Integration for Neural Machine Translation,No.,1,"""No evidence""",2023,2023-06-08T10:00:19Z,,,
arXIv2023,K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization,No.,1,"""No evidence""",2023,2023-06-08T09:29:05Z,,,
arXIv2023,Active Inference in Hebbian Learning Networks,No.,1,"""No evidence""",2023,2023-06-08T09:15:01Z,,,
arXIv2023,Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale,No.,1,"""No evidence""",2023,2023-06-08T08:41:30Z,,,
arXIv2023,Log-based Anomaly Detection based on EVT Theory with feedback,No.,1,"""No evidence""",2023,2023-06-08T08:34:58Z,,,
arXIv2023,Leveraging Language Identification to Enhance Code-Mixed Text Classification,No.,1,"""No evidence""",2023,2023-06-08T06:43:10Z,,,
arXIv2023,InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding,No.,1,"""No evidence""",2023,2023-06-08T04:31:48Z,,,
arXIv2023,NOWJ at COLIEE 2023 -- Multi-Task and Ensemble Approaches in Legal Information Processing,No.,1,"""No evidence""",2023,2023-06-08T03:10:49Z,,,
arXIv2023,In-Context Learning through the Bayesian Prism,No.,1,"""No evidence""",2023,2023-06-08T02:38:23Z,,,
arXIv2023,Augmenting Hessians with Inter-Layer Dependencies for Mixed-Precision Post-Training Quantization,No.,1,"""No evidence""",2023,2023-06-08T02:18:58Z,,,
arXIv2023,Privately generating tabular data using language models,No.,1,"""No evidence""",2023,2023-06-07T21:53:14Z,,,
arXIv2023,How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources,No.,1,"""No evidence""",2023,2023-06-07T19:59:23Z,,,
arXIv2023,Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning,No.,1,"""No evidence""",2023,2023-06-07T15:55:34Z,,,
arXIv2023,Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers,No.,1,"""No evidence""",2023,2023-06-07T15:11:26Z,,,
arXIv2023,Neural Embeddings for Protein Graphs,No.,1,"""No evidence""",2023,2023-06-07T14:50:34Z,,,
arXIv2023,Examining Bias in Opinion Summarisation Through the Perspective of Opinion Diversity,No.,1,"""No evidence""",2023,2023-06-07T13:31:02Z,,,
arXIv2023,M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning,No.,1,"""No evidence""",2023,2023-06-07T12:35:37Z,,,
arXIv2023,Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems,No.,1,"""No evidence""",2023,2023-06-07T11:40:07Z,,,
arXIv2023,Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization,No.,1,"""No evidence""",2023,2023-06-07T08:23:58Z,,,
arXIv2023,A New Dataset and Empirical Study for Sentence Simplification in Chinese,No.,1,"""No evidence""",2023,2023-06-07T06:47:34Z,,,
arXIv2023,Understanding Place Identity with Generative AI,No.,1,"""No evidence""",2023,2023-06-07T02:32:45Z,,,
arXIv2023,XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations,No.,1,"""No evidence""",2023,2023-06-07T01:09:37Z,,,
arXIv2023,Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer,No.,1,"""No evidence""",2023,2023-06-07T00:33:02Z,,,
arXIv2023,Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health,No.,1,"""No evidence""",2023,2023-06-06T23:15:59Z,,,
arXIv2023,LLMZip: Lossless Text Compression using Large Language Models,No.,1,"""No evidence""",2023,2023-06-06T22:42:00Z,,,
arXIv2023,TKDP: Threefold Knowledge-enriched Deep Prompt Tuning for Few-shot Named Entity Recognition,No.,1,"""No evidence""",2023,2023-06-06T19:11:59Z,,,
arXIv2023,Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!,No.,1,"""No evidence""",2023,2023-06-06T18:00:47Z,,,
arXIv2023,Turning large language models into cognitive models,No.,1,"""No evidence""",2023,2023-06-06T18:00:01Z,,,
arXIv2023,Iterative Translation Refinement with Large Language Models,No.,1,"""No evidence""",2023,2023-06-06T16:51:03Z,,,
arXIv2023,LEACE: Perfect linear concept erasure in closed form,No.,1,"""No evidence""",2023,2023-06-06T16:07:24Z,,,
arXIv2023,On the Difference of BERT-style and CLIP-style Text Encoders,No.,1,"""No evidence""",2023,2023-06-06T13:41:09Z,,,
arXIv2023,Detecting Human Rights Violations on Social Media during Russia-Ukraine War,No.,1,"""No evidence""",2023,2023-06-06T12:59:03Z,,,
arXIv2023,Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias,No.,1,"""No evidence""",2023,2023-06-06T08:54:49Z,,,
arXIv2023,Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics,No.,1,"""No evidence""",2023,2023-06-06T06:49:58Z,,,
arXIv2023,I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models,No.,1,"""No evidence""",2023,2023-06-06T05:50:58Z,,,
arXIv2023,Vid2Act: Activate Offline Videos for Visual RL,No.,1,"""No evidence""",2023,2023-06-06T02:24:41Z,,,
arXIv2023,CoSiNES: Contrastive Siamese Network for Entity Standardization,No.,1,"""No evidence""",2023,2023-06-05T23:58:40Z,,,
arXIv2023,A Scalable and Adaptive System to Infer the Industry Sectors of Companies: Prompt + Model Tuning of Generative Language Models,No.,1,"""No evidence""",2023,2023-06-05T23:55:09Z,,,
arXIv2023,"""Medium"" LMs of Code in the Era of LLMs: Lessons From StackOverflow",No.,1,"""No evidence""",2023,2023-06-05T21:38:30Z,,,
arXIv2023,Zero-Shot 3D Shape Correspondence,No.,1,"""No evidence""",2023,2023-06-05T21:14:23Z,,,
arXIv2023,Information Flow Control in Machine Learning through Modular Model Architecture,No.,1,"""No evidence""",2023,2023-06-05T20:40:05Z,,,
arXIv2023,Risk-Aware Reward Shaping of Reinforcement Learning Agents for Autonomous Driving,No.,1,"""No evidence""",2023,2023-06-05T20:10:36Z,,,
arXIv2023,AutoScrum: Automating Project Planning Using Large Language Models,No.,1,"""No evidence""",2023,2023-06-05T19:16:37Z,,,
arXIv2023,RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems,No.,1,"""No evidence""",2023,2023-06-05T17:59:41Z,,,
arXIv2023,Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction,No.,1,"""No evidence""",2023,2023-06-05T17:59:21Z,,,
arXIv2023,Semantically-Prompted Language Models Improve Visual Descriptions,No.,1,"""No evidence""",2023,2023-06-05T17:22:54Z,,,
arXIv2023,PokemonChat: Auditing ChatGPT for Pokmon Universe Knowledge,No.,1,"""No evidence""",2023,2023-06-05T16:44:27Z,,,
arXIv2023,PolyVoice: Language Models for Speech to Speech Translation,No.,1,"""No evidence""",2023,2023-06-05T15:53:15Z,,,
arXIv2023,Which Argumentative Aspects of Hate Speech in Social Media can be reliably identified?,No.,1,"""No evidence""",2023,2023-06-05T15:50:57Z,,,
arXIv2023,"On ""Scientific Debt"" in NLP: A Case for More Rigour in Language Model Pre-Training Research",No.,1,"""No evidence""",2023,2023-06-05T13:43:50Z,,,
arXIv2023,Improving Conversational Recommendation Systems via Counterfactual Data Simulation,No.,1,"""No evidence""",2023,2023-06-05T12:48:56Z,,,
arXIv2023,COMET: Learning Cardinality Constrained Mixture of Experts with Trees and Local Search,No.,1,"""No evidence""",2023,2023-06-05T12:21:42Z,,,
arXIv2023,Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language,No.,1,"""No evidence""",2023,2023-06-05T11:46:45Z,,,
arXIv2023,MCTS: A Multi-Reference Chinese Text Simplification Dataset,No.,1,"""No evidence""",2023,2023-06-05T11:46:36Z,,,
arXIv2023,PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients' Problems and Data Augmentation with Black-box Large Language Models,No.,1,"""No evidence""",2023,2023-06-05T10:17:50Z,,,
arXIv2023,Orca: Progressive Learning from Complex Explanation Traces of GPT-4,No.,1,"""No evidence""",2023,2023-06-05T08:58:39Z,,,
arXIv2023,LexGPT 0.1: pre-trained GPT-J models with Pile of Law,No.,1,"""No evidence""",2023,2023-06-05T08:42:59Z,,,
arXIv2023,Efficient GPT Model Pre-training using Tensor Train Matrix Representation,No.,1,"""No evidence""",2023,2023-06-05T08:38:25Z,,,
arXIv2023,CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels,No.,1,"""No evidence""",2023,2023-06-05T08:35:31Z,,,
arXIv2023,Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency,No.,1,"""No evidence""",2023,2023-06-05T06:53:55Z,,,
arXIv2023,Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications,No.,1,"""No evidence""",2023,2023-06-05T04:46:44Z,,,
arXIv2023,Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model,No.,1,"""No evidence""",2023,2023-06-05T04:10:04Z,,,
arXIv2023,LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion,No.,1,"""No evidence""",2023,2023-06-05T03:32:26Z,,,
arXIv2023,RadLing: Towards Efficient Radiology Report Understanding,No.,1,"""No evidence""",2023,2023-06-04T21:53:04Z,,,
arXIv2023,For SALE: State-Action Representation Learning for Deep Reinforcement Learning,No.,1,"""No evidence""",2023,2023-06-04T19:47:46Z,,,
arXIv2023,"Taught by the Internet, Exploring Bias in OpenAIs GPT3",No.,1,"""No evidence""",2023,2023-06-04T18:21:44Z,,,
arXIv2023,Leverage Points in Modality Shifts: Comparing Language-only and Multimodal Word Representations,No.,1,"""No evidence""",2023,2023-06-04T12:53:12Z,,,
arXIv2023,SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings,No.,1,"""No evidence""",2023,2023-06-04T10:00:12Z,,,
arXIv2023,A Mathematical Abstraction for Balancing the Trade-off Between Creativity and Reality in Large Language Models,No.,1,"""No evidence""",2023,2023-06-04T08:12:34Z,,,
arXIv2023,Exploring and Verbalizing Academic Ideas by Concept Co-occurrence,No.,1,"""No evidence""",2023,2023-06-04T07:01:30Z,,,
arXIv2023,SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model,No.,1,"""No evidence""",2023,2023-06-04T03:09:21Z,,,
arXIv2023,Fine-Tuning Language Models with Advantage-Induced Policy Alignment,No.,1,"""No evidence""",2023,2023-06-04T01:59:40Z,,,
arXIv2023,GPT-FL: Generative Pre-trained Model-Assisted Federated Learning,No.,1,"""No evidence""",2023,2023-06-03T22:57:59Z,,,
arXIv2023,A two-way translation system of Chinese sign language based on computer vision,No.,1,"""No evidence""",2023,2023-06-03T16:00:57Z,,,
arXIv2023,Utilizing ChatGPT to Enhance Clinical Trial Enrollment,No.,1,"""No evidence""",2023,2023-06-03T10:54:23Z,,,
arXIv2023,Guided scenarios with simulated expert personae: a remarkable strategy to perform cognitive work,No.,1,"""No evidence""",2023,2023-06-03T00:56:34Z,,,
arXIv2023,LIC-GAN: Language Information Conditioned Graph Generative GAN Model,No.,1,"""No evidence""",2023,2023-06-02T22:39:14Z,,,
arXIv2023,Structural Similarities Between Language Models and Neural Response Measurements,No.,1,"""No evidence""",2023,2023-06-02T22:09:46Z,,,
arXIv2023,Probabilistic Adaptation of Text-to-Video Models,No.,1,"""No evidence""",2023,2023-06-02T19:00:17Z,,,
arXIv2023,DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation,No.,1,"""No evidence""",2023,2023-06-02T16:26:21Z,,,
arXIv2023,Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence Alignment Generation,No.,1,"""No evidence""",2023,2023-06-02T14:13:50Z,,,
arXIv2023,Data-Efficient French Language Modeling with CamemBERTa,No.,1,"""No evidence""",2023,2023-06-02T12:45:34Z,,,
arXIv2023,Concurrent Classifier Error Detection (CCED) in Large Scale Machine Learning Systems,No.,1,"""No evidence""",2023,2023-06-02T12:36:05Z,,,
arXIv2023,Light Coreference Resolution for Russian with Hierarchical Discourse Features,No.,1,"""No evidence""",2023,2023-06-02T11:41:24Z,,,
arXIv2023,Word Embeddings for Banking Industry,No.,1,"""No evidence""",2023,2023-06-02T01:00:44Z,,,
arXIv2023,Adapting an Unadaptable ASR System,No.,1,"""No evidence""",2023,2023-06-01T23:54:11Z,,,
arXIv2023,Multi-Dimensional Evaluation of Text Summarization with In-Context Learning,No.,1,"""No evidence""",2023,2023-06-01T23:27:49Z,,,
arXIv2023,Faster Causal Attention Over Large Sequences Through Sparse Flash Attention,No.,1,"""No evidence""",2023,2023-06-01T21:33:59Z,,,
arXIv2023,Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning,No.,1,"""No evidence""",2023,2023-06-01T21:11:24Z,,,
arXIv2023,"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",No.,1,"""No evidence""",2023,2023-06-01T20:03:56Z,,,
arXIv2023,UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of Multilingual BERT for Low-resource Sentiment Analysis,No.,1,"""No evidence""",2023,2023-06-01T19:10:09Z,,,
arXIv2023,Hierarchical Attention Encoder Decoder,No.,1,"""No evidence""",2023,2023-06-01T18:17:23Z,,,
arXIv2023,Reimagining Retrieval Augmented Language Models for Answering Queries,No.,1,"""No evidence""",2023,2023-06-01T18:08:51Z,,,
arXIv2023,ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER,No.,1,"""No evidence""",2023,2023-06-01T17:33:04Z,,,
arXIv2023,Birth of a Transformer: A Memory Viewpoint,No.,1,"""No evidence""",2023,2023-06-01T15:30:33Z,,,
arXIv2023,Interpretable Math Word Problem Solution Generation Via Step-by-step Planning,No.,1,"""No evidence""",2023,2023-06-01T15:16:18Z,,,
arXIv2023,In-Context Learning User Simulators for Task-Oriented Dialog Systems,No.,1,"""No evidence""",2023,2023-06-01T15:06:11Z,,,
arXIv2023,Column Type Annotation using ChatGPT,No.,1,"""No evidence""",2023,2023-06-01T14:40:52Z,,,
arXIv2023,Boosting the Performance of Transformer Architectures for Semantic Textual Similarity,No.,1,"""No evidence""",2023,2023-06-01T14:16:53Z,,,
arXIv2023,Predicting the Quality of Revisions in Argumentative Writing,No.,1,"""No evidence""",2023,2023-06-01T13:39:33Z,,,
arXIv2023,Automatic Glossary of Clinical Terminology: a Large-Scale Dictionary of Biomedical Definitions Generated from Ontological Knowledge,No.,1,"""No evidence""",2023,2023-06-01T13:37:55Z,,,
arXIv2023,Enhancing Programming eTextbooks with ChatGPT Generated Counterfactual-Thinking-Inspired Questions,No.,1,"""No evidence""",2023,2023-06-01T11:14:15Z,,,
arXIv2023,Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering,No.,1,"""No evidence""",2023,2023-06-01T10:28:12Z,,,
arXIv2023,On Masked Pre-training and the Marginal Likelihood,No.,1,"""No evidence""",2023,2023-06-01T10:20:44Z,,,
arXIv2023,Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning,No.,1,"""No evidence""",2023,2023-06-01T09:26:17Z,,,
arXIv2023,Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting,No.,1,"""No evidence""",2023,2023-06-01T07:19:28Z,,,
arXIv2023,Preference-grounded Token-level Guidance for Language Model Fine-tuning,No.,1,"""No evidence""",2023,2023-06-01T07:00:07Z,,,
arXIv2023,FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization,No.,1,"""No evidence""",2023,2023-06-01T03:31:12Z,,,
arXIv2023,Training-free Neural Architecture Search for RNNs and Transformers,No.,1,"""No evidence""",2023,2023-06-01T02:06:13Z,,,
arXIv2023,Graph-Level Embedding for Time-Evolving Graphs,No.,1,"""No evidence""",2023,2023-06-01T01:50:37Z,,,
arXIv2023,Feature Engineering-Based Detection of Buffer Overflow Vulnerability in Source Code Using Neural Networks,No.,1,"""No evidence""",2023,2023-06-01T01:44:49Z,,,
arXIv2023,Combinatorial Neural Bandits,No.,1,"""No evidence""",2023,2023-05-31T23:27:58Z,,,
arXIv2023,Examining the Emergence of Deductive Reasoning in Generative Language Models,No.,1,"""No evidence""",2023,2023-05-31T21:29:49Z,,,
arXIv2023,MuseCoco: Generating Symbolic Music from Text,No.,1,"""No evidence""",2023,2023-05-31T18:34:16Z,,,
arXIv2023,MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training,No.,1,"""No evidence""",2023,2023-05-31T18:27:43Z,,,
arXIv2023,Chatting Makes Perfect: Chat-based Image Retrieval,No.,1,"""No evidence""",2023,2023-05-31T17:38:08Z,,,
arXIv2023,AI for Low-Code for AI,No.,1,"""No evidence""",2023,2023-05-31T16:44:03Z,,,
arXIv2023,Human or Not? A Gamified Approach to the Turing Test,No.,1,"""No evidence""",2023,2023-05-31T16:32:22Z,,,
arXIv2023,Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind,No.,1,"""No evidence""",2023,2023-05-31T15:17:28Z,,,
arXIv2023,Deliberate then Generate: Enhanced Prompting Framework for Text Generation,No.,1,"""No evidence""",2023,2023-05-31T13:23:04Z,,,
arXIv2023,LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting,No.,1,"""No evidence""",2023,2023-05-31T13:03:17Z,,,
arXIv2023,IDAS: Intent Discovery with Abstractive Summarization,No.,1,"""No evidence""",2023,2023-05-31T12:19:40Z,,,
arXIv2023,Knowledge Base Question Answering for Space Debris Queries,No.,1,"""No evidence""",2023,2023-05-31T10:55:41Z,,,
arXIv2023,Evaluating GPT's Programming Capability through CodeWars' Katas,No.,1,"""No evidence""",2023,2023-05-31T10:36:16Z,,,
arXIv2023,XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech,No.,1,"""No evidence""",2023,2023-05-31T10:05:33Z,,,
arXIv2023,Building Extractive Question Answering System to Support Human-AI Health Coaching Model for Sleep Domain,No.,1,"""No evidence""",2023,2023-05-31T10:03:18Z,,,
arXIv2023,"Adverbs, Surprisingly",No.,1,"""No evidence""",2023,2023-05-31T08:30:08Z,,,
arXIv2023,SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT,No.,1,"""No evidence""",2023,2023-05-31T06:22:07Z,,,
arXIv2023,Perception and Semantic Aware Regularization for Sequential Confidence Calibration,No.,1,"""No evidence""",2023,2023-05-31T02:16:29Z,,,
arXIv2023,Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration,No.,1,"""No evidence""",2023,2023-05-31T01:09:28Z,,,
arXIv2023,Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial Uses,No.,1,"""No evidence""",2023,2023-05-30T21:52:33Z,,,
arXIv2023,"What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization",No.,1,"""No evidence""",2023,2023-05-30T21:23:47Z,,,
arXIv2023,Explaining Hate Speech Classification with Model Agnostic Methods,No.,1,"""No evidence""",2023,2023-05-30T19:52:56Z,,,
arXIv2023,Conceptual Design Generation Using Large Language Models,No.,1,"""No evidence""",2023,2023-05-30T19:32:39Z,,,
arXIv2023,Likelihood-Based Diffusion Language Models,No.,1,"""No evidence""",2023,2023-05-30T16:43:31Z,,,
arXIv2023,PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation,No.,1,"""No evidence""",2023,2023-05-30T16:39:54Z,,,
arXIv2023,Strategic Reasoning with Language Models,No.,1,"""No evidence""",2023,2023-05-30T16:09:19Z,,,
arXIv2023,GAN-MPC: Training Model Predictive Controllers with Parameterized Cost Functions using Demonstrations from Non-identical Experts,No.,1,"""No evidence""",2023,2023-05-30T15:15:30Z,,,
arXIv2023,DisCLIP: Open-Vocabulary Referring Expression Generation,No.,1,"""No evidence""",2023,2023-05-30T15:13:17Z,,,
arXIv2023,Empirical Sufficiency Lower Bounds for Language Modeling with Locally-Bootstrapped Semantic Structures,No.,1,"""No evidence""",2023,2023-05-30T10:09:48Z,,,
arXIv2023,Research on Multilingual News Clustering Based on Cross-Language Word Embeddings,No.,1,"""No evidence""",2023,2023-05-30T09:24:55Z,,,
arXIv2023,PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-05-30T08:41:33Z,,,
arXIv2023,KEYword based Sampling (KEYS) for Large Language Models,No.,1,"""No evidence""",2023,2023-05-30T01:35:04Z,,,
arXIv2023,Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model,No.,1,"""No evidence""",2023,2023-05-29T22:05:29Z,,,
arXIv2023,A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces,No.,1,"""No evidence""",2023,2023-05-29T20:30:38Z,,,
arXIv2023,PaLI-X: On Scaling up a Multilingual Vision and Language Model,No.,1,"""No evidence""",2023,2023-05-29T18:58:38Z,,,
arXIv2023,Transformer Language Models Handle Word Frequency in Prediction Head,No.,1,"""No evidence""",2023,2023-05-29T17:59:15Z,,,
arXIv2023,SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics,No.,1,"""No evidence""",2023,2023-05-29T17:50:52Z,,,
arXIv2023,Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence,No.,1,"""No evidence""",2023,2023-05-29T17:39:22Z,,,
arXIv2023,TaleCrafter: Interactive Story Visualization with Multiple Characters,No.,1,"""No evidence""",2023,2023-05-29T17:11:39Z,,,
arXIv2023,"Syntax and Semantics Meet in the ""Middle"": Probing the Syntax-Semantics Interface of LMs Through Agentivity",No.,1,"""No evidence""",2023,2023-05-29T16:24:01Z,,,
arXIv2023,GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking,No.,1,"""No evidence""",2023,2023-05-29T15:15:53Z,,,
arXIv2023,Writing user personas with Large Language Models: Testing phase 6 of a Thematic Analysis of semi-structured interviews,No.,1,"""No evidence""",2023,2023-05-29T14:09:14Z,,,
arXIv2023,InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions,No.,1,"""No evidence""",2023,2023-05-29T12:24:58Z,,,
arXIv2023,Test-Time Training on Nearest Neighbors for Large Language Models,No.,1,"""No evidence""",2023,2023-05-29T08:03:28Z,,,
arXIv2023,The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification,No.,1,"""No evidence""",2023,2023-05-29T05:35:44Z,,,
arXIv2023,ProcessGPT: Transforming Business Process Management with Generative Artificial Intelligence,No.,1,"""No evidence""",2023,2023-05-29T02:27:46Z,,,
arXIv2023,ChatGPT Informed Graph Neural Network for Stock Movement Prediction,No.,1,"""No evidence""",2023,2023-05-28T21:11:59Z,,,
arXIv2023,A Quantitative Review on Language Model Efficiency Research,No.,1,"""No evidence""",2023,2023-05-28T20:25:20Z,,,
arXIv2023,ConvGenVisMo: Evaluation of Conversational Generative Vision Models,No.,1,"""No evidence""",2023,2023-05-28T17:59:26Z,,,
arXIv2023,Generating EDU Extracts for Plan-Guided Summary Re-Ranking,No.,1,"""No evidence""",2023,2023-05-28T17:22:04Z,,,
arXIv2023,Whitening-based Contrastive Learning of Sentence Embeddings,No.,1,"""No evidence""",2023,2023-05-28T14:58:10Z,,,
arXIv2023,Learning a Structural Causal Model for Intuition Reasoning in Conversation,No.,1,"""No evidence""",2023,2023-05-28T13:54:09Z,,,
arXIv2023,Rethinking Masked Language Modeling for Chinese Spelling Correction,No.,1,"""No evidence""",2023,2023-05-28T13:19:12Z,,,
arXIv2023,RuSentNE-2023: Evaluating Entity-Oriented Sentiment Analysis on Russian News Texts,No.,1,"""No evidence""",2023,2023-05-28T10:04:15Z,,,
arXIv2023,Robust Natural Language Understanding with Residual Attention Debiasing,No.,1,"""No evidence""",2023,2023-05-28T04:25:04Z,,,
arXIv2023,Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making,No.,1,"""No evidence""",2023,2023-05-27T22:15:48Z,,,
arXIv2023,CIF-PT: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training,No.,1,"""No evidence""",2023,2023-05-27T15:39:13Z,,,
arXIv2023,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,No.,1,"""No evidence""",2023,2023-05-27T12:07:21Z,,,
arXIv2023,A Unified Framework for Slot based Response Generation in a Multimodal Dialogue System,No.,1,"""No evidence""",2023,2023-05-27T10:06:03Z,,,
arXIv2023,Understanding Emotion Valence is a Joint Deep Learning Task,No.,1,"""No evidence""",2023,2023-05-27T09:07:18Z,,,
arXIv2023,Towards Explainable Conversational Recommender Systems,No.,1,"""No evidence""",2023,2023-05-27T07:36:08Z,,,
arXIv2023,Complementary and Integrative Health Lexicon (CIHLex) and Entity Recognition in the Literature,No.,1,"""No evidence""",2023,2023-05-27T03:21:36Z,,,
arXIv2023,How Good is Automatic Segmentation as a Multimodal Discourse Annotation Aid?,No.,1,"""No evidence""",2023,2023-05-27T03:06:15Z,,,
arXIv2023,External Language Model Integration for Factorized Neural Transducers,No.,1,"""No evidence""",2023,2023-05-26T23:30:21Z,,,
arXIv2023,"Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale",No.,1,"""No evidence""",2023,2023-05-26T21:22:10Z,,,
arXIv2023,Im-Promptu: In-Context Composition from Image Prompts,No.,1,"""No evidence""",2023,2023-05-26T21:10:11Z,,,
arXIv2023,Generating Images with Multimodal Language Models,No.,1,"""No evidence""",2023,2023-05-26T19:22:03Z,,,
arXIv2023,Entailment as Robust Self-Learner,No.,1,"""No evidence""",2023,2023-05-26T18:41:23Z,,,
arXIv2023,Large Language Models as Tool Makers,No.,1,"""No evidence""",2023,2023-05-26T17:50:11Z,,,
arXIv2023,Reinforcement Learning with Simple Sequence Priors,No.,1,"""No evidence""",2023,2023-05-26T17:18:14Z,,,
arXIv2023,GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation,No.,1,"""No evidence""",2023,2023-05-26T17:15:22Z,,,
arXIv2023,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks",No.,1,"""No evidence""",2023,2023-05-26T17:14:43Z,,,
arXIv2023,How Powerful are Decoder-Only Transformer Neural Models?,No.,1,"""No evidence""",2023,2023-05-26T15:35:43Z,,,
arXIv2023,Diable: Efficient Dialogue State Tracking as Operations on Tables,No.,1,"""No evidence""",2023,2023-05-26T15:26:12Z,,,
arXIv2023,Zero-shot Visual Question Answering with Language Model Feedback,No.,1,"""No evidence""",2023,2023-05-26T15:04:20Z,,,
arXIv2023,An Empirical Comparison of LM-based Question and Answer Generation Methods,No.,1,"""No evidence""",2023,2023-05-26T14:59:53Z,,,
arXIv2023,Theoretical and Practical Perspectives on what Influence Functions Do,No.,1,"""No evidence""",2023,2023-05-26T14:26:36Z,,,
arXIv2023,MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting,No.,1,"""No evidence""",2023,2023-05-26T13:00:58Z,,,
arXIv2023,Green Runner: A tool for efficient model selection from model repositories,No.,1,"""No evidence""",2023,2023-05-26T12:00:37Z,,,
arXIv2023,ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks,No.,1,"""No evidence""",2023,2023-05-26T11:29:06Z,,,
arXIv2023,KNSE: A Knowledge-aware Natural Language Inference Framework for Dialogue Symptom Status Recognition,No.,1,"""No evidence""",2023,2023-05-26T11:23:26Z,,,
arXIv2023,HUB: Guiding Learned Optimizers with Continuous Prompt Tuning,No.,1,"""No evidence""",2023,2023-05-26T11:08:20Z,,,
arXIv2023,Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues,No.,1,"""No evidence""",2023,2023-05-26T10:19:30Z,,,
arXIv2023,Backpack Language Models,No.,1,"""No evidence""",2023,2023-05-26T09:26:23Z,,,
arXIv2023,AlignScore: Evaluating Factual Consistency with a Unified Alignment Function,No.,1,"""No evidence""",2023,2023-05-26T08:41:59Z,,,
arXIv2023,Attention Paper: How Generative AI Reshapes Digital Shadow Industry?,No.,1,"""No evidence""",2023,2023-05-26T08:03:50Z,,,
arXIv2023,CAILA: Concept-Aware Intra-Layer Adapters for Compositional Zero-Shot Learning,No.,1,"""No evidence""",2023,2023-05-26T07:02:57Z,,,
arXIv2023,Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning,No.,1,"""No evidence""",2023,2023-05-26T05:32:29Z,,,
arXIv2023,Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios,No.,1,"""No evidence""",2023,2023-05-26T01:42:57Z,,,
arXIv2023,Preliminary studies: Comparing LSTM and BLSTM Deep Neural Networks for Power Consumption Prediction,No.,1,"""No evidence""",2023,2023-05-26T00:12:50Z,,,
arXIv2023,IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks,No.,1,"""No evidence""",2023,2023-05-25T22:08:57Z,,,
arXIv2023,Voyager: An Open-Ended Embodied Agent with Large Language Models,No.,1,"""No evidence""",2023,2023-05-25T17:46:38Z,,,
arXIv2023,"Prompt-Free Diffusion: Taking ""Text"" out of Text-to-Image Diffusion Models",No.,1,"""No evidence""",2023,2023-05-25T16:30:07Z,,,
arXIv2023,Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning,No.,1,"""No evidence""",2023,2023-05-25T15:46:20Z,,,
arXIv2023,Understanding the Capabilities of Large Language Models for Automated Planning,No.,1,"""No evidence""",2023,2023-05-25T15:21:09Z,,,
arXIv2023,Mapping ChatGPT in Mainstream Media to Unravel Jobs and Diversity Challenges: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis,No.,1,"""No evidence""",2023,2023-05-25T15:10:51Z,,,
arXIv2023,"A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions",No.,1,"""No evidence""",2023,2023-05-25T15:09:11Z,,,
arXIv2023,Language Models Implement Simple Word2Vec-style Vector Arithmetic,No.,1,"""No evidence""",2023,2023-05-25T15:04:01Z,,,
arXIv2023,"VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation",No.,1,"""No evidence""",2023,2023-05-25T14:39:47Z,,,
arXIv2023,ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst,No.,1,"""No evidence""",2023,2023-05-25T14:34:08Z,,,
arXIv2023,Role-Play with Large Language Models,No.,1,"""No evidence""",2023,2023-05-25T11:36:52Z,,,
arXIv2023,Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving,No.,1,"""No evidence""",2023,2023-05-25T11:35:52Z,,,
arXIv2023,Emergence of a phonological bias in ChatGPT,No.,1,"""No evidence""",2023,2023-05-25T10:57:43Z,,,
arXIv2023,Response Generation in Longitudinal Dialogues: Which Knowledge Representation Helps?,No.,1,"""No evidence""",2023,2023-05-25T10:13:53Z,,,
arXIv2023,Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language,No.,1,"""No evidence""",2023,2023-05-25T08:32:41Z,,,
arXIv2023,Service Composition in the ChatGPT Era,No.,1,"""No evidence""",2023,2023-05-25T07:04:57Z,,,
arXIv2023,Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data,No.,1,"""No evidence""",2023,2023-05-25T05:10:28Z,,,
arXIv2023,PandaGPT: One Model To Instruction-Follow Them All,No.,1,"""No evidence""",2023,2023-05-25T04:16:07Z,,,
arXIv2023,BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model,No.,1,"""No evidence""",2023,2023-05-25T02:45:22Z,,,
arXIv2023,"Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education",No.,1,"""No evidence""",2023,2023-05-25T02:01:56Z,,,
arXIv2023,Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models,No.,1,"""No evidence""",2023,2023-05-24T22:09:35Z,,,
arXIv2023,Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps,No.,1,"""No evidence""",2023,2023-05-24T21:39:27Z,,,
arXIv2023,Lexinvariant Language Models,No.,1,"""No evidence""",2023,2023-05-24T19:10:46Z,,,
arXIv2023,Large Language Models for User Interest Journeys,No.,1,"""No evidence""",2023,2023-05-24T18:40:43Z,,,
arXIv2023,SPRING: Studying the Paper and Reasoning to Play Games,No.,1,"""No evidence""",2023,2023-05-24T18:14:35Z,,,
arXIv2023,Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering,No.,1,"""No evidence""",2023,2023-05-24T17:48:40Z,,,
arXIv2023,Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR,No.,1,"""No evidence""",2023,2023-05-24T17:46:03Z,,,
arXIv2023,Self-Evolution Learning for Discriminative Language Model Pretraining,No.,1,"""No evidence""",2023,2023-05-24T16:00:54Z,,,
arXIv2023,Neural Summarization of Electronic Health Records,No.,1,"""No evidence""",2023,2023-05-24T15:05:53Z,,,
arXIv2023,Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning,No.,1,"""No evidence""",2023,2023-05-24T14:51:01Z,,,
arXIv2023,Dynamic Masking Rate Schedules for MLM Pretraining,No.,1,"""No evidence""",2023,2023-05-24T12:24:12Z,,,
arXIv2023,C-STS: Conditional Semantic Textual Similarity,No.,1,"""No evidence""",2023,2023-05-24T12:18:50Z,,,
arXIv2023,Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,No.,1,"""No evidence""",2023,2023-05-24T12:00:24Z,,,
arXIv2023,Meta-Learning Online Adaptation of Language Models,No.,1,"""No evidence""",2023,2023-05-24T11:56:20Z,,,
arXIv2023,PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology,No.,1,"""No evidence""",2023,2023-05-24T11:55:50Z,,,
arXIv2023,Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning,No.,1,"""No evidence""",2023,2023-05-24T11:52:55Z,,,
arXIv2023,AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models,No.,1,"""No evidence""",2023,2023-05-24T11:52:23Z,,,
arXIv2023,A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis,No.,1,"""No evidence""",2023,2023-05-24T11:43:47Z,,,
arXIv2023,Ghostbuster: Detecting Text Ghostwritten by Large Language Models,No.,1,"""No evidence""",2023,2023-05-24T11:37:10Z,,,
arXIv2023,Is GPT-4 a Good Data Analyst?,No.,1,"""No evidence""",2023,2023-05-24T11:26:59Z,,,
arXIv2023,How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives,No.,1,"""No evidence""",2023,2023-05-24T11:16:09Z,,,
arXIv2023,EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought,No.,1,"""No evidence""",2023,2023-05-24T11:04:30Z,,,
arXIv2023,An Efficient Multilingual Language Model Compression through Vocabulary Trimming,No.,1,"""No evidence""",2023,2023-05-24T11:00:33Z,,,
arXIv2023,Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation,No.,1,"""No evidence""",2023,2023-05-24T10:50:31Z,,,
arXIv2023,The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models,No.,1,"""No evidence""",2023,2023-05-24T10:36:14Z,,,
arXIv2023,LAraBench: Benchmarking Arabic AI with Large Language Models,No.,1,"""No evidence""",2023,2023-05-24T10:16:16Z,,,
arXIv2023,Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning,No.,1,"""No evidence""",2023,2023-05-24T10:04:06Z,,,
arXIv2023,Aligning Language Models to User Opinions,No.,1,"""No evidence""",2023,2023-05-24T09:11:11Z,,,
arXIv2023,Structural Ambiguity and its Disambiguation in Language Model Based Parsers: the Case of Dutch Clause Relativization,No.,1,"""No evidence""",2023,2023-05-24T09:04:18Z,,,
arXIv2023,Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering,No.,1,"""No evidence""",2023,2023-05-24T08:55:08Z,,,
arXIv2023,ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games,No.,1,"""No evidence""",2023,2023-05-24T08:31:30Z,,,
arXIv2023,Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis,No.,1,"""No evidence""",2023,2023-05-24T08:29:50Z,,,
arXIv2023,From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions,No.,1,"""No evidence""",2023,2023-05-24T08:28:59Z,,,
arXIv2023,Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers,No.,1,"""No evidence""",2023,2023-05-24T08:08:26Z,,,
arXIv2023,Drafting Event Schemas using Language Models,No.,1,"""No evidence""",2023,2023-05-24T07:57:04Z,,,
arXIv2023,ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation,No.,1,"""No evidence""",2023,2023-05-24T07:42:15Z,,,
arXIv2023,"Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark",No.,1,"""No evidence""",2023,2023-05-24T06:43:23Z,,,
arXIv2023,Psychological Metrics for Dialog System Evaluation,No.,1,"""No evidence""",2023,2023-05-24T06:02:32Z,,,
arXIv2023,Trusting Your Evidence: Hallucinate Less with Context-aware Decoding,No.,1,"""No evidence""",2023,2023-05-24T05:19:15Z,,,
arXIv2023,Leftover-Lunch: Advantage-based Offline Reinforcement Learning for Language Models,No.,1,"""No evidence""",2023,2023-05-24T04:42:17Z,,,
arXIv2023,PruMUX: Augmenting Data Multiplexing with Model Compression,No.,1,"""No evidence""",2023,2023-05-24T04:22:38Z,,,
arXIv2023,Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation,No.,1,"""No evidence""",2023,2023-05-24T04:16:56Z,,,
arXIv2023,Scientific Opinion Summarization: Meta-review Generation with Checklist-guided Iterative Introspection,No.,1,"""No evidence""",2023,2023-05-24T02:33:35Z,,,
arXIv2023,Testing Causal Models of Word Meaning in GPT-3 and -4,No.,1,"""No evidence""",2023,2023-05-24T02:03:23Z,,,
arXIv2023,Instruction Tuning with Lexicons for Zero-Shot Style Classification,No.,1,"""No evidence""",2023,2023-05-24T00:17:36Z,,,
arXIv2023,Contextualized Topic Coherence Metrics,No.,1,"""No evidence""",2023,2023-05-23T23:53:29Z,,,
arXIv2023,Natural Language Decompositions of Implicit Content Enable Better Text Representations,No.,1,"""No evidence""",2023,2023-05-23T23:45:20Z,,,
arXIv2023,Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification Using Graph Neural Networks?,No.,1,"""No evidence""",2023,2023-05-23T23:31:24Z,,,
arXIv2023,Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings,No.,1,"""No evidence""",2023,2023-05-23T23:27:20Z,,,
arXIv2023,From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding,No.,1,"""No evidence""",2023,2023-05-23T23:22:20Z,,,
arXIv2023,Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations,No.,1,"""No evidence""",2023,2023-05-23T22:31:01Z,,,
arXIv2023,All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations,No.,1,"""No evidence""",2023,2023-05-23T22:30:43Z,,,
arXIv2023,Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural Machine Translation,No.,1,"""No evidence""",2023,2023-05-23T21:48:02Z,,,
arXIv2023,Mitigating Test-Time Bias for Fair Image Retrieval,No.,1,"""No evidence""",2023,2023-05-23T21:31:16Z,,,
arXIv2023,RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning,No.,1,"""No evidence""",2023,2023-05-23T20:15:56Z,,,
arXIv2023,NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders,No.,1,"""No evidence""",2023,2023-05-23T20:09:52Z,,,
arXIv2023,Are Large Language Models Robust Coreference Resolvers?,No.,1,"""No evidence""",2023,2023-05-23T19:38:28Z,,,
arXIv2023,CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains,No.,1,"""No evidence""",2023,2023-05-23T18:54:15Z,,,
arXIv2023,ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment,No.,1,"""No evidence""",2023,2023-05-23T18:37:30Z,,,
arXIv2023,Advancing Precise Outline-Conditioned Text Generation with Task Duality and Explicit Outline Control,No.,1,"""No evidence""",2023,2023-05-23T18:33:52Z,,,
arXIv2023,Handling Realistic Label Noise in BERT Text Classification,No.,1,"""No evidence""",2023,2023-05-23T18:30:31Z,,,
arXIv2023,"Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors",No.,1,"""No evidence""",2023,2023-05-23T18:17:43Z,,,
arXIv2023,Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,No.,1,"""No evidence""",2023,2023-05-23T18:07:04Z,,,
arXIv2023,Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training,No.,1,"""No evidence""",2023,2023-05-23T17:59:21Z,,,
arXIv2023,APPLS: Evaluating Evaluation Metrics for Plain Language Summarization,No.,1,"""No evidence""",2023,2023-05-23T17:59:19Z,,,
arXIv2023,Automatic Model Selection with Large Language Models for Reasoning,No.,1,"""No evidence""",2023,2023-05-23T17:57:59Z,,,
arXIv2023,QLoRA: Efficient Finetuning of Quantized LLMs,No.,1,"""No evidence""",2023,2023-05-23T17:50:33Z,,,
arXIv2023,VIP5: Towards Multimodal Foundation Models for Recommendation,No.,1,"""No evidence""",2023,2023-05-23T17:43:46Z,,,
arXIv2023,WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia,No.,1,"""No evidence""",2023,2023-05-23T17:37:36Z,,,
arXIv2023,Query Rewriting for Retrieval-Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-05-23T17:27:50Z,,,
arXIv2023,Active Learning Principles for In-Context Learning with Large Language Models,No.,1,"""No evidence""",2023,2023-05-23T17:16:04Z,,,
arXIv2023,R2H: Building Multimodal Navigation Helpers that Respond to Help Requests,No.,1,"""No evidence""",2023,2023-05-23T17:12:09Z,,,
arXIv2023,Training Transitive and Commutative Multimodal Transformers with LoReTTa,No.,1,"""No evidence""",2023,2023-05-23T16:58:55Z,,,
arXIv2023,Enhancing Chat Language Models by Scaling High-quality Instructional Conversations,No.,1,"""No evidence""",2023,2023-05-23T16:49:14Z,,,
arXIv2023,Skill-Based Few-Shot Selection for In-Context Learning,No.,1,"""No evidence""",2023,2023-05-23T16:28:29Z,,,
arXIv2023,Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks,No.,1,"""No evidence""",2023,2023-05-23T16:20:30Z,,,
arXIv2023,ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding,No.,1,"""No evidence""",2023,2023-05-23T16:15:31Z,,,
arXIv2023,EASE: An Easily-Customized Annotation System Powered by Efficiency Enhancement Mechanisms,No.,1,"""No evidence""",2023,2023-05-23T15:38:37Z,,,
arXIv2023,Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning,No.,1,"""No evidence""",2023,2023-05-23T15:26:20Z,,,
arXIv2023,CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation,No.,1,"""No evidence""",2023,2023-05-23T14:26:17Z,,,
arXIv2023,Assessing Linguistic Generalisation in Language Models: A Dataset for Brazilian Portuguese,No.,1,"""No evidence""",2023,2023-05-23T13:49:14Z,,,
arXIv2023,Embrace Opportunities and Face Challenges: Using ChatGPT in Undergraduate Students' Collaborative Interdisciplinary Learning,No.,1,"""No evidence""",2023,2023-05-23T13:14:49Z,,,
arXIv2023,Does ChatGPT have Theory of Mind?,No.,1,"""No evidence""",2023,2023-05-23T12:55:21Z,,,
arXIv2023,CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained Vision-Language Model,No.,1,"""No evidence""",2023,2023-05-23T12:51:20Z,,,
arXIv2023,When your Cousin has the Right Connections: Unsupervised Bilingual Lexicon Induction for Related Data-Imbalanced Languages,No.,1,"""No evidence""",2023,2023-05-23T12:49:21Z,,,
arXIv2023,Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model,No.,1,"""No evidence""",2023,2023-05-23T12:28:37Z,,,
arXIv2023,Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction,No.,1,"""No evidence""",2023,2023-05-23T12:05:09Z,,,
arXIv2023,Acquiring Frame Element Knowledge with Deep Metric Learning for Semantic Frame Induction,No.,1,"""No evidence""",2023,2023-05-23T11:02:28Z,,,
arXIv2023,Let's Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought,No.,1,"""No evidence""",2023,2023-05-23T10:26:42Z,,,
arXIv2023,NarrativeXL: A Large-scale Dataset For Long-Term Memory Models,No.,1,"""No evidence""",2023,2023-05-23T09:55:32Z,,,
arXIv2023,OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities,No.,1,"""No evidence""",2023,2023-05-23T09:36:51Z,,,
arXIv2023,Probing Brain Context-Sensitivity with Masked-Attention Generation,No.,1,"""No evidence""",2023,2023-05-23T09:36:21Z,,,
arXIv2023,GenSpectrum Chat: Data Exploration in Public Health Using Large Language Models,No.,1,"""No evidence""",2023,2023-05-23T08:43:43Z,,,
arXIv2023,Perception Test: A Diagnostic Benchmark for Multimodal Video Models,No.,1,"""No evidence""",2023,2023-05-23T07:54:37Z,,,
arXIv2023,Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks,No.,1,"""No evidence""",2023,2023-05-23T07:50:36Z,,,
arXIv2023,Goal-Driven Explainable Clustering via Language Descriptions,No.,1,"""No evidence""",2023,2023-05-23T07:05:50Z,,,
arXIv2023,ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings,No.,1,"""No evidence""",2023,2023-05-23T06:19:37Z,,,
arXIv2023,LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models,No.,1,"""No evidence""",2023,2023-05-23T05:57:09Z,,,
arXIv2023,Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models,No.,1,"""No evidence""",2023,2023-05-23T05:46:45Z,,,
arXIv2023,Exploring Large Language Models for Classical Philology,No.,1,"""No evidence""",2023,2023-05-23T05:21:02Z,,,
arXIv2023,Towards Legally Enforceable Hate Speech Detection for Public Forums,No.,1,"""No evidence""",2023,2023-05-23T04:34:41Z,,,
arXIv2023,"Physics of Language Models: Part 1, Context-Free Grammar",No.,1,"""No evidence""",2023,2023-05-23T04:28:16Z,,,
arXIv2023,Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain,No.,1,"""No evidence""",2023,2023-05-23T03:26:32Z,,,
arXIv2023,AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese,No.,1,"""No evidence""",2023,2023-05-23T03:19:21Z,,,
arXIv2023,LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation,No.,1,"""No evidence""",2023,2023-05-23T02:25:01Z,,,
arXIv2023,Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings,No.,1,"""No evidence""",2023,2023-05-23T01:03:40Z,,,
arXIv2023,Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefixes,No.,1,"""No evidence""",2023,2023-05-22T21:31:03Z,,,
arXIv2023,Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference,No.,1,"""No evidence""",2023,2023-05-22T20:58:09Z,,,
arXIv2023,Syntactic Knowledge via Graph Attention with BERT in Machine Translation,No.,1,"""No evidence""",2023,2023-05-22T18:56:14Z,,,
arXIv2023,GATology for Linguistics: What Syntactic Dependencies It Knows,No.,1,"""No evidence""",2023,2023-05-22T18:34:12Z,,,
arXIv2023,Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching,No.,1,"""No evidence""",2023,2023-05-22T17:59:43Z,,,
arXIv2023,Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents,No.,1,"""No evidence""",2023,2023-05-22T17:58:04Z,,,
arXIv2023,Training Diffusion Models with Reinforcement Learning,No.,1,"""No evidence""",2023,2023-05-22T17:57:41Z,,,
arXIv2023,Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design,No.,1,"""No evidence""",2023,2023-05-22T17:56:09Z,,,
arXIv2023,VideoLLM: Modeling Video Sequence with Large Language Models,No.,1,"""No evidence""",2023,2023-05-22T17:51:22Z,,,
arXIv2023,How do languages influence each other? Studying cross-lingual data sharing during LLM fine-tuning,No.,1,"""No evidence""",2023,2023-05-22T17:47:41Z,,,
arXIv2023,Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation,No.,1,"""No evidence""",2023,2023-05-22T17:36:14Z,,,
arXIv2023,GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints,No.,1,"""No evidence""",2023,2023-05-22T17:16:38Z,,,
arXIv2023,Logical Reasoning for Natural Language Inference Using Generated Facts as Atoms,No.,1,"""No evidence""",2023,2023-05-22T16:45:50Z,,,
arXIv2023,SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives,No.,1,"""No evidence""",2023,2023-05-22T16:24:46Z,,,
arXIv2023,Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline,No.,1,"""No evidence""",2023,2023-05-22T15:36:06Z,,,
arXIv2023,Partial Annotation Learning for Biomedical Entity Recognition,No.,1,"""No evidence""",2023,2023-05-22T15:18:38Z,,,
arXIv2023,Automated Feedback Generation for a Chemistry Database and Abstracting Exercise,No.,1,"""No evidence""",2023,2023-05-22T15:04:26Z,,,
arXIv2023,LMGQS: A Large-scale Dataset for Query-focused Summarization,No.,1,"""No evidence""",2023,2023-05-22T14:53:45Z,,,
arXIv2023,Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models,No.,1,"""No evidence""",2023,2023-05-22T14:52:47Z,,,
arXIv2023,Improving Robustness in Knowledge Distillation Using Domain-Targeted Data Augmentation,No.,1,"""No evidence""",2023,2023-05-22T14:37:05Z,,,
arXIv2023,SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents,No.,1,"""No evidence""",2023,2023-05-22T13:47:51Z,,,
arXIv2023,Bidirectional Transformer Reranker for Grammatical Error Correction,No.,1,"""No evidence""",2023,2023-05-22T13:04:48Z,,,
arXIv2023,GPT-SW3: An Autoregressive Language Model for the Nordic Languages,No.,1,"""No evidence""",2023,2023-05-22T12:47:48Z,,,
arXIv2023,Distilling ChatGPT for Explainable Automated Student Answer Assessment,No.,1,"""No evidence""",2023,2023-05-22T12:11:39Z,,,
arXIv2023,Stock and market index prediction using Informer network,No.,1,"""No evidence""",2023,2023-05-22T10:59:42Z,,,
arXIv2023,Meta-in-context learning in large language models,No.,1,"""No evidence""",2023,2023-05-22T10:40:36Z,,,
arXIv2023,FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering,No.,1,"""No evidence""",2023,2023-05-22T08:29:47Z,,,
arXIv2023,Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model,No.,1,"""No evidence""",2023,2023-05-22T08:18:58Z,,,
arXIv2023,Investigating Agency of LLMs in Human-AI Collaboration Tasks,No.,1,"""No evidence""",2023,2023-05-22T08:17:14Z,,,
arXIv2023,Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration,No.,1,"""No evidence""",2023,2023-05-22T07:53:36Z,,,
arXIv2023,Evaluating Pragmatic Abilities of Image Captioners on A3DS,No.,1,"""No evidence""",2023,2023-05-22T07:15:33Z,,,
arXIv2023,Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting,No.,1,"""No evidence""",2023,2023-05-22T05:14:38Z,,,
arXIv2023,Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture,No.,1,"""No evidence""",2023,2023-05-22T04:38:10Z,,,
arXIv2023,Learning Interpretable Style Embeddings via Prompting LLMs,No.,1,"""No evidence""",2023,2023-05-22T04:07:54Z,,,
arXIv2023,MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning,No.,1,"""No evidence""",2023,2023-05-22T04:00:38Z,,,
arXIv2023,Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition,No.,1,"""No evidence""",2023,2023-05-22T03:28:48Z,,,
arXIv2023,A Frustratingly Simple Decoding Method for Neural Text Generation,No.,1,"""No evidence""",2023,2023-05-22T03:28:47Z,,,
arXIv2023,A Comprehensive Survey of Sentence Representations: From the BERT Epoch to the ChatGPT Era and Beyond,No.,1,"""No evidence""",2023,2023-05-22T02:31:15Z,,,
arXIv2023,MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction,No.,1,"""No evidence""",2023,2023-05-22T01:32:50Z,,,
arXIv2023,PrOnto: Language Model Evaluations for 859 Languages,No.,1,"""No evidence""",2023,2023-05-22T00:33:52Z,,,
arXIv2023,PRODIGY: Enabling In-context Learning Over Graphs,No.,1,"""No evidence""",2023,2023-05-21T23:16:30Z,,,
arXIv2023,Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies,No.,1,"""No evidence""",2023,2023-05-21T22:44:25Z,,,
arXIv2023,ChatGPT Is More Likely to Be Perceived as Male Than Female,No.,1,"""No evidence""",2023,2023-05-21T20:57:12Z,,,
arXIv2023,A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers,No.,1,"""No evidence""",2023,2023-05-21T20:40:37Z,,,
arXIv2023,ToxBuster: In-game Chat Toxicity Buster with BERT,No.,1,"""No evidence""",2023,2023-05-21T18:53:26Z,,,
arXIv2023,BertRLFuzzer: A BERT and Reinforcement Learning Based Fuzzer,No.,1,"""No evidence""",2023,2023-05-21T18:26:31Z,,,
arXIv2023,SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly,No.,1,"""No evidence""",2023,2023-05-21T17:31:39Z,,,
arXIv2023,Multi-Head State Space Model for Speech Recognition,No.,1,"""No evidence""",2023,2023-05-21T16:28:57Z,,,
arXIv2023,Augmenting Autotelic Agents with Large Language Models,No.,1,"""No evidence""",2023,2023-05-21T15:42:41Z,,,
arXIv2023,Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models,No.,1,"""No evidence""",2023,2023-05-21T14:40:48Z,,,
arXIv2023,Infor-Coef: Information Bottleneck-based Dynamic Token Downsampling for Compact and Efficient language model,No.,1,"""No evidence""",2023,2023-05-21T13:30:56Z,,,
arXIv2023,F-PABEE: Flexible-patience-based Early Exiting for Single-label and Multi-label text Classification Tasks,No.,1,"""No evidence""",2023,2023-05-21T12:17:27Z,,,
arXIv2023,BiasAsker: Measuring the Bias in Conversational AI System,No.,1,"""No evidence""",2023,2023-05-21T11:25:59Z,,,
arXIv2023,"HIINT: Historical, Intra- and Inter- personal Dynamics Modeling with Cross-person Memory Transformer",No.,1,"""No evidence""",2023,2023-05-21T06:43:35Z,,,
arXIv2023,InstructVid2Vid: Controllable Video Editing with Natural Language Instructions,No.,1,"""No evidence""",2023,2023-05-21T03:28:13Z,,,
arXIv2023,"Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond",No.,1,"""No evidence""",2023,2023-05-20T21:52:24Z,,,
arXIv2023,Contextualizing Argument Quality Assessment with Relevant Knowledge,No.,1,"""No evidence""",2023,2023-05-20T21:04:58Z,,,
arXIv2023,Patton: Language Model Pretraining on Text-Rich Networks,No.,1,"""No evidence""",2023,2023-05-20T19:17:10Z,,,
arXIv2023,SEntFiN 1.0: Entity-Aware Sentiment Analysis for Financial News,No.,1,"""No evidence""",2023,2023-05-20T18:20:39Z,,,
arXIv2023,What Makes for Good Visual Tokenizers for Large Language Models?,No.,1,"""No evidence""",2023,2023-05-20T16:11:26Z,,,
arXIv2023,Practical PCG Through Large Language Models,No.,1,"""No evidence""",2023,2023-05-20T16:08:45Z,,,
arXIv2023,Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge,No.,1,"""No evidence""",2023,2023-05-20T15:24:38Z,,,
arXIv2023,Experimental results from applying GPT-4 to an unpublished formal language,No.,1,"""No evidence""",2023,2023-05-20T14:00:08Z,,,
arXIv2023,Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages,No.,1,"""No evidence""",2023,2023-05-20T12:26:41Z,,,
arXIv2023,Revisiting Automated Topic Model Evaluation with Large Language Models,No.,1,"""No evidence""",2023,2023-05-20T09:42:00Z,,,
arXIv2023,Learning Horn Envelopes via Queries from Large Language Models,No.,1,"""No evidence""",2023,2023-05-20T09:01:33Z,,,
arXIv2023,ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain,No.,1,"""No evidence""",2023,2023-05-20T04:50:20Z,,,
arXIv2023,CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities,No.,1,"""No evidence""",2023,2023-05-20T00:48:52Z,,,
arXIv2023,BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases,No.,1,"""No evidence""",2023,2023-05-19T22:02:55Z,,,
arXIv2023,Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis,No.,1,"""No evidence""",2023,2023-05-19T20:36:21Z,,,
arXIv2023,Evaluation of medium-large Language Models at zero-shot closed book generative question answering,No.,1,"""No evidence""",2023,2023-05-19T20:33:19Z,,,
arXIv2023,Self-QA: Unsupervised Knowledge Guided Language Model Alignment,No.,1,"""No evidence""",2023,2023-05-19T18:26:26Z,,,
arXIv2023,Eye-SpatialNet: Spatial Information Extraction from Ophthalmology Notes,No.,1,"""No evidence""",2023,2023-05-19T18:08:45Z,,,
arXIv2023,Scaling laws for language encoding models in fMRI,No.,1,"""No evidence""",2023,2023-05-19T17:53:03Z,,,
arXIv2023,Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs,No.,1,"""No evidence""",2023,2023-05-19T17:49:25Z,,,
arXIv2023,Multimodal Web Navigation with Instruction-Finetuned Foundation Models,No.,1,"""No evidence""",2023,2023-05-19T17:44:34Z,,,
arXIv2023,SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models,No.,1,"""No evidence""",2023,2023-05-19T17:30:19Z,,,
arXIv2023,Comparing Software Developers with ChatGPT: An Empirical Investigation,No.,1,"""No evidence""",2023,2023-05-19T17:25:54Z,,,
arXIv2023,Introspective Tips: Large Language Model for In-Context Decision Making,No.,1,"""No evidence""",2023,2023-05-19T11:20:37Z,,,
arXIv2023,IKDSumm: Incorporating Key-phrases into BERT for extractive Disaster Tweet Summarization,No.,1,"""No evidence""",2023,2023-05-19T11:05:55Z,,,
arXIv2023,Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition,No.,1,"""No evidence""",2023,2023-05-19T10:15:11Z,,,
arXIv2023,Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling,No.,1,"""No evidence""",2023,2023-05-19T09:26:02Z,,,
arXIv2023,A Sequence-to-Sequence Approach for Arabic Pronoun Resolution,No.,1,"""No evidence""",2023,2023-05-19T08:53:41Z,,,
arXIv2023,From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment,No.,1,"""No evidence""",2023,2023-05-19T08:06:50Z,,,
arXIv2023,AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning,No.,1,"""No evidence""",2023,2023-05-19T07:39:17Z,,,
arXIv2023,PointGPT: Auto-regressively Generative Pre-training from Point Clouds,No.,1,"""No evidence""",2023,2023-05-19T07:39:04Z,,,
arXIv2023,Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models,No.,1,"""No evidence""",2023,2023-05-19T06:21:15Z,,,
arXIv2023,Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast,No.,1,"""No evidence""",2023,2023-05-19T06:04:21Z,,,
arXIv2023,Unsupervised ASR via Cross-Lingual Pseudo-Labeling,No.,1,"""No evidence""",2023,2023-05-19T01:59:20Z,,,
arXIv2023,AutoTrial: Prompting Language Models for Clinical Trial Design,No.,1,"""No evidence""",2023,2023-05-19T01:04:16Z,,,
arXIv2023,Counterfactuals for Design: A Model-Agnostic Method For Design Recommendations,No.,1,"""No evidence""",2023,2023-05-18T21:10:58Z,,,
arXIv2023,Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model,No.,1,"""No evidence""",2023,2023-05-18T17:59:49Z,,,
arXIv2023,Evidence of Meaning in Language Models Trained on Programs,No.,1,"""No evidence""",2023,2023-05-18T17:58:08Z,,,
arXIv2023,LIMA: Less Is More for Alignment,No.,1,"""No evidence""",2023,2023-05-18T17:45:22Z,,,
arXIv2023,mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences,No.,1,"""No evidence""",2023,2023-05-18T17:22:53Z,,,
arXIv2023,LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation,No.,1,"""No evidence""",2023,2023-05-18T16:57:57Z,,,
arXIv2023,PDP: Parameter-free Differentiable Pruning is All You Need,No.,1,"""No evidence""",2023,2023-05-18T16:57:10Z,,,
arXIv2023,DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs,No.,1,"""No evidence""",2023,2023-05-18T16:22:33Z,,,
arXIv2023,Generalized Multiple Intent Conditioned Slot Filling,No.,1,"""No evidence""",2023,2023-05-18T15:04:52Z,,,
arXIv2023,SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,No.,1,"""No evidence""",2023,2023-05-18T14:23:25Z,,,
arXIv2023,Vaxformer: Antigenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2,No.,1,"""No evidence""",2023,2023-05-18T13:36:57Z,,,
arXIv2023,AIwriting: Relations Between Image Generation and Digital Writing,No.,1,"""No evidence""",2023,2023-05-18T09:23:05Z,,,
arXIv2023,MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts,No.,1,"""No evidence""",2023,2023-05-18T08:19:33Z,,,
arXIv2023,Discounted Thompson Sampling for Non-Stationary Bandit Problems,No.,1,"""No evidence""",2023,2023-05-18T05:29:52Z,,,
arXIv2023,A Survey on Time-Series Pre-Trained Models,No.,1,"""No evidence""",2023,2023-05-18T05:27:46Z,,,
arXIv2023,Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency,No.,1,"""No evidence""",2023,2023-05-18T05:17:57Z,,,
arXIv2023,ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval,No.,1,"""No evidence""",2023,2023-05-18T04:30:09Z,,,
arXIv2023,MolXPT: Wrapping Molecules with Text for Generative Pre-training,No.,1,"""No evidence""",2023,2023-05-18T03:58:19Z,,,
arXIv2023,Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation,No.,1,"""No evidence""",2023,2023-05-18T03:32:54Z,,,
arXIv2023,UMDFood: Vision-language models boost food composition compilation,No.,1,"""No evidence""",2023,2023-05-18T03:18:12Z,,,
arXIv2023,Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions,No.,1,"""No evidence""",2023,2023-05-17T23:55:32Z,,,
arXIv2023,A Better Way to Do Masked Language Model Scoring,No.,1,"""No evidence""",2023,2023-05-17T21:51:58Z,,,
arXIv2023,From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?,No.,1,"""No evidence""",2023,2023-05-17T21:05:23Z,,,
arXIv2023,Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning,No.,1,"""No evidence""",2023,2023-05-17T20:07:30Z,,,
arXIv2023,Sequential Best-Arm Identification with Application to Brain-Computer Interface,No.,1,"""No evidence""",2023,2023-05-17T18:49:44Z,,,
arXIv2023,DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining,No.,1,"""No evidence""",2023,2023-05-17T17:58:13Z,,,
arXIv2023,SLiC-HF: Sequence Likelihood Calibration with Human Feedback,No.,1,"""No evidence""",2023,2023-05-17T17:57:10Z,,,
arXIv2023,PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering,No.,1,"""No evidence""",2023,2023-05-17T17:50:16Z,,,
arXIv2023,PaLM 2 Technical Report,No.,1,"""No evidence""",2023,2023-05-17T17:46:53Z,,,
arXIv2023,Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents,No.,1,"""No evidence""",2023,2023-05-17T17:18:26Z,,,
arXIv2023,Human Choice Prediction in Language-based Persuasion Games: Simulation-based Off-Policy Evaluation,No.,1,"""No evidence""",2023,2023-05-17T16:38:11Z,,,
arXIv2023,Interactive Learning of Hierarchical Tasks from Dialog with GPT,No.,1,"""No evidence""",2023,2023-05-17T16:32:40Z,,,
arXIv2023,Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability,No.,1,"""No evidence""",2023,2023-05-17T14:58:06Z,,,
arXIv2023,M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models,No.,1,"""No evidence""",2023,2023-05-17T14:56:31Z,,,
arXIv2023,A quantitative study of NLP approaches to question difficulty estimation,No.,1,"""No evidence""",2023,2023-05-17T14:26:00Z,,,
arXIv2023,Prompt Engineering for Transformer-based Chemical Similarity Search Identifies Structurally Distinct Functional Analogues,No.,1,"""No evidence""",2023,2023-05-17T13:37:15Z,,,
arXIv2023,A Survey on Zero Pronoun Translation,No.,1,"""No evidence""",2023,2023-05-17T13:19:01Z,,,
arXIv2023,Generation of 3D Molecules in Pockets via Language Model,No.,1,"""No evidence""",2023,2023-05-17T11:31:06Z,,,
arXIv2023,When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario,No.,1,"""No evidence""",2023,2023-05-17T07:48:28Z,,,
arXIv2023,AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression,No.,1,"""No evidence""",2023,2023-05-17T07:40:12Z,,,
arXIv2023,Explaining black box text modules in natural language with language models,No.,1,"""No evidence""",2023,2023-05-17T00:29:18Z,,,
arXIv2023,CoEdIT: Text Editing by Task-Specific Instruction Tuning,No.,1,"""No evidence""",2023,2023-05-17T00:05:24Z,,,
arXIv2023,SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification,No.,1,"""No evidence""",2023,2023-05-16T20:12:59Z,,,
arXIv2023,A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot,No.,1,"""No evidence""",2023,2023-05-16T19:13:11Z,,,
arXIv2023,StructGPT: A General Framework for Large Language Model to Reason over Structured Data,No.,1,"""No evidence""",2023,2023-05-16T17:45:23Z,,,
arXIv2023,Advising OpenMP Parallelization via a Graph-Based Approach with Transformers,No.,1,"""No evidence""",2023,2023-05-16T16:56:10Z,,,
arXIv2023,Cooperation Is All You Need,No.,1,"""No evidence""",2023,2023-05-16T16:48:12Z,,,
arXIv2023,MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers,No.,1,"""No evidence""",2023,2023-05-16T13:50:24Z,,,
arXIv2023,Generating coherent comic with rich story using ChatGPT and Stable Diffusion,No.,1,"""No evidence""",2023,2023-05-16T13:11:45Z,,,
arXIv2023,A Preliminary Analysis on the Code Generation Capabilities of GPT-3.5 and Bard AI Models for Java Functions,No.,1,"""No evidence""",2023,2023-05-16T12:44:39Z,,,
arXIv2023,Constructing and Interpreting Causal Knowledge Graphs from News,No.,1,"""No evidence""",2023,2023-05-16T11:33:32Z,,,
arXIv2023,Weight-Inherited Distillation for Task-Agnostic BERT Compression,No.,1,"""No evidence""",2023,2023-05-16T01:51:22Z,,,
arXIv2023,ChatGPT and the Labor Market: Unraveling the Effect of AI Discussions on Students' Earnings Expectations,No.,1,"""No evidence""",2023,2023-05-15T23:13:27Z,,,
arXIv2023,Private Training Set Inspection in MLaaS,No.,1,"""No evidence""",2023,2023-05-15T22:56:27Z,,,
arXIv2023,DATED: Guidelines for Creating Synthetic Datasets for Engineering Design Applications,No.,1,"""No evidence""",2023,2023-05-15T21:00:09Z,,,
arXIv2023,Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models,No.,1,"""No evidence""",2023,2023-05-15T18:38:25Z,,,
arXIv2023,Interpretability at Scale: Identifying Causal Mechanisms in Alpaca,No.,1,"""No evidence""",2023,2023-05-15T17:15:40Z,,,
arXIv2023,Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text,No.,1,"""No evidence""",2023,2023-05-15T17:10:19Z,,,
arXIv2023,sustain.AI: a Recommender System to analyze Sustainability Reports,No.,1,"""No evidence""",2023,2023-05-15T15:16:19Z,,,
arXIv2023,Schema-adaptable Knowledge Graph Construction,No.,1,"""No evidence""",2023,2023-05-15T15:06:20Z,,,
arXIv2023,NeuSTIP: A Novel Neuro-Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs,No.,1,"""No evidence""",2023,2023-05-15T13:46:34Z,,,
arXIv2023,Text2Gender: A Deep Learning Architecture for Analysis of Blogger's Age and Gender,No.,1,"""No evidence""",2023,2023-05-15T13:26:50Z,,,
arXIv2023,DarkBERT: A Language Model for the Dark Side of the Internet,No.,1,"""No evidence""",2023,2023-05-15T12:23:10Z,,,
arXIv2023,Estimating the Causal Effects of Natural Logic Features in Neural NLI Models,No.,1,"""No evidence""",2023,2023-05-15T12:01:09Z,,,
arXIv2023,Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks,No.,1,"""No evidence""",2023,2023-05-15T09:52:36Z,,,
arXIv2023,Coreference-aware Double-channel Attention Network for Multi-party Dialogue Reading Comprehension,No.,1,"""No evidence""",2023,2023-05-15T05:01:29Z,,,
arXIv2023,Assessing the potential of AI-assisted pragmatic annotation: The case of apologies,No.,1,"""No evidence""",2023,2023-05-15T04:10:13Z,,,
arXIv2023,A Language Model of Java Methods with Train/Test Deduplication,No.,1,"""No evidence""",2023,2023-05-15T00:22:02Z,,,
arXIv2023,MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling,No.,1,"""No evidence""",2023,2023-05-14T22:01:24Z,,,
arXIv2023,Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives,No.,1,"""No evidence""",2023,2023-05-14T07:33:59Z,,,
arXIv2023,Improving End-to-End SLU performance with Prosodic Attention and Distillation,No.,1,"""No evidence""",2023,2023-05-14T04:38:20Z,,,
arXIv2023,GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content,No.,1,"""No evidence""",2023,2023-05-13T17:12:11Z,,,
arXIv2023,Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion,No.,1,"""No evidence""",2023,2023-05-13T12:53:11Z,,,
arXIv2023,Scalable Educational Question Generation with Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-05-13T09:08:27Z,,,
arXIv2023,"Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and GoogleBARD in Predictive Accuracy and Fact Checking",No.,1,"""No evidence""",2023,2023-05-13T08:58:37Z,,,
arXIv2023,Learning to Reason over Scene Graphs: A Case Study of Finetuning GPT-2 into a Robot Language Model for Grounded Task Planning,No.,1,"""No evidence""",2023,2023-05-12T18:14:32Z,,,
arXIv2023,Using Language Models to Detect Alarming Student Responses,No.,1,"""No evidence""",2023,2023-05-12T18:07:00Z,,,
arXIv2023,PALR: Personalization Aware LLMs for Recommendation,No.,1,"""No evidence""",2023,2023-05-12T17:21:33Z,,,
arXIv2023,WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models,No.,1,"""No evidence""",2023,2023-05-12T14:42:47Z,,,
arXIv2023,Unlocking the Potential of Medical Imaging with ChatGPT's Intelligent Diagnostics,No.,1,"""No evidence""",2023,2023-05-12T12:52:14Z,,,
arXIv2023,Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding,No.,1,"""No evidence""",2023,2023-05-12T12:46:13Z,,,
arXIv2023,Synergistic Interplay between Search and Large Language Models for Information Retrieval,No.,1,"""No evidence""",2023,2023-05-12T11:58:15Z,,,
arXIv2023,MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine,No.,1,"""No evidence""",2023,2023-05-12T09:37:13Z,,,
arXIv2023,A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering,No.,1,"""No evidence""",2023,2023-05-12T07:21:59Z,,,
arXIv2023,Harvesting Event Schemas from Large Language Models,No.,1,"""No evidence""",2023,2023-05-12T06:51:05Z,,,
arXIv2023,Masked Audio Text Encoders are Effective Multi-Modal Rescorers,No.,1,"""No evidence""",2023,2023-05-11T22:44:43Z,,,
arXIv2023,Overinformative Question Answering by Humans and Machines,No.,1,"""No evidence""",2023,2023-05-11T21:41:41Z,,,
arXIv2023,The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain,No.,1,"""No evidence""",2023,2023-05-11T21:06:39Z,,,
arXIv2023,SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues,No.,1,"""No evidence""",2023,2023-05-11T20:58:10Z,,,
arXIv2023,Simple Token-Level Confidence Improves Caption Correctness,No.,1,"""No evidence""",2023,2023-05-11T17:58:17Z,,,
arXIv2023,Musketeer: Joint Training for Multi-task Vision Language Model with Task Explanation Prompts,No.,1,"""No evidence""",2023,2023-05-11T17:57:49Z,,,
arXIv2023,Self-Chained Image-Language Model for Video Localization and Question Answering,No.,1,"""No evidence""",2023,2023-05-11T17:23:00Z,,,
arXIv2023,Transformers for CT Reconstruction From Monoplanar and Biplanar Radiographs,No.,1,"""No evidence""",2023,2023-05-11T16:43:39Z,,,
arXIv2023,Abugida Normalizer and Parser for Unicode texts,No.,1,"""No evidence""",2023,2023-05-11T14:34:08Z,,,
arXIv2023,Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning,No.,1,"""No evidence""",2023,2023-05-11T13:42:58Z,,,
arXIv2023,INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models,No.,1,"""No evidence""",2023,2023-05-11T09:24:41Z,,,
arXIv2023,BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews,No.,1,"""No evidence""",2023,2023-05-11T06:27:38Z,,,
arXIv2023,How to Index Item IDs for Recommendation Foundation Models,No.,1,"""No evidence""",2023,2023-05-11T05:02:37Z,,,
arXIv2023,ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models,No.,1,"""No evidence""",2023,2023-05-11T04:51:21Z,,,
arXIv2023,Long-Tailed Question Answering in an Open World,No.,1,"""No evidence""",2023,2023-05-11T04:28:58Z,,,
arXIv2023,Domain Incremental Lifelong Learning in an Open World,No.,1,"""No evidence""",2023,2023-05-11T04:19:08Z,,,
arXIv2023,ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps,No.,1,"""No evidence""",2023,2023-05-10T21:37:44Z,,,
arXIv2023,Autonomous GIS: the next-generation AI-powered GIS,No.,1,"""No evidence""",2023,2023-05-10T20:46:29Z,,,
arXIv2023,BIOT: Cross-data Biosignal Learning in the Wild,No.,1,"""No evidence""",2023,2023-05-10T19:26:58Z,,,
arXIv2023,A Method to Automate the Discharge Summary Hospital Course for Neurology Patients,No.,1,"""No evidence""",2023,2023-05-10T18:53:51Z,,,
arXIv2023,LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM,No.,1,"""No evidence""",2023,2023-05-10T18:26:42Z,,,
arXIv2023,Text-To-Concept (and Back) via Cross-Model Alignment,No.,1,"""No evidence""",2023,2023-05-10T18:01:06Z,,,
arXIv2023,VideoChat: Chat-Centric Video Understanding,No.,1,"""No evidence""",2023,2023-05-10T17:59:04Z,,,
arXIv2023,When ChatGPT for Computer Vision Will Come? From 2D to 3D,No.,1,"""No evidence""",2023,2023-05-10T13:29:51Z,,,
arXIv2023,Talking with Machines: A Comprehensive Survey of Emergent Dialogue Systems,No.,1,"""No evidence""",2023,2023-05-10T12:24:03Z,,,
arXIv2023,A Glimpse in ChatGPT Capabilities and its impact for AI research,No.,1,"""No evidence""",2023,2023-05-10T12:10:51Z,,,
arXIv2023,GPT Models Meet Robotic Applications: Co-Speech Gesturing Chat System,No.,1,"""No evidence""",2023,2023-05-10T10:14:16Z,,,
arXIv2023,Bits of Grass: Does GPT already know how to write like Whitman?,No.,1,"""No evidence""",2023,2023-05-10T09:02:34Z,,,
arXIv2023,Fast Distributed Inference Serving for Large Language Models,No.,1,"""No evidence""",2023,2023-05-10T06:17:50Z,,,
arXIv2023,Towards an Automatic Optimisation Model Generator Assisted with Generative Pre-trained Transformer,No.,1,"""No evidence""",2023,2023-05-09T23:51:14Z,,,
arXIv2023,Multilevel Sentence Embeddings for Personality Prediction,No.,1,"""No evidence""",2023,2023-05-09T20:02:18Z,,,
arXIv2023,Exploring the Efficacy of ChatGPT in Analyzing Student Teamwork Feedback with an Existing Taxonomy,No.,1,"""No evidence""",2023,2023-05-09T19:55:50Z,,,
arXIv2023,Vision-Language Models in Remote Sensing: Current Progress and Future Trends,No.,1,"""No evidence""",2023,2023-05-09T19:17:07Z,,,
arXIv2023,CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors,No.,1,"""No evidence""",2023,2023-05-09T18:40:31Z,,,
arXIv2023,Policy Gradient Methods in the Presence of Symmetries and State Abstractions,No.,1,"""No evidence""",2023,2023-05-09T17:59:10Z,,,
arXIv2023,InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language,No.,1,"""No evidence""",2023,2023-05-09T17:58:34Z,,,
arXIv2023,A Review of Vision-Language Models and their Performance on the Hateful Memes Challenge,No.,1,"""No evidence""",2023,2023-05-09T17:55:29Z,,,
arXIv2023,Large Language Models Humanize Technology,No.,1,"""No evidence""",2023,2023-05-09T16:05:36Z,,,
arXIv2023,Effects of sub-word segmentation on performance of transformer language models,No.,1,"""No evidence""",2023,2023-05-09T14:30:29Z,,,
arXIv2023,What is the best recipe for character-level encoder-only modelling?,No.,1,"""No evidence""",2023,2023-05-09T14:00:15Z,,,
arXIv2023,Estimating related words computationally using language model from the Mahabharata -- an Indian epic,No.,1,"""No evidence""",2023,2023-05-09T13:13:26Z,,,
arXIv2023,PLM-GNN: A Webpage Classification Method based on Joint Pre-trained Language Model and Graph Neural Network,No.,1,"""No evidence""",2023,2023-05-09T12:19:10Z,,,
arXIv2023,Large Language Model Programs,No.,1,"""No evidence""",2023,2023-05-09T11:55:36Z,,,
arXIv2023,GPT-NAS: Evolutionary Neural Architecture Search with the Generative Pre-Trained Model,No.,1,"""No evidence""",2023,2023-05-09T11:29:42Z,,,
arXIv2023,Alleviating Over-smoothing for Unsupervised Sentence Representation,No.,1,"""No evidence""",2023,2023-05-09T11:00:02Z,,,
arXIv2023,Detection of depression on social networks using transformers and ensembles,No.,1,"""No evidence""",2023,2023-05-09T10:21:14Z,,,
arXIv2023,The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation,No.,1,"""No evidence""",2023,2023-05-09T09:35:03Z,,,
arXIv2023,Robust Acoustic and Semantic Contextual Biasing in Neural Transducers for Speech Recognition,No.,1,"""No evidence""",2023,2023-05-09T08:51:44Z,,,
arXIv2023,Distilling Script Knowledge from Large Language Models for Constrained Language Planning,No.,1,"""No evidence""",2023,2023-05-09T08:19:32Z,,,
arXIv2023,StarCoder: may the source be with you!,No.,1,"""No evidence""",2023,2023-05-09T08:16:42Z,,,
arXIv2023,FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance,No.,1,"""No evidence""",2023,2023-05-09T05:11:02Z,,,
arXIv2023,"Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media",No.,1,"""No evidence""",2023,2023-05-09T02:49:09Z,,,
arXIv2023,Generating Phishing Attacks using ChatGPT,No.,1,"""No evidence""",2023,2023-05-09T02:38:05Z,,,
arXIv2023,Vulnerability Detection Using Two-Stage Deep Learning Models,No.,1,"""No evidence""",2023,2023-05-08T22:12:34Z,,,
arXIv2023,"Dreams Are More ""Predictable'' Than You Think",No.,1,"""No evidence""",2023,2023-05-08T21:24:12Z,,,
arXIv2023,GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning,No.,1,"""No evidence""",2023,2023-05-08T19:16:26Z,,,
arXIv2023,Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?,No.,1,"""No evidence""",2023,2023-05-08T18:26:39Z,,,
arXIv2023,The impact and applications of ChatGPT: a systematic review of literature reviews,No.,1,"""No evidence""",2023,2023-05-08T17:57:34Z,,,
arXIv2023,MultiModal-GPT: A Vision and Language Model for Dialogue with Humans,No.,1,"""No evidence""",2023,2023-05-08T15:45:42Z,,,
arXIv2023,ChatGPT: Vision and Challenges,No.,1,"""No evidence""",2023,2023-05-08T14:54:44Z,,,
arXIv2023,DEFENDER: DTW-Based Episode Filtering Using Demonstrations for Enhancing RL Safety,No.,1,"""No evidence""",2023,2023-05-08T14:23:27Z,,,
arXIv2023,PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-05-08T12:51:00Z,,,
arXIv2023,HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation,No.,1,"""No evidence""",2023,2023-05-08T09:31:13Z,,,
arXIv2023,Prompted LLMs as Chatbot Modules for Long Open-domain Conversation,No.,1,"""No evidence""",2023,2023-05-08T08:09:00Z,,,
arXIv2023,A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues,No.,1,"""No evidence""",2023,2023-05-08T08:05:40Z,,,
arXIv2023,Scene Text Recognition with Image-Text Matching-guided Dictionary,No.,1,"""No evidence""",2023,2023-05-08T07:47:49Z,,,
arXIv2023,Event Knowledge Incorporation with Posterior Regularization for Event-Centric Question Answering,No.,1,"""No evidence""",2023,2023-05-08T07:45:12Z,,,
arXIv2023,Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models,No.,1,"""No evidence""",2023,2023-05-08T03:34:33Z,,,
arXIv2023,Improving Cross-Task Generalization with Step-by-Step Instructions,No.,1,"""No evidence""",2023,2023-05-08T02:50:41Z,,,
arXIv2023,Optimizing National Security Strategies through LLM-Driven Artificial Intelligence Integration,No.,1,"""No evidence""",2023,2023-05-07T21:51:39Z,,,
arXIv2023,Stanford MLab at SemEval-2023 Task 10: Exploring GloVe- and Transformer-Based Methods for the Explainable Detection of Online Sexism,No.,1,"""No evidence""",2023,2023-05-07T18:58:54Z,,,
arXIv2023,Empowering Language Model with Guided Knowledge Fusion for Biomedical Document Re-ranking,No.,1,"""No evidence""",2023,2023-05-07T17:45:47Z,,,
arXIv2023,FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering,No.,1,"""No evidence""",2023,2023-05-07T16:52:21Z,,,
arXIv2023,Unified Demonstration Retriever for In-Context Learning,No.,1,"""No evidence""",2023,2023-05-07T16:07:11Z,,,
arXIv2023,Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models,No.,1,"""No evidence""",2023,2023-05-07T06:48:06Z,,,
arXIv2023,Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models,No.,1,"""No evidence""",2023,2023-05-07T00:56:58Z,,,
arXIv2023,Exploring Human-Like Translation Strategy with Large Language Models,No.,1,"""No evidence""",2023,2023-05-06T19:03:12Z,,,
arXIv2023,Rhetorical Role Labeling of Legal Documents using Transformers and Graph Neural Networks,No.,1,"""No evidence""",2023,2023-05-06T17:04:51Z,,,
arXIv2023,An improved regret analysis for UCB-N and TS-N,No.,1,"""No evidence""",2023,2023-05-06T16:42:11Z,,,
arXIv2023,Refining the Responses of LLMs by Themselves,No.,1,"""No evidence""",2023,2023-05-06T13:03:45Z,,,
arXIv2023,Knowledge Transfer from Teachers to Learners in Growing-Batch Reinforcement Learning,No.,1,"""No evidence""",2023,2023-05-05T22:55:34Z,,,
arXIv2023,CHAI-DT: A Framework for Prompting Conversational Generative AI Agents to Actively Participate in Co-Creation,No.,1,"""No evidence""",2023,2023-05-05T21:25:35Z,,,
arXIv2023,Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces,No.,1,"""No evidence""",2023,2023-05-05T19:47:41Z,,,
arXIv2023,Neural Exploitation and Exploration of Contextual Bandits,No.,1,"""No evidence""",2023,2023-05-05T18:34:49Z,,,
arXIv2023,Otter: A Multi-Modal Model with In-Context Instruction Tuning,No.,1,"""No evidence""",2023,2023-05-05T17:59:46Z,,,
arXIv2023,DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition,No.,1,"""No evidence""",2023,2023-05-05T16:59:26Z,,,
arXIv2023,Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models,No.,1,"""No evidence""",2023,2023-05-05T16:28:03Z,,,
arXIv2023,In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models,No.,1,"""No evidence""",2023,2023-05-05T14:30:20Z,,,
arXIv2023,NewsQuote: A Dataset Built on Quote Extraction and Attribution for Expert Recommendation in Fact-Checking,No.,1,"""No evidence""",2023,2023-05-05T11:10:48Z,,,
arXIv2023,Simulating H.P. Lovecraft horror literature with the ChatGPT large language model,No.,1,"""No evidence""",2023,2023-05-05T11:03:03Z,,,
arXIv2023,Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering,No.,1,"""No evidence""",2023,2023-05-05T09:58:40Z,,,
arXIv2023,MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic,No.,1,"""No evidence""",2023,2023-05-05T08:14:48Z,,,
arXIv2023,Block the Label and Noise: An N-Gram Masked Speller for Chinese Spell Checking,No.,1,"""No evidence""",2023,2023-05-05T06:43:56Z,,,
arXIv2023,LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa,No.,1,"""No evidence""",2023,2023-05-05T06:05:45Z,,,
arXIv2023,Open Information Extraction via Chunks,No.,1,"""No evidence""",2023,2023-05-05T06:03:54Z,,,
arXIv2023,Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition,No.,1,"""No evidence""",2023,2023-05-04T22:14:44Z,,,
arXIv2023,Influence of various text embeddings on clustering performance in NLP,No.,1,"""No evidence""",2023,2023-05-04T20:53:19Z,,,
arXIv2023,Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks,No.,1,"""No evidence""",2023,2023-05-04T18:34:50Z,,,
arXIv2023,Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models,No.,1,"""No evidence""",2023,2023-05-04T17:49:09Z,,,
arXIv2023,Rethinking Population-assisted Off-policy Reinforcement Learning,No.,1,"""No evidence""",2023,2023-05-04T15:53:00Z,,,
arXIv2023,ChatGPT and Works Scholarly: Best Practices and Legal Pitfalls in Writing with AI,No.,1,"""No evidence""",2023,2023-05-04T15:38:20Z,,,
arXIv2023,The AI generation gap: Are Gen Z students more interested in adopting generative AI such as ChatGPT in teaching and learning than their Gen X and Millennial Generation teachers?,No.,1,"""No evidence""",2023,2023-05-04T14:42:06Z,,,
arXIv2023,Masked Structural Growth for 2x Faster Language Model Pre-training,No.,1,"""No evidence""",2023,2023-05-04T14:28:39Z,,,
arXIv2023,Leveraging BERT Language Model for Arabic Long Document Classification,No.,1,"""No evidence""",2023,2023-05-04T13:56:32Z,,,
arXIv2023,Caption Anything: Interactive Image Description with Diverse Multimodal Controls,No.,1,"""No evidence""",2023,2023-05-04T09:48:22Z,,,
arXIv2023,DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning,No.,1,"""No evidence""",2023,2023-05-04T07:28:45Z,,,
arXIv2023,Analyzing Hong Kong's Legal Judgments from a Computational Linguistics point-of-view,No.,1,"""No evidence""",2023,2023-05-04T05:23:11Z,,,
arXIv2023,Late-Binding Scholarship in the Age of AI: Navigating Legal and Normative Challenges of a New Form of Knowledge Production,No.,1,"""No evidence""",2023,2023-05-04T04:14:28Z,,,
arXIv2023,"Governance of the AI, by the AI, and for the AI",No.,1,"""No evidence""",2023,2023-05-04T03:29:07Z,,,
arXIv2023,Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction,No.,1,"""No evidence""",2023,2023-05-04T00:12:52Z,,,
arXIv2023,Shap-E: Generating Conditional 3D Implicit Functions,No.,1,"""No evidence""",2023,2023-05-03T23:59:13Z,,,
arXIv2023,evaluating bert and parsbert for analyzing persian advertisement data,No.,1,"""No evidence""",2023,2023-05-03T20:50:05Z,,,
arXIv2023,PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer,No.,1,"""No evidence""",2023,2023-05-03T20:30:51Z,,,
arXIv2023,Defending against Insertion-based Textual Backdoor Attacks via Attribution,No.,1,"""No evidence""",2023,2023-05-03T19:29:26Z,,,
arXIv2023,"A Novel Plagiarism Detection Approach Combining BERT-based Word Embedding, Attention-based LSTMs and an Improved Differential Evolution Algorithm",No.,1,"""No evidence""",2023,2023-05-03T18:26:47Z,,,
arXIv2023,Using Language Models on Low-end Hardware,No.,1,"""No evidence""",2023,2023-05-03T18:00:03Z,,,
arXIv2023,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,No.,1,"""No evidence""",2023,2023-05-03T17:55:25Z,,,
arXIv2023,Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages,No.,1,"""No evidence""",2023,2023-05-03T15:52:17Z,,,
arXIv2023,Zero-Shot Listwise Document Reranking with a Large Language Model,No.,1,"""No evidence""",2023,2023-05-03T14:45:34Z,,,
arXIv2023,Gym-preCICE: Reinforcement Learning Environments for Active Flow Control,No.,1,"""No evidence""",2023,2023-05-03T10:54:56Z,,,
arXIv2023,A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training,No.,1,"""No evidence""",2023,2023-05-03T10:49:38Z,,,
arXIv2023,Exploring the Protein Sequence Space with Global Generative Models,No.,1,"""No evidence""",2023,2023-05-03T07:45:29Z,,,
arXIv2023,Robust Multi-bit Natural Language Watermarking through Invariant Features,No.,1,"""No evidence""",2023,2023-05-03T05:37:30Z,,,
arXIv2023,Few-shot Event Detection: An Empirical Study and a Unified View,No.,1,"""No evidence""",2023,2023-05-03T05:31:48Z,,,
arXIv2023,GPTutor: a ChatGPT-powered programming tool for code explanation,No.,1,"""No evidence""",2023,2023-05-03T02:30:13Z,,,
arXIv2023,Towards Imperceptible Document Manipulations against Neural Ranking Models,No.,1,"""No evidence""",2023,2023-05-03T02:09:29Z,,,
arXIv2023,Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information,No.,1,"""No evidence""",2023,2023-05-02T21:33:10Z,,,
arXIv2023,The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers,No.,1,"""No evidence""",2023,2023-05-02T17:42:37Z,,,
arXIv2023,FreeLM: Fine-Tuning-Free Language Model,No.,1,"""No evidence""",2023,2023-05-02T17:17:56Z,,,
arXIv2023,AutoColor: Learned Light Power Control for Multi-Color Holograms,No.,1,"""No evidence""",2023,2023-05-02T17:14:03Z,,,
arXIv2023,From Words to Code: Harnessing Data for Program Synthesis from Natural Language,No.,1,"""No evidence""",2023,2023-05-02T16:56:32Z,,,
arXIv2023,Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise,No.,1,"""No evidence""",2023,2023-05-02T16:28:10Z,,,
arXIv2023,How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?,No.,1,"""No evidence""",2023,2023-05-02T15:55:41Z,,,
arXIv2023,Accelerating Neural Self-Improvement via Bootstrapping,No.,1,"""No evidence""",2023,2023-05-02T15:52:34Z,,,
arXIv2023,Improving Cancer Hallmark Classification with BERT-based Deep Learning Approach,No.,1,"""No evidence""",2023,2023-05-02T09:57:54Z,,,
arXIv2023,ADVISE: AI-accelerated Design of Evidence Synthesis for Global Development,No.,1,"""No evidence""",2023,2023-05-02T01:29:53Z,,,
arXIv2023,Logion: Machine Learning for Greek Philology,No.,1,"""No evidence""",2023,2023-05-01T21:56:25Z,,,
arXIv2023,SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory Text: Comparison of Direct Training vs Aggregation,No.,1,"""No evidence""",2023,2023-05-01T19:30:32Z,,,
arXIv2023,CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation,No.,1,"""No evidence""",2023,2023-05-01T19:01:01Z,,,
arXIv2023,Automated Paper Screening for Clinical Reviews Using Large Language Models,No.,1,"""No evidence""",2023,2023-05-01T14:16:37Z,,,
arXIv2023,An Iterative Algorithm for Rescaled Hyperbolic Functions Regression,No.,1,"""No evidence""",2023,2023-05-01T05:16:07Z,,,
arXIv2023,How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model,No.,1,"""No evidence""",2023,2023-04-30T21:44:21Z,,,
arXIv2023,Working Memory Capacity of ChatGPT: An Empirical Study,No.,1,"""No evidence""",2023,2023-04-30T11:54:40Z,,,
arXIv2023,SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support,No.,1,"""No evidence""",2023,2023-04-30T11:26:10Z,,,
arXIv2023,NewsPanda: Media Monitoring for Timely Conservation Action,No.,1,"""No evidence""",2023,2023-04-30T07:15:29Z,,,
arXIv2023,Can ChatGPT Pass An Introductory Level Functional Language Programming Course?,No.,1,"""No evidence""",2023,2023-04-29T20:30:32Z,,,
arXIv2023,"Students' Voices on Generative AI: Perceptions, Benefits, and Challenges in Higher Education",No.,1,"""No evidence""",2023,2023-04-29T15:53:38Z,,,
arXIv2023,"A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions",No.,1,"""No evidence""",2023,2023-04-29T11:25:43Z,,,
arXIv2023,Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT,No.,1,"""No evidence""",2023,2023-04-29T08:59:12Z,,,
arXIv2023,"Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",No.,1,"""No evidence""",2023,2023-04-28T22:35:03Z,,,
arXIv2023,NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis,No.,1,"""No evidence""",2023,2023-04-28T21:02:58Z,,,
arXIv2023,Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning Framework that Supports Diverse Compositional Reasoning,No.,1,"""No evidence""",2023,2023-04-28T19:27:26Z,,,
arXIv2023,Hedonic Prices and Quality Adjusted Price Indices Powered by AI,No.,1,"""No evidence""",2023,2023-04-28T18:37:59Z,,,
arXIv2023,Are Emergent Abilities of Large Language Models a Mirage?,No.,1,"""No evidence""",2023,2023-04-28T17:52:11Z,,,
arXIv2023,Towards Automated Circuit Discovery for Mechanistic Interpretability,No.,1,"""No evidence""",2023,2023-04-28T17:36:53Z,,,
arXIv2023,ChatGPT in the Classroom: An Analysis of Its Strengths and Weaknesses for Solving Undergraduate Computer Science Questions,No.,1,"""No evidence""",2023,2023-04-28T17:26:32Z,,,
arXIv2023,MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks,No.,1,"""No evidence""",2023,2023-04-28T17:03:57Z,,,
arXIv2023,CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data,No.,1,"""No evidence""",2023,2023-04-28T16:12:18Z,,,
arXIv2023,ResiDual: Transformer with Dual Residual Connections,No.,1,"""No evidence""",2023,2023-04-28T12:19:47Z,,,
arXIv2023,Training and Evaluation of a Multilingual Tokenizer for GPT-SW3,No.,1,"""No evidence""",2023,2023-04-28T11:40:48Z,,,
arXIv2023,FlowTransformer: A Transformer Framework for Flow-based Network Intrusion Detection Systems,No.,1,"""No evidence""",2023,2023-04-28T10:40:34Z,,,
arXIv2023,Prompt Engineering for Healthcare: Methodologies and Applications,No.,1,"""No evidence""",2023,2023-04-28T08:03:42Z,,,
arXIv2023,Deep Intellectual Property Protection: A Survey,No.,1,"""No evidence""",2023,2023-04-28T03:34:43Z,,,
arXIv2023,"Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation",No.,1,"""No evidence""",2023,2023-04-28T01:47:09Z,,,
arXIv2023,Appropriateness is all you need!,No.,1,"""No evidence""",2023,2023-04-27T22:21:52Z,,,
arXIv2023,Adversarial Policy Optimization in Deep Reinforcement Learning,No.,1,"""No evidence""",2023,2023-04-27T21:01:08Z,,,
arXIv2023,Multivariate Representation Learning for Information Retrieval,No.,1,"""No evidence""",2023,2023-04-27T20:30:46Z,,,
arXIv2023,pyBibX -- A Python Library for Bibliometric and Scientometric Analysis Powered with Artificial Intelligence Tools,No.,1,"""No evidence""",2023,2023-04-27T20:06:07Z,,,
arXIv2023,ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger,No.,1,"""No evidence""",2023,2023-04-27T19:26:25Z,,,
arXIv2023,Framing the News:From Human Perception to Large Language Model Inferences,No.,1,"""No evidence""",2023,2023-04-27T18:30:18Z,,,
arXIv2023,Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement,No.,1,"""No evidence""",2023,2023-04-27T17:55:13Z,,,
arXIv2023,ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT,No.,1,"""No evidence""",2023,2023-04-27T17:07:29Z,,,
arXIv2023,"Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space",No.,1,"""No evidence""",2023,2023-04-27T17:06:20Z,,,
arXIv2023,BCQQ: Batch-Constraint Quantum Q-Learning with Cyclic Data Re-uploading,No.,1,"""No evidence""",2023,2023-04-27T16:43:01Z,,,
arXIv2023,q2d: Turning Questions into Dialogs to Teach Models How to Search,No.,1,"""No evidence""",2023,2023-04-27T16:39:15Z,,,
arXIv2023,Large Language Models are Strong Zero-Shot Retriever,No.,1,"""No evidence""",2023,2023-04-27T14:45:55Z,,,
arXIv2023,A Modular Approach for Multilingual Timex Detection and Normalization using Deep Learning and Grammar-based methods,No.,1,"""No evidence""",2023,2023-04-27T14:32:23Z,,,
arXIv2023,UIO at SemEval-2023 Task 12: Multilingual fine-tuning for sentiment classification in low-resource languages,No.,1,"""No evidence""",2023,2023-04-27T13:51:18Z,,,
arXIv2023,mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality,No.,1,"""No evidence""",2023,2023-04-27T13:27:01Z,,,
arXIv2023,ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task,No.,1,"""No evidence""",2023,2023-04-27T13:25:43Z,,,
arXIv2023,DataComp: In search of the next generation of multimodal datasets,No.,1,"""No evidence""",2023,2023-04-27T11:37:18Z,,,
arXIv2023,ChatLog: Recording and Analyzing ChatGPT Across Time,No.,1,"""No evidence""",2023,2023-04-27T11:33:48Z,,,
arXIv2023,Learning Human-Human Interactions in Images from Weak Textual Supervision,No.,1,"""No evidence""",2023,2023-04-27T11:32:48Z,,,
arXIv2023,SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish,No.,1,"""No evidence""",2023,2023-04-27T07:32:37Z,,,
arXIv2023,Extracting Structured Seed-Mediated Gold Nanorod Growth Procedures from Literature with GPT-3,No.,1,"""No evidence""",2023,2023-04-26T22:21:33Z,,,
arXIv2023,Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models,No.,1,"""No evidence""",2023,2023-04-26T21:41:17Z,,,
arXIv2023,Towards ethical multimodal systems,No.,1,"""No evidence""",2023,2023-04-26T18:11:33Z,,,
arXIv2023,HausaNLP at SemEval-2023 Task 12: Leveraging African Low Resource TweetData for Sentiment Analysis,No.,1,"""No evidence""",2023,2023-04-26T15:47:50Z,,,
arXIv2023,Technical Report: Impact of Position Bias on Language Models in Token Classification,No.,1,"""No evidence""",2023,2023-04-26T13:57:25Z,,,
arXIv2023,Multidimensional Evaluation for Text Style Transfer Using ChatGPT,No.,1,"""No evidence""",2023,2023-04-26T11:33:35Z,,,
arXIv2023,Zero-Shot Slot and Intent Detection in Low-Resource Languages,No.,1,"""No evidence""",2023,2023-04-26T05:10:12Z,,,
arXIv2023,Exploring the Curious Case of Code Prompts,No.,1,"""No evidence""",2023,2023-04-26T02:37:52Z,,,
arXIv2023,Hypernymization of named entity-rich captions for grounding-based multi-modal pretraining,No.,1,"""No evidence""",2023,2023-04-25T20:17:40Z,,,
arXIv2023,Injecting structural hints: Using language models to study inductive biases in language learning,No.,1,"""No evidence""",2023,2023-04-25T18:00:08Z,,,
arXIv2023,Stable and low-precision training for large-scale vision-language models,No.,1,"""No evidence""",2023,2023-04-25T17:38:18Z,,,
arXIv2023,Unstructured and structured data: Can we have the best of both worlds with large language models?,No.,1,"""No evidence""",2023,2023-04-25T17:30:05Z,,,
arXIv2023,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,No.,1,"""No evidence""",2023,2023-04-25T17:27:37Z,,,
arXIv2023,Measuring Massive Multitask Chinese Understanding,No.,1,"""No evidence""",2023,2023-04-25T16:51:53Z,,,
arXIv2023,GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters,No.,1,"""No evidence""",2023,2023-04-25T16:39:51Z,,,
arXIv2023,Escaping the sentence-level paradigm in machine translation,No.,1,"""No evidence""",2023,2023-04-25T16:09:02Z,,,
arXIv2023,A Closer Look at Reward Decomposition for High-Level Robotic Explanations,No.,1,"""No evidence""",2023,2023-04-25T16:01:42Z,,,
arXIv2023,Blockchain Large Language Models,No.,1,"""No evidence""",2023,2023-04-25T11:56:18Z,,,
arXIv2023,CitePrompt: Using Prompts to Identify Citation Intent in Scientific Papers,No.,1,"""No evidence""",2023,2023-04-25T11:19:52Z,,,
arXIv2023,What does BERT learn about prosody?,No.,1,"""No evidence""",2023,2023-04-25T10:34:56Z,,,
arXIv2023,Compressing Sentence Representation with maximum Coding Rate Reduction,No.,1,"""No evidence""",2023,2023-04-25T09:23:43Z,,,
arXIv2023,KINLP at SemEval-2023 Task 12: Kinyarwanda Tweet Sentiment Analysis,No.,1,"""No evidence""",2023,2023-04-25T04:30:03Z,,,
arXIv2023,AGI: Artificial General Intelligence for Education,No.,1,"""No evidence""",2023,2023-04-24T22:31:59Z,,,
arXIv2023,Understanding and Predicting Human Label Variation in Natural Language Inference through Explanation,No.,1,"""No evidence""",2023,2023-04-24T20:45:09Z,,,
arXIv2023,Semantic Tokenizer for Enhanced Natural Language Processing,No.,1,"""No evidence""",2023,2023-04-24T19:33:41Z,,,
arXIv2023,Better Question-Answering Models on a Budget,No.,1,"""No evidence""",2023,2023-04-24T18:06:27Z,,,
arXIv2023,Enriching Source Code with Contextual Data for Code Completion Models: An Empirical Study,No.,1,"""No evidence""",2023,2023-04-24T17:09:14Z,,,
arXIv2023,"AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays",No.,1,"""No evidence""",2023,2023-04-24T12:58:28Z,,,
arXIv2023,SocialDial: A Benchmark for Socially-Aware Dialogue Systems,No.,1,"""No evidence""",2023,2023-04-24T11:55:22Z,,,
arXIv2023,CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts,No.,1,"""No evidence""",2023,2023-04-24T11:19:33Z,,,
arXIv2023,"Pre-trained Embeddings for Entity Resolution: An Experimental Analysis [Experiment, Analysis & Benchmark]",No.,1,"""No evidence""",2023,2023-04-24T08:53:54Z,,,
arXIv2023,"ChatLLM Network: More brains, More intelligence",No.,1,"""No evidence""",2023,2023-04-24T08:29:14Z,,,
arXIv2023,Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model,No.,1,"""No evidence""",2023,2023-04-24T07:45:28Z,,,
arXIv2023,System III: Learning with Domain Knowledge for Safety Constraints,No.,1,"""No evidence""",2023,2023-04-23T09:44:41Z,,,
arXIv2023,Epistemic considerations when AI answers questions for us,No.,1,"""No evidence""",2023,2023-04-23T08:26:42Z,,,
arXIv2023,Processing Natural Language on Embedded Devices: How Well Do Transformer Models Perform?,No.,1,"""No evidence""",2023,2023-04-23T03:01:39Z,,,
arXIv2023,Recurrent Neural Networks and Long Short-Term Memory Networks: Tutorial and Survey,No.,1,"""No evidence""",2023,2023-04-22T18:22:10Z,,,
arXIv2023,L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT,No.,1,"""No evidence""",2023,2023-04-22T15:45:40Z,,,
arXIv2023,Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism,No.,1,"""No evidence""",2023,2023-04-22T14:09:14Z,,,
arXIv2023,LaMP: When Large Language Models Meet Personalization,No.,1,"""No evidence""",2023,2023-04-22T13:42:04Z,,,
arXIv2023,Semantic Specialization for Knowledge-based Word Sense Disambiguation,No.,1,"""No evidence""",2023,2023-04-22T07:40:23Z,,,
arXIv2023,Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs,No.,1,"""No evidence""",2023,2023-04-22T06:28:46Z,,,
arXIv2023,The Role of AI in Human-AI Creative Writing for Hong Kong Secondary Students,No.,1,"""No evidence""",2023,2023-04-21T23:50:09Z,,,
arXIv2023,"Generative AI Perceptions: A Survey to Measure the Perceptions of Faculty, Staff, and Students on Generative AI Tools in Academia",No.,1,"""No evidence""",2023,2023-04-21T23:08:39Z,,,
arXIv2023,"ChatGPT, Large Language Technologies, and the Bumpy Road of Benefiting Humanity",No.,1,"""No evidence""",2023,2023-04-21T22:53:45Z,,,
arXIv2023,Spatial-Language Attention Policies for Efficient Robot Learning,No.,1,"""No evidence""",2023,2023-04-21T20:02:49Z,,,
arXIv2023,Building Multimodal AI Chatbots,No.,1,"""No evidence""",2023,2023-04-21T16:43:54Z,,,
arXIv2023,Robot-Enabled Construction Assembly with Automated Sequence Planning based on ChatGPT: RoboGPT,No.,1,"""No evidence""",2023,2023-04-21T15:04:41Z,,,
arXIv2023,BERT Based Clinical Knowledge Extraction for Biomedical Knowledge Graph Construction and Analysis,No.,1,"""No evidence""",2023,2023-04-21T14:45:33Z,,,
arXIv2023,Can GPT-4 Perform Neural Architecture Search?,No.,1,"""No evidence""",2023,2023-04-21T14:06:44Z,,,
arXIv2023,Text2Time: Transformer-based Article Time Period Prediction,No.,1,"""No evidence""",2023,2023-04-21T10:05:03Z,,,
arXIv2023,Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams,No.,1,"""No evidence""",2023,2023-04-21T04:29:05Z,,,
arXIv2023,KitchenScale: Learning to predict ingredient quantities from recipe contexts,No.,1,"""No evidence""",2023,2023-04-21T04:28:16Z,,,
arXIv2023,Word Sense Induction with Knowledge Distillation from BERT,No.,1,"""No evidence""",2023,2023-04-20T21:05:35Z,,,
arXIv2023,Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models,No.,1,"""No evidence""",2023,2023-04-20T18:39:41Z,,,
arXIv2023,"""Can We Detect Substance Use Disorder?"": Knowledge and Time Aware Classification on Social Media from Darkweb",No.,1,"""No evidence""",2023,2023-04-20T17:47:13Z,,,
arXIv2023,Efficient Deep Reinforcement Learning Requires Regulating Overfitting,No.,1,"""No evidence""",2023,2023-04-20T17:11:05Z,,,
arXIv2023,Performance of ChatGPT on the US Fundamentals of Engineering Exam: Comprehensive Assessment of Proficiency and Potential Implications for Professional Environmental Engineering Practice,No.,1,"""No evidence""",2023,2023-04-20T16:54:34Z,,,
arXIv2023,Phoenix: Democratizing ChatGPT across Languages,No.,1,"""No evidence""",2023,2023-04-20T16:50:04Z,,,
arXIv2023,On the Potential of Artificial Intelligence Chatbots for Data Exploration of Federated Bioinformatics Knowledge Graphs,No.,1,"""No evidence""",2023,2023-04-20T16:16:40Z,,,
arXIv2023,SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation,No.,1,"""No evidence""",2023,2023-04-20T16:01:55Z,,,
arXIv2023,Attention Scheme Inspired Softmax Regression,No.,1,"""No evidence""",2023,2023-04-20T15:50:35Z,,,
arXIv2023,Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects,No.,1,"""No evidence""",2023,2023-04-20T14:42:54Z,,,
arXIv2023,CEIL: A General Classification-Enhanced Iterative Learning Framework for Text Clustering,No.,1,"""No evidence""",2023,2023-04-20T14:04:31Z,,,
arXIv2023,Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining,No.,1,"""No evidence""",2023,2023-04-20T13:42:27Z,,,
arXIv2023,Indian Sign Language Recognition Using Mediapipe Holistic,No.,1,"""No evidence""",2023,2023-04-20T12:25:47Z,,,
arXIv2023,Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs,No.,1,"""No evidence""",2023,2023-04-20T10:45:57Z,,,
arXIv2023,Is Cross-modal Information Retrieval Possible without Training?,No.,1,"""No evidence""",2023,2023-04-20T02:36:18Z,,,
arXIv2023,MasakhaNEWS: News Topic Classification for African languages,No.,1,"""No evidence""",2023,2023-04-19T21:12:23Z,,,
arXIv2023,A Latent Space Theory for Emergent Abilities in Large Language Models,No.,1,"""No evidence""",2023,2023-04-19T20:45:01Z,,,
arXIv2023,Progressive-Hint Prompting Improves Reasoning in Large Language Models,No.,1,"""No evidence""",2023,2023-04-19T16:29:48Z,,,
arXIv2023,BRENT: Bidirectional Retrieval Enhanced Norwegian Transformer,No.,1,"""No evidence""",2023,2023-04-19T13:40:47Z,,,
arXIv2023,ChatGPT as a Therapist Assistant: A Suitability Study,No.,1,"""No evidence""",2023,2023-04-19T13:35:23Z,,,
arXIv2023,CB-Conformer: Contextual biasing Conformer for biased word recognition,No.,1,"""No evidence""",2023,2023-04-19T12:26:04Z,,,
arXIv2023,Is ChatGPT Equipped with Emotional Dialogue Capabilities?,No.,1,"""No evidence""",2023,2023-04-19T11:42:40Z,,,
arXIv2023,Conversational Process Modeling: Can Generative AI Empower Domain Experts in Creating and Redesigning Process Models?,No.,1,"""No evidence""",2023,2023-04-19T06:54:14Z,,,
arXIv2023,EC^2: Emergent Communication for Embodied Control,No.,1,"""No evidence""",2023,2023-04-19T06:36:02Z,,,
arXIv2023,How to Do Things with Deep Learning Code,No.,1,"""No evidence""",2023,2023-04-19T03:46:12Z,,,
arXIv2023,Shuffle & Divide: Contrastive Learning for Long Text,No.,1,"""No evidence""",2023,2023-04-19T02:02:29Z,,,
arXIv2023,The Unintended Consequences of Censoring Digital Technology -- Evidence from Italy's ChatGPT Ban,No.,1,"""No evidence""",2023,2023-04-18T23:11:04Z,,,
arXIv2023,Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models,No.,1,"""No evidence""",2023,2023-04-18T22:59:11Z,,,
arXIv2023,BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval,No.,1,"""No evidence""",2023,2023-04-18T22:46:02Z,,,
arXIv2023,UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining,No.,1,"""No evidence""",2023,2023-04-18T17:45:50Z,,,
arXIv2023,LLM-based Interaction for Content Generation: A Case Study on the Perception of Employees in an IT department,No.,1,"""No evidence""",2023,2023-04-18T15:35:43Z,,,
arXIv2023,CodeKGC: Code Language Model for Generative Knowledge Graph Construction,No.,1,"""No evidence""",2023,2023-04-18T15:12:34Z,,,
arXIv2023,Revisiting the Role of Similarity and Dissimilarity in Best Counter Argument Retrieval,No.,1,"""No evidence""",2023,2023-04-18T08:13:48Z,,,
arXIv2023,HeRo: RoBERTa and Longformer Hebrew Language Models,No.,1,"""No evidence""",2023,2023-04-18T05:56:32Z,,,
arXIv2023,Generative Disco: Text-to-Video Generation for Music Visualization,No.,1,"""No evidence""",2023,2023-04-17T18:44:00Z,,,
arXIv2023,Pretrained Language Models as Visual Planners for Human Assistance,No.,1,"""No evidence""",2023,2023-04-17T18:07:36Z,,,
arXIv2023,Learning to Compress Prompts with Gist Tokens,No.,1,"""No evidence""",2023,2023-04-17T17:47:37Z,,,
arXIv2023,The MiniPile Challenge for Data-Efficient Language Models,No.,1,"""No evidence""",2023,2023-04-17T17:03:56Z,,,
arXIv2023,Multimodal Short Video Rumor Detection System Based on Contrastive Learning,No.,1,"""No evidence""",2023,2023-04-17T16:07:00Z,,,
arXIv2023,New Product Development (NPD) through Social Media-based Analysis by Comparing Word2Vec and BERT Word Embeddings,No.,1,"""No evidence""",2023,2023-04-17T15:32:11Z,,,
arXIv2023,VECO 2.0: Cross-lingual Language Model Pre-training with Multi-granularity Contrastive Learning,No.,1,"""No evidence""",2023,2023-04-17T12:23:41Z,,,
arXIv2023,Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca,No.,1,"""No evidence""",2023,2023-04-17T11:39:53Z,,,
arXIv2023,Political corpus creation through automatic speech recognition on EU debates,No.,1,"""No evidence""",2023,2023-04-17T10:41:59Z,,,
arXIv2023,An Empirical Study of Multitask Learning to Improve Open Domain Dialogue Systems,No.,1,"""No evidence""",2023,2023-04-17T09:44:56Z,,,
arXIv2023,A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model,No.,1,"""No evidence""",2023,2023-04-17T09:36:36Z,,,
arXIv2023,From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning,No.,1,"""No evidence""",2023,2023-04-17T05:29:42Z,,,
arXIv2023,Chinese Open Instruction Generalist: A Preliminary Release,No.,1,"""No evidence""",2023,2023-04-17T04:45:06Z,,,
arXIv2023,Learning To Rank Resources with GNN,No.,1,"""No evidence""",2023,2023-04-17T02:01:45Z,,,
arXIv2023,Exploring the Use of ChatGPT as a Tool for Learning and Assessment in Undergraduate Computer Science Curriculum: Opportunities and Challenges,No.,1,"""No evidence""",2023,2023-04-16T21:04:52Z,,,
arXIv2023,Sabi: Portuguese Large Language Models,No.,1,"""No evidence""",2023,2023-04-16T20:11:19Z,,,
arXIv2023,Neural Machine Translation For Low Resource Languages,No.,1,"""No evidence""",2023,2023-04-16T19:27:48Z,,,
arXIv2023,Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation,No.,1,"""No evidence""",2023,2023-04-16T18:37:39Z,,,
arXIv2023,A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers,No.,1,"""No evidence""",2023,2023-04-16T17:45:21Z,,,
arXIv2023,Enhancing Automated Program Repair through Fine-tuning and Prompt Engineering,No.,1,"""No evidence""",2023,2023-04-16T17:29:51Z,,,
arXIv2023,The language of sounds unheard: Exploring musical timbre semantics of large language models,No.,1,"""No evidence""",2023,2023-04-16T16:50:25Z,,,
arXIv2023,SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities,No.,1,"""No evidence""",2023,2023-04-16T13:25:24Z,,,
arXIv2023,"ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models",No.,1,"""No evidence""",2023,2023-04-16T01:50:26Z,,,
arXIv2023,Interpretable Detection of Out-of-Context Misinformation with Neural-Symbolic-Enhanced Large Multimodal Model,No.,1,"""No evidence""",2023,2023-04-15T21:11:55Z,,,
arXIv2023,Analyzing the Performance of ChatGPT in Cardiology and Vascular Pathologies,No.,1,"""No evidence""",2023,2023-04-15T20:08:48Z,,,
arXIv2023,Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models,No.,1,"""No evidence""",2023,2023-04-15T19:22:37Z,,,
arXIv2023,A CTC Alignment-based Non-autoregressive Transformer for End-to-end Automatic Speech Recognition,No.,1,"""No evidence""",2023,2023-04-15T18:34:29Z,,,
arXIv2023,Zero-Shot Multi-Label Topic Inference with Sentence Encoders,No.,1,"""No evidence""",2023,2023-04-14T20:27:09Z,,,
arXIv2023,"ChatGPT: Applications, Opportunities, and Threats",No.,1,"""No evidence""",2023,2023-04-14T16:25:03Z,,,
arXIv2023,Model Predictive Control with Self-supervised Representation Learning,No.,1,"""No evidence""",2023,2023-04-14T16:02:04Z,,,
arXIv2023,Just Tell Me: Prompt Engineering in Business Process Management,No.,1,"""No evidence""",2023,2023-04-14T14:55:19Z,,,
arXIv2023,OPI at SemEval 2023 Task 9: A Simple But Effective Approach to Multilingual Tweet Intimacy Analysis,No.,1,"""No evidence""",2023,2023-04-14T13:49:28Z,,,
arXIv2023,SimpLex: a lexical text simplification architecture,No.,1,"""No evidence""",2023,2023-04-14T08:52:31Z,,,
arXIv2023,Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning,No.,1,"""No evidence""",2023,2023-04-14T07:07:42Z,,,
arXIv2023,The Future of ChatGPT-enabled Labor Market: A Preliminary Study in China,No.,1,"""No evidence""",2023,2023-04-14T06:56:35Z,,,
arXIv2023,HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition,No.,1,"""No evidence""",2023,2023-04-14T03:25:00Z,,,
arXIv2023,Automated Mapping of CVE Vulnerability Records to MITRE CWE Weaknesses,No.,1,"""No evidence""",2023,2023-04-13T19:46:54Z,,,
arXIv2023,"ChatGPT cites the most-cited articles and journals, relying solely on Google Scholar's citation counts. As a result, AI may amplify the Matthew Effect in environmental science",No.,1,"""No evidence""",2023,2023-04-13T19:29:49Z,,,
arXIv2023,RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment,No.,1,"""No evidence""",2023,2023-04-13T18:22:40Z,,,
arXIv2023,Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study,No.,1,"""No evidence""",2023,2023-04-13T18:04:19Z,,,
arXIv2023,Segment Everything Everywhere All at Once,No.,1,"""No evidence""",2023,2023-04-13T17:59:40Z,,,
arXIv2023,How Useful are Educational Questions Generated by Large Language Models?,No.,1,"""No evidence""",2023,2023-04-13T16:05:25Z,,,
arXIv2023,A-CAP: Anticipation Captioning with Commonsense Knowledge,No.,1,"""No evidence""",2023,2023-04-13T15:10:47Z,,,
arXIv2023,Solving Tensor Low Cycle Rank Approximation,No.,1,"""No evidence""",2023,2023-04-13T15:00:50Z,,,
arXIv2023,ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning,No.,1,"""No evidence""",2023,2023-04-13T14:51:40Z,,,
arXIv2023,Priors for symbolic regression,No.,1,"""No evidence""",2023,2023-04-13T08:29:16Z,,,
arXIv2023,LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model,No.,1,"""No evidence""",2023,2023-04-13T04:01:14Z,,,
arXIv2023,[CLS] Token is All You Need for Zero-Shot Semantic Segmentation,No.,1,"""No evidence""",2023,2023-04-13T01:35:07Z,,,
arXIv2023,Using Large Language Models for (De-)Formalization and Natural Argumentation Exercises for Beginner's Students,No.,1,"""No evidence""",2023,2023-04-12T23:05:02Z,,,
arXIv2023,Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model,No.,1,"""No evidence""",2023,2023-04-12T22:21:36Z,,,
arXIv2023,RECLIP: Resource-efficient CLIP by Training with Small Images,No.,1,"""No evidence""",2023,2023-04-12T17:59:58Z,,,
arXIv2023,Evaluation of ChatGPT Model for Vulnerability Detection,No.,1,"""No evidence""",2023,2023-04-12T17:24:03Z,,,
arXIv2023,HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting,No.,1,"""No evidence""",2023,2023-04-12T16:54:26Z,,,
arXIv2023,Boosted Prompt Ensembles for Large Language Models,No.,1,"""No evidence""",2023,2023-04-12T16:47:15Z,,,
arXIv2023,Localizing Model Behavior with Path Patching,No.,1,"""No evidence""",2023,2023-04-12T16:46:43Z,,,
arXIv2023,Using Multiple RDF Knowledge Graphs for Enriching ChatGPT Responses,No.,1,"""No evidence""",2023,2023-04-12T11:33:00Z,,,
arXIv2023,Semantic Feature Verification in FLAN-T5,No.,1,"""No evidence""",2023,2023-04-12T03:37:57Z,,,
arXIv2023,Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature,No.,1,"""No evidence""",2023,2023-04-12T03:02:20Z,,,
arXIv2023,ChatGPT is all you need to decolonize sub-Saharan Vocational Education,No.,1,"""No evidence""",2023,2023-04-11T23:50:37Z,,,
arXIv2023,"Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis",No.,1,"""No evidence""",2023,2023-04-11T23:29:56Z,,,
arXIv2023,Exact and Cost-Effective Automated Transformation of Neural Network Controllers to Decision Tree Controllers,No.,1,"""No evidence""",2023,2023-04-11T19:52:30Z,,,
arXIv2023,Bayesian Optimization of Catalysts With In-context Learning,No.,1,"""No evidence""",2023,2023-04-11T17:00:35Z,,,
arXIv2023,Exploring the Use of Foundation Models for Named Entity Recognition and Lemmatization Tasks in Slavic Languages,No.,1,"""No evidence""",2023,2023-04-11T16:55:11Z,,,
arXIv2023,Emergent autonomous scientific research capabilities of large language models,No.,1,"""No evidence""",2023,2023-04-11T16:50:17Z,,,
arXIv2023,RRHF: Rank Responses to Align Language Models with Human Feedback without tears,No.,1,"""No evidence""",2023,2023-04-11T15:53:40Z,,,
arXIv2023,Prompt Learning for News Recommendation,No.,1,"""No evidence""",2023,2023-04-11T14:56:06Z,,,
arXIv2023,r-softmax: Generalized Softmax with Controllable Sparsity Rate,No.,1,"""No evidence""",2023,2023-04-11T14:28:29Z,,,
arXIv2023,Boosting Cross-task Transferability of Adversarial Patches with Visual Relations,No.,1,"""No evidence""",2023,2023-04-11T11:43:57Z,,,
arXIv2023,Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT,No.,1,"""No evidence""",2023,2023-04-11T01:17:11Z,,,
arXIv2023,Incorporating Structured Sentences with Time-enhanced BERT for Fully-inductive Temporal Relation Prediction,No.,1,"""No evidence""",2023,2023-04-10T17:22:15Z,,,
arXIv2023,Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection,No.,1,"""No evidence""",2023,2023-04-10T16:08:59Z,,,
arXIv2023,SELFormer: Molecular Representation Learning via SELFIES Language Models,No.,1,"""No evidence""",2023,2023-04-10T15:38:25Z,,,
arXIv2023,Automated Reading Passage Generation with OpenAI's Large Language Model,No.,1,"""No evidence""",2023,2023-04-10T14:30:39Z,,,
arXIv2023,DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment,No.,1,"""No evidence""",2023,2023-04-10T11:08:15Z,,,
arXIv2023,Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension,No.,1,"""No evidence""",2023,2023-04-10T05:52:38Z,,,
arXIv2023,Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study,No.,1,"""No evidence""",2023,2023-04-10T00:55:59Z,,,
arXIv2023,CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model,No.,1,"""No evidence""",2023,2023-04-09T12:56:54Z,,,
arXIv2023,Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions,No.,1,"""No evidence""",2023,2023-04-09T12:46:18Z,,,
arXIv2023,Extractive Summarization via ChatGPT for Faithful Summary Generation,No.,1,"""No evidence""",2023,2023-04-09T08:26:04Z,,,
arXIv2023,Similarity-Aware Multimodal Prompt Learning for Fake News Detection,No.,1,"""No evidence""",2023,2023-04-09T08:10:05Z,,,
arXIv2023,Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning,No.,1,"""No evidence""",2023,2023-04-08T19:28:26Z,,,
arXIv2023,Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder,No.,1,"""No evidence""",2023,2023-04-08T15:44:29Z,,,
arXIv2023,FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement,No.,1,"""No evidence""",2023,2023-04-08T07:34:26Z,,,
arXIv2023,Factify 2: A Multimodal Fake News and Satire News Dataset,No.,1,"""No evidence""",2023,2023-04-08T03:14:19Z,,,
arXIv2023,Towards Automated Urban Planning: When Generative and ChatGPT-like AI Meets Urban Planning,No.,1,"""No evidence""",2023,2023-04-08T02:19:59Z,,,
arXIv2023,Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic Data,No.,1,"""No evidence""",2023,2023-04-07T16:38:40Z,,,
arXIv2023,Generative AI for learning: Investigating the potential of synthetic learning videos,No.,1,"""No evidence""",2023,2023-04-07T12:57:42Z,,,
arXIv2023,Language-aware Multiple Datasets Detection Pretraining for DETRs,No.,1,"""No evidence""",2023,2023-04-07T10:34:04Z,,,
arXIv2023,From Retrieval to Generation: Efficient and Effective Entity Set Expansion,No.,1,"""No evidence""",2023,2023-04-07T08:09:50Z,,,
arXIv2023,SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers,No.,1,"""No evidence""",2023,2023-04-07T07:24:32Z,,,
arXIv2023,Hierarchical Catalogue Generation for Literature Review: A Benchmark,No.,1,"""No evidence""",2023,2023-04-07T07:13:35Z,,,
arXIv2023,Deep Learning for Opinion Mining and Topic Classification of Course Reviews,No.,1,"""No evidence""",2023,2023-04-06T21:48:29Z,,,
arXIv2023,ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about,No.,1,"""No evidence""",2023,2023-04-06T18:42:47Z,,,
arXIv2023,"Making AI Less ""Thirsty"": Uncovering and Addressing the Secret Water Footprint of AI Models",No.,1,"""No evidence""",2023,2023-04-06T17:55:27Z,,,
arXIv2023,Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster,No.,1,"""No evidence""",2023,2023-04-06T16:43:16Z,,,
arXIv2023,Micron-BERT: BERT-based Facial Micro-Expression Recognition,No.,1,"""No evidence""",2023,2023-04-06T16:19:09Z,,,
arXIv2023,"ChatGPT: More than a Weapon of Mass Deception, Ethical challenges and responses from the Human-Centered Artificial Intelligence (HCAI) perspective",No.,1,"""No evidence""",2023,2023-04-06T07:40:12Z,,,
arXIv2023,Causal Repair of Learning-enabled Cyber-physical Systems,No.,1,"""No evidence""",2023,2023-04-06T01:26:58Z,,,
arXIv2023,Revolutionizing Single Cell Analysis: The Power of Large Language Models for Cell Type Annotation,No.,1,"""No evidence""",2023,2023-04-05T18:45:54Z,,,
arXIv2023,Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks,No.,1,"""No evidence""",2023,2023-04-05T17:47:11Z,,,
arXIv2023,Human-like Summarization Evaluation with ChatGPT,No.,1,"""No evidence""",2023,2023-04-05T16:17:32Z,,,
arXIv2023,ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback,No.,1,"""No evidence""",2023,2023-04-05T13:12:00Z,,,
arXIv2023,What's in a Name? Beyond Class Indices for Image Recognition,No.,1,"""No evidence""",2023,2023-04-05T11:01:23Z,,,
arXIv2023,"Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines",No.,1,"""No evidence""",2023,2023-04-05T09:09:10Z,,,
arXIv2023,Towards Self-Explainability of Deep Neural Networks with Heatmap Captioning and Large-Language Models,No.,1,"""No evidence""",2023,2023-04-05T03:29:37Z,,,
arXIv2023,How to Design Translation Prompts for ChatGPT: An Empirical Study,No.,1,"""No evidence""",2023,2023-04-05T01:17:59Z,,,
arXIv2023,Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model,No.,1,"""No evidence""",2023,2023-04-04T23:53:34Z,,,
arXIv2023,Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT,No.,1,"""No evidence""",2023,2023-04-04T23:19:53Z,,,
arXIv2023,Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data,No.,1,"""No evidence""",2023,2023-04-04T19:11:05Z,,,
arXIv2023,LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,No.,1,"""No evidence""",2023,2023-04-04T16:31:37Z,,,
arXIv2023,REFINER: Reasoning Feedback on Intermediate Representations,No.,1,"""No evidence""",2023,2023-04-04T15:57:28Z,,,
arXIv2023,San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants,No.,1,"""No evidence""",2023,2023-04-04T15:47:26Z,,,
arXIv2023,Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models,No.,1,"""No evidence""",2023,2023-04-04T15:01:06Z,,,
arXIv2023,Can BERT eat RuCoLA? Topological Data Analysis to Explain,No.,1,"""No evidence""",2023,2023-04-04T10:11:06Z,,,
arXIv2023,Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA,No.,1,"""No evidence""",2023,2023-04-04T07:46:40Z,,,
arXIv2023,Unsupervised Improvement of Factual Knowledge in Language Models,No.,1,"""No evidence""",2023,2023-04-04T07:37:06Z,,,
arXIv2023,"One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era",No.,1,"""No evidence""",2023,2023-04-04T06:22:09Z,,,
arXIv2023,GPT-4 to GPT-3.5: 'Hold My Scalpel' -- A Look at the Competency of OpenAI's GPT on the Plastic Surgery In-Service Training Exam,No.,1,"""No evidence""",2023,2023-04-04T03:30:12Z,,,
arXIv2023,Improved Visual Fine-tuning with Natural Language Supervision,No.,1,"""No evidence""",2023,2023-04-04T03:08:02Z,,,
arXIv2023,The Vector Grounding Problem,No.,1,"""No evidence""",2023,2023-04-04T02:54:04Z,,,
arXIv2023,TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings,No.,1,"""No evidence""",2023,2023-04-04T00:52:46Z,,,
arXIv2023,Scientists' Perspectives on the Potential for Generative AI in their Fields,No.,1,"""No evidence""",2023,2023-04-04T00:06:28Z,,,
arXIv2023,A Bibliometric Review of Large Language Models Research from 2017 to 2023,No.,1,"""No evidence""",2023,2023-04-03T21:46:41Z,,,
arXIv2023,A Simple and Effective Method of Cross-Lingual Plagiarism Detection,No.,1,"""No evidence""",2023,2023-04-03T20:27:10Z,,,
arXIv2023,Creating Custom Event Data Without Dictionaries: A Bag-of-Tricks,No.,1,"""No evidence""",2023,2023-04-03T19:51:00Z,,,
arXIv2023,Open-Vocabulary Semantic Segmentation with Decoupled One-Pass Network,No.,1,"""No evidence""",2023,2023-04-03T17:59:21Z,,,
arXIv2023,Hate Speech Targets Detection in Parler using BERT,No.,1,"""No evidence""",2023,2023-04-03T17:49:04Z,,,
arXIv2023,Generative Adversarial Neuroevolution for Control Behaviour Imitation,No.,1,"""No evidence""",2023,2023-04-03T16:33:22Z,,,
arXIv2023,Neuroevolution of Recurrent Architectures on Control Tasks,No.,1,"""No evidence""",2023,2023-04-03T16:29:18Z,,,
arXIv2023,Self-building Neural Networks,No.,1,"""No evidence""",2023,2023-04-03T15:42:28Z,,,
arXIv2023,Detection of Homophobia & Transphobia in Dravidian Languages: Exploring Deep Learning Methods,No.,1,"""No evidence""",2023,2023-04-03T12:15:27Z,,,
arXIv2023,GreekBART: The First Pretrained Greek Sequence-to-Sequence Model,No.,1,"""No evidence""",2023,2023-04-03T10:48:51Z,,,
arXIv2023,Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency Department,No.,1,"""No evidence""",2023,2023-04-03T06:32:00Z,,,
arXIv2023,Classifying COVID-19 Related Tweets for Fake News Detection and Sentiment Analysis with BERT-based Models,No.,1,"""No evidence""",2023,2023-04-02T22:00:27Z,,,
arXIv2023,Eight Things to Know about Large Language Models,No.,1,"""No evidence""",2023,2023-04-02T20:03:27Z,,,
arXIv2023,PK-Chat: Pointer Network Guided Knowledge Driven Generative Dialogue Model,No.,1,"""No evidence""",2023,2023-04-02T18:23:13Z,,,
arXIv2023,Large Language Models are Few-shot Publication Scoopers,No.,1,"""No evidence""",2023,2023-04-02T12:20:49Z,,,
arXIv2023,From Zero to Hero: Convincing with Extremely Complicated Math,No.,1,"""No evidence""",2023,2023-04-01T22:09:35Z,,,
arXIv2023,Network Visualization of ChatGPT Research: a study based on term and keyword co-occurrence network analysis,No.,1,"""No evidence""",2023,2023-04-01T06:12:20Z,,,
arXIv2023,Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning,No.,1,"""No evidence""",2023,2023-03-31T22:50:47Z,,,
arXIv2023,Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using Transformer-based Natural Language Processing Methods,No.,1,"""No evidence""",2023,2023-03-31T20:23:58Z,,,
arXIv2023,Identifying Symptoms of Delirium from Clinical Narratives Using Natural Language Processing,No.,1,"""No evidence""",2023,2023-03-31T20:16:44Z,,,
arXIv2023,"""Genlangs"" and Zipf's Law: Do languages generated by ChatGPT statistically look human?",No.,1,"""No evidence""",2023,2023-03-31T20:10:59Z,,,
arXIv2023,Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders,No.,1,"""No evidence""",2023,2023-03-31T15:44:13Z,,,
arXIv2023,Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams?,No.,1,"""No evidence""",2023,2023-03-31T15:37:17Z,,,
arXIv2023,Pair Programming with Large Language Models for Sampling and Estimation of Copulas,No.,1,"""No evidence""",2023,2023-03-31T15:02:48Z,,,
arXIv2023,Augmented Collective Intelligence in Collaborative Ideation: Agenda and Challenges,No.,1,"""No evidence""",2023,2023-03-31T12:31:29Z,,,
arXIv2023,JobHam-place with smart recommend job options and candidate filtering options,No.,1,"""No evidence""",2023,2023-03-31T09:54:47Z,,,
arXIv2023,Can AI Put Gamma-Ray Astrophysicists Out of a Job?,No.,1,"""No evidence""",2023,2023-03-31T07:29:47Z,,,
arXIv2023,Attention is Not Always What You Need: Towards Efficient Classification of Domain-Specific Text,No.,1,"""No evidence""",2023,2023-03-31T03:17:23Z,,,
arXIv2023,"CAMEL: Communicative Agents for ""Mind"" Exploration of Large Language Model Society",No.,1,"""No evidence""",2023,2023-03-31T01:09:00Z,,,
arXIv2023,Towards Mitigating ChatGPT's Negative Impact on Education: Optimizing Question Design through Bloom's Taxonomy,No.,1,"""No evidence""",2023,2023-03-31T00:01:59Z,,,
arXIv2023,Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text,No.,1,"""No evidence""",2023,2023-03-30T22:06:10Z,,,
arXIv2023,Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages,No.,1,"""No evidence""",2023,2023-03-30T19:51:18Z,,,
arXIv2023,Comparing Abstractive Summaries Generated by ChatGPT to Real Summaries Through Blinded Reviewers and Text Classification Algorithms,No.,1,"""No evidence""",2023,2023-03-30T18:28:33Z,,,
arXIv2023,Aligning a medium-size GPT model in English to a small closed domain in Spanish,No.,1,"""No evidence""",2023,2023-03-30T18:27:15Z,,,
arXIv2023,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,No.,1,"""No evidence""",2023,2023-03-30T17:48:28Z,,,
arXIv2023,Elastic Weight Removal for Faithful and Abstractive Dialogue Generation,No.,1,"""No evidence""",2023,2023-03-30T17:40:30Z,,,
arXIv2023,CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X,No.,1,"""No evidence""",2023,2023-03-30T17:34:01Z,,,
arXIv2023,BloombergGPT: A Large Language Model for Finance,No.,1,"""No evidence""",2023,2023-03-30T17:30:36Z,,,
arXIv2023,Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study,No.,1,"""No evidence""",2023,2023-03-30T15:43:39Z,,,
arXIv2023,A BERT-based Unsupervised Grammatical Error Correction Framework,No.,1,"""No evidence""",2023,2023-03-30T13:29:49Z,,,
arXIv2023,Synthesis of Mathematical programs from Natural Language Specifications,No.,1,"""No evidence""",2023,2023-03-30T06:10:00Z,,,
arXIv2023,Matrix diagonalization and singular value decomposition: Static SageMath and dynamic ChatGPT juxtaposed,No.,1,"""No evidence""",2023,2023-03-30T05:51:27Z,,,
arXIv2023,Deep Generative Model and Its Applications in Efficient Wireless Network Management: A Tutorial and Case Study,No.,1,"""No evidence""",2023,2023-03-30T02:59:51Z,,,
arXIv2023,"oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes",No.,1,"""No evidence""",2023,2023-03-30T01:37:19Z,,,
arXIv2023,DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents,No.,1,"""No evidence""",2023,2023-03-30T00:30:19Z,,,
arXIv2023,BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection,No.,1,"""No evidence""",2023,2023-03-29T20:30:52Z,,,
arXIv2023,How do decoding algorithms distribute information in dialogue responses?,No.,1,"""No evidence""",2023,2023-03-29T20:21:45Z,,,
arXIv2023,Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams,No.,1,"""No evidence""",2023,2023-03-29T20:10:13Z,,,
arXIv2023,AutoAD: Movie Description in Context,No.,1,"""No evidence""",2023,2023-03-29T17:59:58Z,,,
arXIv2023,ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance,No.,1,"""No evidence""",2023,2023-03-29T17:59:10Z,,,
arXIv2023,Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations,No.,1,"""No evidence""",2023,2023-03-29T17:58:39Z,,,
arXIv2023,Questions of science: chatting with ChatGPT about complex systems,No.,1,"""No evidence""",2023,2023-03-29T17:27:05Z,,,
arXIv2023,AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators,No.,1,"""No evidence""",2023,2023-03-29T17:03:21Z,,,
arXIv2023,Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks,No.,1,"""No evidence""",2023,2023-03-29T09:45:50Z,,,
arXIv2023,RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes,No.,1,"""No evidence""",2023,2023-03-29T08:06:22Z,,,
arXIv2023,An Over-parameterized Exponential Regression,No.,1,"""No evidence""",2023,2023-03-29T07:29:07Z,,,
arXIv2023,TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs,No.,1,"""No evidence""",2023,2023-03-29T03:30:38Z,,,
arXIv2023,Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering,No.,1,"""No evidence""",2023,2023-03-29T02:46:18Z,,,
arXIv2023,ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools,No.,1,"""No evidence""",2023,2023-03-28T23:16:00Z,,,
arXIv2023,"A ""Perspectival"" Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, YouTube, and Wikipedia",No.,1,"""No evidence""",2023,2023-03-28T19:49:58Z,,,
arXIv2023,Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema,No.,1,"""No evidence""",2023,2023-03-28T18:56:31Z,,,
arXIv2023,On Codex Prompt Engineering for OCL Generation: An Empirical Study,No.,1,"""No evidence""",2023,2023-03-28T18:50:51Z,,,
arXIv2023,LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,No.,1,"""No evidence""",2023,2023-03-28T17:59:12Z,,,
arXIv2023,Planning with Sequence Models through Iterative Energy Minimization,No.,1,"""No evidence""",2023,2023-03-28T17:53:22Z,,,
arXIv2023,Ecosystem Graphs: The Social Footprint of Foundation Models,No.,1,"""No evidence""",2023,2023-03-28T07:18:29Z,,,
arXIv2023,"Solving Regularized Exp, Cosh and Sinh Regression Problems",No.,1,"""No evidence""",2023,2023-03-28T04:26:51Z,,,
arXIv2023,Explicit Planning Helps Language Models in Logical Reasoning,No.,1,"""No evidence""",2023,2023-03-28T03:55:03Z,,,
arXIv2023,Comparative Analysis of CHATGPT and the evolution of language models,No.,1,"""No evidence""",2023,2023-03-28T03:11:28Z,,,
arXIv2023,Model and Evaluation: Towards Fairness in Multilingual Text Classification,No.,1,"""No evidence""",2023,2023-03-28T03:00:01Z,,,
arXIv2023,ChatGPT4PCG Competition: Character-like Level Generation for Science Birds,No.,1,"""No evidence""",2023,2023-03-28T01:07:38Z,,,
arXIv2023,Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-03-27T22:27:23Z,,,
arXIv2023,On the Creativity of Large Language Models,No.,1,"""No evidence""",2023,2023-03-27T18:00:01Z,,,
arXIv2023,Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation,No.,1,"""No evidence""",2023,2023-03-27T17:31:13Z,,,
arXIv2023,"Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention",No.,1,"""No evidence""",2023,2023-03-27T15:02:48Z,,,
arXIv2023,Cross-utterance ASR Rescoring with Graph-based Label Propagation,No.,1,"""No evidence""",2023,2023-03-27T12:08:05Z,,,
arXIv2023,ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks,No.,1,"""No evidence""",2023,2023-03-27T09:59:48Z,,,
arXIv2023,Unified Text Structuralization with Instruction-tuned Language Models,No.,1,"""No evidence""",2023,2023-03-27T07:39:05Z,,,
arXIv2023,Model-Based Reinforcement Learning with Isolated Imaginations,No.,1,"""No evidence""",2023,2023-03-27T02:55:56Z,,,
arXIv2023,GPT-PINN: Generative Pre-Trained Physics-Informed Neural Networks toward non-intrusive Meta-learning of parametric PDEs,No.,1,"""No evidence""",2023,2023-03-27T02:22:09Z,,,
arXIv2023,Koala: An Index for Quantifying Overlaps with Pre-training Corpora,No.,1,"""No evidence""",2023,2023-03-26T16:29:18Z,,,
arXIv2023,Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases,No.,1,"""No evidence""",2023,2023-03-26T14:49:37Z,,,
arXIv2023,Exploring Multimodal Sentiment Analysis via CBAM Attention and Double-layer BiLSTM Architecture,No.,1,"""No evidence""",2023,2023-03-26T12:34:01Z,,,
arXIv2023,Guiding AI-Generated Digital Content with Wireless Perception,No.,1,"""No evidence""",2023,2023-03-26T04:39:03Z,,,
arXIv2023,Can Large Language Models assist in Hazard Analysis?,No.,1,"""No evidence""",2023,2023-03-25T19:43:27Z,,,
arXIv2023,Indonesian Text-to-Image Synthesis with Sentence-BERT and FastGAN,No.,1,"""No evidence""",2023,2023-03-25T16:54:22Z,,,
arXIv2023,IFSeg: Image-free Semantic Segmentation via Vision-Language Model,No.,1,"""No evidence""",2023,2023-03-25T08:19:31Z,,,
arXIv2023,Analyzing the Performance of GPT-3.5 and GPT-4 in Grammatical Error Correction,No.,1,"""No evidence""",2023,2023-03-25T03:08:49Z,,,
arXIv2023,GPT is becoming a Turing machine: Here are some ways to program it,No.,1,"""No evidence""",2023,2023-03-25T00:43:41Z,,,
arXIv2023,VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining,No.,1,"""No evidence""",2023,2023-03-24T23:57:28Z,,,
arXIv2023,Depression detection in social media posts using affective and social norm features,No.,1,"""No evidence""",2023,2023-03-24T21:26:27Z,,,
arXIv2023,Lay Text Summarisation Using Natural Language Processing: A Narrative Literature Review,No.,1,"""No evidence""",2023,2023-03-24T18:30:50Z,,,
arXIv2023,Scaling Expert Language Models with Unsupervised Domain Discovery,No.,1,"""No evidence""",2023,2023-03-24T17:38:58Z,,,
arXIv2023,SEAL: Semantic Frame Execution And Localization for Perceiving Afforded Robot Actions,No.,1,"""No evidence""",2023,2023-03-24T15:25:41Z,,,
arXIv2023,Prompt Tuning based Adapter for Vision-Language Model Adaption,No.,1,"""No evidence""",2023,2023-03-24T15:05:17Z,,,
arXIv2023,"Generative AI Assistants in Software Development Education: A vision for integrating Generative AI into educational practice, not instinctively defending against it",No.,1,"""No evidence""",2023,2023-03-24T11:45:52Z,,,
arXIv2023,Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation,No.,1,"""No evidence""",2023,2023-03-24T09:30:09Z,,,
arXIv2023,Unleashing ChatGPT on the Metaverse: Savior or Destroyer?,No.,1,"""No evidence""",2023,2023-03-24T08:35:37Z,,,
arXIv2023,marl-jax: Multi-Agent Reinforcement Leaning Framework,No.,1,"""No evidence""",2023,2023-03-24T05:05:01Z,,,
arXIv2023,Toward Open-domain Slot Filling via Self-supervised Co-training,No.,1,"""No evidence""",2023,2023-03-24T04:51:22Z,,,
arXIv2023,Towards Making the Most of ChatGPT for Machine Translation,No.,1,"""No evidence""",2023,2023-03-24T03:35:21Z,,,
arXIv2023,EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms,No.,1,"""No evidence""",2023,2023-03-24T01:56:21Z,,,
arXIv2023,Natural language processing to automatically extract the presence and severity of esophagitis in notes of patients undergoing radiotherapy,No.,1,"""No evidence""",2023,2023-03-24T00:26:07Z,,,
arXIv2023,Detecting Backdoors in Pre-trained Encoders,No.,1,"""No evidence""",2023,2023-03-23T19:04:40Z,,,
arXIv2023,Three ways to improve feature alignment for open vocabulary detection,No.,1,"""No evidence""",2023,2023-03-23T17:59:53Z,,,
arXIv2023,NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations,No.,1,"""No evidence""",2023,2023-03-23T17:50:40Z,,,
arXIv2023,SwissBERT: The Multilingual Language Model for Switzerland,No.,1,"""No evidence""",2023,2023-03-23T14:44:47Z,,,
arXIv2023,GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering,No.,1,"""No evidence""",2023,2023-03-23T14:06:26Z,,,
arXIv2023,Visual-Language Prompt Tuning with Knowledge-guided Context Optimization,No.,1,"""No evidence""",2023,2023-03-23T14:04:23Z,,,
arXIv2023,Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World,No.,1,"""No evidence""",2023,2023-03-23T13:06:38Z,,,
arXIv2023,Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels,No.,1,"""No evidence""",2023,2023-03-23T12:39:20Z,,,
arXIv2023,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,No.,1,"""No evidence""",2023,2023-03-23T12:34:30Z,,,
arXIv2023,GesGPT: Speech Gesture Synthesis With Text Parsing from GPT,No.,1,"""No evidence""",2023,2023-03-23T03:30:30Z,,,
arXIv2023,Is ChatGPT A Good Keyphrase Generator? A Preliminary Study,No.,1,"""No evidence""",2023,2023-03-23T02:50:38Z,,,
arXIv2023,The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs,No.,1,"""No evidence""",2023,2023-03-22T23:54:14Z,,,
arXIv2023,Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs,No.,1,"""No evidence""",2023,2023-03-22T22:53:44Z,,,
arXIv2023,TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics,No.,1,"""No evidence""",2023,2023-03-22T21:09:49Z,,,
arXIv2023,Cross-Layer Design for AI Acceleration with Non-Coherent Optical Computing,No.,1,"""No evidence""",2023,2023-03-22T21:03:40Z,,,
arXIv2023,Salient Span Masking for Temporal Understanding,No.,1,"""No evidence""",2023,2023-03-22T18:49:43Z,,,
arXIv2023,RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation,No.,1,"""No evidence""",2023,2023-03-22T13:54:46Z,,,
arXIv2023,$P^{3}O$: Transferring Visual Representations for Reinforcement Learning via Prompting,No.,1,"""No evidence""",2023,2023-03-22T08:14:23Z,,,
arXIv2023,ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions,No.,1,"""No evidence""",2023,2023-03-22T08:03:27Z,,,
arXIv2023,Frozen Language Model Helps ECG Zero-Shot Learning,No.,1,"""No evidence""",2023,2023-03-22T05:01:14Z,,,
arXIv2023,Generate labeled training data using Prompt Programming and GPT-3. An example of Big Five Personality Classification,No.,1,"""No evidence""",2023,2023-03-22T03:12:40Z,,,
arXIv2023,Understand Legal Documents with Contextualized Large Language Models,No.,1,"""No evidence""",2023,2023-03-21T18:48:11Z,,,
arXIv2023,Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement,No.,1,"""No evidence""",2023,2023-03-21T17:51:23Z,,,
arXIv2023,Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding,No.,1,"""No evidence""",2023,2023-03-21T17:30:40Z,,,
arXIv2023,Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity,No.,1,"""No evidence""",2023,2023-03-21T16:35:01Z,,,
arXIv2023,Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning,No.,1,"""No evidence""",2023,2023-03-21T14:12:08Z,,,
arXIv2023,Chinese Intermediate English Learners outdid ChatGPT in deep cohesion: Evidence from English narrative writing,No.,1,"""No evidence""",2023,2023-03-21T12:55:54Z,,,
arXIv2023,Multi-modal Prompting for Low-Shot Temporal Action Localization,No.,1,"""No evidence""",2023,2023-03-21T10:40:13Z,,,
arXIv2023,Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks,No.,1,"""No evidence""",2023,2023-03-21T07:25:36Z,,,
arXIv2023,"Large AI Models in Health Informatics: Applications, Challenges, and the Future",No.,1,"""No evidence""",2023,2023-03-21T03:28:33Z,,,
arXIv2023,Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency,No.,1,"""No evidence""",2023,2023-03-21T01:06:37Z,,,
arXIv2023,Mind meets machine: Unravelling GPT-4's cognitive psychology,No.,1,"""No evidence""",2023,2023-03-20T20:28:26Z,,,
arXIv2023,eP-ALM: Efficient Perceptual Augmentation of Language Models,No.,1,"""No evidence""",2023,2023-03-20T19:20:34Z,,,
arXIv2023,MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,No.,1,"""No evidence""",2023,2023-03-20T18:31:47Z,,,
arXIv2023,Unit Scaling: Out-of-the-Box Low-Precision Training,No.,1,"""No evidence""",2023,2023-03-20T16:42:25Z,,,
arXIv2023,Capabilities of GPT-4 on Medical Challenge Problems,No.,1,"""No evidence""",2023,2023-03-20T16:18:38Z,,,
arXIv2023,Maximizing Penetration Testing Success with Effective Reconnaissance Techniques using ChatGPT,No.,1,"""No evidence""",2023,2023-03-20T15:59:48Z,,,
arXIv2023,Multimodal Shannon Game with Images,No.,1,"""No evidence""",2023,2023-03-20T15:22:11Z,,,
arXIv2023,SGFormer: Semantic Graph Transformer for Point Cloud-based 3D Scene Graph Generation,No.,1,"""No evidence""",2023,2023-03-20T11:59:23Z,,,
arXIv2023,Hospitalization Length of Stay Prediction using Patient Event Sequences,No.,1,"""No evidence""",2023,2023-03-20T11:48:36Z,,,
arXIv2023,Neural Implicit Vision-Language Feature Fields,No.,1,"""No evidence""",2023,2023-03-20T09:38:09Z,,,
arXIv2023,On-the-fly Text Retrieval for End-to-End ASR Adaptation,No.,1,"""No evidence""",2023,2023-03-20T08:54:40Z,,,
arXIv2023,"Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models",No.,1,"""No evidence""",2023,2023-03-20T06:20:03Z,,,
arXIv2023,PanGu-?: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,No.,1,"""No evidence""",2023,2023-03-20T03:39:27Z,,,
arXIv2023,FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System,No.,1,"""No evidence""",2023,2023-03-20T02:44:35Z,,,
arXIv2023,CTRAN: CNN-Transformer-based Network for Natural Language Understanding,No.,1,"""No evidence""",2023,2023-03-19T08:57:39Z,,,
arXIv2023,Lightweight Contrastive Protein Structure-Sequence Transformation,No.,1,"""No evidence""",2023,2023-03-19T08:19:10Z,,,
arXIv2023,Label Name is Mantra: Unifying Point Cloud Segmentation across Heterogeneous Datasets,No.,1,"""No evidence""",2023,2023-03-19T06:14:22Z,,,
arXIv2023,CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft,No.,1,"""No evidence""",2023,2023-03-19T05:20:52Z,,,
arXIv2023,NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online,No.,1,"""No evidence""",2023,2023-03-18T14:54:57Z,,,
arXIv2023,Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting,No.,1,"""No evidence""",2023,2023-03-17T17:29:02Z,,,
arXIv2023,She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models,No.,1,"""No evidence""",2023,2023-03-17T17:16:53Z,,,
arXIv2023,GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models,No.,1,"""No evidence""",2023,2023-03-17T17:15:20Z,,,
arXIv2023,Trained on 100 million words and still in shape: BERT meets British National Corpus,No.,1,"""No evidence""",2023,2023-03-17T09:53:33Z,,,
arXIv2023,GADFormer: An Attention-based Model for Group Anomaly Detection on Trajectories,No.,1,"""No evidence""",2023,2023-03-17T08:49:09Z,,,
arXIv2023,DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing,No.,1,"""No evidence""",2023,2023-03-17T08:12:36Z,,,
arXIv2023,CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos,No.,1,"""No evidence""",2023,2023-03-17T01:10:33Z,,,
arXIv2023,TypeT5: Seq2seq Type Inference using Static Analysis,No.,1,"""No evidence""",2023,2023-03-16T23:48:00Z,,,
arXIv2023,HIVE: Harnessing Human Feedback for Instructional Visual Editing,No.,1,"""No evidence""",2023,2023-03-16T19:47:41Z,,,
arXIv2023,Jump to Conclusions: Short-Cutting Transformers With Linear Transformations,No.,1,"""No evidence""",2023,2023-03-16T16:10:16Z,,,
arXIv2023,Logical Implications for Visual Question Answering Consistency,No.,1,"""No evidence""",2023,2023-03-16T16:00:18Z,,,
arXIv2023,Towards the Scalable Evaluation of Cooperativeness in Language Models,No.,1,"""No evidence""",2023,2023-03-16T15:34:23Z,,,
arXIv2023,SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference,No.,1,"""No evidence""",2023,2023-03-16T12:44:16Z,,,
arXIv2023,Block-wise Bit-Compression of Transformer-based Models,No.,1,"""No evidence""",2023,2023-03-16T09:53:57Z,,,
arXIv2023,Rethinking Model Ensemble in Transfer-based Adversarial Attacks,No.,1,"""No evidence""",2023,2023-03-16T06:37:16Z,,,
arXIv2023,Automatic Geo-alignment of Artwork in Children's Story Books,No.,1,"""No evidence""",2023,2023-03-16T06:23:06Z,,,
arXIv2023,Self-Inspection Method of Unmanned Aerial Vehicles in Power Plants Using Deep Q-Network Reinforcement Learning,No.,1,"""No evidence""",2023,2023-03-16T00:58:50Z,,,
arXIv2023,"MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge",No.,1,"""No evidence""",2023,2023-03-15T20:17:41Z,,,
arXIv2023,GPT-4 Technical Report,No.,1,"""No evidence""",2023,2023-03-15T17:15:04Z,,,
arXIv2023,"Mirror: A Natural Language Interface for Data Querying, Summarization, and Visualization",No.,1,"""No evidence""",2023,2023-03-15T15:31:51Z,,,
arXIv2023,Automated Query Generation for Evidence Collection from Web Search Engines,No.,1,"""No evidence""",2023,2023-03-15T14:32:00Z,,,
arXIv2023,GCRE-GPT: A Generative Model for Comparative Relation Extraction,No.,1,"""No evidence""",2023,2023-03-15T13:15:22Z,,,
arXIv2023,Efficient Uncertainty Estimation with Gaussian Process for Reliable Dialog Response Retrieval,No.,1,"""No evidence""",2023,2023-03-15T13:12:16Z,,,
arXIv2023,ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark,No.,1,"""No evidence""",2023,2023-03-15T00:35:50Z,,,
arXIv2023,Clinical Concept and Relation Extraction Using Prompt-based Machine Reading Comprehension,No.,1,"""No evidence""",2023,2023-03-14T22:37:31Z,,,
arXIv2023,NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions,No.,1,"""No evidence""",2023,2023-03-14T20:59:04Z,,,
arXIv2023,MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain,No.,1,"""No evidence""",2023,2023-03-14T18:58:08Z,,,
arXIv2023,Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs,No.,1,"""No evidence""",2023,2023-03-14T17:47:25Z,,,
arXIv2023,Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers,No.,1,"""No evidence""",2023,2023-03-14T15:45:35Z,,,
arXIv2023,Features matching using natural language processing,No.,1,"""No evidence""",2023,2023-03-14T13:31:19Z,,,
arXIv2023,The Learnability of In-Context Learning,No.,1,"""No evidence""",2023,2023-03-14T13:28:39Z,,,
arXIv2023,Geolocation Predicting of Tweets Using BERT-Based Models,No.,1,"""No evidence""",2023,2023-03-14T12:56:47Z,,,
arXIv2023,Query2doc: Query Expansion with Large Language Models,No.,1,"""No evidence""",2023,2023-03-14T07:27:30Z,,,
arXIv2023,Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences,No.,1,"""No evidence""",2023,2023-03-14T03:13:02Z,,,
arXIv2023,Architext: Language-Driven Generative Architecture Design,No.,1,"""No evidence""",2023,2023-03-13T23:11:05Z,,,
arXIv2023,Deep Learning Approach for Classifying the Aggressive Comments on Social Media: Machine Translated Data Vs Real Life Data,No.,1,"""No evidence""",2023,2023-03-13T21:43:08Z,,,
arXIv2023,AMOM: Adaptive Masking over Masking for Conditional Masked Language Model,No.,1,"""No evidence""",2023,2023-03-13T20:34:56Z,,,
arXIv2023,Model-tuning Via Prompts Makes NLP Models Adversarially Robust,No.,1,"""No evidence""",2023,2023-03-13T17:41:57Z,,,
arXIv2023,Transformer-based approaches to Sentiment Detection,No.,1,"""No evidence""",2023,2023-03-13T17:12:03Z,,,
arXIv2023,Analyzing ChatGPT's Aptitude in an Introductory Computer Engineering Course,No.,1,"""No evidence""",2023,2023-03-13T16:22:43Z,,,
arXIv2023,ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in,No.,1,"""No evidence""",2023,2023-03-13T03:28:36Z,,,
arXIv2023,Improving the Diproche CNL through Autoformalization via Large Language Models,No.,1,"""No evidence""",2023,2023-03-12T20:11:25Z,,,
arXIv2023,LUKE-Graph: A Transformer-based Approach with Gated Relational Graph Attention for Cloze-style Reading Comprehension,No.,1,"""No evidence""",2023,2023-03-12T14:31:44Z,,,
arXIv2023,"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions",No.,1,"""No evidence""",2023,2023-03-12T07:22:08Z,,,
arXIv2023,Accommodating Audio Modality in CLIP for Multimodal Processing,No.,1,"""No evidence""",2023,2023-03-12T06:57:01Z,,,
arXIv2023,Predictive Experience Replay for Continual Visual Control and Forecasting,No.,1,"""No evidence""",2023,2023-03-12T05:08:03Z,,,
arXIv2023,A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability,No.,1,"""No evidence""",2023,2023-03-12T04:22:01Z,,,
arXIv2023,Learning Combinatorial Prompts for Universal Controllable Image Captioning,No.,1,"""No evidence""",2023,2023-03-11T07:53:15Z,,,
arXIv2023,Rewarding Chatbots for Real-World Engagement with Millions of Users,No.,1,"""No evidence""",2023,2023-03-10T18:53:52Z,,,
arXIv2023,A General Recipe for the Analysis of Randomized Multi-Armed Bandit Algorithms,No.,1,"""No evidence""",2023,2023-03-10T16:43:48Z,,,
arXIv2023,ChatGPT as the Transportation Equity Information Source for Scientific Writing,No.,1,"""No evidence""",2023,2023-03-10T16:21:54Z,,,
arXIv2023,Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation,No.,1,"""No evidence""",2023,2023-03-10T15:35:11Z,,,
arXIv2023,Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,No.,1,"""No evidence""",2023,2023-03-10T15:17:22Z,,,
arXIv2023,Algorithmic Ghost in the Research Shell: Large Language Models and Academic Knowledge Creation in Management Research,No.,1,"""No evidence""",2023,2023-03-10T14:25:29Z,,,
arXIv2023,Tag2Text: Guiding Vision-Language Model via Image Tagging,No.,1,"""No evidence""",2023,2023-03-10T02:16:35Z,,,
arXIv2023,Iterative Few-shot Semantic Segmentation from Image Label Text,No.,1,"""No evidence""",2023,2023-03-10T01:48:14Z,,,
arXIv2023,"ChatGPT may Pass the Bar Exam soon, but has a Long Way to Go for the LexGLUE benchmark",No.,1,"""No evidence""",2023,2023-03-09T16:42:29Z,,,
arXIv2023,Seeing ChatGPT Through Students' Eyes: An Analysis of TikTok Data,No.,1,"""No evidence""",2023,2023-03-09T15:46:54Z,,,
arXIv2023,Knowledge-augmented Few-shot Visual Relation Detection,No.,1,"""No evidence""",2023,2023-03-09T15:38:40Z,,,
arXIv2023,Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?,No.,1,"""No evidence""",2023,2023-03-09T10:12:18Z,,,
arXIv2023,Exploiting Contextual Structure to Generate Useful Auxiliary Tasks,No.,1,"""No evidence""",2023,2023-03-09T05:11:30Z,,,
arXIv2023,InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning,No.,1,"""No evidence""",2023,2023-03-08T23:40:47Z,,,
arXIv2023,Baldur: Whole-Proof Generation and Repair with Large Language Models,No.,1,"""No evidence""",2023,2023-03-08T22:00:15Z,,,
arXIv2023,The Carbon Emissions of Writing and Illustrating Are Lower for AI than for Humans,No.,1,"""No evidence""",2023,2023-03-08T21:44:34Z,,,
arXIv2023,FaceChat: An Emotion-Aware Face-to-face Dialogue Framework,No.,1,"""No evidence""",2023,2023-03-08T20:45:37Z,,,
arXIv2023,"Extending the Pre-Training of BLOOM for Improved Support of Traditional Chinese: Models, Methods and Results",No.,1,"""No evidence""",2023,2023-03-08T16:53:19Z,,,
arXIv2023,"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",No.,1,"""No evidence""",2023,2023-03-08T15:50:02Z,,,
arXIv2023,ChatGPT Participates in a Computer Science Exam,No.,1,"""No evidence""",2023,2023-03-08T15:46:14Z,,,
arXIv2023,A Prompt Log Analysis of Text-to-Image Generation Systems,No.,1,"""No evidence""",2023,2023-03-08T13:59:41Z,,,
arXIv2023,A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT,No.,1,"""No evidence""",2023,2023-03-07T20:36:13Z,,,
arXIv2023,A Strategy-Oriented Bayesian Soft Actor-Critic Model,No.,1,"""No evidence""",2023,2023-03-07T19:31:25Z,,,
arXIv2023,Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction,No.,1,"""No evidence""",2023,2023-03-07T18:48:55Z,,,
arXIv2023,Is ChatGPT a Good NLG Evaluator? A Preliminary Study,No.,1,"""No evidence""",2023,2023-03-07T16:57:20Z,,,
arXIv2023,Making a Computational Attorney,No.,1,"""No evidence""",2023,2023-03-07T16:44:29Z,,,
arXIv2023,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,No.,1,"""No evidence""",2023,2023-03-07T14:31:55Z,,,
arXIv2023,The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset,No.,1,"""No evidence""",2023,2023-03-07T14:25:44Z,,,
arXIv2023,German BERT Model for Legal Named Entity Recognition,No.,1,"""No evidence""",2023,2023-03-07T11:54:39Z,,,
arXIv2023,Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning,No.,1,"""No evidence""",2023,2023-03-07T10:48:20Z,,,
arXIv2023,Preparing the Vuk'uzenzele and ZA-gov-multilingual South African multilingual corpora,No.,1,"""No evidence""",2023,2023-03-07T09:20:09Z,,,
arXIv2023,Graph Decision Transformer,No.,1,"""No evidence""",2023,2023-03-07T09:10:34Z,,,
arXIv2023,Classifying Text-Based Conspiracy Tweets related to COVID-19 using Contextualized Word Embeddings,No.,1,"""No evidence""",2023,2023-03-07T07:42:26Z,,,
arXIv2023,ADELT: Transpilation Between Deep Learning Frameworks,No.,1,"""No evidence""",2023,2023-03-07T01:57:10Z,,,
arXIv2023,Two-stage Pipeline for Multilingual Dialect Detection,No.,1,"""No evidence""",2023,2023-03-06T20:35:51Z,,,
arXIv2023,PaLM-E: An Embodied Multimodal Language Model,No.,1,"""No evidence""",2023,2023-03-06T18:58:06Z,,,
arXIv2023,ChatGPT is on the Horizon: Could a Large Language Model be Suitable for Intelligent Traffic Safety Research and Applications?,No.,1,"""No evidence""",2023,2023-03-06T16:36:17Z,,,
arXIv2023,Video Question Answering Using CLIP-Guided Visual-Text Attention,No.,1,"""No evidence""",2023,2023-03-06T13:49:15Z,,,
arXIv2023,Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback,No.,1,"""No evidence""",2023,2023-03-06T04:49:38Z,,,
arXIv2023,Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning,No.,1,"""No evidence""",2023,2023-03-06T03:25:59Z,,,
arXIv2023,A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification,No.,1,"""No evidence""",2023,2023-03-06T03:25:43Z,,,
arXIv2023,Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in Finance,No.,1,"""No evidence""",2023,2023-03-06T02:24:48Z,,,
arXIv2023,Local Environment Poisoning Attacks on Federated Reinforcement Learning,No.,1,"""No evidence""",2023,2023-03-05T17:44:23Z,,,
arXIv2023,FQP 2.0: Industry Trend Analysis via Hierarchical Financial Data,No.,1,"""No evidence""",2023,2023-03-05T16:17:56Z,,,
arXIv2023,Expectation consistency for calibration of neural networks,No.,1,"""No evidence""",2023,2023-03-05T11:21:03Z,,,
arXIv2023,Prismer: A Vision-Language Model with Multi-Task Experts,No.,1,"""No evidence""",2023,2023-03-04T21:22:47Z,,,
arXIv2023,FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks,No.,1,"""No evidence""",2023,2023-03-04T19:07:48Z,,,
arXIv2023,Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction,No.,1,"""No evidence""",2023,2023-03-04T17:59:43Z,,,
arXIv2023,ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing,No.,1,"""No evidence""",2023,2023-03-04T14:59:25Z,,,
arXIv2023,Double A3C: Deep Reinforcement Learning on OpenAI Gym Games,No.,1,"""No evidence""",2023,2023-03-04T00:06:27Z,,,
arXIv2023,Exploring Data Augmentation Methods on Social Media Corpora,No.,1,"""No evidence""",2023,2023-03-03T20:15:35Z,,,
arXIv2023,"Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners",No.,1,"""No evidence""",2023,2023-03-03T18:58:16Z,,,
arXIv2023,Pre-trained Model Representations and their Robustness against Noise for Speech Emotion Analysis,No.,1,"""No evidence""",2023,2023-03-03T18:22:32Z,,,
arXIv2023,Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT,No.,1,"""No evidence""",2023,2023-03-03T16:11:37Z,,,
arXIv2023,Intelligent O-RAN Traffic Steering for URLLC Through Deep Reinforcement Learning,No.,1,"""No evidence""",2023,2023-03-03T14:34:25Z,,,
arXIv2023,Multi label classification of Artificial Intelligence related patents using Modified D2SBERT and Sentence Attention mechanism,No.,1,"""No evidence""",2023,2023-03-03T12:27:24Z,,,
arXIv2023,Early Warning Signals of Social Instabilities in Twitter Data,No.,1,"""No evidence""",2023,2023-03-03T11:18:02Z,,,
arXIv2023,Exploiting Language Relatedness in Machine Translation Through Domain Adaptation Techniques,No.,1,"""No evidence""",2023,2023-03-03T09:07:30Z,,,
arXIv2023,Ask and You Shall Receive (a Graph Drawing): Testing ChatGPT's Potential to Apply Graph Layout Algorithms,No.,1,"""No evidence""",2023,2023-03-03T04:26:43Z,,,
arXIv2023,RePreM: Representation Pre-training with Masked Model for Reinforcement Learning,No.,1,"""No evidence""",2023,2023-03-03T02:04:14Z,,,
arXIv2023,Miipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations,No.,1,"""No evidence""",2023,2023-03-03T01:57:16Z,,,
arXIv2023,End-to-End Speech Recognition: A Survey,No.,1,"""No evidence""",2023,2023-03-03T01:46:41Z,,,
arXIv2023,ConTEXTual Net: A Multimodal Vision-Language Model for Segmentation of Pneumothorax,No.,1,"""No evidence""",2023,2023-03-02T22:36:19Z,,,
arXIv2023,BenchDirect: A Directed Language Model for Compiler Benchmarks,No.,1,"""No evidence""",2023,2023-03-02T20:17:24Z,,,
arXIv2023,The Ladder in Chaos: A Simple and Effective Improvement to General DRL Algorithms by Policy Path Trimming and Boosting,No.,1,"""No evidence""",2023,2023-03-02T16:20:46Z,,,
arXIv2023,Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning,No.,1,"""No evidence""",2023,2023-03-02T15:06:52Z,,,
arXIv2023,UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction,No.,1,"""No evidence""",2023,2023-03-02T12:18:53Z,,,
arXIv2023,INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification,No.,1,"""No evidence""",2023,2023-03-02T11:18:56Z,,,
arXIv2023,Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study,No.,1,"""No evidence""",2023,2023-03-02T09:03:43Z,,,
arXIv2023,Evaluating Parameter-Efficient Transfer Learning Approaches on SURE Benchmark for Speech Understanding,No.,1,"""No evidence""",2023,2023-03-02T08:57:33Z,,,
arXIv2023,AI and the FCI: Can ChatGPT Project an Understanding of Introductory Physics?,No.,1,"""No evidence""",2023,2023-03-02T08:43:11Z,,,
arXIv2023,Adopting the Multi-answer Questioning Task with an Auxiliary Metric for Extreme Multi-label Text Classification Utilizing the Label Hierarchy,No.,1,"""No evidence""",2023,2023-03-02T08:40:31Z,,,
arXIv2023,Image Labels Are All You Need for Coarse Seagrass Segmentation,No.,1,"""No evidence""",2023,2023-03-02T05:10:57Z,,,
arXIv2023,Variance-reduced Clipping for Non-convex Optimization,No.,1,"""No evidence""",2023,2023-03-02T00:57:38Z,,,
arXIv2023,SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks,No.,1,"""No evidence""",2023,2023-03-01T18:47:41Z,,,
arXIv2023,ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models with Interactive Visualization,No.,1,"""No evidence""",2023,2023-03-01T17:24:15Z,,,
arXIv2023,N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space,No.,1,"""No evidence""",2023,2023-03-01T12:32:34Z,,,
arXIv2023,Succinct Representations for Concepts,No.,1,"""No evidence""",2023,2023-03-01T12:11:23Z,,,
arXIv2023,A Framework for Neurosymbolic Robot Action Planning using Large Language Models,No.,1,"""No evidence""",2023,2023-03-01T11:54:22Z,,,
arXIv2023,Large Language Models Are State-of-the-Art Evaluators of Translation Quality,No.,1,"""No evidence""",2023,2023-02-28T12:23:48Z,,,
arXIv2023,Text classification dataset and analysis for Uzbek language,No.,1,"""No evidence""",2023,2023-02-28T11:21:24Z,,,
arXIv2023,Hierarchical Reinforcement Learning in Complex 3D Environments,No.,1,"""No evidence""",2023,2023-02-28T09:56:36Z,,,
arXIv2023,The Choice of Noninformative Priors for Thompson Sampling in Multiparameter Bandit Models,No.,1,"""No evidence""",2023,2023-02-28T08:42:42Z,,,
arXIv2023,GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation,No.,1,"""No evidence""",2023,2023-02-28T08:35:28Z,,,
arXIv2023,"Information-Restricted Neural Language Models Reveal Different Brain Regions' Sensitivity to Semantics, Syntax and Context",No.,1,"""No evidence""",2023,2023-02-28T08:16:18Z,,,
arXIv2023,BrainBERT: Self-supervised representation learning for intracranial recordings,No.,1,"""No evidence""",2023,2023-02-28T07:40:37Z,,,
arXIv2023,An evaluation of Google Translate for Sanskrit to English translation via sentiment and semantic analysis,No.,1,"""No evidence""",2023,2023-02-28T04:24:55Z,,,
arXIv2023,Weighted Sampling for Masked Language Modeling,No.,1,"""No evidence""",2023,2023-02-28T01:07:39Z,,,
arXIv2023,A Language-Guided Benchmark for Weakly Supervised Open Vocabulary Semantic Segmentation,No.,1,"""No evidence""",2023,2023-02-27T21:55:48Z,,,
arXIv2023,Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning,No.,1,"""No evidence""",2023,2023-02-27T19:53:49Z,,,
arXIv2023,Language Is Not All You Need: Aligning Perception with Language Models,No.,1,"""No evidence""",2023,2023-02-27T18:55:27Z,,,
arXIv2023,The ROOTS Search Tool: Data Transparency for LLMs,No.,1,"""No evidence""",2023,2023-02-27T18:45:18Z,,,
arXIv2023,LLaMA: Open and Efficient Foundation Language Models,No.,1,"""No evidence""",2023,2023-02-27T17:11:15Z,,,
arXIv2023,Inseq: An Interpretability Toolkit for Sequence Generation Models,No.,1,"""No evidence""",2023,2023-02-27T16:45:50Z,,,
arXIv2023,Argument Mining using BERT and Self-Attention based Embeddings,No.,1,"""No evidence""",2023,2023-02-27T15:52:31Z,,,
arXIv2023,Using Auxiliary Tasks In Multimodal Fusion Of Wav2vec 2.0 And BERT For Multimodal Emotion Recognition,No.,1,"""No evidence""",2023,2023-02-27T10:59:08Z,,,
arXIv2023,Duration-aware pause insertion using pre-trained language model for multi-speaker text-to-speech,No.,1,"""No evidence""",2023,2023-02-27T10:40:41Z,,,
arXIv2023,Finding Support Examples for In-Context Learning,No.,1,"""No evidence""",2023,2023-02-27T06:32:45Z,,,
arXIv2023,Elementwise Language Representation,No.,1,"""No evidence""",2023,2023-02-27T02:15:56Z,,,
arXIv2023,Comparing Sentence-Level Suggestions to Message-Level Suggestions in AI-Mediated Communication,No.,1,"""No evidence""",2023,2023-02-26T18:40:38Z,,,
arXIv2023,Efficient Ensemble for Multimodal Punctuation Restoration using Time-Delay Neural Network,No.,1,"""No evidence""",2023,2023-02-26T18:28:20Z,,,
arXIv2023,A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors,No.,1,"""No evidence""",2023,2023-02-25T18:29:32Z,,,
arXIv2023,Topic-Selective Graph Network for Topic-Focused Summarization,No.,1,"""No evidence""",2023,2023-02-25T15:56:06Z,,,
arXIv2023,Human-in-the-Loop Schema Induction,No.,1,"""No evidence""",2023,2023-02-25T10:20:02Z,,,
arXIv2023,Choice Fusion as Knowledge for Zero-Shot Dialogue State Tracking,No.,1,"""No evidence""",2023,2023-02-25T07:32:04Z,,,
arXIv2023,AugGPT: Leveraging ChatGPT for Text Data Augmentation,No.,1,"""No evidence""",2023,2023-02-25T06:58:16Z,,,
arXIv2023,NoPPA: Non-Parametric Pairwise Attention Random Walk Model for Sentence Representation,No.,1,"""No evidence""",2023,2023-02-24T21:20:25Z,,,
arXIv2023,MUX-PLMs: Data Multiplexing for High-throughput Language Models,No.,1,"""No evidence""",2023,2023-02-24T04:03:15Z,,,
arXIv2023,Factual Consistency Oriented Speech Recognition,No.,1,"""No evidence""",2023,2023-02-24T00:01:41Z,,,
arXIv2023,Side Adapter Network for Open-Vocabulary Semantic Segmentation,No.,1,"""No evidence""",2023,2023-02-23T18:58:28Z,,,
arXIv2023,UniXGen: A Unified Vision-Language Model for Multi-View Chest X-ray Generation and Report Generation,No.,1,"""No evidence""",2023,2023-02-23T17:13:25Z,,,
arXIv2023,Generative Sentiment Transfer via Adaptive Masking,No.,1,"""No evidence""",2023,2023-02-23T14:17:34Z,,,
arXIv2023,A metric to compare the anatomy variation between image time series,No.,1,"""No evidence""",2023,2023-02-23T11:18:04Z,,,
arXIv2023,Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers,No.,1,"""No evidence""",2023,2023-02-23T06:48:24Z,,,
arXIv2023,VLSP2022-EVJVQA Challenge: Multilingual Visual Question Answering,No.,1,"""No evidence""",2023,2023-02-23T02:38:39Z,,,
arXIv2023,Solution for the EPO CodeFest on Green Plastics: Hierarchical multi-label classification of patents relating to green plastics using deep learning,No.,1,"""No evidence""",2023,2023-02-22T19:06:58Z,,,
arXIv2023,On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective,No.,1,"""No evidence""",2023,2023-02-22T11:01:20Z,,,
arXIv2023,Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting,No.,1,"""No evidence""",2023,2023-02-22T07:56:45Z,,,
arXIv2023,Playing the Werewolf game with artificial intelligence for language understanding,No.,1,"""No evidence""",2023,2023-02-21T13:03:20Z,,,
arXIv2023,Connecting Humanities and Social Sciences: Applying Language and Speech Technology to Online Panel Surveys,No.,1,"""No evidence""",2023,2023-02-21T10:52:15Z,,,
arXIv2023,BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT,No.,1,"""No evidence""",2023,2023-02-21T08:59:22Z,,,
arXIv2023,Online Evolutionary Neural Architecture Search for Multivariate Non-Stationary Time Series Forecasting,No.,1,"""No evidence""",2023,2023-02-20T22:25:47Z,,,
arXIv2023,Towards Universal Fake Image Detectors that Generalize Across Generative Models,No.,1,"""No evidence""",2023,2023-02-20T18:59:04Z,,,
arXIv2023,Federated Learning for ASR based on Wav2vec 2.0,No.,1,"""No evidence""",2023,2023-02-20T18:36:46Z,,,
arXIv2023,ChatGPT: A Meta-Analysis after 2.5 Months,No.,1,"""No evidence""",2023,2023-02-20T15:43:22Z,,,
arXIv2023,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,No.,1,"""No evidence""",2023,2023-02-20T15:34:03Z,,,
arXIv2023,Boosting classification reliability of NLP transformer models in the long run,No.,1,"""No evidence""",2023,2023-02-20T14:46:54Z,,,
arXIv2023,Can discrete information extraction prompts generalize across language models?,No.,1,"""No evidence""",2023,2023-02-20T09:56:51Z,,,
arXIv2023,ChatGPT for Robotics: Design Principles and Model Abilities,No.,1,"""No evidence""",2023,2023-02-20T06:39:06Z,,,
arXIv2023,STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training,No.,1,"""No evidence""",2023,2023-02-20T03:13:45Z,,,
arXIv2023,Emphasizing Unseen Words: New Vocabulary Acquisition for End-to-End Speech Recognition,No.,1,"""No evidence""",2023,2023-02-20T02:21:30Z,,,
arXIv2023,Generalization in Visual Reinforcement Learning with the Reward Sequence Distribution,No.,1,"""No evidence""",2023,2023-02-19T15:47:24Z,,,
arXIv2023,Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT,No.,1,"""No evidence""",2023,2023-02-19T12:29:33Z,,,
arXIv2023,Text Classification in the Wild: a Large-scale Long-tailed Name Normalization Dataset,No.,1,"""No evidence""",2023,2023-02-19T08:44:21Z,,,
arXIv2023,ChatGPT (Feb 13 Version) is a Chinese Room,No.,1,"""No evidence""",2023,2023-02-19T01:52:06Z,,,
arXIv2023,Online Continuous Hyperparameter Optimization for Generalized Linear Contextual Bandits,No.,1,"""No evidence""",2023,2023-02-18T23:31:20Z,,,
arXIv2023,"BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark",No.,1,"""No evidence""",2023,2023-02-18T22:20:37Z,,,
arXIv2023,A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT,No.,1,"""No evidence""",2023,2023-02-18T20:51:09Z,,,
arXIv2023,BERT is not The Count: Learning to Match Mathematical Statements with Proofs,No.,1,"""No evidence""",2023,2023-02-18T14:48:20Z,,,
arXIv2023,Bag of Tricks for Effective Language Model Pretraining and Downstream Adaptation: A Case Study on GLUE,No.,1,"""No evidence""",2023,2023-02-18T09:26:35Z,,,
arXIv2023,Approximate Thompson Sampling via Epistemic Neural Networks,No.,1,"""No evidence""",2023,2023-02-18T01:58:15Z,,,
arXIv2023,Prompting Large Language Models With the Socratic Method,No.,1,"""No evidence""",2023,2023-02-17T23:25:57Z,,,
arXIv2023,Conveying the Predicted Future to Users: A Case Study of Story Plot Prediction,No.,1,"""No evidence""",2023,2023-02-17T20:10:55Z,,,
arXIv2023,ViTA: A Vision Transformer Inference Accelerator for Edge Applications,No.,1,"""No evidence""",2023,2023-02-17T19:35:36Z,,,
arXIv2023,A Simplistic Model of Neural Scaling Laws: Multiperiodic Santa Fe Processes,No.,1,"""No evidence""",2023,2023-02-17T18:27:27Z,,,
arXIv2023,Entry Separation using a Mixed Visual and Textual Language Model: Application to 19th century French Trade Directories,No.,1,"""No evidence""",2023,2023-02-17T15:30:44Z,,,
arXIv2023,Hate Speech and Offensive Language Detection using an Emotion-aware Shared Encoder,No.,1,"""No evidence""",2023,2023-02-17T09:31:06Z,,,
arXIv2023,GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis,No.,1,"""No evidence""",2023,2023-02-17T06:33:06Z,,,
arXIv2023,PAC Prediction Sets for Large Language Models of Code,No.,1,"""No evidence""",2023,2023-02-17T05:32:24Z,,,
arXIv2023,What A Situated Language-Using Agent Must be Able to Do: A Top-Down Analysis,No.,1,"""No evidence""",2023,2023-02-16T21:30:26Z,,,
arXIv2023,Syntactic Structure Processing in the Brain while Listening,No.,1,"""No evidence""",2023,2023-02-16T21:28:11Z,,,
arXIv2023,JEIT: Joint End-to-End Model and Internal Language Model Training for Speech Recognition,No.,1,"""No evidence""",2023,2023-02-16T21:07:38Z,,,
arXIv2023,"For Generated Text, Is NLI-Neutral Text the Best Text?",No.,1,"""No evidence""",2023,2023-02-16T20:46:36Z,,,
arXIv2023,Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media,No.,1,"""No evidence""",2023,2023-02-16T20:42:04Z,,,
arXIv2023,Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data,No.,1,"""No evidence""",2023,2023-02-16T18:20:27Z,,,
arXIv2023,Retrieval-augmented Image Captioning,No.,1,"""No evidence""",2023,2023-02-16T12:54:13Z,,,
arXIv2023,Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model,No.,1,"""No evidence""",2023,2023-02-16T08:54:05Z,,,
arXIv2023,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,No.,1,"""No evidence""",2023,2023-02-16T08:37:22Z,,,
arXIv2023,Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization,No.,1,"""No evidence""",2023,2023-02-16T04:41:30Z,,,
arXIv2023,Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK,No.,1,"""No evidence""",2023,2023-02-16T02:27:13Z,,,
arXIv2023,Platform-Independent and Curriculum-Oriented Intelligent Assistant for Higher Education,No.,1,"""No evidence""",2023,2023-02-15T19:02:01Z,,,
arXIv2023,Learning Performance-Improving Code Edits,No.,1,"""No evidence""",2023,2023-02-15T18:59:21Z,,,
arXIv2023,NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation,No.,1,"""No evidence""",2023,2023-02-15T18:31:36Z,,,
arXIv2023,The Capacity for Moral Self-Correction in Large Language Models,No.,1,"""No evidence""",2023,2023-02-15T04:25:40Z,,,
arXIv2023,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,No.,1,"""No evidence""",2023,2023-02-15T03:53:26Z,,,
arXIv2023,A Psycholinguistic Analysis of BERT's Representations of Compounds,No.,1,"""No evidence""",2023,2023-02-14T18:23:15Z,,,
arXIv2023,Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking,No.,1,"""No evidence""",2023,2023-02-14T17:00:06Z,,,
arXIv2023,A Modern Look at the Relationship between Sharpness and Generalization,No.,1,"""No evidence""",2023,2023-02-14T12:38:12Z,,,
arXIv2023,Exploring Category Structure with Contextual Language Models and Lexical Semantic Networks,No.,1,"""No evidence""",2023,2023-02-14T09:57:23Z,,,
arXIv2023,SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains,No.,1,"""No evidence""",2023,2023-02-14T07:14:08Z,,,
arXIv2023,BLIAM: Literature-based Data Synthesis for Synergistic Drug Combination Prediction,No.,1,"""No evidence""",2023,2023-02-14T06:48:52Z,,,
arXIv2023,AI Chat Assistants can Improve Conversations about Divisive Topics,No.,1,"""No evidence""",2023,2023-02-14T06:42:09Z,,,
arXIv2023,Language Model Analysis for Ontology Subsumption Inference,No.,1,"""No evidence""",2023,2023-02-14T00:21:56Z,,,
arXIv2023,STREET: A Multi-Task Structured Reasoning and Explanation Benchmark,No.,1,"""No evidence""",2023,2023-02-13T22:34:02Z,,,
arXIv2023,Towards Agile Text Classifiers for Everyone,No.,1,"""No evidence""",2023,2023-02-13T17:34:13Z,,,
arXIv2023,Linguistic ambiguity analysis in ChatGPT,No.,1,"""No evidence""",2023,2023-02-13T15:03:07Z,,,
arXIv2023,Distinguishability Calibration to In-Context Learning,No.,1,"""No evidence""",2023,2023-02-13T09:15:00Z,,,
arXIv2023,"Academic Writing with GPT-3.5: Reflections on Practices, Efficacy and Transparency",No.,1,"""No evidence""",2023,2023-02-12T22:05:08Z,,,
arXIv2023,RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL,No.,1,"""No evidence""",2023,2023-02-12T17:41:16Z,,,
arXIv2023,SemanticAC: Semantics-Assisted Framework for Audio Classification,No.,1,"""No evidence""",2023,2023-02-12T15:30:28Z,,,
arXIv2023,Transformer models: an introduction and catalog,No.,1,"""No evidence""",2023,2023-02-12T01:26:49Z,,,
arXIv2023,A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3,No.,1,"""No evidence""",2023,2023-02-11T15:50:20Z,,,
arXIv2023,Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis,No.,1,"""No evidence""",2023,2023-02-11T05:46:21Z,,,
arXIv2023,CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code,No.,1,"""No evidence""",2023,2023-02-10T22:12:05Z,,,
arXIv2023,Satellite Anomaly Detection Using Variance Based Genetic Ensemble of Neural Networks,No.,1,"""No evidence""",2023,2023-02-10T22:09:00Z,,,
arXIv2023,Alloprof: a new French question-answer education dataset and its use in an information retrieval case study,No.,1,"""No evidence""",2023,2023-02-10T20:23:27Z,,,
arXIv2023,Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media,No.,1,"""No evidence""",2023,2023-02-10T19:40:10Z,,,
arXIv2023,Scaling Vision Transformers to 22 Billion Parameters,No.,1,"""No evidence""",2023,2023-02-10T18:58:21Z,,,
arXIv2023,Adversarial Transformer Language Models for Contextual Commonsense Inference,No.,1,"""No evidence""",2023,2023-02-10T18:21:13Z,,,
arXIv2023,GTR-CTRL: Instrument and Genre Conditioning for Guitar-Focused Music Generation with Transformers,No.,1,"""No evidence""",2023,2023-02-10T17:43:03Z,,,
arXIv2023,The Wisdom of Hindsight Makes Language Models Better Instruction Followers,No.,1,"""No evidence""",2023,2023-02-10T12:16:38Z,,,
arXIv2023,Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources,No.,1,"""No evidence""",2023,2023-02-10T08:54:05Z,,,
arXIv2023,Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information,No.,1,"""No evidence""",2023,2023-02-10T07:37:49Z,,,
arXIv2023,BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization,No.,1,"""No evidence""",2023,2023-02-10T06:23:44Z,,,
arXIv2023,Offsite-Tuning: Transfer Learning without Full Model,No.,1,"""No evidence""",2023,2023-02-09T18:59:55Z,,,
arXIv2023,Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,No.,1,"""No evidence""",2023,2023-02-09T18:57:56Z,,,
arXIv2023,Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting,No.,1,"""No evidence""",2023,2023-02-09T18:02:34Z,,,
arXIv2023,A Biomedical Knowledge Graph for Biomarker Discovery in Cancer,No.,1,"""No evidence""",2023,2023-02-09T16:17:57Z,,,
arXIv2023,ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design,No.,1,"""No evidence""",2023,2023-02-09T15:44:43Z,,,
arXIv2023,"Better by you, better than me, chatgpt3 as writing assistance in students essays",No.,1,"""No evidence""",2023,2023-02-09T10:04:18Z,,,
arXIv2023,Bag of Tricks for Training Data Extraction from Language Models,No.,1,"""No evidence""",2023,2023-02-09T06:46:42Z,,,
arXIv2023,Global Constraints with Prompting for Zero-Shot Event Argument Classification,No.,1,"""No evidence""",2023,2023-02-09T06:39:29Z,,,
arXIv2023,Enhancing E-Commerce Recommendation using Pre-Trained Language Model and Fine-Tuning,No.,1,"""No evidence""",2023,2023-02-09T05:20:52Z,,,
arXIv2023,Will ChatGPT get you caught? Rethinking of Plagiarism Detection,No.,1,"""No evidence""",2023,2023-02-08T20:59:18Z,,,
arXIv2023,Algorithmic Collective Action in Machine Learning,No.,1,"""No evidence""",2023,2023-02-08T18:55:49Z,,,
arXIv2023,Efficient Joint Learning for Clinical Named Entity Recognition and Relation Extraction Using Fourier Networks: A Use Case in Adverse Drug Events,No.,1,"""No evidence""",2023,2023-02-08T16:44:27Z,,,
arXIv2023,Automating Code-Related Tasks Through Transformers: The Impact of Pre-training,No.,1,"""No evidence""",2023,2023-02-08T13:37:33Z,,,
arXIv2023,Improving (Dis)agreement Detection with Inductive Social Relation Information From Comment-Reply Interactions,No.,1,"""No evidence""",2023,2023-02-08T09:09:47Z,,,
arXIv2023,Noise2Music: Text-conditioned Music Generation with Diffusion Models,No.,1,"""No evidence""",2023,2023-02-08T07:27:27Z,,,
arXIv2023,Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models,No.,1,"""No evidence""",2023,2023-02-08T06:24:06Z,,,
arXIv2023,Toward a Theory of Causation for Interpreting Neural Code Models,No.,1,"""No evidence""",2023,2023-02-07T22:56:58Z,,,
arXIv2023,ZipLM: Inference-Aware Structured Pruning of Language Models,No.,1,"""No evidence""",2023,2023-02-07T18:55:28Z,,,
arXIv2023,Characterizing Financial Market Coverage using Artificial Intelligence,No.,1,"""No evidence""",2023,2023-02-07T16:03:33Z,,,
arXIv2023,"A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends",No.,1,"""No evidence""",2023,2023-02-07T14:56:52Z,,,
arXIv2023,What do Language Models know about word senses? Zero-Shot WSD with Language Models and Domain Inventories,No.,1,"""No evidence""",2023,2023-02-07T09:55:07Z,,,
arXIv2023,The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study,No.,1,"""No evidence""",2023,2023-02-07T09:34:41Z,,,
arXIv2023,Leveraging Demonstrations to Improve Online Learning: Quality Matters,No.,1,"""No evidence""",2023,2023-02-07T08:49:12Z,,,
arXIv2023,UDApter -- Efficient Domain Adaptation Using Adapters,No.,1,"""No evidence""",2023,2023-02-07T02:04:17Z,,,
arXIv2023,Applying BERT and ChatGPT for Sentiment Analysis of Lyme Disease in Scientific Literature,No.,1,"""No evidence""",2023,2023-02-07T01:15:05Z,,,
arXIv2023,Data Selection for Language Models via Importance Resampling,No.,1,"""No evidence""",2023,2023-02-06T23:57:56Z,,,
arXIv2023,Techniques to Improve Neural Math Word Problem Solvers,No.,1,"""No evidence""",2023,2023-02-06T22:41:51Z,,,
arXIv2023,Context-Gloss Augmentation for Improving Arabic Target Sense Verification,No.,1,"""No evidence""",2023,2023-02-06T21:24:02Z,,,
arXIv2023,Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds,No.,1,"""No evidence""",2023,2023-02-06T18:27:11Z,,,
arXIv2023,Controllable Lexical Simplification for English,No.,1,"""No evidence""",2023,2023-02-06T16:09:27Z,,,
arXIv2023,Chain of Hindsight Aligns Language Models with Feedback,No.,1,"""No evidence""",2023,2023-02-06T10:28:16Z,,,
arXIv2023,CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets,No.,1,"""No evidence""",2023,2023-02-06T03:59:15Z,,,
arXIv2023,FineDeb: A Debiasing Framework for Language Models,No.,1,"""No evidence""",2023,2023-02-05T18:35:21Z,,,
arXIv2023,Regulating ChatGPT and other Large Generative AI Models,No.,1,"""No evidence""",2023,2023-02-05T08:56:45Z,,,
arXIv2023,PubGraph: A Large-Scale Scientific Knowledge Graph,No.,1,"""No evidence""",2023,2023-02-04T20:03:55Z,,,
arXIv2023,A New cross-domain strategy based XAI models for fake news detection,No.,1,"""No evidence""",2023,2023-02-04T07:36:17Z,,,
arXIv2023,Knowledge Graph Completion Method Combined With Adaptive Enhanced Semantic Information,No.,1,"""No evidence""",2023,2023-02-04T07:01:02Z,,,
arXIv2023,REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers,No.,1,"""No evidence""",2023,2023-02-04T00:32:50Z,,,
arXIv2023,Witscript: A System for Generating Improvised Jokes in a Conversation,No.,1,"""No evidence""",2023,2023-02-03T21:30:34Z,,,
arXIv2023,Measuring The Impact Of Programming Language Distribution,No.,1,"""No evidence""",2023,2023-02-03T19:47:22Z,,,
arXIv2023,Exploring the Cognitive Dynamics of Artificial Intelligence in the Post-COVID-19 and Learning 3.0 Era: A Case Study of ChatGPT,No.,1,"""No evidence""",2023,2023-02-03T19:25:13Z,,,
arXIv2023,GLADIS: A General and Large Acronym Disambiguation Benchmark,No.,1,"""No evidence""",2023,2023-02-03T17:07:23Z,,,
arXIv2023,A Case Study for Compliance as Code with Graphs and Language Models: Public release of the Regulatory Knowledge Graph,No.,1,"""No evidence""",2023,2023-02-03T16:37:08Z,,,
arXIv2023,Mitigating Data Scarcity for Large Language Models,No.,1,"""No evidence""",2023,2023-02-03T15:17:53Z,,,
arXIv2023,Controlling for Stereotypes in Multimodal Language Model Evaluation,No.,1,"""No evidence""",2023,2023-02-03T07:27:50Z,,,
arXIv2023,Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN,No.,1,"""No evidence""",2023,2023-02-03T06:22:18Z,,,
arXIv2023,Optimality of Thompson Sampling with Noninformative Priors for Pareto Bandits,No.,1,"""No evidence""",2023,2023-02-03T04:47:14Z,,,
arXIv2023,Revisiting Intermediate Layer Distillation for Compressing Language Models: An Overfitting Perspective,No.,1,"""No evidence""",2023,2023-02-03T04:09:22Z,,,
arXIv2023,ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics,No.,1,"""No evidence""",2023,2023-02-03T02:31:12Z,,,
arXIv2023,Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?,No.,1,"""No evidence""",2023,2023-02-03T01:28:25Z,,,
arXIv2023,Accelerating Large Language Model Decoding with Speculative Sampling,No.,1,"""No evidence""",2023,2023-02-02T18:44:11Z,,,
arXIv2023,Large language models predict human sensory judgments across six modalities,No.,1,"""No evidence""",2023,2023-02-02T18:32:46Z,,,
arXIv2023,Mnemosyne: Learning to Train Transformers with Transformers,No.,1,"""No evidence""",2023,2023-02-02T14:40:28Z,,,
arXIv2023,Semantic Coherence Markers for the Early Diagnosis of the Alzheimer Disease,No.,1,"""No evidence""",2023,2023-02-02T11:40:16Z,,,
arXIv2023,Visual Imitation Learning with Patch Rewards,No.,1,"""No evidence""",2023,2023-02-02T09:13:10Z,,,
arXIv2023,Resilient Binary Neural Network,No.,1,"""No evidence""",2023,2023-02-02T08:51:07Z,,,
arXIv2023,TransFool: An Adversarial Attack against Neural Machine Translation Models,No.,1,"""No evidence""",2023,2023-02-02T08:35:34Z,,,
arXIv2023,Improving Rare Words Recognition through Homophone Extension and Unified Writing for Low-resource Cantonese Speech Recognition,No.,1,"""No evidence""",2023,2023-02-02T02:46:32Z,,,
arXIv2023,Creating a Large Language Model of a Philosopher,No.,1,"""No evidence""",2023,2023-02-02T01:10:26Z,,,
arXIv2023,A Survey of Deep Learning: From Activations to Transformers,No.,1,"""No evidence""",2023,2023-02-01T19:34:55Z,,,
arXIv2023,Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data,No.,1,"""No evidence""",2023,2023-02-01T18:59:36Z,,,
arXIv2023,Clinical Decision Transformer: Intended Treatment Recommendation through Goal Prompting,No.,1,"""No evidence""",2023,2023-02-01T17:26:01Z,,,
arXIv2023,An Empirical Study on the Transferability of Transformer Modules in Parameter-Efficient Fine-Tuning,No.,1,"""No evidence""",2023,2023-02-01T11:20:18Z,,,
arXIv2023,"Netizens, Academicians, and Information Professionals' Opinions About AI With Special Reference To ChatGPT",No.,1,"""No evidence""",2023,2023-02-01T10:59:04Z,,,
arXIv2023,Grading Conversational Responses Of Chatbots,No.,1,"""No evidence""",2023,2023-02-01T02:54:43Z,,,
arXIv2023,In-Context Retrieval-Augmented Language Models,No.,1,"""No evidence""",2023,2023-01-31T20:26:16Z,,,
arXIv2023,Mathematical Capabilities of ChatGPT,No.,1,"""No evidence""",2023,2023-01-31T18:59:03Z,,,
arXIv2023,Grounding Language Models to Images for Multimodal Inputs and Outputs,No.,1,"""No evidence""",2023,2023-01-31T18:33:44Z,,,
arXIv2023,The Touch23-ValueEval Dataset for Identifying Human Values behind Arguments,No.,1,"""No evidence""",2023,2023-01-31T17:15:33Z,,,
arXIv2023,Towards fully covariant machine learning,No.,1,"""No evidence""",2023,2023-01-31T16:01:12Z,,,
arXIv2023,TopoBERT: Plug and Play Toponym Recognition Module Harnessing Fine-tuned BERT,No.,1,"""No evidence""",2023,2023-01-31T13:44:34Z,,,
arXIv2023,Faithful Chain-of-Thought Reasoning,No.,1,"""No evidence""",2023,2023-01-31T03:04:26Z,,,
arXIv2023,"Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference",No.,1,"""No evidence""",2023,2023-01-30T23:26:33Z,,,
arXIv2023,Contextual Dynamic Prompting for Response Generation in Task-oriented Dialog Systems,No.,1,"""No evidence""",2023,2023-01-30T20:26:02Z,,,
arXIv2023,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,No.,1,"""No evidence""",2023,2023-01-30T18:37:32Z,,,
arXIv2023,Representation biases in sentence transformers,No.,1,"""No evidence""",2023,2023-01-30T16:35:23Z,,,
arXIv2023,"The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest",No.,1,"""No evidence""",2023,2023-01-30T15:29:40Z,,,
arXIv2023,Direct Preference-based Policy Optimization without Reward Modeling,No.,1,"""No evidence""",2023,2023-01-30T12:51:13Z,,,
arXIv2023,Crawling the Internal Knowledge-Base of Language Models,No.,1,"""No evidence""",2023,2023-01-30T12:03:36Z,,,
arXIv2023,ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text,No.,1,"""No evidence""",2023,2023-01-30T08:06:08Z,,,
arXIv2023,REPLUG: Retrieval-Augmented Black-Box Language Models,No.,1,"""No evidence""",2023,2023-01-30T04:18:09Z,,,
arXIv2023,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,No.,1,"""No evidence""",2023,2023-01-30T00:56:51Z,,,
arXIv2023,Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining,No.,1,"""No evidence""",2023,2023-01-30T00:53:50Z,,,
arXIv2023,Unifying Molecular and Textual Representations via Multi-task Language Modelling,No.,1,"""No evidence""",2023,2023-01-29T23:56:45Z,,,
arXIv2023,BERT-based Authorship Attribution on the Romanian Dataset called ROST,No.,1,"""No evidence""",2023,2023-01-29T17:37:29Z,,,
arXIv2023,Semantics-enhanced Temporal Graph Networks for Content Popularity Prediction,No.,1,"""No evidence""",2023,2023-01-29T04:17:32Z,,,
arXIv2023,Semantic Tagging with LSTM-CRF,No.,1,"""No evidence""",2023,2023-01-28T14:06:17Z,,,
arXIv2023,Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset,No.,1,"""No evidence""",2023,2023-01-28T03:10:33Z,,,
arXIv2023,Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation,No.,1,"""No evidence""",2023,2023-01-27T22:02:27Z,,,
arXIv2023,Prompt-Based Editing for Text Style Transfer,No.,1,"""No evidence""",2023,2023-01-27T21:31:14Z,,,
arXIv2023,Byte Pair Encoding for Symbolic Music,No.,1,"""No evidence""",2023,2023-01-27T20:22:18Z,,,
arXIv2023,SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient,No.,1,"""No evidence""",2023,2023-01-27T18:55:19Z,,,
arXIv2023,Can We Use Probing to Better Understand Fine-tuning and Knowledge Distillation of the BERT NLU?,No.,1,"""No evidence""",2023,2023-01-27T12:56:29Z,,,
arXIv2023,Neural Episodic Control with State Abstraction,No.,1,"""No evidence""",2023,2023-01-27T01:55:05Z,,,
arXIv2023,Talk the Walk: Synthetic Data Generation for Conversational Music Recommendation,No.,1,"""No evidence""",2023,2023-01-27T01:54:16Z,,,
arXIv2023,A benchmark for toxic comment classification on Civil Comments dataset,No.,1,"""No evidence""",2023,2023-01-26T14:25:09Z,,,
arXIv2023,Time-sensitive Learning for Heterogeneous Federated Edge Intelligence,No.,1,"""No evidence""",2023,2023-01-26T08:13:22Z,,,
arXIv2023,Affective Faces for Goal-Driven Dyadic Communication,No.,1,"""No evidence""",2023,2023-01-26T05:00:09Z,,,
arXIv2023,Joint action loss for proximal policy optimization,No.,1,"""No evidence""",2023,2023-01-26T03:42:29Z,,,
arXIv2023,Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning,No.,1,"""No evidence""",2023,2023-01-26T03:01:59Z,,,
arXIv2023,Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning,No.,1,"""No evidence""",2023,2023-01-26T01:06:46Z,,,
arXIv2023,Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content,No.,1,"""No evidence""",2023,2023-01-25T23:32:32Z,,,
arXIv2023,A Study on FGSM Adversarial Training for Neural Retrieval,No.,1,"""No evidence""",2023,2023-01-25T13:28:54Z,,,
arXIv2023,FewShotTextGCN: K-hop neighborhood regularization for few-shot learning on graphs,No.,1,"""No evidence""",2023,2023-01-25T09:30:32Z,,,
arXIv2023,XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models,No.,1,"""No evidence""",2023,2023-01-25T09:15:17Z,,,
arXIv2023,ViDeBERTa: A powerful pre-trained language model for Vietnamese,No.,1,"""No evidence""",2023,2023-01-25T07:26:54Z,,,
arXIv2023,Editing Language Model-based Knowledge Graph Embeddings,No.,1,"""No evidence""",2023,2023-01-25T04:45:06Z,,,
arXIv2023,Semi-Automated Construction of Food Composition Knowledge Base,No.,1,"""No evidence""",2023,2023-01-24T22:08:49Z,,,
arXIv2023,Large language models can segment narrative events similarly to humans,No.,1,"""No evidence""",2023,2023-01-24T20:34:37Z,,,
arXIv2023,PushWorld: A benchmark for manipulation planning with tools and movable obstacles,No.,1,"""No evidence""",2023,2023-01-24T20:20:17Z,,,
arXIv2023,Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis,No.,1,"""No evidence""",2023,2023-01-24T19:23:38Z,,,
arXIv2023,Large Language Models as Fiduciaries: A Case Study Toward Robustly Communicating With Artificial Intelligence Through Legal Standards,No.,1,"""No evidence""",2023,2023-01-24T16:03:20Z,,,
arXIv2023,Multitask Instruction-based Prompting for Fallacy Recognition,No.,1,"""No evidence""",2023,2023-01-24T13:39:23Z,,,
arXIv2023,The Next Chapter: A Study of Large Language Models in Storytelling,No.,1,"""No evidence""",2023,2023-01-24T02:44:02Z,,,
arXIv2023,AI model GPT-3 (dis)informs us better than humans,No.,1,"""No evidence""",2023,2023-01-23T14:36:29Z,,,
arXIv2023,Lexi: Self-Supervised Learning of the UI Language,No.,1,"""No evidence""",2023,2023-01-23T09:05:49Z,,,
arXIv2023,SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis,No.,1,"""No evidence""",2023,2023-01-22T20:59:40Z,,,
arXIv2023,DiffSDS: A language diffusion model for protein backbone inpainting under geometric conditions and constraints,No.,1,"""No evidence""",2023,2023-01-22T05:07:54Z,,,
arXIv2023,Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?,No.,1,"""No evidence""",2023,2023-01-21T21:58:00Z,,,
arXIv2023,REDAffectiveLM: Leveraging Affect Enriched Embedding and Transformer-based Neural Language Model for Readers' Emotion Detection,No.,1,"""No evidence""",2023,2023-01-21T19:28:25Z,,,
arXIv2023,Adapting a Language Model While Preserving its General Knowledge,No.,1,"""No evidence""",2023,2023-01-21T17:57:53Z,,,
arXIv2023,SuperScaler: Supporting Flexible DNN Parallelization via a Unified Abstraction,No.,1,"""No evidence""",2023,2023-01-21T17:47:55Z,,,
arXIv2023,Exploring Methods for Building Dialects-Mandarin Code-Mixing Corpora: A Case Study in Taiwanese Hokkien,No.,1,"""No evidence""",2023,2023-01-21T11:04:20Z,,,
arXIv2023,Stress Test for BERT and Deep Models: Predicting Words from Italian Poetry,No.,1,"""No evidence""",2023,2023-01-21T09:44:19Z,,,
arXIv2023,Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning,No.,1,"""No evidence""",2023,2023-01-21T08:18:11Z,,,
arXIv2023,A Trustworthiness Score to Evaluate DNN Predictions,No.,1,"""No evidence""",2023,2023-01-21T00:48:18Z,,,
arXIv2023,Asynchronous Deep Double Duelling Q-Learning for Trading-Signal Execution in Limit Order Book Markets,No.,1,"""No evidence""",2023,2023-01-20T17:19:18Z,,,
arXIv2023,Data Augmentation for Modeling Human Personality: The Dexter Machine,No.,1,"""No evidence""",2023,2023-01-20T14:36:22Z,,,
arXIv2023,Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine,No.,1,"""No evidence""",2023,2023-01-20T08:51:36Z,,,
arXIv2023,Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning,No.,1,"""No evidence""",2023,2023-01-20T05:39:26Z,,,
arXIv2023,JCSE: Contrastive Learning of Japanese Sentence Embeddings and Its Applications,No.,1,"""No evidence""",2023,2023-01-19T17:41:46Z,,,
arXIv2023,From English to More Languages: Parameter-Efficient Model Reprogramming for Cross-Lingual Speech Recognition,No.,1,"""No evidence""",2023,2023-01-19T02:37:56Z,,,
arXIv2023,CLIPTER: Looking at the Bigger Picture in Scene Text Recognition,No.,1,"""No evidence""",2023,2023-01-18T12:16:19Z,,,
arXIv2023,Sharp Eyes: A Salient Object Detector Working The Same Way as Human Visual Characteristics,No.,1,"""No evidence""",2023,2023-01-18T11:00:45Z,,,
arXIv2023,Class Enhancement Losses with Pseudo Labels for Zero-shot Semantic Segmentation,No.,1,"""No evidence""",2023,2023-01-18T06:55:02Z,,,
arXIv2023,Face Recognition in the age of CLIP & Billion image datasets,No.,1,"""No evidence""",2023,2023-01-18T05:34:57Z,,,
arXIv2023,Prompting Large Language Model for Machine Translation: A Case Study,No.,1,"""No evidence""",2023,2023-01-17T18:32:06Z,,,
arXIv2023,Which Model Shall I Choose? Cost/Quality Trade-offs for Text Classification Tasks,No.,1,"""No evidence""",2023,2023-01-17T16:51:58Z,,,
arXIv2023,Syllable Subword Tokens for Open Vocabulary Speech Recognition in Malayalam,No.,1,"""No evidence""",2023,2023-01-17T07:29:47Z,,,
arXIv2023,VaxxHesitancy: A Dataset for Studying Hesitancy towards COVID-19 Vaccination on Twitter,No.,1,"""No evidence""",2023,2023-01-17T02:00:31Z,,,
arXIv2023,PromptShots at the FinNLP-2022 ERAI Tasks: Pairwise Comparison and Unsupervised Ranking,No.,1,"""No evidence""",2023,2023-01-16T20:54:27Z,,,
arXIv2023,Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling,No.,1,"""No evidence""",2023,2023-01-16T19:04:45Z,,,
arXIv2023,Using Kaldi for Automatic Speech Recognition of Conversational Austrian German,No.,1,"""No evidence""",2023,2023-01-16T15:28:28Z,,,
arXIv2023,Continuous Trajectory Generation Based on Two-Stage GAN,No.,1,"""No evidence""",2023,2023-01-16T09:54:02Z,,,
arXIv2023,An Error-Guided Correction Model for Chinese Spelling Error Correction,No.,1,"""No evidence""",2023,2023-01-16T09:27:45Z,,,
arXIv2023,Computational Assessment of Hyperpartisanship in News Titles,No.,1,"""No evidence""",2023,2023-01-16T05:56:58Z,,,
arXIv2023,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,No.,1,"""No evidence""",2023,2023-01-15T09:34:42Z,,,
arXIv2023,tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation,No.,1,"""No evidence""",2023,2023-01-14T16:38:04Z,,,
arXIv2023,The moral authority of ChatGPT,No.,1,"""No evidence""",2023,2023-01-13T20:24:38Z,,,
arXIv2023,A Case Study in Engineering a Conversational Programming Assistant's Persona,No.,1,"""No evidence""",2023,2023-01-13T14:48:47Z,,,
arXIv2023,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,No.,1,"""No evidence""",2023,2023-01-13T12:01:18Z,,,
arXIv2023,Blind Judgement: Agent-Based Supreme Court Modelling With GPT,No.,1,"""No evidence""",2023,2023-01-12T23:07:55Z,,,
arXIv2023,SemPPL: Predicting pseudo-labels for better contrastive representations,No.,1,"""No evidence""",2023,2023-01-12T17:24:08Z,,,
arXIv2023,Improving Inference Performance of Machine Learning with the Divide-and-Conquer Principle,No.,1,"""No evidence""",2023,2023-01-12T15:55:12Z,,,
arXIv2023,Tracr: Compiled Transformers as a Laboratory for Interpretability,No.,1,"""No evidence""",2023,2023-01-12T14:59:19Z,,,
arXIv2023,A Cohesive Distillation Architecture for Neural Language Models,No.,1,"""No evidence""",2023,2023-01-12T08:01:53Z,,,
arXIv2023,KAER: A Knowledge Augmented Pre-Trained Language Model for Entity Resolution,No.,1,"""No evidence""",2023,2023-01-12T00:15:40Z,,,
arXIv2023,NarrowBERT: Accelerating Masked Language Model Pretraining and Inference,No.,1,"""No evidence""",2023,2023-01-11T23:45:50Z,,,
arXIv2023,ChatGPT is not all you need. A State of the Art Review of large Generative AI models,No.,1,"""No evidence""",2023,2023-01-11T15:48:36Z,,,
arXIv2023,GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities,No.,1,"""No evidence""",2023,2023-01-11T11:30:42Z,,,
arXIv2023,Topics in Contextualised Attention Embeddings,No.,1,"""No evidence""",2023,2023-01-11T07:26:19Z,,,
arXIv2023,There is No Big Brother or Small Brother: Knowledge Infusion in Language Models for Link Prediction and Question Answering,No.,1,"""No evidence""",2023,2023-01-10T14:59:33Z,,,
arXIv2023,Spectral Cross-Domain Neural Network with Soft-adaptive Threshold Spectral Enhancement,No.,1,"""No evidence""",2023,2023-01-10T14:23:43Z,,,
arXIv2023,Chatbots in a Honeypot World,No.,1,"""No evidence""",2023,2023-01-10T03:43:35Z,,,
arXIv2023,Memory Augmented Large Language Models are Computationally Universal,No.,1,"""No evidence""",2023,2023-01-10T02:37:44Z,,,
arXIv2023,Transfer learning for conflict and duplicate detection in software requirement pairs,No.,1,"""No evidence""",2023,2023-01-09T22:47:12Z,,,
arXIv2023,Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling,No.,1,"""No evidence""",2023,2023-01-09T18:59:50Z,,,
arXIv2023,ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization,No.,1,"""No evidence""",2023,2023-01-09T15:12:50Z,,,
arXIv2023,FullStop:Punctuation and Segmentation Prediction for Dutch with Transformers,No.,1,"""No evidence""",2023,2023-01-09T13:12:05Z,,,
arXIv2023,SantaCoder: don't reach for the stars!,No.,1,"""No evidence""",2023,2023-01-09T10:52:35Z,,,
arXIv2023,Online Fake Review Detection Using Supervised Machine Learning And BERT Model,No.,1,"""No evidence""",2023,2023-01-09T09:40:56Z,,,
arXIv2023,Automatic Generation of German Drama Texts Using Fine Tuned GPT-2 Models,No.,1,"""No evidence""",2023,2023-01-08T23:12:46Z,,,
arXIv2023,Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method,No.,1,"""No evidence""",2023,2023-01-08T12:33:58Z,,,
arXIv2023,InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers,No.,1,"""No evidence""",2023,2023-01-08T08:03:46Z,,,
arXIv2023,Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,No.,1,"""No evidence""",2023,2023-01-07T17:24:11Z,,,
arXIv2023,RLAS-BIABC: A Reinforcement Learning-Based Answer Selection Using the BERT Model Boosted by an Improved ABC Algorithm,No.,1,"""No evidence""",2023,2023-01-07T08:33:05Z,,,
arXIv2023,Generative Antibody Design for Complementary Chain Pairing Sequences through Encoder-Decoder Language Model,No.,1,"""No evidence""",2023,2023-01-06T23:34:52Z,,,
arXIv2023,Topics as Entity Clusters: Entity-based Topics from Language Models and Graph Neural Networks,No.,1,"""No evidence""",2023,2023-01-06T10:54:54Z,,,
arXIv2023,Sequentially Controlled Text Generation,No.,1,"""No evidence""",2023,2023-01-05T21:23:51Z,,,
arXIv2023,Reprogramming Pretrained Language Models for Protein Sequence Representation Learning,No.,1,"""No evidence""",2023,2023-01-05T15:55:18Z,,,
arXIv2023,Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,No.,1,"""No evidence""",2023,2023-01-05T15:37:15Z,,,
arXIv2023,Emotion-Cause Pair Extraction as Question Answering,No.,1,"""No evidence""",2023,2023-01-05T09:33:41Z,,,
arXIv2023,Adaptively Clustering Neighbor Elements for Image Captioning,No.,1,"""No evidence""",2023,2023-01-05T08:37:36Z,,,
arXIv2023,Learning Trajectory-Word Alignments for Video-Language Tasks,No.,1,"""No evidence""",2023,2023-01-05T08:21:01Z,,,
arXIv2023,Critical Perspectives: A Benchmark Revealing Pitfalls in PerspectiveAPI,No.,1,"""No evidence""",2023,2023-01-05T02:12:47Z,,,
arXIv2023,PMP: Privacy-Aware Matrix Profile against Sensitive Pattern Inference for Time Series,No.,1,"""No evidence""",2023,2023-01-04T22:11:38Z,,,
arXIv2023,InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval,No.,1,"""No evidence""",2023,2023-01-04T20:58:43Z,,,
arXIv2023,UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?,No.,1,"""No evidence""",2023,2023-01-04T18:59:20Z,,,
arXIv2023,Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible AI Engineering Approach,No.,1,"""No evidence""",2023,2023-01-03T23:11:03Z,,,
arXIv2023,PIE-QG: Paraphrased Information Extraction for Unsupervised Question Generation from Small Corpora,No.,1,"""No evidence""",2023,2023-01-03T12:20:51Z,,,
arXIv2023,ClusTop: An unsupervised and integrated text clustering and topic extraction framework,No.,1,"""No evidence""",2023,2023-01-03T03:26:26Z,,,
arXIv2023,Understanding Political Polarisation using Language Models: A dataset and method,No.,1,"""No evidence""",2023,2023-01-02T22:15:04Z,,,
arXIv2023,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,No.,1,"""No evidence""",2023,2023-01-02T17:48:56Z,,,
arXIv2023,Using Active Learning Methods to Strategically Select Essays for Automated Scoring,No.,1,"""No evidence""",2023,2023-01-02T12:46:10Z,,,
arXIv2023,Russia-Ukraine war: Modeling and Clustering the Sentiments Trends of Various Countries,No.,1,"""No evidence""",2023,2023-01-02T11:32:47Z,,,
arXIv2023,Leveraging Semantic Representations Combined with Contextual Word Representations for Recognizing Textual Entailment in Vietnamese,No.,1,"""No evidence""",2023,2023-01-01T15:13:25Z,,,
arXIv2023,Chatbots as Problem Solvers: Playing Twenty Questions with Role Reversals,No.,1,"""No evidence""",2023,2023-01-01T03:04:04Z,,,
arXIv2023,Floods Relevancy and Identification of Location from Twitter Posts using NLP Techniques,No.,1,"""No evidence""",2023,2023-01-01T01:36:32Z,,,
arXIv2023,HSC-GPT: A Large Language Model for Human Settlements Construction,Yes.,2,"""These factors lead to limitations when applying general generative AI in this field, further exacerbated by a lack of high-quality data for model training.""",2023,2023-12-31T13:56:15Z,,,
arXIv2023,BatchEval: Towards Human-like Text Evaluation,Yes.,3,"""current sample-wise evaluation paradigm suffers from the following issues",2023,2023-12-31T09:34:51Z,,,
arXIv2023,Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles,Yes.,3,"""However, a notable challenge in RLHF is overoptimization, where beyond a certain threshold, the pursuit of higher rewards leads to a decline in human preferences.""",2023,2023-12-30T14:14:14Z,,,
arXIv2023,Is Knowledge All Large Language Models Needed for Causal Reasoning?,Yes.,3,"""On the contrary, in the absence of knowledge, LLMs still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations.""",2023,2023-12-30T04:51:46Z,,,
arXIv2023,ChatEd: A Chatbot Leveraging ChatGPT for an Enhanced Learning Experience in Higher Education,Yes.,3,"""the possibility of generating incorrect, biased, or unhelpful answers are a key challenge to resolve when deploying LLMs in an education context.""",2023,2023-12-29T19:11:55Z,,,
arXIv2023,Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning,Yes.,3,"""1) despite avoiding raw data exposure, there is a risk of inferring sensitive information from model outputs, and 2) federated learning for LLMs incurs notable communication overhead.""",2023,2023-12-29T06:50:38Z,,,
arXIv2023,Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters,Yes.,3,"""these studies have not always properly accounted for the sensitivity of LLMs' behavior to hyperparameters and variations in the prompt.""",2023,2023-12-29T05:19:11Z,,,
arXIv2023,LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model,Yes.,3,"""While LISA effectively bridges the gap between segmentation and large language models to enable reasoning segmentation, it poses certain limitations",2023,2023-12-28T18:58:33Z,,,
arXIv2023,Fast Inference of Mixture-of-Experts Language Models with Offloading,Yes.,2,"""Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs.""",2023,2023-12-28T18:58:13Z,,,
arXIv2023,Large Language Model for Causal Decision Making,Yes.,3,"""However, their capability to inference based on user-specified structured data and knowledge in corpus-rare concepts like causal decision-making is still limited.""",2023,2023-12-28T16:59:06Z,,,
arXIv2023,GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension,Yes.,3,"""While Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated exceptional proficiency in natural language processing, their efficacy in addressing complex, multifaceted tasks remains limited.""",2023,2023-12-28T15:47:30Z,,,
arXIv2023,Experiential Co-Learning of Software-Developing Agents,Yes.,3,"""However, these agents often approach a diverse range of tasks in isolation, without benefiting from past experiences. This isolation can lead to repeated mistakes and inefficient trials in task solving.""",2023,2023-12-28T13:50:42Z,,,
arXIv2023,AI Content Self-Detection for Transformer-based Large Language Models,Yes.,3,"""Existing plagiarism detection systems can trace the source of submitted text but are not yet equipped with methods to accurately detect AI-generated text."" and ""Results reveal varying capabilities of AI systems to identify their generated text.""",2023,2023-12-28T10:08:57Z,,,
arXIv2023,Large Language Models for Conducting Advanced Text Analytics Information Systems Research,Yes.,2,"""We also outline potential challenges and limitations in adopting LLMs for IS.""",2023,2023-12-27T19:49:00Z,,,
arXIv2023,Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model,Yes.,3,"""traditional BERT-based methods still suffer from two limitations. First, although previous works have identified that explicit prior knowledge like Part-Of-Speech (POS) tagging can benefit in the CSC task, they neglected the fact that spelling errors inherent in CSC data can lead to incorrect",2023,2023-12-27T16:11:07Z,,,
arXIv2023,"Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges",Yes.,3,"""Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required."" and ""Finally, we explore the challenges and future directions, providing new research opportunities and perspectives on adapting LLMs for education.""",2023,2023-12-27T14:37:32Z,,,
arXIv2023,LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,Yes.,3,"""we summarize the challenges and opportunities introduced by LLMs to provide a complete view of this design pattern for more discussions.""",2023,2023-12-26T23:08:38Z,,,
arXIv2023,Can ChatGPT Read Who You Are?,Yes.,3,"""We also uncover a 'positivity bias' in ChatGPT's assessments across all personality dimensions"" and ""highlighting both the potential and limitations of using large language models for personality inference.""",2023,2023-12-26T14:43:04Z,,,
arXIv2023,A Prompt Learning Framework for Source Code Summarization,Yes.,3,"""instruction prompting involves designing crafted prompts for zero-shot learning or selecting appropriate samples for few-shot learning and requires users to have professional domain knowledge, while task-oriented fine-tuning requires high training costs.""",2023,2023-12-26T14:37:55Z,,,
arXIv2023,SecQA: A Concise Question-Answering Dataset for Evaluating Large Language Models in Computer Security,Yes.,3,"""Our results, encapsulated in the SecQA v1 and v2 datasets, highlight the varying capabilities and limitations of these models in the computer security context.""",2023,2023-12-26T00:59:30Z,,,
arXIv2023,EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data,Yes.,3,"""applying these models to specific domains still poses significant challenges, such as lack of domain knowledge, limited capacity to leverage domain knowledge and inadequate adaptation to domain-specific data formats.""",2023,2023-12-25T11:31:47Z,,,
arXIv2023,Instruction Fusion: Advancing Prompt Evolution through Hybridization,Yes.,2,"""Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks.""",2023,2023-12-25T11:00:37Z,,,
arXIv2023,ESGReveal: An LLM-based approach for extracting structured data from ESG reports,Yes.,2,"""While current iterations of ESGReveal do not process pictorial information, a functionality intended for future enhancement, the study calls for continued research to further develop and compare the analytical capabilities of various LLMs.""",2023,2023-12-25T06:44:32Z,,,
arXIv2023,Privacy-Preserved Neural Graph Databases,Yes.,1,"""In the era of large language models (LLMs), efficient and accurate data retrieval has become increasingly crucial for the use of domain-specific or private data in the retrieval augmented generation (RAG).""",2023,2023-12-25T02:32:05Z,,,
arXIv2023,A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators,Yes.,3,"""Yet, existing works on utilizing LLMs for automatic dialogue evaluation are limited in their scope in terms of the number of meta-evaluation datasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains inconclusive how effective these LLMs are.""",2023,2023-12-24T04:50:57Z,,,
arXIv2023,PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs,Yes.,2,"""with the rise of Large Language Models (LLMs), full retraining has become infeasible due to memory and compute constraints.""",2023,2023-12-23T11:45:22Z,,,
arXIv2023,PokeMQA: Programmable knowledge editing for Multi-hop Question Answering,Yes.,3,"""the coupling of these functionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking.""",2023,2023-12-23T08:32:13Z,,,
arXIv2023,ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and Uncertainty in Zeroth-order Optimization,Yes.,3,"""the simulated perturbation stochastic approximation for gradient estimate in MeZO leads to severe oscillations and incurs a substantial time overhead. Moreover, without momentum regularization, MeZO shows severe over-fitting problems.""",2023,2023-12-23T07:46:31Z,,,
arXIv2023,Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning,Yes.,3,"""Large language models (LLMs) emerged as a fundamental way to incorporate cross-domain knowledge into AI agents but lack crucial learning and adaptation toward specific decision problems.""",2023,2023-12-22T17:57:57Z,,,
arXIv2023,"Plan, Posture and Go: Towards Open-World Text-to-Motion Generation",Yes.,2,"""Conventional text-to-motion generation methods are usually trained on limited text-motion pairs, making them hard to generalize to open-world scenarios."" and ""The motion planner instructs Large Language Models (LLMs) to generate a sequence of scripts describing the key postures in the target motion.""",2023,2023-12-22T17:02:45Z,,,
arXIv2023,Theory of Hallucinations based on Equivariance,Yes.,3,"""Hallucinations in contemporary large language models are often attributed to a misunderstanding of real-world social relationships.""",2023,2023-12-22T08:08:45Z,,,
arXIv2023,Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs,Yes.,3,"""However, despite the size of the LLM, most existing models struggle to produce zero-shot explanations reliably.""",2023,2023-12-22T00:30:10Z,,,
arXIv2023,Parameter Efficient Tuning Allows Scalable Personalization of LLMs for Text Entry: A Case Study on Abbreviation Expansion,Yes.,2,"""our case study with a deployed 8B parameter LLM on a real user living with ALS, and experiments on movie character personalization indicates that (1) customization may be necessary in some scenarios and prompt-tuning generalizes well to those, (",2023,2023-12-21T22:52:44Z,,,
arXIv2023,LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding,Yes.,2,"""While these models are powerful, they have not yet been developed to comprehend the more challenging 3D physical scenes, especially when it comes to the sparse outdoor LiDAR data.""",2023,2023-12-21T17:52:12Z,,,
arXIv2023,T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step,Yes.,2,"""Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored.""",2023,2023-12-21T17:02:06Z,,,
arXIv2023,AsyncMLD: Asynchronous Multi-LLM Framework for Dialogue Recommendation System,Yes.,3,"""we still need help with the utterance content's effectiveness and the efficiency of its output speed, even if using LLM.""",2023,2023-12-21T15:12:59Z,,,
arXIv2023,L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs,Yes.,1,"""Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing.""",2023,2023-12-21T01:47:49Z,,,
arXIv2023,ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation,Yes.,3,"""Our experimental results reveal that our GUI Parser and Reasoning mechanism outshine existing methods in performance. Nevertheless, the potential remains substantial, with the best model attaining only a 46% success rate on our benchmark. We conclude with a thorough analysis of the current methods' limitations, setting the stage for future breakthroughs in this domain.""",2023,2023-12-20T15:28:38Z,,,
arXIv2023,Retrieval-augmented Multilingual Knowledge Editing,Yes.,3,"""Knowledge represented in Large Language Models (LLMs) is quite often incorrect and can also become obsolete over time. Updating knowledge via fine-tuning is computationally resource-hungry and not reliable.""",2023,2023-12-20T14:08:58Z,,,
arXIv2023,Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors,Yes.,3,"""Existing zero-shot detectors, typically designed for specific tasks or topics, often assume uniform testing scenarios, limiting their practicality.""",2023,2023-12-20T10:53:53Z,,,
arXIv2023,Testing the Segment Anything Model on radiology data,No.,1,The abstract discusses the Segment Anything Model (SAM) and its application to MRI data but does not mention large language models (LLMs) or their limitations.,2023,2023-12-20T09:45:21Z,,,
arXIv2023,Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data,Yes.,3,"""Large Language Models (LLMs) have performed well on various reasoning tasks, but their inaccessibility and numerous parameters hinder wide application in practice."" and ""In some cases, however, LLMs may produce incorrect reasoning chains, especially when facing complex mathematical problems.""",2023,2023-12-20T08:28:36Z,,,
arXIv2023,A Case Study on Test Case Construction with Large Language Models: Unveiling Practical Insights and Challenges,Yes.,3,"""Additionally, delves into challenges such as model interpretability and adaptation to diverse software contexts.""",2023,2023-12-19T20:59:02Z,,,
arXIv2023,Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes,Yes.,2,"""However, not all the data generated by LLMs will improve downstream utility, as for any generative model.""",2023,2023-12-19T12:34:46Z,,,
arXIv2023,A Performance Evaluation of a Quantized Large Language Model on Various Smartphones,Yes.,1,"""This paper explores the feasibility and performance of on-device large language model (LLM) inference on various Apple iPhone models.""",2023,2023-12-19T10:19:39Z,,,
arXIv2023,Climate Change from Large Language Models,Yes.,3,"""we evaluate several state-of-the-art LLMs and find that their knowledge falls short in terms of timeliness.""",2023,2023-12-19T09:26:46Z,,,
arXIv2023,Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives,Yes.,3,"""We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation.""",2023,2023-12-19T09:06:45Z,,,
arXIv2023,Text-Conditioned Resampler For Long Form Video Understanding,Yes.,1,"""In this paper we present a text-conditioned video resampler (TCR) module that uses a pre-trained and frozen visual encoder and large language model (LLM) to process long video sequences for a task.""",2023,2023-12-19T06:42:47Z,,,
arXIv2023,Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models,Yes.,3,"""Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected",2023,2023-12-19T06:28:43Z,,,
arXIv2023,Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction,Yes.,1,"""a Large Language Model (LLM)-based framework for difficulty prediction.""",2023,2023-12-19T06:26:25Z,,,
arXIv2023,Efficient LLM inference solution on Intel GPU,Yes.,3,"""LLMs are usually complicatedly designed in model structure with massive operations and perform inference in the auto-regressive mode, making it a challenging task to design a system with high efficiency.""",2023,2023-12-19T05:40:43Z,,,
arXIv2023,Indoor and Outdoor 3D Scene Graph Generation via Language-Enabled Spatial Ontologies,Yes.,1,"""In particular, we use a Large Language Model (LLM) to build such an ontology, thus largely reducing the amount of manual effort required.""",2023,2023-12-18T21:20:28Z,,,
arXIv2023,Language-Assisted 3D Scene Understanding,Yes.,3,"""However, challenges remain, including the requirement for paired triplet data, redundancy and ambiguity in supervised features, and the disruption of the original priors.""",2023,2023-12-18T18:54:56Z,,,
arXIv2023,G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model,Yes.,3,"""We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area",2023,2023-12-18T17:36:20Z,,,
arXIv2023,Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs,Yes.,3,"""However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages.""",2023,2023-12-18T15:23:06Z,,,
arXIv2023,MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL,Yes.,3,"""Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on 'huge' databases and complex user questions that require multi-step reasoning.""",2023,2023-12-18T14:40:20Z,,,
arXIv2023,CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update,Yes.,2,"""However, these methods often overlook the potential for continual learning, typically by freezing the utilized tools, thus limiting their adaptation to environments requiring new knowledge.""",2023,2023-12-18T03:34:07Z,,,
arXIv2023,Students' Perceptions and Preferences of Generative Artificial Intelligence Feedback for Programming,Yes.,1,"""This paper explored the feasibility of utilizing ChatGPT, one of the most popular LLMs, for automating feedback for Java programming assignments in an introductory computer science (CS1) class.""",2023,2023-12-17T22:26:53Z,,,
arXIv2023,Mixed Distillation Helps Smaller Language Model Better Reasoning,Yes.,3,"""While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications.""",2023,2023-12-17T14:28:28Z,,,
arXIv2023,LLM-Twin: Mini-Giant Model-driven Beyond 5G Digital Twin Networking Framework with Semantic Secure Communication and Computation,Yes.,1,"""we propose a large language model (LLM) empowered DTNs networking framework, LLM-Twin.""",2023,2023-12-17T07:13:59Z,,,
arXIv2023,TrojFSP: Trojan Insertion in Few-shot Prompt Tuning,Yes.,1,"""Prompt tuning is one of the most effective solutions to adapting a fixed pre-trained language model (PLM) for various downstream tasks, especially with only a few input samples.""",2023,2023-12-16T14:49:36Z,,,
arXIv2023,Resolving Crash Bugs via Large Language Models: An Empirical Study,Yes.,3,"""We observe that ChatGPT performs better at resolving code-related crash bugs compared to environment-related ones, and its primary challenge in resolution lies in inaccurate localization.""",2023,2023-12-16T13:41:04Z,,,
arXIv2023,When Graph Data Meets Multimodal: A New Paradigm for Graph Understanding and Reasoning,Yes.,3,"""integrating complex graph information into text sequences has become exceptionally difficult, which hinders the ability to interact with graph data through natural language instructions.""",2023,2023-12-16T08:14:11Z,,,
arXIv2023,LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?,Yes.,3,"""Our experiments demonstrate using our techniques, LLMs is a promising tool to help data engineers in writing semantically equivalent SQL queries, however challenges still persist, and is a better metric for evaluating SQL generation than the popular execution accuracy.""",2023,2023-12-16T05:01:23Z,,,
arXIv2023,KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know,Yes.,3,"""Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs.""",2023,2023-12-15T23:34:05Z,,,
arXIv2023,Student as an Inherent Denoiser of Noisy Teacher,Yes.,3,"""pseudo labels generated by teacher models are usually noisy and may influence KD performance.""",2023,2023-12-15T20:21:45Z,,,
arXIv2023,ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs),Yes.,3,"""These LLMs are also known for hallucinations (i.e. fake facts)"" and ""ProCoT can prevent cheating because of clear limitations in existing LLMs.""",2023,2023-12-15T14:01:46Z,,,
arXIv2023,Prompting Large Language Models for Topic Modeling,Yes.,3,"""existing models have certain limitations, particularly when dealing with short text datasets that lack co-occurring words. Moreover, these models often neglect sentence-level semantics, focusing primarily on token-level semantics.""",2023,2023-12-15T11:15:05Z,,,
arXIv2023,Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models,Yes.,3,"""Our findings highlight both the transformative potential of LLMs in this field and the challenges yet to be overcome.""",2023,2023-12-15T08:32:28Z,,,
arXIv2023,Privacy-Aware Document Visual Question Answering,Yes.,3,"""We highlight privacy issues in state of the art multi-modal LLM models used for DocVQA, and explore possible solutions.""",2023,2023-12-15T06:30:55Z,,,
arXIv2023,CERN for AGI: A Theoretical Framework for Autonomous Simulation-Based Artificial Intelligence Testing and Alignment,Yes.,3,"""Due to the rapid development and wide application of LLMs, challenges such as ethical alignment, controllability, and predictability of these models have become important research topics.""",2023,2023-12-14T23:48:51Z,,,
arXIv2023,Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft,Yes.,1,"""leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency.""",2023,2023-12-14T18:58:12Z,,,
arXIv2023,Measurement in the Age of LLMs: An Application to Ideological Scaling,Yes.,1,"""This paper explores the use of large language models (LLMs) to flexibly navigate the conceptual clutter inherent to social scientific measurement tasks.""",2023,2023-12-14T18:34:06Z,,,
arXIv2023,CogAgent: A Visual Language Model for GUI Agents,Yes.,3,"""Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels.""",2023,2023-12-14T13:20:57Z,,,
arXIv2023,Future-proofing geotechnics workflows: accelerating problem-solving with large language models,Yes.,3,"""It also addresses the challenges in implementing LLMs, particularly in achieving high precision and accuracy in specialized tasks, and underscores the need for expert oversight.""",2023,2023-12-14T05:17:27Z,,,
arXIv2023,Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention,Yes.,3,"""Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra.""",2023,2023-12-14T02:45:31Z,,,
arXIv2023,Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach,Yes.,1,"""inference with a locally installed large language model (LLM) to reliably identify planetary names despite these challenges.""",2023,2023-12-14T00:50:14Z,,,
arXIv2023,Distributed Inference and Fine-tuning of Large Language Models Over The Internet,Yes.,3,"""However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers.""",2023,2023-12-13T18:52:49Z,,,
arXIv2023,Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models,Yes.,3,"""However, efficiently designing prompts for LLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder their deployments in production.""",2023,2023-12-13T17:22:19Z,,,
arXIv2023,Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers,Yes.,3,"""However, current models are constrained to addressing object-centric tasks, where each question-answer pair focuses solely on an individual object."" and ""While this solution appears straightforward, it presents two main challenges",2023,2023-12-13T14:27:45Z,,,
arXIv2023,CoRTEx: Contrastive Learning for Representing Terms via Explanations with Applications on Constructing Biomedical Knowledge Graphs,Yes.,1,"""we leverage the world knowledge from Large Language Models (LLMs) and propose Contrastive Learning for Representing Terms via Explanations (CoRTEx) to enhance term representation and significantly improves term clustering.""",2023,2023-12-13T10:29:34Z,,,
arXIv2023,Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI,Yes.,3,"""the growing diversity of input data modalities prevents incorporating all modalities into LLMs, especially when LLMs are deployed on resource-constrained edge devices for embodied AI applications"" and ""existing work adopts fixed connections between encoders and the LLM's input layer, leading to high training cost at runtime and ineffective cross-modal interaction.""",2023,2023-12-13T04:08:59Z,,,
arXIv2023,"Tell, don't show: Declarative facts influence how LLMs generalize",Yes.,3,"""Nevertheless, the effect of declarative statements on model likelihoods is small in absolute terms and increases surprisingly little with model size (i.e. from 330 million to 175 billion parameters).""",2023,2023-12-12T22:47:42Z,,,
arXIv2023,Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization,Yes.,3,"""We find that they struggle with complex compositional questions due to cumulative errors in long reasoning steps and intricate logic required for tool-making.""",2023,2023-12-12T22:11:17Z,,,
arXIv2023,LLM in a flash: Efficient Large Language Model Inference with Limited Memory,Yes.,3,"""However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity.""",2023,2023-12-12T18:57:08Z,,,
arXIv2023,Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales,Yes.,1,"""Machine reasoning has made great progress in recent years owing to large language models (LLMs).""",2023,2023-12-12T16:14:45Z,,,
arXIv2023,A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames,No.,1,The abstract does not mention LLMs or their limitations.,2023,2023-12-12T16:10:19Z,,,
arXIv2023,Maatphor: Automated Variant Analysis for Prompt Injection Attacks,Yes.,3,"""Prompt injection has emerged as a serious security threat to large language models (LLMs)."" and ""variants of a prompt injection can be created to evade the LLM's guardrails.""",2023,2023-12-12T14:22:20Z,,,
arXIv2023,Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models,Yes.,1,"""We design attack helper prompts that effectively guide LLMs to break down an unethical drawing intent into multiple benign descriptions of individual image elements, allowing them to bypass safety filters while still generating unethical images.""",2023,2023-12-12T10:04:43Z,,,
arXIv2023,Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models,Yes.,3,"""Due to the substantial scale of Large Language Models (LLMs), the direct application of conventional compression methodologies proves impractical. The computational demands associated with even minimal gradient updates present challenges, particularly on consumer-grade hardware.""",2023,2023-12-12T07:56:57Z,,,
arXIv2023,HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts,Yes.,3,"""However, this strategy has two key limitations",2023,2023-12-12T07:40:23Z,,,
arXIv2023,Interactive Planning Using Large Language Models for Partially Observable Robotics Tasks,Yes.,3,"""However, planning for these tasks in the presence of uncertainties is challenging as it requires \enquote{chain-of-thought} reasoning, aggregating information from the environment, updating state estimates, and generating actions based on the updated state estimates.""",2023,2023-12-11T22:54:44Z,,,
arXIv2023,Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,Yes.,3,"""Large language models (LLMs) offer promise in text understanding but may not efficiently handle real-world clinical documentation.""",2023,2023-12-11T15:45:27Z,,,
arXIv2023,MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples,Yes.,2,"""Although In-Context Learning (ICL) brings remarkable performance gains to Large Language Models (LLMs), the improvements remain lower than fine-tuning on downstream tasks.""",2023,2023-12-11T13:11:04Z,,,
arXIv2023,Linguistic and Structural Basis of Engineering Design Knowledge,Yes.,2,"""Existing ontological design theories are less likely to guide the large-language models whose applications are currently limited to ideation and learning purposes.""",2023,2023-12-11T13:03:39Z,,,
arXIv2023,Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models,Yes.,3,"""Preexisting prompt tuning methods exhibit inadequacies in managing this structured knowledge.""",2023,2023-12-11T12:14:06Z,,,
arXIv2023,GTA: Gated Toxicity Avoidance for LM Performance Preservation,Yes.,3,"""However, existing CTG methods not only reduce toxicity but also negatively impact several aspects of the language model's generation performance, including topic consistency, grammar, and perplexity.""",2023,2023-12-11T05:04:17Z,,,
arXIv2023,Can LLMs Configure Software Tools,Yes.,2,"""Existing methods, including Bayesian optimization, have limitations regarding initial setup, computational cost, and convergence efficiency."" and ""While our initial insights are promising, they also indicate the need for further in-depth investigations and experiments in this domain.""",2023,2023-12-11T05:03:02Z,,,
arXIv2023,User Modeling in the Era of Large Language Models: Current Research and Future Directions,Yes.,3,"""Finally, it presents remaining challenges and future directions in the LLM-UM research.""",2023,2023-12-11T03:59:36Z,,,
arXIv2023,EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models,Yes.,3,"""We evaluate various open-source MLLMs, revealing that these models have not yet evolved into embodied planning generalists (even GPT-4V).""",2023,2023-12-11T03:35:58Z,,,
arXIv2023,Large Language Models on Lexical Semantic Change Detection: An Evaluation,Yes.,1,"""Despite the comprehensive coverage of various natural language processing domains by LLMs, there is a notable scarcity of literature concerning their application in this specific realm.""",2023,2023-12-10T21:26:35Z,,,
arXIv2023,ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models,Yes.,3,"""We delve into the challenges of LLM compression, notably their dependency on extensive training data and computational resources.""",2023,2023-12-10T08:41:24Z,,,
arXIv2023,Context Tuning for Retrieval Augmented Generation,Yes.,3,"""RAG's tool retrieval step requires all the required information to be explicitly present in the query. This is a limitation, as semantic search, the widely adopted tool retrieval method, can fail when the query is incomplete or lacks context.""",2023,2023-12-09T23:33:16Z,,,
arXIv2023,Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data,Yes.,1,"""Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is underexplored.""",2023,2023-12-09T19:35:40Z,,,
arXIv2023,PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching,Yes.,2,"""Instruction fine-tuning has conventionally been employed to adapt Large Language Models (LLMs) to a variety of tasks. Nonetheless, this technique often necessitates substantial computational resources, making it impractical for deployment by individuals or small-scale entities.""",2023,2023-12-09T17:38:39Z,,,
arXIv2023,Sim-GPT: Text Similarity via GPT Annotated Data,Yes.,1,"""Sim-GPT framework utilizes LLMs to provide a substantial amount of reliable annotated data filling the gap of the lack of training signals for STS.""",2023,2023-12-09T16:10:23Z,,,
arXIv2023,ESPN: Memory-Efficient Multi-Vector Information Retrieval,Yes.,3,"""However, these models significantly amplify memory and storage requirements for retrieval indices by an order of magnitude. This escalation in index size renders the scalability of multi-vector IR models progressively challenging due to their substantial memory demands.""",2023,2023-12-09T00:19:42Z,,,
arXIv2023,DeltaZip: Multi-Tenant Language Model Serving via Delta Compression,Yes.,2,"""serving many different fine-tuned LLMs concurrently for users in multi-tenant environments is challenging.""",2023,2023-12-08T18:07:05Z,,,
arXIv2023,LLM Interactive Optimization of Open Source Python Libraries -- Case Studies and Generalization,Yes.,3,"""However, this is only the case in interactive use, with a human expert in the loop.""",2023,2023-12-08T13:52:57Z,,,
arXIv2023,SparQ Attention: Bandwidth-Efficient LLM Inference,Yes.,3,"""The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment.""",2023,2023-12-08T11:47:35Z,,,
arXIv2023,Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models,Yes.,3,"""However, little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT), where translation begins before the entire source context is available to the model.""",2023,2023-12-07T20:42:05Z,,,
arXIv2023,Testing LLM performance on the Physics GRE: some observations,Yes.,2,"""There is a need to evaluate these powerful language models on several benchmarks, in order to understand their risks and limitations.""",2023,2023-12-07T17:33:12Z,,,
arXIv2023,Prompt Highlighter: Interactive Control for Multi-Modal LLMs,Yes.,3,"""Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature.""",2023,2023-12-07T13:53:29Z,,,
arXIv2023,Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss,Yes.,3,"""Participants appreciated AI's empathy and personalized suggestions but found them more formulaic, less authentic, and too data-focused.""",2023,2023-12-07T05:45:24Z,,,
arXIv2023,Large Language Models for Intent-Driven Session Recommendations,Yes.,1,"""Addressing these challenges, we introduce a novel ISR approach, utilizing the advanced reasoning capabilities of large language models (LLMs).""",2023,2023-12-07T02:25:14Z,,,
arXIv2023,Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration,Yes.,3,"""existing ICL approaches to ER typically necessitate providing a task description and a set of demonstrations for each entity pair and thus have limitations on the monetary cost of interfacing LLMs.""",2023,2023-12-07T02:09:27Z,,,
arXIv2023,Understanding (Un)Intended Memorization in Text-to-Image Generative Models,Yes.,3,"""Historically, memorization in machine learning has been context-dependent, with diverse definitions emerging from classification tasks to complex models like Large Language Models (LLMs) and Diffusion models.""",2023,2023-12-06T19:53:17Z,,,
arXIv2023,Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification,Yes.,3,"""independent thinking by LLMs can introduce variability in their thought processes, leading to potential inaccuracies"" and ""offering insights into their potential to cause misinterpretations and errors in the overall comprehension of the final text.""",2023,2023-12-06T12:34:46Z,,,
arXIv2023,SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM,Yes.,3,"""However their huge model size and the consequent demand for computational and memory resources also pose challenges to model deployment.""",2023,2023-12-06T11:10:55Z,,,
arXIv2023,Teaching Specific Scientific Knowledge into Large Language Models through Additional Training,Yes.,3,"""the study highlights the complexities and limitations of incorporating specialized information into LLMs, suggesting areas for further improvement.""",2023,2023-12-06T08:55:55Z,,,
arXIv2023,Clinical Notes Reveal Physician Fatigue,Yes.,3,"""Our model indicates that fatigued doctors write more predictable notes. Perhaps unsurprisingly, because word prediction is the core of how LLMs work, we find that LLM-written notes have 17% higher predicted fatigue than real physicians' notes. This indicates that LLMs may introduce distortions in generated text that are not yet fully understood.""",2023,2023-12-05T19:00:18Z,,,
arXIv2023,Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models,Yes.,3,"""generated programs are error-prone",2023,2023-12-05T18:58:37Z,,,
arXIv2023,Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models,Yes.,2,"""current works in this direction all depend on the GPT models, making it a single point of failure in scientific reproducibility. Moreover, it raises the concern that the current research findings only hold for GPT models but not LLM in general.""",2023,2023-12-05T18:57:40Z,,,
arXIv2023,Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions,Yes.,2,"""To address this issue and investigate the applicability of large language models (LLMs) in CQA simulation,"" and ""understand the disparities between LLM- and human-generated conversations.""",2023,2023-12-05T17:38:02Z,,,
arXIv2023,Creative Agents: Empowering Agents with Imagination for Creative Tasks,Yes.,3,"""This limitation comes from their inability to convert abstract language instructions into concrete task goals in the environment and perform long-horizon planning for such complicated goals.""",2023,2023-12-05T06:00:52Z,,,
arXIv2023,MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following,Yes.,3,"""LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs.""",2023,2023-12-05T02:32:08Z,,,
arXIv2023,Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data,Yes.,3,"""Code datasets, often collected from diverse and uncontrolled sources such as GitHub, potentially suffer from quality issues, thereby affecting the performance and training efficiency of Large Language Models (LLMs) optimized for code generation.""",2023,2023-12-05T01:19:30Z,,,
arXIv2023,TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques,Yes.,3,"""While LMs have exhibited exceptional performance across a wide range of natural language processing tasks, there are notable challenges associated with their utilization on small datasets and their ability to replicate more creative human capacities.""",2023,2023-12-04T18:52:26Z,,,
arXIv2023,A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia,Yes.,3,"""Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters,"" and ""We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries.""",2023,2023-12-04T17:35:42Z,,,
arXIv2023,Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness,Yes.,3,"""Existing work on editing LLMs has partially addressed the issue of dependency,"" and ""Extensive experiments on DepEdit show that existing knowledge editing methods are sensitive to the surface form of knowledge, and that they have limited performance in inferring the implications of edited facts.""",2023,2023-12-04T12:45:30Z,,,
arXIv2023,Intelligent Virtual Assistants with LLM-based Process Automation,Yes.,3,"""applying LLMs to create more advanced virtual assistants still faces challenges like ensuring robust performance and handling variability in real-world user commands.""",2023,2023-12-04T07:51:58Z,,,
arXIv2023,"Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation",Yes.,3,"""However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited.""",2023,2023-12-04T06:13:35Z,,,
arXIv2023,Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment,Yes.,3,"""Due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not. As a result, during representation learning, there is a mismatch",2023,2023-12-04T03:16:48Z,,,
arXIv2023,Generating Action-conditioned Prompts for Open-vocabulary Video Action Recognition,Yes.,1,"""we innovatively blend video models with Large Language Models (LLMs) to devise Action-conditioned Prompts.""",2023,2023-12-04T02:31:38Z,,,
arXIv2023,TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents,Yes.,3,"""there are major challenges in scaling up Shapley values for LLMs, particularly when dealing with long input contexts containing thousands of tokens and autoregressively generated output sequences.""",2023,2023-12-03T04:35:04Z,,,
arXIv2023,Large Language Models Are Zero-Shot Text Classifiers,Yes.,3,"""text classification problems have garnered considerable focus, but still faced with some limitations related to expensive computational cost, time consumption, and robust performance to unseen classes.""",2023,2023-12-02T06:33:23Z,,,
arXIv2023,Nash Learning from Human Feedback,Yes.,3,"""an inherent limitation of current reward models is their inability to fully represent the richness of human preferences and their dependency on the sampling distribution.""",2023,2023-12-01T19:26:23Z,,,
arXIv2023,LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices,Yes.,3,"""Deploying Large Language Models (LLMs) locally on mobile devices presents a significant challenge due to their extensive memory requirements.""",2023,2023-12-01T07:19:42Z,,,
arXIv2023,Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration,Yes.,1,"""A notable recent example arises in reinforcement learning from human feedback (RLHF) on large language models.""",2023,2023-12-01T00:54:02Z,,,
arXIv2023,Towards Accurate Differential Diagnosis with Large Language Models,Yes.,1,"""Interactive interfaces powered by Large Language Models (LLMs) present new opportunities to both assist and automate aspects of this process.""",2023,2023-11-30T19:55:51Z,,,
arXIv2023,PoseGPT: Chatting about 3D Human Pose,Yes.,1,"""PoseGPT addresses these limitations by embedding SMPL poses as a distinct signal token within a multi-modal LLM, enabling direct generation of 3D body poses from both textual and visual inputs.""",2023,2023-11-30T18:59:52Z,,,
arXIv2023,OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition,Yes.,3,"""leading to less distinct semantic space and potential performance limitations"" and ""To address the limitations of the less distinct semantic space of category names, we prompt a large language model (LLM) to augment action class names into Spatio-Temporal Descriptors.""",2023,2023-11-30T13:32:43Z,,,
arXIv2023,mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model,Yes.,3,"""However, the weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit their application scenarios, especially for scientific academic paper writing.""",2023,2023-11-30T04:43:26Z,,,
arXIv2023,Large Language Models for Travel Behavior Prediction,Yes.,3,"""However, though in most of the cases, the output explanations are reasonable, we still observe cases that violate logic or with hallucinations.""",2023,2023-11-30T04:35:55Z,,,
arXIv2023,Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models,Yes.,3,"""The advent of Large Language Models (LLMs) necessitates the development of training approaches that mitigate the generation of unethical language and aptly manage toxic user queries.""",2023,2023-11-30T03:19:45Z,,,
arXIv2023,TaskWeaver: A Code-First Agent Framework,Yes.,3,"""However, existing LLM frameworks face limitations in handling domain-specific data analytics tasks with rich data structures. Moreover, they struggle with flexibility to meet diverse user requirements.""",2023,2023-11-29T11:23:42Z,,,
arXIv2023,LLM-State: Expandable State Representation for Long-horizon Task Planning in the Open World,Yes.,3,"""Existing works fail to explicitly track key objects and attributes, leading to erroneous decisions in long-horizon tasks, or rely on highly engineered state features and feedback, which is not generalizable.""",2023,2023-11-29T07:23:22Z,,,
arXIv2023,Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention,Yes.,3,"""recent studies show their vulnerability to textual adversarial attacks where the model's output can be misled by intentionally manipulating the text inputs.""",2023,2023-11-29T07:09:13Z,,,
arXIv2023,Exploring Large Language Models for Human Mobility Prediction under Public Events,Yes.,3,"""Despite the great potential of LLMs, we also identify key challenges including misinformation and high costs that remain barriers to their broader adoption in large-scale human mobility analysis.""",2023,2023-11-29T04:25:15Z,,,
arXIv2023,Contrastive Vision-Language Alignment Makes Efficient Instruction Learner,Yes.,3,"""This task is crucial but challenging since the LLM is trained on text modality only, making it hard to effectively digest the visual modality.""",2023,2023-11-29T03:29:46Z,,,
arXIv2023,War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars,Yes.,2,"""By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings.""",2023,2023-11-28T20:59:49Z,,,
arXIv2023,Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis,Yes.,1,"""In this work, we introduce a novel approach to improving T2I diffusion models using Large Language Models (LLMs) as layout generators.""",2023,2023-11-28T14:51:13Z,,,
arXIv2023,A Survey of the Evolution of Language Model-Based Dialogue Systems,Yes.,2,"""we focus on emerging topics and discuss open challenges, providing valuable insights into future directions for LLM-based_dialogue_systems.""",2023,2023-11-28T13:51:32Z,,,
arXIv2023,Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld,Yes.,3,"""While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals.""",2023,2023-11-28T11:53:56Z,,,
arXIv2023,A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA,Yes.,3,"""This scaling factor, which divides adapters by a factor of the rank, results in slowed learning and stunted performance for LoRA with higher-rank adapters. Consequently, the use of LoRA in practice has generally been limited to very low ranks.""",2023,2023-11-28T03:23:20Z,,,
arXIv2023,Self-correcting LLM-controlled Diffusion Models,Yes.,3,"""current text-to-image diffusion models still often struggle to accurately interpret and follow complex input text prompts.""",2023,2023-11-27T18:56:37Z,,,
arXIv2023,MEDITRON-70B: Scaling Medical Pretraining for Large Language Models,Yes.,2,"""the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities.""",2023,2023-11-27T18:49:43Z,,,
arXIv2023,Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models,Yes.,3,"""logic errors relate to the runtime performance of code and thus may not be as well suited to analysis by LLMs.""",2023,2023-11-27T17:28:33Z,,,
arXIv2023,vTrain: A Simulation Framework for Evaluating Cost-effective and Compute-optimal Large Language Model Training,Yes.,3,"""Existing LLM training plans typically employ a heuristic based parallel training strategy which is based on empirical observations rather than grounded upon a thorough examination of the search space of LLM parallelization. Such limitation renders existing systems to leave significant performance left on the table, wasting millions of dollars worth of training cost.""",2023,2023-11-27T13:35:15Z,,,
arXIv2023,RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks,Yes.,3,"""Despite LLMs' great generalization and comprehension of instruction tasks, LLMs-generated task plans sometimes lack feasibility and correctness.""",2023,2023-11-27T09:20:23Z,,,
arXIv2023,SpotServe: Serving Generative Large Language Models on Preemptible Instances,Yes.,2,"""The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them cheaply.""",2023,2023-11-27T06:31:17Z,,,
arXIv2023,Function-constrained Program Synthesis,Yes.,3,"""Generating computer programs in general-purpose programming languages like Python poses a challenge for LLMs when instructed to use code provided in the prompt."" and ""Moreover, current systems lack effective recovery methods, forcing users to iteratively re-prompt the model with modified prompts until a sufficient solution is reached.""",2023,2023-11-27T02:55:34Z,,,
arXIv2023,DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer,Yes.,3,"""Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information."" and ""Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider.""",2023,2023-11-27T02:01:10Z,,,
arXIv2023,KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All,Yes.,2,"""However, they may encounter limitations when lacking control over the correlations between old task queries and keys of future tasks, the shift of features in the latent space, and the relative separation of latent vectors learned in independent tasks.""",2023,2023-11-26T20:35:19Z,,,
arXIv2023,Gender inference: can chatGPT outperform common commercial tools?,Yes.,2,"""An important limitation of these tools is that they fail to appropriately capture the fact that gender exists on a non-binary scale"" and ""In the future, it might even be possible for ChatGPT or other large scale language models to better identify self-reported gender rather than report gender on a binary scale.""",2023,2023-11-24T22:09:14Z,,,
arXIv2023,Large Language Models as Automated Aligners for benchmarking Vision-Language Models,Yes.,2,"""existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence.""",2023,2023-11-24T16:12:05Z,,,
arXIv2023,Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language,Yes.,2,"""human preference on LLM outputs can come in much richer forms including natural language, which may provide detailed feedback on strengths and weaknesses of a given response.""",2023,2023-11-24T15:20:36Z,,,
arXIv2023,Machine Translation for Ge'ez Language,Yes.,3,"""We also attempt to finetune the NLLB-200 model, one of the most advanced translation models available today, but find that it performs poorly with only 4k training samples for Ge'ez."" and ""We observe that GPT-3.5 achieves a remarkable BLEU score of 9.2 with no initial knowledge of Ge'ez, but still lower",2023,2023-11-24T14:55:23Z,,,
arXIv2023,PrivateLoRA For Efficient Privacy Preserving LLM,Yes.,2,"""End users face a choice between privacy and efficiency in current Large Language Model (LLM) service paradigms.""",2023,2023-11-23T14:36:30Z,,,
arXIv2023,Dialogue Quality and Emotion Annotations for Customer Support Conversations,Yes.,3,"""However, with the advent of Large Language Models (LLMs) pretrained on extensive, multilingual and diverse text data, these limitations seem overcome. Nevertheless, their generalisability to different languages and domains in dialogue applications remains uncertain without benchmarking datasets.""",2023,2023-11-23T10:56:14Z,,,
arXIv2023,Compositional Zero-shot Learning via Progressive Language-based Observations,Yes.,1,"""PLO-LLM",2023,2023-11-23T10:14:23Z,,,
arXIv2023,Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models,Yes.,3,"""visual question answering using a large language model suggested Lego-generated concepts are better aligned with the text description of the concept."" and ""Two key characteristics of these concepts contribute to the limitations of current inversion methods.""",2023,2023-11-23T07:33:38Z,,,
arXIv2023,AutoKG: Efficient Automated Knowledge Graph Generation for Language Models,Yes.,3,"""Traditional methods of linking large language models (LLMs) to knowledge bases via the semantic similarity search often fall short of capturing complex relational dynamics.""",2023,2023-11-22T08:58:25Z,,,
arXIv2023,Large Language Models in Education: Vision and Opportunities,Yes.,3,"""while still facing technical, ethical, and practical challenges requiring further research and exploration.""",2023,2023-11-22T05:04:20Z,,,
arXIv2023,Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions,Yes.,1,"""We propose utilizing large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties.""",2023,2023-11-21T16:20:49Z,,,
arXIv2023,Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study,Yes.,3,"""LLMs with diverse prompts achieve comparable performance in text-based misinformation detection but exhibit notably constrained capabilities in comprehending propagation structure compared to existing models in propagation-based misinformation detection.""",2023,2023-11-21T16:03:51Z,,,
arXIv2023,HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis,Yes.,3,"""they require a large-scale data and possess the same limitations as previous autoregressive speech models, including slow inference speed and lack of robustness.""",2023,2023-11-21T09:07:11Z,,,
arXIv2023,Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis,Yes.,3,"""user-generated data usually contains sensitive and private information,"" and ""the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data.""",2023,2023-11-21T01:34:02Z,,,
arXIv2023,On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software,Yes.,3,"""We perform a systematic analysis on the potential and limitations of in-context learning with Large Language Models(LLMs) for this task.""",2023,2023-11-20T18:12:28Z,,,
arXIv2023,LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions,Yes.,3,"""their outputs often suffer from ambiguity and inaccuracy. We attribute this to two primary factors",2023,2023-11-20T16:37:45Z,,,
arXIv2023,Refactoring Programs Using Large Language Models with Few-Shot Examples,Yes.,2,"""unnecessary behaviors such as deleting or translating comments are also observed.""",2023,2023-11-20T11:43:45Z,,,
arXIv2023,Incorporating LLM Priors into Tabular Learners,Yes.,3,"""addressing LLMs challenges like data serialization sensitivity and biases.""",2023,2023-11-20T09:27:09Z,,,
arXIv2023,Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning,Yes.,3,"""However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains.""",2023,2023-11-20T06:06:20Z,,,
arXIv2023,Zero-Shot Question Answering over Financial Documents using Large Language Models,Yes.,3,"""While LLMs have exhibited remarkable performance on various natural language and reasoning tasks, complex reasoning problems often rely on few-shot prompts that require carefully crafted examples."" and ""mitigating the limitations of LLM in performing accurate arithmetic calculations.""",2023,2023-11-19T16:23:34Z,,,
arXIv2023,Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness,Yes.,3,"""Recent advancements in generative AI, exemplified by large language models, hold promise in facilitating the arduous task. However, developing accountable, fair, and inclusive models remains a complicated undertaking.""",2023,2023-11-19T03:29:45Z,,,
arXIv2023,Few-Shot Classification & Segmentation Using Large Language Models Agent,Yes.,1,"""We introduce a method that utilises large language models (LLM) as an agent to address the FS-CS problem in a training-free manner.""",2023,2023-11-19T00:33:41Z,,,
arXIv2023,An Embodied Generalist Agent in 3D World,Yes.,3,"""However, a significant challenge remains as these models exhibit limited ability in understanding and interacting with the 3D world.""",2023,2023-11-18T01:21:38Z,,,
arXIv2023,Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections,Yes.,1,"""a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings.""",2023,2023-11-17T18:00:20Z,,,
arXIv2023,A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest,Yes.,3,"""Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains.""",2023,2023-11-17T16:09:10Z,,,
arXIv2023,TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in LLMs through Translation-Assisted Chain-of-Thought Processes,Yes.,3,"""Creating multilingual LLMs poses a significant challenge. Pretraining or fine-tuning LLMs to adopt new languages is evidently very costly. Furthermore, there exist limitations concerning benchmark datasets and the metrics used to measure model performance in multilingual settings.""",2023,2023-11-17T06:55:32Z,,,
arXIv2023,"ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond",Yes.,3,"""LLMs face challenges in crafting outputs below a sixth-grade reading level.""",2023,2023-11-16T18:30:14Z,,,
arXIv2023,Leveraging LLMs in Scholarly Knowledge Graph Question Answering,Yes.,1,"""This paper presents a scholarly Knowledge Graph Question Answering (KGQA) that answers bibliographic natural language questions by leveraging a large language model (LLM) in a few-shot manner.""",2023,2023-11-16T12:13:49Z,,,
arXIv2023,MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning,Yes.,3,"""Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge.""",2023,2023-11-16T11:47:58Z,,,
arXIv2023,How Far Can We Extract Diverse Perspectives from Large Language Models?,Yes.,3,"""However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question.""",2023,2023-11-16T11:23:38Z,,,
arXIv2023,Interpreting User Requests in the Context of Natural Language Standing Instructions,Yes.,3,"""Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls.""",2023,2023-11-16T11:19:26Z,,,
arXIv2023,Video-LLaVA: Learning United Visual Representation by Alignment Before Projection,Yes.,3,"""However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.""",2023,2023-11-16T10:59:44Z,,,
arXIv2023,Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations,Yes.,2,"""Existing studies in backdoor defense have predominantly focused on the training phase, overlooking the critical aspect of testing time defense.""",2023,2023-11-16T10:38:43Z,,,
arXIv2023,How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?,Yes.,3,"""However, no prior work has systematically investigated how the calibration data impacts the effectiveness of model compression methods.""",2023,2023-11-16T10:30:00Z,,,
arXIv2023,GEO: Generative Engine Optimization,Yes.,3,"""Given the black-box and fast-moving nature of Generative Engines, content creators have little to no control over when and how their content is displayed.""",2023,2023-11-16T10:06:09Z,,,
arXIv2023,MOKA: Moral Knowledge Augmentation for Moral Event Extraction,Yes.,3,"""insufficient moral reasoning capability in most existing NLP systems, where LLMs are no exception.""",2023,2023-11-16T10:04:49Z,,,
arXIv2023,"OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning",Yes.,3,"""Large language models (LLMs) often struggle with maintaining accuracy throughout multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer.""",2023,2023-11-16T09:56:28Z,,,
arXIv2023,Large Language Model Inference with Lexical Shortlisting,Yes.,3,"""Large language model (LLM) inference is computation and memory intensive,"" and ""we also identify the drawbacks of such vocabulary selection methods and propose avenues for future research.""",2023,2023-11-16T09:35:50Z,,,
arXIv2023,Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance,Yes.,3,"""While this is a promising result, we also found that none of the verifications were flawless, and there remain significant challenges in achieving autonomous, human-level research using only generic instructions.""",2023,2023-11-16T09:34:23Z,,,
arXIv2023,Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies,Yes.,3,"""Our findings reveal the model's effectiveness in capturing emotional and moral nuances, albeit with some challenges in stance detection, highlighting the intricacies and potential for refinement in NLP tools for politically sensitive contexts.""",2023,2023-11-16T08:57:53Z,,,
arXIv2023,Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring,Yes.,3,"""Token-level watermarking inserts watermarks in the generated texts by altering the token probability distributions with a private random number generator seeded by its prefix tokens. However, this watermarking algorithm alters the logits during generation, which can lead to a downgraded text quality if it chooses to promote tokens that are less relevant given the input.""",2023,2023-11-16T08:36:00Z,,,
arXIv2023,The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents,Yes.,3,"""We then identify several factors that interfere with convergence, including the use of chain-of-thought prompt and lack of details in personas.""",2023,2023-11-16T08:30:15Z,,,
arXIv2023,Automatic Engineering of Long Prompts,Yes.,2,"""However, these prompts can be lengthy, often comprising hundreds of lines and thousands of tokens, and their design often requires considerable human effort.""",2023,2023-11-16T07:42:46Z,,,
arXIv2023,Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations,Yes.,3,"""when applied to specific task domains, an LLM pre-trained on a general-purpose corpus may exhibit a deficit or inadequacy in two types of domain-specific knowledge... The absence or inadequacy of such knowledge impacts the performance of the LLM.""",2023,2023-11-16T07:09:38Z,,,
arXIv2023,Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion,Yes.,3,"""Our analysis reveals that emotion triggers are largely not considered salient features for emotion prediction models, instead there is intricate interplay between various features and the task of emotion detection.""",2023,2023-11-16T06:20:13Z,,,
arXIv2023,Prompt-based Pseudo-labeling Strategy for Sample-Efficient Semi-Supervised Extractive Summarization,Yes.,1,"""we propose a prompt-based pseudo-labeling strategy with LLMs that picks unlabeled examples with more accurate pseudo-labels than using just the classifier's probability outputs.""",2023,2023-11-16T04:29:41Z,,,
arXIv2023,Leveraging Code to Improve In-context Learning for Semantic Parsing,Yes.,3,"""learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs.""",2023,2023-11-16T02:50:06Z,,,
arXIv2023,One Size Does Not Fit All: Customizing Open-Domain Procedures,Yes.,1,"""Our goal is to measure and improve an LLM's ability to perform such customization.""",2023,2023-11-16T02:25:36Z,,,
arXIv2023,Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections,Yes.,3,"""such alignments are often vulnerable",2023,2023-11-15T23:52:05Z,,,
arXIv2023,LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction,Yes.,1,"""The advent of Large Language Models (LLMs), especially the commercially available OpenAI models, have reset expectations for what is possible with deep learning models and have created a new field called prompt engineering.""",2023,2023-11-15T20:57:44Z,,,
arXIv2023,Improving fit to human reading times via temperature-scaled surprisal,Yes.,3,"""In general, these studies have implicitly assumed that the probability scores from LLMs are accurate, ignoring the discrepancies between human cognition and LLMs from this standpoint.""",2023,2023-11-15T19:34:06Z,,,
arXIv2023,Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models,Yes.,3,"""the massive scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained environments"" and ""there is a risk that distilled SLMs may still inherit flawed reasoning and hallucinations from LLMs.""",2023,2023-11-15T18:56:23Z,,,
arXIv2023,CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models,Yes.,3,"""However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination,"" and ""Our experiments illustrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.""",2023,2023-11-15T17:50:30Z,,,
arXIv2023,MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation,Yes.,2,"""Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs).""",2023,2023-11-15T16:52:14Z,,,
arXIv2023,How Multilingual is Multilingual LLM?,Yes.,3,"""Large Language Models (LLMs), trained predominantly on extensive English data, often exhibit limitations when applied to other languages.""",2023,2023-11-15T16:13:14Z,,,
arXIv2023,Enabling Large Language Models to Learn from Rules,Yes.,3,"""However, this learning paradigm may not well learn those complicated rules, especially when the training examples are limited.""",2023,2023-11-15T11:42:41Z,,,
arXIv2023,"StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",Yes.,3,"""Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps.""",2023,2023-11-15T09:18:09Z,,,
arXIv2023,Auto-ICL: In-Context Learning without Human Supervision,Yes.,3,"""Despite this, LLMs are heavily reliant on well-structured prompts to function efficiently within the realm of In-Context Learning.""",2023,2023-11-15T07:37:28Z,,,
arXIv2023,Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling,Yes.,3,"""Performing uncertainty decomposition for large language models (LLMs) is an important step toward improving the reliability, trustworthiness, and interpretability of LLMs, but this research task is very challenging and remains unresolved."" and ""The existing canonical method, Bayesian Neural Network (BNN), cannot be applied to LLMs, because BNN requires training and ensembling multiple variants of",2023,2023-11-15T05:58:35Z,,,
arXIv2023,PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning,Yes.,3,"""Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data.""",2023,2023-11-15T05:28:07Z,,,
arXIv2023,Comparing Generalization in Learning with Limited Numbers of Exemplars: Transformer vs. RNN in Attractor Dynamics,Yes.,3,"""Our simulation results suggest that under conditions of limited data availability, Transformer's GIL abilities are markedly inferior to those of RNN.""",2023,2023-11-15T00:37:49Z,,,
arXIv2023,Towards Open-Ended Visual Recognition with Large Language Model,Yes.,3,"""However, it is worth noting that these open-vocabulary recognition models still exhibit limitations in practical applications. On one hand, they rely on the provision of class names during testing, where the recognition performance heavily depends on this predefined set of semantic classes by users. On the other hand, when training with multiple datasets, human intervention is required to alleviate the label definition conflict between them.""",2023,2023-11-14T18:59:01Z,,,
arXIv2023,On What Basis? Predicting Text Preference Via Structured Comparative Reasoning,Yes.,3,"""large language models (LLMs) often demonstrate inconsistencies in their reasoning"" and ""they struggle to consistently distinguish the similarities and differences of complex texts.""",2023,2023-11-14T18:51:38Z,,,
arXIv2023,Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning,Yes.,2,"""Enhancing the instruction-following ability of Large Language Models (LLMs) primarily demands substantial instruction-tuning datasets. However, the sheer volume of these imposes a considerable computational burden and annotation cost.""",2023,2023-11-14T14:10:40Z,,,
arXIv2023,Vision-Language Instruction Tuning: A Review and Analysis,Yes.,3,"""we discuss the current challenges and future research directions of VLIT, providing insights for the continuous development of this field.""",2023,2023-11-14T14:02:32Z,,,
arXIv2023,"MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge",Yes.,2,"""they often lack physical intuition since knowledge is baked into the parametric complement through training, offering less flexibility when it comes to incorporating mathematical or physical insights.""",2023,2023-11-14T13:49:03Z,,,
arXIv2023,Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios,Yes.,3,"""To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model",2023,2023-11-14T13:30:54Z,,,
arXIv2023,Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts,Yes.,3,"""the ability to deliver multi-step reasoning remains limited to English due to the imbalance in the distribution of the pre-training data, making the other languages a barrier.""",2023,2023-11-14T11:49:43Z,,,
arXIv2023,Adversarial Preference Optimization,Yes.,3,"""However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency.""",2023,2023-11-14T10:10:31Z,,,
arXIv2023,TempTabQA: Temporal Question Answering for Semi-Structured Tables,Yes.,3,"""We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points.""",2023,2023-11-14T08:57:01Z,,,
arXIv2023,Instruction-Following Evaluation for Large Language Models,Yes.,3,"""Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM.""",2023,2023-11-14T05:13:55Z,,,
arXIv2023,LLatrieval: LLM-Verified Retrieval for Verifiable Generation,Yes.,3,"""However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs.""",2023,2023-11-14T01:38:02Z,,,
arXIv2023,On The Truthfulness of 'Surprisingly Likely' Responses of Large Language Models,Yes.,1,"""We investigate the relevance of a similar criterion for responses of LLMs.""",2023,2023-11-13T19:21:25Z,,,
arXIv2023,Using Natural Language Explanations to Improve Robustness of In-context Learning for Natural Language Inference,Yes.,3,"""However, the existing literature shows that ICL encounters performance deterioration when exposed to adversarial inputs.""",2023,2023-11-13T18:49:13Z,,,
arXIv2023,InCA: Rethinking In-Car Conversational System Assessment Leveraging Large Language Models,Yes.,3,"""The unique demands of these systems, where answers may relate to driver or car safety and are confined within the car domain, highlight the limitations of current metrics.""",2023,2023-11-13T17:02:06Z,,,
arXIv2023,Story-to-Motion: Synthesizing Infinite and Controllable Character Animation from Long Text,Yes.,1,"""We leverage contemporary Large Language Models to act as a text-driven motion scheduler to extract a series of (text, position, duration) pairs from long text.""",2023,2023-11-13T16:22:38Z,,,
arXIv2023,What Large Language Models Bring to Text-rich VQA?,Yes.,3,"""we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem"" and ""The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part.""",2023,2023-11-13T12:52:29Z,,,
arXIv2023,Can LLMs Patch Security Issues?,Yes.,3,"""Nonetheless, similar to human developers, these models might generate code that contains security vulnerabilities and flaws.""",2023,2023-11-13T08:54:37Z,,,
arXIv2023,WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models,Yes.,3,"""we observe the common struggles for current methods on maintaining the generation quality.""",2023,2023-11-13T08:09:01Z,,,
arXIv2023,SELF-EXPLAIN: Teaching Large Language Models to Reason Complex Questions by Themselves,Yes.,3,"""such chain-of-thought examples are expensive to craft, especially for professional domains, and can have high variance depending on human annotators.""",2023,2023-11-12T23:14:43Z,,,
arXIv2023,Assessing the Interpretability of Programmatic Policies with Large Language Models,Yes.,1,"""we introduce a novel metric that uses large-language models (LLM) to assess the interpretability of programmatic policies.""",2023,2023-11-12T22:43:26Z,,,
arXIv2023,Can Large Language Models Augment a Biomedical Ontology with missing Concepts and Relations?,Yes.,1,"""Here, we explore the potential of large language models (LLM) to expand an existing ontology in a semi-automated fashion.""",2023,2023-11-12T14:20:55Z,,,
arXIv2023,Evaluating the Efficacy of Interactive Language Therapy Based on LLM for High-Functioning Autistic Adolescent Psychological Counseling,Yes.,3,"""challenges in achieving the depth of personalization and emotional understanding characteristic of human therapists were noted.""",2023,2023-11-12T07:55:39Z,,,
arXIv2023,Trusted Source Alignment in Large Language Models,Yes.,2,"""Large language models (LLMs) are trained on web-scale corpora that inevitably include contradictory factual information from sources of varying reliability.""",2023,2023-11-12T00:25:25Z,,,
arXIv2023,Intentional Biases in LLM Responses,Yes.,3,"""We find that the guardrails in the GPT-4 mixture of experts models with a supervisor, while useful in assuring AI alignment in general, are detrimental in trying to construct personas with a variety of uncommon viewpoints.""",2023,2023-11-11T19:59:24Z,,,
arXIv2023,BizBench: A Quantitative Reasoning Benchmark for Business and Finance,Yes.,3,"""We demonstrate that the current bottleneck in performance is due to LLMs' limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.""",2023,2023-11-11T16:16:11Z,,,
arXIv2023,Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study,Yes.,3,"""The brittleness of finetuned language model performance on out-of-distribution (OOD) test samples in unseen domains has been well-studied for English, yet is unexplored for multi-lingual models."" and ""Results echo the OOD performance decline observed in the",2023,2023-11-11T11:56:56Z,,,
arXIv2023,Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering,Yes.,3,"""the model needs to utilize domain knowledge properly to generate reliable answers. These two issues are the two major difficulties in the LLM application as vanilla fine-tuning can not adequately address them.""",2023,2023-11-11T07:56:40Z,,,
arXIv2023,Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks,Yes.,2,"""their real-world adoption faces challenges due to absence of comprehensive benchmarks for various HR tasks, and lack of smaller models with competitive capabilities.""",2023,2023-11-10T20:25:42Z,,,
arXIv2023,Language Models can be Logical Solvers,Yes.,3,"""Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions.""",2023,2023-11-10T16:23:50Z,,,
arXIv2023,Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking,Yes.,2,"""However, the performance-cost trade-offs of these methods remain underexplored, a critical concern for budget-limited organizations.""",2023,2023-11-10T15:10:36Z,,,
arXIv2023,Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration,Yes.,3,"""Existing MIAs designed for LMs can be classified into two categories",2023,2023-11-10T13:55:05Z,,,
arXIv2023,Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service,Yes.,3,"""existing studies indicate that EaaS is vulnerable to model extraction attacks that induce great loss for the owners of VLPs"" and ""A major solution of watermarking model for EaaS implants a backdoor in the model by inserting verifiable trigger embeddings into texts, but it is only applicable for large language models and is unrealistic due to data and model privacy.""",2023,2023-11-10T04:27:27Z,,,
arXIv2023,Tamil-Llama: A New Tamil Language Model Based on Llama 2,Yes.,3,"""However, a prevailing limitation is the underrepresentation of languages like Tamil in these cutting-edge models, leading to suboptimal performance in diverse linguistic contexts.""",2023,2023-11-10T03:02:39Z,,,
arXIv2023,Conversational AI Threads for Visualizing Multidimensional Datasets,Yes.,3,"""We surfaced the strengths and weaknesses of LLM-driven analytic chatbots, finding that they fell short in supporting progressive visualization refinements.""",2023,2023-11-09T18:47:46Z,,,
arXIv2023,Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations,Yes.,3,"""LLMs trained with supervised fine-tuning or 'single-step' RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction.""",2023,2023-11-09T18:45:16Z,,,
arXIv2023,BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings,Yes.,3,"""Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling.""",2023,2023-11-09T11:53:52Z,,,
arXIv2023,Chain of Images for Intuitively Reasoning,Yes.,3,"""current Large Language Models (LLMs) do not utilize such visual intuition to help their thinking. Even the most advanced version language models (e.g., GPT-4V and LLaVA) merely align images into textual space, which means their reasoning processes remain purely verbal.""",2023,2023-11-09T11:14:51Z,,,
arXIv2023,Prompt Engineering a Prompt Engineer,Yes.,3,"""we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt.""",2023,2023-11-09T08:00:32Z,,,
arXIv2023,Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks,Yes.,3,"""Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required."" and ""how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales.""",2023,2023-11-09T01:04:44Z,,,
arXIv2023,SEMQA: Semi-Extractive Multi-Source Question Answering,Yes.,3,"""Yet, attributing and verifying their generated abstractive answers can be difficult, and automatically evaluating their accuracy remains an ongoing challenge."" and ""Experimenting with several LLMs in various settings, we find this task to be surprisingly challenging.""",2023,2023-11-08T18:46:32Z,,,
arXIv2023,Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection,Yes.,3,"""while large language models show strong potential as an alternative to human annotators, their sensitivity to task-specific instructions and their intrinsic biases pose intriguing yet unique challenges in machine annotation.""",2023,2023-11-08T06:54:34Z,,,
arXIv2023,Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models,Yes.,1,"The paper discusses watermarking schemes for generative models, including large language models, but does not mention any explicit limitations of the language models themselves in the abstract.",2023,2023-11-07T22:52:54Z,,,
arXIv2023,Prompt Cache: Modular Attention Reuse for Low-Latency Inference,Yes.,1,"""We present Prompt Cache, an approach for accelerating inference for large language models (LLM) by reusing attention states across different LLM prompts.""",2023,2023-11-07T18:17:05Z,,,
arXIv2023,Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment,Yes.,3,"""A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors.""",2023,2023-11-07T15:36:40Z,,,
arXIv2023,Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features,Yes.,3,"""During this stage, models may be guided by their inductive biases to rely on simpler features which may be easier to extract, at a cost to robustness and generalisation.""",2023,2023-11-07T15:00:39Z,,,
arXIv2023,Leveraging Large Language Models for Automated Proof Synthesis in Rust,Yes.,3,"""However, LLMs lack the ability to retain and propagate context information, a strength of traditional static analysis.""",2023,2023-11-07T05:47:47Z,,,
arXIv2023,Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning,Yes.,3,"""However, several challenges still remain",2023,2023-11-07T05:32:39Z,,,
arXIv2023,ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity,Yes.,3,"""However, these VEPs are not disease-specific, limiting their adaptation at point-of-care.""",2023,2023-11-06T18:43:47Z,,,
arXIv2023,DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase,Yes.,2,"""However, ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios.""",2023,2023-11-06T18:12:55Z,,,
arXIv2023,Ziya2: Data-centric Learning is All LLMs Need,Yes.,3,"""the development of LLMs still faces several issues, such as high cost of training models from scratch, and continual pre-training leading to catastrophic forgetting, etc."" and ""an important yet practical limitation is that many studies overly pursue enlarging model sizes without comprehensively analyzing and optimizing the use of pre-training data in their learning process, as well as appropriate organization and leveraging of such data",2023,2023-11-06T17:49:34Z,,,
arXIv2023,ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents,Yes.,1,"""Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also highlight their potential in advancing game theory knowledge.""",2023,2023-11-06T16:03:46Z,,,
arXIv2023,Zero-shot Bilingual App Reviews Mining with Large Language Models,Yes.,1,"""In this work, we propose Mini-BAR, a tool that integrates large language models (LLMs) to perform zero-shot mining of user reviews in both English and French.""",2023,2023-11-06T12:36:46Z,,,
arXIv2023,LitSumm: Large language models for literature summarisation of non-coding RNAs,Yes.,2,"""We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. We also applied the",2023,2023-11-06T12:22:19Z,,,
arXIv2023,Tailoring Self-Rationalizers with Multi-Reward Distillation,Yes.,3,"""prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3);"" and ""focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans?""",2023,2023-11-06T00:20:11Z,,,
arXIv2023,Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation,Yes.,3,"""The key findings reveal ChatGPT's strengths in crafting concise, efficient code with advanced constructs, showcasing strengths in data analysis tasks (93.1% accuracy) but limitations in visual-graphical challenges.""",2023,2023-11-05T12:56:40Z,,,
arXIv2023,Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models,Yes.,3,"""Nevertheless, they are not designed to directly control low-level robotic motions, as their pretraining is based on vast internet data rather than specific robotics data.""",2023,2023-11-04T11:21:38Z,,,
arXIv2023,LLMs-augmented Contextual Bandit,Yes.,1,"""In this paper, we propose a novel integration of large language models (LLMs) with the contextual bandit framework.""",2023,2023-11-03T23:12:57Z,,,
arXIv2023,Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs,Yes.,1,"""Existing methods, however, are constrained to process plain text and do not support such a mechanism.""",2023,2023-11-03T22:56:43Z,,,
arXIv2023,Grounded Intuition of GPT-Vision's Abilities with Scientific Images,Yes.,3,"""GPT-Vision has impressed us on a range of vision-language tasks, but it comes with the familiar new challenge",2023,2023-11-03T17:53:43Z,,,
arXIv2023,Post Turing: Mapping the landscape of LLM Evaluation,Yes.,3,"""introduction of well-defined and standardized evaluation methodologies remains a crucial challenge"" and ""traditional evaluation proxies, such as the Turing test, have become less reliable.""",2023,2023-11-03T17:24:50Z,,,
arXIv2023,Sentiment Analysis through LLM Negotiations,Yes.,3,"""This framework suffers the key disadvantage that the single-turn output generated by a single LLM might not deliver the perfect decision, just as humans sometimes need multiple attempts to get things right.""",2023,2023-11-03T12:35:29Z,,,
arXIv2023,Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT,Yes.,3,"""Our results indicate that LLMs efficiently interpret workflows but achieve lower performance for exchanging components or purposeful workflow extensions. We characterize their limitations in these challenging scenarios and suggest future research directions.""",2023,2023-11-03T10:28:53Z,,,
arXIv2023,AFPQ: Asymmetric Floating Point Quantization for LLMs,Yes.,3,"""Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth.""",2023,2023-11-03T09:07:09Z,,,
arXIv2023,TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine,Yes.,3,"""However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency.""",2023,2023-11-03T08:54:50Z,,,
arXIv2023,Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization,Yes.,3,"""However, their ever-increasing size has raised concerns about their effective deployment and the need for LLM compression.""",2023,2023-11-02T18:55:53Z,,,
arXIv2023,GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks,Yes.,2,"""Despite limitations like restricted visual clarity grading and real-world complex reasoning, its ability to provide human-aligned scores enriched with detailed explanations is promising for universal automatic evaluator.""",2023,2023-11-02T16:11:09Z,,,
arXIv2023,Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations,Yes.,1,"""Our approach utilizes a large language model (LLM) to transform expressive TTS into a style retrieval task.""",2023,2023-11-02T14:20:37Z,,,
arXIv2023,CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL,Yes.,1,"""First, we instruct an LLM to hallucinate a minimal DB schema deemed adequate to answer the query.""",2023,2023-11-02T12:13:52Z,,,
arXIv2023,Making Harmful Behaviors Unlearnable for Large Language Models,Yes.,3,"""the powerful learning ability of LLMs not only enables them to acquire new tasks but also makes them susceptible to learning undesired behaviors.""",2023,2023-11-02T09:18:21Z,,,
arXIv2023,LLM4Drive: A Survey of Large Language Models for Autonomous Driving,Yes.,2,"""This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field.""",2023,2023-11-02T07:23:33Z,,,
arXIv2023,M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place,Yes.,3,"""These generic models are able to interpret complex tasks using language commands, but they often have difficulties generalizing to out-of-distribution objects due to the inability of low-level action primitives.""",2023,2023-11-02T01:42:52Z,,,
arXIv2023,Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving,Yes.,3,"""Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems.""",2023,2023-11-01T17:52:15Z,,,
arXIv2023,Improving Interpersonal Communication by Simulating Audiences with Language Models,Yes.,1,"""we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better.""",2023,2023-11-01T17:44:50Z,,,
arXIv2023,Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation,Yes.,3,"""However, T5 suffers from the dispersed attention issue",2023,2023-11-01T17:43:35Z,,,
arXIv2023,Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions,Yes.,3,"""While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set.""",2023,2023-11-01T02:31:35Z,,,
arXIv2023,ChatGPT-Powered Hierarchical Comparisons for Image Classification,Yes.,2,"""However, biases in CLIP lead to similar descriptions for distinct but related classes, prompting our novel image classification framework via hierarchical comparisons.""",2023,2023-11-01T00:26:40Z,,,
arXIv2023,Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding,Yes.,2,"""This paper introduces MedFusionRank, a novel approach to zero-shot medical information retrieval (MIR) that combines the strengths of pre-trained language models and statistical methods while addressing their limitations.""",2023,2023-10-31T16:26:33Z,,,
arXIv2023,Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning,Yes.,1,"""Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL.""",2023,2023-10-31T16:24:17Z,,,
arXIv2023,InstructCoder: Instruction Tuning Large Language Models for Code Editing,Yes.,3,"""Evaluated on a novel human-written execution-based benchmark dubbed EditEval, we found current models often struggle to fulfill the instructions.""",2023,2023-10-31T10:15:35Z,,,
arXIv2023,Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision,Yes.,3,"""However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs.""",2023,2023-10-31T03:39:23Z,,,
arXIv2023,Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models,Yes.,3,"""retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges.""",2023,2023-10-30T23:40:41Z,,,
arXIv2023,Chain-of-Thought Embeddings for Stance Detection on Social Media,Yes.,3,"""Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels."" and ""However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend before a",2023,2023-10-30T17:18:10Z,,,
arXIv2023,Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection,Yes.,1,"""In this work, we explore the use of large language models (LLMs) to provide explanations and additional background knowledge for decision tree NID systems.""",2023,2023-10-30T15:40:34Z,,,
arXIv2023,MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models,Yes.,3,"""existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones.""",2023,2023-10-30T13:33:21Z,,,
arXIv2023,MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion,Yes.,3,"""Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs.""",2023,2023-10-29T16:04:10Z,,,
arXIv2023,Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding,Yes.,3,"""Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the author's thoughts remains uncertain. This limitation hinders the practical applications of narrative understanding.""",2023,2023-10-28T18:47:57Z,,,
arXIv2023,Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers,Yes.,1,"""Our attack, LLMBkd, leverages language models to automatically insert diverse style-based triggers into texts.""",2023,2023-10-28T06:11:07Z,,,
arXIv2023,LLMs-Healthcare : Current Applications and Challenges of Large Language Models in various Medical Specialties,Yes.,3,"""Throughout our analysis, we explore the challenges and opportunities associated with integrating LLMs in healthcare, recognizing their potential across various medical specialties despite existing limitations.""",2023,2023-10-28T01:01:30Z,,,
arXIv2023,PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction,Yes.,1,"""While current unified information extraction models showcase state-of-the-art performance, they face challenges in understanding relationships between entities within intricate biomedical sentences.""",2023,2023-10-27T20:15:23Z,,,
arXIv2023,T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large Language Models,Yes.,3,"""LLMs are able to accurately predict the author of surprisingly short passages but are also prone to confidently misattribute texts to specific authors."" and ""we see indications that the presence of certain authors in the model's pre-training data affects predictive results in ways that are difficult to assess.""",2023,2023-10-27T20:04:57Z,,,
arXIv2023,Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models,Yes.,2,"""Such a formulation has the potential to overcome the cognitive and computational limitations of current models.""",2023,2023-10-27T17:04:10Z,,,
arXIv2023,ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models,Yes.,3,"""Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings",2023,2023-10-27T15:31:22Z,,,
arXIv2023,Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey,Yes.,3,"""This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements.""",2023,2023-10-27T05:01:20Z,,,
arXIv2023,Outlier Dimensions Encode Task-Specific Knowledge,Yes.,3,"""Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings.""",2023,2023-10-26T18:22:13Z,,,
arXIv2023,Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks,No.,1,The abstract discusses the Transformer model and its multi-head attention mechanism but does not specifically mention language models (LLMs or LMs).,2023,2023-10-26T14:43:07Z,,,
arXIv2023,ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation,Yes.,3,"""Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat.""",2023,2023-10-26T13:35:41Z,,,
arXIv2023,Cultural Adaptation of Recipes,Yes.,3,"""While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese.""",2023,2023-10-26T12:39:20Z,,,
arXIv2023,Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs,Yes.,1,"""Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset.""",2023,2023-10-26T04:13:49Z,,,
arXIv2023,math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories,Yes.,3,"""Despite their current limitations in logic and mathematical tasks,"" and ""Given the noted reasoning shortcomings of LLMs.""",2023,2023-10-25T23:54:04Z,,,
arXIv2023,BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation,Yes.,3,"""However, amidst their successes, a crucial issue persists",2023,2023-10-25T23:32:12Z,,,
arXIv2023,Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning,Yes.,3,"""We demonstrate that existing few-shot techniques do not perform well in this setting,"" and ""new safety issues and policies emerge, to which existing safety classifiers do not generalize well.""",2023,2023-10-25T19:57:07Z,,,
arXIv2023,LLM-FP4: 4-Bit Floating-Point Quantized Transformers,Yes.,1,"""We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner.""",2023,2023-10-25T17:59:32Z,,,
arXIv2023,Exploring Large Language Models for Code Explanation,Yes.,1,"""This study specifically delves into the task of generating natural-language summaries for code snippets, using various LLMs.""",2023,2023-10-25T14:38:40Z,,,
arXIv2023,Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons,Yes.,3,"""Previous methods either relied on fine-tuning LLMs on specific corpora or necessitated manually crafted prompts to elicit specific personalities from LLMs. However, the former approach is inefficient and costly, while the latter cannot precisely manipulate personality traits at a fine-grained level",2023,2023-10-25T12:16:33Z,,,
arXIv2023,Graph Agent: Explicit Reasoning Agent for Graphs,Yes.,1,"""In this paper, we introduce the Graph Agent (GA), an intelligent agent methodology of leveraging large language models (LLMs), inductive-deductive reasoning modules, and long-term memory for knowledge graph reasoning tasks.""",2023,2023-10-25T07:20:16Z,,,
arXIv2023,Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model,Yes.,3,"""Since we adopt multiple key-value strategies, LLM is hard to learn well among these keys.""",2023,2023-10-25T06:49:19Z,,,
arXIv2023,"Evaluating, Understanding, and Improving Constrained Text Generation for Large Language Models",Yes.,3,"""However, integrating intricate constraints into neural text generation, due to LLMs' opacity, remains challenging."" and ""Results illuminate LLMs' capacity and deficiency to incorporate constraints.""",2023,2023-10-25T03:58:49Z,,,
arXIv2023,ConDefects: A New Dataset to Address the Data Leakage Concern for LLM-based Fault Localization and Program Repair,Yes.,2,"""The code in existing widely-adopted benchmarks for these tasks was written before the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics.""",2023,2023-10-25T00:06:02Z,,,
arXIv2023,Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature,Yes.,2,"""While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools",2023,2023-10-24T19:43:39Z,,,
arXIv2023,Locally Differentially Private Document Generation Using Zero Shot Prompting,Yes.,2,"""Numerous studies have highlighted the privacy risks associated with pretrained large language models.""",2023,2023-10-24T18:25:13Z,,,
arXIv2023,What's Left? Concept Grounding with Logic-Enhanced Foundation Models,Yes.,3,"""However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language",2023,2023-10-24T17:50:20Z,,,
arXIv2023,Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation,Yes.,2,"""their application carries notable drawbacks"" and ""The operation of matching experts and tokens is discrete, which makes MoE models prone to issues like training instability and uneven expert utilization.""",2023,2023-10-24T16:03:57Z,,,
arXIv2023,NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes,Yes.,1,"""We introduce NoteChat, a novel cooperative multi-agent framework leveraging Large Language Models (LLMs) to generate patient-physician dialogues.""",2023,2023-10-24T15:59:43Z,,,
arXIv2023,Representation Learning with Large Language Models for Recommendation,Yes.,3,"""challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems.""",2023,2023-10-24T15:51:13Z,,,
arXIv2023,E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity,Yes.,3,"""Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands.""",2023,2023-10-24T15:27:15Z,,,
arXIv2023,Improving generalization in large language models by learning prefix subspaces,Yes.,3,"""Their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed.""",2023,2023-10-24T12:44:09Z,,,
arXIv2023,Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection,Yes.,3,"""The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data.""",2023,2023-10-24T11:55:16Z,,,
arXIv2023,Large Language Models are Temporal and Causal Reasoners for Video Question Answering,Yes.,3,"""such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, $\textit{i.e.}$, $\textit{linguistic bias}$, while ignoring visual content. This is also known as ungrounded guesses or hallucinations.""",2023,2023-10-24T11:44:39Z,,,
arXIv2023,A Survey on Detection of LLMs-Generated Content,Yes.,2,"""We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy.""",2023,2023-10-24T09:10:26Z,,,
arXIv2023,TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction,Yes.,3,"""One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs.""",2023,2023-10-24T06:56:38Z,,,
arXIv2023,SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation,Yes.,2,"""However, there is still a need for improvement in code translation functionality with efficient training techniques.""",2023,2023-10-24T06:04:28Z,,,
arXIv2023,GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions,Yes.,1,"""This paper investigates using large language models (LLMs) as a cost-effective, reference-free method for evaluating figure captions.""",2023,2023-10-23T23:24:57Z,,,
arXIv2023,DoGE: Domain Reweighting with Generalization Estimation,Yes.,3,"""Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains.""",2023,2023-10-23T22:51:58Z,,,
arXIv2023,Irreducible Curriculum for Language Model Pretraining,Yes.,3,"""Automatic data selection and curriculum design for training large language models is challenging,"" and ""It is difficult to apply traditional datapoint selection methods on large language models",2023,2023-10-23T22:41:33Z,,,
arXIv2023,Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number,Yes.,1,"""Deep architectures such as Transformers are sometimes criticized for having uninterpretable 'black-box' representations.""",2023,2023-10-23T17:53:47Z,,,
arXIv2023,Causal Inference Using LLM-Guided Discovery,Yes.,3,"""Acknowledging LLMs' limitations, we also study possible techniques to integrate LLMs with established causal discovery algorithms, including constraint-based and score-based methods, to enhance their performance.""",2023,2023-10-23T17:23:56Z,,,
arXIv2023,Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization,Yes.,3,"""the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios."" and ""the decentralized data is generally non-Independent and Identically Distributed (non-IID), which",2023,2023-10-23T16:37:59Z,,,
arXIv2023,Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models,Yes.,3,"""questions are starting to be raised about the dataset(s) they learned from. These questions range from potential bias or misinformation LLMs could retain from their training data to questions of copyright and fair use of human-generated text.""",2023,2023-10-23T15:00:46Z,,,
arXIv2023,Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing,Yes.,3,"""Surprisingly, our initial experiments find that fine-tuning for translation purposes even led to performance degradation.""",2023,2023-10-23T12:22:15Z,,,
arXIv2023,Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic,Yes.,2,"""there is a lack of comprehensive evaluation of their abilities in responding to multi-turn instructions in less-commonly tested languages like Arabic.""",2023,2023-10-23T11:40:04Z,,,
arXIv2023,Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages,Yes.,2,"""Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development.""",2023,2023-10-23T10:56:03Z,,,
arXIv2023,Reasoning about Ambiguous Definite Descriptions,Yes.,3,"""But no resources exist to evaluate how well Large Language Models can use explicit reasoning to resolve ambiguity in language."" and ""We find this to be a challenging task for recent LLMs.""",2023,2023-10-23T07:52:38Z,,,
arXIv2023,Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue,Yes.,3,"""Large language models (LLMs) generate responses that mimic pre-sales dialogues after fine-tuning, but lack domain-specific knowledge for accurate recommendations.""",2023,2023-10-23T07:00:51Z,,,
arXIv2023,Large Search Model: Redefining Search Stack in the Era of LLMs,Yes.,2,"""we present a series of proof-of-concept experiments and discuss the potential challenges associated with implementing this approach within real-world search systems.""",2023,2023-10-23T05:52:09Z,,,
arXIv2023,QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing,Yes.,3,"""Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality.""",2023,2023-10-23T03:03:58Z,,,
arXIv2023,Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design,Yes.,1,"""While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following capabilities and high quality reasoning, a goal-driven combinatorial search using LLMs has not been explored in detail.""",2023,2023-10-22T21:29:33Z,,,
arXIv2023,NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval,Yes.,3,"""the capabilities provided by LLMs are not the end of NER research, but rather an exciting beginning."" and ""We show that all of these are far from being solved.""",2023,2023-10-22T12:23:00Z,,,
arXIv2023,CXR-LLAVA: a multimodal large language model for interpreting chest X-ray images,Yes.,3,"""This study highlights the significant potential of multimodal LLMs for CXR interpretation, while also acknowledging the performance limitations.""",2023,2023-10-22T06:22:37Z,,,
arXIv2023,PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain,Yes.,2,"""most current benchmarks",2023,2023-10-22T02:20:38Z,,,
arXIv2023,Learning Reward for Physical Skills using Large Language Model,Yes.,3,"""the direct application of LLMs for proposing reward functions has its limitations such as numerical instability and inability to incorporate the environment feedback.""",2023,2023-10-21T19:10:06Z,,,
arXIv2023,LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions,Yes.,1,"""We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions.""",2023,2023-10-21T14:49:58Z,,,
arXIv2023,GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4,Yes.,3,"""we advise caution when using it in academic works to demonstrate improvements over other methods due to its dependence on the proprietary, black-box GPT model.""",2023,2023-10-21T12:30:33Z,,,
arXIv2023,HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts using Large Language Models,Yes.,3,"""We also perform human evaluations and interestingly, find that the rephrasings generated by GPT-3.5 outperform even the human-generated ground-truth rephrasings in the dataset. We also conduct detailed ablation studies to investigate why LLM",2023,2023-10-21T12:18:29Z,,,
arXIv2023,Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs,Yes.,3,"""One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public.""",2023,2023-10-21T10:21:17Z,,,
arXIv2023,Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models,Yes.,3,"""We overcome the tendency of hallucination in LLMs by incorporating finite-state constraints during decoding; these eliminate invalid outputs without requiring additional training.""",2023,2023-10-20T17:31:39Z,,,
arXIv2023,Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models,Yes.,3,"""However, a key challenge in data synthesis is that the synthesized dataset often suffers from a large distributional discrepancy from the real task data distribution.""",2023,2023-10-20T17:14:25Z,,,
arXIv2023,Benchmarking and Improving Text-to-SQL Generation under Ambiguity,Yes.,3,"""We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal.""",2023,2023-10-20T17:00:53Z,,,
arXIv2023,A Simple Baseline for Knowledge-Based Visual Question Answering,Yes.,2,"""A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API.""",2023,2023-10-20T15:08:17Z,,,
arXIv2023,Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning,Yes.,3,"""existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs.""",2023,2023-10-20T14:51:10Z,,,
arXIv2023,The Perils & Promises of Fact-checking with Large Language Models,Yes.,3,"""Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem."" and ""While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy.""",2023,2023-10-20T14:49:47Z,,,
arXIv2023,Teaching Language Models to Self-Improve through Interactive Demonstrations,Yes.,3,"""this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones.""",2023,2023-10-20T14:11:04Z,,,
arXIv2023,Democratizing Reasoning Ability: Tailored Learning from Large Language Model,Yes.,3,"""Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature.""",2023,2023-10-20T07:50:10Z,,,
arXIv2023,Test-Time Self-Adaptive Small Language Models for Question Answering,Yes.,3,"""they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks.""",2023,2023-10-20T06:49:32Z,,,
arXIv2023,MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model,Yes.,3,"""Even Large Language Models (LLMs) like GPT-4 fall short in this task.""",2023,2023-10-20T04:09:36Z,,,
arXIv2023,Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models,Yes.,3,"""existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process.""",2023,2023-10-19T23:02:29Z,,,
arXIv2023,Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models,Yes.,3,"""Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process.""",2023,2023-10-19T19:52:55Z,,,
arXIv2023,CLAIR: Evaluating Image Captions with Large Language Models,Yes.,1,"""we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions.""",2023,2023-10-19T17:59:01Z,,,
arXIv2023,Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation,Yes.,3,"""We argue that current interaction paradigms fall short, guiding users towards rapid convergence on a limited set of ideas, rather than empowering them to explore the vast latent design space in generative models.""",2023,2023-10-19T17:53:14Z,,,
arXIv2023,AgentTuning: Enabling Generalized Agent Abilities for LLMs,Yes.,3,"""However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world.""",2023,2023-10-19T15:19:53Z,,,
arXIv2023,Exploring Large Language Models as a Source of Common-Sense Knowledge for Robots,Yes.,3,"""Our experiments reveal limited effectiveness in the selective extraction of contextual action knowledge, suggesting that LLMs may not be sufficient on their own.""",2023,2023-10-19T14:20:30Z,,,
arXIv2023,TabuLa: Harnessing Language Models for Tabular Data Synthesis,Yes.,3,"""However, their long training time and limited re-usability on new tasks prevent them from replacing exiting tabular generative models."" and ""Through Tabula, we demonstrate the inherent limitation of employing pre-trained language models designed for natural language processing (NLP) in the context of tabular data synthesis.""",2023,2023-10-19T13:50:56Z,,,
arXIv2023,Product Attribute Value Extraction using Large Language Models,Yes.,3,"""State-of-the-art attribute/value extraction methods based on pre-trained language models (PLMs), such as BERT, face two drawbacks (i) the methods require significant amounts of task-specific training data and (ii) the fine-tuned models have problems to generalize to attribute values that were not part of the training data.""",2023,2023-10-19T07:39:00Z,,,
arXIv2023,Improving Generalization of Alignment with Human Preferences through Group Invariant Learning,Yes.,3,"""However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data.""",2023,2023-10-18T13:54:15Z,,,
arXIv2023,Enhancing Genetic Improvement Mutations Using Large Language Models,Yes.,3,"""We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits.""",2023,2023-10-18T10:24:14Z,,,
arXIv2023,Telecom AI Native Systems in the Age of Generative AI -- An Engineering Perspective,Yes.,2,"""Despite the enormous potential of FMs, ethical, regulatory, and operational challenges require careful consideration, especially in mission-critical telecom contexts.""",2023,2023-10-18T07:55:54Z,,,
arXIv2023,MISAR: A Multimodal Instructional System with Augmented Reality,Yes.,1,"""the potential of large language models (LLMs) in this landscape remains largely untapped.""",2023,2023-10-18T04:15:12Z,,,
arXIv2023,Language Models as Zero-Shot Trajectory Generators,Yes.,3,"""However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves.""",2023,2023-10-17T21:57:36Z,,,
arXIv2023,Automated Evaluation of Personalized Text Generation using Large Language Models,Yes.,2,"""even though interesting new challenges still remain.""",2023,2023-10-17T21:35:06Z,,,
arXIv2023,Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging,Yes.,2,"""While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives.""",2023,2023-10-17T20:22:13Z,,,
arXIv2023,Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue,Yes.,3,"""models based on in-context learning cannot acquire discipline-specific rules, resulting in lower performance in several categories. This limitation arises from the limited contextual information available for subject inference.""",2023,2023-10-17T14:52:33Z,,,
arXIv2023,Entity Matching using Large Language Models,Yes.,3,"""Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities.""",2023,2023-10-17T13:12:32Z,,,
arXIv2023,Watermarking LLMs with Weight Quantization,Yes.,1,"""Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed.""",2023,2023-10-17T13:06:59Z,,,
arXIv2023,Core Building Blocks: Next Gen Geo Spatial GPT Application,Yes.,1,"""This paper proposes MapGPT which is a novel approach that integrates the capabilities of language models, specifically large language models (LLMs), with spatial data processing techniques.""",2023,2023-10-17T06:59:31Z,,,
arXIv2023,EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset,Yes.,3,"""Automatic data generation through large models is a cost-effective method, but for open-domain multimodal dialogue tasks, there are still three drawbacks",2023,2023-10-17T03:28:29Z,,,
arXIv2023,Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models,Yes.,2,"""Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations.""",2023,2023-10-17T03:21:43Z,,,
arXIv2023,Unlocking Emergent Modularity in Large Language Models,Yes.,3,"""Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized.""",2023,2023-10-17T01:02:32Z,,,
arXIv2023,Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks,Yes.,3,"""LLM-camouflaged fake news content leads to substantial performance degradation of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), posing a significant challenge for automated detection in online ecosystems.""",2023,2023-10-16T21:05:12Z,,,
arXIv2023,Vision and Language Navigation in the Real World via Online Visual Language Mapping,Yes.,1,"""an LLMs-based instruction parser that converts the language instruction into a sequence of pre-defined macro-action descriptions.""",2023,2023-10-16T20:44:09Z,,,
arXIv2023,Towards reducing hallucination in extracting information from financial reports using Large Language Models,Yes.,3,"""extracting valuable insights from the Q\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic",2023,2023-10-16T18:45:38Z,,,
arXIv2023,Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes,Yes.,2,"""We evaluate state-of-the-art LLMs on our dataset and find that the expert's decision-making model is critical for LLMs to close the gap",2023,2023-10-16T17:59:50Z,,,
arXIv2023,LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts,Yes.,3,"""The initial Global Scene Generation utilizes object layouts and background context to create an initial scene but often falls short in faithfully representing object characteristics as specified in the prompts. To address this limitation, we introduce an Iterative Refinement Scheme that iteratively evaluates and refines box-level content to align them with their textual descriptions, recomposing objects as needed to ensure consistency.""",2023,2023-10-16T17:57:37Z,,,
arXIv2023,"""Mistakes Help Us Grow"": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms",Yes.,1,"""We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers' use of GMSL.""",2023,2023-10-16T17:56:07Z,,,
arXIv2023,Metric Ensembles For Hallucination Detection,Yes.,2,"""One of the most pressing problems related to generation of abstractive summaries is the need to reduce 'hallucinations,' information that was not included in the document being summarized, and which may be wholly incorrect.""",2023,2023-10-16T15:17:22Z,,,
arXIv2023,Large Language Model-Empowered Agents for Simulating Macroeconomic Activities,Yes.,1,"""Large language models (LLMs) have recently gained prominence in offering autonomous human-like characteristics.""",2023,2023-10-16T14:19:40Z,,,
arXIv2023,Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World,Yes.,3,"""We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme."" and ""However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64",2023,2023-10-16T09:19:18Z,,,
arXIv2023,LoBaSS: Gauging Learnability in Supervised Fine-tuning Data,Yes.,1,"""Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites.""",2023,2023-10-16T07:26:24Z,,,
arXIv2023,Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset,Yes.,2,"""While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels.""",2023,2023-10-16T06:53:12Z,,,
arXIv2023,On Generative Agents in Recommendation,Yes.,3,"""We delve into both the capabilities and limitations of Agent4Rec, aiming to explore an essential research question",2023,2023-10-16T06:41:16Z,,,
arXIv2023,JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning,Yes.,3,"""While instruction-tuning is used to fine-tune some LLMs, its precise roles in domain adaptation remain unknown."" and ""highlighting the persisting limitations of Japanese-centric models.""",2023,2023-10-16T05:28:28Z,,,
arXIv2023,TRANSOM: An Efficient Fault-Tolerant System for Training LLMs,Yes.,3,"""However, training LLMs with super-large-scale parameters requires large high-performance GPU clusters and long training periods lasting for months. Due to the inevitable hardware and software failures in large-scale clusters, maintaining uninterrupted and long-duration training is extremely challenging.""",2023,2023-10-16T04:06:52Z,,,
arXIv2023,Empower Text-Attributed Graphs Learning with Large Language Models (LLMs),Yes.,1,"""Recently, the advent of Large Language Models (LLMs) has introduced their powerful capabilities in information retrieval and text generation, which can greatly enhance the text attributes of graph data.""",2023,2023-10-15T16:04:28Z,,,
arXIv2023,ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors,Yes.,1,"""With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation.""",2023,2023-10-15T14:57:14Z,,,
arXIv2023,Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting,Yes.,1,"""To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions.""",2023,2023-10-15T03:04:17Z,,,
arXIv2023,Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model,Yes.,2,"""While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute.""",2023,2023-10-14T07:19:47Z,,,
arXIv2023,One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models,Yes.,3,"""However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency.""",2023,2023-10-14T05:43:09Z,,,
arXIv2023,A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models,Yes.,2,"""Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others.""",2023,2023-10-14T05:20:02Z,,,
arXIv2023,Large Language Model Unlearning,Yes.,3,"""removing harmful responses,"" ""erasing copyright-protected content as requested,"" and ""reducing hallucinations.""",2023,2023-10-14T00:32:55Z,,,
arXIv2023,Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents,Yes.,3,"""Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge.""",2023,2023-10-13T18:17:23Z,,,
arXIv2023,Ranking LLM-Generated Loop Invariants for Program Verification,Yes.,3,"""Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant.""",2023,2023-10-13T18:13:52Z,,,
arXIv2023,PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming,Yes.,3,"""Weakly-supervised document-level relation extraction faces significant challenges due to an imbalanced number 'no relation' instances and the failure of directly probing pretrained large language models for document relation extraction.""",2023,2023-10-13T17:23:17Z,,,
arXIv2023,Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration,Yes.,2,"""existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas.""",2023,2023-10-13T15:03:15Z,,,
arXIv2023,Split-and-Denoise: Protect large language model inference with local differential privacy,Yes.,3,"""the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage.""",2023,2023-10-13T14:17:33Z,,,
arXIv2023,Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model,Yes.,3,"""Integrating large language models (LLMs) into healthcare presents potential but faces challenges. Directly pre-training LLMs for domains like medicine is resource-heavy and sometimes unfeasible. Sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain specific insights.""",2023,2023-10-13T13:17:03Z,,,
arXIv2023,CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules,Yes.,3,"""solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules.""",2023,2023-10-13T10:17:48Z,,,
arXIv2023,Embarrassingly Simple Text Watermarks,Yes.,3,"""LLMs can generate texts that cannot be distinguished from human-written texts. This is a serious problem for the credibility of the text.""",2023,2023-10-13T07:44:05Z,,,
arXIv2023,Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs,Yes.,3,"""The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment."" and ""network pruning appears to lag behind in the era of LLMs, due mostly to its costly",2023,2023-10-13T07:38:52Z,,,
arXIv2023,SeqXGPT: Sentence-Level AI-Generated Text Detection,Yes.,2,"""Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs.""",2023,2023-10-13T07:18:53Z,,,
arXIv2023,Exploration with Principles for Diverse AI Supervision,Yes.,3,"""This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation.""",2023,2023-10-13T07:03:39Z,,,
arXIv2023,Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue,Yes.,3,"""existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses.""",2023,2023-10-13T03:38:38Z,,,
arXIv2023,Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception,Yes.,2,"""However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results.""",2023,2023-10-13T01:21:52Z,,,
arXIv2023,End-to-end Story Plot Generator,Yes.,3,"""existing plot generators (e.g., DOC (Yang et al., 2022a)) require hundreds to thousands of calls to LLMs (e.g., OpenAI API) in the planning stage of the story plot, which is costly and takes at least several minutes. Moreover, the hard-wired nature of the method makes the pipeline non-differentiable, blocking fast",2023,2023-10-13T00:49:59Z,,,
arXIv2023,LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models,Yes.,3,"""it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach.""",2023,2023-10-12T18:34:08Z,,,
arXIv2023,Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?,Yes.,3,"""We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained.""",2023,2023-10-12T17:32:09Z,,,
arXIv2023,LLM-augmented Preference Learning from Natural Language,Yes.,1,"""Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformer-based model, we investigate their ability to classify comparative text directly.""",2023,2023-10-12T17:17:27Z,,,
arXIv2023,Prometheus: Inducing Fine-grained Evaluation Capability in Language Models,Yes.,3,"""using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs.""",2023,2023-10-12T16:50:08Z,,,
arXIv2023,Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization,Yes.,3,"""evaluating how well large language models (LLMs) follow user instructions remains an open problem"" and ""limited work on the correctness of these methods has been conducted.""",2023,2023-10-12T15:07:11Z,,,
arXIv2023,Large language models can replicate cross-cultural differences in personality,Yes.,3,"""Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity.""",2023,2023-10-12T11:17:23Z,,,
arXIv2023,Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification,Yes.,3,"""Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability.""",2023,2023-10-12T08:24:15Z,,,
arXIv2023,Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques,Yes.,2,"""However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases.""",2023,2023-10-12T07:51:43Z,,,
arXIv2023,Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection,Yes.,3,"""Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis.""",2023,2023-10-12T04:14:28Z,,,
arXIv2023,Effects of Human Adversarial and Affable Samples on BERT Generalization,No.,1,"The abstract discusses BERT-based models and their generalization performance, but does not mention LLMs or their limitations.",2023,2023-10-12T03:20:43Z,,,
arXIv2023,GenTKG: Generative Forecasting on Temporal Knowledge Graph,Yes.,3,"""However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.""",2023,2023-10-11T18:27:12Z,,,
arXIv2023,Composite Backdoor Attacks Against Large Language Models,Yes.,3,"""However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks."" and ""Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.""",2023,2023-10-11T17:21:03Z,,,
arXIv2023,Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue,Yes.,1,"""knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.""",2023,2023-10-11T17:00:29Z,,,
arXIv2023,Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models,Yes.,3,"""However, a prevalent limitation persists in the effective communication with these popular T2I models, such as Stable Diffusion, using natural language descriptions. This typically makes an engaging image hard to obtain without expertise in prompt engineering with complex word compositions, magic tags, and annotations.""",2023,2023-10-11T16:53:40Z,,,
arXIv2023,Evaluating Large Language Models at Evaluating Instruction Following,Yes.,3,"""we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement.""",2023,2023-10-11T16:38:11Z,,,
arXIv2023,OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models,Yes.,3,"""the performance of current LLMs in Ops tasks is yet to be determined"" and ""discussed findings related to various topics, including model quantification, QA evaluation, and hallucination issues.""",2023,2023-10-11T16:33:29Z,,,
arXIv2023,In-Context Unlearning: Language Models as Few Shot Unlearners,Yes.,3,"""Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models."" and ""These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API.""",2023,2023-10-11T15:19:31Z,,,
arXIv2023,Fast-ELECTRA for Efficient Pre-training,Yes.,2,"""its potential is constrained by the training cost brought by the auxiliary model"" and ""This results in a substantial amount of training cost being expended in vain.""",2023,2023-10-11T09:55:46Z,,,
arXIv2023,Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction,Yes.,1,"""Specifically, we propose a model named LLM-TSE, wherein a large language model (LLM) extracts useful semantic cues from the user's typed text input.""",2023,2023-10-11T08:17:54Z,,,
arXIv2023,CoPAL: Corrective Planning of Robot Actions with Large Language Models,Yes.,1,"""this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots.""",2023,2023-10-11T07:39:42Z,,,
arXIv2023,CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving,Yes.,3,"""using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM.""",2023,2023-10-11T07:08:20Z,,,
arXIv2023,Adaptive Gating in Mixture-of-Experts based Language Models,Yes.,2,"""Little is discussed in prior research on the trade-off between computation per token and model performance.""",2023,2023-10-11T04:30:18Z,,,
arXIv2023,Online Speculative Decoding,Yes.,3,"""Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models.""",2023,2023-10-11T04:03:42Z,,,
arXIv2023,LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing,Yes.,3,"""However, the LLM's capabilities to deal with more complex networks, sophisticated vulnerabilities, and the sensitivity of prompts are open questions.""",2023,2023-10-10T18:49:20Z,,,
arXIv2023,A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging,Yes.,2,"""However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood.""",2023,2023-10-10T18:09:32Z,,,
arXIv2023,Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach,Yes.,3,"""the quality of the generated code is not guaranteed,"" and ""effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency.""",2023,2023-10-10T14:56:26Z,,,
arXIv2023,Automated clinical coding using off-the-shelf large language models,Yes.,3,"""Unsupervised pre-training alone does not guarantee precise knowledge of the ICD ontology and specialist clinical coding task.""",2023,2023-10-10T11:56:48Z,,,
arXIv2023,Constructive Large Language Models Alignment with Diverse Feedback,Yes.,3,"""current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available.""",2023,2023-10-10T09:20:14Z,,,
arXIv2023,Large Language Models for Propaganda Detection,Yes.,3,"""Further, this study analyzes the potential and challenges of LLMs in complex tasks like propaganda detection.""",2023,2023-10-10T08:46:10Z,,,
arXIv2023,Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction,Yes.,3,"""When LLMs are frozen, small (i.e., 345 million parameters) LLMs have a big gap to be competitive with unfrozen models; scaling LLMs up to billions of parameters makes frozen LLMs competitive with unfrozen LLMs.""",2023,2023-10-10T01:27:08Z,,,
arXIv2023,Cost-Efficient Prompt Engineering for Unsupervised Entity Resolution,Yes.,3,"""However, it is also well known that LLMs can pose risks,"" and ""Finally, we consider some limitations of LLMs when applied to ER.""",2023,2023-10-09T21:57:07Z,,,
arXIv2023,OptiMUS: Optimization Modeling Using MIP Solvers and large language models,Yes.,1,"""We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions.""",2023,2023-10-09T19:47:03Z,,,
arXIv2023,Improving Summarization with Human Edits,Yes.,1,"""Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training.""",2023,2023-10-09T16:52:07Z,,,
arXIv2023,GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence,Yes.,2,"""Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories.""",2023,2023-10-09T03:55:55Z,,,
arXIv2023,MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling,Yes.,3,"""though their inherent complexity and low controllability have raised questions about their suitability in clinical settings.""",2023,2023-10-08T17:00:04Z,,,
arXIv2023,Scaling Laws of RoPE-based Extrapolation,Yes.,2,"""In this process, we also explain the origin of the RoPE-based extrapolation issue by critical dimension for extrapolation.""",2023,2023-10-08T15:50:36Z,,,
arXIv2023,Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity,Yes.,3,"""Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size.""",2023,2023-10-08T14:22:58Z,,,
arXIv2023,On the Zero-Shot Generalization of Machine-Generated Text Detectors,Yes.,3,"""While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version.""",2023,2023-10-08T13:49:51Z,,,
arXIv2023,Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge,Yes.,1,"""Using the flexibility of LLMs to be prompted to do various novel tasks using zero-shot, few-shot, context-grounded prompting, we explore the feasibility of using LLMs to solve the ARC Challenge.""",2023,2023-10-08T12:37:28Z,,,
arXIv2023,DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models,Yes.,3,"""Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters.""",2023,2023-10-08T08:52:13Z,,,
arXIv2023,AvalonBench: Evaluating LLMs Playing the Game of Avalon,Yes.,3,"""Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots playing evil, while good-role bot achieves 38.2% win rate in the same setting.""",2023,2023-10-08T06:37:08Z,,,
arXIv2023,Revisiting Large Language Models as Zero-shot Relation Extractors,Yes.,3,"""On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE.""",2023,2023-10-08T06:17:39Z,,,
arXIv2023,Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models,Yes.,3,"""the massive size poses significant deployment challenges, particularly on resource-constrained hardware"" and ""existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges.""",2023,2023-10-08T05:16:28Z,,,
arXIv2023,Self-Knowledge Guided Retrieval Augmentation for Large Language Models,Yes.,3,"""the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs"" and ""the retrieved knowledge does not always help and even has a negative impact on original responses occasionally.""",2023,2023-10-08T04:22:33Z,,,
arXIv2023,Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models,Yes.,3,"""Yet, the standard CoT is less effective in problems demanding multiple reasoning steps. This limitation arises from the complex reasoning process in multi-step problems",2023,2023-10-07T08:56:28Z,,,
arXIv2023,Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis,Yes.,3,"""Currently, LLMs are unable to extract or comprehend information from images and may generate inaccurate text due to a lack of domain knowledge, limiting their use in forestry data analysis.""",2023,2023-10-07T06:12:39Z,,,
arXIv2023,Data-Centric Financial Large Language Models,Yes.,3,"""Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information.""",2023,2023-10-07T04:53:31Z,,,
arXIv2023,Label-free Node Classification on Graphs with Large Language Models (LLMS),Yes.,3,"""Yet, they face challenges in efficiently processing structural data and suffer from high inference costs.""",2023,2023-10-07T03:14:11Z,,,
arXIv2023,Confronting Reward Model Overoptimization with Constrained RLHF,Yes.,3,"""Large language models are typically aligned with human preferences by optimizing reward models fitted to human feedback... Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to overoptimization, wherein past a certain point, accumulating higher reward is associated with worse human ratings.""",2023,2023-10-06T16:59:17Z,,,
arXIv2023,Ada-Instruct: Adapting Instruction Generators for Complex Reasoning,Yes.,3,"""we found that in-context prompting cannot generate complex instructions with length  100 for tasks like code completion.""",2023,2023-10-06T13:28:04Z,,,
arXIv2023,Conversational Financial Information Retrieval Model (ConFIRM),Yes.,2,"""regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks.""",2023,2023-10-06T12:31:05Z,,,
arXIv2023,Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning,Yes.,3,"""Recognizing that most LLMs have limited tool-use capabilities, Reverse Chain limits LLMs to executing simple tasks, e.g., API Selection and Argument Completion.""",2023,2023-10-06T05:20:18Z,,,
arXIv2023,Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms,Yes.,1,"""we argue that LLMs hold considerable potential to improve simulation research on social media and many other complex social settings.""",2023,2023-10-05T18:26:06Z,,,
arXIv2023,HeaP: Hierarchical Policies for Web Actions using LLMs,Yes.,3,"""However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces.""",2023,2023-10-05T17:40:09Z,,,
arXIv2023,GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction,Yes.,3,"""Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out of the box.""",2023,2023-10-05T16:43:13Z,,,
arXIv2023,Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards,Yes.,3,"""Memory-efficient large language models are good at refining text input for better readability. However, controllability is a matter of concern when it comes to text generation tasks with long inputs, such as multi-document summarization.""",2023,2023-10-05T11:29:09Z,,,
arXIv2023,Fine-tune Language Models to Approximate Unbiased In-context Learning,Yes.,3,"""the performance of models heavily relies on the quality of the input prompt when implementing in-context learning. Biased or imbalanced input prompts can significantly degrade the performance of language models.""",2023,2023-10-05T06:16:01Z,,,
arXIv2023,Investigating the Limitation of CLIP Models: The Worst-Performing Categories,Yes.,3,"""This phenomenon reveals the potential risks associated with using CLIP models, particularly in risk-sensitive applications where specific categories hold significant importance.""",2023,2023-10-05T05:37:33Z,,,
arXIv2023,Learning Personalized Story Evaluation,Yes.,3,"""While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers' personal preferences.""",2023,2023-10-05T04:15:48Z,,,
arXIv2023,InstructProtein: Aligning Human and Protein Language via Knowledge Instruction,Yes.,3,"""Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins.""",2023,2023-10-05T02:45:39Z,,,
arXIv2023,Predicting Emergent Abilities with Infinite Resolution Evaluation,Yes.,3,"""Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the 'emergent abilities'.""",2023,2023-10-05T02:35:00Z,,,
arXIv2023,Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference,Yes.,3,"""However, LLM responses to math questions can be incorrect or mismatched to the educational context - such as being misaligned with a school's curriculum.""",2023,2023-10-04T22:09:28Z,,,
arXIv2023,$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis,Yes.,2,"""Yet, training value-based methods presents challenges due to the enormous search space inherent to program synthesis.""",2023,2023-10-04T21:40:36Z,,,
arXIv2023,Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions,Yes.,3,"""However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs).""",2023,2023-10-04T17:57:33Z,,,
arXIv2023,Large language models in textual analysis for gesture selection,Yes.,1,"""Here, we approach these challenges by using large language models (LLMs) to show that these powerful models of large amounts of data can be adapted for gesture analysis and generation.""",2023,2023-10-04T14:46:37Z,,,
arXIv2023,Conversational Health Agents: A Personalized LLM-Powered Agent Framework,Yes.,3,"""Current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation aspects. However, they offer limited agent capabilities, specifically lacking multi-step problem-solving, personalized conversations, and multimodal data analysis.""",2023,2023-10-03T18:54:10Z,,,
arXIv2023,Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation,Yes.,3,"""However, these LLMs are often trained on vast amounts of publicly available code, which may include test cases that do not adhere to best practices and may even contain test smells (anti-patterns).""",2023,2023-10-03T18:48:31Z,,,
arXIv2023,Editing Personality for Large Language Models,Yes.,3,"""We conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in LLMs. Our intriguing findings uncover potential challenges of the proposed task, illustrating several remaining issues.""",2023,2023-10-03T16:02:36Z,,,
arXIv2023,OceanGPT: A Large Language Model for Ocean Science Tasks,Yes.,3,"""current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge.""",2023,2023-10-03T13:17:35Z,,,
arXIv2023,Formalizing Natural Language Intent into Program Specifications via Large Language Models,Yes.,3,"""However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.""",2023,2023-10-03T06:55:45Z,,,
arXIv2023,Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs,Yes.,1,"""In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs).""",2023,2023-10-03T05:17:08Z,,,
arXIv2023,Time-LLM: Time Series Forecasting by Reprogramming Large Language Models,Yes.,3,"""the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities.""",2023,2023-10-03T01:31:25Z,,,
arXIv2023,Large Language Models for Test-Free Fault Localization,Yes.,1,"""Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization.""",2023,2023-10-03T01:26:39Z,,,
arXIv2023,Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making,Yes.,3,"""highlighted potential errors in LLM outputs, ranging from 5% to 30%.""",2023,2023-10-03T00:08:23Z,,,
arXIv2023,VAL: Interactive Task Learning with GPT Dialog Parsing,Yes.,3,"""Large language models (LLMs) are resistant to brittleness but are not interpretable and cannot learn incrementally.""",2023,2023-10-02T20:45:41Z,,,
arXIv2023,Towards reporting bias in visual-language datasets: bimodal augmentation by decoupling object-attribute association,Yes.,2,"""Reporting bias arises when people assume that some knowledge is universally understood and hence, do not necessitate explicit elaboration."" and ""We employ large language models (LLMs) in conjunction with a grounding object detector to extract target objects.""",2023,2023-10-02T16:48:50Z,,,
arXIv2023,SPELL: Semantic Prompt Evolution based on a LLM,Yes.,2,"""We further explore the evolution process and discuss on the limitations, potential possibilities and future work.""",2023,2023-10-02T14:51:16Z,,,
arXIv2023,Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback,Yes.,3,"""for classifying individual utterances, there is still room for improvement of automated methods compared to human-level judgments.""",2023,2023-10-02T12:11:17Z,,,
arXIv2023,GraphText: Graph Reasoning in Text Space,Yes.,3,"""LLMs have not made significant advancements in the realm of graph machine learning. This limitation arises because graphs encapsulate distinct relational data, making it challenging to transform them into natural language that LLMs understand.""",2023,2023-10-02T11:03:57Z,,,
arXIv2023,Towards human-like spoken dialogue generation between AI agents from written dialogue,Yes.,3,"""The advent of large language models (LLMs) has made it possible to generate natural written dialogues between two agents. However, generating human-like spoken dialogues from these written dialogues remains challenging.""",2023,2023-10-02T11:03:20Z,,,
arXIv2023,DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models,Yes.,2,"""This issue becomes more pronounced in the setting of large language models and text-to-image models.""",2023,2023-10-02T04:59:19Z,,,
arXIv2023,Enabling Language Models to Implicitly Learn Self-Improvement,Yes.,3,"""However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses."" and ""It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful).""",2023,2023-10-02T04:29:40Z,,,
arXIv2023,Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models,Yes.,3,"""the findings indicate that LLMs excel most in abductive reasoning, followed by deductive reasoning, while they are least effective at inductive reasoning.""",2023,2023-10-02T01:00:50Z,,,
arXIv2023,LEGO-Prover: Neural Theorem Proving with Growing Libraries,Yes.,3,"""Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process.""",2023,2023-10-01T12:47:59Z,,,
arXIv2023,Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals,Yes.,3,"""While this approach is demonstrated to be very effective, applying LLM at inference-time is costly.""",2023,2023-10-01T07:31:04Z,,,
arXIv2023,GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length,Yes.,2,"""The evolving sophistication and intricacies of Large Language Models (LLMs) yield unprecedented advancements, yet they simultaneously demand considerable computational resources and incur significant costs.""",2023,2023-10-01T05:25:24Z,,,
arXIv2023,Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting,Yes.,3,"""While LLMs have demonstrated impressive performance across various text-related tasks, they encounter challenges in tasks associated with reasoning.""",2023,2023-09-30T06:25:27Z,,,
arXIv2023,Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment,Yes.,3,"""However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values."" and ""Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss.""",2023,2023-09-30T01:23:22Z,,,
arXIv2023,SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation,Yes.,3,"""Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets.""",2023,2023-09-29T18:25:46Z,,,
arXIv2023,L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models,Yes.,3,"""This enables us to identify and analyze the typical failure modes across various tasks and models.""",2023,2023-09-29T17:57:00Z,,,
arXIv2023,LLM-grounded Video Diffusion Models,Yes.,3,"""current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion.""",2023,2023-09-29T17:54:46Z,,,
arXIv2023,Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings,No.,1,The paper discusses deep neural networks in general and does not specifically mention language models (LLMs or LMs).,2023,2023-09-29T16:04:55Z,,,
arXIv2023,Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4,Yes.,1,"""GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities.""",2023,2023-09-29T14:30:03Z,,,
arXIv2023,LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games,Yes.,3,"""Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks.""",2023,2023-09-29T13:33:06Z,,,
arXIv2023,Training and inference of large language models using 8-bit floating point,Yes.,1,"""This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B.""",2023,2023-09-29T13:24:33Z,,,
arXIv2023,Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training,Yes.,3,"""As a result, these methods will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning.""",2023,2023-09-29T12:20:19Z,,,
arXIv2023,LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud,Yes.,2,"""the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves.""",2023,2023-09-29T11:46:07Z,,,
arXIv2023,Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?,Yes.,3,"""While our findings show that the latest commercial models outperform their forerunners in terms of proficiency with the Turtle language, they also reveal an apparent weakness. These models fall short when it comes to adhering strictly to the output formatting constraints, a crucial requirement in this context.""",2023,2023-09-29T10:36:04Z,,,
arXIv2023,I Wish to Have an Argument: Argumentative Reasoning in Large Language Models,Yes.,3,"""We find that, although LLMs are able to match or surpass the state-of-the-art in AM and APE, their argumentative reasoning performance is very dependent on the input and output representation. We also find an 'exemplar effect', where too many exemplars increasingly become detrimental for task performance.""",2023,2023-09-29T02:41:38Z,,,
arXIv2023,GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond,Yes.,3,"""there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations.""",2023,2023-09-28T16:43:35Z,,,
arXIv2023,Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks,Yes.,1,"""By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks.""",2023,2023-09-28T11:14:52Z,,,
arXIv2023,Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints,Yes.,2,"""The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment.""",2023,2023-09-28T08:29:44Z,,,
arXIv2023,AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events,Yes.,1,"""Recently, Large Language Models (LLMs) have shown promise in effectively identifying and cataloging AEs within clinical reports.""",2023,2023-09-28T03:53:21Z,,,
arXIv2023,ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers,Yes.,1,"""We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU",2023,2023-09-28T02:55:01Z,,,
arXIv2023,Lyra: Orchestrating Dual Correction in Automated Theorem Proving,Yes.,3,"""Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated.""",2023,2023-09-27T17:29:41Z,,,
arXIv2023,Large Language Model Routing with Benchmark Datasets,Yes.,3,"""We demonstrate the utility and limitations of learning model routers from various benchmark datasets, where we consistently improve performance upon using any single model for all tasks.""",2023,2023-09-27T17:08:40Z,,,
arXIv2023,Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models,Yes.,3,"""However, efficiently serving LLMs has been a challenge due to the large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices).""",2023,2023-09-27T09:48:31Z,,,
arXIv2023,Beyond the Chat: Executable and Verifiable Text-Editing with LLMs,Yes.,3,"""Because LLMs are known to introduce factual errors, Inksync also supports a 3-stage approach to mitigate this risk",2023,2023-09-27T00:56:17Z,,,
arXIv2023,RAGAS: Automated Evaluation of Retrieval Augmented Generation,Yes.,1,"""RAG systems are composed of a retrieval and an LLM based generation module,"" and ""reducing the risk of hallucinations.""",2023,2023-09-26T19:23:54Z,,,
arXIv2023,ConPET: Continual Parameter-Efficient Tuning for Large Language Models,Yes.,3,"""This is extremely challenging for large language models (LLMs) with vanilla full-parameter tuning due to high computation costs, memory consumption, and forgetting issue.""",2023,2023-09-26T08:52:04Z,,,
arXIv2023,QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models,Yes.,3,"""Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices.""",2023,2023-09-26T07:22:23Z,,,
arXIv2023,Are Human-generated Demonstrations Necessary for In-context Learning?,Yes.,3,"""the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations.""",2023,2023-09-26T05:10:08Z,,,
arXIv2023,Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator,Yes.,1,"""we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence""",2023,2023-09-25T19:42:16Z,,,
arXIv2023,Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering,Yes.,2,"""However, the relative contributions of the vision encoder and the language model in these tasks remain unclear.""",2023,2023-09-25T07:01:16Z,,,
arXIv2023,Resolving References in Visually-Grounded Dialogue via Text Generation,Yes.,1,"""Vision-language models (VLMs) have shown to be effective at image retrieval based on simple text queries, but text-image retrieval based on conversational input remains a challenge.""",2023,2023-09-23T17:07:54Z,,,
arXIv2023,A Chat About Boring Problems: Studying GPT-based text normalization,Yes.,3,"""we note key limitations in the conventional design of text normalization tasks.""",2023,2023-09-23T16:32:59Z,,,
arXIv2023,Towards LLM-guided Causal Explainability for Black-box Text Classifiers,Yes.,3,"""model qualities like explainability and interpretability, albeit highly desirable, are becoming harder challenges to tackle and solve.""",2023,2023-09-23T11:22:28Z,,,
arXIv2023,AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling,Yes.,3,"""However, developing an LLM for problem formulation is challenging, due to training data, token limitations, and lack of appropriate performance metrics.""",2023,2023-09-22T23:45:21Z,,,
arXIv2023,Investigating Large Language Models and Control Mechanisms to Improve Text Readability of Biomedical Abstracts,Yes.,1,"""In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (PLABA).""",2023,2023-09-22T22:47:32Z,,,
arXIv2023,Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation,Yes.,2,"""intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact.""",2023,2023-09-22T21:55:18Z,,,
arXIv2023,Contextual Emotion Estimation from Image Captions,Yes.,2,"""One initial challenge is to construct a caption that describes a person within a scene with information relevant for emotion perception."" and ""accuracy can depend on the emotion concept.""",2023,2023-09-22T18:44:34Z,,,
arXIv2023,Construction contract risk identification based on knowledge-augmented language model,Yes.,3,"""While large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues.""",2023,2023-09-22T05:27:06Z,,,
arXIv2023,Studying and improving reasoning in humans and machines,Yes.,3,"""most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning"" and ""an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models limitations disappearing almost entirely in more recent LLMs releases.""",2023,2023-09-21T21:02:05Z,,,
arXIv2023,Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges,Yes.,3,"""highlighting the unique opportunities and challenges.""",2023,2023-09-21T18:48:02Z,,,
arXIv2023,LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent,Yes.,1,"""While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline.""",2023,2023-09-21T17:59:45Z,,,
arXIv2023,"AceGPT, Localizing Large Language Models in Arabic",Yes.,3,"""Significant concerns emerge when addressing cultural sensitivity and local values.""",2023,2023-09-21T13:20:13Z,,,
arXIv2023,LPML: LLM-Prompting Markup Language for Mathematical Reasoning,Yes.,3,"""addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge"" and ""control the undesired behaviors of LLMs.""",2023,2023-09-21T02:46:20Z,,,
arXIv2023,LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination,Yes.,3,"""While one can fully train an LLM for this objective, the resource consumption is unaffordable."" and ""We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly.""",2023,2023-09-21T00:34:33Z,,,
arXIv2023,Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation,Yes.,3,"""We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.""",2023,2023-09-20T22:30:20Z,,,
arXIv2023,Towards Effective Disambiguation for Machine Translation with Large Language Models,Yes.,3,"""Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to handle many such cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling",2023,2023-09-20T22:22:52Z,,,
arXIv2023,Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning,Yes.,1,"""To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs).""",2023,2023-09-20T17:39:13Z,,,
arXIv2023,GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction,Yes.,1,"""Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties.""",2023,2023-09-20T17:21:43Z,,,
arXIv2023,Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction,Yes.,3,"""it is not straightforward to specify a complex format to generate explanations, because explicit control of generation is difficult with prompts.""",2023,2023-09-20T16:14:10Z,,,
arXIv2023,Speak While You Think: Streaming Speech Synthesis During Text Generation,Yes.,1,"""Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text.""",2023,2023-09-20T11:00:15Z,,,
arXIv2023,Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training,Yes.,3,"""While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs.""",2023,2023-09-19T21:01:40Z,,,
arXIv2023,FRASIMED: a Clinical French Annotated Resource Produced through Crosslingual BERT-Based Annotation Projection,Yes.,2,"""NLP applications such as named entity recognition (NER) for low-resource corpora do not benefit from recent advances in the development of large language models (LLMs) where there is still a need for larger annotated datasets.""",2023,2023-09-19T17:17:28Z,,,
arXIv2023,GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models,Yes.,3,"""we first perform an in-depth investigation into LLMs' limitations and capabilities for AI accelerator design.""",2023,2023-09-19T16:14:57Z,,,
arXIv2023,Model Leeching: An Extraction Attack Targeting LLMs,Yes.,1,"""Model Leeching is a novel extraction attack targeting Large Language Models (LLMs), capable of distilling task-specific knowledge from a target LLM into a reduced parameter model.""",2023,2023-09-19T11:45:29Z,,,
arXIv2023,Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models,Yes.,1,"""Large language models exhibit superior capabilities in processing and understanding language, yet their applications in educational contexts remain underexplored.""",2023,2023-09-19T09:04:15Z,,,
arXIv2023,RadOnc-GPT: A Large Language Model for Radiation Oncology,Yes.,3,"""However, our model's clinical relevance requires confirmation, and it specializes in only the aforementioned three specific tasks and lacks broader applicability. Furthermore, its evaluation through ROUGE scores might not reflect the true semantic and clinical accuracy - challenges we intend to address in future research.""",2023,2023-09-18T21:15:02Z,,,
arXIv2023,Prompt a Robot to Walk with Large Language Models,Yes.,3,"""this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions.""",2023,2023-09-18T17:50:17Z,,,
arXIv2023,Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding,Yes.,3,"""Recent advances in transformer-based large language model (LLM) technologies have been applied to code synthesis. However, studies show that many of such synthesized codes contain vulnerabilities.""",2023,2023-09-18T14:47:34Z,,,
arXIv2023,Proposition from the Perspective of Chinese Language: A Chinese Proposition Classification Evaluation Benchmark,Yes.,3,"""ChatGPT performs poorly, but its classification ability can be improved by providing more proposition information. Many issues are still far from being resolved and require further study.""",2023,2023-09-18T09:18:39Z,,,
arXIv2023,Progressive Text-to-Image Diffusion with Soft Latent Direction,Yes.,3,"""while a pre-trained text-to-image diffusion model adeptly handles one or two entities, it often falters when dealing with a greater number.""",2023,2023-09-18T04:01:25Z,,,
arXIv2023,Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM,Yes.,3,"""Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts.""",2023,2023-09-18T02:07:22Z,,,
arXIv2023,Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model,Yes.,2,"""However, there is a limited understanding of LLMs' role during the communication.""",2023,2023-09-17T19:46:03Z,,,
arXIv2023,RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification,Yes.,2,"""Preliminary tests on the dataset using GPT-based and BERT-based models reveal variations in the models' performance across different labels, indicating that the dataset effectively challenges the ability of various language models to verify the authenticity of such information.""",2023,2023-09-16T18:35:08Z,,,
arXIv2023,ODSum: New Benchmarks for Open Domain Multi-Document Summarization,Yes.,3,"""We also found that LLMs suffer great performance loss from retrieving errors.""",2023,2023-09-16T11:27:34Z,,,
arXIv2023,Rethinking Learning Rate Tuning in the Era of Large Language Models,Yes.,3,"""Existing learning rate policies are primarily designed for training traditional deep neural networks (DNNs), which may not work well for LLM fine-tuning.""",2023,2023-09-16T03:37:00Z,,,
arXIv2023,S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs,Yes.,3,"""These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts.""",2023,2023-09-16T00:59:23Z,,,
arXIv2023,Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata,Yes.,3,"""These results demonstrate that the knowledge of LLMs varies significantly depending on the domain and that further experimentation is required to determine the circumstances under which LLMs can be used for automatic Knowledge Base (e.g., Wikidata) completion and correction.""",2023,2023-09-15T15:51:14Z,,,
arXIv2023,Adversarial Attacks on Tables with Entity Swap,Yes.,3,"""Adversarial attacks on text have been shown to greatly affect the performance of LLMs, but currently, there are no attacks targeting tabular language models.""",2023,2023-09-15T15:03:33Z,,,
arXIv2023,CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending,Yes.,3,"""anomalous behaviors harming long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention unveiled by our work.""",2023,2023-09-15T09:36:51Z,,,
arXIv2023,Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level,Yes.,1,"""Our work demonstrates that large language model (LLM) pre-trained on texts can not only solve pure math word problems, but also physics word problems, whose solution requires calculation and inference based on prior physical knowledge.""",2023,2023-09-15T06:13:06Z,,,
arXIv2023,LASER: LLM Agent with State-Space Exploration for Web Navigation,Yes.,3,"""Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance.""",2023,2023-09-15T05:44:08Z,,,
arXIv2023,Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding,Yes.,1,"""We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model.""",2023,2023-09-15T05:34:32Z,,,
arXIv2023,"Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies",Yes.,3,"""However, using LLMs to generate a user intent taxonomy and apply it for log analysis can be problematic for two main reasons",2023,2023-09-14T20:46:48Z,,,
arXIv2023,Ambiguity-Aware In-Context Learning with Large Language Models,Yes.,3,"""However, LLMs are sensitive to the choice of prompts,"" and ""labels paired with the demonstrations bias the model predictions.""",2023,2023-09-14T17:48:34Z,,,
arXIv2023,TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild,Yes.,3,"""their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following.""",2023,2023-09-14T15:34:01Z,,,
arXIv2023,Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text,Yes.,2,"""These models can potentially deceive by generating artificial text that appears to be human-generated.""",2023,2023-09-14T13:05:20Z,,,
arXIv2023,Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos,No.,1,"The abstract does not mention any language models, focusing instead on diffusion models and visual representation learning.",2023,2023-09-14T03:25:37Z,,,
arXIv2023,RAIN: Your Language Models Can Align Themselves without Finetuning,Yes.,2,"""Large language models (LLMs) often demonstrate inconsistencies with human preferences.""",2023,2023-09-13T17:59:09Z,,,
arXIv2023,Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding,Yes.,2,"""Hallucinations and off-target translation remain unsolved problems in MT, especially for low-resource languages and massively multilingual models.""",2023,2023-09-13T17:15:27Z,,,
arXIv2023,Generative AI,Yes.,2,"""we introduce limitations of current generative AI and provide an agenda for Business & Information Systems Engineering (BISE) research.""",2023,2023-09-13T08:21:59Z,,,
arXIv2023,Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics,Yes.,1,"""Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning.""",2023,2023-09-13T02:56:56Z,,,
arXIv2023,Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL,Yes.,3,"""One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.""",2023,2023-09-13T01:12:52Z,,,
arXIv2023,Statistical Rejection Sampling Improves Preference Optimization,Yes.,2,"""Improving the alignment of language models with human preferences remains an active research challenge.""",2023,2023-09-13T01:07:25Z,,,
arXIv2023,Exploring Large Language Models for Ontology Alignment,Yes.,1,"""This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies.""",2023,2023-09-12T17:01:02Z,,,
arXIv2023,The Moral Machine Experiment on Large Language Models,Yes.,3,"""there are significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans.""",2023,2023-09-12T04:49:39Z,,,
arXIv2023,Balanced and Explainable Social Media Analysis for Public Health with Large Language Models,Yes.,3,"""the costs of training an in-domain LLM for every specific public health task are especially expensive"" and ""such kinds of in-domain datasets from social media are generally highly imbalanced, which will hinder the efficiency of LLMs tuning.""",2023,2023-09-12T04:15:34Z,,,
arXIv2023,Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs,Yes.,3,"""However, their deployment poses significant challenges due to their considerable memory and storage requirements.""",2023,2023-09-11T14:58:23Z,,,
arXIv2023,DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning,Yes.,3,"""PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying.""",2023,2023-09-11T00:02:05Z,,,
arXIv2023,Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps,Yes.,1,"""large language models (LLMs) like ChatGPT have shown great potential in tasks such as context understanding and reasoning, displaying a high degree of consistency with human natural language.""",2023,2023-09-10T13:06:45Z,,,
arXIv2023,Leveraging Large Language Models for Exploiting ASR Uncertainty,Yes.,3,"""LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input."" and ""a high word-error-rate can limit the LLM's ability to understand the spoken intent.""",2023,2023-09-09T17:02:33Z,,,
arXIv2023,Code-Style In-Context Learning for Knowledge-Based Question Answering,Yes.,3,"""current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate.""",2023,2023-09-09T06:27:00Z,,,
arXIv2023,LLMCad: Fast and Scalable On-device Large Language Model Inference,Yes.,3,"""the limited memory capacity of these devices presents a formidable challenge to the scalability of such models.""",2023,2023-09-08T10:44:19Z,,,
arXIv2023,Matching Table Metadata with Business Glossaries Using Large Language Models,Yes.,1,"""In this work, we leverage the power of large language models (LLMs) to design generic matching methods that do not require manual tuning and can identify complex relations between column names and glossaries.""",2023,2023-09-08T02:23:59Z,,,
arXIv2023,Evaluation of large language models for discovery of gene set function,Yes.,2,"""Gemini-Pro and Mixtral-Instruct showed ability in naming but were falsely confident for random sets, whereas Llama2-70b had poor performance overall.""",2023,2023-09-07T21:10:48Z,,,
arXIv2023,ConDA: Contrastive Domain Adaptation for AI-generated Text Detection,Yes.,2,"""Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck.""",2023,2023-09-07T19:51:30Z,,,
arXIv2023,Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media,Yes.,3,"""we deeply explored and analyzed the performance of these large language models from a psychological perspective, shedding light on their strengths and limitations in identifying and understanding complex human emotions.""",2023,2023-09-07T08:50:46Z,,,
arXIv2023,Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences,Yes.,2,"""We compare the performance of LLMs to several state-of-the-art benchmarks and highlight opportunities for future research in this area.""",2023,2023-09-07T04:15:17Z,,,
arXIv2023,Large Language Models as Optimizers,Yes.,1,"""In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language.""",2023,2023-09-07T00:07:15Z,,,
arXIv2023,Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks,No.,1,The abstract focuses on self-supervised learning for Digital Elevation Models and does not discuss LLMs or their limitations.,2023,2023-09-06T21:20:10Z,,,
arXIv2023,Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection,Yes.,3,"""Previous research on secure reasoning using multi-party computation (MPC) has proven to be impractical for LLM applications due to its time-consuming and communication-intensive nature.""",2023,2023-09-06T14:54:11Z,,,
arXIv2023,Aligning Large Language Models for Clinical Tasks,Yes.,3,"""effective alignment of LLMs remains a crucial challenge when deploying them for specific clinical applications.""",2023,2023-09-06T10:20:06Z,,,
arXIv2023,Norm Tweaking: High-performance Low-bit Quantization of Large Language Models,Yes.,3,"""attempts at lower-bit quantization often result in severe performance degradation.""",2023,2023-09-06T06:51:15Z,,,
arXIv2023,Large Language Models for Automated Open-domain Scientific Hypotheses Discovery,Yes.,1,"""this is the first work showing that LLMs are able to generate novel (""not existing in the literature"") and valid (""reflecting reality"") scientific hypotheses.""",2023,2023-09-06T05:19:41Z,,,
arXIv2023,Physically Grounded Vision-Language Models for Robotic Manipulation,Yes.,3,"""current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects.""",2023,2023-09-05T20:21:03Z,,,
arXIv2023,Automating Behavioral Testing in Machine Translation,Yes.,2,"""To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations.""",2023,2023-09-05T19:40:45Z,,,
arXIv2023,Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices,Yes.,3,"""Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks.""",2023,2023-09-05T17:40:34Z,,,
arXIv2023,PromptTTS 2: Describing and Generating Voices with Text Prompt,Yes.,1,"""we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts.""",2023,2023-09-05T14:45:27Z,,,
arXIv2023,Sample Size in Natural Language Processing within Healthcare Research,No.,1,The abstract does not mention LLMs or any specific limitations related to them. It focuses on sample size calculations for text classification tasks in the healthcare domain.,2023,2023-09-05T13:42:43Z,,,
arXIv2023,Language Models for Novelty Detection in System Call Traces,No.,1,The abstract focuses on novelty detection methodologies using language models for system call traces but does not specifically mention LLMs or their limitations.,2023,2023-09-05T13:11:40Z,,,
arXIv2023,Data-Juicer: A One-Stop Data Processing System for Large Language Models,Yes.,2,"""Firstly, the possible data sources for forming data recipes are truly heterogeneous and massive with various qualities. Secondly, it is extremely expensive to precisely evaluate data recipes' impact on LLMs' performance.""",2023,2023-09-05T08:22:07Z,,,
arXIv2023,"On the Planning, Search, and Memorization Capabilities of Large Language Models",Yes.,3,"""we investigate the potential of the state-of-the-art large language model (GPT-4) for planning tasks. We explore its effectiveness in multiple planning subfields, highlighting both its strengths and limitations.""",2023,2023-09-05T00:19:31Z,,,
arXIv2023,Softmax Bias Correction for Quantized Generative Models,Yes.,2,"""PTQ methods commonly keep the softmax activation in higher precision as it has been shown to be very sensitive to quantization noise.""",2023,2023-09-04T17:29:31Z,,,
arXIv2023,eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models,Yes.,3,"""However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices."" and ""its training overhead is prohibitively significant for LLM fine-tuning.""",2023,2023-09-02T15:16:35Z,,,
arXIv2023,LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs,Yes.,3,"""However, widespread LLM integration presents a challenge for small businesses due to the high expenses of LLM API usage. Costs rise rapidly when domain-specific data (context) is used alongside queries for accurate domain-specific LLM responses. ... However, this can also filter out useful information that is necessary to answer some domain-specific queries.""",2023,2023-09-02T06:33:18Z,,,
arXIv2023,Baseline Defenses for Adversarial Attacks Against Aligned Language Models,Yes.,3,"""understand their security vulnerabilities"" and ""we find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs.""",2023,2023-09-01T17:59:44Z,,,
arXIv2023,Taken out of context: On measuring situational awareness in LLMs,Yes.,3,"""An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment"" and ""Situational awareness may emerge unexpectedly as a byproduct of model scaling.""",2023,2023-09-01T17:27:37Z,,,
arXIv2023,FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking,Yes.,3,"""However, their knowledge may not always be up-to-date or sufficient, potentially leading to inaccuracies in fact-checking.""",2023,2023-09-01T04:14:39Z,,,
arXIv2023,RepCodec: A Speech Representation Codec for Speech Tokenization,Yes.,3,"""However, this discretization gives rise to a loss of information, consequently impairing overall performance.""",2023,2023-08-31T23:26:10Z,,,
arXIv2023,LLM in the Shell: Generative Honeypots,Yes.,1,"""This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models.""",2023,2023-08-31T22:05:46Z,,,
arXIv2023,Towards Multilingual Automatic Dialogue Evaluation,Yes.,3,"""We empirically show that the naive approach of finetuning a pretrained multilingual encoder model with translated data is insufficient to outperform the strong baseline of finetuning a multilingual model with only source data.""",2023,2023-08-31T15:15:26Z,,,
arXIv2023,Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection,Yes.,3,"""CoT relies primarily on a model's pre-trained internal knowledge during reasoning, thereby neglecting the valuable external information that is previously unknown to the model. This omission, especially within the unsupervised reasoning process, can affect the model's overall performance. Moreover, while CoT enhances Large Language",2023,2023-08-31T14:31:48Z,,,
arXIv2023,Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis,Yes.,3,"""Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate.""",2023,2023-08-31T13:14:47Z,,,
arXIv2023,SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills,Yes.,2,"""The varying prefill and decode times also lead to imbalance across micro-batches when using pipeline parallelism, resulting in further inefficiency due to bubbles.""",2023,2023-08-31T00:03:02Z,,,
arXIv2023,Large Language Models as Data Preprocessors,Yes.,3,"""Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency.""",2023,2023-08-30T23:28:43Z,,,
arXIv2023,"Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",Yes.,3,"""We conclude that multi-party conversations still challenge state-of-the-art LLMs.""",2023,2023-08-29T11:40:03Z,,,
arXIv2023,SwapMoE: Efficient Memory-Constrained Serving of Large Sparse MoE Models via Dynamic Expert Pruning and Swapping,Yes.,3,"""However, serving such large models on edge devices is challenging due to memory constraints. Typical solutions like memory swapping or weight pruning may lead to significantly higher latency or severe accuracy loss.""",2023,2023-08-29T05:25:21Z,,,
arXIv2023,LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks,Yes.,3,"""to address the potential inaccuracies or illogical actions arising from LLM.""",2023,2023-08-29T01:54:49Z,,,
arXIv2023,Identifying and Mitigating the Security Risks of Generative AI,Yes.,2,"""Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities... However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks.""",2023,2023-08-28T18:51:09Z,,,
arXIv2023,Distilled GPT for Source Code Summarization,Yes.,3,"""However, to use these tools, programmers must send their code to untrusted third parties for processing (e.g., via an API call). This loss of custody is not acceptable to many organizations.""",2023,2023-08-28T17:34:07Z,,,
arXIv2023,LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors,Yes.,3,"""Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the",2023,2023-08-26T15:21:47Z,,,
arXIv2023,Planning with Logical Graph-based Language Model for Instruction Generation,Yes.,3,"""it is hard to generate texts with correct logic according to a given task, due to the difficulties for neural models to capture implied rules from free-form texts.""",2023,2023-08-26T06:28:14Z,,,
arXIv2023,Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content,Yes.,3,"""we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs)"" and ""evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4.""",2023,2023-08-26T05:20:58Z,,,
arXIv2023,Large Language Models Should Ask Clarifying Questions to Increase Confidence in Generated Code,Yes.,3,"""the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated.""",2023,2023-08-25T17:33:05Z,,,
arXIv2023,Measuring Spurious Correlation in Classification: 'Clever Hans' in Translationese,No.,1,The abstract discusses neural translationese classifiers and BERT-based classifiers but does not mention LLMs or their limitations.,2023,2023-08-25T04:19:58Z,,,
arXIv2023,"Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4",Yes.,3,"""This study investigated their usefulness and boundaries in extracting information and answering queries related to accidents from 100 crash narratives from Iowa and Kansas.""",2023,2023-08-25T00:09:16Z,,,
arXIv2023,ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching,Yes.,2,"""The situation will only worsen as the pace of code development accelerates, with developers relying on Large Language Models (LLMs) to automatically generate code.""",2023,2023-08-24T20:04:36Z,,,
arXIv2023,POLCA: Power Oversubscription in LLM Cloud Providers,Yes.,2,"""However, the stringent set of telemetry and controls that GPUs offer in a virtualized environment, makes it challenging to have a reliable and robust power oversubscription mechanism.""",2023,2023-08-24T16:32:34Z,,,
arXIv2023,Large Language Models Vote: Prompting for Rare Disease Identification,Yes.,2,"""Furthermore, in using MVP, each model is prompted multiple times, substantially increasing the time needed for manual annotation, and to address this, we assess the feasibility of using JSON for automating generative LLM evaluation.""",2023,2023-08-24T16:09:13Z,,,
arXIv2023,Improving Translation Faithfulness of Large Language Models via Augmenting Instructions,Yes.,3,"""As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding.""",2023,2023-08-24T09:32:29Z,,,
arXIv2023,Benchmarking Causal Study to Interpret Large Language Models for Source Code,Yes.,3,"""Existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome, but do not take into account confounding variables (e.g., lines of code, prompt size) that equally influence the accuracy metrics.""",2023,2023-08-23T20:32:12Z,,,
arXIv2023,D4: Improving LLM Pretraining via Document De-Duplication and Diversification,Yes.,3,"""the size of these improvements diminishes with scale,"" and ""calls into question the common practice of training for a single epoch on as much data as possible.""",2023,2023-08-23T17:58:14Z,,,
arXIv2023,Prompt2Model: Generating Deployable Models from Natural Language Instructions,Yes.,3,"""they require extensive computational resources for deployment and can be gated behind APIs.""",2023,2023-08-23T17:28:21Z,,,
arXIv2023,Evaluation of Faithfulness Using the Longest Supported Subsequence,Yes.,2,"""Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers.""",2023,2023-08-23T14:18:44Z,,,
arXIv2023,From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models,Yes.,3,"""However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm."" and ""Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire.""",2023,2023-08-23T09:11:13Z,,,
arXIv2023,Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs,Yes.,3,"""Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference.""",2023,2023-08-23T04:59:21Z,,,
arXIv2023,Dcc --help: Generating Context-Aware Compiler Error Explanations with Large Language Models,Yes.,3,"""We found that the LLM-generated explanations were conceptually accurate in 90% of compile-time and 75% of run-time cases, but often disregarded the instruction not to provide solutions in code.""",2023,2023-08-23T02:36:19Z,,,
arXIv2023,Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation,Yes.,1,"""a hybrid augmentation strategy based on large language models is utilized to mitigate data scarcity limitations.""",2023,2023-08-22T16:45:35Z,,,
arXIv2023,LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning,Yes.,1,"""Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge.""",2023,2023-08-22T03:10:40Z,,,
arXIv2023,Dataset Quantization,Yes.,2,"""The expensive computation and memory costs make it difficult to train them on limited hardware resources, especially for recent popular large language models (LLM) and computer vision models (CV).""",2023,2023-08-21T07:24:29Z,,,
arXIv2023,An Examination of the Compositionality of Large Generative Vision-Language Models,Yes.,3,"""We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs.""",2023,2023-08-21T06:50:29Z,,,
arXIv2023,Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models,Yes.,3,"""While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters."" and ""ICL introduces inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance.""",2023,2023-08-21T04:31:06Z,,,
arXIv2023,Dynamic Strategy Chain: Dynamic Zero-Shot CoT for Long Mental Health Support Generation,Yes.,3,"""Zero-shot CoT prompting can not simulate a counselor or provide personalized strategies without effective mental health counseling strategy prompts.""",2023,2023-08-21T03:31:20Z,,,
arXIv2023,LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient Querying,Yes.,3,"""However, as RL training is generally not sample-efficient, deploying this approach would inherently imply that the LLM be repeatedly queried for solutions; a process that can be expensive and infeasible.""",2023,2023-08-21T02:07:35Z,,,
arXIv2023,Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts,Yes.,3,"""However, their effectiveness and limitations in the education domain are yet to be fully explored."" and ""certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors.""",2023,2023-08-21T01:32:45Z,,,
arXIv2023,A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability,Yes.,1,"""This paper outlines a natural conversational approach to solving personalized energy-related problems using large language models (LLMs).""",2023,2023-08-20T22:42:04Z,,,
arXIv2023,How Good Are Large Language Models at Out-of-Distribution Detection?,Yes.,3,"""the stark differences in scales, pre-training objectives, and inference paradigms call into question the applicability of these findings to LLMs.""",2023,2023-08-20T13:15:18Z,,,
arXIv2023,March in Chat: Interactive Prompting for Remote Embodied Referring Expression,Yes.,3,"""Large Language Models (LLMs) show great potential in robot action planning by providing proper prompts. Still, this strategy has not been explored under the REVERIE settings. There are several new challenges. For example, the LLM should be environment-aware so that the navigation plan can be adjusted",2023,2023-08-20T03:00:20Z,,,
arXIv2023,"UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding",Yes.,3,"""existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored.""",2023,2023-08-19T17:32:34Z,,,
arXIv2023,GameEval: Evaluating LLMs on Conversational Games,Yes.,3,"""overcoming the limitations of previous methods.""",2023,2023-08-19T14:33:40Z,,,
arXIv2023,FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models,Yes.,2,"""the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge.""",2023,2023-08-19T10:38:00Z,,,
arXIv2023,Tackling Vision Language Tasks Through Learning Inner Monologues,Yes.,2,"""The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability.""",2023,2023-08-19T10:10:49Z,,,
arXIv2023,Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis,Yes.,3,"""These approaches aim to harness the strengths of both LLMs and CAs, while mitigating their weaknesses, thereby advancing the development of more robust AI systems. We discuss the tradeoffs and challenges associated with each approach.""",2023,2023-08-18T21:42:47Z,,,
arXIv2023,Learning Representations on Logs for AIOps,Yes.,1,"""Large Language Models (LLMs) such as BERT and GPT3 are trained using self-supervision on a vast amount of unlabeled data.""",2023,2023-08-18T20:34:46Z,,,
arXIv2023,Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning,Yes.,3,"""current research is mostly limited to basic scenarios of simple questions that can be straightforward answered in a few inference steps. Planning for the more challenging multi-hop visual reasoning tasks remains under-explored.""",2023,2023-08-18T16:21:40Z,,,
arXIv2023,Semantic Consistency for Assuring Reliability of Large Language Models,Yes.,3,"""However, recent research has highlighted their sensitivity to variations in input prompts.""",2023,2023-08-17T18:11:33Z,,,
arXIv2023,Building Emotional Support Chatbots in the Era of LLMs,Yes.,2,"""However, there are unsolved challenges that hinder real-world applications in this field, including limited data availability and the absence of well-accepted model training paradigms.""",2023,2023-08-17T10:49:18Z,,,
arXIv2023,BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction,Yes.,3,"""how to integrate pre-trained language models that handle only textual signals into a prediction pipeline with non-textual features is challenging.""",2023,2023-08-17T08:25:54Z,,,
arXIv2023,CMB: A Comprehensive Medical Benchmark in Chinese,Yes.,1,"""Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine.""",2023,2023-08-17T07:51:23Z,,,
arXIv2023,PMET: Precise Model Editing in a Transformer,Yes.,3,"""Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts",2023,2023-08-17T02:33:43Z,,,
arXIv2023,FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs,Yes.,3,"""Large Language Models (LLMs) have achieved state-of-the-art performance across various language tasks but pose challenges for practical deployment due to their substantial memory requirements. Furthermore, the latest generative models suffer from high inference costs caused by the memory bandwidth bottleneck in the auto",2023,2023-08-16T23:57:41Z,,,
arXIv2023,LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters,Yes.,3,"""incorporating LLMs with time-series data presents challenges of limited adaptation due to different compositions between time-series and linguistic data, and the inability to process multi-scale temporal information.""",2023,2023-08-16T16:19:50Z,,,
arXIv2023,DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue,Yes.,3,"""While current LLMs proficiently answer general questions, they often fall short in complex diagnostic scenarios such as legal, medical, or other specialized consultations."" and ""Previous fine-tuning models have underperformed in TOD and the full potential of this capability in current LLMs has not yet been fully explored.""",2023,2023-08-15T21:14:09Z,,,
arXIv2023,Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,Yes.,3,"""However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios.""",2023,2023-08-15T17:40:34Z,,,
arXIv2023,Link-Context Learning for Multimodal LLMs,Yes.,3,"""Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge.""",2023,2023-08-15T17:33:24Z,,,
arXIv2023,LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation,Yes.,3,"""Despite their alluring technological potential, there is no unified and comprehensive evaluation criterion, leading to the inability to evaluate the quality and potential risks of medical LLMs, further hindering the application of LLMs in medical treatment scenarios.""",2023,2023-08-15T08:32:20Z,,,
arXIv2023,"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",Yes.,2,"""Large language models (LLMs) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. Adversarial prompts can bypass their safety measures.""",2023,2023-08-14T17:54:10Z,,,
arXIv2023,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,Yes.,3,"""experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.""",2023,2023-08-14T15:13:04Z,,,
arXIv2023,Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice,Yes.,2,"""much of this discourse relies on claims and assumptions that are worth re-examining"" and ""outlines the existing evidence for and against them.""",2023,2023-08-14T13:00:53Z,,,
arXIv2023,Generative Interpretation,Yes.,3,"""After offering best practices for the use of these models given their limitations.""",2023,2023-08-14T02:59:27Z,,,
arXIv2023,"Large Language Models and Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",Yes.,3,"""Moreover, challenges and risks associated with developing AFMs are discussed, including model training, validation, and deployment.""",2023,2023-08-13T02:59:36Z,,,
arXIv2023,Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation,Yes.,2,"""However, there remains the challenge of adapting the model to safety-first scenarios.""",2023,2023-08-12T16:56:55Z,,,
arXIv2023,Large Language Models for Telecom: Forthcoming Impact on the Industry,Yes.,3,"""To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations.""",2023,2023-08-11T08:41:00Z,,,
arXIv2023,PIPPA: A Partially Synthetic Conversational Dataset,Yes.,1,"""With the emergence of increasingly powerful large language models, there is a burgeoning interest in leveraging these models for casual conversation and role-play applications.""",2023,2023-08-11T00:33:26Z,,,
arXIv2023,NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search,Yes.,3,"""This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values.""",2023,2023-08-10T14:19:58Z,,,
arXIv2023,Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length,Yes.,3,"""LLMs often harbor misleading content, highlighting the urgency to align them with human values for secure AI systems.""",2023,2023-08-10T13:50:17Z,,,
arXIv2023,"A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology",Yes.,3,"""We show that current widely used open-sourced LLMs do poorly in their ability for zero-shot reasoning when compared to GPT-4 and Claude 2.""",2023,2023-08-09T05:01:28Z,,,
arXIv2023,Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA,Yes.,3,"""it is essential to highlight that these advanced reasoning abilities appear to emerge in models with a minimum of 10 billion parameters, thereby limiting its efficacy in situations where computational resources are constrained.""",2023,2023-08-09T03:18:07Z,,,
arXIv2023,ChatGPT for Arabic Grammatical Error Correction,Yes.,3,"""Despite these positive results, we find that instruction fine-tuned models, regardless of their size, significantly underperform compared to fully fine-tuned models of significantly smaller sizes. This disparity highlights a substantial room for improvements for LLMs.""",2023,2023-08-08T18:00:39Z,,,
arXIv2023,A Comparative Study of Code Generation using ChatGPT 3.5 across 10 Programming Languages,Yes.,3,"""Based on the findings derived from this research, major unexpected behaviors and limitations of the model have been identified.""",2023,2023-08-08T15:02:32Z,,,
arXIv2023,AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models,Yes.,3,"""revealing their limitations as a generalized PCF knowledge database.""",2023,2023-08-08T13:12:03Z,,,
arXIv2023,Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance,Yes.,3,"""the computational demands for retrieval augmented large language models (LLMs) pose a challenge when applying them to real-time tasks, such as composition assistance.""",2023,2023-08-08T12:27:20Z,,,
arXIv2023,Revisiting Prompt Engineering via Declarative Crowdsourcing,Yes.,3,"""Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone.""",2023,2023-08-07T18:04:12Z,,,
arXIv2023,Learning Concise and Descriptive Attributes for Visual Recognition,Yes.,3,"""our further investigation on 8 datasets reveals that LLM-generated attributes in a large quantity perform almost the same as random words. This surprising finding suggests that significant noise may be present in these attributes.""",2023,2023-08-07T16:00:22Z,,,
arXIv2023,MedMine: Examining Pre-trained Language Models on Medication Mining,Yes.,3,"""Such obstacles include their imbalanced performances on different entity types and clinical events.""",2023,2023-08-07T14:36:03Z,,,
arXIv2023,Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing,Yes.,3,"""Researchers have long studied the vulnerabilities and limitations of LLMs, such as adversarial attacks and model toxicity.""",2023,2023-08-07T13:10:35Z,,,
arXIv2023,Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models,Yes.,3,"""existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates.""",2023,2023-08-07T10:11:42Z,,,
arXIv2023,GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis,Yes.,3,"""Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge.""",2023,2023-08-07T05:48:53Z,,,
arXIv2023,Exploiting Code Symmetries for Learning Program Semantics,Yes.,1,"""Our results suggest that code LLMs that encode the code structural prior via the code symmetry group generalize better and faster.""",2023,2023-08-07T05:40:58Z,,,
arXIv2023,LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning,Yes.,2,"""The low-rank adaptation (LoRA) method can largely reduce the amount of trainable parameters for fine-tuning large language models (LLMs), however, it still requires expensive activation memory to update low-rank weights.""",2023,2023-08-07T05:12:27Z,,,
arXIv2023,LARCH: Large Language Model-based Automatic Readme Creation with Heuristics,Yes.,2,"""automatically creating one remains a challenge even with the recent advancements in large language models (LLMs), because it requires generating an abstract description from thousands of lines of code.""",2023,2023-08-06T12:28:24Z,,,
arXIv2023,"Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models",Yes.,2,"""We find that while a fine-tuned LLM achieves the best accuracy, some alternate configurations provide huge (up to 24, 283 *) resource savings for a marginal (<1%) loss in accuracy.""",2023,2023-08-03T20:29:27Z,,,
arXIv2023,Does Correction Remain A Problem For Large Language Models?,Yes.,3,"""This paper investigates the role of correction in the context of large language models by conducting two experiments.""",2023,2023-08-03T14:09:31Z,,,
arXIv2023,Evaluating ChatGPT text-mining of clinical records for obesity monitoring,Yes.,3,"""Subtle prompt engineering is needed to improve ChatGPT output"" and ""require careful implementation to avoid unpredictable errors.""",2023,2023-08-03T10:11:42Z,,,
arXIv2023,Towards More Human-like AI Communication: A Review of Emergent Communication Research,Yes.,3,"""While a common approach to achieve this is to train large language models, this method presents a form of learning misalignment where the model may not capture the underlying structure and reasoning humans employ in using natural language, potentially leading to unexpected or unreliable behavior.""",2023,2023-08-01T14:43:10Z,,,
arXIv2023,Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education,Yes.,3,"""Despite their impressive performances in general tasks, LLMs need to be aligned when applying for domain specific tasks to mitigate the problems of hallucination and producing harmful answers.""",2023,2023-08-01T12:04:50Z,,,
arXIv2023,LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack,Yes.,3,"""Natural language processing models are vulnerable to adversarial examples."" and ""In addition, we evaluate the effectiveness of LimeAttack on large language models, and results indicate that adversarial examples remain a significant threat to large language models.""",2023,2023-08-01T06:30:37Z,,,
arXIv2023,Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting,Yes.,1,"""Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query.""",2023,2023-08-01T05:31:36Z,,,
arXIv2023,HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution,Yes.,1,"""The rise of large language models (LLMs) had a transformative impact on search,"" and ""HAGRID is constructed based on human and LLM collaboration.""",2023,2023-07-31T17:49:18Z,,,
arXIv2023,Transferable Decoding with Visual Entities for Zero-Shot Image Captioning,Yes.,3,"""we have observed and empirically demonstrated that these methods are susceptible to modality bias induced by LLMs and tend to generate descriptions containing objects (entities) that do not actually exist in the image but frequently appear during training (i.e., object hallucination).""",2023,2023-07-31T09:47:06Z,,,
arXIv2023,A Benchmark for Understanding Dialogue Safety in Mental Health Support,Yes.,3,"""Our study reveals that ChatGPT struggles to detect safety categories with detailed safety definitions in a zero- and few-shot paradigm, whereas the fine-tuned model proves to be more suitable.""",2023,2023-07-31T07:33:16Z,,,
arXIv2023,HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field,Yes.,2,"""However, there has not been a specially designed LLM for renewable energy. Meanwhile, there has not been any dataset of renewable energy for training LLMs.""",2023,2023-07-31T06:59:36Z,,,
arXIv2023,Distractor generation for multiple-choice questions with predictive prompting and large language models,Yes.,3,"""we still observe a performance gap in generating distractors -- i.e., plausible yet incorrect answers -- with LLMs for multiple-choice questions (MCQs).""",2023,2023-07-30T23:15:28Z,,,
arXIv2023,An Unforgeable Publicly Verifiable Watermark for Large Language Models,Yes.,2,"""current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection.""",2023,2023-07-30T13:43:27Z,,,
arXIv2023,Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback,Yes.,2,"""existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world.""",2023,2023-07-29T18:01:46Z,,,
arXIv2023,Towards Codable Watermarking for Injecting Multi-bits Information to LLMs,Yes.,1,"""we argue that existing LLM watermarking methods are encoding-inefficient and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.).""",2023,2023-07-29T14:11:15Z,,,
arXIv2023,CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools,Yes.,3,"""developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop.""",2023,2023-07-28T18:58:16Z,,,
arXIv2023,Matching Patients to Clinical Trials with Large Language Models,Yes.,3,"""Our error analysis suggests that current LLMs still make some mistakes due to limited medical knowledge and domain-specific context understanding.""",2023,2023-07-27T17:56:56Z,,,
arXIv2023,WavJourney: Compositional Audio Creation with Large Language Models,Yes.,1,"""We present WavJourney, a novel framework that leverages Large Language Models (LLMs) to connect various audio models for audio creation.""",2023,2023-07-26T17:54:04Z,,,
arXIv2023,Evaluating Large Language Models for Radiology Natural Language Processing,Yes.,2,"""The outcomes of this evaluation provide key insights into the performance, strengths, and weaknesses of these LLMs, informing their practical applications within the medical domain.""",2023,2023-07-25T17:57:18Z,,,
arXIv2023,How Can Large Language Models Help Humans in Design and Manufacturing?,Yes.,3,"""Through a series of examples, we highlight both the benefits and the limitations of the current LLMs.""",2023,2023-07-25T17:30:38Z,,,
arXIv2023,GPT-3 Models are Few-Shot Financial Reasoners,Yes.,3,"""We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of financial questions and the complex information stored in financial documents.""",2023,2023-07-25T16:21:07Z,,,
arXIv2023,Predicting Code Coverage without Execution,Yes.,1,"""We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs).""",2023,2023-07-25T10:07:02Z,,,
arXIv2023,The potential of LLMs for coding with low-resource and domain-specific programming languages,Yes.,3,"""While the LLM showcased promoting docstring-to-code translation capability, we also identify some limitations, such as its inability to improve certain sections of code and to write accurate unit tests.""",2023,2023-07-24T17:17:13Z,,,
arXIv2023,The Imitation Game: Detecting Human and AI-Generated Texts in the Era of ChatGPT and BARD,Yes.,3,"""However, distinguishing between human-written and AI-generated text has become a significant task."" and ""the task becomes more challenging when classifying GPT-generated text, particularly in story writing.""",2023,2023-07-22T21:00:14Z,,,
arXIv2023,FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models,Yes.,1,"""the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs);""",2023,2023-07-22T09:27:05Z,,,
arXIv2023,Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models,Yes.,2,"""This article discusses the potential and limitations of using large language models to enhance mental health support through AI technologies.""",2023,2023-07-22T06:21:41Z,,,
arXIv2023,Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors,Yes.,3,"""Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out.""",2023,2023-07-21T22:02:50Z,,,
arXIv2023,The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention,Yes.,1,"""It is a significant finding since the proliferation of several LLMs in the near future makes it extremely challenging to design a single system that can identify profiles created with various LLMs.""",2023,2023-07-21T19:09:24Z,,,
arXIv2023,OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples,Yes.,3,"""However, existing detectors lack robustness against attacks",2023,2023-07-21T17:40:47Z,,,
arXIv2023,"""Tidy Up the Table"": Grounding Common-sense Objective for Tabletop Object Rearrangement",Yes.,3,"""Large Language Models (LLMs) have proven capable of capturing common sense knowledge to reason over this vague concept of tidiness. However, they alone may struggle with table tidying due to the limited grasp on the spatio-visual aspects of tidiness.""",2023,2023-07-21T03:00:31Z,,,
arXIv2023,FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets,Yes.,3,"""previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition.""",2023,2023-07-20T14:56:35Z,,,
arXIv2023,Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?,Yes.,3,"""it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models.""",2023,2023-07-19T22:03:40Z,,,
arXIv2023,PharmacyGPT: The AI Pharmacist,Yes.,2,"""Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy.""",2023,2023-07-19T19:40:34Z,,,
arXIv2023,Code Detection for Hardware Acceleration Using Large Language Models,Yes.,2,"""Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives.""",2023,2023-07-19T17:21:58Z,,,
arXIv2023,ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats,Yes.,3,"""Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers,"" and ""mitigate the overhead from precision alignment caused by the disparity between weights and activations.""",2023,2023-07-19T06:58:03Z,,,
arXIv2023,On the (In)Effectiveness of Large Language Models for Chinese Text Correction,Yes.,3,"""we empirically find that the LLMs currently have both amazing performance and unsatisfactory behavior for Chinese Text Correction.""",2023,2023-07-18T06:48:52Z,,,
arXIv2023,"TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",Yes.,1,"""The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality.""",2023,2023-07-17T17:36:09Z,,,
arXIv2023,Generating Efficient Training Data via LLM-based Attribute Manipulation,Yes.,1,"""In this paper, we propose a novel method, Chain-of-Thoughts Attribute Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from Large Language Models (LLMs).""",2023,2023-07-14T00:10:03Z,,,
arXIv2023,A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023,Yes.,2,"""To further enhance the model's adaptability to novel action labels, we experiment with rules generated using GPT-3.5, which leads to a slight decrease in performance.""",2023,2023-07-13T05:54:05Z,,,
arXIv2023,AutoHint: Automatic Prompt Optimization with Hint Generation,Yes.,1,"""While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks, the key to applying this ability to specific tasks lies in developing high-quality prompts.""",2023,2023-07-13T00:49:27Z,,,
arXIv2023,T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation,Yes.,3,"""We further propose several evaluation metrics specifically designed to evaluate compositional text-to-image generation and explore the potential and limitations of multimodal LLMs for evaluation.""",2023,2023-07-12T17:59:42Z,,,
arXIv2023,Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models,Yes.,3,"""Large Language Models (LLMs) are the heart of many state-of-the-art ALSC solutions, but they perform poorly in some scenarios requiring Coreference Resolution (CR).""",2023,2023-07-11T12:43:28Z,,,
arXIv2023,Secrets of RLHF in Large Language Models Part I: PPO,Yes.,3,"""However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs.""",2023,2023-07-11T01:55:24Z,,,
arXIv2023,AmadeusGPT: a natural language interface for interactive animal behavioral analysis,Yes.,3,"""the comprehension capability of these LLMs is limited by the context window size, which prevents it from remembering distant conversations.""",2023,2023-07-10T19:15:17Z,,,
arXIv2023,BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,Yes.,2,"""We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs.""",2023,2023-07-10T15:56:17Z,,,
arXIv2023,Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training,Yes.,1,"""Our results indicate that a Large Language Model without domain-specific pre-training can have a significant edge in some domain-specific biomedical text generation tasks.""",2023,2023-07-10T08:32:45Z,,,
arXIv2023,TIM: Teaching Large Language Models to Translate with Comparison,Yes.,3,"""However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation."" and ""Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data.""",2023,2023-07-10T08:15:40Z,,,
arXIv2023,Towards Cross-Table Masked Pretraining for Web Data Mining,No.,1,The abstract discusses pretraining techniques for mining tabular data and does not mention language models or their limitations.,2023,2023-07-10T02:27:38Z,,,
arXIv2023,Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media,Yes.,2,"""However, the effectiveness of conventional supervised learning methods in detecting drug trafficking heavily relies on having access to substantial amounts of labeled data, while data annotation is time-consuming and resource-intensive. Furthermore, these models often face challenges in accurately identifying trafficking activities when drug dealers use deceptive language and euphemisms to avoid detection.""",2023,2023-07-07T16:15:59Z,,,
arXIv2023,PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations,Yes.,3,"""However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias.""",2023,2023-07-06T04:05:44Z,,,
arXIv2023,Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues,Yes.,3,"""GPT-4 performs moderately well in identifying instances when the tutor offers specific and immediate praise. However, GPT-4 underperforms in identifying the tutor's ability to deliver sincere praise, particularly in the zero-shot prompting scenario where examples of sincere tutor praise statements were not provided.""",2023,2023-07-05T04:14:01Z,,,
arXIv2023,Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations,Yes.,3,"""challenges remain in estimating numeric values and retrieving spatially related objects.""",2023,2023-07-05T03:50:08Z,,,
arXIv2023,Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification,Yes.,1,"""we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs).""",2023,2023-07-05T01:00:44Z,,,
arXIv2023,Evaluating Shutdown Avoidance of Language Models in Textual Scenarios,Yes.,2,"""we explore whether shutdown avoidance is merely a result of simple pattern matching between the dataset and the prompt or if it is a consistent behaviour across different environments and variations.""",2023,2023-07-03T07:05:59Z,,,
arXIv2023,DocLLM: A layout-aware generative language model for multimodal document understanding,No.,1,"""No evidence""",2023,2023-12-31T22:37:52Z,,,
arXIv2023,State of What Art? A Call for Multi-Prompt LLM Evaluation,No.,1,"""No evidence""",2023,2023-12-31T22:21:36Z,,,
arXIv2023,LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models,No.,1,"""No evidence""",2023,2023-12-31T21:18:16Z,,,
arXIv2023,An Analysis of Embedding Layers and Similarity Scores using Siamese Neural Networks,No.,1,"""No evidence""",2023,2023-12-31T20:21:58Z,,,
arXIv2023,Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing,No.,1,"""No evidence""",2023,2023-12-31T20:02:10Z,,,
arXIv2023,Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets,No.,1,"""No evidence""",2023,2023-12-31T19:25:34Z,,,
arXIv2023,KernelGPT: Enhanced Kernel Fuzzing via Large Language Models,No.,1,"""No evidence""",2023,2023-12-31T18:47:33Z,,,
arXIv2023,AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities,No.,1,"""No evidence""",2023,2023-12-31T17:21:02Z,,,
arXIv2023,A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models,No.,1,"""No evidence""",2023,2023-12-31T17:15:25Z,,,
arXIv2023,GraphGPT: Graph Learning with Generative Pre-trained Transformers,No.,1,"""No evidence""",2023,2023-12-31T16:19:30Z,,,
arXIv2023,Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI,No.,1,"""No evidence""",2023,2023-12-31T13:53:06Z,,,
arXIv2023,Generation Z's Ability to Discriminate Between AI-generated and Human-Authored Text on Discord,No.,1,"""No evidence""",2023,2023-12-31T11:52:15Z,,,
arXIv2023,Analyzing Local Representations of Self-supervised Vision Transformers,No.,1,"""No evidence""",2023,2023-12-31T11:38:50Z,,,
arXIv2023,Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws,No.,1,"""No evidence""",2023,2023-12-31T10:53:58Z,,,
arXIv2023,GeoGalactica: A Scientific Large Language Model in Geoscience,No.,1,"""No evidence""",2023,2023-12-31T09:22:54Z,,,
arXIv2023,keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM,No.,1,"""No evidence""",2023,2023-12-31T08:39:04Z,,,
arXIv2023,SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation for Multi-modal Intent Detection,No.,1,"""No evidence""",2023,2023-12-31T08:33:37Z,,,
arXIv2023,Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data,No.,1,"""No evidence""",2023,2023-12-31T05:13:13Z,,,
arXIv2023,Improving Text Embeddings with Large Language Models,No.,1,"""No evidence""",2023,2023-12-31T02:13:18Z,,,
arXIv2023,Trace and Edit Relation Associations in GPT,No.,1,"""No evidence""",2023,2023-12-30T21:04:41Z,,,
arXIv2023,Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks,No.,1,"""No evidence""",2023,2023-12-30T17:59:12Z,,,
arXIv2023,Evaluation is all you need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer using Open Models,No.,1,"""No evidence""",2023,2023-12-30T17:22:01Z,,,
arXIv2023,Boosting Large Language Model for Speech Synthesis: An Empirical Study,No.,1,"""No evidence""",2023,2023-12-30T14:20:04Z,,,
arXIv2023,Open-TI: Open Traffic Intelligence with Augmented Language Model,No.,1,"""No evidence""",2023,2023-12-30T11:50:11Z,,,
arXIv2023,The Problem of Alignment,No.,1,"""No evidence""",2023,2023-12-30T11:44:59Z,,,
arXIv2023,KAXAI: An Integrated Environment for Knowledge Analysis and Explainable AI,No.,1,"""No evidence""",2023,2023-12-30T10:20:47Z,,,
arXIv2023,L3Cube-MahaSocialNER: A Social Media based Marathi NER Dataset and BERT models,No.,1,"""No evidence""",2023,2023-12-30T08:30:24Z,,,
arXIv2023,ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph,No.,1,"""No evidence""",2023,2023-12-30T07:18:54Z,,,
arXIv2023,Unicron: Economizing Self-Healing LLM Training at Scale,No.,1,"""No evidence""",2023,2023-12-30T04:06:16Z,,,
arXIv2023,Why is the User Interface a Dark Pattern? : Explainable Auto-Detection and its Analysis,No.,1,"""No evidence""",2023,2023-12-30T03:53:58Z,,,
arXIv2023,Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models,No.,1,"""No evidence""",2023,2023-12-30T03:19:54Z,,,
arXIv2023,LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning,No.,1,"""No evidence""",2023,2023-12-30T02:53:45Z,,,
arXIv2023,FlashVideo: A Framework for Swift Inference in Text-to-Video Generation,No.,1,"""No evidence""",2023,2023-12-30T00:06:28Z,,,
arXIv2023,TuPy-E: detecting hate speech in Brazilian Portuguese social media with a novel dataset and comprehensive analysis of models,No.,1,"""No evidence""",2023,2023-12-29T17:47:00Z,,,
arXIv2023,Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online,No.,1,"""No evidence""",2023,2023-12-29T16:42:02Z,,,
arXIv2023,Large Language Models for Generative Information Extraction: A Survey,No.,1,"""No evidence""",2023,2023-12-29T14:25:22Z,,,
arXIv2023,The Tyranny of Possibilities in the Design of Task-Oriented LLM Systems: A Scoping Survey,No.,1,"""No evidence""",2023,2023-12-29T13:35:20Z,,,
arXIv2023,Action-Item-Driven Summarization of Long Meeting Transcripts,No.,1,"""No evidence""",2023,2023-12-29T12:33:21Z,,,
arXIv2023,Building Efficient Universal Classifiers with Natural Language Inference,No.,1,"""No evidence""",2023,2023-12-29T10:18:36Z,,,
arXIv2023,Olapa-MCoT: Enhancing the Chinese Mathematical Reasoning Capability of LLMs,No.,1,"""No evidence""",2023,2023-12-29T09:33:35Z,,,
arXIv2023,Overview of the PromptCBLUE Shared Task in CHIP2023,No.,1,"""No evidence""",2023,2023-12-29T09:05:00Z,,,
arXIv2023,Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game,No.,1,"""No evidence""",2023,2023-12-29T08:26:54Z,,,
arXIv2023,Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning,No.,1,"""No evidence""",2023,2023-12-29T06:08:18Z,,,
arXIv2023,MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining,No.,1,"""No evidence""",2023,2023-12-29T06:05:19Z,,,
arXIv2023,EHR Interaction Between Patients and AI: NoteAid EHR Interaction,No.,1,"""No evidence""",2023,2023-12-29T05:13:40Z,,,
arXIv2023,Tracking with Human-Intent Reasoning,No.,1,"""No evidence""",2023,2023-12-29T03:22:18Z,,,
arXIv2023,State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving,No.,1,"""No evidence""",2023,2023-12-29T03:00:04Z,,,
arXIv2023,Video Understanding with Large Language Models: A Survey,No.,1,"""No evidence""",2023,2023-12-29T01:56:17Z,,,
arXIv2023,Towards Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal cross- and self-attention Large Language Model Approach,No.,1,"""No evidence""",2023,2023-12-28T20:41:24Z,,,
arXIv2023,Language Model as an Annotator: Unsupervised Context-aware Quality Phrase Generation,No.,1,"""No evidence""",2023,2023-12-28T20:32:44Z,,,
arXIv2023,AQUALLM: Audio Question Answering Data Generation Using Large Language Models,No.,1,"""No evidence""",2023,2023-12-28T20:01:27Z,,,
arXIv2023,SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models for Private and Secure Inference,No.,1,"""No evidence""",2023,2023-12-28T19:55:11Z,,,
arXIv2023,Learning Vision from Models Rivals Learning Vision from Data,No.,1,"""No evidence""",2023,2023-12-28T18:59:55Z,,,
arXIv2023,Do Androids Know They're Only Dreaming of Electric Sheep?,No.,1,"""No evidence""",2023,2023-12-28T18:59:50Z,,,
arXIv2023,The LLM Surgeon,No.,1,"""No evidence""",2023,2023-12-28T18:59:09Z,,,
arXIv2023,Learning to Generate Text in Arbitrary Writing Styles,No.,1,"""No evidence""",2023,2023-12-28T18:58:52Z,,,
arXIv2023,A Simple LLM Framework for Long-Range Video Question-Answering,No.,1,"""No evidence""",2023,2023-12-28T18:58:01Z,,,
arXIv2023,Virtual Scientific Companion for Synchrotron Beamlines: A Prototype,No.,1,"""No evidence""",2023,2023-12-28T18:12:42Z,,,
arXIv2023,Non-Vacuous Generalization Bounds for Large Language Models,No.,1,"""No evidence""",2023,2023-12-28T17:58:42Z,,,
arXIv2023,Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos,No.,1,"""No evidence""",2023,2023-12-28T16:54:21Z,,,
arXIv2023,Structured Packing in LLM Training Improves Long Context Utilization,No.,1,"""No evidence""",2023,2023-12-28T16:25:52Z,,,
arXIv2023,Optimizing watermarks for large language models,No.,1,"""No evidence""",2023,2023-12-28T16:10:51Z,,,
arXIv2023,MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation,No.,1,"""No evidence""",2023,2023-12-28T15:49:43Z,,,
arXIv2023,LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation,No.,1,"""No evidence""",2023,2023-12-28T15:09:14Z,,,
arXIv2023,Improving In-context Learning via Bidirectional Alignment,No.,1,"""No evidence""",2023,2023-12-28T15:02:03Z,,,
arXIv2023,Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding,No.,1,"""No evidence""",2023,2023-12-28T14:42:24Z,,,
arXIv2023,On the rate of convergence of an over-parametrized Transformer classifier learned by gradient descent,No.,1,"""No evidence""",2023,2023-12-28T13:20:36Z,,,
arXIv2023,Effect of dimensionality change on the bias of word embeddings,No.,1,"""No evidence""",2023,2023-12-28T13:01:10Z,,,
arXIv2023,DrugAssist: A Large Language Model for Molecule Optimization,No.,1,"""No evidence""",2023,2023-12-28T10:46:56Z,,,
arXIv2023,BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence,No.,1,"""No evidence""",2023,2023-12-28T08:34:17Z,,,
arXIv2023,"MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices",No.,1,"""No evidence""",2023,2023-12-28T08:21:24Z,,,
arXIv2023,TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones,No.,1,"""No evidence""",2023,2023-12-28T07:11:41Z,,,
arXIv2023,Evaluating the Performance of Large Language Models for Spanish Language in Undergraduate Admissions Exams,No.,1,"""No evidence""",2023,2023-12-28T06:23:39Z,,,
arXIv2023,Multi-Prompts Learning with Cross-Modal Alignment for Attribute-based Person Re-Identification,No.,1,"""No evidence""",2023,2023-12-28T03:00:19Z,,,
arXIv2023,Rethinking Tabular Data Understanding with Large Language Models,No.,1,"""No evidence""",2023,2023-12-27T19:58:52Z,,,
arXIv2023,Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss,No.,1,"""No evidence""",2023,2023-12-27T18:53:09Z,,,
arXIv2023,A Large Language Model-based Computational Approach to Improve Identity-Related Write-Ups,No.,1,"""No evidence""",2023,2023-12-27T18:08:50Z,,,
arXIv2023,Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN),No.,1,"""No evidence""",2023,2023-12-27T14:50:09Z,,,
arXIv2023,PanGu-$?$: Enhancing Language Model Architectures via Nonlinearity Compensation,No.,1,"""No evidence""",2023,2023-12-27T11:49:24Z,,,
arXIv2023,Gemini Pro Defeated by GPT-4V: Evidence from Education,No.,1,"""No evidence""",2023,2023-12-27T02:56:41Z,,,
arXIv2023,Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs,No.,1,"""No evidence""",2023,2023-12-27T02:31:51Z,,,
arXIv2023,Conversational Question Answering with Reformulations over Knowledge Graph,No.,1,"""No evidence""",2023,2023-12-27T00:03:05Z,,,
arXIv2023,Observable Propagation: A Data-Efficient Approach to Uncover Feature Vectors in Transformers,No.,1,"""No evidence""",2023,2023-12-26T19:00:56Z,,,
arXIv2023,"Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",No.,1,"""No evidence""",2023,2023-12-26T18:59:33Z,,,
arXIv2023,Cloud-Device Collaborative Learning for Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-12-26T18:46:14Z,,,
arXIv2023,Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,No.,1,"""No evidence""",2023,2023-12-26T18:38:54Z,,,
arXIv2023,RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models,No.,1,"""No evidence""",2023,2023-12-26T17:40:55Z,,,
arXIv2023,LLM-SAP: Large Language Model Situational Awareness Based Planning,No.,1,"""No evidence""",2023,2023-12-26T17:19:09Z,,,
arXIv2023,A bi-objective $?$-constrained framework for quality-cost optimization in language model ensembles,No.,1,"""No evidence""",2023,2023-12-26T16:56:22Z,,,
arXIv2023,A Logically Consistent Chain-of-Thought Approach for Stance Detection,No.,1,"""No evidence""",2023,2023-12-26T13:54:00Z,,,
arXIv2023,LLMLight: Large Language Models as Traffic Signal Control Agents,No.,1,"""No evidence""",2023,2023-12-26T13:17:06Z,,,
arXIv2023,Aligning Large Language Models with Human Preferences through Representation Engineering,No.,1,"""No evidence""",2023,2023-12-26T11:01:36Z,,,
arXIv2023,Dynamic In-Context Learning from Nearest Neighbors for Bundle Generation,No.,1,"""No evidence""",2023,2023-12-26T08:24:24Z,,,
arXIv2023,Towards Probing Contact Center Large Language Models,No.,1,"""No evidence""",2023,2023-12-26T07:34:39Z,,,
arXIv2023,Supervised Knowledge Makes Large Language Models Better In-context Learners,No.,1,"""No evidence""",2023,2023-12-26T07:24:46Z,,,
arXIv2023,ChartBench: A Benchmark for Complex Visual Reasoning in Charts,No.,1,"""No evidence""",2023,2023-12-26T07:20:55Z,,,
arXIv2023,Align on the Fly: Adapting Chatbot Behavior to Established Norms,No.,1,"""No evidence""",2023,2023-12-26T06:51:09Z,,,
arXIv2023,Think and Retrieval: A Hypothesis Knowledge Graph Enhanced Medical Large Language Models,No.,1,"""No evidence""",2023,2023-12-26T04:49:56Z,,,
arXIv2023,More than Correlation: Do Large Language Models Learn Causal Representations of Space?,No.,1,"""No evidence""",2023,2023-12-26T01:27:29Z,,,
arXIv2023,Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments,No.,1,"""No evidence""",2023,2023-12-26T01:24:25Z,,,
arXIv2023,Compositional Generalization in Spoken Language Understanding,No.,1,"""No evidence""",2023,2023-12-25T21:46:06Z,,,
arXIv2023,A Closed-Loop Multi-perspective Visual Servoing Approach with Reinforcement Learning,No.,1,"""No evidence""",2023,2023-12-25T20:46:36Z,,,
arXIv2023,"AHAM: Adapt, Help, Ask, Model -- Harvesting LLMs for literature mining",No.,1,"""No evidence""",2023,2023-12-25T18:23:03Z,,,
arXIv2023,PersianLLaMA: Towards Building First Persian Large Language Model,No.,1,"""No evidence""",2023,2023-12-25T12:48:55Z,,,
arXIv2023,RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair,No.,1,"""No evidence""",2023,2023-12-25T11:39:46Z,,,
arXIv2023,What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning,No.,1,"""No evidence""",2023,2023-12-25T10:29:28Z,,,
arXIv2023,IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models,No.,1,"""No evidence""",2023,2023-12-25T09:13:18Z,,,
arXIv2023,Unlocking the Potential of Large Language Models for Explainable Recommendations,No.,1,"""No evidence""",2023,2023-12-25T09:09:54Z,,,
arXIv2023,A Split-and-Privatize Framework for Large Language Model Fine-Tuning,No.,1,"""No evidence""",2023,2023-12-25T03:53:33Z,,,
arXIv2023,Zero-Inflated Bandits,No.,1,"""No evidence""",2023,2023-12-25T03:13:21Z,,,
arXIv2023,Chatbot is Not All You Need: Information-rich Prompting for More Realistic Responses,No.,1,"""No evidence""",2023,2023-12-25T02:18:58Z,,,
arXIv2023,README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP,No.,1,"""No evidence""",2023,2023-12-24T23:01:00Z,,,
arXIv2023,Multi-level biomedical NER through multi-granularity embeddings and enhanced labeling,No.,1,"""No evidence""",2023,2023-12-24T21:45:36Z,,,
arXIv2023,YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction,No.,1,"""No evidence""",2023,2023-12-24T21:33:03Z,,,
arXIv2023,The Persuasive Power of Large Language Models,No.,1,"""No evidence""",2023,2023-12-24T16:21:11Z,,,
arXIv2023,Making Large Language Models A Better Foundation For Dense Retrieval,No.,1,"""No evidence""",2023,2023-12-24T15:10:35Z,,,
arXIv2023,Multimodal Classification of Teaching Activities from University Lecture Recordings,No.,1,"""No evidence""",2023,2023-12-24T08:33:30Z,,,
arXIv2023,Make-A-Character: High Quality Text-to-3D Character Generation within Minutes,No.,1,"""No evidence""",2023,2023-12-24T08:11:39Z,,,
arXIv2023,ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation,No.,1,"""No evidence""",2023,2023-12-24T06:38:11Z,,,
arXIv2023,Prompt Valuation Based on Shapley Values,No.,1,"""No evidence""",2023,2023-12-24T03:37:11Z,,,
arXIv2023,DEAP: Design Space Exploration for DNN Accelerator Parallelism,No.,1,"""No evidence""",2023,2023-12-24T02:43:01Z,,,
arXIv2023,MaDi: Learning to Mask Distractions for Generalization in Visual Deep Reinforcement Learning,No.,1,"""No evidence""",2023,2023-12-23T20:11:05Z,,,
arXIv2023,Multimodal Machine Learning Combining Facial Images and Clinical Texts Improves Diagnosis of Rare Genetic Diseases,No.,1,"""No evidence""",2023,2023-12-23T18:40:25Z,,,
arXIv2023,Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue,No.,1,"""No evidence""",2023,2023-12-23T18:14:56Z,,,
arXIv2023,An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development,No.,1,"""No evidence""",2023,2023-12-23T17:40:41Z,,,
arXIv2023,Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and Person Name Recognition,No.,1,"""No evidence""",2023,2023-12-23T17:30:28Z,,,
arXIv2023,Q-Boost: On Visual Quality Assessment Ability of Low-level Multi-Modality Foundation Models,No.,1,"""No evidence""",2023,2023-12-23T17:02:25Z,,,
arXIv2023,LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination,No.,1,"""No evidence""",2023,2023-12-23T11:09:48Z,,,
arXIv2023,Do LLM Agents Exhibit Social Behavior?,No.,1,"""No evidence""",2023,2023-12-23T08:46:53Z,,,
arXIv2023,SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling,No.,1,"""No evidence""",2023,2023-12-23T05:11:37Z,,,
arXIv2023,Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference,No.,1,"""No evidence""",2023,2023-12-23T04:27:06Z,,,
arXIv2023,CodeScholar: Growing Idiomatic Code Examples,No.,1,"""No evidence""",2023,2023-12-23T04:06:15Z,,,
arXIv2023,Large Language Models as Zero-Shot Keyphrase Extractors: A Preliminary Empirical Study,No.,1,"""No evidence""",2023,2023-12-23T03:50:49Z,,,
arXIv2023,Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models,No.,1,"""No evidence""",2023,2023-12-22T22:34:49Z,,,
arXIv2023,Unsupervised Auditory and Semantic Entrainment Models with Deep Neural Networks,No.,1,"""No evidence""",2023,2023-12-22T22:33:54Z,,,
arXIv2023,Refining GPT-3 Embeddings with a Siamese Structure for Technical Post Duplicate Detection,No.,1,"""No evidence""",2023,2023-12-22T21:14:37Z,,,
arXIv2023,Towards a Unified Multimodal Reasoning Framework,No.,1,"""No evidence""",2023,2023-12-22T19:07:00Z,,,
arXIv2023,Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases,No.,1,"""No evidence""",2023,2023-12-22T18:59:58Z,,,
arXIv2023,A Survey of Reinforcement Learning from Human Feedback,No.,1,"""No evidence""",2023,2023-12-22T18:58:06Z,,,
arXIv2023,Numerical Reasoning for Financial Reports,No.,1,"""No evidence""",2023,2023-12-22T17:46:36Z,,,
arXIv2023,VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation,No.,1,"""No evidence""",2023,2023-12-22T17:45:19Z,,,
arXIv2023,Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities,No.,1,"""No evidence""",2023,2023-12-22T17:39:40Z,,,
arXIv2023,YAYI 2: Multilingual Open-Source Large Language Models,No.,1,"""No evidence""",2023,2023-12-22T17:34:47Z,,,
arXIv2023,Voila-A: Aligning Vision-Language Models with User's Gaze Attention,No.,1,"""No evidence""",2023,2023-12-22T17:34:01Z,,,
arXIv2023,FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing,No.,1,"""No evidence""",2023,2023-12-22T16:56:02Z,,,
arXIv2023,Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs. SQL for No-Code Access to Relational Databases,No.,1,"""No evidence""",2023,2023-12-22T16:16:15Z,,,
arXIv2023,An Empirical Study on Compliance with Ranking Transparency in the Software Documentation of EU Online Platforms,No.,1,"""No evidence""",2023,2023-12-22T16:08:32Z,,,
arXIv2023,Zero-shot Causal Graph Extrapolation from Text via LLMs,No.,1,"""No evidence""",2023,2023-12-22T13:14:38Z,,,
arXIv2023,Multi-Modal Cognitive Maps based on Neural Networks trained on Successor Representations,No.,1,"""No evidence""",2023,2023-12-22T12:44:15Z,,,
arXIv2023,"Towards Message Brokers for Generative AI: Survey, Challenges, and Opportunities",No.,1,"""No evidence""",2023,2023-12-22T12:30:18Z,,,
arXIv2023,Future-proofing Education: A Prototype for Simulating Oral Examinations Using Large Language Models,No.,1,"""No evidence""",2023,2023-12-22T10:41:32Z,,,
arXIv2023,Reasons to Reject? Aligning Language Models with Judgments,No.,1,"""No evidence""",2023,2023-12-22T10:29:43Z,,,
arXIv2023,SIG: Speaker Identification in Literature via Prompt-Based Generation,No.,1,"""No evidence""",2023,2023-12-22T10:29:18Z,,,
arXIv2023,MMGPL: Multimodal Medical Data Analysis with Graph Prompt Learning,No.,1,"""No evidence""",2023,2023-12-22T10:10:50Z,,,
arXIv2023,Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse Mixture-of-Experts through Instruction-Tuning,No.,1,"""No evidence""",2023,2023-12-22T09:30:41Z,,,
arXIv2023,Revisiting Few-Shot Object Detection with Vision-Language Models,No.,1,"""No evidence""",2023,2023-12-22T07:42:00Z,,,
arXIv2023,Language Model is a Branch Predictor for Simultaneous Machine Translation,No.,1,"""No evidence""",2023,2023-12-22T07:32:47Z,,,
arXIv2023,MetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models,No.,1,"""No evidence""",2023,2023-12-22T07:15:55Z,,,
arXIv2023,FM-OV3D: Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection,No.,1,"""No evidence""",2023,2023-12-22T06:34:23Z,,,
arXIv2023,A Unified Industrial Large Knowledge Model Framework in Smart Manufacturing,No.,1,"""No evidence""",2023,2023-12-22T04:30:27Z,,,
arXIv2023,Efficacy of Machine-Generated Instructions,No.,1,"""No evidence""",2023,2023-12-22T04:01:30Z,,,
arXIv2023,Generative Pretraining at Scale: Transformer-Based Encoding of Transactional Behavior for Fraud Detection,No.,1,"""No evidence""",2023,2023-12-22T03:15:17Z,,,
arXIv2023,Evolving Large Language Model Assistant with Long-Term Conditional Memory,No.,1,"""No evidence""",2023,2023-12-22T02:39:15Z,,,
arXIv2023,Generative AI Beyond LLMs: System Implications of Multi-Modal Generation,No.,1,"""No evidence""",2023,2023-12-22T02:21:26Z,,,
arXIv2023,Exploiting Novel GPT-4 APIs,No.,1,"""No evidence""",2023,2023-12-21T21:22:41Z,,,
arXIv2023,Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians,No.,1,"""No evidence""",2023,2023-12-21T20:20:52Z,,,
arXIv2023,InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks,No.,1,"""No evidence""",2023,2023-12-21T18:59:31Z,,,
arXIv2023,V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs,No.,1,"""No evidence""",2023,2023-12-21T18:55:06Z,,,
arXIv2023,VCoder: Versatile Vision Encoders for Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-12-21T18:49:47Z,,,
arXIv2023,VideoPoet: A Large Language Model for Zero-Shot Video Generation,No.,1,"""No evidence""",2023,2023-12-21T18:46:41Z,,,
arXIv2023,LingoQA: Video Question Answering for Autonomous Driving,No.,1,"""No evidence""",2023,2023-12-21T18:40:34Z,,,
arXIv2023,A Strong Baseline for Temporal Video-Text Alignment,No.,1,"""No evidence""",2023,2023-12-21T17:28:09Z,,,
arXIv2023,Deep de Finetti: Recovering Topic Distributions from Large Language Models,No.,1,"""No evidence""",2023,2023-12-21T16:44:39Z,,,
arXIv2023,On the choice of the optimal temporal support for audio classification with Pre-trained embeddings,No.,1,"""No evidence""",2023,2023-12-21T16:36:33Z,,,
arXIv2023,Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning,No.,1,"""No evidence""",2023,2023-12-21T16:10:33Z,,,
arXIv2023,ChatGPT as a commenter to the news: can LLMs generate human-like opinions?,No.,1,"""No evidence""",2023,2023-12-21T15:46:36Z,,,
arXIv2023,Typhoon: Thai Large Language Models,No.,1,"""No evidence""",2023,2023-12-21T15:38:41Z,,,
arXIv2023,Multi-Agent Probabilistic Ensembles with Trajectory Sampling for Connected Autonomous Vehicles,No.,1,"""No evidence""",2023,2023-12-21T14:55:21Z,,,
arXIv2023,Domain-Specific Fine-Tuning of Large Language Models for Interactive Robot Programming,No.,1,"""No evidence""",2023,2023-12-21T14:51:04Z,,,
arXIv2023,Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs,No.,1,"""No evidence""",2023,2023-12-21T14:26:57Z,,,
arXIv2023,Capture the Flag: Uncovering Data Insights with Large Language Models,No.,1,"""No evidence""",2023,2023-12-21T14:20:06Z,,,
arXIv2023,"Evaluating Task-oriented Dialogue Systems: A Systematic Review of Measures, Constructs and their Operationalisations",No.,1,"""No evidence""",2023,2023-12-21T14:15:46Z,,,
arXIv2023,De novo Drug Design using Reinforcement Learning with Multiple GPT Agents,No.,1,"""No evidence""",2023,2023-12-21T13:24:03Z,,,
arXIv2023,AppAgent: Multimodal Agents as Smartphone Users,No.,1,"""No evidence""",2023,2023-12-21T11:52:45Z,,,
arXIv2023,Team Irisapu Project Description for DRC2023,No.,1,"""No evidence""",2023,2023-12-21T11:44:13Z,,,
arXIv2023,A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties,No.,1,"""No evidence""",2023,2023-12-21T11:43:41Z,,,
arXIv2023,Experimenting with Large Language Models and vector embeddings in NASA SciX,No.,1,"""No evidence""",2023,2023-12-21T10:19:58Z,,,
arXIv2023,TextFusion: Unveiling the Power of Textual Semantics for Controllable Image Fusion,No.,1,"""No evidence""",2023,2023-12-21T09:25:10Z,,,
arXIv2023,Text2Analysis: A Benchmark of Table Question Answering with Advanced Data Analysis and Unclear Queries,No.,1,"""No evidence""",2023,2023-12-21T08:50:41Z,,,
arXIv2023,LLM4VG: Large Language Models Evaluation for Video Grounding,No.,1,"""No evidence""",2023,2023-12-21T08:15:02Z,,,
arXIv2023,Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation,No.,1,"""No evidence""",2023,2023-12-21T06:51:34Z,,,
arXIv2023,Speech Translation with Large Language Models: An Industrial Practice,No.,1,"""No evidence""",2023,2023-12-21T05:32:49Z,,,
arXIv2023,Shai: A large language model for asset management,No.,1,"""No evidence""",2023,2023-12-21T05:08:57Z,,,
arXIv2023,Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models,No.,1,"""No evidence""",2023,2023-12-21T04:57:21Z,,,
arXIv2023,The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction,No.,1,"""No evidence""",2023,2023-12-21T03:51:08Z,,,
arXIv2023,Empowering Few-Shot Recommender Systems with Large Language Models -- Enhanced Representations,No.,1,"""No evidence""",2023,2023-12-21T03:50:09Z,,,
arXIv2023,"How to Prune Your Language Model: Recovering Accuracy on the ""Sparsity May Cry'' Benchmark",No.,1,"""No evidence""",2023,2023-12-21T03:11:30Z,,,
arXIv2023,Developing Interactive Tourism Planning: A Dialogue Robot System Powered by a Large Language Model,No.,1,"""No evidence""",2023,2023-12-21T03:09:38Z,,,
arXIv2023,HW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for Root Cause Analysis with GPT-assisted Mitigation Suggestion,No.,1,"""No evidence""",2023,2023-12-21T02:14:41Z,,,
arXIv2023,InfoVisDial: An Informative Visual Dialogue Dataset by Bridging Large Multimodal and Language Models,No.,1,"""No evidence""",2023,2023-12-21T00:44:45Z,,,
arXIv2023,Time is Encoded in the Weights of Finetuned Language Models,No.,1,"""No evidence""",2023,2023-12-20T20:04:45Z,,,
arXIv2023,A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production,No.,1,"""No evidence""",2023,2023-12-20T19:27:59Z,,,
arXIv2023,DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines,No.,1,"""No evidence""",2023,2023-12-20T19:13:26Z,,,
arXIv2023,dIR -- Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models,No.,1,"""No evidence""",2023,2023-12-20T18:41:44Z,,,
arXIv2023,LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,No.,1,"""No evidence""",2023,2023-12-20T17:25:23Z,,,
arXIv2023,Contextual Code Switching for Machine Translation using Language Models,No.,1,"""No evidence""",2023,2023-12-20T16:40:33Z,,,
arXIv2023,Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation,No.,1,"""No evidence""",2023,2023-12-20T16:00:43Z,,,
arXIv2023,Exploring Multimodal Large Language Models for Radiology Report Error-checking,No.,1,"""No evidence""",2023,2023-12-20T15:20:33Z,,,
arXIv2023,In Generative AI we Trust: Can Chatbots Effectively Verify Political Information?,No.,1,"""No evidence""",2023,2023-12-20T15:17:03Z,,,
arXIv2023,Domain-Specific Code Language Models: Unraveling the Potential for HPC Codes and Tasks,No.,1,"""No evidence""",2023,2023-12-20T15:11:06Z,,,
arXIv2023,RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications,No.,1,"""No evidence""",2023,2023-12-20T15:00:10Z,,,
arXIv2023,AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation,No.,1,"""No evidence""",2023,2023-12-20T13:22:41Z,,,
arXIv2023,Machine Mindset: An MBTI Exploration of Large Language Models,No.,1,"""No evidence""",2023,2023-12-20T12:59:31Z,,,
arXIv2023,"Benchmarking and Analyzing In-context Learning, Fine-tuning and Supervised Learning for Biomedical Knowledge Curation: a focused study on chemical entities of biological interest",No.,1,"""No evidence""",2023,2023-12-20T12:46:44Z,,,
arXIv2023,ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training,No.,1,"""No evidence""",2023,2023-12-20T11:00:54Z,,,
arXIv2023,Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method,No.,1,"""No evidence""",2023,2023-12-20T09:55:21Z,,,
arXIv2023,Language Resources for Dutch Large Language Modelling,No.,1,"""No evidence""",2023,2023-12-20T09:06:06Z,,,
arXIv2023,WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation,No.,1,"""No evidence""",2023,2023-12-20T09:02:29Z,,,
arXIv2023,OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using Semantic Understanding in Mixed Reality,No.,1,"""No evidence""",2023,2023-12-20T07:34:20Z,,,
arXIv2023,Enhancing Consistency in Multimodal Dialogue System Using LLM with Dialogue Scenario,No.,1,"""No evidence""",2023,2023-12-20T07:15:04Z,,,
arXIv2023,MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models,No.,1,"""No evidence""",2023,2023-12-20T07:01:49Z,,,
arXIv2023,AMD:Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion,No.,1,"""No evidence""",2023,2023-12-20T04:49:45Z,,,
arXIv2023,Fine-tuning Large Language Models for Adaptive Machine Translation,No.,1,"""No evidence""",2023,2023-12-20T03:21:48Z,,,
arXIv2023,MetaSegNet: Metadata-collaborative Vision-Language Representation Learning for Semantic Segmentation of Remote Sensing Images,No.,1,"""No evidence""",2023,2023-12-20T03:16:34Z,,,
arXIv2023,Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy,No.,1,"""No evidence""",2023,2023-12-20T02:55:15Z,,,
arXIv2023,BloomVQA: Assessing Hierarchical Multi-modal Comprehension,No.,1,"""No evidence""",2023,2023-12-20T02:22:49Z,,,
arXIv2023,Response Enhanced Semi-supervised Dialogue Query Generation,No.,1,"""No evidence""",2023,2023-12-20T02:19:54Z,,,
arXIv2023,Optimizing Distributed Training on Frontier for Large Language Models,No.,1,"""No evidence""",2023,2023-12-20T02:03:15Z,,,
arXIv2023,Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?,No.,1,"""No evidence""",2023,2023-12-20T00:49:52Z,,,
arXIv2023,Mini-GPTs: Efficient Large Language Models through Contextual Pruning,No.,1,"""No evidence""",2023,2023-12-20T00:48:13Z,,,
arXIv2023,Combinatorial Gaussian Process Bandits in Bayesian Settings: Theory and Application for Energy-Efficient Navigation,No.,1,"""No evidence""",2023,2023-12-20T00:31:43Z,,,
arXIv2023,Single-channel speech enhancement using learnable loss mixup,No.,1,"""No evidence""",2023,2023-12-20T00:25:55Z,,,
arXIv2023,RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios,No.,1,"""No evidence""",2023,2023-12-19T23:11:06Z,,,
arXIv2023,Can Transformers Learn Sequential Function Classes In Context?,No.,1,"""No evidence""",2023,2023-12-19T22:57:13Z,,,
arXIv2023,MotionScript: Natural Language Descriptions for Expressive 3D Human Motions,No.,1,"""No evidence""",2023,2023-12-19T22:33:17Z,,,
arXIv2023,SimQ-NAS: Simultaneous Quantization Policy and Neural Architecture Search,No.,1,"""No evidence""",2023,2023-12-19T22:08:49Z,,,
arXIv2023,Building a Llama2-finetuned LLM for Odia Language Utilizing Domain Knowledge Instruction Set,No.,1,"""No evidence""",2023,2023-12-19T22:01:01Z,,,
arXIv2023,Automated Assessment of Students' Code Comprehension using LLMs,No.,1,"""No evidence""",2023,2023-12-19T20:39:12Z,,,
arXIv2023,Faithful Model Evaluation for Model-Based Metrics,No.,1,"""No evidence""",2023,2023-12-19T19:41:33Z,,,
arXIv2023,A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise,No.,1,"""No evidence""",2023,2023-12-19T18:59:22Z,,,
arXIv2023,"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model",No.,1,"""No evidence""",2023,2023-12-19T18:53:01Z,,,
arXIv2023,Towards Automatic Support of Software Model Evolution with Large Language~Models,No.,1,"""No evidence""",2023,2023-12-19T18:38:01Z,,,
arXIv2023,Large Language Models in Medical Term Classification and Unexpected Misalignment Between Response and Reasoning,No.,1,"""No evidence""",2023,2023-12-19T17:36:48Z,,,
arXIv2023,LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction,No.,1,"""No evidence""",2023,2023-12-19T17:16:43Z,,,
arXIv2023,Instruct-SCTG: Guiding Sequential Controlled Text Generation through Instructions,No.,1,"""No evidence""",2023,2023-12-19T16:20:49Z,,,
arXIv2023,VQA4CIR: Boosting Composed Image Retrieval with Visual Question Answering,No.,1,"""No evidence""",2023,2023-12-19T15:56:08Z,,,
arXIv2023,Geo-located Aspect Based Sentiment Analysis (ABSA) for Crowdsourced Evaluation of Urban Environments,No.,1,"""No evidence""",2023,2023-12-19T15:37:27Z,,,
arXIv2023,GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning,No.,1,"""No evidence""",2023,2023-12-19T15:25:39Z,,,
arXIv2023,CUDC: A Curiosity-Driven Unsupervised Data Collection Method with Adaptive Temporal Distances for Offline Reinforcement Learning,No.,1,"""No evidence""",2023,2023-12-19T14:26:23Z,,,
arXIv2023,Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space,No.,1,"""No evidence""",2023,2023-12-19T13:23:18Z,,,
arXIv2023,Founder-GPT: Self-play to evaluate the Founder-Idea fit,No.,1,"""No evidence""",2023,2023-12-19T10:46:13Z,,,
arXIv2023,Active Preference Inference using Language Models and Probabilistic Reasoning,No.,1,"""No evidence""",2023,2023-12-19T09:58:54Z,,,
arXIv2023,Can ChatGPT be Your Personal Medical Assistant?,No.,1,"""No evidence""",2023,2023-12-19T09:54:27Z,,,
arXIv2023,Xpert: Empowering Incident Management with Query Recommendations via Large Language Models,No.,1,"""No evidence""",2023,2023-12-19T09:30:58Z,,,
arXIv2023,Fluctuation-based Adaptive Structured Pruning for Large Language Models,No.,1,"""No evidence""",2023,2023-12-19T09:23:48Z,,,
arXIv2023,"IPAD: Iterative, Parallel, and Diffusion-based Network for Scene Text Recognition",No.,1,"""No evidence""",2023,2023-12-19T08:03:19Z,,,
arXIv2023,External Knowledge Augmented Polyphone Disambiguation Using Large Language Model,No.,1,"""No evidence""",2023,2023-12-19T08:00:10Z,,,
arXIv2023,Sparse is Enough in Fine-tuning Pre-trained Large Language Model,No.,1,"""No evidence""",2023,2023-12-19T06:06:30Z,,,
arXIv2023,A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT,No.,1,"""No evidence""",2023,2023-12-19T05:46:11Z,,,
arXIv2023,Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach,No.,1,"""No evidence""",2023,2023-12-19T05:27:16Z,,,
arXIv2023,An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training,No.,1,"""No evidence""",2023,2023-12-19T03:24:55Z,,,
arXIv2023,Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment,No.,1,"""No evidence""",2023,2023-12-19T03:12:13Z,,,
arXIv2023,NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs,No.,1,"""No evidence""",2023,2023-12-19T02:35:13Z,,,
arXIv2023,MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA,No.,1,"""No evidence""",2023,2023-12-19T02:11:01Z,,,
arXIv2023,Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs,No.,1,"""No evidence""",2023,2023-12-19T01:48:31Z,,,
arXIv2023,Dynamic Topic Language Model on Heterogeneous Children's Mental Health Clinical Notes,No.,1,"""No evidence""",2023,2023-12-19T00:36:53Z,,,
arXIv2023,Towards Better Serialization of Tabular Data for Few-shot Classification with Large Language Models,No.,1,"""No evidence""",2023,2023-12-18T21:11:17Z,,,
arXIv2023,Agent-based Learning of Materials Datasets from Scientific Literature,No.,1,"""No evidence""",2023,2023-12-18T20:29:58Z,,,
arXIv2023,Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows,No.,1,"""No evidence""",2023,2023-12-18T20:01:58Z,,,
arXIv2023,"Turing's Test, a Beautiful Thought Experiment",No.,1,"""No evidence""",2023,2023-12-18T19:38:26Z,,,
arXIv2023,Cascade Speculative Drafting for Even Faster LLM Inference,No.,1,"""No evidence""",2023,2023-12-18T18:59:46Z,,,
arXIv2023,Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint,No.,1,"""No evidence""",2023,2023-12-18T18:58:42Z,,,
arXIv2023,An In-depth Look at Gemini's Language Abilities,No.,1,"""No evidence""",2023,2023-12-18T18:47:42Z,,,
arXIv2023,Social Learning: Towards Collaborative Learning with Large Language Models,No.,1,"""No evidence""",2023,2023-12-18T18:44:10Z,,,
arXIv2023,Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning,No.,1,"""No evidence""",2023,2023-12-18T18:21:43Z,,,
arXIv2023,Counting Reward Automata: Sample Efficient Reinforcement Learning Through the Exploitation of Reward Function Structure,No.,1,"""No evidence""",2023,2023-12-18T17:20:38Z,,,
arXIv2023,DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation,No.,1,"""No evidence""",2023,2023-12-18T16:41:22Z,,,
arXIv2023,UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts,No.,1,"""No evidence""",2023,2023-12-18T13:18:24Z,,,
arXIv2023,"The Good, The Bad, and Why: Unveiling Emotions in Generative AI",No.,1,"""No evidence""",2023,2023-12-18T11:19:45Z,,,
arXIv2023,Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction,No.,1,"""No evidence""",2023,2023-12-18T09:58:19Z,,,
arXIv2023,UniGen: A Unified Generative Framework for Retrieval and Question Answering with Large Language Models,No.,1,"""No evidence""",2023,2023-12-18T09:13:41Z,,,
arXIv2023,VinaLLaMA: LLaMA-based Vietnamese Foundation Model,No.,1,"""No evidence""",2023,2023-12-18T08:27:33Z,,,
arXIv2023,Knowledge Graphs and Pre-trained Language Models enhanced Representation Learning for Conversational Recommender Systems,No.,1,"""No evidence""",2023,2023-12-18T06:41:23Z,,,
arXIv2023,Generative linguistic representation for spoken language identification,No.,1,"""No evidence""",2023,2023-12-18T06:40:24Z,,,
arXIv2023,From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting,No.,1,"""No evidence""",2023,2023-12-18T06:31:23Z,,,
arXIv2023,A Multimodal Approach for Advanced Pest Detection and Classification,No.,1,"""No evidence""",2023,2023-12-18T05:54:20Z,,,
arXIv2023,Understanding the Multi-modal Prompts of the Pre-trained Vision-Language Model,No.,1,"""No evidence""",2023,2023-12-18T04:49:03Z,,,
arXIv2023,Satellite Captioning: Large Language Models to Augment Labeling,No.,1,"""No evidence""",2023,2023-12-18T03:21:58Z,,,
arXIv2023,Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI),No.,1,"""No evidence""",2023,2023-12-18T03:19:31Z,,,
arXIv2023,Generalized Category Discovery with Large Language Models in the Loop,No.,1,"""No evidence""",2023,2023-12-18T02:55:14Z,,,
arXIv2023,Dissecting Bias of ChatGPT in College Major Recommendations,No.,1,"""No evidence""",2023,2023-12-18T02:33:53Z,,,
arXIv2023,From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape,No.,1,"""No evidence""",2023,2023-12-18T01:11:39Z,,,
arXIv2023,Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters,No.,1,"""No evidence""",2023,2023-12-17T20:42:43Z,,,
arXIv2023,A mathematical perspective on Transformers,No.,1,"""No evidence""",2023,2023-12-17T19:06:29Z,,,
arXIv2023,Demystifying Instruction Mixing for Fine-tuning Large Language Models,No.,1,"""No evidence""",2023,2023-12-17T18:44:26Z,,,
arXIv2023,RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models,No.,1,"""No evidence""",2023,2023-12-17T17:57:50Z,,,
arXIv2023,kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning,No.,1,"""No evidence""",2023,2023-12-17T17:26:50Z,,,
arXIv2023,M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts,No.,1,"""No evidence""",2023,2023-12-17T16:53:30Z,,,
arXIv2023,"Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and Statistical Approach",No.,1,"""No evidence""",2023,2023-12-17T15:56:05Z,,,
arXIv2023,Multi-Label Classification of COVID-Tweets Using Large Language Models,No.,1,"""No evidence""",2023,2023-12-17T15:50:05Z,,,
arXIv2023,CEIR: Concept-based Explainable Image Representation Learning,No.,1,"""No evidence""",2023,2023-12-17T15:37:41Z,,,
arXIv2023,Knowledge Trees: Gradient Boosting Decision Trees on Knowledge Neurons as Probing Classifier,No.,1,"""No evidence""",2023,2023-12-17T15:37:03Z,,,
arXIv2023,A Survey of Reasoning with Foundation Models,No.,1,"""No evidence""",2023,2023-12-17T15:16:13Z,,,
arXIv2023,Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks,No.,1,"""No evidence""",2023,2023-12-17T14:42:46Z,,,
arXIv2023,Pedestrian Attribute Recognition via CLIP based Prompt Vision-Language Fusion,No.,1,"""No evidence""",2023,2023-12-17T11:59:14Z,,,
arXIv2023,Bengali Intent Classification with Generative Adversarial BERT,No.,1,"""No evidence""",2023,2023-12-17T10:45:50Z,,,
arXIv2023,Silkie: Preference Distillation for Large Visual Language Models,No.,1,"""No evidence""",2023,2023-12-17T09:44:27Z,,,
arXIv2023,Explorers at #SMM4H 2023: Enhancing BERT for Health Applications through Knowledge and Model Fusion,No.,1,"""No evidence""",2023,2023-12-17T08:52:05Z,,,
arXIv2023,FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph Completion,No.,1,"""No evidence""",2023,2023-12-17T08:09:27Z,,,
arXIv2023,StarVector: Generating Scalable Vector Graphics Code from Images,No.,1,"""No evidence""",2023,2023-12-17T08:07:32Z,,,
arXIv2023,Artificial intelligence optical hardware empowers high-resolution hyperspectral video understanding at 1.2 Tb/s,No.,1,"""No evidence""",2023,2023-12-17T07:51:38Z,,,
arXIv2023,HyperPIE: Hyperparameter Information Extraction from Scientific Publications,No.,1,"""No evidence""",2023,2023-12-17T07:39:07Z,,,
arXIv2023,T2M-HiFiGPT: Generating High Quality Human Motion from Textual Descriptions with Residual Discrete Representations,No.,1,"""No evidence""",2023,2023-12-17T06:58:31Z,,,
arXIv2023,Decoding Concerns: Multi-label Classification of Vaccine Sentiments in Social Media,No.,1,"""No evidence""",2023,2023-12-17T06:55:04Z,,,
arXIv2023,Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools,No.,1,"""No evidence""",2023,2023-12-17T06:38:11Z,,,
arXIv2023,Deep dive into language traits of AI-generated Abstracts,No.,1,"""No evidence""",2023,2023-12-17T06:03:33Z,,,
arXIv2023,SeGA: Preference-Aware Self-Contrastive Learning with Prompts for Anomalous User Detection on Twitter,No.,1,"""No evidence""",2023,2023-12-17T05:35:28Z,,,
arXIv2023,Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization,No.,1,"""No evidence""",2023,2023-12-17T05:13:58Z,,,
arXIv2023,Investigating salient representations and label Variance in Dimensional Speech Emotion Analysis,No.,1,"""No evidence""",2023,2023-12-17T04:54:41Z,,,
arXIv2023,Evaluating AI Vocational Skills Through Professional Testing,No.,1,"""No evidence""",2023,2023-12-17T04:41:59Z,,,
arXIv2023,Learning Interpretable Queries for Explainable Image Classification with Information Pursuit,No.,1,"""No evidence""",2023,2023-12-16T21:43:07Z,,,
arXIv2023,"Cross-Linguistic Offensive Language Detection: BERT-Based Analysis of Bengali, Assamese, & Bodo Conversational Hateful Content from Social Media",No.,1,"""No evidence""",2023,2023-12-16T19:59:07Z,,,
arXIv2023,Paloma: A Benchmark for Evaluating Language Model Fit,No.,1,"""No evidence""",2023,2023-12-16T19:12:45Z,,,
arXIv2023,A Soft Contrastive Learning-based Prompt Model for Few-shot Sentiment Analysis,No.,1,"""No evidence""",2023,2023-12-16T15:17:28Z,,,
arXIv2023,RIGHT: Retrieval-augmented Generation for Mainstream Hashtag Recommendation,No.,1,"""No evidence""",2023,2023-12-16T14:47:03Z,,,
arXIv2023,Fusion of Deep and Shallow Features for Face Kinship Verification,No.,1,"""No evidence""",2023,2023-12-16T14:36:43Z,,,
arXIv2023,M2ConceptBase: A Fine-grained Aligned Multi-modal Conceptual Knowledge Base,No.,1,"""No evidence""",2023,2023-12-16T11:06:11Z,,,
arXIv2023,SPT: Fine-Tuning Transformer-based Language Models Efficiently with Sparsification,No.,1,"""No evidence""",2023,2023-12-16T07:44:52Z,,,
arXIv2023,CoAScore: Chain-of-Aspects Prompting for NLG Evaluation,No.,1,"""No evidence""",2023,2023-12-16T06:57:20Z,,,
arXIv2023,A Comparative Analysis of Large Language Models for Code Documentation Generation,No.,1,"""No evidence""",2023,2023-12-16T06:40:09Z,,,
arXIv2023,ProTIP: Progressive Tool Retrieval Improves Planning,No.,1,"""No evidence""",2023,2023-12-16T05:43:11Z,,,
arXIv2023,Continuous Prompt Generation from Linear Combination of Discrete Prompt Embeddings,No.,1,"""No evidence""",2023,2023-12-16T05:02:06Z,,,
arXIv2023,One Shot Learning as Instruction Data Prospector for Large Language Models,No.,1,"""No evidence""",2023,2023-12-16T03:33:12Z,,,
arXIv2023,CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare,No.,1,"""No evidence""",2023,2023-12-16T03:02:05Z,,,
arXIv2023,PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU,No.,1,"""No evidence""",2023,2023-12-16T02:27:00Z,,,
arXIv2023,VoCopilot: Voice-Activated Tracking of Everyday Interactions,No.,1,"""No evidence""",2023,2023-12-15T23:46:52Z,,,
arXIv2023,Catwalk: A Unified Language Model Evaluation Framework for Many Datasets,No.,1,"""No evidence""",2023,2023-12-15T23:11:45Z,,,
arXIv2023,Rich Human Feedback for Text-to-Image Generation,No.,1,"""No evidence""",2023,2023-12-15T22:18:38Z,,,
arXIv2023,Low-resource classification of mobility functioning information in clinical sentences using large language models,No.,1,"""No evidence""",2023,2023-12-15T20:59:17Z,,,
arXIv2023,Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey,No.,1,"""No evidence""",2023,2023-12-15T19:17:15Z,,,
arXIv2023,Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in Chart Captioning,No.,1,"""No evidence""",2023,2023-12-15T19:16:21Z,,,
arXIv2023,Osprey: Pixel Understanding with Visual Instruction Tuning,No.,1,"""No evidence""",2023,2023-12-15T18:58:11Z,,,
arXIv2023,Faithful Persona-based Conversational Dataset Generation with Large Language Models,No.,1,"""No evidence""",2023,2023-12-15T18:23:50Z,,,
arXIv2023,ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent,No.,1,"""No evidence""",2023,2023-12-15T18:20:15Z,,,
arXIv2023,LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language,No.,1,"""No evidence""",2023,2023-12-15T18:06:22Z,,,
arXIv2023,Distilling Large Language Models for Matching Patients to Clinical Trials,No.,1,"""No evidence""",2023,2023-12-15T17:11:07Z,,,
arXIv2023,Peer Learning: Learning Complex Policies in Groups from Scratch via Action Recommendations,No.,1,"""No evidence""",2023,2023-12-15T17:01:35Z,,,
arXIv2023,"Neurosymbolic Value-Inspired AI (Why, What, and How)",No.,1,"""No evidence""",2023,2023-12-15T16:33:57Z,,,
arXIv2023,A Novel Dataset for Financial Education Text Simplification in Spanish,No.,1,"""No evidence""",2023,2023-12-15T15:47:08Z,,,
arXIv2023,Generative Context-aware Fine-tuning of Self-supervised Speech Models,No.,1,"""No evidence""",2023,2023-12-15T15:46:02Z,,,
arXIv2023,Grammatical information in BERT sentence embeddings as two-dimensional arrays,No.,1,"""No evidence""",2023,2023-12-15T15:41:52Z,,,
arXIv2023,Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation,No.,1,"""No evidence""",2023,2023-12-15T15:01:10Z,,,
arXIv2023,Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,No.,1,"""No evidence""",2023,2023-12-15T14:26:06Z,,,
arXIv2023,SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models,No.,1,"""No evidence""",2023,2023-12-15T14:17:45Z,,,
arXIv2023,RJUA-QA: A Comprehensive QA Dataset for Urology,No.,1,"""No evidence""",2023,2023-12-15T13:40:25Z,,,
arXIv2023,3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V,No.,1,"""No evidence""",2023,2023-12-15T12:24:19Z,,,
arXIv2023,InstructPipe: Building Visual Programming Pipelines with Human Instructions,No.,1,"""No evidence""",2023,2023-12-15T10:34:53Z,,,
arXIv2023,Algorithms for automatic intents extraction and utterances classification for goal-oriented dialogue systems,No.,1,"""No evidence""",2023,2023-12-15T10:12:43Z,,,
arXIv2023,Vectorizing string entries for data processing on tables: when are larger language models better?,No.,1,"""No evidence""",2023,2023-12-15T09:23:56Z,,,
arXIv2023,TF-CLIP: Learning Text-free CLIP for Video-based Person Re-Identification,No.,1,"""No evidence""",2023,2023-12-15T09:10:05Z,,,
arXIv2023,A novel dual-stream time-frequency contrastive pretext tasks framework for sleep stage classification,No.,1,"""No evidence""",2023,2023-12-15T09:05:06Z,,,
arXIv2023,Leveraging Language ID to Calculate Intermediate CTC Loss for Enhanced Code-Switching Speech Recognition,No.,1,"""No evidence""",2023,2023-12-15T07:46:35Z,,,
arXIv2023,Integrating AI and Learning Analytics for Data-Driven Pedagogical Decisions and Personalized Interventions in Education,No.,1,"""No evidence""",2023,2023-12-15T06:00:26Z,,,
arXIv2023,GPT-4 Surpassing Human Performance in Linguistic Pragmatics,No.,1,"""No evidence""",2023,2023-12-15T05:40:15Z,,,
arXIv2023,Grounding for Artificial Intelligence,No.,1,"""No evidence""",2023,2023-12-15T04:45:48Z,,,
arXIv2023,ICD-LM: Configuring Vision-Language In-Context Demonstrations by Language Modeling,No.,1,"""No evidence""",2023,2023-12-15T03:11:03Z,,,
arXIv2023,GSVA: Generalized Segmentation via Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-12-15T02:54:31Z,,,
arXIv2023,A Review of Repository Level Prompting for LLMs,No.,1,"""No evidence""",2023,2023-12-15T00:34:52Z,,,
arXIv2023,Inter-Layer Scheduling Space Exploration for Multi-model Inference on Heterogeneous Chiplets,No.,1,"""No evidence""",2023,2023-12-14T23:45:55Z,,,
arXIv2023,Large Language Models for Autonomous Driving: Real-World Experiments,No.,1,"""No evidence""",2023,2023-12-14T23:23:37Z,,,
arXIv2023,Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision,No.,1,"""No evidence""",2023,2023-12-14T23:07:33Z,,,
arXIv2023,Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM,No.,1,"""No evidence""",2023,2023-12-14T22:04:07Z,,,
arXIv2023,ArchiGuesser -- AI Art Architecture Educational Game,No.,1,"""No evidence""",2023,2023-12-14T20:48:26Z,,,
arXIv2023,Well-calibrated Confidence Measures for Multi-label Text Classification with a Large Number of Labels,No.,1,"""No evidence""",2023,2023-12-14T19:17:42Z,,,
arXIv2023,Self-Evaluation Improves Selective Generation in Large Language Models,No.,1,"""No evidence""",2023,2023-12-14T19:09:22Z,,,
arXIv2023,VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation,No.,1,"""No evidence""",2023,2023-12-14T18:59:43Z,,,
arXIv2023,DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving,No.,1,"""No evidence""",2023,2023-12-14T18:59:05Z,,,
arXIv2023,Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking,No.,1,"""No evidence""",2023,2023-12-14T18:59:04Z,,,
arXIv2023,TinyGSM: achieving >80% on GSM8k with small language models,No.,1,"""No evidence""",2023,2023-12-14T18:58:28Z,,,
arXIv2023,Pixel Aligned Language Models,No.,1,"""No evidence""",2023,2023-12-14T18:57:58Z,,,
arXIv2023,"Successor Heads: Recurring, Interpretable Attention Heads In The Wild",No.,1,"""No evidence""",2023,2023-12-14T18:55:47Z,,,
arXIv2023,Weaving Pathways for Justice with GPT: LLM-driven automated drafting of interactive legal applications,No.,1,"""No evidence""",2023,2023-12-14T18:20:59Z,,,
arXIv2023,General Object Foundation Model for Images and Videos at Scale,No.,1,"""No evidence""",2023,2023-12-14T17:26:00Z,,,
arXIv2023,Towards Trustworthy AI Software Development Assistance,No.,1,"""No evidence""",2023,2023-12-14T16:59:49Z,,,
arXIv2023,The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation,No.,1,"""No evidence""",2023,2023-12-14T16:16:50Z,,,
arXIv2023,Language Modeling on a SpiNNaker 2 Neuromorphic Chip,No.,1,"""No evidence""",2023,2023-12-14T16:16:35Z,,,
arXIv2023,Holodeck: Language Guided Generation of 3D Embodied AI Environments,No.,1,"""No evidence""",2023,2023-12-14T16:04:14Z,,,
arXIv2023,"TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",No.,1,"""No evidence""",2023,2023-12-14T15:37:04Z,,,
arXIv2023,LLMind: Orchestrating AI and IoT with LLM for Complex Task Execution,No.,1,"""No evidence""",2023,2023-12-14T14:57:58Z,,,
arXIv2023,Unbiased organism-agnostic and highly sensitive signal peptide predictor with deep protein language model,No.,1,"""No evidence""",2023,2023-12-14T14:32:48Z,,,
arXIv2023,Dynamic Retrieval-Augmented Generation,No.,1,"""No evidence""",2023,2023-12-14T14:26:57Z,,,
arXIv2023,Detecting value-expressive text posts in Russian social media,No.,1,"""No evidence""",2023,2023-12-14T14:18:27Z,,,
arXIv2023,Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models,No.,1,"""No evidence""",2023,2023-12-14T14:10:02Z,,,
arXIv2023,LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers,No.,1,"""No evidence""",2023,2023-12-14T14:07:41Z,,,
arXIv2023,Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations,No.,1,"""No evidence""",2023,2023-12-14T13:41:54Z,,,
arXIv2023,Learning Long Sequences in Spiking Neural Networks,No.,1,"""No evidence""",2023,2023-12-14T13:30:27Z,,,
arXIv2023,Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning,No.,1,"""No evidence""",2023,2023-12-14T13:03:13Z,,,
arXIv2023,Motion Flow Matching for Human Motion Synthesis and Editing,No.,1,"""No evidence""",2023,2023-12-14T12:57:35Z,,,
arXIv2023,How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots,No.,1,"""No evidence""",2023,2023-12-14T11:09:50Z,,,
arXIv2023,Forbidden Facts: An Investigation of Competing Objectives in Llama-2,No.,1,"""No evidence""",2023,2023-12-14T10:27:15Z,,,
arXIv2023,Dissecting vocabulary biases datasets through statistical testing and automated data augmentation for artifact mitigation in Natural Language Inference,No.,1,"""No evidence""",2023,2023-12-14T08:46:26Z,,,
arXIv2023,A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis,No.,1,"""No evidence""",2023,2023-12-14T08:13:28Z,,,
arXIv2023,Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory,No.,1,"""No evidence""",2023,2023-12-14T07:38:12Z,,,
arXIv2023,TigerBot: An Open Multilingual Multitask LLM,No.,1,"""No evidence""",2023,2023-12-14T07:05:42Z,,,
arXIv2023,Heterogeneous Graph Neural Architecture Search with GPT-4,No.,1,"""No evidence""",2023,2023-12-14T06:31:52Z,,,
arXIv2023,Temporal-Spatial Entropy Balancing for Causal Continuous Treatment-Effect Estimation,No.,1,"""No evidence""",2023,2023-12-14T06:05:13Z,,,
arXIv2023,Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement,No.,1,"""No evidence""",2023,2023-12-14T03:49:52Z,,,
arXIv2023,MmAP : Multi-modal Alignment Prompt for Cross-domain Multi-task Learning,No.,1,"""No evidence""",2023,2023-12-14T03:33:02Z,,,
arXIv2023,Dietary Assessment with Multimodal ChatGPT: A Systematic Analysis,No.,1,"""No evidence""",2023,2023-12-14T01:26:45Z,,,
arXIv2023,ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks,No.,1,"""No evidence""",2023,2023-12-14T01:06:37Z,,,
arXIv2023,Beyond Accuracy: Automated De-Identification of Large Real-World Clinical Text Datasets,No.,1,"""No evidence""",2023,2023-12-13T20:15:29Z,,,
arXIv2023,VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering,No.,1,"""No evidence""",2023,2023-12-13T18:58:15Z,,,
arXIv2023,An Invitation to Deep Reinforcement Learning,No.,1,"""No evidence""",2023,2023-12-13T18:57:23Z,,,
arXIv2023,FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,No.,1,"""No evidence""",2023,2023-12-13T18:28:09Z,,,
arXIv2023,LD-SDM: Language-Driven Hierarchical Species Distribution Modeling,No.,1,"""No evidence""",2023,2023-12-13T18:11:37Z,,,
arXIv2023,Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4,No.,1,"""No evidence""",2023,2023-12-13T17:39:44Z,,,
arXIv2023,EquiReact: An equivariant neural network for chemical reactions,No.,1,"""No evidence""",2023,2023-12-13T17:26:54Z,,,
arXIv2023,Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes to Analyze Longitudinal Social Media Data,No.,1,"""No evidence""",2023,2023-12-13T17:15:12Z,,,
arXIv2023,Prompting LLMs with content plans to enhance the summarization of scientific articles,No.,1,"""No evidence""",2023,2023-12-13T16:57:31Z,,,
arXIv2023,High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models,No.,1,"""No evidence""",2023,2023-12-13T16:43:41Z,,,
arXIv2023,Extending Whisper with prompt tuning to target-speaker ASR,No.,1,"""No evidence""",2023,2023-12-13T11:49:16Z,,,
arXIv2023,Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation,No.,1,"""No evidence""",2023,2023-12-13T11:47:28Z,,,
arXIv2023,"Synocene, Beyond the Anthropocene: De-Anthropocentralising Human-Nature-AI Interaction",No.,1,"""No evidence""",2023,2023-12-13T11:04:06Z,,,
arXIv2023,Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision,No.,1,"""No evidence""",2023,2023-12-13T11:03:07Z,,,
arXIv2023,Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning,No.,1,"""No evidence""",2023,2023-12-13T10:00:44Z,,,
arXIv2023,Mono3DVG: 3D Visual Grounding in Monocular Images,No.,1,"""No evidence""",2023,2023-12-13T09:49:59Z,,,
arXIv2023,Assessing GPT4-V on Structured Reasoning Tasks,No.,1,"""No evidence""",2023,2023-12-13T08:54:49Z,,,
arXIv2023,CBQ: Cross-Block Quantization for Large Language Models,No.,1,"""No evidence""",2023,2023-12-13T07:56:27Z,,,
arXIv2023,LDM$^2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement,No.,1,"""No evidence""",2023,2023-12-13T06:44:37Z,,,
arXIv2023,A Survey of Text Watermarking in the Era of Large Language Models,No.,1,"""No evidence""",2023,2023-12-13T06:11:42Z,,,
arXIv2023,PromptBench: A Unified Library for Evaluation of Large Language Models,No.,1,"""No evidence""",2023,2023-12-13T05:58:34Z,,,
arXIv2023,Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction,No.,1,"""No evidence""",2023,2023-12-13T05:33:25Z,,,
arXIv2023,CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation,No.,1,"""No evidence""",2023,2023-12-13T03:48:45Z,,,
arXIv2023,Causality Analysis for Evaluating the Security of Large Language Models,No.,1,"""No evidence""",2023,2023-12-13T03:35:43Z,,,
arXIv2023,Finetuning an LLM on Contextual Knowledge of Classics for Q&A,No.,1,"""No evidence""",2023,2023-12-13T02:32:01Z,,,
arXIv2023,Large Language Models are Complex Table Parsers,No.,1,"""No evidence""",2023,2023-12-13T01:34:42Z,,,
arXIv2023,Native Language Identification with Large Language Models,No.,1,"""No evidence""",2023,2023-12-13T00:52:15Z,,,
arXIv2023,A Foundational Multimodal Vision Language AI Assistant for Human Pathology,No.,1,"""No evidence""",2023,2023-12-13T00:24:37Z,,,
arXIv2023,Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings concatenation?,No.,1,"""No evidence""",2023,2023-12-12T23:23:23Z,,,
arXIv2023,Leveraging Large Language Models to Build and Execute Computational Workflows,No.,1,"""No evidence""",2023,2023-12-12T20:17:13Z,,,
arXIv2023,A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors,No.,1,"""No evidence""",2023,2023-12-12T19:34:23Z,,,
arXIv2023,VILA: On Pre-training for Visual Language Models,No.,1,"""No evidence""",2023,2023-12-12T18:58:18Z,,,
arXIv2023,BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability,No.,1,"""No evidence""",2023,2023-12-12T18:55:43Z,,,
arXIv2023,Translating Natural Language Queries to SQL Using the T5 Model,No.,1,"""No evidence""",2023,2023-12-12T18:32:06Z,,,
arXIv2023,LMDrive: Closed-Loop End-to-End Driving with Large Language Models,No.,1,"""No evidence""",2023,2023-12-12T18:24:15Z,,,
arXIv2023,MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception,No.,1,"""No evidence""",2023,2023-12-12T17:55:45Z,,,
arXIv2023,How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation,No.,1,"""No evidence""",2023,2023-12-12T16:48:07Z,,,
arXIv2023,QSMVM: QoS-aware and social-aware multimetric routing protocol for video-streaming services over MANETs,No.,1,"""No evidence""",2023,2023-12-12T16:34:32Z,,,
arXIv2023,LLMEval: A Preliminary Study on How to Evaluate Large Language Models,No.,1,"""No evidence""",2023,2023-12-12T16:14:43Z,,,
arXIv2023,SEOpinion: Summarization and Exploration Opinion of E-Commerce Websites,No.,1,"""No evidence""",2023,2023-12-12T15:45:58Z,,,
arXIv2023,Can ChatGPT Play the Role of a Teaching Assistant in an Introductory Programming Course?,No.,1,"""No evidence""",2023,2023-12-12T15:06:44Z,,,
arXIv2023,SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion,No.,1,"""No evidence""",2023,2023-12-12T14:24:54Z,,,
arXIv2023,Towards Equipping Transformer with the Ability of Systematic Compositionality,No.,1,"""No evidence""",2023,2023-12-12T13:57:57Z,,,
arXIv2023,The GUA-Speech System Description for CNVSRC Challenge 2023,No.,1,"""No evidence""",2023,2023-12-12T13:35:33Z,,,
arXIv2023,Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning,No.,1,"""No evidence""",2023,2023-12-12T13:26:42Z,,,
arXIv2023,Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming,No.,1,"""No evidence""",2023,2023-12-12T12:26:48Z,,,
arXIv2023,Classifying complex documents: comparing bespoke solutions to large language models,No.,1,"""No evidence""",2023,2023-12-12T11:38:09Z,,,
arXIv2023,Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation,No.,1,"""No evidence""",2023,2023-12-12T11:06:07Z,,,
arXIv2023,Image Content Generation with Causal Reasoning,No.,1,"""No evidence""",2023,2023-12-12T10:07:16Z,,,
arXIv2023,TransMed: Large Language Models Enhance Vision Transformer for Biomedical Image Classification,No.,1,"""No evidence""",2023,2023-12-12T09:58:07Z,,,
arXIv2023,Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens,No.,1,"""No evidence""",2023,2023-12-12T09:47:59Z,,,
arXIv2023,Efficiently Programming Large Language Models using SGLang,No.,1,"""No evidence""",2023,2023-12-12T09:34:27Z,,,
arXIv2023,ThinkBot: Embodied Instruction Following with Thought Chain Reasoning,No.,1,"""No evidence""",2023,2023-12-12T08:30:09Z,,,
arXIv2023,Large Foundation Models for Power Systems,No.,1,"""No evidence""",2023,2023-12-12T07:55:48Z,,,
arXIv2023,Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling,No.,1,"""No evidence""",2023,2023-12-12T06:56:31Z,,,
arXIv2023,Vision-language Assisted Attribute Learning,No.,1,"""No evidence""",2023,2023-12-12T06:45:19Z,,,
arXIv2023,A dynamical clipping approach with task feedback for Proximal Policy Optimization,No.,1,"""No evidence""",2023,2023-12-12T06:35:56Z,,,
arXIv2023,ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity,No.,1,"""No evidence""",2023,2023-12-12T05:38:55Z,,,
arXIv2023,SM70: A Large Language Model for Medical Devices,No.,1,"""No evidence""",2023,2023-12-12T04:25:26Z,,,
arXIv2023,Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment,No.,1,"""No evidence""",2023,2023-12-12T03:39:07Z,,,
arXIv2023,Rethinking the Instruction Quality: LIFT is What You Need,No.,1,"""No evidence""",2023,2023-12-12T03:30:21Z,,,
arXIv2023,AI Control: Improving Safety Despite Intentional Subversion,No.,1,"""No evidence""",2023,2023-12-12T02:34:06Z,,,
arXIv2023,Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI,No.,1,"""No evidence""",2023,2023-12-12T02:28:12Z,,,
arXIv2023,Mathematical Language Models: A Survey,No.,1,"""No evidence""",2023,2023-12-12T01:39:16Z,,,
arXIv2023,Perseus: Removing Energy Bloat from Large Model Training,No.,1,"""No evidence""",2023,2023-12-12T00:16:18Z,,,
arXIv2023,Get an A in Math: Progressive Rectification Prompting,No.,1,"""No evidence""",2023,2023-12-11T22:25:57Z,,,
arXIv2023,Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates,No.,1,"""No evidence""",2023,2023-12-11T22:12:20Z,,,
arXIv2023,Self-supervised Machine Learning Based Approach to Orbit Modelling Applied to Space Traffic Management,No.,1,"""No evidence""",2023,2023-12-11T21:52:05Z,,,
arXIv2023,LLF-Bench: Benchmark for Interactive Learning from Language Feedback,No.,1,"""No evidence""",2023,2023-12-11T21:49:04Z,,,
arXIv2023,User Friendly and Adaptable Discriminative AI: Using the Lessons from the Success of LLMs and Image Generation Models,No.,1,"""No evidence""",2023,2023-12-11T20:37:58Z,,,
arXIv2023,Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning,No.,1,"""No evidence""",2023,2023-12-11T20:12:46Z,,,
arXIv2023,Honeybee: Locality-enhanced Projector for Multimodal LLM,No.,1,"""No evidence""",2023,2023-12-11T18:59:06Z,,,
arXIv2023,4M: Massively Multimodal Masked Modeling,No.,1,"""No evidence""",2023,2023-12-11T18:57:35Z,,,
arXIv2023,AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes,No.,1,"""No evidence""",2023,2023-12-11T18:56:37Z,,,
arXIv2023,Gated Linear Attention Transformers with Hardware-Efficient Training,No.,1,"""No evidence""",2023,2023-12-11T18:51:59Z,,,
arXIv2023,Neural Text to Articulate Talk: Deep Text to Audiovisual Speech Synthesis achieving both Auditory and Photo-realism,No.,1,"""No evidence""",2023,2023-12-11T18:41:55Z,,,
arXIv2023,SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-12-11T17:54:11Z,,,
arXIv2023,InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following,No.,1,"""No evidence""",2023,2023-12-11T17:53:45Z,,,
arXIv2023,On Meta-Prompting,No.,1,"""No evidence""",2023,2023-12-11T17:46:44Z,,,
arXIv2023,LLM360: Towards Fully Transparent Open-Source LLMs,No.,1,"""No evidence""",2023,2023-12-11T17:39:00Z,,,
arXIv2023,Revisiting the Role of Label Smoothing in Enhanced Text Sentiment Classification,No.,1,"""No evidence""",2023,2023-12-11T17:00:35Z,,,
arXIv2023,Where exactly does contextualization in a PLM happen?,No.,1,"""No evidence""",2023,2023-12-11T16:39:52Z,,,
arXIv2023,Grounded Question-Answering in Long Egocentric Videos,No.,1,"""No evidence""",2023,2023-12-11T16:31:55Z,,,
arXIv2023,DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven Differentiable Physics,No.,1,"""No evidence""",2023,2023-12-11T14:29:25Z,,,
arXIv2023,Debiased Machine Learning and Network Cohesion for Doubly-Robust Differential Reward Models in Contextual Bandits,No.,1,"""No evidence""",2023,2023-12-11T14:24:24Z,,,
arXIv2023,Contrastive News and Social Media Linking using BERT for Articles and Tweets across Dual Platforms,No.,1,"""No evidence""",2023,2023-12-11T13:38:16Z,,,
arXIv2023,Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes,No.,1,"""No evidence""",2023,2023-12-11T13:03:21Z,,,
arXIv2023,Evaluation of Large Language Models for Decision Making in Autonomous Driving,No.,1,"""No evidence""",2023,2023-12-11T12:56:40Z,,,
arXIv2023,MMDesign: Multi-Modality Transfer Learning for Generative Protein Design,No.,1,"""No evidence""",2023,2023-12-11T10:59:23Z,,,
arXIv2023,EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models,No.,1,"""No evidence""",2023,2023-12-11T10:35:32Z,,,
arXIv2023,Survey on Foundation Models for Prognostics and Health Management in Industrial Cyber-Physical Systems,No.,1,"""No evidence""",2023,2023-12-11T09:58:46Z,,,
arXIv2023,Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator,No.,1,"""No evidence""",2023,2023-12-11T09:44:41Z,,,
arXIv2023,Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models,No.,1,"""No evidence""",2023,2023-12-11T08:49:18Z,,,
arXIv2023,Decoupling SQL Query Hardness Parsing for Text-to-SQL,No.,1,"""No evidence""",2023,2023-12-11T07:20:46Z,,,
arXIv2023,Textual Prompt Guided Image Restoration,No.,1,"""No evidence""",2023,2023-12-11T06:56:41Z,,,
arXIv2023,"""What's important here?"": Opportunities and Challenges of Using LLMs in Retrieving Information from Web Interfaces",No.,1,"""No evidence""",2023,2023-12-11T06:26:38Z,,,
arXIv2023,Generative Large Language Models Are All-purpose Text Analytics Engines: Text-to-text Learning Is All Your Need,No.,1,"""No evidence""",2023,2023-12-11T04:00:26Z,,,
arXIv2023,PromptMTopic: Unsupervised Multimodal Topic Modeling of Memes using Large Language Models,No.,1,"""No evidence""",2023,2023-12-11T03:36:50Z,,,
arXIv2023,Audio-Visual LLM for Video Understanding,No.,1,"""No evidence""",2023,2023-12-11T02:50:46Z,,,
arXIv2023,Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions,No.,1,"""No evidence""",2023,2023-12-11T02:27:45Z,,,
arXIv2023,Privacy Issues in Large Language Models: A Survey,No.,1,"""No evidence""",2023,2023-12-11T01:26:53Z,,,
arXIv2023,Evidence-based Interpretable Open-domain Fact-checking with Large Language Models,No.,1,"""No evidence""",2023,2023-12-10T09:27:50Z,,,
arXIv2023,Early ChatGPT User Portrait through the Lens of Data,No.,1,"""No evidence""",2023,2023-12-10T07:08:51Z,,,
arXIv2023,Negative Pre-aware for Noisy Cross-modal Matching,No.,1,"""No evidence""",2023,2023-12-10T05:52:36Z,,,
arXIv2023,GAMC: An Unsupervised Method for Fake News Detection using Graph Autoencoder with Masking,No.,1,"""No evidence""",2023,2023-12-10T03:34:29Z,,,
arXIv2023,FP8-BERT: Post-Training Quantization for Transformer,No.,1,"""No evidence""",2023,2023-12-10T02:14:34Z,,,
arXIv2023,Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in Video-Language Joint Learning,No.,1,"""No evidence""",2023,2023-12-10T02:03:51Z,,,
arXIv2023,Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning,No.,1,"""No evidence""",2023,2023-12-10T01:19:59Z,,,
arXIv2023,Labrador: Exploring the Limits of Masked Language Modeling for Laboratory Data,No.,1,"""No evidence""",2023,2023-12-09T23:43:35Z,,,
arXIv2023,GPT-4 and Safety Case Generation: An Exploratory Analysis,No.,1,"""No evidence""",2023,2023-12-09T22:28:48Z,,,
arXIv2023,Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge,No.,1,"""No evidence""",2023,2023-12-09T22:12:52Z,,,
arXIv2023,NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?,No.,1,"""No evidence""",2023,2023-12-09T21:42:20Z,,,
arXIv2023,Hate Speech and Offensive Content Detection in Indo-Aryan Languages: A Battle of LSTM and Transformers,No.,1,"""No evidence""",2023,2023-12-09T20:24:00Z,,,
arXIv2023,Leveraging Reinforcement Learning and Large Language Models for Code Optimization,No.,1,"""No evidence""",2023,2023-12-09T19:50:23Z,,,
arXIv2023,Redefining Developer Assistance: Through Large Language Models in Software Ecosystem,No.,1,"""No evidence""",2023,2023-12-09T18:02:37Z,,,
arXIv2023,Artificial Intelligence in the automatic coding of interviews on Landscape Quality Objectives. Comparison and case study,No.,1,"""No evidence""",2023,2023-12-09T15:37:19Z,,,
arXIv2023,Generative AI for Physical Layer Communications: A Survey,No.,1,"""No evidence""",2023,2023-12-09T15:20:56Z,,,
arXIv2023,A Review of Hybrid and Ensemble in Deep Learning for Natural Language Processing,No.,1,"""No evidence""",2023,2023-12-09T14:49:34Z,,,
arXIv2023,Language-assisted Vision Model Debugger: A Sample-Free Approach to Finding and Fixing Bugs,No.,1,"""No evidence""",2023,2023-12-09T14:42:58Z,,,
arXIv2023,Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning,No.,1,"""No evidence""",2023,2023-12-09T13:20:49Z,,,
arXIv2023,D3A-TS: Denoising-Driven Data Augmentation in Time Series,No.,1,"""No evidence""",2023,2023-12-09T11:37:07Z,,,
arXIv2023,KEN: Kernel Extensions using Natural Language,No.,1,"""No evidence""",2023,2023-12-09T10:45:54Z,,,
arXIv2023,Stateful Large Language Model Serving with Pensieve,No.,1,"""No evidence""",2023,2023-12-09T09:55:07Z,,,
arXIv2023,Enhanced E-Commerce Attribute Extraction: Innovating with Decorative Relation Correction and LLAMA 2.0-Based Annotation,No.,1,"""No evidence""",2023,2023-12-09T08:26:30Z,,,
arXIv2023,Aligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models,No.,1,"""No evidence""",2023,2023-12-09T08:25:55Z,,,
arXIv2023,History Matters: Temporal Knowledge Editing in Large Language Model,No.,1,"""No evidence""",2023,2023-12-09T07:51:56Z,,,
arXIv2023,Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis,No.,1,"""No evidence""",2023,2023-12-09T07:33:26Z,,,
arXIv2023,Teamwork Dimensions Classification Using BERT,No.,1,"""No evidence""",2023,2023-12-09T07:18:41Z,,,
arXIv2023,Image and Data Mining in Reticular Chemistry Using GPT-4V,No.,1,"""No evidence""",2023,2023-12-09T05:05:25Z,,,
arXIv2023,Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation,No.,1,"""No evidence""",2023,2023-12-09T04:43:49Z,,,
arXIv2023,Steering Llama 2 via Contrastive Activation Addition,No.,1,"""No evidence""",2023,2023-12-09T04:40:46Z,,,
arXIv2023,Domain Adaptation of a State of the Art Text-to-SQL Model: Lessons Learned and Challenges Found,No.,1,"""No evidence""",2023,2023-12-09T03:30:21Z,,,
arXIv2023,Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models,No.,1,"""No evidence""",2023,2023-12-09T01:59:11Z,,,
arXIv2023,Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs,No.,1,"""No evidence""",2023,2023-12-08T20:28:08Z,,,
arXIv2023,PixLore: A Dataset-driven Approach to Rich Image Captioning,No.,1,"""No evidence""",2023,2023-12-08T20:12:26Z,,,
arXIv2023,"Generative AI in Higher Education: Seeing ChatGPT Through Universities' Policies, Resources, and Guidelines",No.,1,"""No evidence""",2023,2023-12-08T18:33:11Z,,,
arXIv2023,GlitchBench: Can large multimodal models detect video game glitches?,No.,1,"""No evidence""",2023,2023-12-08T18:14:21Z,,,
arXIv2023,PathFinder: Guided Search over Multi-Step Reasoning Paths,No.,1,"""No evidence""",2023,2023-12-08T17:05:47Z,,,
arXIv2023,INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers,No.,1,"""No evidence""",2023,2023-12-08T15:21:54Z,,,
arXIv2023,Vision-based Learning for Drones: A Survey,No.,1,"""No evidence""",2023,2023-12-08T12:57:13Z,,,
arXIv2023,The ICL Consistency Test,No.,1,"""No evidence""",2023,2023-12-08T10:22:43Z,,,
arXIv2023,EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism,No.,1,"""No evidence""",2023,2023-12-08T09:31:50Z,,,
arXIv2023,Ophtha-LLaMA2: A Large Language Model for Ophthalmology,No.,1,"""No evidence""",2023,2023-12-08T08:43:46Z,,,
arXIv2023,Cross-BERT for Point Cloud Pretraining,No.,1,"""No evidence""",2023,2023-12-08T08:18:12Z,,,
arXIv2023,KwaiAgents: Generalized Information-seeking Agent System with Large Language Models,No.,1,"""No evidence""",2023,2023-12-08T08:11:11Z,,,
arXIv2023,Localized Symbolic Knowledge Distillation for Visual Commonsense Models,No.,1,"""No evidence""",2023,2023-12-08T05:23:50Z,,,
arXIv2023,Human-Readable Fingerprint for Large Language Models,No.,1,"""No evidence""",2023,2023-12-08T05:01:47Z,,,
arXIv2023,Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation,No.,1,"""No evidence""",2023,2023-12-08T03:44:09Z,,,
arXIv2023,User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning,No.,1,"""No evidence""",2023,2023-12-08T02:08:00Z,,,
arXIv2023,Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs,No.,1,"""No evidence""",2023,2023-12-08T01:41:36Z,,,
arXIv2023,Fine-Tuning InstructPix2Pix for Advanced Image Colorization,No.,1,"""No evidence""",2023,2023-12-08T01:36:49Z,,,
arXIv2023,How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey,No.,1,"""No evidence""",2023,2023-12-08T01:17:28Z,,,
arXIv2023,Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks,No.,1,"""No evidence""",2023,2023-12-07T23:26:06Z,,,
arXIv2023,Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos,No.,1,"""No evidence""",2023,2023-12-07T23:16:37Z,,,
arXIv2023,Efficient Large Language Models Fine-Tuning On Graphs,No.,1,"""No evidence""",2023,2023-12-07T22:35:16Z,,,
arXIv2023,STraceBERT: Source Code Retrieval using Semantic Application Traces,No.,1,"""No evidence""",2023,2023-12-07T22:19:50Z,,,
arXIv2023,From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis,No.,1,"""No evidence""",2023,2023-12-07T21:58:37Z,,,
arXIv2023,Efficient Parallel Reinforcement Learning Framework using the Reactor Model,No.,1,"""No evidence""",2023,2023-12-07T21:19:57Z,,,
arXIv2023,LLM4TDD: Best Practices for Test Driven Development Using Large Language Models,No.,1,"""No evidence""",2023,2023-12-07T20:37:54Z,,,
arXIv2023,Latent Skill Discovery for Chain-of-Thought Reasoning,No.,1,"""No evidence""",2023,2023-12-07T20:36:10Z,,,
arXIv2023,ConVRT: Consistent Video Restoration Through Turbulence with Test-time Optimization of Neural Video Representations,No.,1,"""No evidence""",2023,2023-12-07T20:19:48Z,,,
arXIv2023,Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations,No.,1,"""No evidence""",2023,2023-12-07T19:40:50Z,,,
arXIv2023,LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos,No.,1,"""No evidence""",2023,2023-12-07T19:19:25Z,,,
arXIv2023,On Sarcasm Detection with OpenAI GPT-based Models,No.,1,"""No evidence""",2023,2023-12-07T19:00:56Z,,,
arXIv2023,Large Language Models for Mathematicians,No.,1,"""No evidence""",2023,2023-12-07T18:59:29Z,,,
arXIv2023,Improved Visual Grounding through Self-Consistent Explanations,No.,1,"""No evidence""",2023,2023-12-07T18:59:22Z,,,
arXIv2023,Generating Illustrated Instructions,No.,1,"""No evidence""",2023,2023-12-07T18:59:20Z,,,
arXIv2023,Auto-Vocabulary Semantic Segmentation,No.,1,"""No evidence""",2023,2023-12-07T18:55:52Z,,,
arXIv2023,Trajeglish: Learning the Language of Driving Scenarios,No.,1,"""No evidence""",2023,2023-12-07T18:53:27Z,,,
arXIv2023,Using Large Language Models for Hyperparameter Optimization,No.,1,"""No evidence""",2023,2023-12-07T18:46:50Z,,,
arXIv2023,An LLM Compiler for Parallel Function Calling,No.,1,"""No evidence""",2023,2023-12-07T18:32:04Z,,,
arXIv2023,A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation,No.,1,"""No evidence""",2023,2023-12-07T18:30:15Z,,,
arXIv2023,AVA: Towards Autonomous Visualization Agents through Visual Perception-Driven Decision-Making,No.,1,"""No evidence""",2023,2023-12-07T18:13:42Z,,,
arXIv2023,Chain of Code: Reasoning with a Language Model-Augmented Code Emulator,No.,1,"""No evidence""",2023,2023-12-07T17:51:43Z,,,
arXIv2023,On the Learnability of Watermarks for Language Models,No.,1,"""No evidence""",2023,2023-12-07T17:41:44Z,,,
arXIv2023,OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization,No.,1,"""No evidence""",2023,2023-12-07T17:06:20Z,,,
arXIv2023,LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs,No.,1,"""No evidence""",2023,2023-12-07T15:43:52Z,,,
arXIv2023,CLadder: Assessing Causal Reasoning in Language Models,No.,1,"""No evidence""",2023,2023-12-07T15:12:12Z,,,
arXIv2023,Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies,No.,1,"""No evidence""",2023,2023-12-07T15:05:59Z,,,
arXIv2023,Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers,No.,1,"""No evidence""",2023,2023-12-07T14:50:41Z,,,
arXIv2023,MIMo: A Multi-Modal Infant Model for Studying Cognitive Development,No.,1,"""No evidence""",2023,2023-12-07T14:21:31Z,,,
arXIv2023,Towards Knowledge-driven Autonomous Driving,No.,1,"""No evidence""",2023,2023-12-07T14:17:17Z,,,
arXIv2023,GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives,No.,1,"""No evidence""",2023,2023-12-07T14:11:00Z,,,
arXIv2023,GPT-4V with Emotion: A Zero-shot Benchmark for Generalized Emotion Recognition,No.,1,"""No evidence""",2023,2023-12-07T13:27:37Z,,,
arXIv2023,Language Model Knowledge Distillation for Efficient Question Answering in Spanish,No.,1,"""No evidence""",2023,2023-12-07T10:21:22Z,,,
arXIv2023,AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online Labor Platform,No.,1,"""No evidence""",2023,2023-12-07T10:06:34Z,,,
arXIv2023,Text as Image: Learning Transferable Adapter for Multi-Label Classification,No.,1,"""No evidence""",2023,2023-12-07T09:22:20Z,,,
arXIv2023,Using a Large Language Model to generate a Design Structure Matrix,No.,1,"""No evidence""",2023,2023-12-07T08:48:54Z,,,
arXIv2023,VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal Models,No.,1,"""No evidence""",2023,2023-12-07T06:53:55Z,,,
arXIv2023,Large Language Models are Good Prompt Learners for Low-Shot Image Classification,No.,1,"""No evidence""",2023,2023-12-07T06:43:34Z,,,
arXIv2023,Making Translators Privacy-aware on the User's Side,No.,1,"""No evidence""",2023,2023-12-07T06:23:17Z,,,
arXIv2023,Combining inherent knowledge of vision-language models with unsupervised domain adaptation through self-knowledge distillation,No.,1,"""No evidence""",2023,2023-12-07T06:16:39Z,,,
arXIv2023,Multimodal Misinformation Detection in a South African Social Media Environment,No.,1,"""No evidence""",2023,2023-12-07T05:20:15Z,,,
arXIv2023,Efficiently Predicting Protein Stability Changes Upon Single-point Mutation with Large Language Models,No.,1,"""No evidence""",2023,2023-12-07T03:25:49Z,,,
arXIv2023,PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation,No.,1,"""No evidence""",2023,2023-12-07T03:10:03Z,,,
arXIv2023,Improving Medical Report Generation with Adapter Tuning and Knowledge Enhancement in Vision-Language Foundation Models,No.,1,"""No evidence""",2023,2023-12-07T01:01:45Z,,,
arXIv2023,Assessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk,No.,1,"""No evidence""",2023,2023-12-06T23:20:03Z,,,
arXIv2023,Language Model Alignment with Elastic Reset,No.,1,"""No evidence""",2023,2023-12-06T22:53:34Z,,,
arXIv2023,A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints,No.,1,"""No evidence""",2023,2023-12-06T20:58:07Z,,,
arXIv2023,WonderJourney: Going from Anywhere to Everywhere,No.,1,"""No evidence""",2023,2023-12-06T20:22:32Z,,,
arXIv2023,FoMo Rewards: Can we cast foundation models as reward functions?,No.,1,"""No evidence""",2023,2023-12-06T20:11:02Z,,,
arXIv2023,Efficient Large Language Models: A Survey,No.,1,"""No evidence""",2023,2023-12-06T19:18:42Z,,,
arXIv2023,Dr. Jekyll and Mr. Hyde: Two Faces of LLMs,No.,1,"""No evidence""",2023,2023-12-06T19:07:38Z,,,
arXIv2023,LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning,No.,1,"""No evidence""",2023,2023-12-06T19:02:40Z,,,
arXIv2023,Alpha-CLIP: A CLIP Model Focusing on Wherever You Want,No.,1,"""No evidence""",2023,2023-12-06T18:59:30Z,,,
arXIv2023,OneLLM: One Framework to Align All Modalities with Language,No.,1,"""No evidence""",2023,2023-12-06T18:59:19Z,,,
arXIv2023,Evaluating and Mitigating Discrimination in Language Model Decisions,No.,1,"""No evidence""",2023,2023-12-06T18:53:01Z,,,
arXIv2023,"LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem",No.,1,"""No evidence""",2023,2023-12-06T18:50:26Z,,,
arXIv2023,An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition,No.,1,"""No evidence""",2023,2023-12-06T18:34:42Z,,,
arXIv2023,"Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia",No.,1,"""No evidence""",2023,2023-12-06T18:33:50Z,,,
arXIv2023,Improving Activation Steering in Language Models with Mean-Centring,No.,1,"""No evidence""",2023,2023-12-06T18:27:07Z,,,
arXIv2023,"Not All Large Language Models (LLMs) Succumb to the ""Reversal Curse"": A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models",No.,1,"""No evidence""",2023,2023-12-06T17:29:45Z,,,
arXIv2023,Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models,No.,1,"""No evidence""",2023,2023-12-06T17:29:03Z,,,
arXIv2023,XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering,No.,1,"""No evidence""",2023,2023-12-06T15:59:06Z,,,
arXIv2023,Enhancing Kinship Verification through Multiscale Retinex and Combined Deep-Shallow features,No.,1,"""No evidence""",2023,2023-12-06T15:52:31Z,,,
arXIv2023,Blueprinting the Future: Automatic Item Categorization using Hierarchical Zero-Shot and Few-Shot Classifiers,No.,1,"""No evidence""",2023,2023-12-06T15:51:49Z,,,
arXIv2023,Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment,No.,1,"""No evidence""",2023,2023-12-06T15:27:26Z,,,
arXIv2023,GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models,No.,1,"""No evidence""",2023,2023-12-06T15:14:30Z,,,
arXIv2023,Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation,No.,1,"""No evidence""",2023,2023-12-06T13:59:22Z,,,
arXIv2023,PneumoLLM: Harnessing the Power of Large Language Model for Pneumoconiosis Diagnosis,No.,1,"""No evidence""",2023,2023-12-06T13:31:52Z,,,
arXIv2023,JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live,No.,1,"""No evidence""",2023,2023-12-06T13:19:34Z,,,
arXIv2023,DBCopilot: Scaling Natural Language Querying to Massive Databases,No.,1,"""No evidence""",2023,2023-12-06T12:37:28Z,,,
arXIv2023,Run LoRA Run: Faster and Lighter LoRA Implementations,No.,1,"""No evidence""",2023,2023-12-06T10:54:34Z,,,
arXIv2023,Compressed Context Memory For Online Language Model Interaction,No.,1,"""No evidence""",2023,2023-12-06T10:50:43Z,,,
arXIv2023,A Text-to-Text Model for Multilingual Offensive Language Identification,No.,1,"""No evidence""",2023,2023-12-06T09:37:27Z,,,
arXIv2023,Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym,No.,1,"""No evidence""",2023,2023-12-06T04:48:26Z,,,
arXIv2023,VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation,No.,1,"""No evidence""",2023,2023-12-06T04:02:28Z,,,
arXIv2023,Rethinking E-Commerce Search,No.,1,"""No evidence""",2023,2023-12-06T01:15:40Z,,,
arXIv2023,Corporate Bankruptcy Prediction with Domain-Adapted BERT,No.,1,"""No evidence""",2023,2023-12-06T00:05:25Z,,,
arXIv2023,A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education,No.,1,"""No evidence""",2023,2023-12-05T22:29:43Z,,,
arXIv2023,FlexModel: A Framework for Interpretability of Distributed Large Language Models,No.,1,"""No evidence""",2023,2023-12-05T21:19:33Z,,,
arXIv2023,A Hardware Evaluation Framework for Large Language Model Inference,No.,1,"""No evidence""",2023,2023-12-05T21:01:33Z,,,
arXIv2023,Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations,No.,1,"""No evidence""",2023,2023-12-05T20:41:34Z,,,
arXIv2023,Evaluating Agents using Social Choice Theory,No.,1,"""No evidence""",2023,2023-12-05T20:40:37Z,,,
arXIv2023,Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment,No.,1,"""No evidence""",2023,2023-12-05T20:07:34Z,,,
arXIv2023,GPT4Point: A Unified Framework for Point-Language Understanding and Generation,No.,1,"""No evidence""",2023,2023-12-05T18:59:55Z,,,
arXIv2023,Describing Differences in Image Sets with Natural Language,No.,1,"""No evidence""",2023,2023-12-05T18:59:16Z,,,
arXIv2023,MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures,No.,1,"""No evidence""",2023,2023-12-05T18:50:12Z,,,
arXIv2023,WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words,No.,1,"""No evidence""",2023,2023-12-05T18:03:13Z,,,
arXIv2023,Diversified in-domain synthesis with efficient fine-tuning for few-shot classification,No.,1,"""No evidence""",2023,2023-12-05T17:18:09Z,,,
arXIv2023,BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models,No.,1,"""No evidence""",2023,2023-12-05T17:06:59Z,,,
arXIv2023,Customization Assistant for Text-to-image Generation,No.,1,"""No evidence""",2023,2023-12-05T16:54:42Z,,,
arXIv2023,Toward autocorrection of chemical process flowsheets using large language models,No.,1,"""No evidence""",2023,2023-12-05T16:39:41Z,,,
arXIv2023,Leveraging Domain Adaptation and Data Augmentation to Improve Qur'anic IR in English and Arabic,No.,1,"""No evidence""",2023,2023-12-05T14:44:08Z,,,
arXIv2023,Large Language Models on Graphs: A Comprehensive Survey,No.,1,"""No evidence""",2023,2023-12-05T14:14:27Z,,,
arXIv2023,Scaling Laws for Adversarial Attacks on Language Model Activations,No.,1,"""No evidence""",2023,2023-12-05T14:12:15Z,,,
arXIv2023,Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions,No.,1,"""No evidence""",2023,2023-12-05T14:01:43Z,,,
arXIv2023,Towards Measuring Representational Similarity of Large Language Models,No.,1,"""No evidence""",2023,2023-12-05T12:48:04Z,,,
arXIv2023,Large Knowledge Model: Perspectives and Challenges,No.,1,"""No evidence""",2023,2023-12-05T12:07:30Z,,,
arXIv2023,Prompt Optimization via Adversarial In-Context Learning,No.,1,"""No evidence""",2023,2023-12-05T09:44:45Z,,,
arXIv2023,Impact of Tokenization on LLaMa Russian Adaptation,No.,1,"""No evidence""",2023,2023-12-05T09:16:03Z,,,
arXIv2023,Empathy and Distress Detection using Ensembles of Transformer Models,No.,1,"""No evidence""",2023,2023-12-05T08:50:34Z,,,
arXIv2023,ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference,No.,1,"""No evidence""",2023,2023-12-05T07:52:12Z,,,
arXIv2023,Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning,No.,1,"""No evidence""",2023,2023-12-05T07:29:14Z,,,
arXIv2023,Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction,No.,1,"""No evidence""",2023,2023-12-05T07:27:08Z,,,
arXIv2023,DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework,No.,1,"""No evidence""",2023,2023-12-05T06:28:45Z,,,
arXIv2023,Towards More Unified In-context Visual Understanding,No.,1,"""No evidence""",2023,2023-12-05T06:02:21Z,,,
arXIv2023,ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU,No.,1,"""No evidence""",2023,2023-12-05T05:38:38Z,,,
arXIv2023,MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative Models on Medical Conversation Tasks,No.,1,"""No evidence""",2023,2023-12-05T04:55:54Z,,,
arXIv2023,EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model,No.,1,"""No evidence""",2023,2023-12-05T04:15:56Z,,,
arXIv2023,MedDM:LLM-executable clinical guidance tree for clinical decision-making,No.,1,"""No evidence""",2023,2023-12-05T02:44:07Z,,,
arXIv2023,Lenna: Language Enhanced Reasoning Detection Assistant,No.,1,"""No evidence""",2023,2023-12-05T02:19:35Z,,,
arXIv2023,PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models,No.,1,"""No evidence""",2023,2023-12-05T02:08:48Z,,,
arXIv2023,Protein Language Model-Powered 3D Ligand Binding Site Prediction from Protein Sequence,No.,1,"""No evidence""",2023,2023-12-05T01:47:38Z,,,
arXIv2023,Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey,No.,1,"""No evidence""",2023,2023-12-05T01:10:54Z,,,
arXIv2023,Efficient Online Data Mixing For Language Model Pre-Training,No.,1,"""No evidence""",2023,2023-12-05T00:42:35Z,,,
arXIv2023,Breast Ultrasound Report Generation using LangChain,No.,1,"""No evidence""",2023,2023-12-05T00:28:26Z,,,
arXIv2023,Harmonizing Global Voices: Culturally-Aware Models for Enhanced Content Moderation,No.,1,"""No evidence""",2023,2023-12-05T00:11:09Z,,,
arXIv2023,RINAS: Training with Dataset Shuffling Can Be General and Fast,No.,1,"""No evidence""",2023,2023-12-04T21:50:08Z,,,
arXIv2023,Measuring Distributional Shifts in Text: The Advantage of Language Model-Based Embeddings,No.,1,"""No evidence""",2023,2023-12-04T20:46:48Z,,,
arXIv2023,Revisiting Topic-Guided Language Models,No.,1,"""No evidence""",2023,2023-12-04T20:33:24Z,,,
arXIv2023,VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding,No.,1,"""No evidence""",2023,2023-12-04T19:48:02Z,,,
arXIv2023,LLMs Accelerate Annotation for Medical Information Extraction,No.,1,"""No evidence""",2023,2023-12-04T19:26:13Z,,,
arXIv2023,Rejuvenating image-GPT as Strong Visual Representation Learners,No.,1,"""No evidence""",2023,2023-12-04T18:59:20Z,,,
arXIv2023,Object Recognition as Next Token Prediction,No.,1,"""No evidence""",2023,2023-12-04T18:58:40Z,,,
arXIv2023,Magicoder: Source Code Is All You Need,No.,1,"""No evidence""",2023,2023-12-04T18:50:35Z,,,
arXIv2023,Tree of Attacks: Jailbreaking Black-Box LLMs Automatically,No.,1,"""No evidence""",2023,2023-12-04T18:49:23Z,,,
arXIv2023,"Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images",No.,1,"""No evidence""",2023,2023-12-04T18:35:27Z,,,
arXIv2023,StoryGPT-V: Large Language Models as Consistent Story Visualizers,No.,1,"""No evidence""",2023,2023-12-04T18:14:29Z,,,
arXIv2023,Physics simulation capabilities of LLMs,No.,1,"""No evidence""",2023,2023-12-04T18:06:41Z,,,
arXIv2023,Fine-Tuning Language Models for Context-Specific SQL Query Generation,No.,1,"""No evidence""",2023,2023-12-04T18:04:27Z,,,
arXIv2023,Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data and Large-Language Models,No.,1,"""No evidence""",2023,2023-12-04T17:09:58Z,,,
arXIv2023,TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding,No.,1,"""No evidence""",2023,2023-12-04T17:09:52Z,,,
arXIv2023,Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models,No.,1,"""No evidence""",2023,2023-12-04T16:43:36Z,,,
arXIv2023,Towards Learning a Generalist Model for Embodied Navigation,No.,1,"""No evidence""",2023,2023-12-04T16:32:51Z,,,
arXIv2023,"A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly",No.,1,"""No evidence""",2023,2023-12-04T16:25:18Z,,,
arXIv2023,Bootstrapping SparseFormers from Vision Foundation Models,No.,1,"""No evidence""",2023,2023-12-04T16:04:41Z,,,
arXIv2023,Semantics-aware Motion Retargeting with Vision-Language Models,No.,1,"""No evidence""",2023,2023-12-04T15:23:49Z,,,
arXIv2023,Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective,No.,1,"""No evidence""",2023,2023-12-04T15:16:12Z,,,
arXIv2023,Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large Language Models,No.,1,"""No evidence""",2023,2023-12-04T15:12:04Z,,,
arXIv2023,Intrusion Detection System with Machine Learning and Multiple Datasets,No.,1,"""No evidence""",2023,2023-12-04T14:58:19Z,,,
arXIv2023,A Reliable Representation with Bidirectional Transition Model for Visual Reinforcement Learning Generalization,No.,1,"""No evidence""",2023,2023-12-04T14:19:36Z,,,
arXIv2023,InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models,No.,1,"""No evidence""",2023,2023-12-04T13:40:05Z,,,
arXIv2023,Unleashing the Potential of Large Language Model: Zero-shot VQA for Flood Disaster Scenario,No.,1,"""No evidence""",2023,2023-12-04T13:25:16Z,,,
arXIv2023,Prompting Disentangled Embeddings for Knowledge Graph Completion with Pre-trained Language Model,No.,1,"""No evidence""",2023,2023-12-04T12:20:25Z,,,
arXIv2023,Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication,No.,1,"""No evidence""",2023,2023-12-04T11:53:56Z,,,
arXIv2023,Learning Machine Morality through Experience and Interaction,No.,1,"""No evidence""",2023,2023-12-04T11:46:34Z,,,
arXIv2023,LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics,No.,1,"""No evidence""",2023,2023-12-04T10:37:58Z,,,
arXIv2023,Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models,No.,1,"""No evidence""",2023,2023-12-04T08:07:21Z,,,
arXIv2023,Data Management For Large Language Models: A Survey,No.,1,"""No evidence""",2023,2023-12-04T07:42:16Z,,,
arXIv2023,Jellyfish: A Large Language Model for Data Preprocessing,No.,1,"""No evidence""",2023,2023-12-04T07:01:54Z,,,
arXIv2023,MedXChat: Bridging CXR Modalities with a Unified Multimodal Large Model,No.,1,"""No evidence""",2023,2023-12-04T06:40:12Z,,,
arXIv2023,ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions,No.,1,"""No evidence""",2023,2023-12-04T06:23:37Z,,,
arXIv2023,The Contemporary Art of Image Search: Iterative User Intent Expansion via Vision-Language Model,No.,1,"""No evidence""",2023,2023-12-04T06:14:25Z,,,
arXIv2023,Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation,No.,1,"""No evidence""",2023,2023-12-04T06:01:32Z,,,
arXIv2023,CLAMP: Contrastive LAnguage Model Prompt-tuning,No.,1,"""No evidence""",2023,2023-12-04T05:13:59Z,,,
arXIv2023,Good Questions Help Zero-Shot Image Reasoning,No.,1,"""No evidence""",2023,2023-12-04T03:18:51Z,,,
arXIv2023,How to Configure Good In-Context Sequence for Visual Question Answering,No.,1,"""No evidence""",2023,2023-12-04T02:03:23Z,,,
arXIv2023,The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning,No.,1,"""No evidence""",2023,2023-12-04T00:46:11Z,,,
arXIv2023,Using Large Language Models to Accelerate Communication for Users with Severe Motor Impairments,No.,1,"""No evidence""",2023,2023-12-03T23:12:49Z,,,
arXIv2023,SymNoise: Advancing Language Model Fine-tuning with Symmetric Noise,No.,1,"""No evidence""",2023,2023-12-03T22:44:58Z,,,
arXIv2023,Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation,No.,1,"""No evidence""",2023,2023-12-03T20:42:38Z,,,
arXIv2023,Personality of AI,No.,1,"""No evidence""",2023,2023-12-03T18:23:45Z,,,
arXIv2023,Next-Step Hint Generation for Introductory Programming Using Large Language Models,No.,1,"""No evidence""",2023,2023-12-03T17:51:07Z,,,
arXIv2023,"Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models",No.,1,"""No evidence""",2023,2023-12-03T16:39:36Z,,,
arXIv2023,Automatic Report Generation for Histopathology images using pre-trained Vision Transformers and BERT,No.,1,"""No evidence""",2023,2023-12-03T15:56:09Z,,,
arXIv2023,Improving In-Context Learning in Diffusion Models with Visual Context-Modulated Prompts,No.,1,"""No evidence""",2023,2023-12-03T14:15:52Z,,,
arXIv2023,Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder's Perspective,No.,1,"""No evidence""",2023,2023-12-03T13:52:32Z,,,
arXIv2023,ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications,No.,1,"""No evidence""",2023,2023-12-03T10:03:50Z,,,
arXIv2023,MABViT -- Modified Attention Block Enhances Vision Transformers,No.,1,"""No evidence""",2023,2023-12-03T09:00:31Z,,,
arXIv2023,NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian,No.,1,"""No evidence""",2023,2023-12-03T08:09:45Z,,,
arXIv2023,SAGE: Bridging Semantic and Actionable Parts for GEneralizable Manipulation of Articulated Objects,No.,1,"""No evidence""",2023,2023-12-03T07:22:42Z,,,
arXIv2023,JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization,No.,1,"""No evidence""",2023,2023-12-03T07:03:04Z,,,
arXIv2023,On Significance of Subword tokenization for Low Resource and Efficient Named Entity Recognition: A case study in Marathi,No.,1,"""No evidence""",2023,2023-12-03T06:53:53Z,,,
arXIv2023,Axiomatic Preference Modeling for Longform Question Answering,No.,1,"""No evidence""",2023,2023-12-02T23:11:41Z,,,
arXIv2023,Just-in-Time Security Patch Detection -- LLM At the Rescue for Data Augmentation,No.,1,"""No evidence""",2023,2023-12-02T22:53:26Z,,,
arXIv2023,Automatic Scoring of Students' Science Writing Using Hybrid Neural Network,No.,1,"""No evidence""",2023,2023-12-02T20:36:13Z,,,
arXIv2023,From Voices to Validity: Leveraging Large Language Models (LLMs) for Textual Analysis of Policy Stakeholder Interviews,No.,1,"""No evidence""",2023,2023-12-02T18:55:14Z,,,
arXIv2023,Bootstrapping Interactive Image-Text Alignment for Remote Sensing Image Captioning,No.,1,"""No evidence""",2023,2023-12-02T17:32:17Z,,,
arXIv2023,A ripple in time: a discontinuity in American history,No.,1,"""No evidence""",2023,2023-12-02T17:24:17Z,,,
arXIv2023,Kattis vs. ChatGPT: Assessment and Evaluation of Programming Tasks in the Age of Artificial Intelligence,No.,1,"""No evidence""",2023,2023-12-02T11:09:17Z,,,
arXIv2023,Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large Language Model,No.,1,"""No evidence""",2023,2023-12-02T09:45:45Z,,,
arXIv2023,Exploring and Improving the Spatial Reasoning Abilities of Large Language Models,No.,1,"""No evidence""",2023,2023-12-02T07:41:46Z,,,
arXIv2023,Prompt Tuning for Zero-shot Compositional Learning,No.,1,"""No evidence""",2023,2023-12-02T07:32:24Z,,,
arXIv2023,From Beginner to Expert: Modeling Medical Knowledge into General LLMs,No.,1,"""No evidence""",2023,2023-12-02T05:54:06Z,,,
arXIv2023,Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models,No.,1,"""No evidence""",2023,2023-12-02T05:13:28Z,,,
arXIv2023,Knowledge Graph Enhanced Aspect-Level Sentiment Analysis,No.,1,"""No evidence""",2023,2023-12-02T04:45:17Z,,,
arXIv2023,"Advanced Large Language Model (LLM)-Driven Verilog Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis",No.,1,"""No evidence""",2023,2023-12-02T04:14:23Z,,,
arXIv2023,The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models,No.,1,"""No evidence""",2023,2023-12-01T22:27:12Z,,,
arXIv2023,Hyperparameter Optimization for Large Language Model Instruction-Tuning,No.,1,"""No evidence""",2023,2023-12-01T22:03:12Z,,,
arXIv2023,Zero-Shot Video Question Answering with Procedural Programs,No.,1,"""No evidence""",2023,2023-12-01T21:34:10Z,,,
arXIv2023,Analyzing the Impact of Fake News on the Anticipated Outcome of the 2024 Election Ahead of Time,No.,1,"""No evidence""",2023,2023-12-01T20:14:16Z,,,
arXIv2023,LLM-TAKE: Theme Aware Keyword Extraction Using Large Language Models,No.,1,"""No evidence""",2023,2023-12-01T20:13:08Z,,,
arXIv2023,Context Retrieval via Normalized Contextual Latent Interaction for Conversational Agent,No.,1,"""No evidence""",2023,2023-12-01T18:53:51Z,,,
arXIv2023,Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses,No.,1,"""No evidence""",2023,2023-12-01T18:31:28Z,,,
arXIv2023,Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games,No.,1,"""No evidence""",2023,2023-12-01T17:33:57Z,,,
arXIv2023,SeaLLMs -- Large Language Models for Southeast Asia,No.,1,"""No evidence""",2023,2023-12-01T17:17:56Z,,,
arXIv2023,Rethinking Detection Based Table Structure Recognition for Visually Rich Document Images,No.,1,"""No evidence""",2023,2023-12-01T16:31:17Z,,,
arXIv2023,Open-vocabulary object 6D pose estimation,No.,1,"""No evidence""",2023,2023-12-01T16:17:16Z,,,
arXIv2023,The Efficiency Spectrum of Large Language Models: An Algorithmic Survey,No.,1,"""No evidence""",2023,2023-12-01T16:00:25Z,,,
arXIv2023,Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation,No.,1,"""No evidence""",2023,2023-12-01T15:16:00Z,,,
arXIv2023,Pathway to a fully data-driven geotechnics: lessons from materials informatics,No.,1,"""No evidence""",2023,2023-12-01T13:45:42Z,,,
arXIv2023,Instruction-tuning Aligns LLMs to the Human Brain,No.,1,"""No evidence""",2023,2023-12-01T13:31:02Z,,,
arXIv2023,Explanatory Argument Extraction of Correct Answers in Resident Medical Exams,No.,1,"""No evidence""",2023,2023-12-01T13:22:35Z,,,
arXIv2023,RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback,No.,1,"""No evidence""",2023,2023-12-01T11:36:08Z,,,
arXIv2023,A Bayesian approach for prompt optimization in pre-trained language models,No.,1,"""No evidence""",2023,2023-12-01T10:10:18Z,,,
arXIv2023,Dolphins: Multimodal Language Model for Driving,No.,1,"""No evidence""",2023,2023-12-01T09:10:33Z,,,
arXIv2023,Towards a Psychological Generalist AI: A Survey of Current Applications of Large Language Models and Future Prospects,No.,1,"""No evidence""",2023,2023-12-01T08:35:18Z,,,
arXIv2023,CoLLiE: Collaborative Training of Large Language Models in an Efficient Way,No.,1,"""No evidence""",2023,2023-12-01T08:02:16Z,,,
arXIv2023,VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things,No.,1,"""No evidence""",2023,2023-12-01T07:50:53Z,,,
arXIv2023,Event-driven Real-time Retrieval in Web Search,No.,1,"""No evidence""",2023,2023-12-01T06:30:31Z,,,
arXIv2023,On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs,No.,1,"""No evidence""",2023,2023-12-01T05:08:47Z,,,
arXIv2023,Manipulating the Label Space for In-Context Classification,No.,1,"""No evidence""",2023,2023-12-01T04:57:20Z,,,
arXIv2023,Exploring the Robustness of Decentralized Training for Large Language Models,No.,1,"""No evidence""",2023,2023-12-01T04:04:03Z,,,
arXIv2023,ESM-NBR: fast and accurate nucleic acid-binding residue prediction via protein language model feature representation and multi-task learning,No.,1,"""No evidence""",2023,2023-12-01T04:00:20Z,,,
arXIv2023,Agent-OM: Leveraging LLM Agents for Ontology Matching,No.,1,"""No evidence""",2023,2023-12-01T03:44:54Z,,,
arXIv2023,Conceptual Engineering Using Large Language Models,No.,1,"""No evidence""",2023,2023-12-01T01:58:16Z,,,
arXIv2023,Mark My Words: Analyzing and Evaluating Language Model Watermarks,No.,1,"""No evidence""",2023,2023-12-01T01:22:46Z,,,
arXIv2023,Applying Large Language Models and Chain-of-Thought for Automatic Scoring,No.,1,"""No evidence""",2023,2023-11-30T21:22:43Z,,,
arXIv2023,Robust Concept Erasure via Kernelized Rate-Distortion Maximization,No.,1,"""No evidence""",2023,2023-11-30T21:10:44Z,,,
arXIv2023,GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs,No.,1,"""No evidence""",2023,2023-11-30T18:59:58Z,,,
arXIv2023,InstructSeq: Unifying Vision Tasks with Instruction-conditioned Multi-modal Sequence Generation,No.,1,"""No evidence""",2023,2023-11-30T18:59:51Z,,,
arXIv2023,"A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval",No.,1,"""No evidence""",2023,2023-11-30T18:59:45Z,,,
arXIv2023,LucidDreaming: Controllable Object-Centric 3D Generation,No.,1,"""No evidence""",2023,2023-11-30T18:55:23Z,,,
arXIv2023,What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations,No.,1,"""No evidence""",2023,2023-11-30T18:53:13Z,,,
arXIv2023,Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text,No.,1,"""No evidence""",2023,2023-11-30T18:51:38Z,,,
arXIv2023,X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning,No.,1,"""No evidence""",2023,2023-11-30T18:43:51Z,,,
arXIv2023,Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language Models for Violence Inciting Text Detection,No.,1,"""No evidence""",2023,2023-11-30T18:23:38Z,,,
arXIv2023,"CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation",No.,1,"""No evidence""",2023,2023-11-30T18:21:25Z,,,
arXIv2023,MLLMs-Augmented Visual-Language Representation Learning,No.,1,"""No evidence""",2023,2023-11-30T18:05:52Z,,,
arXIv2023,TaskBench: Benchmarking Large Language Models for Task Automation,No.,1,"""No evidence""",2023,2023-11-30T18:02:44Z,,,
arXIv2023,Merlin:Empowering Multimodal LLMs with Foresight Minds,No.,1,"""No evidence""",2023,2023-11-30T17:57:34Z,,,
arXIv2023,AlignBench: Benchmarking Chinese Alignment of Large Language Models,No.,1,"""No evidence""",2023,2023-11-30T17:41:30Z,,,
arXIv2023,Controlgym: Large-Scale Safety-Critical Control Environments for Benchmarking Reinforcement Learning Algorithms,No.,1,"""No evidence""",2023,2023-11-30T17:34:05Z,,,
arXIv2023,CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation,No.,1,"""No evidence""",2023,2023-11-30T16:52:42Z,,,
arXIv2023,Evaluating Large Language Model Creativity from a Literary Perspective,No.,1,"""No evidence""",2023,2023-11-30T16:46:25Z,,,
arXIv2023,RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance,No.,1,"""No evidence""",2023,2023-11-30T16:28:40Z,,,
arXIv2023,ArcMMLU: A Library and Information Science Benchmark for Large Language Models,No.,1,"""No evidence""",2023,2023-11-30T16:08:04Z,,,
arXIv2023,Detailed Human-Centric Text Description-Driven Large Scene Synthesis,No.,1,"""No evidence""",2023,2023-11-30T16:04:30Z,,,
arXIv2023,"LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",No.,1,"""No evidence""",2023,2023-11-30T16:00:23Z,,,
arXIv2023,Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models,No.,1,"""No evidence""",2023,2023-11-30T14:35:51Z,,,
arXIv2023,Overcoming Label Noise for Source-free Unsupervised Video Domain Adaptation,No.,1,"""No evidence""",2023,2023-11-30T14:06:27Z,,,
arXIv2023,Search Still Matters: Information Retrieval in the Era of Generative AI,No.,1,"""No evidence""",2023,2023-11-30T13:36:21Z,,,
arXIv2023,ESG Accountability Made Easy: DocQA at Your Service,No.,1,"""No evidence""",2023,2023-11-30T11:47:50Z,,,
arXIv2023,VTimeLLM: Empower LLM to Grasp Video Moments,No.,1,"""No evidence""",2023,2023-11-30T10:49:56Z,,,
arXIv2023,IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions,No.,1,"""No evidence""",2023,2023-11-30T09:48:51Z,,,
arXIv2023,Transfer Learning across Different Chemical Domains: Virtual Screening of Organic Materials with Deep Learning Models Pretrained on Small Molecule and Chemical Reaction Data,No.,1,"""No evidence""",2023,2023-11-30T09:20:24Z,,,
arXIv2023,Hubness Reduction Improves Sentence-BERT Semantic Spaces,No.,1,"""No evidence""",2023,2023-11-30T09:03:49Z,,,
arXIv2023,Generative Artificial Intelligence in Learning Analytics: Contextualising Opportunities and Challenges through the Learning Analytics Cycle,No.,1,"""No evidence""",2023,2023-11-30T07:25:34Z,,,
arXIv2023,Categorical Traffic Transformer: Interpretable and Diverse Behavior Prediction with Tokenized Latent,No.,1,"""No evidence""",2023,2023-11-30T07:25:24Z,,,
arXIv2023,OmniMotionGPT: Animal Motion Generation with Limited Data,No.,1,"""No evidence""",2023,2023-11-30T07:14:00Z,,,
arXIv2023,TLDR: Text Based Last-layer Retraining for Debiasing Image Classifiers,No.,1,"""No evidence""",2023,2023-11-30T06:51:48Z,,,
arXIv2023,LLVMs4Protest: Harnessing the Power of Large Language and Vision Models for Deciphering Protests in the News,No.,1,"""No evidence""",2023,2023-11-30T04:17:30Z,,,
arXIv2023,LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models,No.,1,"""No evidence""",2023,2023-11-30T03:59:31Z,,,
arXIv2023,TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model,No.,1,"""No evidence""",2023,2023-11-30T03:59:23Z,,,
arXIv2023,COVID-19 Vaccine Misinformation in Middle Income Countries,No.,1,"""No evidence""",2023,2023-11-30T02:27:34Z,,,
arXIv2023,DisCGen: A Framework for Discourse-Informed Counterspeech Generation,No.,1,"""No evidence""",2023,2023-11-29T23:20:17Z,,,
arXIv2023,ROBBIE: Robust Bias Evaluation of Large Generative Language Models,No.,1,"""No evidence""",2023,2023-11-29T23:03:04Z,,,
arXIv2023,The perpetual motion machine of AI-generated data and the distraction of ChatGPT-as-scientist,No.,1,"""No evidence""",2023,2023-11-29T21:52:34Z,,,
arXIv2023,GELDA: A generative language annotation framework to reveal visual biases in datasets,No.,1,"""No evidence""",2023,2023-11-29T20:27:58Z,,,
arXIv2023,TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis,No.,1,"""No evidence""",2023,2023-11-29T20:22:44Z,,,
arXIv2023,Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation,No.,1,"""No evidence""",2023,2023-11-29T20:16:23Z,,,
arXIv2023,I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text,No.,1,"""No evidence""",2023,2023-11-29T20:04:57Z,,,
arXIv2023,Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings,No.,1,"""No evidence""",2023,2023-11-29T19:20:14Z,,,
arXIv2023,Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting,No.,1,"""No evidence""",2023,2023-11-29T19:09:28Z,,,
arXIv2023,Understanding and Improving In-Context Learning on Vision-language Models,No.,1,"""No evidence""",2023,2023-11-29T19:08:11Z,,,
arXIv2023,MoMask: Generative Masked Modeling of 3D Human Motions,No.,1,"""No evidence""",2023,2023-11-29T19:04:10Z,,,
arXIv2023,Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis,No.,1,"""No evidence""",2023,2023-11-29T18:51:46Z,,,
arXIv2023,"Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects",No.,1,"""No evidence""",2023,2023-11-29T17:54:22Z,,,
arXIv2023,Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning,No.,1,"""No evidence""",2023,2023-11-29T17:46:25Z,,,
arXIv2023,End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data,No.,1,"""No evidence""",2023,2023-11-29T15:44:39Z,,,
arXIv2023,How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation,No.,1,"""No evidence""",2023,2023-11-29T15:02:46Z,,,
arXIv2023,AviationGPT: A Large Language Model for the Aviation Domain,No.,1,"""No evidence""",2023,2023-11-29T14:49:31Z,,,
arXIv2023,TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models,No.,1,"""No evidence""",2023,2023-11-29T14:30:16Z,,,
arXIv2023,VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following,No.,1,"""No evidence""",2023,2023-11-29T14:08:53Z,,,
arXIv2023,ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model,No.,1,"""No evidence""",2023,2023-11-29T13:26:29Z,,,
arXIv2023,LanGWM: Language Grounded World Model,No.,1,"""No evidence""",2023,2023-11-29T12:41:55Z,,,
arXIv2023,ChatIllusion: Efficient-Aligning Interleaved Generation ability with Visual Instruction Model,No.,1,"""No evidence""",2023,2023-11-29T11:30:33Z,,,
arXIv2023,Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation,No.,1,"""No evidence""",2023,2023-11-29T11:10:40Z,,,
arXIv2023,Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models,No.,1,"""No evidence""",2023,2023-11-29T10:24:50Z,,,
arXIv2023,Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model,No.,1,"""No evidence""",2023,2023-11-29T09:48:34Z,,,
arXIv2023,"AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents",No.,1,"""No evidence""",2023,2023-11-29T09:13:00Z,,,
arXIv2023,CLOMO: Counterfactual Logical Modification with Large Language Models,No.,1,"""No evidence""",2023,2023-11-29T08:29:54Z,,,
arXIv2023,MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning,No.,1,"""No evidence""",2023,2023-11-29T08:27:00Z,,,
arXIv2023,Grounding Foundation Models through Federated Transfer Learning: A General Framework,No.,1,"""No evidence""",2023,2023-11-29T08:21:42Z,,,
arXIv2023,TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4,No.,1,"""No evidence""",2023,2023-11-29T08:12:09Z,,,
arXIv2023,How Generative-AI can be Effectively used in Government Chatbots,No.,1,"""No evidence""",2023,2023-11-29T07:27:15Z,,,
arXIv2023,VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models,No.,1,"""No evidence""",2023,2023-11-29T07:15:34Z,,,
arXIv2023,CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs,No.,1,"""No evidence""",2023,2023-11-29T06:02:16Z,,,
arXIv2023,Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs,No.,1,"""No evidence""",2023,2023-11-29T05:54:41Z,,,
arXIv2023,Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning,No.,1,"""No evidence""",2023,2023-11-29T05:27:14Z,,,
arXIv2023,A natural language processing-based approach: mapping human perception by understanding deep semantic features in street view images,No.,1,"""No evidence""",2023,2023-11-29T05:00:43Z,,,
arXIv2023,Efficient Stitchable Task Adaptation,No.,1,"""No evidence""",2023,2023-11-29T04:31:35Z,,,
arXIv2023,LEAP: LLM-Generation of Egocentric Action Programs,No.,1,"""No evidence""",2023,2023-11-29T04:25:52Z,,,
arXIv2023,Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering,No.,1,"""No evidence""",2023,2023-11-29T03:10:42Z,,,
arXIv2023,Biomedical knowledge graph-enhanced prompt generation for large language models,No.,1,"""No evidence""",2023,2023-11-29T03:07:00Z,,,
arXIv2023,LALM: Long-Term Action Anticipation with Language Models,No.,1,"""No evidence""",2023,2023-11-29T02:17:27Z,,,
arXIv2023,Universal Self-Consistency for Large Language Model Generation,No.,1,"""No evidence""",2023,2023-11-29T02:07:09Z,,,
arXIv2023,Language Models: A Guide for the Perplexed,No.,1,"""No evidence""",2023,2023-11-29T01:19:02Z,,,
arXIv2023,Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?,No.,1,"""No evidence""",2023,2023-11-28T23:40:13Z,,,
arXIv2023,E-ViLM: Efficient Video-Language Model via Masked Video Modeling with Semantic Vector-Quantized Tokenizer,No.,1,"""No evidence""",2023,2023-11-28T22:57:17Z,,,
arXIv2023,Quantifying the redundancy between prosody and text,No.,1,"""No evidence""",2023,2023-11-28T21:15:24Z,,,
arXIv2023,General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Structured Data from Chest Radiology Reports,No.,1,"""No evidence""",2023,2023-11-28T20:34:40Z,,,
arXIv2023,Unlocking Spatial Comprehension in Text-to-Image Diffusion Models,No.,1,"""No evidence""",2023,2023-11-28T19:00:02Z,,,
arXIv2023,LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models,No.,1,"""No evidence""",2023,2023-11-28T18:53:43Z,,,
arXIv2023,Efficient In-Context Learning in Vision-Language Models for Egocentric Videos,No.,1,"""No evidence""",2023,2023-11-28T18:53:06Z,,,
arXIv2023,Comparing Generative Chatbots Based on Process Requirements,No.,1,"""No evidence""",2023,2023-11-28T18:25:22Z,,,
arXIv2023,MVBench: A Comprehensive Multi-modal Video Understanding Benchmark,No.,1,"""No evidence""",2023,2023-11-28T17:59:04Z,,,
arXIv2023,Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following,No.,1,"""No evidence""",2023,2023-11-28T17:57:44Z,,,
arXIv2023,Prompting in Autoregressive Large Language Models,No.,1,"""No evidence""",2023,2023-11-28T17:56:34Z,,,
arXIv2023,Training Chain-of-Thought via Latent-Variable Inference,No.,1,"""No evidence""",2023,2023-11-28T17:47:32Z,,,
arXIv2023,ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?,No.,1,"""No evidence""",2023,2023-11-28T17:44:51Z,,,
arXIv2023,COLE: A Hierarchical Generation Framework for Multi-Layered and Editable Graphic Design,No.,1,"""No evidence""",2023,2023-11-28T17:22:17Z,,,
arXIv2023,Natural Language Processing Through Transfer Learning: A Case Study on Sentiment Analysis,No.,1,"""No evidence""",2023,2023-11-28T17:12:06Z,,,
arXIv2023,LLaFS: When Large Language Models Meet Few-Shot Segmentation,No.,1,"""No evidence""",2023,2023-11-28T16:31:27Z,,,
arXIv2023,Syntax-Informed Interactive Model for Comprehensive Aspect-Based Sentiment Analysis,No.,1,"""No evidence""",2023,2023-11-28T16:03:22Z,,,
arXIv2023,The Falcon Series of Open Language Models,No.,1,"""No evidence""",2023,2023-11-28T15:12:47Z,,,
arXIv2023,RELIC: Investigating Large Language Model Responses using Self-Consistency,No.,1,"""No evidence""",2023,2023-11-28T14:55:52Z,,,
arXIv2023,CharacterGLM: Customizing Chinese Conversational AI Characters with Large Language Models,No.,1,"""No evidence""",2023,2023-11-28T14:49:23Z,,,
arXIv2023,Large Model Based Referring Camouflaged Object Detection,No.,1,"""No evidence""",2023,2023-11-28T13:45:09Z,,,
arXIv2023,LLMs for Science: Usage for Code Generation and Data Analysis,No.,1,"""No evidence""",2023,2023-11-28T12:29:33Z,,,
arXIv2023,CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs,No.,1,"""No evidence""",2023,2023-11-28T11:27:48Z,,,
arXIv2023,Large Language Models Meet Computer Vision: A Brief Survey,No.,1,"""No evidence""",2023,2023-11-28T10:39:19Z,,,
arXIv2023,Scaling Political Texts with ChatGPT,No.,1,"""No evidence""",2023,2023-11-28T09:45:02Z,,,
arXIv2023,Agents meet OKR: An Object and Key Results Driven Agent System with Hierarchical Self-Collaboration and Self-Evaluation,No.,1,"""No evidence""",2023,2023-11-28T06:16:30Z,,,
arXIv2023,StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models,No.,1,"""No evidence""",2023,2023-11-28T04:49:17Z,,,
arXIv2023,"AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond",No.,1,"""No evidence""",2023,2023-11-28T04:10:07Z,,,
arXIv2023,Large language models can enhance persuasion through linguistic feature alignment,No.,1,"""No evidence""",2023,2023-11-28T04:07:34Z,,,
arXIv2023,TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering,No.,1,"""No evidence""",2023,2023-11-28T04:02:40Z,,,
arXIv2023,Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine,No.,1,"""No evidence""",2023,2023-11-28T03:16:12Z,,,
arXIv2023,Empowering Autonomous Driving with Large Language Models: A Safety Perspective,No.,1,"""No evidence""",2023,2023-11-28T03:13:09Z,,,
arXIv2023,CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models,No.,1,"""No evidence""",2023,2023-11-28T02:01:25Z,,,
arXIv2023,Compositional Chain-of-Thought Prompting for Large Multimodal Models,No.,1,"""No evidence""",2023,2023-11-27T22:23:27Z,,,
arXIv2023,Releasing the CRaQAn (Coreference Resolution in Question-Answering): An open-source dataset and dataset creation methodology using instruction-following models,No.,1,"""No evidence""",2023,2023-11-27T21:54:50Z,,,
arXIv2023,Student Mastery or AI Deception? Analyzing ChatGPT's Assessment Proficiency and Evaluating Detection Strategies,No.,1,"""No evidence""",2023,2023-11-27T20:10:13Z,,,
arXIv2023,Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model,No.,1,"""No evidence""",2023,2023-11-27T19:17:39Z,,,
arXIv2023,IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers,No.,1,"""No evidence""",2023,2023-11-27T19:00:06Z,,,
arXIv2023,Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models,No.,1,"""No evidence""",2023,2023-11-27T18:59:58Z,,,
arXIv2023,How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs,No.,1,"""No evidence""",2023,2023-11-27T18:59:42Z,,,
arXIv2023,Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?,No.,1,"""No evidence""",2023,2023-11-27T18:59:14Z,,,
arXIv2023,BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights,No.,1,"""No evidence""",2023,2023-11-27T18:46:17Z,,,
arXIv2023,OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving,No.,1,"""No evidence""",2023,2023-11-27T17:59:41Z,,,
arXIv2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,No.,1,"""No evidence""",2023,2023-11-27T17:33:21Z,,,
arXIv2023,VLPrompt: Vision-Language Prompting for Panoptic Scene Graph Generation,No.,1,"""No evidence""",2023,2023-11-27T17:05:25Z,,,
arXIv2023,InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery,No.,1,"""No evidence""",2023,2023-11-27T16:47:51Z,,,
arXIv2023,Sparsify-then-Classify: From Internal Neurons of Large Language Models To Efficient Text Classifiers,No.,1,"""No evidence""",2023,2023-11-27T16:28:20Z,,,
arXIv2023,Leveraging deep active learning to identify low-resource mobility functioning information in public clinical notes,No.,1,"""No evidence""",2023,2023-11-27T15:53:11Z,,,
arXIv2023,Real Customization or Just Marketing: Are Customized Versions of Chat GPT Useful?,No.,1,"""No evidence""",2023,2023-11-27T15:46:15Z,,,
arXIv2023,Towards Responsible Governance of Biological Design Tools,No.,1,"""No evidence""",2023,2023-11-27T15:45:02Z,,,
arXIv2023,ChartLlama: A Multimodal LLM for Chart Understanding and Generation,No.,1,"""No evidence""",2023,2023-11-27T15:20:23Z,,,
arXIv2023,EVCap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for Open-World Comprehension,No.,1,"""No evidence""",2023,2023-11-27T14:51:37Z,,,
arXIv2023,InterControl: Generate Human Motion Interactions by Controlling Every Joint,No.,1,"""No evidence""",2023,2023-11-27T14:32:33Z,,,
arXIv2023,FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax,No.,1,"""No evidence""",2023,2023-11-27T13:39:44Z,,,
arXIv2023,LLMGA: Multimodal Large Language Model based Generation Assistant,No.,1,"""No evidence""",2023,2023-11-27T13:37:26Z,,,
arXIv2023,C-SAW: Self-Supervised Prompt Learning for Image Generalization in Remote Sensing,No.,1,"""No evidence""",2023,2023-11-27T13:35:20Z,,,
arXIv2023,A Social-aware Gaussian Pre-trained Model for Effective Cold-start Recommendation,No.,1,"""No evidence""",2023,2023-11-27T13:04:33Z,,,
arXIv2023,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention,No.,1,"""No evidence""",2023,2023-11-27T13:01:59Z,,,
arXIv2023,Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs,No.,1,"""No evidence""",2023,2023-11-27T12:54:47Z,,,
arXIv2023,Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs,No.,1,"""No evidence""",2023,2023-11-27T12:29:20Z,,,
arXIv2023,GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?,No.,1,"""No evidence""",2023,2023-11-27T11:29:10Z,,,
arXIv2023,Italian Crossword Generator: Enhancing Education through Interactive Word Puzzles,No.,1,"""No evidence""",2023,2023-11-27T11:17:29Z,,,
arXIv2023,Cerbero-7B: A Leap Forward in Language-Specific LLMs Through Enhanced Chat Corpus Generation and Evaluation,No.,1,"""No evidence""",2023,2023-11-27T10:34:55Z,,,
arXIv2023,ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models,No.,1,"""No evidence""",2023,2023-11-27T10:34:44Z,,,
arXIv2023,MoDS: Model-oriented Data Selection for Instruction Tuning,No.,1,"""No evidence""",2023,2023-11-27T09:33:13Z,,,
arXIv2023,Tokenized Model: A Blockchain-Empowered Decentralized Model Ownership Verification Platform,No.,1,"""No evidence""",2023,2023-11-27T09:02:57Z,,,
arXIv2023,chatGPT for generating questions and assessments based on accreditations,No.,1,"""No evidence""",2023,2023-11-27T08:58:44Z,,,
arXIv2023,DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection,No.,1,"""No evidence""",2023,2023-11-27T08:49:26Z,,,
arXIv2023,Injecting linguistic knowledge into BERT for Dialogue State Tracking,No.,1,"""No evidence""",2023,2023-11-27T08:38:42Z,,,
arXIv2023,FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models,No.,1,"""No evidence""",2023,2023-11-27T08:23:08Z,,,
arXIv2023,EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models,No.,1,"""No evidence""",2023,2023-11-27T07:44:25Z,,,
arXIv2023,Instruct2Attack: Language-Guided Semantic Adversarial Attacks,No.,1,"""No evidence""",2023,2023-11-27T05:35:49Z,,,
arXIv2023,The effect of source disclosure on evaluation of AI-generated messages: A two-part study,No.,1,"""No evidence""",2023,2023-11-27T05:20:47Z,,,
arXIv2023,EAFP-Med: An Efficient Adaptive Feature Processing Module Based on Prompts for Medical Image Detection,No.,1,"""No evidence""",2023,2023-11-27T05:10:15Z,,,
arXIv2023,SSIN: Self-Supervised Learning for Rainfall Spatial Interpolation,No.,1,"""No evidence""",2023,2023-11-27T04:23:47Z,,,
arXIv2023,Optimizing and Fine-tuning Large Language Model for Urban Renewal,No.,1,"""No evidence""",2023,2023-11-27T02:17:11Z,,,
arXIv2023,MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers,No.,1,"""No evidence""",2023,2023-11-27T01:20:11Z,,,
arXIv2023,ChatGPT Application In Summarizing An Evolution Of Deep Learning Techniques In Imaging: A Qualitative Study,No.,1,"""No evidence""",2023,2023-11-26T23:22:37Z,,,
arXIv2023,Leveraging AI-derived Data for Carbon Accounting: Information Extraction from Alternative Sources,No.,1,"""No evidence""",2023,2023-11-26T22:49:41Z,,,
arXIv2023,Uncertainty-aware Language Modeling for Selective Question Answering,No.,1,"""No evidence""",2023,2023-11-26T22:47:54Z,,,
arXIv2023,Learning to Skip for Language Modeling,No.,1,"""No evidence""",2023,2023-11-26T21:45:53Z,,,
arXIv2023,Local Convergence of Approximate Newton Method for Two Layer Nonlinear Regression,No.,1,"""No evidence""",2023,2023-11-26T19:19:02Z,,,
arXIv2023,Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding,No.,1,"""No evidence""",2023,2023-11-26T19:01:14Z,,,
arXIv2023,Enhancing Empathetic and Emotion Support Dialogue Generation with Prophetic Commonsense Inference,No.,1,"""No evidence""",2023,2023-11-26T14:35:23Z,,,
arXIv2023,AV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset,No.,1,"""No evidence""",2023,2023-11-26T14:17:51Z,,,
arXIv2023,Algorithm Evolution Using Large Language Model,No.,1,"""No evidence""",2023,2023-11-26T09:38:44Z,,,
arXIv2023,See and Think: Embodied Agent in Virtual Environment,No.,1,"""No evidence""",2023,2023-11-26T06:38:16Z,,,
arXIv2023,ChatGPT and Beyond: The Generative AI Revolution in Education,No.,1,"""No evidence""",2023,2023-11-26T05:34:22Z,,,
arXIv2023,"Comparative Analysis of ChatGPT, GPT-4, and Microsoft Bing Chatbots for GRE Test",No.,1,"""No evidence""",2023,2023-11-26T05:27:35Z,,,
arXIv2023,SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling,No.,1,"""No evidence""",2023,2023-11-25T22:51:01Z,,,
arXIv2023,"Localizing Lying in Llama: Understanding Instructed Dishonesty on True-False Questions Through Prompting, Probing, and Patching",No.,1,"""No evidence""",2023,2023-11-25T22:41:23Z,,,
arXIv2023,Relevance feedback strategies for recall-oriented neural information retrieval,No.,1,"""No evidence""",2023,2023-11-25T19:50:41Z,,,
arXIv2023,Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion,No.,1,"""No evidence""",2023,2023-11-25T19:35:53Z,,,
arXIv2023,Enhancing Sentiment Analysis Results through Outlier Detection Optimization,No.,1,"""No evidence""",2023,2023-11-25T18:20:43Z,,,
arXIv2023,Multilingual self-supervised speech representations improve the speech recognition of low-resource African languages with codeswitching,No.,1,"""No evidence""",2023,2023-11-25T17:05:21Z,,,
arXIv2023,GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation,No.,1,"""No evidence""",2023,2023-11-25T04:05:59Z,,,
arXIv2023,GBD-TS: Goal-based Pedestrian Trajectory Prediction with Diffusion using Tree Sampling Algorithm,No.,1,"""No evidence""",2023,2023-11-25T03:55:06Z,,,
arXIv2023,LLM-Assisted Code Cleaning For Training Accurate Code Generators,No.,1,"""No evidence""",2023,2023-11-25T02:45:50Z,,,
arXIv2023,Tracing Influence at Scale: A Contrastive Learning Approach to Linking Public Comments and Regulator Responses,No.,1,"""No evidence""",2023,2023-11-24T23:32:13Z,,,
arXIv2023,"OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models",No.,1,"""No evidence""",2023,2023-11-24T20:24:00Z,,,
arXIv2023,Data-to-Text Bilingual Generation,No.,1,"""No evidence""",2023,2023-11-24T19:05:57Z,,,
arXIv2023,GeoChat: Grounded Large Vision-Language Model for Remote Sensing,No.,1,"""No evidence""",2023,2023-11-24T18:59:10Z,,,
arXIv2023,Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs,No.,1,"""No evidence""",2023,2023-11-24T18:46:02Z,,,
arXIv2023,Evaluating Large Language Models through Gender and Racial Stereotypes,No.,1,"""No evidence""",2023,2023-11-24T18:41:16Z,,,
arXIv2023,GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction,No.,1,"""No evidence""",2023,2023-11-24T18:02:49Z,,,
arXIv2023,Who is leading in AI? An analysis of industry AI research,No.,1,"""No evidence""",2023,2023-11-24T17:36:09Z,,,
arXIv2023,GPT Struct Me: Probing GPT Models on Narrative Entity Extraction,No.,1,"""No evidence""",2023,2023-11-24T16:19:04Z,,,
arXIv2023,From Text to Image: Exploring GPT-4Vision's Potential in Advanced Radiological Analysis across Subspecialties,No.,1,"""No evidence""",2023,2023-11-24T15:39:29Z,,,
arXIv2023,Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models,No.,1,"""No evidence""",2023,2023-11-24T15:35:07Z,,,
arXIv2023,CMed-GPT: Prompt Tuning for Entity-Aware Chinese Medical Dialogue Generation,No.,1,"""No evidence""",2023,2023-11-24T15:10:56Z,,,
arXIv2023,Automatic detection of problem-gambling signs from online texts using large language models,No.,1,"""No evidence""",2023,2023-11-24T13:48:02Z,,,
arXIv2023,Controlled Text Generation via Language Model Arithmetic,No.,1,"""No evidence""",2023,2023-11-24T13:41:12Z,,,
arXIv2023,Universal Jailbreak Backdoors from Poisoned Human Feedback,No.,1,"""No evidence""",2023,2023-11-24T13:09:34Z,,,
arXIv2023,LLamol: A Dynamic Multi-Conditional Generative Transformer for De Novo Molecular Design,No.,1,"""No evidence""",2023,2023-11-24T10:59:12Z,,,
arXIv2023,FAL CorPipe at CRAC 2023: Larger Context Improves Multilingual Coreference Resolution,No.,1,"""No evidence""",2023,2023-11-24T10:15:34Z,,,
arXIv2023,Ethical implications of ChatGPT in higher education: A scoping review,No.,1,"""No evidence""",2023,2023-11-24T09:52:49Z,,,
arXIv2023,GATGPT: A Pre-trained Large Language Model with Graph Attention Network for Spatiotemporal Imputation,No.,1,"""No evidence""",2023,2023-11-24T08:15:11Z,,,
arXIv2023,Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs,No.,1,"""No evidence""",2023,2023-11-24T07:53:48Z,,,
arXIv2023,Reinforcement Learning from Statistical Feedback: the Journey from AB Testing to ANT Testing,No.,1,"""No evidence""",2023,2023-11-24T07:50:52Z,,,
arXIv2023,Paragraph-to-Image Generation with Information-Enriched Diffusion Model,No.,1,"""No evidence""",2023,2023-11-24T05:17:01Z,,,
arXIv2023,Image Super-Resolution with Text Prompt Diffusion,No.,1,"""No evidence""",2023,2023-11-24T05:11:35Z,,,
arXIv2023,Annotation Sensitivity: Training Data Collection Methods Affect Model Performance,No.,1,"""No evidence""",2023,2023-11-23T21:54:22Z,,,
arXIv2023,Evaluating GPT-4's Vision Capabilities on Brazilian University Admission Exams,No.,1,"""No evidence""",2023,2023-11-23T19:20:59Z,,,
arXIv2023,A density estimation perspective on learning from pairwise human preferences,No.,1,"""No evidence""",2023,2023-11-23T17:20:36Z,,,
arXIv2023,"Forecasting Cryptocurrency Prices Using Deep Learning: Integrating Financial, Blockchain, and Text Data",No.,1,"""No evidence""",2023,2023-11-23T16:14:44Z,,,
arXIv2023,Hardware Resilience Properties of Text-Guided Image Classifiers,No.,1,"""No evidence""",2023,2023-11-23T15:38:13Z,,,
arXIv2023,Towards Explainable Strategy Templates using NLP Transformers,No.,1,"""No evidence""",2023,2023-11-23T15:37:19Z,,,
arXIv2023,Understanding the Vulnerability of CLIP to Image Compression,No.,1,"""No evidence""",2023,2023-11-23T14:33:53Z,,,
arXIv2023,"MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V",No.,1,"""No evidence""",2023,2023-11-23T12:04:25Z,,,
arXIv2023,A Multi-solution Study on GDPR AI-enabled Completeness Checking of DPAs,No.,1,"""No evidence""",2023,2023-11-23T10:05:52Z,,,
arXIv2023,Minimizing Factual Inconsistency and Hallucination in Large Language Models,No.,1,"""No evidence""",2023,2023-11-23T09:58:39Z,,,
arXIv2023,A Cross Attention Approach to Diagnostic Explainability using Clinical Practice Guidelines for Depression,No.,1,"""No evidence""",2023,2023-11-23T08:42:18Z,,,
arXIv2023,DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for Korean NLP,No.,1,"""No evidence""",2023,2023-11-23T03:03:54Z,,,
arXIv2023,FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design,No.,1,"""No evidence""",2023,2023-11-23T00:24:40Z,,,
arXIv2023,"Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case",No.,1,"""No evidence""",2023,2023-11-22T22:52:00Z,,,
arXIv2023,Nova$^+$: Generative Language Models for Binaries,No.,1,"""No evidence""",2023,2023-11-22T22:27:54Z,,,
arXIv2023,Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning,No.,1,"""No evidence""",2023,2023-11-22T22:27:47Z,,,
arXIv2023,MAIRA-1: A specialised large multimodal model for radiology report generation,No.,1,"""No evidence""",2023,2023-11-22T19:45:40Z,,,
arXIv2023,Language Model Inversion,No.,1,"""No evidence""",2023,2023-11-22T19:04:04Z,,,
arXIv2023,Visual In-Context Prompting,No.,1,"""No evidence""",2023,2023-11-22T18:59:48Z,,,
arXIv2023,Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models,No.,1,"""No evidence""",2023,2023-11-22T18:50:47Z,,,
arXIv2023,Physical Reasoning and Object Planning for Household Embodied Agents,No.,1,"""No evidence""",2023,2023-11-22T18:32:03Z,,,
arXIv2023,ADriver-I: A General World Model for Autonomous Driving,No.,1,"""No evidence""",2023,2023-11-22T17:44:29Z,,,
arXIv2023,Vamos: Versatile Action Models for Video Understanding,No.,1,"""No evidence""",2023,2023-11-22T17:44:24Z,,,
arXIv2023,Speak Like a Native: Prompting Large Language Models in a Native Style,No.,1,"""No evidence""",2023,2023-11-22T17:24:21Z,,,
arXIv2023,LM-Cocktail: Resilient Tuning of Language Models via Model Merging,No.,1,"""No evidence""",2023,2023-11-22T17:14:54Z,,,
arXIv2023,Current Topological and Machine Learning Applications for Bias Detection in Text,No.,1,"""No evidence""",2023,2023-11-22T16:12:42Z,,,
arXIv2023,Generation of Explanations for Logic Reasoning,No.,1,"""No evidence""",2023,2023-11-22T15:22:04Z,,,
arXIv2023,PG-Video-LLaVA: Pixel Grounding Large Video-Language Models,No.,1,"""No evidence""",2023,2023-11-22T14:48:30Z,,,
arXIv2023,Confidant: Customizing Transformer-based LLMs via Collaborative Edge Training,No.,1,"""No evidence""",2023,2023-11-22T13:20:59Z,,,
arXIv2023,Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting,No.,1,"""No evidence""",2023,2023-11-22T09:51:53Z,,,
arXIv2023,@ve: A Chatbot for Latin,No.,1,"""No evidence""",2023,2023-11-22T09:06:11Z,,,
arXIv2023,CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM Instruction Tuning,No.,1,"""No evidence""",2023,2023-11-22T09:04:57Z,,,
arXIv2023,On the Calibration of Large Language Models and Alignment,No.,1,"""No evidence""",2023,2023-11-22T08:57:55Z,,,
arXIv2023,Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model,No.,1,"""No evidence""",2023,2023-11-22T08:42:46Z,,,
arXIv2023,"Towards Detecting, Recognizing, and Parsing the Address Information from Bangla Signboard: A Deep Learning-based Approach",No.,1,"""No evidence""",2023,2023-11-22T08:25:15Z,,,
arXIv2023,Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs,No.,1,"""No evidence""",2023,2023-11-22T06:46:37Z,,,
arXIv2023,Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation,No.,1,"""No evidence""",2023,2023-11-22T06:23:18Z,,,
arXIv2023,ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization,No.,1,"""No evidence""",2023,2023-11-22T05:28:59Z,,,
arXIv2023,Multimodal Large Language Models: A Survey,No.,1,"""No evidence""",2023,2023-11-22T05:15:12Z,,,
arXIv2023,Towards Responsible Generative AI: A Reference Architecture for Designing Foundation Model based Agents,No.,1,"""No evidence""",2023,2023-11-22T04:21:47Z,,,
arXIv2023,LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms,No.,1,"""No evidence""",2023,2023-11-22T03:37:01Z,,,
arXIv2023,Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer,No.,1,"""No evidence""",2023,2023-11-22T02:46:57Z,,,
arXIv2023,White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?,No.,1,"""No evidence""",2023,2023-11-22T02:23:32Z,,,
arXIv2023,Perceptual Structure in the Absence of Grounding for LLMs: The Impact of Abstractedness and Subjectivity in Color Language,No.,1,"""No evidence""",2023,2023-11-22T02:12:36Z,,,
arXIv2023,Detecting out-of-distribution text using topological features of transformer-based language models,No.,1,"""No evidence""",2023,2023-11-22T02:04:35Z,,,
arXIv2023,From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models,No.,1,"""No evidence""",2023,2023-11-21T23:53:27Z,,,
arXIv2023,Attribution and Alignment: Effects of Local Context Repetition on Utterance Production and Comprehension in Dialogue,No.,1,"""No evidence""",2023,2023-11-21T23:50:33Z,,,
arXIv2023,Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning,No.,1,"""No evidence""",2023,2023-11-21T23:30:01Z,,,
arXIv2023,Beyond Text: Unveiling Multimodal Proficiency of Large Language Models with MultiAPI Benchmark,No.,1,"""No evidence""",2023,2023-11-21T23:26:05Z,,,
arXIv2023,Latent Lab: Large Language Models for Knowledge Exploration,No.,1,"""No evidence""",2023,2023-11-21T23:23:16Z,,,
arXIv2023,Systematic word meta-sense extension,No.,1,"""No evidence""",2023,2023-11-21T22:30:37Z,,,
arXIv2023,GeoLocator: a location-integrated large multimodal model for inferring geo-privacy,No.,1,"""No evidence""",2023,2023-11-21T21:48:51Z,,,
arXIv2023,NERIF: GPT-4V for Automatic Scoring of Drawn Models,No.,1,"""No evidence""",2023,2023-11-21T20:52:04Z,,,
arXIv2023,GAIA: a benchmark for General AI Assistants,No.,1,"""No evidence""",2023,2023-11-21T20:34:47Z,,,
arXIv2023,Generative Machine Learning for Multivariate Equity Returns,No.,1,"""No evidence""",2023,2023-11-21T18:41:48Z,,,
arXIv2023,Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching,No.,1,"""No evidence""",2023,2023-11-21T17:52:30Z,,,
arXIv2023,LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language,No.,1,"""No evidence""",2023,2023-11-21T17:21:15Z,,,
arXIv2023,Diffusion Model Alignment Using Direct Preference Optimization,No.,1,"""No evidence""",2023,2023-11-21T15:24:05Z,,,
arXIv2023,From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design,No.,1,"""No evidence""",2023,2023-11-21T15:20:48Z,,,
arXIv2023,GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning,No.,1,"""No evidence""",2023,2023-11-21T14:24:37Z,,,
arXIv2023,IMGTB: A Framework for Machine-Generated Text Detection Benchmarking,No.,1,"""No evidence""",2023,2023-11-21T12:40:01Z,,,
arXIv2023,In-Context Learning Functions with Varying Number of Minima,No.,1,"""No evidence""",2023,2023-11-21T11:33:03Z,,,
arXIv2023,Oasis: Data Curation and Assessment System for Pretraining of Large Language Models,No.,1,"""No evidence""",2023,2023-11-21T11:32:23Z,,,
arXIv2023,ALPHA: AnomaLous Physiological Health Assessment Using Large Language Models,No.,1,"""No evidence""",2023,2023-11-21T11:09:57Z,,,
arXIv2023,PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords,No.,1,"""No evidence""",2023,2023-11-21T09:37:42Z,,,
arXIv2023,Extracting Definienda in Mathematical Scholarly Articles with Transformers,No.,1,"""No evidence""",2023,2023-11-21T08:58:57Z,,,
arXIv2023,How Far Have We Gone in Vulnerability Detection Using Large Language Models,No.,1,"""No evidence""",2023,2023-11-21T08:20:39Z,,,
arXIv2023,A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs,No.,1,"""No evidence""",2023,2023-11-21T08:09:00Z,,,
arXIv2023,nach0: Multimodal Natural and Chemical Languages Foundation Model,No.,1,"""No evidence""",2023,2023-11-21T07:56:30Z,,,
arXIv2023,InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk Factors in Reddit Posts,No.,1,"""No evidence""",2023,2023-11-21T07:43:50Z,,,
arXIv2023,A Survey of Graph Meets Large Language Model: Progress and Future Directions,No.,1,"""No evidence""",2023,2023-11-21T07:22:48Z,,,
arXIv2023,Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text,No.,1,"""No evidence""",2023,2023-11-21T06:23:38Z,,,
arXIv2023,Enhancing Scene Graph Generation with Hierarchical Relationships and Commonsense Knowledge,No.,1,"""No evidence""",2023,2023-11-21T06:03:20Z,,,
arXIv2023,Utilizing Language Models for Tour Itinerary Recommendation,No.,1,"""No evidence""",2023,2023-11-21T05:15:56Z,,,
arXIv2023,A Survey on Large Language Models for Personalized and Explainable Recommendations,No.,1,"""No evidence""",2023,2023-11-21T04:14:09Z,,,
arXIv2023,ViLaM: A Vision-Language Model with Enhanced Visual Grounding and Generalization Capability,No.,1,"""No evidence""",2023,2023-11-21T03:40:09Z,,,
arXIv2023,A Survey on Multimodal Large Language Models for Autonomous Driving,No.,1,"""No evidence""",2023,2023-11-21T03:32:01Z,,,
arXIv2023,AcademicGPT: Empowering Academic Research,No.,1,"""No evidence""",2023,2023-11-21T03:17:14Z,,,
arXIv2023,ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science,No.,1,"""No evidence""",2023,2023-11-21T02:02:46Z,,,
arXIv2023,Boosting Audio-visual Zero-shot Learning with Large Language Models,No.,1,"""No evidence""",2023,2023-11-21T01:18:23Z,,,
arXIv2023,Resilient Control of Networked Microgrids using Vertical Federated Reinforcement Learning: Designs and Real-Time Test-Bed Validations,No.,1,"""No evidence""",2023,2023-11-21T00:59:27Z,,,
arXIv2023,InteraSSort: Interactive Assortment Planning Using Large Language Models,No.,1,"""No evidence""",2023,2023-11-20T23:36:41Z,,,
arXIv2023,Unifying Corroborative and Contributive Attributions in Large Language Models,No.,1,"""No evidence""",2023,2023-11-20T23:17:20Z,,,
arXIv2023,NeuroPrompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation,No.,1,"""No evidence""",2023,2023-11-20T22:57:47Z,,,
arXIv2023,ChatGPT and post-test probability,No.,1,"""No evidence""",2023,2023-11-20T21:14:04Z,,,
arXIv2023,Applications of Large Scale Foundation Models for Autonomous Driving,No.,1,"""No evidence""",2023,2023-11-20T19:45:27Z,,,
arXIv2023,MemoryCompanion: A Smart Healthcare Solution to Empower Efficient Alzheimer's Care Via Unleashing Generative AI,No.,1,"""No evidence""",2023,2023-11-20T19:41:50Z,,,
arXIv2023,Fingerspelling PoseNet: Enhancing Fingerspelling Translation with Pose-Based Transformer Models,No.,1,"""No evidence""",2023,2023-11-20T19:11:16Z,,,
arXIv2023,LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning,No.,1,"""No evidence""",2023,2023-11-20T18:57:41Z,,,
arXIv2023,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,No.,1,"""No evidence""",2023,2023-11-20T18:57:34Z,,,
arXIv2023,GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration,No.,1,"""No evidence""",2023,2023-11-20T18:54:39Z,,,
arXIv2023,VLM-Eval: A General Evaluation on Video Large Language Models,No.,1,"""No evidence""",2023,2023-11-20T16:02:10Z,,,
arXIv2023,Generating Valid and Natural Adversarial Examples with Large Language Models,No.,1,"""No evidence""",2023,2023-11-20T15:57:04Z,,,
arXIv2023,LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge,No.,1,"""No evidence""",2023,2023-11-20T15:56:44Z,,,
arXIv2023,How to Use Large Language Models for Text Coding: The Case of Fatherhood Roles in Public Policy Documents,No.,1,"""No evidence""",2023,2023-11-20T15:34:45Z,,,
arXIv2023,Zero redundancy distributed learning with differential privacy,No.,1,"""No evidence""",2023,2023-11-20T14:58:56Z,,,
arXIv2023,Large Language Models and Explainable Law: a Hybrid Methodology,No.,1,"""No evidence""",2023,2023-11-20T14:47:20Z,,,
arXIv2023,DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding,No.,1,"""No evidence""",2023,2023-11-20T14:42:25Z,,,
arXIv2023,"LogLead -- Fast and Integrated Log Loader, Enhancer, and Anomaly Detector",No.,1,"""No evidence""",2023,2023-11-20T14:42:13Z,,,
arXIv2023,Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents,No.,1,"""No evidence""",2023,2023-11-20T14:30:55Z,,,
arXIv2023,Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems,No.,1,"""No evidence""",2023,2023-11-20T14:29:45Z,,,
arXIv2023,Optimal Strategies to Perform Multilingual Analysis of Social Content for a Novel Dataset in the Tourism Domain,No.,1,"""No evidence""",2023,2023-11-20T13:08:21Z,,,
arXIv2023,Sparse Low-rank Adaptation of Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-11-20T11:56:25Z,,,
arXIv2023,Clarity ChatGPT: An Interactive and Adaptive Processing System for Image Restoration and Enhancement,No.,1,"""No evidence""",2023,2023-11-20T11:51:13Z,,,
arXIv2023,Causal Structure Learning Supervised by Large Language Model,No.,1,"""No evidence""",2023,2023-11-20T11:43:20Z,,,
arXIv2023,Web News Timeline Generation with Extended Task Prompting,No.,1,"""No evidence""",2023,2023-11-20T10:38:22Z,,,
arXIv2023,Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks,No.,1,"""No evidence""",2023,2023-11-20T08:51:30Z,,,
arXIv2023,Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions,No.,1,"""No evidence""",2023,2023-11-20T08:23:39Z,,,
arXIv2023,How well ChatGPT understand Malaysian English? An Evaluation on Named Entity Recognition and Relation Extraction,No.,1,"""No evidence""",2023,2023-11-20T07:41:30Z,,,
arXIv2023,InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models,No.,1,"""No evidence""",2023,2023-11-20T07:06:31Z,,,
arXIv2023,KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model,No.,1,"""No evidence""",2023,2023-11-20T07:02:35Z,,,
arXIv2023,Exploring Prompting Large Language Models as Explainable Metrics,No.,1,"""No evidence""",2023,2023-11-20T06:06:22Z,,,
arXIv2023,"Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT",No.,1,"""No evidence""",2023,2023-11-20T05:55:05Z,,,
arXIv2023,Assessing Prompt Injection Risks in 200+ Custom GPTs,No.,1,"""No evidence""",2023,2023-11-20T04:56:46Z,,,
arXIv2023,GPT in Data Science: A Practical Exploration of Model Selection,No.,1,"""No evidence""",2023,2023-11-20T03:42:24Z,,,
arXIv2023,MultiLoRA: Democratizing LoRA for Better Multi-Task Learning,No.,1,"""No evidence""",2023,2023-11-20T02:59:18Z,,,
arXIv2023,Meta Prompting for AI Systems,No.,1,"""No evidence""",2023,2023-11-20T01:51:13Z,,,
arXIv2023,LLM aided semi-supervision for Extractive Dialog Summarization,No.,1,"""No evidence""",2023,2023-11-19T23:59:22Z,,,
arXIv2023,SecureBERT and LLAMA 2 Empowered Control Area Network Intrusion Detection and Classification,No.,1,"""No evidence""",2023,2023-11-19T23:49:08Z,,,
arXIv2023,Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques,No.,1,"""No evidence""",2023,2023-11-19T22:29:15Z,,,
arXIv2023,Tensor-Aware Energy Accounting,No.,1,"""No evidence""",2023,2023-11-19T21:06:00Z,,,
arXIv2023,A Turing Test: Are AI Chatbots Behaviorally Similar to Humans?,No.,1,"""No evidence""",2023,2023-11-19T16:44:09Z,,,
arXIv2023,Using Causal Threads to Explain Changes in a Dynamic System,No.,1,"""No evidence""",2023,2023-11-19T14:32:06Z,,,
arXIv2023,Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens,No.,1,"""No evidence""",2023,2023-11-19T09:05:52Z,,,
arXIv2023,AutoStory: Generating Diverse Storytelling Images with Minimal Human Effort,No.,1,"""No evidence""",2023,2023-11-19T06:07:37Z,,,
arXIv2023,Perceptions and Detection of AI Use in Manuscript Preparation for Academic Journals,No.,1,"""No evidence""",2023,2023-11-19T06:04:46Z,,,
arXIv2023,Open-Vocabulary Camouflaged Object Segmentation,No.,1,"""No evidence""",2023,2023-11-19T06:00:39Z,,,
arXIv2023,AtomXR: Streamlined XR Prototyping with Natural Language and Immersive Physical Interaction,No.,1,"""No evidence""",2023,2023-11-19T05:52:25Z,,,
arXIv2023,An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback,No.,1,"""No evidence""",2023,2023-11-19T04:42:24Z,,,
arXIv2023,Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models,No.,1,"""No evidence""",2023,2023-11-19T02:34:12Z,,,
arXIv2023,Best uses of ChatGPT and Generative AI for computer science research,No.,1,"""No evidence""",2023,2023-11-18T21:57:54Z,,,
arXIv2023,Experts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q&A,No.,1,"""No evidence""",2023,2023-11-18T20:32:59Z,,,
arXIv2023,Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language,No.,1,"""No evidence""",2023,2023-11-18T18:36:16Z,,,
arXIv2023,Compositional Fusion of Signals in Data Embedding,No.,1,"""No evidence""",2023,2023-11-18T14:20:56Z,,,
arXIv2023,Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning,No.,1,"""No evidence""",2023,2023-11-18T13:53:26Z,,,
arXIv2023,SBTRec- A Transformer Framework for Personalized Tour Recommendation Problem with Sentiment Analysis,No.,1,"""No evidence""",2023,2023-11-18T13:30:01Z,,,
arXIv2023,Designing Interpretable ML System to Enhance Trust in Healthcare: A Systematic Review to Proposed Responsible Clinician-AI-Collaboration Framework,No.,1,"""No evidence""",2023,2023-11-18T12:29:18Z,,,
arXIv2023,Bit Cipher -- A Simple yet Powerful Word Representation System that Integrates Efficiently with Language Models,No.,1,"""No evidence""",2023,2023-11-18T08:47:35Z,,,
arXIv2023,Behavior Optimized Image Generation,No.,1,"""No evidence""",2023,2023-11-18T07:07:38Z,,,
arXIv2023,RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability,No.,1,"""No evidence""",2023,2023-11-18T03:05:43Z,,,
arXIv2023,Case Repositories: Towards Case-Based Reasoning for AI Alignment,No.,1,"""No evidence""",2023,2023-11-18T02:02:40Z,,,
arXIv2023,Representing visual classification as a linear combination of words,No.,1,"""No evidence""",2023,2023-11-18T02:00:20Z,,,
arXIv2023,Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism,No.,1,"""No evidence""",2023,2023-11-18T01:58:23Z,,,
arXIv2023,Flexible Model Interpretability through Natural Language Model Editing,No.,1,"""No evidence""",2023,2023-11-17T23:02:42Z,,,
arXIv2023,On Functional Activations in Deep Neural Networks,No.,1,"""No evidence""",2023,2023-11-17T22:34:47Z,,,
arXIv2023,Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization,No.,1,"""No evidence""",2023,2023-11-17T20:07:54Z,,,
arXIv2023,A Language Agent for Autonomous Driving,No.,1,"""No evidence""",2023,2023-11-17T18:59:56Z,,,
arXIv2023,Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2,No.,1,"""No evidence""",2023,2023-11-17T18:45:45Z,,,
arXIv2023,PEFT-MedAware: Large Language Model for Medical Awareness,No.,1,"""No evidence""",2023,2023-11-17T18:32:17Z,,,
arXIv2023,Use GPT-J Prompt Generation with RoBERTa for NER Models on Diagnosis Extraction of Periodontal Diagnosis from Electronic Dental Records,No.,1,"""No evidence""",2023,2023-11-17T18:14:08Z,,,
arXIv2023,Hashing it Out: Predicting Unhealthy Conversations on Twitter,No.,1,"""No evidence""",2023,2023-11-17T15:49:11Z,,,
arXIv2023,Countering Misinformation via Emotional Response Generation,No.,1,"""No evidence""",2023,2023-11-17T15:37:18Z,,,
arXIv2023,Testing Language Model Agents Safely in the Wild,No.,1,"""No evidence""",2023,2023-11-17T14:06:05Z,,,
arXIv2023,Regions are Who Walk Them: a Large Pre-trained Spatiotemporal Model Based on Human Mobility for Ubiquitous Urban Sensing,No.,1,"""No evidence""",2023,2023-11-17T11:55:11Z,,,
arXIv2023,Causal Graph in Language Model Rediscovers Cortical Hierarchy in Human Narrative Processing,No.,1,"""No evidence""",2023,2023-11-17T10:09:12Z,,,
arXIv2023,DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines,No.,1,"""No evidence""",2023,2023-11-17T09:48:45Z,,,
arXIv2023,Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads,No.,1,"""No evidence""",2023,2023-11-17T08:56:13Z,,,
arXIv2023,Exploring the Relationship between In-Context Learning and Instruction Tuning,No.,1,"""No evidence""",2023,2023-11-17T07:40:46Z,,,
arXIv2023,Video-based Sequential Bayesian Homography Estimation for Soccer Field Registration,No.,1,"""No evidence""",2023,2023-11-17T07:30:00Z,,,
arXIv2023,Complementary Advantages of ChatGPTs and Human Readers in Reasoning: Evidence from English Text Reading Comprehension,No.,1,"""No evidence""",2023,2023-11-17T06:13:02Z,,,
arXIv2023,Energy and Carbon Considerations of Fine-Tuning BERT,No.,1,"""No evidence""",2023,2023-11-17T01:27:01Z,,,
arXIv2023,Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2,No.,1,"""No evidence""",2023,2023-11-17T01:20:08Z,,,
arXIv2023,FREE: The Foundational Semantic Recognition for Modeling Environmental Ecosystems,No.,1,"""No evidence""",2023,2023-11-17T00:53:09Z,,,
arXIv2023,"Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers",No.,1,"""No evidence""",2023,2023-11-17T00:08:19Z,,,
arXIv2023,Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities,No.,1,"""No evidence""",2023,2023-11-16T22:49:27Z,,,
arXIv2023,Predictive Minds: LLMs As Atypical Active Inference Agents,No.,1,"""No evidence""",2023,2023-11-16T22:11:12Z,,,
arXIv2023,Text Sanitization Beyond Specific Domains: Zero-Shot Redaction & Substitution with Large Language Models,No.,1,"""No evidence""",2023,2023-11-16T18:42:37Z,,,
arXIv2023,Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations,No.,1,"""No evidence""",2023,2023-11-16T18:38:25Z,,,
arXIv2023,"Is ""A Helpful Assistant"" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts",No.,1,"""No evidence""",2023,2023-11-16T17:48:55Z,,,
arXIv2023,Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning,No.,1,"""No evidence""",2023,2023-11-16T17:14:26Z,,,
arXIv2023,Learning interactions to boost human creativity with bandits and GPT-4,No.,1,"""No evidence""",2023,2023-11-16T16:53:17Z,,,
arXIv2023,Generative AI for Hate Speech Detection: Evaluation and Findings,No.,1,"""No evidence""",2023,2023-11-16T16:09:43Z,,,
arXIv2023,A BERT based Ensemble Approach for Sentiment Classification of Customer Reviews and its Application to Nudge Marketing in e-Commerce,No.,1,"""No evidence""",2023,2023-11-16T14:18:24Z,,,
arXIv2023,Language Generation from Brain Recordings,No.,1,"""No evidence""",2023,2023-11-16T13:37:21Z,,,
arXIv2023,UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework,No.,1,"""No evidence""",2023,2023-11-16T13:01:25Z,,,
arXIv2023,INTERVENOR: Prompting the Coding Ability of Large Language Models with the Interactive Chain of Repair,No.,1,"""No evidence""",2023,2023-11-16T12:55:20Z,,,
arXIv2023,PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models,No.,1,"""No evidence""",2023,2023-11-16T12:43:18Z,,,
arXIv2023,"GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets",No.,1,"""No evidence""",2023,2023-11-16T12:43:02Z,,,
arXIv2023,WatME: Towards Lossless Watermarking Through Lexical Redundancy,No.,1,"""No evidence""",2023,2023-11-16T11:58:31Z,,,
arXIv2023,AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL,No.,1,"""No evidence""",2023,2023-11-16T11:55:27Z,,,
arXIv2023,Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks,No.,1,"""No evidence""",2023,2023-11-16T11:51:13Z,,,
arXIv2023,Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning,No.,1,"""No evidence""",2023,2023-11-16T11:49:29Z,,,
arXIv2023,SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models,No.,1,"""No evidence""",2023,2023-11-16T11:48:17Z,,,
arXIv2023,Performance Trade-offs of Watermarking Large Language Models,No.,1,"""No evidence""",2023,2023-11-16T11:44:58Z,,,
arXIv2023,Large Language Models for Propaganda Span Annotation,No.,1,"""No evidence""",2023,2023-11-16T11:37:54Z,,,
arXIv2023,$\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning,No.,1,"""No evidence""",2023,2023-11-16T11:25:44Z,,,
arXIv2023,KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains,No.,1,"""No evidence""",2023,2023-11-16T11:22:08Z,,,
arXIv2023,Can Language Model Moderators Improve the Health of Online Discourse?,No.,1,"""No evidence""",2023,2023-11-16T11:14:22Z,,,
arXIv2023,More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering,No.,1,"""No evidence""",2023,2023-11-16T11:02:49Z,,,
arXIv2023,"HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs",No.,1,"""No evidence""",2023,2023-11-16T10:56:24Z,,,
arXIv2023,To be or not to be? an exploration of continuously controllable prompt engineering,No.,1,"""No evidence""",2023,2023-11-16T10:55:29Z,,,
arXIv2023,LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores,No.,1,"""No evidence""",2023,2023-11-16T10:43:26Z,,,
arXIv2023,OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking,No.,1,"""No evidence""",2023,2023-11-16T10:30:55Z,,,
arXIv2023,FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children's Storybook Narratives,No.,1,"""No evidence""",2023,2023-11-16T10:30:26Z,,,
arXIv2023,P^3SUM: Preserving Author's Perspective in News Summarization with Diffusion Language Models,No.,1,"""No evidence""",2023,2023-11-16T10:14:28Z,,,
arXIv2023,CARE: Extracting Experimental Findings From Clinical Literature,No.,1,"""No evidence""",2023,2023-11-16T10:06:19Z,,,
arXIv2023,On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering,No.,1,"""No evidence""",2023,2023-11-16T09:55:07Z,,,
arXIv2023,You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments,No.,1,"""No evidence""",2023,2023-11-16T09:50:53Z,,,
arXIv2023,GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding,No.,1,"""No evidence""",2023,2023-11-16T09:35:00Z,,,
arXIv2023,Fumbling in Babel: An Investigation into ChatGPT's Language Identification Ability,No.,1,"""No evidence""",2023,2023-11-16T09:12:20Z,,,
arXIv2023,Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation,No.,1,"""No evidence""",2023,2023-11-16T08:54:52Z,,,
arXIv2023,MacGyver: Are Large Language Models Creative Problem Solvers?,No.,1,"""No evidence""",2023,2023-11-16T08:52:27Z,,,
arXIv2023,"""It's not like Jarvis, but it's pretty close!"" -- Examining ChatGPT's Usage among Undergraduate Students in Computer Science",No.,1,"""No evidence""",2023,2023-11-16T08:10:18Z,,,
arXIv2023,Event Causality Is Key to Computational Story Understanding,No.,1,"""No evidence""",2023,2023-11-16T07:59:12Z,,,
arXIv2023,Evaluating In-Context Learning of Libraries for Code Generation,No.,1,"""No evidence""",2023,2023-11-16T07:37:25Z,,,
arXIv2023,Online Continual Knowledge Learning for Language Models,No.,1,"""No evidence""",2023,2023-11-16T07:31:03Z,,,
arXIv2023,Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning,No.,1,"""No evidence""",2023,2023-11-16T07:03:54Z,,,
arXIv2023,On Retrieval Augmentation and the Limitations of Language Model Training,No.,1,"""No evidence""",2023,2023-11-16T06:59:54Z,,,
arXIv2023,Digital Socrates: Evaluating LLMs through Explanation Critiques,No.,1,"""No evidence""",2023,2023-11-16T06:51:46Z,,,
arXIv2023,Efficient End-to-End Visual Document Understanding with Rationale Distillation,No.,1,"""No evidence""",2023,2023-11-16T06:50:26Z,,,
arXIv2023,GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks,No.,1,"""No evidence""",2023,2023-11-16T06:28:05Z,,,
arXIv2023,Multi-Step Dialogue Workflow Action Prediction,No.,1,"""No evidence""",2023,2023-11-16T06:05:47Z,,,
arXIv2023,LifeTox: Unveiling Implicit Toxicity in Life Advice,No.,1,"""No evidence""",2023,2023-11-16T05:43:02Z,,,
arXIv2023,Crafting In-context Examples according to LMs' Parametric Knowledge,No.,1,"""No evidence""",2023,2023-11-16T05:30:07Z,,,
arXIv2023,Prompt Optimisation with Random Sampling,No.,1,"""No evidence""",2023,2023-11-16T05:08:33Z,,,
arXIv2023,"TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction",No.,1,"""No evidence""",2023,2023-11-16T04:43:03Z,,,
arXIv2023,Program-Aided Reasoners (better) Know What They Know,No.,1,"""No evidence""",2023,2023-11-16T04:17:49Z,,,
arXIv2023,Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition,No.,1,"""No evidence""",2023,2023-11-16T04:17:47Z,,,
arXIv2023,A Speed Odyssey for Deployable Quantization of LLMs,No.,1,"""No evidence""",2023,2023-11-16T04:11:19Z,,,
arXIv2023,Reducing Privacy Risks in Online Self-Disclosures with Language Models,No.,1,"""No evidence""",2023,2023-11-16T03:28:43Z,,,
arXIv2023,Effective Large Language Model Adaptation for Improved Grounding and Citation Generation,No.,1,"""No evidence""",2023,2023-11-16T03:22:25Z,,,
arXIv2023,HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM,No.,1,"""No evidence""",2023,2023-11-16T03:13:29Z,,,
arXIv2023,GEE! Grammar Error Explanation with Large Language Models,No.,1,"""No evidence""",2023,2023-11-16T02:45:47Z,,,
arXIv2023,Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents,No.,1,"""No evidence""",2023,2023-11-16T02:37:58Z,,,
arXIv2023,Chemist-X: Large Language Model-empowered Agent for Reaction Condition Recommendation in Chemical Synthesis,No.,1,"""No evidence""",2023,2023-11-16T01:21:33Z,,,
arXIv2023,ToolTalk: Evaluating Tool-Usage in a Conversational Setting,No.,1,"""No evidence""",2023,2023-11-15T23:50:31Z,,,
arXIv2023,User Persona Identification and New Service Adaptation Recommendation,No.,1,"""No evidence""",2023,2023-11-15T22:11:39Z,,,
arXIv2023,LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems,No.,1,"""No evidence""",2023,2023-11-15T21:35:25Z,,,
arXIv2023,zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models,No.,1,"""No evidence""",2023,2023-11-15T21:25:15Z,,,
arXIv2023,Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science,No.,1,"""No evidence""",2023,2023-11-15T20:42:11Z,,,
arXIv2023,Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models,No.,1,"""No evidence""",2023,2023-11-15T20:14:21Z,,,
arXIv2023,Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization,No.,1,"""No evidence""",2023,2023-11-15T20:04:58Z,,,
arXIv2023,LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback,No.,1,"""No evidence""",2023,2023-11-15T19:52:11Z,,,
arXIv2023,VideoCon: Robust Video-Language Alignment via Contrast Captions,No.,1,"""No evidence""",2023,2023-11-15T19:51:57Z,,,
arXIv2023,Assessing Translation capabilities of Large Language Models involving English and Indian Languages,No.,1,"""No evidence""",2023,2023-11-15T18:58:19Z,,,
arXIv2023,GRIM: GRaph-based Interactive narrative visualization for gaMes,No.,1,"""No evidence""",2023,2023-11-15T18:55:45Z,,,
arXIv2023,Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models,No.,1,"""No evidence""",2023,2023-11-15T18:54:53Z,,,
arXIv2023,Contrastive Chain-of-Thought Prompting,No.,1,"""No evidence""",2023,2023-11-15T18:54:01Z,,,
arXIv2023,TableLlama: Towards Open Large Generalist Models for Tables,No.,1,"""No evidence""",2023,2023-11-15T18:47:52Z,,,
arXIv2023,Fusion-Eval: Integrating Evaluators with LLMs,No.,1,"""No evidence""",2023,2023-11-15T18:46:56Z,,,
arXIv2023,Exponentially Faster Language Modelling,No.,1,"""No evidence""",2023,2023-11-15T18:42:50Z,,,
arXIv2023,Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models,No.,1,"""No evidence""",2023,2023-11-15T18:39:56Z,,,
arXIv2023,PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health,No.,1,"""No evidence""",2023,2023-11-15T18:32:27Z,,,
arXIv2023,PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers,No.,1,"""No evidence""",2023,2023-11-15T18:19:58Z,,,
arXIv2023,SiRA: Sparse Mixture of Low Rank Adaptation,No.,1,"""No evidence""",2023,2023-11-15T18:15:37Z,,,
arXIv2023,"Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers",No.,1,"""No evidence""",2023,2023-11-15T18:11:41Z,,,
arXIv2023,Grounding Gaps in Language Model Generations,No.,1,"""No evidence""",2023,2023-11-15T17:40:27Z,,,
arXIv2023,Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation,No.,1,"""No evidence""",2023,2023-11-15T17:27:14Z,,,
arXIv2023,Aligning Neural Machine Translation Models: Human Feedback in Training and Inference,No.,1,"""No evidence""",2023,2023-11-15T17:21:58Z,,,
arXIv2023,"""We Demand Justice!"": Towards Social Context Grounding of Political Texts",No.,1,"""No evidence""",2023,2023-11-15T16:53:35Z,,,
arXIv2023,Towards A Unified View of Answer Calibration for Multi-Step Reasoning,No.,1,"""No evidence""",2023,2023-11-15T16:47:57Z,,,
arXIv2023,Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks,No.,1,"""No evidence""",2023,2023-11-15T15:52:40Z,,,
arXIv2023,Assessing Knowledge Editing in Language Models via Relation Perspective,No.,1,"""No evidence""",2023,2023-11-15T15:44:42Z,,,
arXIv2023,Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts,No.,1,"""No evidence""",2023,2023-11-15T15:40:46Z,,,
arXIv2023,MELA: Multilingual Evaluation of Linguistic Acceptability,No.,1,"""No evidence""",2023,2023-11-15T15:25:28Z,,,
arXIv2023,Exploring the Potential of Large Language Models in Computational Argumentation,No.,1,"""No evidence""",2023,2023-11-15T15:12:15Z,,,
arXIv2023,Data Similarity is Not Enough to Explain Language Model Performance,No.,1,"""No evidence""",2023,2023-11-15T14:48:08Z,,,
arXIv2023,Speculative Contrastive Decoding,No.,1,"""No evidence""",2023,2023-11-15T14:15:30Z,,,
arXIv2023,Identifying Linear Relational Concepts in Large Language Models,No.,1,"""No evidence""",2023,2023-11-15T14:01:41Z,,,
arXIv2023,I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots,No.,1,"""No evidence""",2023,2023-11-15T13:47:00Z,,,
arXIv2023,Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models,No.,1,"""No evidence""",2023,2023-11-15T12:47:52Z,,,
arXIv2023,An Empathetic User-Centric Chatbot for Emotional Support,No.,1,"""No evidence""",2023,2023-11-15T12:19:34Z,,,
arXIv2023,HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence,No.,1,"""No evidence""",2023,2023-11-15T12:02:52Z,,,
arXIv2023,Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning,No.,1,"""No evidence""",2023,2023-11-15T11:56:56Z,,,
arXIv2023,Large Language Models are legal but they are not: Making the case for a powerful LegalLLM,No.,1,"""No evidence""",2023,2023-11-15T11:50:10Z,,,
arXIv2023,CLIMB: Curriculum Learning for Infant-inspired Model Building,No.,1,"""No evidence""",2023,2023-11-15T11:48:16Z,,,
arXIv2023,Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder,No.,1,"""No evidence""",2023,2023-11-15T10:34:14Z,,,
arXIv2023,Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values,No.,1,"""No evidence""",2023,2023-11-15T10:29:28Z,,,
arXIv2023,Enhancing Machine Translation through Advanced In-Context Learning: A Methodological Strategy for GPT-4 Improvement,No.,1,"""No evidence""",2023,2023-11-15T10:28:28Z,,,
arXIv2023,Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem under the ASPIRE Framework,No.,1,"""No evidence""",2023,2023-11-15T10:25:41Z,,,
arXIv2023,Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English,No.,1,"""No evidence""",2023,2023-11-15T10:25:14Z,,,
arXIv2023,Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration,No.,1,"""No evidence""",2023,2023-11-15T10:20:49Z,,,
arXIv2023,German FinBERT: A German Pre-trained Language Model,No.,1,"""No evidence""",2023,2023-11-15T09:07:29Z,,,
arXIv2023,X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects,No.,1,"""No evidence""",2023,2023-11-15T09:01:55Z,,,
arXIv2023,Deep Representation Learning for Open Vocabulary Electroencephalography-to-Text Decoding,No.,1,"""No evidence""",2023,2023-11-15T08:03:09Z,,,
arXIv2023,Token Prediction as Implicit Classification to Identify LLM-Generated Text,No.,1,"""No evidence""",2023,2023-11-15T06:33:52Z,,,
arXIv2023,Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory,No.,1,"""No evidence""",2023,2023-11-15T06:08:35Z,,,
arXIv2023,Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains,No.,1,"""No evidence""",2023,2023-11-15T05:11:26Z,,,
arXIv2023,Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models,No.,1,"""No evidence""",2023,2023-11-15T04:40:43Z,,,
arXIv2023,An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping,No.,1,"""No evidence""",2023,2023-11-15T04:30:20Z,,,
arXIv2023,Understanding Calibration for Multilingual Question Answering Models,No.,1,"""No evidence""",2023,2023-11-15T03:29:02Z,,,
arXIv2023,Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets,No.,1,"""No evidence""",2023,2023-11-15T02:59:10Z,,,
arXIv2023,Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing,No.,1,"""No evidence""",2023,2023-11-15T01:59:40Z,,,
arXIv2023,Explore Spurious Correlations at the Concept Level in Language Models for Text Classification,No.,1,"""No evidence""",2023,2023-11-15T01:58:54Z,,,
arXIv2023,Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation,No.,1,"""No evidence""",2023,2023-11-15T01:28:28Z,,,
arXIv2023,XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making,No.,1,"""No evidence""",2023,2023-11-15T00:34:28Z,,,
arXIv2023,DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Language Models,No.,1,"""No evidence""",2023,2023-11-14T23:43:47Z,,,
arXIv2023,"ACID: Abstractive, Content-Based IDs for Document Retrieval with Language Models",No.,1,"""No evidence""",2023,2023-11-14T23:28:36Z,,,
arXIv2023,AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications,No.,1,"""No evidence""",2023,2023-11-14T23:28:23Z,,,
arXIv2023,PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models,No.,1,"""No evidence""",2023,2023-11-14T23:20:51Z,,,
arXIv2023,Towards Evaluating AI Systems for Moral Status Using Self-Reports,No.,1,"""No evidence""",2023,2023-11-14T22:45:44Z,,,
arXIv2023,Low-Rank Adaptation for Multilingual Summarization: An Empirical Study,No.,1,"""No evidence""",2023,2023-11-14T22:32:39Z,,,
arXIv2023,"MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration",No.,1,"""No evidence""",2023,2023-11-14T21:46:27Z,,,
arXIv2023,UT5: Pretraining Non autoregressive T5 with unrolled denoising,No.,1,"""No evidence""",2023,2023-11-14T21:28:10Z,,,
arXIv2023,DeepThought: An Architecture for Autonomous Self-motivated Systems,No.,1,"""No evidence""",2023,2023-11-14T21:20:23Z,,,
arXIv2023,Efficient Continual Pre-training for Building Domain Specific Large Language Models,No.,1,"""No evidence""",2023,2023-11-14T21:19:14Z,,,
arXIv2023,Artificial intelligence and the skill premium,No.,1,"""No evidence""",2023,2023-11-14T20:16:55Z,,,
arXIv2023,Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning,No.,1,"""No evidence""",2023,2023-11-14T19:53:53Z,,,
arXIv2023,Alignment is not sufficient to prevent large language models from generating harmful information: A psychoanalytic perspective,No.,1,"""No evidence""",2023,2023-11-14T19:28:51Z,,,
arXIv2023,UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations,No.,1,"""No evidence""",2023,2023-11-14T19:00:55Z,,,
arXIv2023,Zero-shot audio captioning with audio-language model guidance and audio context keywords,No.,1,"""No evidence""",2023,2023-11-14T18:55:48Z,,,
arXIv2023,TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer,No.,1,"""No evidence""",2023,2023-11-14T18:50:51Z,,,
arXIv2023,Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding,No.,1,"""No evidence""",2023,2023-11-14T18:43:51Z,,,
arXIv2023,A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts,No.,1,"""No evidence""",2023,2023-11-14T18:40:42Z,,,
arXIv2023,How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection,No.,1,"""No evidence""",2023,2023-11-14T18:32:52Z,,,
arXIv2023,Plum: Prompt Learning using Metaheuristic,No.,1,"""No evidence""",2023,2023-11-14T18:14:56Z,,,
arXIv2023,AI-generated text boundary detection with RoFT,No.,1,"""No evidence""",2023,2023-11-14T17:48:19Z,,,
arXIv2023,MC^2: A Multilingual Corpus of Minority Languages in China,No.,1,"""No evidence""",2023,2023-11-14T17:45:50Z,,,
arXIv2023,Anti-LM Decoding for Zero-shot In-context Machine Translation,No.,1,"""No evidence""",2023,2023-11-14T17:09:43Z,,,
arXIv2023,On-the-Fly Fusion of Large Language Models and Machine Translation,No.,1,"""No evidence""",2023,2023-11-14T16:49:33Z,,,
arXIv2023,Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster,No.,1,"""No evidence""",2023,2023-11-14T15:56:18Z,,,
arXIv2023,Large Language Model-Driven Classroom Flipping: Empowering Student-Centric Peer Questioning with Flipped Interaction,No.,1,"""No evidence""",2023,2023-11-14T15:48:19Z,,,
arXIv2023,REST: Retrieval-Based Speculative Decoding,No.,1,"""No evidence""",2023,2023-11-14T15:43:47Z,,,
arXIv2023,Investigating the Encoding of Words in BERT's Neurons using Feature Textualization,No.,1,"""No evidence""",2023,2023-11-14T15:21:49Z,,,
arXIv2023,Eval-GCSC: A New Metric for Evaluating ChatGPT's Performance in Chinese Spelling Correction,No.,1,"""No evidence""",2023,2023-11-14T14:56:33Z,,,
arXIv2023,Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models,No.,1,"""No evidence""",2023,2023-11-14T14:49:46Z,,,
arXIv2023,Human-Centric Autonomous Systems With LLMs for User Command Reasoning,No.,1,"""No evidence""",2023,2023-11-14T14:42:28Z,,,
arXIv2023,Secure Transformer Inference,No.,1,"""No evidence""",2023,2023-11-14T14:37:23Z,,,
arXIv2023,All Data on the Table: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction,No.,1,"""No evidence""",2023,2023-11-14T14:22:47Z,,,
arXIv2023,Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration,No.,1,"""No evidence""",2023,2023-11-14T13:27:07Z,,,
arXIv2023,Memory-efficient Stochastic methods for Memory-based Transformers,No.,1,"""No evidence""",2023,2023-11-14T12:37:25Z,,,
arXIv2023,SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training with Adversarial Remarks,No.,1,"""No evidence""",2023,2023-11-14T12:12:25Z,,,
arXIv2023,DiLoCo: Distributed Low-Communication Training of Language Models,No.,1,"""No evidence""",2023,2023-11-14T12:05:45Z,,,
arXIv2023,Exploring Semi-supervised Hierarchical Stacked Encoder for Legal Judgement Prediction,No.,1,"""No evidence""",2023,2023-11-14T12:03:26Z,,,
arXIv2023,Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding,No.,1,"""No evidence""",2023,2023-11-14T10:11:36Z,,,
arXIv2023,Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models,No.,1,"""No evidence""",2023,2023-11-14T09:12:40Z,,,
arXIv2023,Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code,No.,1,"""No evidence""",2023,2023-11-14T08:34:26Z,,,
arXIv2023,"The ART of LLM Refinement: Ask, Refine, and Trust",No.,1,"""No evidence""",2023,2023-11-14T07:26:32Z,,,
arXIv2023,Language Models are Better Bug Detector Through Code-Pair Classification,No.,1,"""No evidence""",2023,2023-11-14T07:20:57Z,,,
arXIv2023,Finding Inductive Loop Invariants using Large Language Models,No.,1,"""No evidence""",2023,2023-11-14T06:58:09Z,,,
arXIv2023,Well begun is half done: Importance of Starting Right in Multi-Step Math Reasoning,No.,1,"""No evidence""",2023,2023-11-14T06:45:31Z,,,
arXIv2023,It's All Relative! -- A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction,No.,1,"""No evidence""",2023,2023-11-14T06:16:49Z,,,
arXIv2023,Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model,No.,1,"""No evidence""",2023,2023-11-14T05:30:43Z,,,
arXIv2023,"Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",No.,1,"""No evidence""",2023,2023-11-14T04:33:49Z,,,
arXIv2023,CPopQA: Ranking Cultural Concept Popularity by LLMs,No.,1,"""No evidence""",2023,2023-11-14T04:10:40Z,,,
arXIv2023,Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators,No.,1,"""No evidence""",2023,2023-11-14T03:18:28Z,,,
arXIv2023,Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA,No.,1,"""No evidence""",2023,2023-11-14T02:05:29Z,,,
arXIv2023,On the Analysis of Cross-Lingual Prompt Tuning for Decoder-based Multilingual Model,No.,1,"""No evidence""",2023,2023-11-14T00:43:33Z,,,
arXIv2023,Leveraging Large Language Models to Detect Influence Campaigns in Social Media,No.,1,"""No evidence""",2023,2023-11-14T00:25:09Z,,,
arXIv2023,"In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax",No.,1,"""No evidence""",2023,2023-11-13T23:52:43Z,,,
arXIv2023,GreekT5: A Series of Greek Sequence-to-Sequence Models for News Summarization,No.,1,"""No evidence""",2023,2023-11-13T21:33:12Z,,,
arXIv2023,Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains,No.,1,"""No evidence""",2023,2023-11-13T20:07:36Z,,,
arXIv2023,AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising,No.,1,"""No evidence""",2023,2023-11-13T19:36:54Z,,,
arXIv2023,Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games,No.,1,"""No evidence""",2023,2023-11-13T19:12:49Z,,,
arXIv2023,"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",No.,1,"""No evidence""",2023,2023-11-13T18:59:47Z,,,
arXIv2023,To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning,No.,1,"""No evidence""",2023,2023-11-13T18:59:31Z,,,
arXIv2023,GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation,No.,1,"""No evidence""",2023,2023-11-13T18:53:37Z,,,
arXIv2023,A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering,No.,1,"""No evidence""",2023,2023-11-13T18:22:32Z,,,
arXIv2023,A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases,No.,1,"""No evidence""",2023,2023-11-13T17:54:50Z,,,
arXIv2023,Reducing the Need for Backpropagation and Discovering Better Optima With Explicit Optimizations of Neural Networks,No.,1,"""No evidence""",2023,2023-11-13T17:38:07Z,,,
arXIv2023,Multilingual Nonce Dependency Treebanks: Understanding how LLMs represent and process syntactic structure,No.,1,"""No evidence""",2023,2023-11-13T17:36:58Z,,,
arXIv2023,Psychometric Predictive Power of Large Language Models,No.,1,"""No evidence""",2023,2023-11-13T17:19:14Z,,,
arXIv2023,Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer,No.,1,"""No evidence""",2023,2023-11-13T17:03:02Z,,,
arXIv2023,"MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks",No.,1,"""No evidence""",2023,2023-11-13T16:45:37Z,,,
arXIv2023,Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor,No.,1,"""No evidence""",2023,2023-11-13T16:03:23Z,,,
arXIv2023,Hallucination Augmented Recitations for Language Models,No.,1,"""No evidence""",2023,2023-11-13T15:58:18Z,,,
arXIv2023,Speech-based Slot Filling using Large Language Models,No.,1,"""No evidence""",2023,2023-11-13T15:54:30Z,,,
arXIv2023,Past as a Guide: Leveraging Retrospective Learning for Python Code Completion,No.,1,"""No evidence""",2023,2023-11-13T14:40:33Z,,,
arXIv2023,The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4,No.,1,"""No evidence""",2023,2023-11-13T14:26:12Z,,,
arXIv2023,Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models,No.,1,"""No evidence""",2023,2023-11-13T13:10:44Z,,,
arXIv2023,Danish Foundation Models,No.,1,"""No evidence""",2023,2023-11-13T12:03:52Z,,,
arXIv2023,Large Language Models for Robotics: A Survey,No.,1,"""No evidence""",2023,2023-11-13T10:46:35Z,,,
arXIv2023,Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback,No.,1,"""No evidence""",2023,2023-11-13T10:15:19Z,,,
arXIv2023,On Elastic Language Models,No.,1,"""No evidence""",2023,2023-11-13T09:55:52Z,,,
arXIv2023,Applying Large Language Models for Causal Structure Learning in Non Small Cell Lung Cancer,No.,1,"""No evidence""",2023,2023-11-13T09:31:14Z,,,
arXIv2023,VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency,No.,1,"""No evidence""",2023,2023-11-13T09:06:58Z,,,
arXIv2023,STEER: Unified Style Transfer with Expert Reinforcement,No.,1,"""No evidence""",2023,2023-11-13T09:02:30Z,,,
arXIv2023,Pruning random resistive memory for optimizing analogue AI,No.,1,"""No evidence""",2023,2023-11-13T08:59:01Z,,,
arXIv2023,Interaction is all You Need? A Study of Robots Ability to Understand and Execute,No.,1,"""No evidence""",2023,2023-11-13T08:39:06Z,,,
arXIv2023,Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions,No.,1,"""No evidence""",2023,2023-11-13T07:12:57Z,,,
arXIv2023,SpectralGPT: Spectral Remote Sensing Foundation Model,No.,1,"""No evidence""",2023,2023-11-13T07:09:30Z,,,
arXIv2023,Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning,No.,1,"""No evidence""",2023,2023-11-13T06:13:38Z,,,
arXIv2023,To Tell The Truth: Language of Deception and Language Models,No.,1,"""No evidence""",2023,2023-11-13T05:40:11Z,,,
arXIv2023,On the Discussion of Large Language Models: Symmetry of Agents and Interplay with Prompts,No.,1,"""No evidence""",2023,2023-11-13T04:56:48Z,,,
arXIv2023,PROPANE: Prompt design as an inverse problem,No.,1,"""No evidence""",2023,2023-11-13T04:08:49Z,,,
arXIv2023,Open-Vocabulary Video Anomaly Detection,No.,1,"""No evidence""",2023,2023-11-13T02:54:17Z,,,
arXIv2023,Pretrain like Your Inference: Masked Tuning Improves Zero-Shot Composed Image Retrieval,No.,1,"""No evidence""",2023,2023-11-13T02:49:57Z,,,
arXIv2023,To Transformers and Beyond: Large Language Models for the Genome,No.,1,"""No evidence""",2023,2023-11-13T02:13:58Z,,,
arXIv2023,ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models,No.,1,"""No evidence""",2023,2023-11-13T02:13:13Z,,,
arXIv2023,Teach me with a Whisper: Enhancing Large Language Models for Analyzing Spoken Transcripts using Speech Embeddings,No.,1,"""No evidence""",2023,2023-11-13T01:53:12Z,,,
arXIv2023,Retrieval and Generative Approaches for a Pregnancy Chatbot in Nepali with Stemmed and Non-Stemmed Data : A Comparative Study,No.,1,"""No evidence""",2023,2023-11-12T17:16:46Z,,,
arXIv2023,Modeling User Viewing Flow Using Large Language Models for Article Recommendation,No.,1,"""No evidence""",2023,2023-11-12T15:32:57Z,,,
arXIv2023,GIELLM: Japanese General Information Extraction Large Language Model Utilizing Mutual Reinforcement Effect,No.,1,"""No evidence""",2023,2023-11-12T13:30:38Z,,,
arXIv2023,Evaluation of GPT-4 for chest X-ray impression generation: A reader study on performance and perception,No.,1,"""No evidence""",2023,2023-11-12T11:40:57Z,,,
arXIv2023,Tunable Soft Prompts are Messengers in Federated Learning,No.,1,"""No evidence""",2023,2023-11-12T11:01:10Z,,,
arXIv2023,InfMLLM: A Unified Framework for Visual-Language Tasks,No.,1,"""No evidence""",2023,2023-11-12T09:58:16Z,,,
arXIv2023,Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models,No.,1,"""No evidence""",2023,2023-11-12T09:10:51Z,,,
arXIv2023,ChatAnything: Facetime Chat with LLM-Enhanced Personas,No.,1,"""No evidence""",2023,2023-11-12T08:29:41Z,,,
arXIv2023,Large Language Models' Understanding of Math: Source Criticism and Extrapolation,No.,1,"""No evidence""",2023,2023-11-12T07:52:32Z,,,
arXIv2023,From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models,No.,1,"""No evidence""",2023,2023-11-12T06:56:21Z,,,
arXIv2023,Towards General-Purpose Speech Abilities for Large Language Models Using Unpaired Data,No.,1,"""No evidence""",2023,2023-11-12T06:56:14Z,,,
arXIv2023,Detecting and Correcting Hate Speech in Multimodal Memes with Large Visual Language Model,No.,1,"""No evidence""",2023,2023-11-12T05:20:20Z,,,
arXIv2023,Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer,No.,1,"""No evidence""",2023,2023-11-12T03:25:34Z,,,
arXIv2023,In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering,No.,1,"""No evidence""",2023,2023-11-11T21:19:44Z,,,
arXIv2023,TrainerAgent: Customizable and Efficient Model Training through LLM-Powered Multi-Agent System,No.,1,"""No evidence""",2023,2023-11-11T17:39:24Z,,,
arXIv2023,PerceptionGPT: Effectively Fusing Visual Perception into LLM,No.,1,"""No evidence""",2023,2023-11-11T16:59:20Z,,,
arXIv2023,From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL,No.,1,"""No evidence""",2023,2023-11-11T15:40:21Z,,,
arXIv2023,Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction,No.,1,"""No evidence""",2023,2023-11-11T12:05:01Z,,,
arXIv2023,How ChatGPT is Solving Vulnerability Management Problem,No.,1,"""No evidence""",2023,2023-11-11T11:01:13Z,,,
arXIv2023,Conceptual Model Interpreter for Large Language Models,No.,1,"""No evidence""",2023,2023-11-11T09:41:37Z,,,
arXIv2023,Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems,No.,1,"""No evidence""",2023,2023-11-11T09:06:15Z,,,
arXIv2023,LayoutPrompter: Awaken the Design Ability of Large Language Models,No.,1,"""No evidence""",2023,2023-11-11T07:15:14Z,,,
arXIv2023,L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models,No.,1,"""No evidence""",2023,2023-11-11T06:59:50Z,,,
arXIv2023,Online Advertisements with LLMs: Opportunities and Challenges,No.,1,"""No evidence""",2023,2023-11-11T02:13:32Z,,,
arXIv2023,THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech,No.,1,"""No evidence""",2023,2023-11-11T00:30:31Z,,,
arXIv2023,Testing LLMs on Code Generation with Varying Levels of Prompt Specificity,No.,1,"""No evidence""",2023,2023-11-10T23:41:41Z,,,
arXIv2023,ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages,No.,1,"""No evidence""",2023,2023-11-10T23:25:34Z,,,
arXIv2023,Analyzing Modular Approaches for Visual Question Decomposition,No.,1,"""No evidence""",2023,2023-11-10T22:14:26Z,,,
arXIv2023,Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs,No.,1,"""No evidence""",2023,2023-11-10T21:32:34Z,,,
arXIv2023,ChatGPT in the context of precision agriculture data analytics,No.,1,"""No evidence""",2023,2023-11-10T20:44:30Z,,,
arXIv2023,Heaps' Law in GPT-Neo Large Language Model Emulated Corpora,No.,1,"""No evidence""",2023,2023-11-10T20:07:32Z,,,
arXIv2023,Relation Extraction in underexplored biomedical domains: A diversity-optimised sampling and synthetic data generation approach,No.,1,"""No evidence""",2023,2023-11-10T19:36:00Z,,,
arXIv2023,Word Definitions from Large Language Models,No.,1,"""No evidence""",2023,2023-11-10T19:27:20Z,,,
arXIv2023,Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking,No.,1,"""No evidence""",2023,2023-11-10T19:00:02Z,,,
arXIv2023,Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization,No.,1,"""No evidence""",2023,2023-11-10T18:59:54Z,,,
arXIv2023,Argumentation Element Annotation Modeling using XLNet,No.,1,"""No evidence""",2023,2023-11-10T18:55:23Z,,,
arXIv2023,Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations,No.,1,"""No evidence""",2023,2023-11-10T18:54:33Z,,,
arXIv2023,Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild,No.,1,"""No evidence""",2023,2023-11-10T18:52:58Z,,,
arXIv2023,Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models,No.,1,"""No evidence""",2023,2023-11-10T18:48:58Z,,,
arXIv2023,Holistic Evaluation of GPT-4V for Biomedical Imaging,No.,1,"""No evidence""",2023,2023-11-10T18:40:44Z,,,
arXIv2023,Syntax-semantics interface: an algebraic model,No.,1,"""No evidence""",2023,2023-11-10T17:12:09Z,,,
arXIv2023,ChatGPT as Co-Advisor in Scientific Initiation: Action Research with Project-Based Learning in Elementary Education,No.,1,"""No evidence""",2023,2023-11-10T16:41:45Z,,,
arXIv2023,Is it indeed bigger better? The comprehensive study of claim detection LMs applied for disinformation tackling,No.,1,"""No evidence""",2023,2023-11-10T15:36:35Z,,,
arXIv2023,Large Language Models are Zero Shot Hypothesis Proposers,No.,1,"""No evidence""",2023,2023-11-10T10:03:49Z,,,
arXIv2023,TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree transformation,No.,1,"""No evidence""",2023,2023-11-10T09:05:23Z,,,
arXIv2023,Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction,No.,1,"""No evidence""",2023,2023-11-10T08:12:00Z,,,
arXIv2023,Fake Alignment: Are LLMs Really Aligned Well?,No.,1,"""No evidence""",2023,2023-11-10T08:01:23Z,,,
arXIv2023,FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores,No.,1,"""No evidence""",2023,2023-11-10T07:33:35Z,,,
arXIv2023,"Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",No.,1,"""No evidence""",2023,2023-11-10T07:13:06Z,,,
arXIv2023,Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification,No.,1,"""No evidence""",2023,2023-11-10T05:24:07Z,,,
arXIv2023,AI-native Interconnect Framework for Integration of Large Language Model Technologies in 6G Systems,No.,1,"""No evidence""",2023,2023-11-10T02:59:16Z,,,
arXIv2023,CloudEval-YAML: A Practical Benchmark for Cloud Configuration Generation,No.,1,"""No evidence""",2023,2023-11-10T01:49:57Z,,,
arXIv2023,Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion,No.,1,"""No evidence""",2023,2023-11-10T01:18:47Z,,,
arXIv2023,CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model,No.,1,"""No evidence""",2023,2023-11-10T01:12:03Z,,,
arXIv2023,Model-as-a-Service (MaaS): A Survey,No.,1,"""No evidence""",2023,2023-11-10T00:35:00Z,,,
arXIv2023,Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval,No.,1,"""No evidence""",2023,2023-11-10T00:17:10Z,,,
arXIv2023,Chatbots Are Not Reliable Text Annotators,No.,1,"""No evidence""",2023,2023-11-09T22:28:14Z,,,
arXIv2023,Deep Natural Language Feature Learning for Interpretable Prediction,No.,1,"""No evidence""",2023,2023-11-09T21:43:27Z,,,
arXIv2023,Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models,No.,1,"""No evidence""",2023,2023-11-09T21:10:52Z,,,
arXIv2023,Efficient Parallelization Layouts for Large-Scale Distributed Model Training,No.,1,"""No evidence""",2023,2023-11-09T18:59:38Z,,,
arXIv2023,What Do I Hear? Generating Sounds for Visuals with ChatGPT,No.,1,"""No evidence""",2023,2023-11-09T18:59:24Z,,,
arXIv2023,FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts,No.,1,"""No evidence""",2023,2023-11-09T18:59:11Z,,,
arXIv2023,LLM Augmented Hierarchical Agents,No.,1,"""No evidence""",2023,2023-11-09T18:54:28Z,,,
arXIv2023,Accuracy of a Vision-Language Model on Challenging Medical Cases,No.,1,"""No evidence""",2023,2023-11-09T18:48:02Z,,,
arXIv2023,Cognitively Inspired Components for Social Conversational Agents,No.,1,"""No evidence""",2023,2023-11-09T15:38:58Z,,,
arXIv2023,Fair Coresets via Optimal Transport,No.,1,"""No evidence""",2023,2023-11-09T15:21:56Z,,,
arXIv2023,TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs,No.,1,"""No evidence""",2023,2023-11-09T13:58:59Z,,,
arXIv2023,u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model,No.,1,"""No evidence""",2023,2023-11-09T13:18:27Z,,,
arXIv2023,On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving,No.,1,"""No evidence""",2023,2023-11-09T12:58:37Z,,,
arXIv2023,Improving Vision-and-Language Reasoning via Spatial Relations Modeling,No.,1,"""No evidence""",2023,2023-11-09T11:54:55Z,,,
arXIv2023,Leveraging Artificial Intelligence Technology for Mapping Research to Sustainable Development Goals: A Case Study,No.,1,"""No evidence""",2023,2023-11-09T11:44:22Z,,,
arXIv2023,Model-Based Minimum Bayes Risk Decoding,No.,1,"""No evidence""",2023,2023-11-09T10:46:09Z,,,
arXIv2023,Vision Encoder-Decoder Models for AI Coaching,No.,1,"""No evidence""",2023,2023-11-09T09:06:21Z,,,
arXIv2023,An Experiment in Retrofitting Competency Questions for Existing Ontologies,No.,1,"""No evidence""",2023,2023-11-09T08:57:39Z,,,
arXIv2023,PRODIGy: a PROfile-based DIalogue Generation dataset,No.,1,"""No evidence""",2023,2023-11-09T08:19:34Z,,,
arXIv2023,Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation,No.,1,"""No evidence""",2023,2023-11-09T06:45:04Z,,,
arXIv2023,Labor Space: A Unifying Representation of the Labor Market via Large Language Models,No.,1,"""No evidence""",2023,2023-11-09T06:41:10Z,,,
arXIv2023,Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization,No.,1,"""No evidence""",2023,2023-11-09T06:19:51Z,,,
arXIv2023,Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset,No.,1,"""No evidence""",2023,2023-11-09T02:58:17Z,,,
arXIv2023,"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge",No.,1,"""No evidence""",2023,2023-11-09T02:55:58Z,,,
arXIv2023,GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT),No.,1,"""No evidence""",2023,2023-11-09T01:50:47Z,,,
arXIv2023,Agent Lumos: Unified and Modular Training for Open-Source Language Agents,No.,1,"""No evidence""",2023,2023-11-09T00:30:13Z,,,
arXIv2023,Large Human Language Models: A Need and the Challenges,No.,1,"""No evidence""",2023,2023-11-09T00:27:28Z,,,
arXIv2023,Zero-shot Translation of Attention Patterns in VQA Models to Natural Language,No.,1,"""No evidence""",2023,2023-11-08T22:18:53Z,,,
arXIv2023,NLQxform: A Language Model-based Question to SPARQL Transformer,No.,1,"""No evidence""",2023,2023-11-08T21:41:45Z,,,
arXIv2023,"First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models",No.,1,"""No evidence""",2023,2023-11-08T21:13:38Z,,,
arXIv2023,DEMASQ: Unmasking the ChatGPT Wordsmith,No.,1,"""No evidence""",2023,2023-11-08T21:13:05Z,,,
arXIv2023,On the steerability of large language models toward data-driven personas,No.,1,"""No evidence""",2023,2023-11-08T19:01:13Z,,,
arXIv2023,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,No.,1,"""No evidence""",2023,2023-11-08T18:59:54Z,,,
arXIv2023,GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs,No.,1,"""No evidence""",2023,2023-11-08T18:59:05Z,,,
arXIv2023,Prompt Sketching for Large Language Models,No.,1,"""No evidence""",2023,2023-11-08T18:57:23Z,,,
arXIv2023,Future Lens: Anticipating Subsequent Tokens from a Single Hidden State,No.,1,"""No evidence""",2023,2023-11-08T18:56:35Z,,,
arXIv2023,LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models,No.,1,"""No evidence""",2023,2023-11-08T18:33:06Z,,,
arXIv2023,ADaPT: As-Needed Decomposition and Planning with Language Models,No.,1,"""No evidence""",2023,2023-11-08T17:59:15Z,,,
arXIv2023,Bridging Dimensions: Confident Reachability for High-Dimensional Controllers,No.,1,"""No evidence""",2023,2023-11-08T17:26:38Z,,,
arXIv2023,DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining,No.,1,"""No evidence""",2023,2023-11-08T16:18:32Z,,,
arXIv2023,Determination of toxic comments and unintended model bias minimization using Deep learning approach,No.,1,"""No evidence""",2023,2023-11-08T16:10:28Z,,,
arXIv2023,Using large language models to study human memory for meaningful narratives,No.,1,"""No evidence""",2023,2023-11-08T15:11:57Z,,,
arXIv2023,Evaluating Generative Ad Hoc Information Retrieval,No.,1,"""No evidence""",2023,2023-11-08T14:05:00Z,,,
arXIv2023,Pre-training LLMs using human-like development data corpus,No.,1,"""No evidence""",2023,2023-11-08T13:13:23Z,,,
arXIv2023,Speech language models lack important brain-relevant semantics,No.,1,"""No evidence""",2023,2023-11-08T13:11:48Z,,,
arXIv2023,Massive Editing for Large Language Models via Meta Learning,No.,1,"""No evidence""",2023,2023-11-08T13:03:06Z,,,
arXIv2023,TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models,No.,1,"""No evidence""",2023,2023-11-08T10:34:16Z,,,
arXIv2023,Assessing Distractors in Multiple-Choice Tests,No.,1,"""No evidence""",2023,2023-11-08T09:37:09Z,,,
arXIv2023,Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures,No.,1,"""No evidence""",2023,2023-11-08T09:26:27Z,,,
arXIv2023,Validating ChatGPT Facts through RDF Knowledge Graphs and Sentence Similarity,No.,1,"""No evidence""",2023,2023-11-08T08:27:11Z,,,
arXIv2023,"NExT-Chat: An LMM for Chat, Detection and Segmentation",No.,1,"""No evidence""",2023,2023-11-08T07:15:05Z,,,
arXIv2023,Improving Pacing in Long-Form Story Planning,No.,1,"""No evidence""",2023,2023-11-08T04:58:29Z,,,
arXIv2023,Human-Centered Planning,No.,1,"""No evidence""",2023,2023-11-08T00:14:05Z,,,
arXIv2023,AI for All: Operationalising Diversity and Inclusion Requirements for AI Systems,No.,1,"""No evidence""",2023,2023-11-07T23:15:03Z,,,
arXIv2023,Evaluating multiple large language models in pediatric ophthalmology,No.,1,"""No evidence""",2023,2023-11-07T22:23:51Z,,,
arXIv2023,Uncovering Intermediate Variables in Transformers using Circuit Probing,No.,1,"""No evidence""",2023,2023-11-07T21:27:17Z,,,
arXIv2023,Formal Aspects of Language Modeling,No.,1,"""No evidence""",2023,2023-11-07T20:21:42Z,,,
arXIv2023,CRAB: Assessing the Strength of Causal Relationships Between Real-world Events,No.,1,"""No evidence""",2023,2023-11-07T19:00:44Z,,,
arXIv2023,Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves,No.,1,"""No evidence""",2023,2023-11-07T18:43:34Z,,,
arXIv2023,Perturbed examples reveal invariances shared by language models,No.,1,"""No evidence""",2023,2023-11-07T17:48:35Z,,,
arXIv2023,Black-Box Prompt Optimization: Aligning Large Language Models without Model Training,No.,1,"""No evidence""",2023,2023-11-07T17:31:50Z,,,
arXIv2023,Modelling Sentiment Analysis: LLMs and data augmentation techniques,No.,1,"""No evidence""",2023,2023-11-07T17:12:39Z,,,
arXIv2023,Interpreting Shared Circuits for Ordered Sequence Prediction in a Large Language Model,No.,1,"""No evidence""",2023,2023-11-07T16:58:51Z,,,
arXIv2023,Evaluating Large Language Models in Ophthalmology,No.,1,"""No evidence""",2023,2023-11-07T16:19:45Z,,,
arXIv2023,Personality Style Recognition via Machine Learning: Identifying Anaclitic and Introjective Personality Styles from Patients' Speech,No.,1,"""No evidence""",2023,2023-11-07T15:56:19Z,,,
arXIv2023,Extracting human interpretable structure-property relationships in chemistry using XAI and large language models,No.,1,"""No evidence""",2023,2023-11-07T15:02:32Z,,,
arXIv2023,mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration,No.,1,"""No evidence""",2023,2023-11-07T14:21:29Z,,,
arXIv2023,Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations,No.,1,"""No evidence""",2023,2023-11-07T13:54:56Z,,,
arXIv2023,Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation,No.,1,"""No evidence""",2023,2023-11-07T12:30:36Z,,,
arXIv2023,Unifying Structure and Language Semantic for Efficient Contrastive Knowledge Graph Completion with Structured Entity Anchors,No.,1,"""No evidence""",2023,2023-11-07T11:17:55Z,,,
arXIv2023,Aspects of human memory and Large Language Models,No.,1,"""No evidence""",2023,2023-11-07T09:39:12Z,,,
arXIv2023,OLaLa: Ontology Matching with Large Language Models,No.,1,"""No evidence""",2023,2023-11-07T09:34:20Z,,,
arXIv2023,Conversations in Galician: a Large Language Model for an Underrepresented Language,No.,1,"""No evidence""",2023,2023-11-07T08:52:28Z,,,
arXIv2023,Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models,No.,1,"""No evidence""",2023,2023-11-07T08:27:32Z,,,
arXIv2023,Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI,No.,1,"""No evidence""",2023,2023-11-07T08:06:27Z,,,
arXIv2023,Meta-Adapter: An Online Few-shot Learner for Vision-Language Model,No.,1,"""No evidence""",2023,2023-11-07T07:27:16Z,,,
arXIv2023,Neuro-GPT: Towards A Foundation Model for EEG,No.,1,"""No evidence""",2023,2023-11-07T07:07:18Z,,,
arXIv2023,Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds,No.,1,"""No evidence""",2023,2023-11-07T06:54:40Z,,,
arXIv2023,Multilingual Mathematical Autoformalization,No.,1,"""No evidence""",2023,2023-11-07T06:42:15Z,,,
arXIv2023,Which is better? Exploring Prompting Strategy For LLM-based Metrics,No.,1,"""No evidence""",2023,2023-11-07T06:36:39Z,,,
arXIv2023,Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning,No.,1,"""No evidence""",2023,2023-11-07T06:19:37Z,,,
arXIv2023,LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators,No.,1,"""No evidence""",2023,2023-11-07T04:44:40Z,,,
arXIv2023,Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning,No.,1,"""No evidence""",2023,2023-11-07T04:30:51Z,,,
arXIv2023,"Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models",No.,1,"""No evidence""",2023,2023-11-07T03:25:56Z,,,
arXIv2023,GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks,No.,1,"""No evidence""",2023,2023-11-07T02:36:24Z,,,
arXIv2023,Principles from Clinical Research for NLP Model Generalization,No.,1,"""No evidence""",2023,2023-11-07T02:17:25Z,,,
arXIv2023,The Linear Representation Hypothesis and the Geometry of Large Language Models,No.,1,"""No evidence""",2023,2023-11-07T01:59:11Z,,,
arXIv2023,Analysis of the User Perception of Chatbots in Education Using A Partial Least Squares Structural Equation Modeling Approach,No.,1,"""No evidence""",2023,2023-11-07T00:44:56Z,,,
arXIv2023,GPT4All: An Ecosystem of Open Source Compressed Language Models,No.,1,"""No evidence""",2023,2023-11-06T23:50:20Z,,,
arXIv2023,Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication,No.,1,"""No evidence""",2023,2023-11-06T23:42:01Z,,,
arXIv2023,Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models,No.,1,"""No evidence""",2023,2023-11-06T21:34:49Z,,,
arXIv2023,In-Context Exemplars as Clues to Retrieving from Large Associative Memory,No.,1,"""No evidence""",2023,2023-11-06T20:13:29Z,,,
arXIv2023,"Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design",No.,1,"""No evidence""",2023,2023-11-06T19:58:26Z,,,
arXIv2023,GLaMM: Pixel Grounding Large Multimodal Model,No.,1,"""No evidence""",2023,2023-11-06T18:59:57Z,,,
arXIv2023,CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding,No.,1,"""No evidence""",2023,2023-11-06T18:59:44Z,,,
arXIv2023,TS-Diffusion: Generating Highly Complex Time Series with Diffusion Models,No.,1,"""No evidence""",2023,2023-11-06T17:52:08Z,,,
arXIv2023,S-LoRA: Serving Thousands of Concurrent LoRA Adapters,No.,1,"""No evidence""",2023,2023-11-06T17:26:17Z,,,
arXIv2023,Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM,No.,1,"""No evidence""",2023,2023-11-06T16:31:48Z,,,
arXIv2023,An Efficient Self-Supervised Cross-View Training For Sentence Embedding,No.,1,"""No evidence""",2023,2023-11-06T16:12:25Z,,,
arXIv2023,Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection,No.,1,"""No evidence""",2023,2023-11-06T15:24:18Z,,,
arXIv2023,Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs,No.,1,"""No evidence""",2023,2023-11-06T14:23:49Z,,,
arXIv2023,Injecting Categorical Labels and Syntactic Information into Biomedical NER,No.,1,"""No evidence""",2023,2023-11-06T14:03:59Z,,,
arXIv2023,Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch,No.,1,"""No evidence""",2023,2023-11-06T13:43:07Z,,,
arXIv2023,A Simple yet Efficient Ensemble Approach for AI-generated Text Detection,No.,1,"""No evidence""",2023,2023-11-06T13:11:02Z,,,
arXIv2023,CogVLM: Visual Expert for Pretrained Language Models,No.,1,"""No evidence""",2023,2023-11-06T13:04:39Z,,,
arXIv2023,Beyond Words: A Mathematical Framework for Interpreting Large Language Models,No.,1,"""No evidence""",2023,2023-11-06T11:13:17Z,,,
arXIv2023,Retrieval-Augmented Code Generation for Universal Information Extraction,No.,1,"""No evidence""",2023,2023-11-06T09:03:21Z,,,
arXIv2023,In-Context Learning for Knowledge Base Question Answering for Unmanned Systems based on Large Language Models,No.,1,"""No evidence""",2023,2023-11-06T08:52:11Z,,,
arXIv2023,SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data,No.,1,"""No evidence""",2023,2023-11-06T05:24:06Z,,,
arXIv2023,Less than One-shot: Named Entity Recognition via Extremely Weak Supervision,No.,1,"""No evidence""",2023,2023-11-06T04:20:42Z,,,
arXIv2023,Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding,No.,1,"""No evidence""",2023,2023-11-06T03:41:57Z,,,
arXIv2023,Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs,No.,1,"""No evidence""",2023,2023-11-06T03:26:41Z,,,
arXIv2023,QualEval: Qualitative Evaluation for Model Improvement,No.,1,"""No evidence""",2023,2023-11-06T00:21:44Z,,,
arXIv2023,CausalCite: A Causal Formulation of Paper Citations,No.,1,"""No evidence""",2023,2023-11-05T23:09:39Z,,,
arXIv2023,Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable Manipulation with Tools,No.,1,"""No evidence""",2023,2023-11-05T22:43:29Z,,,
arXIv2023,UID as a Guiding Metric for Automated Authorship Obfuscation,No.,1,"""No evidence""",2023,2023-11-05T22:16:37Z,,,
arXIv2023,Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language,No.,1,"""No evidence""",2023,2023-11-05T22:16:21Z,,,
arXIv2023,Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead,No.,1,"""No evidence""",2023,2023-11-05T22:13:12Z,,,
arXIv2023,AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs,No.,1,"""No evidence""",2023,2023-11-05T21:43:02Z,,,
arXIv2023,Extraction of Atypical Aspects from Customer Reviews: Datasets and Experiments with Language Models,No.,1,"""No evidence""",2023,2023-11-05T16:15:50Z,,,
arXIv2023,ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-11-05T16:01:40Z,,,
arXIv2023,Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE,No.,1,"""No evidence""",2023,2023-11-05T15:48:29Z,,,
arXIv2023,LLM-enhanced Self-training for Cross-domain Constituency Parsing,No.,1,"""No evidence""",2023,2023-11-05T14:13:29Z,,,
arXIv2023,Cross-Dialect Sentence Transformation: A Comparative Analysis of Language Models for Adapting Sentences to British English,No.,1,"""No evidence""",2023,2023-11-05T12:56:28Z,,,
arXIv2023,Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection,No.,1,"""No evidence""",2023,2023-11-05T10:01:18Z,,,
arXIv2023,BanMANI: A Dataset to Identify Manipulated Social Media News in Bangla,No.,1,"""No evidence""",2023,2023-11-05T05:49:57Z,,,
arXIv2023,Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions,No.,1,"""No evidence""",2023,2023-11-05T03:34:17Z,,,
arXIv2023,Can Chat GPT solve a Linguistics Exam?,No.,1,"""No evidence""",2023,2023-11-04T20:02:57Z,,,
arXIv2023,Can ChatGPT support software verification?,No.,1,"""No evidence""",2023,2023-11-04T15:25:18Z,,,
arXIv2023,You Only Forward Once: Prediction and Rationalization in A Single Forward Pass,No.,1,"""No evidence""",2023,2023-11-04T08:04:28Z,,,
arXIv2023,MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning,No.,1,"""No evidence""",2023,2023-11-04T02:22:40Z,,,
arXIv2023,LLMs grasp morality in concept,No.,1,"""No evidence""",2023,2023-11-04T01:37:41Z,,,
arXIv2023,Not all layers are equally as important: Every Layer Counts BERT,No.,1,"""No evidence""",2023,2023-11-03T23:08:50Z,,,
arXIv2023,COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning,No.,1,"""No evidence""",2023,2023-11-03T21:47:03Z,,,
arXIv2023,Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data,No.,1,"""No evidence""",2023,2023-11-03T20:05:30Z,,,
arXIv2023,Automating Governing Knowledge Commons and Contextual Integrity (GKC-CI) Privacy Policy Annotations with Large Language Models,No.,1,"""No evidence""",2023,2023-11-03T18:49:05Z,,,
arXIv2023,Abstraction via exemplars? A representational case study on lexical category inference in BERT,No.,1,"""No evidence""",2023,2023-11-03T18:45:19Z,,,
arXIv2023,Leveraging Large Language Models for Collective Decision-Making,No.,1,"""No evidence""",2023,2023-11-03T18:27:21Z,,,
arXIv2023,A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction,No.,1,"""No evidence""",2023,2023-11-03T15:44:28Z,,,
arXIv2023,Conditions on Preference Relations that Guarantee the Existence of Optimal Policies,No.,1,"""No evidence""",2023,2023-11-03T15:42:12Z,,,
arXIv2023,ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models,No.,1,"""No evidence""",2023,2023-11-03T15:34:02Z,,,
arXIv2023,The language of prompting: What linguistic properties make a prompt successful?,No.,1,"""No evidence""",2023,2023-11-03T15:03:36Z,,,
arXIv2023,Don't Make Your LLM an Evaluation Benchmark Cheater,No.,1,"""No evidence""",2023,2023-11-03T14:59:54Z,,,
arXIv2023,Too Much Information: Keeping Training Simple for BabyLMs,No.,1,"""No evidence""",2023,2023-11-03T14:50:00Z,,,
arXIv2023,More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems,No.,1,"""No evidence""",2023,2023-11-03T14:47:17Z,,,
arXIv2023,Supermind Ideator: Exploring generative AI to support creative problem-solving,No.,1,"""No evidence""",2023,2023-11-03T14:21:39Z,,,
arXIv2023,Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review,No.,1,"""No evidence""",2023,2023-11-03T13:51:36Z,,,
arXIv2023,LLM-driven Multimodal Target Volume Contouring in Radiation Oncology,No.,1,"""No evidence""",2023,2023-11-03T13:38:42Z,,,
arXIv2023,BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification,No.,1,"""No evidence""",2023,2023-11-03T13:34:08Z,,,
arXIv2023,Simplifying Transformer Blocks,No.,1,"""No evidence""",2023,2023-11-03T13:30:52Z,,,
arXIv2023,The risks of risk-based AI regulation: taking liability seriously,No.,1,"""No evidence""",2023,2023-11-03T12:51:37Z,,,
arXIv2023,Indicative Summarization of Long Discussions,No.,1,"""No evidence""",2023,2023-11-03T12:44:59Z,,,
arXIv2023,Efficient Black-Box Adversarial Attacks on Neural Text Detectors,No.,1,"""No evidence""",2023,2023-11-03T12:29:32Z,,,
arXIv2023,Towards Concept-Aware Large Language Models,No.,1,"""No evidence""",2023,2023-11-03T12:19:22Z,,,
arXIv2023,PILL: Plug Into LLM with Adapter Expert and Attention Gate,No.,1,"""No evidence""",2023,2023-11-03T09:31:10Z,,,
arXIv2023,UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis,No.,1,"""No evidence""",2023,2023-11-03T08:20:48Z,,,
arXIv2023,EmojiLM: Modeling the New Emoji Language,No.,1,"""No evidence""",2023,2023-11-03T07:06:51Z,,,
arXIv2023,An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction,No.,1,"""No evidence""",2023,2023-11-03T05:00:44Z,,,
arXIv2023,Data-Free Distillation of Language Model by Text-to-Text Transfer,No.,1,"""No evidence""",2023,2023-11-03T03:31:47Z,,,
arXIv2023,CASE: Commonsense-Augmented Score with an Expanded Answer Space,No.,1,"""No evidence""",2023,2023-11-03T03:15:26Z,,,
arXIv2023,DialogBench: Evaluating LLMs as Human-like Dialogue Systems,No.,1,"""No evidence""",2023,2023-11-03T02:59:56Z,,,
arXIv2023,"""Close...but not as good as an educator."" -- Using ChatGPT to provide formative feedback in large-class collaborative learning",No.,1,"""No evidence""",2023,2023-11-02T23:00:38Z,,,
arXIv2023,FLAP: Fast Language-Audio Pre-training,No.,1,"""No evidence""",2023,2023-11-02T21:58:50Z,,,
arXIv2023,Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers,No.,1,"""No evidence""",2023,2023-11-02T19:16:21Z,,,
arXIv2023,Market Concentration Implications of Foundation Models,No.,1,"""No evidence""",2023,2023-11-02T19:00:42Z,,,
arXIv2023,Implicit Chain of Thought Reasoning via Knowledge Distillation,No.,1,"""No evidence""",2023,2023-11-02T17:59:49Z,,,
arXIv2023,DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing,No.,1,"""No evidence""",2023,2023-11-02T17:57:38Z,,,
arXIv2023,TopicGPT: A Prompt-based Topic Modeling Framework,No.,1,"""No evidence""",2023,2023-11-02T17:57:10Z,,,
arXIv2023,The Behavior of Large Language Models When Prompted to Generate Code Explanations,No.,1,"""No evidence""",2023,2023-11-02T17:14:38Z,,,
arXIv2023,Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants,No.,1,"""No evidence""",2023,2023-11-02T17:07:23Z,,,
arXIv2023,What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning,No.,1,"""No evidence""",2023,2023-11-02T15:36:12Z,,,
arXIv2023,AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models,No.,1,"""No evidence""",2023,2023-11-02T15:18:22Z,,,
arXIv2023,ProAgent: From Robotic Process Automation to Agentic Process Automation,No.,1,"""No evidence""",2023,2023-11-02T14:32:16Z,,,
arXIv2023,People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection,No.,1,"""No evidence""",2023,2023-11-02T14:31:25Z,,,
arXIv2023,An energy-based comparative analysis of common approaches to text classification in the Legal domain,No.,1,"""No evidence""",2023,2023-11-02T14:16:48Z,,,
arXIv2023,Long Story Short: a Summarize-then-Search Method for Long Video Question Answering,No.,1,"""No evidence""",2023,2023-11-02T13:36:11Z,,,
arXIv2023,Continual Learning Under Language Shift,No.,1,"""No evidence""",2023,2023-11-02T12:54:50Z,,,
arXIv2023,Generative Input: Towards Next-Generation Input Methods Paradigm,No.,1,"""No evidence""",2023,2023-11-02T12:01:29Z,,,
arXIv2023,Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist,No.,1,"""No evidence""",2023,2023-11-02T11:55:07Z,,,
arXIv2023,Predicting Question-Answering Performance of Large Language Models through Semantic Consistency,No.,1,"""No evidence""",2023,2023-11-02T11:27:21Z,,,
arXIv2023,Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance,No.,1,"""No evidence""",2023,2023-11-02T09:20:38Z,,,
arXIv2023,Adapting Fake News Detection to the Era of Large Language Models,No.,1,"""No evidence""",2023,2023-11-02T08:39:45Z,,,
arXIv2023,Multi-dimensional data refining strategy for effective fine-tuning LLMs,No.,1,"""No evidence""",2023,2023-11-02T07:50:43Z,,,
arXIv2023,AI-assisted Learning for Electronic Engineering Courses in High Education,No.,1,"""No evidence""",2023,2023-11-02T07:48:10Z,,,
arXIv2023,Integrating Language-Derived Appearance Elements with Visual Cues in Pedestrian Detection,No.,1,"""No evidence""",2023,2023-11-02T06:38:19Z,,,
arXIv2023,Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion,No.,1,"""No evidence""",2023,2023-11-02T06:21:56Z,,,
arXIv2023,Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning,No.,1,"""No evidence""",2023,2023-11-02T06:21:35Z,,,
arXIv2023,COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances,No.,1,"""No evidence""",2023,2023-11-02T06:14:41Z,,,
arXIv2023,Effective Human-AI Teams via Learned Natural Language Rules and Onboarding,No.,1,"""No evidence""",2023,2023-11-02T06:00:48Z,,,
arXIv2023,Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning,No.,1,"""No evidence""",2023,2023-11-02T05:44:13Z,,,
arXIv2023,Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia,No.,1,"""No evidence""",2023,2023-11-02T05:27:48Z,,,
arXIv2023,Vision-Language Interpreter for Robot Task Planning,No.,1,"""No evidence""",2023,2023-11-02T03:32:30Z,,,
arXIv2023,Measuring Five Accountable Talk Moves to Improve Instruction at Scale,No.,1,"""No evidence""",2023,2023-11-02T03:04:50Z,,,
arXIv2023,Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models,No.,1,"""No evidence""",2023,2023-11-02T02:21:39Z,,,
arXIv2023,Self-Influence Guided Data Reweighting for Language Model Pre-training,No.,1,"""No evidence""",2023,2023-11-02T01:00:46Z,,,
arXIv2023,Relax: Composable Abstractions for End-to-End Dynamic Machine Learning,No.,1,"""No evidence""",2023,2023-11-01T23:03:59Z,,,
arXIv2023,Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing,No.,1,"""No evidence""",2023,2023-11-01T20:39:12Z,,,
arXIv2023,"An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach",No.,1,"""No evidence""",2023,2023-11-01T18:41:50Z,,,
arXIv2023,From Text to Structure: Using Large Language Models to Support the Development of Legal Expert Systems,No.,1,"""No evidence""",2023,2023-11-01T18:31:02Z,,,
arXIv2023,Language Model Training Paradigms for Clinical Feature Embeddings,No.,1,"""No evidence""",2023,2023-11-01T18:23:12Z,,,
arXIv2023,Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task,No.,1,"""No evidence""",2023,2023-11-01T17:44:35Z,,,
arXIv2023,Emotion Detection for Misinformation: A Review,No.,1,"""No evidence""",2023,2023-11-01T17:21:09Z,,,
arXIv2023,De-Diffusion Makes Text a Strong Cross-Modal Interface,No.,1,"""No evidence""",2023,2023-11-01T16:12:40Z,,,
arXIv2023,The Development of LLMs for Embodied Navigation,No.,1,"""No evidence""",2023,2023-11-01T14:08:56Z,,,
arXIv2023,Efficient LLM Inference on CPUs,No.,1,"""No evidence""",2023,2023-11-01T13:08:50Z,,,
arXIv2023,Comparing Optimization Targets for Contrast-Consistent Search,No.,1,"""No evidence""",2023,2023-11-01T12:42:14Z,,,
arXIv2023,CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection,No.,1,"""No evidence""",2023,2023-11-01T11:39:22Z,,,
arXIv2023,On the Opportunities of Green Computing: A Survey,No.,1,"""No evidence""",2023,2023-11-01T11:16:41Z,,,
arXIv2023,Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements,No.,1,"""No evidence""",2023,2023-11-01T11:12:02Z,,,
arXIv2023,Efficient Human-AI Coordination via Preparatory Language-based Convention,No.,1,"""No evidence""",2023,2023-11-01T10:18:23Z,,,
arXIv2023,AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification,No.,1,"""No evidence""",2023,2023-11-01T10:00:15Z,,,
arXIv2023,Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition,No.,1,"""No evidence""",2023,2023-11-01T08:38:08Z,,,
arXIv2023,Space Narrative: Generating Images and 3D Scenes of Chinese Garden from Text using Deep Learning,No.,1,"""No evidence""",2023,2023-11-01T07:16:01Z,,,
arXIv2023,HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning,No.,1,"""No evidence""",2023,2023-11-01T06:09:54Z,,,
arXIv2023,Unsupervised Lexical Simplification with Context Augmentation,No.,1,"""No evidence""",2023,2023-11-01T05:48:05Z,,,
arXIv2023,Entity Alignment Method of Science and Technology Patent based on Graph Convolution Network and Information Fusion,No.,1,"""No evidence""",2023,2023-11-01T05:04:55Z,,,
arXIv2023,Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks,No.,1,"""No evidence""",2023,2023-11-01T04:40:05Z,,,
arXIv2023,Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models,No.,1,"""No evidence""",2023,2023-11-01T04:37:28Z,,,
arXIv2023,JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models,No.,1,"""No evidence""",2023,2023-11-01T04:36:45Z,,,
arXIv2023,"SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations",No.,1,"""No evidence""",2023,2023-11-01T03:49:52Z,,,
arXIv2023,ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation,No.,1,"""No evidence""",2023,2023-11-01T03:46:36Z,,,
arXIv2023,Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?,No.,1,"""No evidence""",2023,2023-11-01T03:32:46Z,,,
arXIv2023,Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents,No.,1,"""No evidence""",2023,2023-11-01T03:20:16Z,,,
arXIv2023,Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis,No.,1,"""No evidence""",2023,2023-11-01T03:15:05Z,,,
arXIv2023,The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis,No.,1,"""No evidence""",2023,2023-11-01T02:40:42Z,,,
arXIv2023,Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering,No.,1,"""No evidence""",2023,2023-11-01T00:18:00Z,,,
arXIv2023,ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection,No.,1,"""No evidence""",2023,2023-11-01T00:17:37Z,,,
arXIv2023,Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities,No.,1,"""No evidence""",2023,2023-11-01T00:17:11Z,,,
arXIv2023,ChipNeMo: Domain-Adapted LLMs for Chip Design,No.,1,"""No evidence""",2023,2023-10-31T22:35:58Z,,,
arXIv2023,Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield,No.,1,"""No evidence""",2023,2023-10-31T22:22:10Z,,,
arXIv2023,Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data,No.,1,"""No evidence""",2023,2023-10-31T20:17:32Z,,,
arXIv2023,Farthest Greedy Path Sampling for Two-shot Recommender Search,No.,1,"""No evidence""",2023,2023-10-31T17:59:14Z,,,
arXIv2023,Learning From Mistakes Makes LLM Better Reasoner,No.,1,"""No evidence""",2023,2023-10-31T17:52:22Z,,,
arXIv2023,Defining a New NLP Playground,No.,1,"""No evidence""",2023,2023-10-31T17:02:33Z,,,
arXIv2023,Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building,No.,1,"""No evidence""",2023,2023-10-31T16:26:36Z,,,
arXIv2023,CapsFusion: Rethinking Image-Text Data at Scale,No.,1,"""No evidence""",2023,2023-10-31T15:31:39Z,,,
arXIv2023,Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models,No.,1,"""No evidence""",2023,2023-10-31T14:37:42Z,,,
arXIv2023,Critical Role of Artificially Intelligent Conversational Chatbot,No.,1,"""No evidence""",2023,2023-10-31T14:08:07Z,,,
arXIv2023,The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models,No.,1,"""No evidence""",2023,2023-10-31T13:22:38Z,,,
arXIv2023,Do large language models solve verbal analogies like children do?,No.,1,"""No evidence""",2023,2023-10-31T11:49:11Z,,,
arXIv2023,A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis,No.,1,"""No evidence""",2023,2023-10-31T11:39:09Z,,,
arXIv2023,Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model,No.,1,"""No evidence""",2023,2023-10-31T10:57:35Z,,,
arXIv2023,SemanticBoost: Elevating Motion Generation with Augmented Textual Cues,No.,1,"""No evidence""",2023,2023-10-31T09:58:11Z,,,
arXIv2023,FA Team at the NTCIR-17 UFO Task,No.,1,"""No evidence""",2023,2023-10-31T09:56:12Z,,,
arXIv2023,Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests,No.,1,"""No evidence""",2023,2023-10-31T09:55:07Z,,,
arXIv2023,PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection,No.,1,"""No evidence""",2023,2023-10-31T08:23:33Z,,,
arXIv2023,Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations,No.,1,"""No evidence""",2023,2023-10-31T08:09:20Z,,,
arXIv2023,VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision,No.,1,"""No evidence""",2023,2023-10-31T06:56:51Z,,,
arXIv2023,Does GPT-4 Pass the Turing Test?,No.,1,"""No evidence""",2023,2023-10-31T06:27:52Z,,,
arXIv2023,Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer,No.,1,"""No evidence""",2023,2023-10-31T04:40:20Z,,,
arXIv2023,DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text,No.,1,"""No evidence""",2023,2023-10-31T04:37:57Z,,,
arXIv2023,Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts,No.,1,"""No evidence""",2023,2023-10-31T03:54:11Z,,,
arXIv2023,GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval,No.,1,"""No evidence""",2023,2023-10-31T03:52:08Z,,,
arXIv2023,Multi-Agent Consensus Seeking via Large Language Models,No.,1,"""No evidence""",2023,2023-10-31T03:37:11Z,,,
arXIv2023,Unlearn What You Want to Forget: Efficient Unlearning for LLMs,No.,1,"""No evidence""",2023,2023-10-31T03:35:59Z,,,
arXIv2023,EELBERT: Tiny Models through Dynamic Embeddings,No.,1,"""No evidence""",2023,2023-10-31T03:28:08Z,,,
arXIv2023,DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models,No.,1,"""No evidence""",2023,2023-10-31T03:09:36Z,,,
arXIv2023,Making Large Language Models Better Data Creators,No.,1,"""No evidence""",2023,2023-10-31T01:08:34Z,,,
arXIv2023,Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models,No.,1,"""No evidence""",2023,2023-10-31T00:56:33Z,,,
arXIv2023,Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics,No.,1,"""No evidence""",2023,2023-10-31T00:51:14Z,,,
arXIv2023,Partial Tensorized Transformers for Natural Language Processing,No.,1,"""No evidence""",2023,2023-10-30T23:19:06Z,,,
arXIv2023,Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection,No.,1,"""No evidence""",2023,2023-10-30T22:03:55Z,,,
arXIv2023,Leveraging Language Models to Detect Greenwashing,No.,1,"""No evidence""",2023,2023-10-30T21:41:49Z,,,
arXIv2023,Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design,No.,1,"""No evidence""",2023,2023-10-30T20:31:50Z,,,
arXIv2023,Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning,No.,1,"""No evidence""",2023,2023-10-30T20:26:12Z,,,
arXIv2023,"'Person' == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion",No.,1,"""No evidence""",2023,2023-10-30T19:57:01Z,,,
arXIv2023,BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing,No.,1,"""No evidence""",2023,2023-10-30T19:38:50Z,,,
arXIv2023,Remember what you did so you know what to do next,No.,1,"""No evidence""",2023,2023-10-30T19:29:00Z,,,
arXIv2023,The Impact of Depth and Width on Transformer Language Model Generalization,No.,1,"""No evidence""",2023,2023-10-30T19:10:06Z,,,
arXIv2023,Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications,No.,1,"""No evidence""",2023,2023-10-30T18:57:28Z,,,
arXIv2023,Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents,No.,1,"""No evidence""",2023,2023-10-30T18:35:30Z,,,
arXIv2023,GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models,No.,1,"""No evidence""",2023,2023-10-30T18:28:50Z,,,
arXIv2023,"Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer",No.,1,"""No evidence""",2023,2023-10-30T18:11:02Z,,,
arXIv2023,BTRec: BERT-Based Trajectory Recommendation for Personalized Tours,No.,1,"""No evidence""",2023,2023-10-30T18:00:26Z,,,
arXIv2023,The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics,No.,1,"""No evidence""",2023,2023-10-30T17:55:08Z,,,
arXIv2023,LILO: Learning Interpretable Libraries by Compressing and Documenting Code,No.,1,"""No evidence""",2023,2023-10-30T17:55:02Z,,,
arXIv2023,MM-VID: Advancing Video Understanding with GPT-4V(ision),No.,1,"""No evidence""",2023,2023-10-30T17:44:09Z,,,
arXIv2023,Generating Medical Prescriptions with Conditional Transformer,No.,1,"""No evidence""",2023,2023-10-30T16:53:11Z,,,
arXIv2023,Integrating Pre-trained Language Model into Neural Machine Translation,No.,1,"""No evidence""",2023,2023-10-30T16:00:13Z,,,
arXIv2023,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,No.,1,"""No evidence""",2023,2023-10-30T15:57:32Z,,,
arXIv2023,Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding,No.,1,"""No evidence""",2023,2023-10-30T15:51:04Z,,,
arXIv2023,DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization,No.,1,"""No evidence""",2023,2023-10-30T15:50:56Z,,,
arXIv2023,Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck,No.,1,"""No evidence""",2023,2023-10-30T15:41:32Z,,,
arXIv2023,Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace,No.,1,"""No evidence""",2023,2023-10-30T15:37:10Z,,,
arXIv2023,Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities,No.,1,"""No evidence""",2023,2023-10-30T15:19:15Z,,,
arXIv2023,Large Trajectory Models are Scalable Motion Predictors and Planners,No.,1,"""No evidence""",2023,2023-10-30T15:12:41Z,,,
arXIv2023,Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models,No.,1,"""No evidence""",2023,2023-10-30T15:12:09Z,,,
arXIv2023,LLMaAA: Making Large Language Models as Active Annotators,No.,1,"""No evidence""",2023,2023-10-30T14:54:15Z,,,
arXIv2023,Improving Input-label Mapping with Demonstration Replay for In-context Learning,No.,1,"""No evidence""",2023,2023-10-30T14:29:41Z,,,
arXIv2023,"Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination",No.,1,"""No evidence""",2023,2023-10-30T12:51:52Z,,,
arXIv2023,Constituency Parsing using LLMs,No.,1,"""No evidence""",2023,2023-10-30T11:39:11Z,,,
arXIv2023,Denoising Diffusion Probabilistic Models for Hardware-Impaired Communication Systems: Towards Wireless Generative AI,No.,1,"""No evidence""",2023,2023-10-30T11:33:01Z,,,
arXIv2023,A Lightweight Method to Generate Unanswerable Questions in English,No.,1,"""No evidence""",2023,2023-10-30T10:14:52Z,,,
arXIv2023,Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction,No.,1,"""No evidence""",2023,2023-10-30T09:00:05Z,,,
arXIv2023,Skywork: A More Open Bilingual Foundation Model,No.,1,"""No evidence""",2023,2023-10-30T08:31:47Z,,,
arXIv2023,Musical Form Generation,No.,1,"""No evidence""",2023,2023-10-30T08:02:08Z,,,
arXIv2023,ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense,No.,1,"""No evidence""",2023,2023-10-30T06:35:37Z,,,
arXIv2023,Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering,No.,1,"""No evidence""",2023,2023-10-30T06:12:50Z,,,
arXIv2023,Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective,No.,1,"""No evidence""",2023,2023-10-30T02:25:21Z,,,
arXIv2023,Adapter Pruning using Tropical Characterization,No.,1,"""No evidence""",2023,2023-10-30T02:20:44Z,,,
arXIv2023,EHRTutor: Enhancing Patient Understanding of Discharge Instructions,No.,1,"""No evidence""",2023,2023-10-30T00:46:03Z,,,
arXIv2023,LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses,No.,1,"""No evidence""",2023,2023-10-30T00:30:34Z,,,
arXIv2023,Leveraging generative artificial intelligence to simulate student learning behavior,No.,1,"""No evidence""",2023,2023-10-30T00:09:59Z,,,
arXIv2023,Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing,No.,1,"""No evidence""",2023,2023-10-30T00:01:51Z,,,
arXIv2023,Robustifying Language Models with Test-Time Adaptation,No.,1,"""No evidence""",2023,2023-10-29T22:37:54Z,,,
arXIv2023,BERT Lost Patience Won't Be Robust to Adversarial Slowdown,No.,1,"""No evidence""",2023,2023-10-29T21:06:34Z,,,
arXIv2023,Unified Representation for Non-compositional and Compositional Expressions,No.,1,"""No evidence""",2023,2023-10-29T19:28:22Z,,,
arXIv2023,PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators,No.,1,"""No evidence""",2023,2023-10-29T18:43:19Z,,,
arXIv2023,Atom: Low-bit Quantization for Efficient and Accurate LLM Serving,No.,1,"""No evidence""",2023,2023-10-29T18:33:05Z,,,
arXIv2023,Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention,No.,1,"""No evidence""",2023,2023-10-29T17:16:40Z,,,
arXIv2023,Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery,No.,1,"""No evidence""",2023,2023-10-29T17:03:12Z,,,
arXIv2023,Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection,No.,1,"""No evidence""",2023,2023-10-29T16:49:45Z,,,
arXIv2023,Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V,No.,1,"""No evidence""",2023,2023-10-29T16:26:28Z,,,
arXIv2023,TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding,No.,1,"""No evidence""",2023,2023-10-29T16:25:32Z,,,
arXIv2023,A Unique Training Strategy to Enhance Language Models Capabilities for Health Mention Detection from Social Media Content,No.,1,"""No evidence""",2023,2023-10-29T16:08:33Z,,,
arXIv2023,"TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise",No.,1,"""No evidence""",2023,2023-10-29T14:16:54Z,,,
arXIv2023,Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders,No.,1,"""No evidence""",2023,2023-10-29T12:27:18Z,,,
arXIv2023,EtiCor: Corpus for Analyzing LLMs for Etiquettes,No.,1,"""No evidence""",2023,2023-10-29T10:47:23Z,,,
arXIv2023,LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection,No.,1,"""No evidence""",2023,2023-10-29T10:07:32Z,,,
arXIv2023,Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game,No.,1,"""No evidence""",2023,2023-10-29T09:02:57Z,,,
arXIv2023,Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning,No.,1,"""No evidence""",2023,2023-10-29T07:43:34Z,,,
arXIv2023,Debiasing Algorithm through Model Adaptation,No.,1,"""No evidence""",2023,2023-10-29T05:50:03Z,,,
arXIv2023,Prompt-Engineering and Transformer-based Question Generation and Evaluation,No.,1,"""No evidence""",2023,2023-10-29T01:45:30Z,,,
arXIv2023,Counterfactually Probing Language Identity in Multilingual Models,No.,1,"""No evidence""",2023,2023-10-29T01:21:36Z,,,
arXIv2023,Automating the Correctness Assessment of AI-generated Code for Security Contexts,No.,1,"""No evidence""",2023,2023-10-28T22:28:32Z,,,
arXIv2023,Translating away Translationese without Parallel Data,No.,1,"""No evidence""",2023,2023-10-28T22:11:25Z,,,
arXIv2023,The Synergy of Speculative Decoding and Batching in Serving Large Language Models,No.,1,"""No evidence""",2023,2023-10-28T20:36:36Z,,,
arXIv2023,ProMap: Effective Bilingual Lexicon Induction via Language Model Prompting,No.,1,"""No evidence""",2023,2023-10-28T18:33:24Z,,,
arXIv2023,Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in News Reporting,No.,1,"""No evidence""",2023,2023-10-28T17:50:13Z,,,
arXIv2023,Emotion-Oriented Behavior Model Using Deep Learning,No.,1,"""No evidence""",2023,2023-10-28T17:27:59Z,,,
arXIv2023,"Reboost Large Language Model-based Text-to-SQL, Text-to-Python, and Text-to-Function -- with Real Applications in Traffic Domain",No.,1,"""No evidence""",2023,2023-10-28T16:32:40Z,,,
arXIv2023,TLM: Token-Level Masking for Transformers,No.,1,"""No evidence""",2023,2023-10-28T15:42:47Z,,,
arXIv2023,Using Large Language Models to Support Thematic Analysis in Empirical Legal Studies,No.,1,"""No evidence""",2023,2023-10-28T15:20:44Z,,,
arXIv2023,Probing LLMs for Joint Encoding of Linguistic Categories,No.,1,"""No evidence""",2023,2023-10-28T12:46:40Z,,,
arXIv2023,From Indeterminacy to Determinacy: Augmenting Logical Reasoning Capabilities with Large Language Models,No.,1,"""No evidence""",2023,2023-10-28T10:05:51Z,,,
arXIv2023,SSL Framework for Causal Inconsistency between Structures and Representations,No.,1,"""No evidence""",2023,2023-10-28T08:29:49Z,,,
arXIv2023,Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation,No.,1,"""No evidence""",2023,2023-10-28T07:54:39Z,,,
arXIv2023,Efficient kernel surrogates for neural network-based regression,No.,1,"""No evidence""",2023,2023-10-28T06:41:47Z,,,
arXIv2023,Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with LITE,No.,1,"""No evidence""",2023,2023-10-28T04:07:58Z,,,
arXIv2023,Punica: Multi-Tenant LoRA Serving,No.,1,"""No evidence""",2023,2023-10-28T00:33:37Z,,,
arXIv2023,Identifying Conspiracy Theories News based on Event Relation Graph,No.,1,"""No evidence""",2023,2023-10-28T00:27:21Z,,,
arXIv2023,Preventing Language Models From Hiding Their Reasoning,No.,1,"""No evidence""",2023,2023-10-27T22:02:29Z,,,
arXIv2023,GPT-4 Vision on Medical Image Classification -- A Case Study on COVID-19 Dataset,No.,1,"""No evidence""",2023,2023-10-27T21:28:36Z,,,
arXIv2023,LLMSTEP: LLM proofstep suggestions in Lean,No.,1,"""No evidence""",2023,2023-10-27T20:10:56Z,,,
arXIv2023,Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement,No.,1,"""No evidence""",2023,2023-10-27T19:27:59Z,,,
arXIv2023,FP8-LM: Training FP8 Large Language Models,No.,1,"""No evidence""",2023,2023-10-27T17:59:51Z,,,
arXIv2023,Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models,No.,1,"""No evidence""",2023,2023-10-27T17:55:32Z,,,
arXIv2023,Image Clustering Conditioned on Text Criteria,No.,1,"""No evidence""",2023,2023-10-27T17:35:01Z,,,
arXIv2023,Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation,No.,1,"""No evidence""",2023,2023-10-27T16:20:10Z,,,
arXIv2023,INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System,No.,1,"""No evidence""",2023,2023-10-27T15:31:16Z,,,
arXIv2023,"Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media",No.,1,"""No evidence""",2023,2023-10-27T15:28:12Z,,,
arXIv2023,Style Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN,No.,1,"""No evidence""",2023,2023-10-27T14:28:41Z,,,
arXIv2023,Personas as a Way to Model Truthfulness in Language Models,No.,1,"""No evidence""",2023,2023-10-27T14:27:43Z,,,
arXIv2023,MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension,No.,1,"""No evidence""",2023,2023-10-27T14:24:06Z,,,
arXIv2023,Generative AI for Software Metadata: Overview of the Information Retrieval in Software Engineering Track at FIRE 2023,No.,1,"""No evidence""",2023,2023-10-27T14:13:23Z,,,
arXIv2023,Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs,No.,1,"""No evidence""",2023,2023-10-27T14:00:04Z,,,
arXIv2023,"Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models",No.,1,"""No evidence""",2023,2023-10-27T13:19:19Z,,,
arXIv2023,OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization,No.,1,"""No evidence""",2023,2023-10-27T13:09:54Z,,,
arXIv2023,Knowledge Corpus Error in Question Answering,No.,1,"""No evidence""",2023,2023-10-27T11:44:06Z,,,
arXIv2023,DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking,No.,1,"""No evidence""",2023,2023-10-27T11:43:46Z,,,
arXIv2023,On General Language Understanding,No.,1,"""No evidence""",2023,2023-10-27T10:36:54Z,,,
arXIv2023,Large language models for aspect-based sentiment analysis,No.,1,"""No evidence""",2023,2023-10-27T10:03:21Z,,,
arXIv2023,OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for Offensive Language Identification,No.,1,"""No evidence""",2023,2023-10-27T09:59:35Z,,,
arXIv2023,SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis,No.,1,"""No evidence""",2023,2023-10-27T09:59:24Z,,,
arXIv2023,InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews,No.,1,"""No evidence""",2023,2023-10-27T08:42:18Z,,,
arXIv2023,Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare,No.,1,"""No evidence""",2023,2023-10-27T08:05:21Z,,,
arXIv2023,ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation,No.,1,"""No evidence""",2023,2023-10-27T03:39:51Z,,,
arXIv2023,TarGEN: Targeted Data Generation with Large Language Models,No.,1,"""No evidence""",2023,2023-10-27T03:32:17Z,,,
arXIv2023,Siamese-DETR for Generic Multi-Object Tracking,No.,1,"""No evidence""",2023,2023-10-27T03:32:05Z,,,
arXIv2023,From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models,No.,1,"""No evidence""",2023,2023-10-27T02:18:10Z,,,
arXIv2023,Real-time Animation Generation and Control on Rigged Models via Large Language Models,No.,1,"""No evidence""",2023,2023-10-27T01:36:35Z,,,
arXIv2023,Large-scale Foundation Models and Generative AI for BigData Neuroscience,No.,1,"""No evidence""",2023,2023-10-27T00:44:40Z,,,
arXIv2023,Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting,No.,1,"""No evidence""",2023,2023-10-26T23:06:38Z,,,
arXIv2023,Clover: Closed-Loop Verifiable Code Generation,No.,1,"""No evidence""",2023,2023-10-26T22:58:19Z,,,
arXIv2023,Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks,No.,1,"""No evidence""",2023,2023-10-26T22:40:30Z,,,
arXIv2023,ControlLLM: Augment Language Models with Tools by Searching on Graphs,No.,1,"""No evidence""",2023,2023-10-26T21:57:21Z,,,
arXIv2023,"Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?",No.,1,"""No evidence""",2023,2023-10-26T20:55:29Z,,,
arXIv2023,PockEngine: Sparse and Efficient Fine-tuning in a Pocket,No.,1,"""No evidence""",2023,2023-10-26T19:46:11Z,,,
arXIv2023,BERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in Time-series Load Profiles,No.,1,"""No evidence""",2023,2023-10-26T19:30:31Z,,,
arXIv2023,ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers,No.,1,"""No evidence""",2023,2023-10-26T18:34:41Z,,,
arXIv2023,Large Language Models as Generalizable Policies for Embodied Tasks,No.,1,"""No evidence""",2023,2023-10-26T18:32:05Z,,,
arXIv2023,From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI,No.,1,"""No evidence""",2023,2023-10-26T18:30:37Z,,,
arXIv2023,Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents,No.,1,"""No evidence""",2023,2023-10-26T18:19:56Z,,,
arXIv2023,Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term,No.,1,"""No evidence""",2023,2023-10-26T18:12:02Z,,,
arXIv2023,The impact of responding to patient messages with large language model assistance,No.,1,"""No evidence""",2023,2023-10-26T18:03:46Z,,,
arXIv2023,"torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP",No.,1,"""No evidence""",2023,2023-10-26T17:57:15Z,,,
arXIv2023,In-Context Learning Dynamics with Random Binary Sequences,No.,1,"""No evidence""",2023,2023-10-26T17:54:52Z,,,
arXIv2023,JudgeLM: Fine-tuned Large Language Models are Scalable Judges,No.,1,"""No evidence""",2023,2023-10-26T17:48:58Z,,,
arXIv2023,InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators,No.,1,"""No evidence""",2023,2023-10-26T17:48:45Z,,,
arXIv2023,Lil-Bevo: Explorations of Strategies for Training Language Models in More Humanlike Ways,No.,1,"""No evidence""",2023,2023-10-26T17:13:07Z,,,
arXIv2023,Can LLMs Grade Short-answer Reading Comprehension Questions : Foundational Literacy Assessment in LMICs,No.,1,"""No evidence""",2023,2023-10-26T17:05:40Z,,,
arXIv2023,Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models,No.,1,"""No evidence""",2023,2023-10-26T16:55:05Z,,,
arXIv2023,Interactive Robot Learning from Verbal Correction,No.,1,"""No evidence""",2023,2023-10-26T16:46:12Z,,,
arXIv2023,Unpacking the Ethical Value Alignment in Big Models,No.,1,"""No evidence""",2023,2023-10-26T16:45:40Z,,,
arXIv2023,The Expressive Power of Low-Rank Adaptation,No.,1,"""No evidence""",2023,2023-10-26T16:08:33Z,,,
arXIv2023,CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents,No.,1,"""No evidence""",2023,2023-10-26T16:06:20Z,,,
arXIv2023,"FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing",No.,1,"""No evidence""",2023,2023-10-26T15:47:44Z,,,
arXIv2023,LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation,No.,1,"""No evidence""",2023,2023-10-26T15:44:57Z,,,
arXIv2023,''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text,No.,1,"""No evidence""",2023,2023-10-26T14:34:06Z,,,
arXIv2023,PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications,No.,1,"""No evidence""",2023,2023-10-26T14:20:44Z,,,
arXIv2023,Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases,No.,1,"""No evidence""",2023,2023-10-26T14:19:48Z,,,
arXIv2023,Meaning and understanding in large language models,No.,1,"""No evidence""",2023,2023-10-26T14:06:14Z,,,
arXIv2023,Dialogue-based generation of self-driving simulation scenarios using Large Language Models,No.,1,"""No evidence""",2023,2023-10-26T13:07:01Z,,,
arXIv2023,ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought,No.,1,"""No evidence""",2023,2023-10-26T12:16:25Z,,,
arXIv2023,Arabic Fine-Grained Entity Recognition,No.,1,"""No evidence""",2023,2023-10-26T11:59:45Z,,,
arXIv2023,An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text,No.,1,"""No evidence""",2023,2023-10-26T11:17:03Z,,,
arXIv2023,In-Context Ability Transfer for Question Decomposition in Complex QA,No.,1,"""No evidence""",2023,2023-10-26T11:11:07Z,,,
arXIv2023,TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World,No.,1,"""No evidence""",2023,2023-10-26T08:27:36Z,,,
arXIv2023,Beyond MLE: Convex Learning for Text Generation,No.,1,"""No evidence""",2023,2023-10-26T08:08:43Z,,,
arXIv2023,How do Language Models Bind Entities in Context?,No.,1,"""No evidence""",2023,2023-10-26T07:10:31Z,,,
arXIv2023,MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and Decoder,No.,1,"""No evidence""",2023,2023-10-26T05:49:44Z,,,
arXIv2023,Content-based Controls For Music Large Language Modeling,No.,1,"""No evidence""",2023,2023-10-26T05:24:38Z,,,
arXIv2023,Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time,No.,1,"""No evidence""",2023,2023-10-26T05:01:09Z,,,
arXIv2023,Techniques for supercharging academic writing with generative AI,No.,1,"""No evidence""",2023,2023-10-26T04:35:00Z,,,
arXIv2023,M2C: Towards Automatic Multimodal Manga Complement,No.,1,"""No evidence""",2023,2023-10-26T04:10:16Z,,,
arXIv2023,Test-time Augmentation for Factual Probing,No.,1,"""No evidence""",2023,2023-10-26T03:41:32Z,,,
arXIv2023,LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs?,No.,1,"""No evidence""",2023,2023-10-26T02:37:43Z,,,
arXIv2023,netFound: Foundation Model for Network Security,No.,1,"""No evidence""",2023,2023-10-25T22:04:57Z,,,
arXIv2023,Controlled Decoding from Language Models,No.,1,"""No evidence""",2023,2023-10-25T22:00:05Z,,,
arXIv2023,Conditionally Combining Robot Skills using Large Language Models,No.,1,"""No evidence""",2023,2023-10-25T21:46:34Z,,,
arXIv2023,An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives,No.,1,"""No evidence""",2023,2023-10-25T21:37:57Z,,,
arXIv2023,Data Augmentation for Emotion Detection in Small Imbalanced Text Data,No.,1,"""No evidence""",2023,2023-10-25T21:29:36Z,,,
arXIv2023,How well can machine-generated texts be identified and can language models be trained to avoid identification?,No.,1,"""No evidence""",2023,2023-10-25T20:43:07Z,,,
arXIv2023,Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation,No.,1,"""No evidence""",2023,2023-10-25T20:05:07Z,,,
arXIv2023,Privately Aligning Language Models with Reinforcement Learning,No.,1,"""No evidence""",2023,2023-10-25T19:58:51Z,,,
arXIv2023,Transferring a molecular foundation model for polymer property predictions,No.,1,"""No evidence""",2023,2023-10-25T19:55:00Z,,,
arXIv2023,Muslim-Violence Bias Persists in Debiased GPT Models,No.,1,"""No evidence""",2023,2023-10-25T19:39:58Z,,,
arXIv2023,Zephyr: Direct Distillation of LM Alignment,No.,1,"""No evidence""",2023,2023-10-25T19:25:16Z,,,
arXIv2023,Learning Transfers over Several Programming Languages,No.,1,"""No evidence""",2023,2023-10-25T19:04:33Z,,,
arXIv2023,Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,No.,1,"""No evidence""",2023,2023-10-25T17:59:12Z,,,
arXIv2023,QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models,No.,1,"""No evidence""",2023,2023-10-25T17:24:53Z,,,
arXIv2023,The Simplest Inflationary Potentials,No.,1,"""No evidence""",2023,2023-10-25T17:20:19Z,,,
arXIv2023,DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection,No.,1,"""No evidence""",2023,2023-10-25T17:06:42Z,,,
arXIv2023,HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis,No.,1,"""No evidence""",2023,2023-10-25T16:23:17Z,,,
arXIv2023,Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning,No.,1,"""No evidence""",2023,2023-10-25T16:00:47Z,,,
arXIv2023,SkyMath: Technical Report,No.,1,"""No evidence""",2023,2023-10-25T15:34:55Z,,,
arXIv2023,LLM Performance Predictors are good initializers for Architecture Search,No.,1,"""No evidence""",2023,2023-10-25T15:34:30Z,,,
arXIv2023,Detection of news written by the ChatGPT through authorship attribution performed by a Bidirectional LSTM model,No.,1,"""No evidence""",2023,2023-10-25T14:48:58Z,,,
arXIv2023,BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?,No.,1,"""No evidence""",2023,2023-10-25T14:45:48Z,,,
arXIv2023,Towards Control-Centric Representations in Reinforcement Learning from Images,No.,1,"""No evidence""",2023,2023-10-25T14:09:53Z,,,
arXIv2023,ChatGPT is a Potential Zero-Shot Dependency Parser,No.,1,"""No evidence""",2023,2023-10-25T14:08:39Z,,,
arXIv2023,EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition,No.,1,"""No evidence""",2023,2023-10-25T13:43:36Z,,,
arXIv2023,Will releasing the weights of future large language models grant widespread access to pandemic agents?,No.,1,"""No evidence""",2023,2023-10-25T13:43:16Z,,,
arXIv2023,$\mathbb{VD}$-$\mathbb{GR}$: Boosting $\mathbb{V}$isual $\mathbb{D}$ialog with Cascaded Spatial-Temporal Multi-Modal $\mathbb{GR}$aphs,No.,1,"""No evidence""",2023,2023-10-25T12:25:53Z,,,
arXIv2023,FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning,No.,1,"""No evidence""",2023,2023-10-25T10:35:09Z,,,
arXIv2023,Lang3DSG: Language-based contrastive pre-training for 3D Scene Graph prediction,No.,1,"""No evidence""",2023,2023-10-25T09:26:16Z,,,
arXIv2023,A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP,No.,1,"""No evidence""",2023,2023-10-25T09:13:19Z,,,
arXIv2023,"Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",No.,1,"""No evidence""",2023,2023-10-25T09:09:55Z,,,
arXIv2023,Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph,No.,1,"""No evidence""",2023,2023-10-25T08:14:49Z,,,
arXIv2023,DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models,No.,1,"""No evidence""",2023,2023-10-25T08:03:10Z,,,
arXIv2023,PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization,No.,1,"""No evidence""",2023,2023-10-25T07:47:01Z,,,
arXIv2023,Decoding Stumpers: Large Language Models vs. Human Problem-Solvers,No.,1,"""No evidence""",2023,2023-10-25T06:54:39Z,,,
arXIv2023,LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking,No.,1,"""No evidence""",2023,2023-10-25T06:23:48Z,,,
arXIv2023,ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters,No.,1,"""No evidence""",2023,2023-10-25T06:22:29Z,,,
arXIv2023,General Point Model with Autoencoding and Autoregressive,No.,1,"""No evidence""",2023,2023-10-25T06:08:24Z,,,
arXIv2023,Evaluating General-Purpose AI with Psychometrics,No.,1,"""No evidence""",2023,2023-10-25T05:38:38Z,,,
arXIv2023,Transformer-based Live Update Generation for Soccer Matches from Microblog Posts,No.,1,"""No evidence""",2023,2023-10-25T05:12:35Z,,,
arXIv2023,InstructPTS: Instruction-Tuning LLMs for Product Title Summarization,No.,1,"""No evidence""",2023,2023-10-25T04:56:07Z,,,
arXIv2023,RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs,No.,1,"""No evidence""",2023,2023-10-25T04:32:35Z,,,
arXIv2023,Unraveling Feature Extraction Mechanisms in Neural Networks,No.,1,"""No evidence""",2023,2023-10-25T04:22:40Z,,,
arXIv2023,RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-10-25T03:53:31Z,,,
arXIv2023,URL-BERT: Training Webpage Representations via Social Media Engagements,No.,1,"""No evidence""",2023,2023-10-25T02:22:50Z,,,
arXIv2023,Is ChatGPT a Good Multi-Party Conversation Solver?,No.,1,"""No evidence""",2023,2023-10-25T02:18:40Z,,,
arXIv2023,XFEVER: Exploring Fact Verification across Languages,No.,1,"""No evidence""",2023,2023-10-25T01:20:17Z,,,
arXIv2023,Using GPT-4 to Augment Unbalanced Data for Automatic Scoring,No.,1,"""No evidence""",2023,2023-10-25T01:07:50Z,,,
arXIv2023,CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment,No.,1,"""No evidence""",2023,2023-10-25T01:05:03Z,,,
arXIv2023,Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism,No.,1,"""No evidence""",2023,2023-10-25T01:03:35Z,,,
arXIv2023,Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper,No.,1,"""No evidence""",2023,2023-10-25T01:01:28Z,,,
arXIv2023,The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining,No.,1,"""No evidence""",2023,2023-10-25T00:31:29Z,,,
arXIv2023,ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality,No.,1,"""No evidence""",2023,2023-10-24T23:30:17Z,,,
arXIv2023,TiC-CLIP: Continual Training of CLIP Models,No.,1,"""No evidence""",2023,2023-10-24T22:41:14Z,,,
arXIv2023,Background Summarization of Event Timelines,No.,1,"""No evidence""",2023,2023-10-24T21:30:15Z,,,
arXIv2023,BLP-2023 Task 2: Sentiment Analysis,No.,1,"""No evidence""",2023,2023-10-24T21:00:41Z,,,
arXIv2023,A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing,No.,1,"""No evidence""",2023,2023-10-24T19:33:27Z,,,
arXIv2023,AI Alignment and Social Choice: Fundamental Limitations and Policy Implications,No.,1,"""No evidence""",2023,2023-10-24T17:59:04Z,,,
arXIv2023,Woodpecker: Hallucination Correction for Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-10-24T17:58:07Z,,,
arXIv2023,WebWISE: Web Interface Control and Sequential Exploration with Large Language Models,No.,1,"""No evidence""",2023,2023-10-24T17:57:03Z,,,
arXIv2023,Instruct and Extract: Instruction Tuning for On-Demand Information Extraction,No.,1,"""No evidence""",2023,2023-10-24T17:54:25Z,,,
arXIv2023,What Algorithms can Transformers Learn? A Study in Length Generalization,No.,1,"""No evidence""",2023,2023-10-24T17:43:29Z,,,
arXIv2023,White-box Compiler Fuzzing Empowered by Large Language Models,No.,1,"""No evidence""",2023,2023-10-24T16:39:06Z,,,
arXIv2023,Dissecting In-Context Learning of Translations in GPTs,No.,1,"""No evidence""",2023,2023-10-24T16:37:18Z,,,
arXIv2023,Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning,No.,1,"""No evidence""",2023,2023-10-24T16:36:51Z,,,
arXIv2023,In-Context Learning Creates Task Vectors,No.,1,"""No evidence""",2023,2023-10-24T15:17:14Z,,,
arXIv2023,Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition,No.,1,"""No evidence""",2023,2023-10-24T15:07:35Z,,,
arXIv2023,A Contextualized Real-Time Multimodal Emotion Recognition for Conversational Agents using Graph Convolutional Networks in Reinforcement Learning,No.,1,"""No evidence""",2023,2023-10-24T14:31:17Z,,,
arXIv2023,CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought Language Prompting,No.,1,"""No evidence""",2023,2023-10-24T13:32:32Z,,,
arXIv2023,Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment,No.,1,"""No evidence""",2023,2023-10-24T13:23:57Z,,,
arXIv2023,DALE: Generative Data Augmentation for Low-Resource Legal NLP,No.,1,"""No evidence""",2023,2023-10-24T12:50:28Z,,,
arXIv2023,"MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications",No.,1,"""No evidence""",2023,2023-10-24T12:22:34Z,,,
arXIv2023,BLESS: Benchmarking Large Language Models on Sentence Simplification,No.,1,"""No evidence""",2023,2023-10-24T12:18:17Z,,,
arXIv2023,Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing Ones?,No.,1,"""No evidence""",2023,2023-10-24T12:01:11Z,,,
arXIv2023,AI-enhanced Auto-correction of Programming Exercises: How Effective is GPT-3.5?,No.,1,"""No evidence""",2023,2023-10-24T10:35:36Z,,,
arXIv2023,Learning-based Scheduling for Information Accuracy and Freshness in Wireless Networks,No.,1,"""No evidence""",2023,2023-10-24T10:31:34Z,,,
arXIv2023,Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning,No.,1,"""No evidence""",2023,2023-10-24T09:11:45Z,,,
arXIv2023,CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation,No.,1,"""No evidence""",2023,2023-10-24T08:56:49Z,,,
arXIv2023,Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression,No.,1,"""No evidence""",2023,2023-10-24T07:58:20Z,,,
arXIv2023,POE: Process of Elimination for Multiple Choice Reasoning,No.,1,"""No evidence""",2023,2023-10-24T07:38:43Z,,,
arXIv2023,"DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding",No.,1,"""No evidence""",2023,2023-10-24T06:21:34Z,,,
arXIv2023,Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation,No.,1,"""No evidence""",2023,2023-10-24T04:50:29Z,,,
arXIv2023,AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing,No.,1,"""No evidence""",2023,2023-10-24T03:15:19Z,,,
arXIv2023,"CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model",No.,1,"""No evidence""",2023,2023-10-24T03:08:58Z,,,
arXIv2023,A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models,No.,1,"""No evidence""",2023,2023-10-24T03:05:21Z,,,
arXIv2023,Interpreting Answers to Yes-No Questions in User-Generated Content,No.,1,"""No evidence""",2023,2023-10-24T02:27:06Z,,,
arXIv2023,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,No.,1,"""No evidence""",2023,2023-10-24T02:23:34Z,,,
arXIv2023,UI Layout Generation with LLMs Guided by UI Grammar,No.,1,"""No evidence""",2023,2023-10-24T02:00:12Z,,,
arXIv2023,PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers' Workflows,No.,1,"""No evidence""",2023,2023-10-24T01:04:27Z,,,
arXIv2023,Leveraging Large Language Models for Enhanced Product Descriptions in eCommerce,No.,1,"""No evidence""",2023,2023-10-24T00:55:14Z,,,
arXIv2023,What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations,No.,1,"""No evidence""",2023,2023-10-24T00:51:29Z,,,
arXIv2023,ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles,No.,1,"""No evidence""",2023,2023-10-24T00:48:48Z,,,
arXIv2023,LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery,No.,1,"""No evidence""",2023,2023-10-24T00:47:26Z,,,
arXIv2023,Health Disparities through Generative AI Models: A Comparison Study Using A Domain Specific large language model,No.,1,"""No evidence""",2023,2023-10-23T21:24:05Z,,,
arXIv2023,Specialist or Generalist? Instruction Tuning for Specific NLP Tasks,No.,1,"""No evidence""",2023,2023-10-23T19:46:48Z,,,
arXIv2023,Videoprompter: an ensemble of foundational models for zero-shot video understanding,No.,1,"""No evidence""",2023,2023-10-23T19:45:46Z,,,
arXIv2023,Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses,No.,1,"""No evidence""",2023,2023-10-23T19:35:01Z,,,
arXIv2023,Probing Representations for Document-level Event Extraction,No.,1,"""No evidence""",2023,2023-10-23T19:33:04Z,,,
arXIv2023,TaskDiff: A Similarity Metric for Task-Oriented Conversations,No.,1,"""No evidence""",2023,2023-10-23T19:03:35Z,,,
arXIv2023,DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM,No.,1,"""No evidence""",2023,2023-10-23T19:03:04Z,,,
arXIv2023,Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards,No.,1,"""No evidence""",2023,2023-10-23T18:08:01Z,,,
arXIv2023,Large Language Models are Visual Reasoning Coordinators,No.,1,"""No evidence""",2023,2023-10-23T17:59:31Z,,,
arXIv2023,LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers,No.,1,"""No evidence""",2023,2023-10-23T17:58:40Z,,,
arXIv2023,Linear Representations of Sentiment in Large Language Models,No.,1,"""No evidence""",2023,2023-10-23T17:55:31Z,,,
arXIv2023,Function Vectors in Large Language Models,No.,1,"""No evidence""",2023,2023-10-23T17:55:24Z,,,
arXIv2023,DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design,No.,1,"""No evidence""",2023,2023-10-23T17:48:38Z,,,
arXIv2023,SpecTr: Fast Speculative Decoding via Optimal Transport,No.,1,"""No evidence""",2023,2023-10-23T17:47:34Z,,,
arXIv2023,AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models,No.,1,"""No evidence""",2023,2023-10-23T17:46:07Z,,,
arXIv2023,Quantifying the Dialect Gap and its Correlates Across Languages,No.,1,"""No evidence""",2023,2023-10-23T17:42:01Z,,,
arXIv2023,Location-Aware Visual Question Generation with Lightweight Models,No.,1,"""No evidence""",2023,2023-10-23T17:33:31Z,,,
arXIv2023,Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-10-23T17:31:55Z,,,
arXIv2023,Branch-Solve-Merge Improves Large Language Model Evaluation and Generation,No.,1,"""No evidence""",2023,2023-10-23T17:29:48Z,,,
arXIv2023,Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,No.,1,"""No evidence""",2023,2023-10-23T17:21:03Z,,,
arXIv2023,GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs,No.,1,"""No evidence""",2023,2023-10-23T17:18:35Z,,,
arXIv2023,LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis,No.,1,"""No evidence""",2023,2023-10-23T17:05:59Z,,,
arXIv2023,'Don't Get Too Technical with Me': A Discourse Structure-Based Framework for Science Journalism,No.,1,"""No evidence""",2023,2023-10-23T16:35:05Z,,,
arXIv2023,TableQAKit: A Comprehensive and Practical Toolkit for Table-based Question Answering,No.,1,"""No evidence""",2023,2023-10-23T16:33:23Z,,,
arXIv2023,Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge,No.,1,"""No evidence""",2023,2023-10-23T16:14:05Z,,,
arXIv2023,Synergizing Human-AI Agency: A Guide of 23 Heuristics for Service Co-Creation with LLM-Based Agents,No.,1,"""No evidence""",2023,2023-10-23T16:11:48Z,,,
arXIv2023,TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge,No.,1,"""No evidence""",2023,2023-10-23T15:55:15Z,,,
arXIv2023,Meta- (out-of-context) learning in neural networks,No.,1,"""No evidence""",2023,2023-10-23T15:50:08Z,,,
arXIv2023,The primacy bias in Model-based RL,No.,1,"""No evidence""",2023,2023-10-23T15:12:20Z,,,
arXIv2023,Simple Hardware-Efficient PCFGs with Independent Left and Right Productions,No.,1,"""No evidence""",2023,2023-10-23T14:48:51Z,,,
arXIv2023,Understanding the Inner Workings of Language Models Through Representation Dissimilarity,No.,1,"""No evidence""",2023,2023-10-23T14:46:20Z,,,
arXIv2023,LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay,No.,1,"""No evidence""",2023,2023-10-23T14:35:26Z,,,
arXIv2023,Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation,No.,1,"""No evidence""",2023,2023-10-23T14:27:45Z,,,
arXIv2023,Unveiling A Core Linguistic Region in Large Language Models,No.,1,"""No evidence""",2023,2023-10-23T13:31:32Z,,,
arXIv2023,Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?,No.,1,"""No evidence""",2023,2023-10-23T12:51:49Z,,,
arXIv2023,BioImage.IO Chatbot: A Community-Driven AI Assistant for Advanced Bioimage Analysis and Tool Integration,No.,1,"""No evidence""",2023,2023-10-23T12:31:42Z,,,
arXIv2023,DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning,No.,1,"""No evidence""",2023,2023-10-23T11:33:41Z,,,
arXIv2023,Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities,No.,1,"""No evidence""",2023,2023-10-23T11:33:24Z,,,
arXIv2023,"Large Language Models can Share Images, Too!",No.,1,"""No evidence""",2023,2023-10-23T10:59:21Z,,,
arXIv2023,Geographical Erasure in Language Generation,No.,1,"""No evidence""",2023,2023-10-23T10:26:14Z,,,
arXIv2023,MCC-KD: Multi-CoT Consistent Knowledge Distillation,No.,1,"""No evidence""",2023,2023-10-23T09:32:53Z,,,
arXIv2023,Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review,No.,1,"""No evidence""",2023,2023-10-23T09:15:18Z,,,
arXIv2023,Generating Prototypes for Contradiction Detection Using Large Language Models and Linguistic Rules,No.,1,"""No evidence""",2023,2023-10-23T09:07:27Z,,,
arXIv2023,Strong and Efficient Baselines for Open Domain Conversational Question Answering,No.,1,"""No evidence""",2023,2023-10-23T08:48:14Z,,,
arXIv2023,Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-10-23T08:42:49Z,,,
arXIv2023,API-Assisted Code Generation for Question Answering on Varied Table Structures,No.,1,"""No evidence""",2023,2023-10-23T08:26:28Z,,,
arXIv2023,Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding,No.,1,"""No evidence""",2023,2023-10-23T08:15:38Z,,,
arXIv2023,Open-Set Image Tagging with Multi-Grained Text Supervision,No.,1,"""No evidence""",2023,2023-10-23T08:13:33Z,,,
arXIv2023,$?$-Split: A Privacy-Preserving Split Computing Framework for Cloud-Powered Generative AI,No.,1,"""No evidence""",2023,2023-10-23T07:44:04Z,,,
arXIv2023,"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",No.,1,"""No evidence""",2023,2023-10-23T07:02:20Z,,,
arXIv2023,CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks,No.,1,"""No evidence""",2023,2023-10-23T06:54:51Z,,,
arXIv2023,Generative Pre-trained Transformer for Vietnamese Community-based COVID-19 Question Answering,No.,1,"""No evidence""",2023,2023-10-23T06:14:07Z,,,
arXIv2023,Prefix-Tuning Based Unsupervised Text Style Transfer,No.,1,"""No evidence""",2023,2023-10-23T06:13:08Z,,,
arXIv2023,Learning to Correct Noisy Labels for Fine-Grained Entity Typing via Co-Prediction Prompt Tuning,No.,1,"""No evidence""",2023,2023-10-23T06:04:07Z,,,
arXIv2023,Exploring the Boundaries of GPT-4 in Radiology,No.,1,"""No evidence""",2023,2023-10-23T05:13:03Z,,,
arXIv2023,HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models,No.,1,"""No evidence""",2023,2023-10-23T04:49:09Z,,,
arXIv2023,NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation,No.,1,"""No evidence""",2023,2023-10-23T04:38:34Z,,,
arXIv2023,AlpaCare:Instruction-tuned Large Language Models for Medical Application,No.,1,"""No evidence""",2023,2023-10-23T04:22:50Z,,,
arXIv2023,Harnessing ChatGPT for thematic analysis: Are we ready?,No.,1,"""No evidence""",2023,2023-10-23T03:55:13Z,,,
arXIv2023,Evaluating Spatial Understanding of Large Language Models,No.,1,"""No evidence""",2023,2023-10-23T03:44:40Z,,,
arXIv2023,Improving Seq2Seq Grammatical Error Correction via Decoding Interventions,No.,1,"""No evidence""",2023,2023-10-23T03:36:37Z,,,
arXIv2023,PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter,No.,1,"""No evidence""",2023,2023-10-23T03:12:00Z,,,
arXIv2023,CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities,No.,1,"""No evidence""",2023,2023-10-23T02:47:27Z,,,
arXIv2023,InstructExcel: A Benchmark for Natural Language Instruction in Excel,No.,1,"""No evidence""",2023,2023-10-23T02:00:55Z,,,
arXIv2023,Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models,No.,1,"""No evidence""",2023,2023-10-23T01:47:29Z,,,
arXIv2023,"""Why Should I Review This Paper?"" Unifying Semantic, Topic, and Citation Factors for Paper-Reviewer Matching",No.,1,"""No evidence""",2023,2023-10-23T01:29:18Z,,,
arXIv2023,Attention-Enhancing Backdoor Attacks Against BERT-based Models,No.,1,"""No evidence""",2023,2023-10-23T01:24:56Z,,,
arXIv2023,DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions,No.,1,"""No evidence""",2023,2023-10-23T01:23:10Z,,,
arXIv2023,GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding,No.,1,"""No evidence""",2023,2023-10-23T01:20:01Z,,,
arXIv2023,Domain Terminology Integration into Machine Translation: Leveraging Large Language Models,No.,1,"""No evidence""",2023,2023-10-22T23:25:28Z,,,
arXIv2023,Text generation for dataset augmentation in security classification tasks,No.,1,"""No evidence""",2023,2023-10-22T22:25:14Z,,,
arXIv2023,Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation,No.,1,"""No evidence""",2023,2023-10-22T21:48:51Z,,,
arXIv2023,Vision Language Models in Autonomous Driving and Intelligent Transportation Systems,No.,1,"""No evidence""",2023,2023-10-22T21:06:10Z,,,
arXIv2023,O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models,No.,1,"""No evidence""",2023,2023-10-22T20:28:33Z,,,
arXIv2023,Merging Generated and Retrieved Knowledge for Open-Domain QA,No.,1,"""No evidence""",2023,2023-10-22T19:37:06Z,,,
arXIv2023,Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models,No.,1,"""No evidence""",2023,2023-10-22T19:12:17Z,,,
arXIv2023,MoPe: Model Perturbation-based Privacy Attacks on Language Models,No.,1,"""No evidence""",2023,2023-10-22T17:33:19Z,,,
arXIv2023,Bi-Encoders based Species Normalization -- Pairwise Sentence Learning to Rank,No.,1,"""No evidence""",2023,2023-10-22T17:30:16Z,,,
arXIv2023,Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques,No.,1,"""No evidence""",2023,2023-10-22T17:03:56Z,,,
arXIv2023,From Chaos to Clarity: Claim Normalization to Empower Fact-Checking,No.,1,"""No evidence""",2023,2023-10-22T16:07:06Z,,,
arXIv2023,ITEm: Unsupervised Image-Text Embedding Learning for eCommerce,No.,1,"""No evidence""",2023,2023-10-22T15:39:44Z,,,
arXIv2023,A Survey on Semantic Processing Techniques,No.,1,"""No evidence""",2023,2023-10-22T15:09:51Z,,,
arXIv2023,Neural Text Sanitization with Privacy Risk Indicators: An Empirical Analysis,No.,1,"""No evidence""",2023,2023-10-22T14:17:27Z,,,
arXIv2023,Boosting Unsupervised Machine Translation with Pseudo-Parallel Data,No.,1,"""No evidence""",2023,2023-10-22T10:57:12Z,,,
arXIv2023,RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and Majority Voted Fine-Tuned Transformers,No.,1,"""No evidence""",2023,2023-10-22T10:55:56Z,,,
arXIv2023,MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control,No.,1,"""No evidence""",2023,2023-10-22T08:44:26Z,,,
arXIv2023,Customising General Large Language Models for Specialised Emotion Recognition Tasks,No.,1,"""No evidence""",2023,2023-10-22T08:09:13Z,,,
arXIv2023,LUNA: A Model-Based Universal Analysis Framework for Large Language Models,No.,1,"""No evidence""",2023,2023-10-22T07:26:21Z,,,
arXIv2023,SUT: Active Defects Probing for Transcompiler Models,No.,1,"""No evidence""",2023,2023-10-22T07:16:02Z,,,
arXIv2023,Prompt Engineering Through the Lens of Optimal Control,No.,1,"""No evidence""",2023,2023-10-22T06:34:09Z,,,
arXIv2023,PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation,No.,1,"""No evidence""",2023,2023-10-22T05:43:23Z,,,
arXIv2023,An In-Context Schema Understanding Method for Knowledge Base Question Answering,No.,1,"""No evidence""",2023,2023-10-22T04:19:17Z,,,
arXIv2023,Can Language Models Laugh at YouTube Short-form Videos?,No.,1,"""No evidence""",2023,2023-10-22T03:01:38Z,,,
arXIv2023,UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web,No.,1,"""No evidence""",2023,2023-10-22T02:32:53Z,,,
arXIv2023,Orthogonal Subspace Learning for Language Model Continual Learning,No.,1,"""No evidence""",2023,2023-10-22T02:23:44Z,,,
arXIv2023,Sentiment Analysis Across Multiple African Languages: A Current Benchmark,No.,1,"""No evidence""",2023,2023-10-21T21:38:06Z,,,
arXIv2023,Zero-shot Learning of Individualized Task Contrast Prediction from Resting-state Functional Connectomes,No.,1,"""No evidence""",2023,2023-10-21T20:12:22Z,,,
arXIv2023,Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications,No.,1,"""No evidence""",2023,2023-10-21T20:04:55Z,,,
arXIv2023,"MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation",No.,1,"""No evidence""",2023,2023-10-21T18:59:41Z,,,
arXIv2023,Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning,No.,1,"""No evidence""",2023,2023-10-21T15:23:20Z,,,
arXIv2023,Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation,No.,1,"""No evidence""",2023,2023-10-21T14:35:42Z,,,
arXIv2023,RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization,No.,1,"""No evidence""",2023,2023-10-21T02:46:03Z,,,
arXIv2023,COVIDFakeExplainer: An Explainable Machine Learning based Web Application for Detecting COVID-19 Fake News,No.,1,"""No evidence""",2023,2023-10-21T02:11:39Z,,,
arXIv2023,Implications of Annotation Artifacts in Edge Probing Test Datasets,No.,1,"""No evidence""",2023,2023-10-20T23:19:35Z,,,
arXIv2023,Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing,No.,1,"""No evidence""",2023,2023-10-20T23:15:59Z,,,
arXIv2023,Ecologically Valid Explanations for Label Variation in NLI,No.,1,"""No evidence""",2023,2023-10-20T22:52:19Z,,,
arXIv2023,Foundation Model's Embedded Representations May Detect Distribution Shift,No.,1,"""No evidence""",2023,2023-10-20T22:20:50Z,,,
arXIv2023,Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT,No.,1,"""No evidence""",2023,2023-10-20T21:31:19Z,,,
arXIv2023,Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks,No.,1,"""No evidence""",2023,2023-10-20T20:17:09Z,,,
arXIv2023,Enhancing Illicit Activity Detection using XAI: A Multimodal Graph-LLM Framework,No.,1,"""No evidence""",2023,2023-10-20T19:33:44Z,,,
arXIv2023,Optimizing Retrieval-augmented Reader Models via Token Elimination,No.,1,"""No evidence""",2023,2023-10-20T17:41:36Z,,,
arXIv2023,StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models,No.,1,"""No evidence""",2023,2023-10-20T17:22:30Z,,,
arXIv2023,Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis,No.,1,"""No evidence""",2023,2023-10-20T17:13:16Z,,,
arXIv2023,Explainable Depression Symptom Detection in Social Media,No.,1,"""No evidence""",2023,2023-10-20T17:05:27Z,,,
arXIv2023,Contrastive Preference Learning: Learning from Human Feedback without RL,No.,1,"""No evidence""",2023,2023-10-20T16:37:56Z,,,
arXIv2023,Bridging Information-Theoretic and Geometric Compression in Language Models,No.,1,"""No evidence""",2023,2023-10-20T16:12:13Z,,,
arXIv2023,"The Impact of Performance Expectancy, Workload, Risk, and Satisfaction on Trust in ChatGPT: Cross-sectional Survey Analysis",No.,1,"""No evidence""",2023,2023-10-20T16:06:11Z,,,
arXIv2023,MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark,No.,1,"""No evidence""",2023,2023-10-20T15:57:17Z,,,
arXIv2023,MarineGPT: Unlocking Secrets of Ocean to the Public,No.,1,"""No evidence""",2023,2023-10-20T15:45:39Z,,,
arXIv2023,Why Can Large Language Models Generate Correct Chain-of-Thoughts?,No.,1,"""No evidence""",2023,2023-10-20T15:09:46Z,,,
arXIv2023,Cache & Distil: Optimising API Calls to Large Language Models,No.,1,"""No evidence""",2023,2023-10-20T15:01:55Z,,,
arXIv2023,Explaining Interactions Between Text Spans,No.,1,"""No evidence""",2023,2023-10-20T13:52:37Z,,,
arXIv2023,Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation,No.,1,"""No evidence""",2023,2023-10-20T13:51:08Z,,,
arXIv2023,Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning,No.,1,"""No evidence""",2023,2023-10-20T13:25:24Z,,,
arXIv2023,Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-10-20T13:14:38Z,,,
arXIv2023,Ask Language Model to Clean Your Noisy Translation Data,No.,1,"""No evidence""",2023,2023-10-20T13:05:32Z,,,
arXIv2023,WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models,No.,1,"""No evidence""",2023,2023-10-20T12:44:44Z,,,
arXIv2023,AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models,No.,1,"""No evidence""",2023,2023-10-20T11:10:14Z,,,
arXIv2023,Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations,No.,1,"""No evidence""",2023,2023-10-20T11:06:21Z,,,
arXIv2023,OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data,No.,1,"""No evidence""",2023,2023-10-20T10:12:18Z,,,
arXIv2023,Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models,No.,1,"""No evidence""",2023,2023-10-20T10:05:07Z,,,
arXIv2023,Tuna: Instruction Tuning using Feedback from Large Language Models,No.,1,"""No evidence""",2023,2023-10-20T09:55:06Z,,,
arXIv2023,Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-10-20T07:09:56Z,,,
arXIv2023,Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models,No.,1,"""No evidence""",2023,2023-10-20T07:04:08Z,,,
arXIv2023,Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting,No.,1,"""No evidence""",2023,2023-10-20T06:17:02Z,,,
arXIv2023,Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks,No.,1,"""No evidence""",2023,2023-10-20T05:44:39Z,,,
arXIv2023,SALMONN: Towards Generic Hearing Abilities for Large Language Models,No.,1,"""No evidence""",2023,2023-10-20T05:41:57Z,,,
arXIv2023,pFedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning,No.,1,"""No evidence""",2023,2023-10-20T05:24:28Z,,,
arXIv2023,Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking,No.,1,"""No evidence""",2023,2023-10-20T02:54:42Z,,,
arXIv2023,Multi-level Contrastive Learning for Script-based Character Understanding,No.,1,"""No evidence""",2023,2023-10-20T02:40:52Z,,,
arXIv2023,ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search,No.,1,"""No evidence""",2023,2023-10-20T02:24:35Z,,,
arXIv2023,Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering,No.,1,"""No evidence""",2023,2023-10-20T02:15:51Z,,,
arXIv2023,Equivariant Transformer is all you need,No.,1,"""No evidence""",2023,2023-10-20T01:57:03Z,,,
arXIv2023,In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern,No.,1,"""No evidence""",2023,2023-10-20T01:55:34Z,,,
arXIv2023,Primacy Effect of ChatGPT,No.,1,"""No evidence""",2023,2023-10-20T00:37:28Z,,,
arXIv2023,NameGuess: Column Name Expansion for Tabular Data,No.,1,"""No evidence""",2023,2023-10-19T23:11:37Z,,,
arXIv2023,The opaque law of artificial intelligence,No.,1,"""No evidence""",2023,2023-10-19T23:02:46Z,,,
arXIv2023,Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries,No.,1,"""No evidence""",2023,2023-10-19T20:02:40Z,,,
arXIv2023,Semi-Supervised Learning of Dynamical Systems with Neural Ordinary Differential Equations: A Teacher-Student Model Approach,No.,1,"""No evidence""",2023,2023-10-19T19:17:12Z,,,
arXIv2023,"No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network",No.,1,"""No evidence""",2023,2023-10-19T18:56:50Z,,,
arXIv2023,Do Language Models Learn about Legal Entity Types during Pretraining?,No.,1,"""No evidence""",2023,2023-10-19T18:47:21Z,,,
arXIv2023,Creative Robot Tool Use with Large Language Models,No.,1,"""No evidence""",2023,2023-10-19T18:02:15Z,,,
arXIv2023,HumanTOMATO: Text-aligned Whole-body Motion Generation,No.,1,"""No evidence""",2023,2023-10-19T17:59:46Z,,,
arXIv2023,Robust multimodal models have outlier features and encode more concepts,No.,1,"""No evidence""",2023,2023-10-19T17:59:12Z,,,
arXIv2023,Frozen Transformers in Language Models Are Effective Visual Encoder Layers,No.,1,"""No evidence""",2023,2023-10-19T17:59:05Z,,,
arXIv2023,AutoMix: Automatically Mixing Language Models,No.,1,"""No evidence""",2023,2023-10-19T17:57:39Z,,,
arXIv2023,An Emulator for Fine-Tuning Large Language Models using Small Language Models,No.,1,"""No evidence""",2023,2023-10-19T17:57:16Z,,,
arXIv2023,SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving,No.,1,"""No evidence""",2023,2023-10-19T17:56:40Z,,,
arXIv2023,3D-GPT: Procedural 3D Modeling with Large Language Models,No.,1,"""No evidence""",2023,2023-10-19T17:41:48Z,,,
arXIv2023,The Foundation Model Transparency Index,No.,1,"""No evidence""",2023,2023-10-19T17:39:02Z,,,
arXIv2023,Eureka: Human-Level Reward Design via Coding Large Language Models,No.,1,"""No evidence""",2023,2023-10-19T17:31:01Z,,,
arXIv2023,Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling,No.,1,"""No evidence""",2023,2023-10-19T16:54:38Z,,,
arXIv2023,A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems,No.,1,"""No evidence""",2023,2023-10-19T16:41:44Z,,,
arXIv2023,The Locality and Symmetry of Positional Encodings,No.,1,"""No evidence""",2023,2023-10-19T16:15:15Z,,,
arXIv2023,Knowledge-Augmented Language Model Verification,No.,1,"""No evidence""",2023,2023-10-19T15:40:00Z,,,
arXIv2023,GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents,No.,1,"""No evidence""",2023,2023-10-19T15:17:34Z,,,
arXIv2023,Model Merging by Uncertainty-Based Gradient Matching,No.,1,"""No evidence""",2023,2023-10-19T15:02:45Z,,,
arXIv2023,Data Augmentations for Improved (Large) Language Model Generalization,No.,1,"""No evidence""",2023,2023-10-19T14:59:25Z,,,
arXIv2023,Label-Aware Automatic Verbalizer for Few-Shot Text Classification,No.,1,"""No evidence""",2023,2023-10-19T14:30:07Z,,,
arXIv2023,Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning,No.,1,"""No evidence""",2023,2023-10-19T14:25:06Z,,,
arXIv2023,Transformer-based Entity Legal Form Classification,No.,1,"""No evidence""",2023,2023-10-19T14:11:43Z,,,
arXIv2023,Character-level Chinese Backpack Language Models,No.,1,"""No evidence""",2023,2023-10-19T13:54:57Z,,,
arXIv2023,LASER: Linear Compression in Wireless Distributed Optimization,No.,1,"""No evidence""",2023,2023-10-19T13:18:57Z,,,
arXIv2023,Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing,No.,1,"""No evidence""",2023,2023-10-19T11:43:15Z,,,
arXIv2023,Knowledge from Uncertainty in Evidential Deep Learning,No.,1,"""No evidence""",2023,2023-10-19T11:41:52Z,,,
arXIv2023,Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model,No.,1,"""No evidence""",2023,2023-10-19T09:39:21Z,,,
arXIv2023,Fast Model Debias with Machine Unlearning,No.,1,"""No evidence""",2023,2023-10-19T08:10:57Z,,,
arXIv2023,Large Language Model for Multi-objective Evolutionary Optimization,No.,1,"""No evidence""",2023,2023-10-19T07:46:54Z,,,
arXIv2023,Reliable Academic Conference Question Answering: A Study Based on Large Language Model,No.,1,"""No evidence""",2023,2023-10-19T07:39:07Z,,,
arXIv2023,ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding,No.,1,"""No evidence""",2023,2023-10-19T07:11:48Z,,,
arXIv2023,Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach,No.,1,"""No evidence""",2023,2023-10-19T06:54:55Z,,,
arXIv2023,Lost in Translation: When GPT-4V(ision) Can't See Eye to Eye with Text. A Vision-Language-Consistency Analysis of VLLMs and Beyond,No.,1,"""No evidence""",2023,2023-10-19T06:45:11Z,,,
arXIv2023,GraphGPT: Graph Instruction Tuning for Large Language Models,No.,1,"""No evidence""",2023,2023-10-19T06:17:46Z,,,
arXIv2023,SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models,No.,1,"""No evidence""",2023,2023-10-19T05:56:25Z,,,
arXIv2023,MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations,No.,1,"""No evidence""",2023,2023-10-19T05:48:28Z,,,
arXIv2023,An Exploration of In-Context Learning for Speech Language Model,No.,1,"""No evidence""",2023,2023-10-19T05:31:45Z,,,
arXIv2023,Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights,No.,1,"""No evidence""",2023,2023-10-19T04:41:01Z,,,
arXIv2023,Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models,No.,1,"""No evidence""",2023,2023-10-19T04:16:40Z,,,
arXIv2023,DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond,No.,1,"""No evidence""",2023,2023-10-19T02:49:09Z,,,
arXIv2023,MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models,No.,1,"""No evidence""",2023,2023-10-19T02:32:39Z,,,
arXIv2023,The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions,No.,1,"""No evidence""",2023,2023-10-19T02:12:17Z,,,
arXIv2023,AI for Mathematics: A Cognitive Science Perspective,No.,1,"""No evidence""",2023,2023-10-19T02:00:31Z,,,
arXIv2023,FinEntity: Entity-level Sentiment Classification for Financial Texts,No.,1,"""No evidence""",2023,2023-10-19T01:38:40Z,,,
arXIv2023,Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing,No.,1,"""No evidence""",2023,2023-10-19T01:20:12Z,,,
arXIv2023,Solving Hard Analogy Questions with Relation Embedding Chains,No.,1,"""No evidence""",2023,2023-10-18T23:13:22Z,,,
arXIv2023,REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models,No.,1,"""No evidence""",2023,2023-10-18T22:14:37Z,,,
arXIv2023,Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking,No.,1,"""No evidence""",2023,2023-10-18T21:42:16Z,,,
arXIv2023,Document-Level Language Models for Machine Translation,No.,1,"""No evidence""",2023,2023-10-18T20:10:07Z,,,
arXIv2023,Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly,No.,1,"""No evidence""",2023,2023-10-18T20:07:41Z,,,
arXIv2023,Pseudointelligence: A Unifying Framework for Language Model Evaluation,No.,1,"""No evidence""",2023,2023-10-18T17:48:05Z,,,
arXIv2023,"DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning",No.,1,"""No evidence""",2023,2023-10-18T17:37:10Z,,,
arXIv2023,Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture,No.,1,"""No evidence""",2023,2023-10-18T17:06:22Z,,,
arXIv2023,Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling,No.,1,"""No evidence""",2023,2023-10-18T16:43:08Z,,,
arXIv2023,On the Benefit of Generative Foundation Models for Human Activity Recognition,No.,1,"""No evidence""",2023,2023-10-18T16:27:06Z,,,
arXIv2023,Transformers for scientific data: a pedagogical review for astronomers,No.,1,"""No evidence""",2023,2023-10-18T16:02:32Z,,,
arXIv2023,Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education,No.,1,"""No evidence""",2023,2023-10-18T15:48:07Z,,,
arXIv2023,Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models,No.,1,"""No evidence""",2023,2023-10-18T15:34:37Z,,,
arXIv2023,Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews,No.,1,"""No evidence""",2023,2023-10-18T13:45:47Z,,,
arXIv2023,Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences,No.,1,"""No evidence""",2023,2023-10-18T13:40:41Z,,,
arXIv2023,MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models,No.,1,"""No evidence""",2023,2023-10-18T13:31:10Z,,,
arXIv2023,Evaluating the Fairness of Discriminative Foundation Models in Computer Vision,No.,1,"""No evidence""",2023,2023-10-18T10:32:39Z,,,
arXIv2023,The Value-Sensitive Conversational Agent Co-Design Framework,No.,1,"""No evidence""",2023,2023-10-18T09:58:39Z,,,
arXIv2023,Towards Graph Foundation Models: A Survey and Beyond,No.,1,"""No evidence""",2023,2023-10-18T09:31:21Z,,,
arXIv2023,Annotated Job Ads with Named Entity Recognition,No.,1,"""No evidence""",2023,2023-10-18T07:55:53Z,,,
arXIv2023,A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction,No.,1,"""No evidence""",2023,2023-10-18T07:38:04Z,,,
arXIv2023,Bias in Emotion Recognition with ChatGPT,No.,1,"""No evidence""",2023,2023-10-18T07:28:12Z,,,
arXIv2023,Quantifying Self-diagnostic Atomic Knowledge in Chinese Medical Foundation Model: A Computational Analysis,No.,1,"""No evidence""",2023,2023-10-18T05:42:22Z,,,
arXIv2023,Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding,No.,1,"""No evidence""",2023,2023-10-18T05:39:20Z,,,
arXIv2023,Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning,No.,1,"""No evidence""",2023,2023-10-18T05:13:47Z,,,
arXIv2023,"A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge",No.,1,"""No evidence""",2023,2023-10-18T04:31:06Z,,,
arXIv2023,Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs,No.,1,"""No evidence""",2023,2023-10-18T03:34:59Z,,,
arXIv2023,Descriptive Knowledge Graph in Biomedical Domain,No.,1,"""No evidence""",2023,2023-10-18T03:10:25Z,,,
arXIv2023,ChatGPT-guided Semantics for Zero-shot Learning,No.,1,"""No evidence""",2023,2023-10-18T02:07:22Z,,,
arXIv2023,Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model,No.,1,"""No evidence""",2023,2023-10-18T01:20:16Z,,,
arXIv2023,Learn Your Tokens: Word-Pooled Tokenization for Language Modeling,No.,1,"""No evidence""",2023,2023-10-17T23:34:39Z,,,
arXIv2023,Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach,No.,1,"""No evidence""",2023,2023-10-17T22:42:12Z,,,
arXIv2023,Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models,No.,1,"""No evidence""",2023,2023-10-17T19:31:05Z,,,
arXIv2023,Multi-stage Large Language Model Correction for Speech Recognition,No.,1,"""No evidence""",2023,2023-10-17T19:02:40Z,,,
arXIv2023,Group Preference Optimization: Few-Shot Alignment of Large Language Models,No.,1,"""No evidence""",2023,2023-10-17T18:41:57Z,,,
arXIv2023,VeRA: Vector-based Random Matrix Adaptation,No.,1,"""No evidence""",2023,2023-10-17T17:59:46Z,,,
arXIv2023,BitNet: Scaling 1-bit Transformers for Large Language Models,No.,1,"""No evidence""",2023,2023-10-17T17:59:15Z,,,
arXIv2023,Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective,No.,1,"""No evidence""",2023,2023-10-17T17:58:34Z,,,
arXIv2023,Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V,No.,1,"""No evidence""",2023,2023-10-17T17:51:31Z,,,
arXIv2023,EvalCrafter: Benchmarking and Evaluating Large Video Generation Models,No.,1,"""No evidence""",2023,2023-10-17T17:50:46Z,,,
arXIv2023,An Empirical Study of Translation Hypothesis Ensembling with Large Language Models,No.,1,"""No evidence""",2023,2023-10-17T17:40:21Z,,,
arXIv2023,LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks,No.,1,"""No evidence""",2023,2023-10-17T17:15:41Z,,,
arXIv2023,Towards Automatic Satellite Images Captions Generation Using Large Language Models,No.,1,"""No evidence""",2023,2023-10-17T16:45:47Z,,,
arXIv2023,Disentangling the Linguistic Competence of Privacy-Preserving BERT,No.,1,"""No evidence""",2023,2023-10-17T16:00:26Z,,,
arXIv2023,QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering,No.,1,"""No evidence""",2023,2023-10-17T14:27:34Z,,,
arXIv2023,"ChapGTP, ILLC's Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation",No.,1,"""No evidence""",2023,2023-10-17T14:06:06Z,,,
arXIv2023,Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering with Large Language Models,No.,1,"""No evidence""",2023,2023-10-17T13:39:26Z,,,
arXIv2023,Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&D Cycle,No.,1,"""No evidence""",2023,2023-10-17T13:18:02Z,,,
arXIv2023,KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models,No.,1,"""No evidence""",2023,2023-10-17T12:51:35Z,,,
arXIv2023,Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations,No.,1,"""No evidence""",2023,2023-10-17T12:34:32Z,,,
arXIv2023,Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models,No.,1,"""No evidence""",2023,2023-10-17T11:41:38Z,,,
arXIv2023,ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing,No.,1,"""No evidence""",2023,2023-10-17T11:34:50Z,,,
arXIv2023,Probing the Creativity of Large Language Models: Can models produce divergent semantic association?,No.,1,"""No evidence""",2023,2023-10-17T11:23:32Z,,,
arXIv2023,Experimenting AI Technologies for Disinformation Combat: the IDMO Project,No.,1,"""No evidence""",2023,2023-10-17T09:27:43Z,,,
arXIv2023,Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads,No.,1,"""No evidence""",2023,2023-10-17T09:25:17Z,,,
arXIv2023,Matrix Compression via Randomized Low Rank and Low Precision Factorization,No.,1,"""No evidence""",2023,2023-10-17T06:56:57Z,,,
arXIv2023,Iterative Shallow Fusion of Backward Language Model for End-to-End Speech Recognition,No.,1,"""No evidence""",2023,2023-10-17T05:44:10Z,,,
arXIv2023,Correction Focused Language Model Training for Speech Recognition,No.,1,"""No evidence""",2023,2023-10-17T05:10:39Z,,,
arXIv2023,Context-Aware Meta-Learning,No.,1,"""No evidence""",2023,2023-10-17T03:35:27Z,,,
arXIv2023,TEQ: Trainable Equivalent Transformation for Quantization of LLMs,No.,1,"""No evidence""",2023,2023-10-17T02:42:34Z,,,
arXIv2023,Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti,No.,1,"""No evidence""",2023,2023-10-17T02:12:12Z,,,
arXIv2023,Towards Training-free Open-world Segmentation via Image Prompt Foundation Models,No.,1,"""No evidence""",2023,2023-10-17T01:12:08Z,,,
arXIv2023,Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT,No.,1,"""No evidence""",2023,2023-10-17T00:22:10Z,,,
arXIv2023,IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models,No.,1,"""No evidence""",2023,2023-10-16T22:53:54Z,,,
arXIv2023,Neural Code Generation Enhancement via Functional Overlap Reranking,No.,1,"""No evidence""",2023,2023-10-16T22:20:31Z,,,
arXIv2023,Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks,No.,1,"""No evidence""",2023,2023-10-16T21:37:24Z,,,
arXIv2023,Approximating Two-Layer Feedforward Networks for Efficient Transformers,No.,1,"""No evidence""",2023,2023-10-16T21:23:16Z,,,
arXIv2023,If the Sources Could Talk: Evaluating Large Language Models for Research Assistance in History,No.,1,"""No evidence""",2023,2023-10-16T20:12:06Z,,,
arXIv2023,Gotta be SAFE: A New Framework for Molecular Design,No.,1,"""No evidence""",2023,2023-10-16T19:12:56Z,,,
arXIv2023,BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys,No.,1,"""No evidence""",2023,2023-10-16T18:59:31Z,,,
arXIv2023,Interactive Task Planning with Language Models,No.,1,"""No evidence""",2023,2023-10-16T17:59:12Z,,,
arXIv2023,In-Context Pretraining: Language Modeling Beyond Document Boundaries,No.,1,"""No evidence""",2023,2023-10-16T17:57:12Z,,,
arXIv2023,OpenAgents: An Open Platform for Language Agents in the Wild,No.,1,"""No evidence""",2023,2023-10-16T17:54:53Z,,,
arXIv2023,Llemma: An Open Language Model For Mathematics,No.,1,"""No evidence""",2023,2023-10-16T17:54:07Z,,,
arXIv2023,How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations,No.,1,"""No evidence""",2023,2023-10-16T17:40:49Z,,,
arXIv2023,ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model,No.,1,"""No evidence""",2023,2023-10-16T17:31:34Z,,,
arXIv2023,Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment,No.,1,"""No evidence""",2023,2023-10-16T17:11:42Z,,,
arXIv2023,BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation,No.,1,"""No evidence""",2023,2023-10-16T17:05:56Z,,,
arXIv2023,RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling,No.,1,"""No evidence""",2023,2023-10-16T16:42:01Z,,,
arXIv2023,Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning,No.,1,"""No evidence""",2023,2023-10-16T16:18:55Z,,,
arXIv2023,Use of probabilistic phrases in a coordination game: human versus GPT-4,No.,1,"""No evidence""",2023,2023-10-16T16:14:27Z,,,
arXIv2023,Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking,No.,1,"""No evidence""",2023,2023-10-16T15:38:02Z,,,
arXIv2023,Unifying Image Processing as Visual Prompting Question Answering,No.,1,"""No evidence""",2023,2023-10-16T15:32:57Z,,,
arXIv2023,"ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models",No.,1,"""No evidence""",2023,2023-10-16T15:25:14Z,,,
arXIv2023,NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails,No.,1,"""No evidence""",2023,2023-10-16T15:20:30Z,,,
arXIv2023,Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation,No.,1,"""No evidence""",2023,2023-10-16T15:11:01Z,,,
arXIv2023,xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection,No.,1,"""No evidence""",2023,2023-10-16T15:03:14Z,,,
arXIv2023,G-SPEED: General SParse Efficient Editing MoDel,No.,1,"""No evidence""",2023,2023-10-16T15:01:18Z,,,
arXIv2023,"Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models",No.,1,"""No evidence""",2023,2023-10-16T14:33:02Z,,,
arXIv2023,"MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities",No.,1,"""No evidence""",2023,2023-10-16T14:29:35Z,,,
arXIv2023,"Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms",No.,1,"""No evidence""",2023,2023-10-16T14:00:07Z,,,
arXIv2023,LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation,No.,1,"""No evidence""",2023,2023-10-16T13:49:46Z,,,
arXIv2023,$\textit{Swap and Predict}$ -- Predicting the Semantic Changes in Words across Corpora by Context Swapping,No.,1,"""No evidence""",2023,2023-10-16T13:39:44Z,,,
arXIv2023,Contextual Data Augmentation for Task-Oriented Dialog Systems,No.,1,"""No evidence""",2023,2023-10-16T13:22:34Z,,,
arXIv2023,Untying the Reversal Curse via Bidirectional Language Model Editing,No.,1,"""No evidence""",2023,2023-10-16T12:04:13Z,,,
arXIv2023,Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques,No.,1,"""No evidence""",2023,2023-10-16T11:43:30Z,,,
arXIv2023,Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization,No.,1,"""No evidence""",2023,2023-10-16T11:16:07Z,,,
arXIv2023,Prediction of Arabic Legal Rulings using Large Language Models,No.,1,"""No evidence""",2023,2023-10-16T10:37:35Z,,,
arXIv2023,RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-10-16T09:30:45Z,,,
arXIv2023,MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations,No.,1,"""No evidence""",2023,2023-10-16T09:09:02Z,,,
arXIv2023,Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook,No.,1,"""No evidence""",2023,2023-10-16T09:06:00Z,,,
arXIv2023,AdaLomo: Low-memory Optimization with Adaptive Learning Rate,No.,1,"""No evidence""",2023,2023-10-16T09:04:28Z,,,
arXIv2023,Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison,No.,1,"""No evidence""",2023,2023-10-16T08:52:41Z,,,
arXIv2023,TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models,No.,1,"""No evidence""",2023,2023-10-16T08:42:39Z,,,
arXIv2023,Joint Music and Language Attention Models for Zero-shot Music Tagging,No.,1,"""No evidence""",2023,2023-10-16T08:00:16Z,,,
arXIv2023,Character-LLM: A Trainable Agent for Role-Playing,No.,1,"""No evidence""",2023,2023-10-16T07:58:56Z,,,
arXIv2023,A Search for Prompts: Generating Structured Answers from Contracts,No.,1,"""No evidence""",2023,2023-10-16T07:29:38Z,,,
arXIv2023,Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning,No.,1,"""No evidence""",2023,2023-10-16T06:21:06Z,,,
arXIv2023,Let's reward step by step: Step-Level reward model as the Navigators for Reasoning,No.,1,"""No evidence""",2023,2023-10-16T05:21:50Z,,,
arXIv2023,Verbosity Bias in Preference Labeling by Large Language Models,No.,1,"""No evidence""",2023,2023-10-16T05:19:02Z,,,
arXIv2023,Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation,No.,1,"""No evidence""",2023,2023-10-16T05:09:58Z,,,
arXIv2023,Fine-tuning ChatGPT for Automatic Scoring,No.,1,"""No evidence""",2023,2023-10-16T05:09:16Z,,,
arXIv2023,"EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge",No.,1,"""No evidence""",2023,2023-10-16T04:20:16Z,,,
arXIv2023,Empirical Study of Zero-Shot NER with ChatGPT,No.,1,"""No evidence""",2023,2023-10-16T03:40:03Z,,,
arXIv2023,Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance,No.,1,"""No evidence""",2023,2023-10-16T02:43:47Z,,,
arXIv2023,Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models,No.,1,"""No evidence""",2023,2023-10-15T20:57:25Z,,,
arXIv2023,FiLM: Fill-in Language Models for Any-Order Generation,No.,1,"""No evidence""",2023,2023-10-15T19:37:39Z,,,
arXIv2023,Prompting Scientific Names for Zero-Shot Species Recognition,No.,1,"""No evidence""",2023,2023-10-15T19:36:43Z,,,
arXIv2023,Empirical study of pretrained multilingual language models for zero-shot cross-lingual generation,No.,1,"""No evidence""",2023,2023-10-15T18:58:53Z,,,
arXIv2023,Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia,No.,1,"""No evidence""",2023,2023-10-15T17:58:47Z,,,
arXIv2023,Large Vocabulary Spontaneous Speech Recognition for Tigrigna,No.,1,"""No evidence""",2023,2023-10-15T13:07:41Z,,,
arXIv2023,Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming,No.,1,"""No evidence""",2023,2023-10-15T12:56:13Z,,,
arXIv2023,VLIS: Unimodal Language Models Guide Multimodal Language Generation,No.,1,"""No evidence""",2023,2023-10-15T07:58:52Z,,,
arXIv2023,Improving Access to Justice for the Indian Population: A Benchmark for Evaluating Translation of Legal Text to Indian Languages,No.,1,"""No evidence""",2023,2023-10-15T07:49:56Z,,,
arXIv2023,Diversifying the Mixture-of-Experts Representation for Language Models with Orthogonal Optimizer,No.,1,"""No evidence""",2023,2023-10-15T07:20:28Z,,,
arXIv2023,GPT-Prompt Controlled Diffusion for Weakly-Supervised Semantic Segmentation,No.,1,"""No evidence""",2023,2023-10-15T07:19:23Z,,,
arXIv2023,EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification,No.,1,"""No evidence""",2023,2023-10-15T06:46:15Z,,,
arXIv2023,Beyond Segmentation: Road Network Generation with Multi-Modal LLMs,No.,1,"""No evidence""",2023,2023-10-15T06:46:15Z,,,
arXIv2023,UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting,No.,1,"""No evidence""",2023,2023-10-15T06:30:22Z,,,
arXIv2023,Large Language Model-Aware In-Context Learning for Code Generation,No.,1,"""No evidence""",2023,2023-10-15T06:12:58Z,,,
arXIv2023,Domain-Specific Language Model Post-Training for Indonesian Financial NLP,No.,1,"""No evidence""",2023,2023-10-15T05:07:08Z,,,
arXIv2023,KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models,No.,1,"""No evidence""",2023,2023-10-15T04:00:36Z,,,
arXIv2023,HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings,No.,1,"""No evidence""",2023,2023-10-15T03:14:33Z,,,
arXIv2023,Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning,No.,1,"""No evidence""",2023,2023-10-14T22:24:58Z,,,
arXIv2023,Efficient Model-Agnostic Multi-Group Equivariant Networks,No.,1,"""No evidence""",2023,2023-10-14T22:24:26Z,,,
arXIv2023,Beyond Testers' Biases: Guiding Model Testing with Knowledge Bases using LLMs,No.,1,"""No evidence""",2023,2023-10-14T21:24:03Z,,,
arXIv2023,Legend at ArAIEval Shared Task: Persuasion Technique Detection using a Language-Agnostic Text Representation Model,No.,1,"""No evidence""",2023,2023-10-14T20:27:04Z,,,
arXIv2023,Enhancing Binary Code Comment Quality Classification: Integrating Generative AI for Improved Accuracy,No.,1,"""No evidence""",2023,2023-10-14T18:19:06Z,,,
arXIv2023,ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models,No.,1,"""No evidence""",2023,2023-10-14T17:10:28Z,,,
arXIv2023,A decoder-only foundation model for time-series forecasting,No.,1,"""No evidence""",2023,2023-10-14T17:01:37Z,,,
arXIv2023,Penetrative AI: Making LLMs Comprehend the Physical World,No.,1,"""No evidence""",2023,2023-10-14T15:48:15Z,,,
arXIv2023,Self-Detoxifying Language Models via Toxification Reversal,No.,1,"""No evidence""",2023,2023-10-14T12:51:38Z,,,
arXIv2023,Leveraging Generative AI: Improving Software Metadata Classification with Generated Code-Comment Pairs,No.,1,"""No evidence""",2023,2023-10-14T12:09:43Z,,,
arXIv2023,A study of the impact of generative AI-based data augmentation on software metadata classification,No.,1,"""No evidence""",2023,2023-10-14T10:47:10Z,,,
arXIv2023,Can Large Language Model Comprehend Ancient Chinese? A Preliminary Test on ACLUE,No.,1,"""No evidence""",2023,2023-10-14T10:06:39Z,,,
arXIv2023,Chatbot-supported Thesis Writing: An Autoethnographic Report,No.,1,"""No evidence""",2023,2023-10-14T09:09:26Z,,,
arXIv2023,TS-ENAS:Two-Stage Evolution for Cell-based Network Architecture Search,No.,1,"""No evidence""",2023,2023-10-14T08:02:01Z,,,
arXIv2023,Software Metadata Classification based on Generative Artificial Intelligence,No.,1,"""No evidence""",2023,2023-10-14T07:38:16Z,,,
arXIv2023,Towards Semantic Communication Protocols for 6G: From Protocol Learning to Language-Oriented Approaches,No.,1,"""No evidence""",2023,2023-10-14T06:28:50Z,,,
arXIv2023,JM3D & JM3D-LLM: Elevating 3D Understanding with Joint Multi-modal Cues,No.,1,"""No evidence""",2023,2023-10-14T06:13:20Z,,,
arXIv2023,MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning,No.,1,"""No evidence""",2023,2023-10-14T03:22:07Z,,,
arXIv2023,LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents,No.,1,"""No evidence""",2023,2023-10-14T00:07:03Z,,,
arXIv2023,Enhancing BERT-Based Visual Question Answering through Keyword-Driven Sentence Selection,No.,1,"""No evidence""",2023,2023-10-13T22:43:55Z,,,
arXIv2023,Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning,No.,1,"""No evidence""",2023,2023-10-13T22:29:15Z,,,
arXIv2023,SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation,No.,1,"""No evidence""",2023,2023-10-13T22:07:33Z,,,
arXIv2023,From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique,No.,1,"""No evidence""",2023,2023-10-13T19:09:31Z,,,
arXIv2023,Unsupervised Domain Adaption for Neural Information Retrieval,No.,1,"""No evidence""",2023,2023-10-13T18:27:33Z,,,
arXIv2023,Vision-by-Language for Training-Free Compositional Image Retrieval,No.,1,"""No evidence""",2023,2023-10-13T17:59:38Z,,,
arXIv2023,QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models,No.,1,"""No evidence""",2023,2023-10-13T17:15:05Z,,,
arXIv2023,Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration,No.,1,"""No evidence""",2023,2023-10-13T16:47:20Z,,,
arXIv2023,Evaluating Machine Perception of Indigeneity: An Analysis of ChatGPT's Perceptions of Indigenous Roles in Diverse Scenarios,No.,1,"""No evidence""",2023,2023-10-13T16:46:23Z,,,
arXIv2023,AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems,No.,1,"""No evidence""",2023,2023-10-13T16:37:14Z,,,
arXIv2023,Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation,No.,1,"""No evidence""",2023,2023-10-13T16:21:07Z,,,
arXIv2023,"PaLI-3 Vision Language Models: Smaller, Faster, Stronger",No.,1,"""No evidence""",2023,2023-10-13T15:45:19Z,,,
arXIv2023,Learning To Teach Large Language Models Logical Reasoning,No.,1,"""No evidence""",2023,2023-10-13T14:53:06Z,,,
arXIv2023,PuoBERTa: Training and evaluation of a curated language model for Setswana,No.,1,"""No evidence""",2023,2023-10-13T14:33:02Z,,,
arXIv2023,The Consensus Game: Language Model Generation via Equilibrium Search,No.,1,"""No evidence""",2023,2023-10-13T14:27:21Z,,,
arXIv2023,GLoRE: Evaluating Logical Reasoning of Large Language Models,No.,1,"""No evidence""",2023,2023-10-13T13:52:15Z,,,
arXIv2023,Privacy-Preserving Encrypted Low-Dose CT Denoising,No.,1,"""No evidence""",2023,2023-10-13T13:40:25Z,,,
arXIv2023,A ML-LLM pairing for better code comment classification,No.,1,"""No evidence""",2023,2023-10-13T12:43:13Z,,,
arXIv2023,SAI: Solving AI Tasks with Systematic Artificial Intelligence in Communication Network,No.,1,"""No evidence""",2023,2023-10-13T12:14:58Z,,,
arXIv2023,MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks,No.,1,"""No evidence""",2023,2023-10-13T11:57:04Z,,,
arXIv2023,"Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans",No.,1,"""No evidence""",2023,2023-10-13T11:28:02Z,,,
arXIv2023,ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models,No.,1,"""No evidence""",2023,2023-10-13T09:45:14Z,,,
arXIv2023,xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark,No.,1,"""No evidence""",2023,2023-10-13T09:07:13Z,,,
arXIv2023,EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs,No.,1,"""No evidence""",2023,2023-10-13T08:38:56Z,,,
arXIv2023,Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning,No.,1,"""No evidence""",2023,2023-10-13T07:49:11Z,,,
arXIv2023,LLaMA Rider: Spurring Large Language Models to Explore the Open World,No.,1,"""No evidence""",2023,2023-10-13T07:47:44Z,,,
arXIv2023,Human-in-the-loop Machine Translation with Large Language Model,No.,1,"""No evidence""",2023,2023-10-13T07:30:27Z,,,
arXIv2023,Welfare Diplomacy: Benchmarking Language Model Cooperation,No.,1,"""No evidence""",2023,2023-10-13T07:15:32Z,,,
arXIv2023,InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems,No.,1,"""No evidence""",2023,2023-10-13T06:36:26Z,,,
arXIv2023,Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System,No.,1,"""No evidence""",2023,2023-10-13T06:03:47Z,,,
arXIv2023,Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models,No.,1,"""No evidence""",2023,2023-10-13T05:59:03Z,,,
arXIv2023,In-Context Learning for Few-Shot Molecular Property Prediction,No.,1,"""No evidence""",2023,2023-10-13T05:12:48Z,,,
arXIv2023,A Case-Based Persistent Memory for a Large Language Model,No.,1,"""No evidence""",2023,2023-10-13T03:56:38Z,,,
arXIv2023,From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models,No.,1,"""No evidence""",2023,2023-10-13T02:41:55Z,,,
arXIv2023,A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models,No.,1,"""No evidence""",2023,2023-10-13T01:00:15Z,,,
arXIv2023,Detection and prediction of clopidogrel treatment failures using longitudinal structured electronic health records,No.,1,"""No evidence""",2023,2023-10-12T22:52:29Z,,,
arXIv2023,Tokenizer Choice For LLM Training: Negligible or Crucial?,No.,1,"""No evidence""",2023,2023-10-12T22:44:19Z,,,
arXIv2023,Search-Adaptor: Embedding Customization for Information Retrieval,No.,1,"""No evidence""",2023,2023-10-12T22:30:15Z,,,
arXIv2023,Circuit Component Reuse Across Tasks in Transformer Language Models,No.,1,"""No evidence""",2023,2023-10-12T22:12:28Z,,,
arXIv2023,A Zero-Shot Language Agent for Computer Control with Structured Reflection,No.,1,"""No evidence""",2023,2023-10-12T21:53:37Z,,,
arXIv2023,Virtual Augmented Reality for Atari Reinforcement Learning,No.,1,"""No evidence""",2023,2023-10-12T19:42:42Z,,,
arXIv2023,Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams,No.,1,"""No evidence""",2023,2023-10-12T19:28:57Z,,,
arXIv2023,Multimodal Large Language Model for Visual Navigation,No.,1,"""No evidence""",2023,2023-10-12T19:01:06Z,,,
arXIv2023,Analyzing Textual Data for Fatality Classification in Afghanistan's Armed Conflicts: A BERT Approach,No.,1,"""No evidence""",2023,2023-10-12T18:26:23Z,,,
arXIv2023,Octopus: Embodied Vision-Language Programmer from Environmental Feedback,No.,1,"""No evidence""",2023,2023-10-12T17:59:58Z,,,
arXIv2023,Tree-Planner: Efficient Close-loop Task Planning with Large Language Models,No.,1,"""No evidence""",2023,2023-10-12T17:59:50Z,,,
arXIv2023,Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias,No.,1,"""No evidence""",2023,2023-10-12T17:50:09Z,,,
arXIv2023,Cross-Episodic Curriculum for Transformer Agents,No.,1,"""No evidence""",2023,2023-10-12T17:45:05Z,,,
arXIv2023,Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation,No.,1,"""No evidence""",2023,2023-10-12T17:34:20Z,,,
arXIv2023,Formally Specifying the High-Level Behavior of LLM-Based Agents,No.,1,"""No evidence""",2023,2023-10-12T17:24:15Z,,,
arXIv2023,HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science,No.,1,"""No evidence""",2023,2023-10-12T17:06:19Z,,,
arXIv2023,The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS,No.,1,"""No evidence""",2023,2023-10-12T16:55:44Z,,,
arXIv2023,GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language Models,No.,1,"""No evidence""",2023,2023-10-12T16:46:58Z,,,
arXIv2023,Can We Edit Multimodal Large Language Models?,No.,1,"""No evidence""",2023,2023-10-12T16:32:44Z,,,
arXIv2023,DistillSpec: Improving Speculative Decoding via Knowledge Distillation,No.,1,"""No evidence""",2023,2023-10-12T16:21:04Z,,,
arXIv2023,Towards Robust Multi-Modal Reasoning via Model Selection,No.,1,"""No evidence""",2023,2023-10-12T16:06:18Z,,,
arXIv2023,A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing,No.,1,"""No evidence""",2023,2023-10-12T15:56:24Z,,,
arXIv2023,Towards Design and Development of an ArUco Markers-Based Quantitative Surface Tactile Sensor,No.,1,"""No evidence""",2023,2023-10-12T15:09:12Z,,,
arXIv2023,Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation,No.,1,"""No evidence""",2023,2023-10-12T15:08:14Z,,,
arXIv2023,Mapping Memes to Words for Multimodal Hateful Meme Classification,No.,1,"""No evidence""",2023,2023-10-12T14:38:52Z,,,
arXIv2023,From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer,No.,1,"""No evidence""",2023,2023-10-12T14:36:13Z,,,
arXIv2023,Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning,No.,1,"""No evidence""",2023,2023-10-12T13:15:11Z,,,
arXIv2023,Expanding the Vocabulary of BERT for Knowledge Base Construction,No.,1,"""No evidence""",2023,2023-10-12T12:52:46Z,,,
arXIv2023,Can Text-based Knowledge Graph Completion Benefit From Zero-Shot Large Language Models?,No.,1,"""No evidence""",2023,2023-10-12T12:31:23Z,,,
arXIv2023,Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting,No.,1,"""No evidence""",2023,2023-10-12T12:29:32Z,,,
arXIv2023,Language Models are Universal Embedders,No.,1,"""No evidence""",2023,2023-10-12T11:25:46Z,,,
arXIv2023,EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation,No.,1,"""No evidence""",2023,2023-10-12T10:21:37Z,,,
arXIv2023,Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach,No.,1,"""No evidence""",2023,2023-10-12T09:55:45Z,,,
arXIv2023,Multiclass Classification of Policy Documents with Large Language Models,No.,1,"""No evidence""",2023,2023-10-12T09:41:22Z,,,
arXIv2023,Ziya-Visual: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning,No.,1,"""No evidence""",2023,2023-10-12T09:39:17Z,,,
arXIv2023,Beyond Training Objectives: Interpreting Reward Model Divergence in Large Language Models,No.,1,"""No evidence""",2023,2023-10-12T09:36:03Z,,,
arXIv2023,Context Compression for Auto-regressive Transformers with Sentinel Tokens,No.,1,"""No evidence""",2023,2023-10-12T09:18:19Z,,,
arXIv2023,Overview of Physics-Informed Machine Learning Inversion of Geophysical Data,No.,1,"""No evidence""",2023,2023-10-12T08:10:31Z,,,
arXIv2023,To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer,No.,1,"""No evidence""",2023,2023-10-12T06:59:10Z,,,
arXIv2023,Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model,No.,1,"""No evidence""",2023,2023-10-12T06:46:07Z,,,
arXIv2023,QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models,No.,1,"""No evidence""",2023,2023-10-12T05:25:49Z,,,
arXIv2023,"Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles",No.,1,"""No evidence""",2023,2023-10-12T04:56:01Z,,,
arXIv2023,Harnessing Large Language Models' Empathetic Response Generation Capabilities for Online Mental Health Counselling Support,No.,1,"""No evidence""",2023,2023-10-12T03:33:06Z,,,
arXIv2023,LEMON: Lossless model expansion,No.,1,"""No evidence""",2023,2023-10-12T03:02:41Z,,,
arXIv2023,"Large Language Models for Scientific Synthesis, Inference and Explanation",No.,1,"""No evidence""",2023,2023-10-12T02:17:59Z,,,
arXIv2023,"Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation",No.,1,"""No evidence""",2023,2023-10-12T01:17:56Z,,,
arXIv2023,AutoRepo: A general framework for multi-modal LLM-based automated construction reporting,No.,1,"""No evidence""",2023,2023-10-11T23:42:00Z,,,
arXIv2023,Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models,No.,1,"""No evidence""",2023,2023-10-11T23:17:43Z,,,
arXIv2023,Crosslingual Structural Priming and the Pre-Training Dynamics of Bilingual Language Models,No.,1,"""No evidence""",2023,2023-10-11T22:57:03Z,,,
arXIv2023,Assessing Evaluation Metrics for Neural Test Oracle Generation,No.,1,"""No evidence""",2023,2023-10-11T19:58:07Z,,,
arXIv2023,Measuring Feature Sparsity in Language Models,No.,1,"""No evidence""",2023,2023-10-11T19:26:52Z,,,
arXIv2023,"When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement",No.,1,"""No evidence""",2023,2023-10-11T19:16:35Z,,,
arXIv2023,Does Synthetic Data Make Large Language Models More Efficient?,No.,1,"""No evidence""",2023,2023-10-11T19:16:09Z,,,
arXIv2023,Large Language Models Are Zero-Shot Time Series Forecasters,No.,1,"""No evidence""",2023,2023-10-11T19:01:28Z,,,
arXIv2023,On the Relationship between Sentence Analogy Identification and Sentence Structure Encoding in Large Language Models,No.,1,"""No evidence""",2023,2023-10-11T18:59:48Z,,,
arXIv2023,Language Models As Semantic Indexers,No.,1,"""No evidence""",2023,2023-10-11T18:56:15Z,,,
arXIv2023,InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining,No.,1,"""No evidence""",2023,2023-10-11T17:59:05Z,,,
arXIv2023,Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models,No.,1,"""No evidence""",2023,2023-10-11T17:59:02Z,,,
arXIv2023,OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation,No.,1,"""No evidence""",2023,2023-10-11T17:58:33Z,,,
arXIv2023,"DiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models",No.,1,"""No evidence""",2023,2023-10-11T17:57:35Z,,,
arXIv2023,MatFormer: Nested Transformer for Elastic Inference,No.,1,"""No evidence""",2023,2023-10-11T17:57:14Z,,,
arXIv2023,Ferret: Refer and Ground Anything Anywhere at Any Granularity,No.,1,"""No evidence""",2023,2023-10-11T17:55:15Z,,,
arXIv2023,VeCLIP: Improving CLIP Training via Visual-enriched Captions,No.,1,"""No evidence""",2023,2023-10-11T17:49:13Z,,,
arXIv2023,LLM4Vis: Explainable Visualization Recommendation using ChatGPT,No.,1,"""No evidence""",2023,2023-10-11T16:51:46Z,,,
arXIv2023,Rethinking the BERT-like Pretraining for DNA Sequences,No.,1,"""No evidence""",2023,2023-10-11T16:40:57Z,,,
arXIv2023,Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models,No.,1,"""No evidence""",2023,2023-10-11T15:56:00Z,,,
arXIv2023,Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT,No.,1,"""No evidence""",2023,2023-10-11T15:20:07Z,,,
arXIv2023,ChatGPT for Computational Topology,No.,1,"""No evidence""",2023,2023-10-11T15:10:07Z,,,
arXIv2023,KwaiYiiMath: Technical Report,No.,1,"""No evidence""",2023,2023-10-11T13:35:05Z,,,
arXIv2023,Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction,No.,1,"""No evidence""",2023,2023-10-11T13:34:22Z,,,
arXIv2023,Towards Foundation Models for Learning on Tabular Data,No.,1,"""No evidence""",2023,2023-10-11T09:37:38Z,,,
arXIv2023,An Empirical Study of Instruction-tuning Large Language Models in Chinese,No.,1,"""No evidence""",2023,2023-10-11T09:18:09Z,,,
arXIv2023,On the Impact of Cross-Domain Data on German Language Models,No.,1,"""No evidence""",2023,2023-10-11T09:09:55Z,,,
arXIv2023,Diffusion Models for Wireless Communications,No.,1,"""No evidence""",2023,2023-10-11T08:57:59Z,,,
arXIv2023,Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions,No.,1,"""No evidence""",2023,2023-10-11T08:36:43Z,,,
arXIv2023,Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators,No.,1,"""No evidence""",2023,2023-10-11T08:22:37Z,,,
arXIv2023,An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT,No.,1,"""No evidence""",2023,2023-10-11T08:16:35Z,,,
arXIv2023,Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog,No.,1,"""No evidence""",2023,2023-10-11T07:37:13Z,,,
arXIv2023,A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation,No.,1,"""No evidence""",2023,2023-10-11T07:30:01Z,,,
arXIv2023,Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs,No.,1,"""No evidence""",2023,2023-10-11T07:27:34Z,,,
arXIv2023,Exploring the landscape of large language models in medical question answering,No.,1,"""No evidence""",2023,2023-10-11T06:26:19Z,,,
arXIv2023,MatChat: A Large Language Model and Application Service Platform for Materials Science,No.,1,"""No evidence""",2023,2023-10-11T05:11:46Z,,,
arXIv2023,PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model,No.,1,"""No evidence""",2023,2023-10-11T03:39:46Z,,,
arXIv2023,LLark: A Multimodal Instruction-Following Language Model for Music,No.,1,"""No evidence""",2023,2023-10-11T03:12:47Z,,,
arXIv2023,"""A Tale of Two Movements"": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction",No.,1,"""No evidence""",2023,2023-10-11T03:01:42Z,,,
arXIv2023,QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources,No.,1,"""No evidence""",2023,2023-10-11T02:47:40Z,,,
arXIv2023,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,No.,1,"""No evidence""",2023,2023-10-11T02:47:21Z,,,
arXIv2023,Risk Assessment and Statistical Significance in the Age of Foundation Models,No.,1,"""No evidence""",2023,2023-10-11T02:08:37Z,,,
arXIv2023,ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations,No.,1,"""No evidence""",2023,2023-10-11T00:39:55Z,,,
arXIv2023,Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning,No.,1,"""No evidence""",2023,2023-10-11T00:18:29Z,,,
arXIv2023,Jaeger: A Concatenation-Based Multi-Transformer VQA Model,No.,1,"""No evidence""",2023,2023-10-11T00:14:40Z,,,
arXIv2023,Diversity of Thought Improves Reasoning Abilities of LLMs,No.,1,"""No evidence""",2023,2023-10-11T00:01:41Z,,,
arXIv2023,Large Language Models can Learn Rules,No.,1,"""No evidence""",2023,2023-10-10T23:07:01Z,,,
arXIv2023,Acoustic Model Fusion for End-to-end Speech Recognition,No.,1,"""No evidence""",2023,2023-10-10T23:00:17Z,,,
arXIv2023,DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records,No.,1,"""No evidence""",2023,2023-10-10T22:53:15Z,,,
arXIv2023,Automatic Macro Mining from Interaction Traces at Scale,No.,1,"""No evidence""",2023,2023-10-10T21:23:47Z,,,
arXIv2023,LLMs as Potential Brainstorming Partners for Math and Science Problems,No.,1,"""No evidence""",2023,2023-10-10T21:16:35Z,,,
arXIv2023,NEWTON: Are Large Language Models Capable of Physical Reasoning?,No.,1,"""No evidence""",2023,2023-10-10T21:08:51Z,,,
arXIv2023,Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs,No.,1,"""No evidence""",2023,2023-10-10T20:49:43Z,,,
arXIv2023,Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models,No.,1,"""No evidence""",2023,2023-10-10T20:05:13Z,,,
arXIv2023,Sparse Fine-tuning for Inference Acceleration of Large Language Models,No.,1,"""No evidence""",2023,2023-10-10T18:28:38Z,,,
arXIv2023,"AutoAD II: The Sequel -- Who, When, and What in Movie Audio Description",No.,1,"""No evidence""",2023,2023-10-10T17:59:53Z,,,
arXIv2023,Mistral 7B,No.,1,"""No evidence""",2023,2023-10-10T17:54:58Z,,,
arXIv2023,OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text,No.,1,"""No evidence""",2023,2023-10-10T16:57:28Z,,,
arXIv2023,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,No.,1,"""No evidence""",2023,2023-10-10T16:47:29Z,,,
arXIv2023,TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models,No.,1,"""No evidence""",2023,2023-10-10T16:38:49Z,,,
arXIv2023,Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning,No.,1,"""No evidence""",2023,2023-10-10T15:13:30Z,,,
arXIv2023,Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models,No.,1,"""No evidence""",2023,2023-10-10T15:10:03Z,,,
arXIv2023,SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA,No.,1,"""No evidence""",2023,2023-10-10T14:50:20Z,,,
arXIv2023,Making Large Language Models Perform Better in Knowledge Graph Completion,No.,1,"""No evidence""",2023,2023-10-10T14:47:09Z,,,
arXIv2023,What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models,No.,1,"""No evidence""",2023,2023-10-10T13:45:59Z,,,
arXIv2023,Rationale-Enhanced Language Models are Better Continual Relation Learners,No.,1,"""No evidence""",2023,2023-10-10T11:50:27Z,,,
arXIv2023,A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A Romanian Clickbait Corpus of News Articles,No.,1,"""No evidence""",2023,2023-10-10T11:38:16Z,,,
arXIv2023,Evaluation of ChatGPT Feedback on ELL Writers' Coherence and Cohesion,No.,1,"""No evidence""",2023,2023-10-10T10:25:56Z,,,
arXIv2023,Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task,No.,1,"""No evidence""",2023,2023-10-10T10:22:05Z,,,
arXIv2023,The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis,No.,1,"""No evidence""",2023,2023-10-10T10:19:58Z,,,
arXIv2023,Understanding the Effects of RLHF on LLM Generalisation and Diversity,No.,1,"""No evidence""",2023,2023-10-10T09:25:44Z,,,
arXIv2023,Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic Reasoning Task 2023,No.,1,"""No evidence""",2023,2023-10-10T09:12:27Z,,,
arXIv2023,Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition,No.,1,"""No evidence""",2023,2023-10-10T09:04:33Z,,,
arXIv2023,Humans and language models diverge when predicting repeating text,No.,1,"""No evidence""",2023,2023-10-10T08:24:28Z,,,
arXIv2023,"Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users",No.,1,"""No evidence""",2023,2023-10-10T07:54:24Z,,,
arXIv2023,Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations,No.,1,"""No evidence""",2023,2023-10-10T07:50:29Z,,,
arXIv2023,A Semantic Invariant Robust Watermark for Large Language Models,No.,1,"""No evidence""",2023,2023-10-10T06:49:43Z,,,
arXIv2023,Dobby: A Conversational Service Robot Driven by GPT-4,No.,1,"""No evidence""",2023,2023-10-10T04:34:00Z,,,
arXIv2023,Selective Demonstrations for Cross-domain Text-to-SQL,No.,1,"""No evidence""",2023,2023-10-10T04:31:41Z,,,
arXIv2023,MuseChat: A Conversational Music Recommendation System for Videos,No.,1,"""No evidence""",2023,2023-10-10T03:32:33Z,,,
arXIv2023,BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models,No.,1,"""No evidence""",2023,2023-10-10T03:18:26Z,,,
arXIv2023,Let Models Speak Ciphers: Multiagent Debate through Embeddings,No.,1,"""No evidence""",2023,2023-10-10T03:06:38Z,,,
arXIv2023,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,No.,1,"""No evidence""",2023,2023-10-10T02:38:44Z,,,
arXIv2023,An experiment on an automated literature survey of data-driven speech enhancement methods,No.,1,"""No evidence""",2023,2023-10-10T02:07:24Z,,,
arXIv2023,Get the gist? Using large language models for few-shot decontextualization,No.,1,"""No evidence""",2023,2023-10-10T02:00:00Z,,,
arXIv2023,We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses,No.,1,"""No evidence""",2023,2023-10-10T01:44:47Z,,,
arXIv2023,Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI,No.,1,"""No evidence""",2023,2023-10-10T00:41:38Z,,,
arXIv2023,GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models,No.,1,"""No evidence""",2023,2023-10-10T00:39:04Z,,,
arXIv2023,GeoLLM: Extracting Geospatial Knowledge from Large Language Models,No.,1,"""No evidence""",2023,2023-10-10T00:03:23Z,,,
arXIv2023,Estimating Numbers without Regression,No.,1,"""No evidence""",2023,2023-10-09T23:07:05Z,,,
arXIv2023,GPT-who: An Information Density-based Machine-Generated Text Detector,No.,1,"""No evidence""",2023,2023-10-09T23:06:05Z,,,
arXIv2023,The Importance of Prompt Tuning for Automated Neuron Explanations,No.,1,"""No evidence""",2023,2023-10-09T23:02:07Z,,,
arXIv2023,Factual and Personalized Recommendations using Language Models and Reinforcement Learning,No.,1,"""No evidence""",2023,2023-10-09T21:58:55Z,,,
arXIv2023,CAW-coref: Conjunction-Aware Word-level Coreference Resolution,No.,1,"""No evidence""",2023,2023-10-09T21:32:49Z,,,
arXIv2023,"Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond",No.,1,"""No evidence""",2023,2023-10-09T20:49:42Z,,,
arXIv2023,Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models,No.,1,"""No evidence""",2023,2023-10-09T19:48:55Z,,,
arXIv2023,BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions,No.,1,"""No evidence""",2023,2023-10-09T19:37:38Z,,,
arXIv2023,Transformers and Large Language Models for Chemistry and Drug Discovery,No.,1,"""No evidence""",2023,2023-10-09T18:40:04Z,,,
arXIv2023,Auditing Gender Analyzers on Text Data,No.,1,"""No evidence""",2023,2023-10-09T18:13:07Z,,,
arXIv2023,LLM for SoC Security: A Paradigm Shift,No.,1,"""No evidence""",2023,2023-10-09T18:02:38Z,,,
arXIv2023,FireAct: Toward Language Agent Fine-tuning,No.,1,"""No evidence""",2023,2023-10-09T17:58:38Z,,,
arXIv2023,NEFTune: Noisy Embeddings Improve Instruction Finetuning,No.,1,"""No evidence""",2023,2023-10-09T17:58:34Z,,,
arXIv2023,TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models,No.,1,"""No evidence""",2023,2023-10-09T17:49:50Z,,,
arXIv2023,Are Large Language Models Geospatially Knowledgeable?,No.,1,"""No evidence""",2023,2023-10-09T17:20:11Z,,,
arXIv2023,Generative quantum machine learning via denoising diffusion probabilistic models,No.,1,"""No evidence""",2023,2023-10-09T17:03:08Z,,,
arXIv2023,Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-10-09T17:00:20Z,,,
arXIv2023,"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",No.,1,"""No evidence""",2023,2023-10-09T16:57:57Z,,,
arXIv2023,A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models,No.,1,"""No evidence""",2023,2023-10-09T16:22:11Z,,,
arXIv2023,Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting,No.,1,"""No evidence""",2023,2023-10-09T16:08:23Z,,,
arXIv2023,Are Large Language Models Post Hoc Explainers?,No.,1,"""No evidence""",2023,2023-10-09T15:31:03Z,,,
arXIv2023,Rethinking Memory and Communication Cost for Efficient Large Language Model Training,No.,1,"""No evidence""",2023,2023-10-09T15:08:32Z,,,
arXIv2023,Foundation Models Meet Visualizations: Challenges and Opportunities,No.,1,"""No evidence""",2023,2023-10-09T14:57:05Z,,,
arXIv2023,Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena,No.,1,"""No evidence""",2023,2023-10-09T14:22:09Z,,,
arXIv2023,Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation,No.,1,"""No evidence""",2023,2023-10-09T14:10:29Z,,,
arXIv2023,LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models,No.,1,"""No evidence""",2023,2023-10-09T14:10:21Z,,,
arXIv2023,The Program Testing Ability of Large Language Models for Code,No.,1,"""No evidence""",2023,2023-10-09T13:55:45Z,,,
arXIv2023,Transformer Fusion with Optimal Transport,No.,1,"""No evidence""",2023,2023-10-09T13:40:31Z,,,
arXIv2023,Guiding Language Model Math Reasoning with Planning Tokens,No.,1,"""No evidence""",2023,2023-10-09T13:29:37Z,,,
arXIv2023,Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors,No.,1,"""No evidence""",2023,2023-10-09T13:29:36Z,,,
arXIv2023,Abstractive Summarization of Large Document Collections Using GPT,No.,1,"""No evidence""",2023,2023-10-09T13:06:21Z,,,
arXIv2023,The potential of large language models for improving probability learning: A study on ChatGPT3.5 and first-year computer engineering students,No.,1,"""No evidence""",2023,2023-10-09T12:54:58Z,,,
arXIv2023,Automated Argument Generation from Legal Facts,No.,1,"""No evidence""",2023,2023-10-09T12:49:35Z,,,
arXIv2023,Making Scalable Meta Learning Practical,No.,1,"""No evidence""",2023,2023-10-09T12:45:13Z,,,
arXIv2023,A Closer Look into Automatic Evaluation Using Large Language Models,No.,1,"""No evidence""",2023,2023-10-09T12:12:55Z,,,
arXIv2023,Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution,No.,1,"""No evidence""",2023,2023-10-09T11:45:59Z,,,
arXIv2023,Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models,No.,1,"""No evidence""",2023,2023-10-09T11:34:41Z,,,
arXIv2023,Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction,No.,1,"""No evidence""",2023,2023-10-09T11:34:18Z,,,
arXIv2023,STREAM: Social data and knowledge collective intelligence platform for TRaining Ethical AI Models,No.,1,"""No evidence""",2023,2023-10-09T09:40:11Z,,,
arXIv2023,Regulation and NLP (RegNLP): Taming Large Language Models,No.,1,"""No evidence""",2023,2023-10-09T09:22:40Z,,,
arXIv2023,Integrating Graphs with Large Language Models: Methods and Prospects,No.,1,"""No evidence""",2023,2023-10-09T07:59:34Z,,,
arXIv2023,How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,No.,1,"""No evidence""",2023,2023-10-09T07:56:16Z,,,
arXIv2023,Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces,No.,1,"""No evidence""",2023,2023-10-09T07:41:19Z,,,
arXIv2023,Generative Judge for Evaluating Alignment,No.,1,"""No evidence""",2023,2023-10-09T07:27:15Z,,,
arXIv2023,Parrot Mind: Towards Explaining the Complex Task Reasoning of Pretrained Large Language Models with Template-Content Structure,No.,1,"""No evidence""",2023,2023-10-09T06:57:45Z,,,
arXIv2023,Empower Nested Boolean Logic via Self-Supervised Curriculum Learning,No.,1,"""No evidence""",2023,2023-10-09T06:54:02Z,,,
arXIv2023,Establishing Trustworthiness: Rethinking Tasks and Model Evaluation,No.,1,"""No evidence""",2023,2023-10-09T06:32:10Z,,,
arXIv2023,Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations,No.,1,"""No evidence""",2023,2023-10-09T05:35:10Z,,,
arXIv2023,Exploring the Maze of Multilingual Modeling,No.,1,"""No evidence""",2023,2023-10-09T04:48:14Z,,,
arXIv2023,Augmented Embeddings for Custom Retrievals,No.,1,"""No evidence""",2023,2023-10-09T03:29:35Z,,,
arXIv2023,Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training,No.,1,"""No evidence""",2023,2023-10-09T02:22:00Z,,,
arXIv2023,Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation,No.,1,"""No evidence""",2023,2023-10-09T00:45:20Z,,,
arXIv2023,Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond,No.,1,"""No evidence""",2023,2023-10-09T00:20:59Z,,,
arXIv2023,Enhancing Pre-Trained Language Models with Sentence Position Embeddings for Rhetorical Roles Recognition in Legal Opinions,No.,1,"""No evidence""",2023,2023-10-08T20:33:55Z,,,
arXIv2023,Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models,No.,1,"""No evidence""",2023,2023-10-08T18:04:05Z,,,
arXIv2023,ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data,No.,1,"""No evidence""",2023,2023-10-08T17:23:17Z,,,
arXIv2023,Generative Spoken Language Model based on continuous word-sized audio tokens,No.,1,"""No evidence""",2023,2023-10-08T16:46:14Z,,,
arXIv2023,Probing Language Models from A Human Behavioral Perspective,No.,1,"""No evidence""",2023,2023-10-08T16:16:21Z,,,
arXIv2023,Towards Optimizing with Large Language Models,No.,1,"""No evidence""",2023,2023-10-08T15:35:00Z,,,
arXIv2023,Optimizing Large Language Models to Expedite the Development of Smart Contracts,No.,1,"""No evidence""",2023,2023-10-08T14:29:33Z,,,
arXIv2023,Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model,No.,1,"""No evidence""",2023,2023-10-08T13:07:42Z,,,
arXIv2023,Retrieval-Generation Synergy Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-10-08T12:50:57Z,,,
arXIv2023,Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements,No.,1,"""No evidence""",2023,2023-10-08T12:21:24Z,,,
arXIv2023,InstructDET: Diversifying Referring Object Detection with Generalized Instructions,No.,1,"""No evidence""",2023,2023-10-08T12:10:44Z,,,
arXIv2023,Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature,No.,1,"""No evidence""",2023,2023-10-08T11:41:28Z,,,
arXIv2023,UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model,No.,1,"""No evidence""",2023,2023-10-08T11:33:09Z,,,
arXIv2023,Breaking Down Word Semantics from Pre-trained Language Models through Layer-wise Dimension Selection,No.,1,"""No evidence""",2023,2023-10-08T11:07:19Z,,,
arXIv2023,Zero-Shot Detection of Machine-Generated Codes,No.,1,"""No evidence""",2023,2023-10-08T10:08:21Z,,,
arXIv2023,How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts,No.,1,"""No evidence""",2023,2023-10-08T09:53:46Z,,,
arXIv2023,Benchmarking Large Language Models with Augmented Instructions for Fine-grained Information Extraction,No.,1,"""No evidence""",2023,2023-10-08T09:41:18Z,,,
arXIv2023,Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?,No.,1,"""No evidence""",2023,2023-10-08T09:05:14Z,,,
arXIv2023,Guideline Learning for In-context Information Extraction,No.,1,"""No evidence""",2023,2023-10-08T08:25:16Z,,,
arXIv2023,Video-CSR: Complex Video Digest Creation for Visual-Language Models,No.,1,"""No evidence""",2023,2023-10-08T08:02:43Z,,,
arXIv2023,BRAINTEASER: Lateral Thinking Puzzles for Large Language Models,No.,1,"""No evidence""",2023,2023-10-08T07:46:01Z,,,
arXIv2023,"FakeGPT: Fake News Generation, Explanation and Detection of Large Language Models",No.,1,"""No evidence""",2023,2023-10-08T07:01:07Z,,,
arXIv2023,Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection,No.,1,"""No evidence""",2023,2023-10-08T06:36:26Z,,,
arXIv2023,Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think -- Introducing AI Detectability Index,No.,1,"""No evidence""",2023,2023-10-08T06:20:36Z,,,
arXIv2023,Synslator: An Interactive Machine Translation Tool with Online Learning,No.,1,"""No evidence""",2023,2023-10-08T06:05:55Z,,,
arXIv2023,"Building an Open-Vocabulary Video CLIP Model with Better Architectures, Optimization and Data",No.,1,"""No evidence""",2023,2023-10-08T04:46:43Z,,,
arXIv2023,MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering,No.,1,"""No evidence""",2023,2023-10-08T04:44:36Z,,,
arXIv2023,Distantly-Supervised Joint Entity and Relation Extraction with Noise-Robust Learning,No.,1,"""No evidence""",2023,2023-10-08T03:42:15Z,,,
arXIv2023,Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling,No.,1,"""No evidence""",2023,2023-10-08T03:35:27Z,,,
arXIv2023,"The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations",No.,1,"""No evidence""",2023,2023-10-08T03:31:29Z,,,
arXIv2023,A new economic and financial theory of money,No.,1,"""No evidence""",2023,2023-10-08T03:16:06Z,,,
arXIv2023,Compositional Semantics for Open Vocabulary Spatio-semantic Representations,No.,1,"""No evidence""",2023,2023-10-08T03:07:14Z,,,
arXIv2023,MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks,No.,1,"""No evidence""",2023,2023-10-08T01:51:17Z,,,
arXIv2023,LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation,No.,1,"""No evidence""",2023,2023-10-08T01:43:39Z,,,
arXIv2023,Exploring the Usage of Chinese Pinyin in Pretraining,No.,1,"""No evidence""",2023,2023-10-08T01:26:44Z,,,
arXIv2023,Towards Better Chain-of-Thought Prompting Strategies: A Survey,No.,1,"""No evidence""",2023,2023-10-08T01:16:55Z,,,
arXIv2023,CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation,No.,1,"""No evidence""",2023,2023-10-08T00:16:18Z,,,
arXIv2023,Domain Knowledge Graph Construction Via A Simple Checker,No.,1,"""No evidence""",2023,2023-10-08T00:09:31Z,,,
arXIv2023,TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting,No.,1,"""No evidence""",2023,2023-10-08T00:02:25Z,,,
arXIv2023,Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy,No.,1,"""No evidence""",2023,2023-10-07T23:29:00Z,,,
arXIv2023,Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data,No.,1,"""No evidence""",2023,2023-10-07T23:25:22Z,,,
arXIv2023,Large Language Models for Spatial Trajectory Patterns Mining,No.,1,"""No evidence""",2023,2023-10-07T23:21:29Z,,,
arXIv2023,HowToCaption: Prompting LLMs to Transform Video Annotations at Scale,No.,1,"""No evidence""",2023,2023-10-07T19:32:55Z,,,
arXIv2023,Question-focused Summarization by Decomposing Articles into Facts and Opinions and Retrieving Entities,No.,1,"""No evidence""",2023,2023-10-07T17:37:48Z,,,
arXIv2023,Hybrid Recommendation System using Graph Neural Network and BERT Embeddings,No.,1,"""No evidence""",2023,2023-10-07T17:24:41Z,,,
arXIv2023,Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models,No.,1,"""No evidence""",2023,2023-10-07T17:16:34Z,,,
arXIv2023,Lemur: Integrating Large Language Models in Automated Program Verification,No.,1,"""No evidence""",2023,2023-10-07T16:44:53Z,,,
arXIv2023,ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations,No.,1,"""No evidence""",2023,2023-10-07T16:32:34Z,,,
arXIv2023,On the Evolution of Knowledge Graphs: A Survey and Perspective,No.,1,"""No evidence""",2023,2023-10-07T14:46:51Z,,,
arXIv2023,An Evaluation of State-of-the-Art Large Language Models for Sarcasm Detection,No.,1,"""No evidence""",2023,2023-10-07T14:45:43Z,,,
arXIv2023,Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages,No.,1,"""No evidence""",2023,2023-10-07T13:34:21Z,,,
arXIv2023,FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets,No.,1,"""No evidence""",2023,2023-10-07T12:52:58Z,,,
arXIv2023,DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures,No.,1,"""No evidence""",2023,2023-10-07T09:10:28Z,,,
arXIv2023,Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API,No.,1,"""No evidence""",2023,2023-10-07T07:22:41Z,,,
arXIv2023,Offline Imitation Learning with Variational Counterfactual Reasoning,No.,1,"""No evidence""",2023,2023-10-07T06:52:18Z,,,
arXIv2023,Integrating Contrastive Learning into a Multitask Transformer Model for Effective Domain Adaptation,No.,1,"""No evidence""",2023,2023-10-07T06:41:29Z,,,
arXIv2023,EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling,No.,1,"""No evidence""",2023,2023-10-07T05:37:41Z,,,
arXIv2023,DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries,No.,1,"""No evidence""",2023,2023-10-07T03:25:06Z,,,
arXIv2023,"LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT",No.,1,"""No evidence""",2023,2023-10-07T03:17:59Z,,,
arXIv2023,Do self-supervised speech and language models extract similar representations as human brain?,No.,1,"""No evidence""",2023,2023-10-07T01:39:56Z,,,
arXIv2023,Automatic Anonymization of Swiss Federal Supreme Court Rulings,No.,1,"""No evidence""",2023,2023-10-07T00:56:49Z,,,
arXIv2023,Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning,No.,1,"""No evidence""",2023,2023-10-06T23:46:33Z,,,
arXIv2023,Copy Suppression: Comprehensively Understanding an Attention Head,No.,1,"""No evidence""",2023,2023-10-06T23:37:24Z,,,
arXIv2023,DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies,No.,1,"""No evidence""",2023,2023-10-06T22:05:15Z,,,
arXIv2023,A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators,No.,1,"""No evidence""",2023,2023-10-06T21:55:57Z,,,
arXIv2023,Segmented Harmonic Loss: Handling Class-Imbalanced Multi-Label Clinical Data for Medical Coding with Large Language Models,No.,1,"""No evidence""",2023,2023-10-06T21:20:28Z,,,
arXIv2023,Self-Confirming Transformer for Locally Consistent Online Adaptation in Multi-Agent Reinforcement Learning,No.,1,"""No evidence""",2023,2023-10-06T20:43:08Z,,,
arXIv2023,Can pruning make Large Language Models more efficient?,No.,1,"""No evidence""",2023,2023-10-06T20:28:32Z,,,
arXIv2023,Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations,No.,1,"""No evidence""",2023,2023-10-06T20:13:07Z,,,
arXIv2023,ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models,No.,1,"""No evidence""",2023,2023-10-06T20:01:33Z,,,
arXIv2023,Talk like a Graph: Encoding Graphs for Large Language Models,No.,1,"""No evidence""",2023,2023-10-06T19:55:21Z,,,
arXIv2023,LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation,No.,1,"""No evidence""",2023,2023-10-06T19:02:04Z,,,
arXIv2023,BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity,No.,1,"""No evidence""",2023,2023-10-06T17:59:53Z,,,
arXIv2023,Why Do We Need Weight Decay in Modern Deep Learning?,No.,1,"""No evidence""",2023,2023-10-06T17:58:21Z,,,
arXIv2023,RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation,No.,1,"""No evidence""",2023,2023-10-06T17:55:36Z,,,
arXIv2023,Policy-Gradient Training of Language Models for Ranking,No.,1,"""No evidence""",2023,2023-10-06T17:55:23Z,,,
arXIv2023,Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models,No.,1,"""No evidence""",2023,2023-10-06T17:55:11Z,,,
arXIv2023,An In-Context Learning Agent for Formal Theorem-Proving,No.,1,"""No evidence""",2023,2023-10-06T16:21:22Z,,,
arXIv2023,KoMultiText: Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services,No.,1,"""No evidence""",2023,2023-10-06T15:19:39Z,,,
arXIv2023,A Process for Topic Modelling Via Word Embeddings,No.,1,"""No evidence""",2023,2023-10-06T15:10:35Z,,,
arXIv2023,Coding by Design: GPT-4 empowers Agile Model Driven Development,No.,1,"""No evidence""",2023,2023-10-06T15:05:05Z,,,
arXIv2023,From task structures to world models: What do LLMs know?,No.,1,"""No evidence""",2023,2023-10-06T14:21:59Z,,,
arXIv2023,A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks,No.,1,"""No evidence""",2023,2023-10-06T14:16:28Z,,,
arXIv2023,Reward Dropout Improves Control: Bi-objective Perspective on Reinforced LM,No.,1,"""No evidence""",2023,2023-10-06T12:33:32Z,,,
arXIv2023,Auto-survey Challenge,No.,1,"""No evidence""",2023,2023-10-06T09:12:35Z,,,
arXIv2023,Automatic Aspect Extraction from Scientific Texts,No.,1,"""No evidence""",2023,2023-10-06T07:59:54Z,,,
arXIv2023,AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models,No.,1,"""No evidence""",2023,2023-10-06T06:51:16Z,,,
arXIv2023,Demystifying Embedding Spaces using Large Language Models,No.,1,"""No evidence""",2023,2023-10-06T05:27:28Z,,,
arXIv2023,SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation,No.,1,"""No evidence""",2023,2023-10-06T03:33:42Z,,,
arXIv2023,Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models,No.,1,"""No evidence""",2023,2023-10-06T01:40:09Z,,,
arXIv2023,Understanding prompt engineering may not require rethinking generalization,No.,1,"""No evidence""",2023,2023-10-06T00:52:48Z,,,
arXIv2023,Exploring the evolution of research topics during the COVID-19 pandemic,No.,1,"""No evidence""",2023,2023-10-05T22:16:41Z,,,
arXIv2023,Automatic and Human-AI Interactive Text Generation,No.,1,"""No evidence""",2023,2023-10-05T20:26:15Z,,,
arXIv2023,Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report,No.,1,"""No evidence""",2023,2023-10-05T20:10:35Z,,,
arXIv2023,MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning,No.,1,"""No evidence""",2023,2023-10-05T17:52:09Z,,,
arXIv2023,A Long Way to Go: Investigating Length Correlations in RLHF,No.,1,"""No evidence""",2023,2023-10-05T17:38:28Z,,,
arXIv2023,Artificial Intelligence Index Report 2023,No.,1,"""No evidence""",2023,2023-10-05T17:37:58Z,,,
arXIv2023,DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,No.,1,"""No evidence""",2023,2023-10-05T17:37:25Z,,,
arXIv2023,Agent Instructs Large Language Models to be General Zero-Shot Reasoners,No.,1,"""No evidence""",2023,2023-10-05T17:36:16Z,,,
arXIv2023,Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization,No.,1,"""No evidence""",2023,2023-10-05T17:35:26Z,,,
arXIv2023,Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation,No.,1,"""No evidence""",2023,2023-10-05T17:02:59Z,,,
arXIv2023,SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks,No.,1,"""No evidence""",2023,2023-10-05T17:01:53Z,,,
arXIv2023,MapperGPT: Large Language Models for Linking and Mapping Entities,No.,1,"""No evidence""",2023,2023-10-05T16:43:04Z,,,
arXIv2023,Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally,No.,1,"""No evidence""",2023,2023-10-05T15:49:04Z,,,
arXIv2023,Neural Language Model Pruning for Automatic Speech Recognition,No.,1,"""No evidence""",2023,2023-10-05T10:01:32Z,,,
arXIv2023,LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction,No.,1,"""No evidence""",2023,2023-10-05T09:38:09Z,,,
arXIv2023,Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning,No.,1,"""No evidence""",2023,2023-10-05T09:09:44Z,,,
arXIv2023,Procedural Text Mining with Large Language Models,No.,1,"""No evidence""",2023,2023-10-05T08:27:33Z,,,
arXIv2023,Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning,No.,1,"""No evidence""",2023,2023-10-05T04:47:49Z,,,
arXIv2023,DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training,No.,1,"""No evidence""",2023,2023-10-05T03:47:57Z,,,
arXIv2023,Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction,No.,1,"""No evidence""",2023,2023-10-05T03:40:06Z,,,
arXIv2023,A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions,No.,1,"""No evidence""",2023,2023-10-05T03:15:01Z,,,
arXIv2023,Investigating Alternative Feature Extraction Pipelines For Clinical Note Phenotyping,No.,1,"""No evidence""",2023,2023-10-05T02:51:51Z,,,
arXIv2023,UniPredict: Large Language Models are Universal Tabular Classifiers,No.,1,"""No evidence""",2023,2023-10-05T02:37:09Z,,,
arXIv2023,On the Performance of Multimodal Language Models,No.,1,"""No evidence""",2023,2023-10-04T23:33:36Z,,,
arXIv2023,Can Language Models Employ the Socratic Method? Experiments with Code Debugging,No.,1,"""No evidence""",2023,2023-10-04T23:32:33Z,,,
arXIv2023,Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models,No.,1,"""No evidence""",2023,2023-10-04T21:57:09Z,,,
arXIv2023,Neural architecture impact on identifying temporally extended Reinforcement Learning tasks,No.,1,"""No evidence""",2023,2023-10-04T21:09:19Z,,,
arXIv2023,"Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly",No.,1,"""No evidence""",2023,2023-10-04T20:27:20Z,,,
arXIv2023,MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use,No.,1,"""No evidence""",2023,2023-10-04T19:39:26Z,,,
arXIv2023,DP-SGD for non-decomposable objective functions,No.,1,"""No evidence""",2023,2023-10-04T18:48:16Z,,,
arXIv2023,Dual Prompt Tuning for Domain-Aware Federated Learning,No.,1,"""No evidence""",2023,2023-10-04T18:47:34Z,,,
arXIv2023,Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning,No.,1,"""No evidence""",2023,2023-10-04T18:21:17Z,,,
arXIv2023,Discovering Knowledge-Critical Subnetworks in Pretrained Language Models,No.,1,"""No evidence""",2023,2023-10-04T18:02:01Z,,,
arXIv2023,LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving,No.,1,"""No evidence""",2023,2023-10-04T17:59:49Z,,,
arXIv2023,Retrieval meets Long Context Large Language Models,No.,1,"""No evidence""",2023,2023-10-04T17:59:41Z,,,
arXIv2023,Multimodal Question Answering for Unified Information Extraction,No.,1,"""No evidence""",2023,2023-10-04T17:58:05Z,,,
arXIv2023,Kosmos-G: Generating Images in Context with Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-10-04T17:28:44Z,,,
arXIv2023,xVal: A Continuous Number Encoding for Large Language Models,No.,1,"""No evidence""",2023,2023-10-04T17:26:16Z,,,
arXIv2023,T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation,No.,1,"""No evidence""",2023,2023-10-04T17:12:18Z,,,
arXIv2023,UniverSLU: Universal Spoken Language Understanding for Diverse Tasks with Natural Language Instructions,No.,1,"""No evidence""",2023,2023-10-04T17:10:23Z,,,
arXIv2023,DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning,No.,1,"""No evidence""",2023,2023-10-04T16:44:37Z,,,
arXIv2023,A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4,No.,1,"""No evidence""",2023,2023-10-04T16:37:05Z,,,
arXIv2023,GPT-4 as an interface between researchers and computational software: improving usability and reproducibility,No.,1,"""No evidence""",2023,2023-10-04T14:25:39Z,,,
arXIv2023,Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation,No.,1,"""No evidence""",2023,2023-10-04T14:11:12Z,,,
arXIv2023,Delving into CLIP latent space for Video Anomaly Recognition,No.,1,"""No evidence""",2023,2023-10-04T14:01:55Z,,,
arXIv2023,Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms,No.,1,"""No evidence""",2023,2023-10-04T13:37:34Z,,,
arXIv2023,DOMINO: A Dual-System for Multi-step Visual Language Reasoning,No.,1,"""No evidence""",2023,2023-10-04T13:29:47Z,,,
arXIv2023,Integrating UMLS Knowledge into Large Language Models for Medical Question Answering,No.,1,"""No evidence""",2023,2023-10-04T12:50:26Z,,,
arXIv2023,Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms,No.,1,"""No evidence""",2023,2023-10-04T12:14:43Z,,,
arXIv2023,Reward Model Ensembles Help Mitigate Overoptimization,No.,1,"""No evidence""",2023,2023-10-04T11:34:22Z,,,
arXIv2023,Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods,No.,1,"""No evidence""",2023,2023-10-04T09:21:01Z,,,
arXIv2023,Evaluating and Improving Value Judgments in AI: A Scenario-Based Study on Large Language Models' Depiction of Social Conventions,No.,1,"""No evidence""",2023,2023-10-04T08:42:02Z,,,
arXIv2023,AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation,No.,1,"""No evidence""",2023,2023-10-04T08:25:37Z,,,
arXIv2023,Improving Automatic VQA Evaluation Using Large Language Models,No.,1,"""No evidence""",2023,2023-10-04T03:59:57Z,,,
arXIv2023,CITING: Large Language Models Create Curriculum for Instruction Tuning,No.,1,"""No evidence""",2023,2023-10-04T01:58:34Z,,,
arXIv2023,Large Language Models Can Be Good Privacy Protection Learners,No.,1,"""No evidence""",2023,2023-10-03T22:37:01Z,,,
arXIv2023,EcoAssistant: Using LLM Assistant More Affordably and Accurately,No.,1,"""No evidence""",2023,2023-10-03T22:16:13Z,,,
arXIv2023,"The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising ""Alignment"" in Large Language Models",No.,1,"""No evidence""",2023,2023-10-03T22:02:17Z,,,
arXIv2023,Learning Optimal Advantage from Preferences and Mistaking it for Reward,No.,1,"""No evidence""",2023,2023-10-03T21:58:24Z,,,
arXIv2023,AXNav: Replaying Accessibility Tests from Natural Language,No.,1,"""No evidence""",2023,2023-10-03T20:37:58Z,,,
arXIv2023,Can a student Large Language Model perform as well as it's teacher?,No.,1,"""No evidence""",2023,2023-10-03T20:34:59Z,,,
arXIv2023,Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models,No.,1,"""No evidence""",2023,2023-10-03T20:07:06Z,,,
arXIv2023,Automated Bug Generation in the era of Large Language Models,No.,1,"""No evidence""",2023,2023-10-03T20:01:51Z,,,
arXIv2023,A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback,No.,1,"""No evidence""",2023,2023-10-03T18:45:21Z,,,
arXIv2023,Automatic Pair Construction for Contrastive Post-training,No.,1,"""No evidence""",2023,2023-10-03T17:59:46Z,,,
arXIv2023,Generalizable Long-Horizon Manipulations with Large Language Models,No.,1,"""No evidence""",2023,2023-10-03T17:59:46Z,,,
arXIv2023,Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation,No.,1,"""No evidence""",2023,2023-10-03T17:59:32Z,,,
arXIv2023,A Neural Scaling Law from Lottery Ticket Ensembling,No.,1,"""No evidence""",2023,2023-10-03T17:58:33Z,,,
arXIv2023,MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts,No.,1,"""No evidence""",2023,2023-10-03T17:57:24Z,,,
arXIv2023,Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving,No.,1,"""No evidence""",2023,2023-10-03T17:53:51Z,,,
arXIv2023,Harnessing Pre-Trained Sentence Transformers for Offensive Language Detection in Indian Languages,No.,1,"""No evidence""",2023,2023-10-03T17:53:09Z,,,
arXIv2023,MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens,No.,1,"""No evidence""",2023,2023-10-03T17:49:04Z,,,
arXIv2023,Who's Harry Potter? Approximate Unlearning in LLMs,No.,1,"""No evidence""",2023,2023-10-03T17:48:14Z,,,
arXIv2023,Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models,No.,1,"""No evidence""",2023,2023-10-03T17:37:22Z,,,
arXIv2023,Can Language Models be Instructed to Protect Personal Information?,No.,1,"""No evidence""",2023,2023-10-03T17:30:33Z,,,
arXIv2023,Language Models Represent Space and Time,No.,1,"""No evidence""",2023,2023-10-03T17:06:52Z,,,
arXIv2023,What's Next in Affective Modeling? Large Language Models,No.,1,"""No evidence""",2023,2023-10-03T16:39:20Z,,,
arXIv2023,"Ask Again, Then Fail: Large Language Models' Vacillations in Judgement",No.,1,"""No evidence""",2023,2023-10-03T16:08:41Z,,,
arXIv2023,Lyfe Agents: Generative agents for low-cost real-time social interactions,No.,1,"""No evidence""",2023,2023-10-03T16:06:30Z,,,
arXIv2023,Large Language Models Meet Knowledge Graphs to Answer Factoid Questions,No.,1,"""No evidence""",2023,2023-10-03T15:57:00Z,,,
arXIv2023,Conceptual Framework for Autonomous Cognitive Entities,No.,1,"""No evidence""",2023,2023-10-03T15:53:55Z,,,
arXIv2023,Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models,No.,1,"""No evidence""",2023,2023-10-03T15:48:22Z,,,
arXIv2023,Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View,No.,1,"""No evidence""",2023,2023-10-03T15:05:52Z,,,
arXIv2023,TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus,No.,1,"""No evidence""",2023,2023-10-03T14:59:35Z,,,
arXIv2023,Sieve: Multimodal Dataset Pruning Using Image Captioning Models,No.,1,"""No evidence""",2023,2023-10-03T14:53:53Z,,,
arXIv2023,Instances Need More Care: Rewriting Prompts for Instances with LLMs in the Loop Yields Better Zero-Shot Performance,No.,1,"""No evidence""",2023,2023-10-03T14:51:34Z,,,
arXIv2023,An evolutionary model of personality traits related to cooperative behavior using a large language model,No.,1,"""No evidence""",2023,2023-10-03T14:35:27Z,,,
arXIv2023,Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond,No.,1,"""No evidence""",2023,2023-10-03T14:13:36Z,,,
arXIv2023,Tuning Large language model for End-to-end Speech Translation,No.,1,"""No evidence""",2023,2023-10-03T13:43:50Z,,,
arXIv2023,Language Models as Knowledge Bases for Visual Word Sense Disambiguation,No.,1,"""No evidence""",2023,2023-10-03T11:11:55Z,,,
arXIv2023,Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving,No.,1,"""No evidence""",2023,2023-10-03T11:05:14Z,,,
arXIv2023,AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval,No.,1,"""No evidence""",2023,2023-10-03T08:34:44Z,,,
arXIv2023,DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models,No.,1,"""No evidence""",2023,2023-10-03T08:15:20Z,,,
arXIv2023,Benchmarking and Improving Generator-Validator Consistency of Language Models,No.,1,"""No evidence""",2023,2023-10-03T07:23:22Z,,,
arXIv2023,Can large language models provide useful feedback on research papers? A large-scale empirical analysis,No.,1,"""No evidence""",2023,2023-10-03T04:14:17Z,,,
arXIv2023,Differentially Encoded Observation Spaces for Perceptive Reinforcement Learning,No.,1,"""No evidence""",2023,2023-10-03T03:23:10Z,,,
arXIv2023,Nugget: Neural Agglomerative Embeddings of Text,No.,1,"""No evidence""",2023,2023-10-03T01:47:49Z,,,
arXIv2023,HPC-GPT: Integrating Large Language Model for High-Performance Computing,No.,1,"""No evidence""",2023,2023-10-03T01:34:55Z,,,
arXIv2023,Large Language Models as Analogical Reasoners,No.,1,"""No evidence""",2023,2023-10-03T00:57:26Z,,,
arXIv2023,LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model,No.,1,"""No evidence""",2023,2023-10-02T23:29:23Z,,,
arXIv2023,Closing the Curious Case of Neural Text Degeneration,No.,1,"""No evidence""",2023,2023-10-02T23:16:25Z,,,
arXIv2023,Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models,No.,1,"""No evidence""",2023,2023-10-02T23:12:21Z,,,
arXIv2023,What's the Magic Word? A Control Theory of LLM Prompting,No.,1,"""No evidence""",2023,2023-10-02T22:35:40Z,,,
arXIv2023,Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations,No.,1,"""No evidence""",2023,2023-10-02T21:27:57Z,,,
arXIv2023,CAT-LM: Training Language Models on Aligned Code And Tests,No.,1,"""No evidence""",2023,2023-10-02T19:52:22Z,,,
arXIv2023,Making Retrieval-Augmented Language Models Robust to Irrelevant Context,No.,1,"""No evidence""",2023,2023-10-02T18:52:35Z,,,
arXIv2023,SmartPlay: A Benchmark for LLMs as Intelligent Agents,No.,1,"""No evidence""",2023,2023-10-02T18:52:11Z,,,
arXIv2023,GPT-Driver: Learning to Drive with GPT,No.,1,"""No evidence""",2023,2023-10-02T17:59:57Z,,,
arXIv2023,DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model,No.,1,"""No evidence""",2023,2023-10-02T17:59:52Z,,,
arXIv2023,Representation Engineering: A Top-Down Approach to AI Transparency,No.,1,"""No evidence""",2023,2023-10-02T17:59:07Z,,,
arXIv2023,Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench,No.,1,"""No evidence""",2023,2023-10-02T17:46:09Z,,,
arXIv2023,Compressing LLMs: The Truth is Rarely Pure and Never Simple,No.,1,"""No evidence""",2023,2023-10-02T17:42:37Z,,,
arXIv2023,UltraFeedback: Boosting Language Models with High-quality Feedback,No.,1,"""No evidence""",2023,2023-10-02T17:40:01Z,,,
arXIv2023,GenSim: Generating Robotic Simulation Tasks via Large Language Models,No.,1,"""No evidence""",2023,2023-10-02T17:23:48Z,,,
arXIv2023,RA-DIT: Retrieval-Augmented Dual Instruction Tuning,No.,1,"""No evidence""",2023,2023-10-02T17:16:26Z,,,
arXIv2023,Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games,No.,1,"""No evidence""",2023,2023-10-02T16:55:37Z,,,
arXIv2023,L2MAC: Large Language Model Automatic Computer for Extensive Code Generation,No.,1,"""No evidence""",2023,2023-10-02T16:55:19Z,,,
arXIv2023,ChoiceMates: Supporting Unfamiliar Online Decision-Making with Multi-Agent Conversational Interactions,No.,1,"""No evidence""",2023,2023-10-02T16:49:39Z,,,
arXIv2023,BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models,No.,1,"""No evidence""",2023,2023-10-02T16:48:47Z,,,
arXIv2023,FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models,No.,1,"""No evidence""",2023,2023-10-02T16:43:14Z,,,
arXIv2023,On the Generalization of Training-based ChatGPT Detection Methods,No.,1,"""No evidence""",2023,2023-10-02T16:13:08Z,,,
arXIv2023,Co-audit: tools to help humans double-check AI-generated content,No.,1,"""No evidence""",2023,2023-10-02T15:59:10Z,,,
arXIv2023,Improving Emotional Expression and Cohesion in Image-Based Playlist Description and Music Topics: A Continuous Parameterization Approach,No.,1,"""No evidence""",2023,2023-10-02T14:32:07Z,,,
arXIv2023,Making LLaMA SEE and Draw with SEED Tokenizer,No.,1,"""No evidence""",2023,2023-10-02T14:03:02Z,,,
arXIv2023,Label Supervised LLaMA Finetuning,No.,1,"""No evidence""",2023,2023-10-02T13:53:03Z,,,
arXIv2023,NarrativePlay: Interactive Narrative Understanding,No.,1,"""No evidence""",2023,2023-10-02T13:24:00Z,,,
arXIv2023,Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives,No.,1,"""No evidence""",2023,2023-10-02T12:37:23Z,,,
arXIv2023,Target-Aware Contextual Political Bias Detection in News,No.,1,"""No evidence""",2023,2023-10-02T12:25:05Z,,,
arXIv2023,Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models,No.,1,"""No evidence""",2023,2023-10-02T11:49:05Z,,,
arXIv2023,Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning,No.,1,"""No evidence""",2023,2023-10-02T10:14:43Z,,,
arXIv2023,Tool-Augmented Reward Modeling,No.,1,"""No evidence""",2023,2023-10-02T09:47:40Z,,,
arXIv2023,Language Model Decoding as Direct Metrics Optimization,No.,1,"""No evidence""",2023,2023-10-02T09:35:27Z,,,
arXIv2023,Controlling Vision-Language Models for Multi-Task Image Restoration,No.,1,"""No evidence""",2023,2023-10-02T09:10:16Z,,,
arXIv2023,ARN: A Comprehensive Framework and Benchmark for Analogical Reasoning on Narratives,No.,1,"""No evidence""",2023,2023-10-02T08:58:29Z,,,
arXIv2023,TADIS: Steering Models for Deep-Thinking about Demonstration Examples,No.,1,"""No evidence""",2023,2023-10-02T04:42:53Z,,,
arXIv2023,Syllable-level lyrics generation from melody exploiting character-level language model,No.,1,"""No evidence""",2023,2023-10-02T02:53:29Z,,,
arXIv2023,Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers,No.,1,"""No evidence""",2023,2023-10-02T02:01:16Z,,,
arXIv2023,Application of frozen large-scale models to multimodal task-oriented dialogue,No.,1,"""No evidence""",2023,2023-10-02T01:42:28Z,,,
arXIv2023,TRAM: Benchmarking Temporal Reasoning for Large Language Models,No.,1,"""No evidence""",2023,2023-10-02T00:59:07Z,,,
arXIv2023,Necessary and Sufficient Watermark for Large Language Models,No.,1,"""No evidence""",2023,2023-10-02T00:48:51Z,,,
arXIv2023,Natural Language Models for Data Visualization Utilizing nvBench Dataset,No.,1,"""No evidence""",2023,2023-10-02T00:48:01Z,,,
arXIv2023,Parameter-Efficient Tuning Helps Language Model Alignment,No.,1,"""No evidence""",2023,2023-10-01T23:27:14Z,,,
arXIv2023,Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks,No.,1,"""No evidence""",2023,2023-10-01T21:06:15Z,,,
arXIv2023,Pre-training with Synthetic Data Helps Offline Reinforcement Learning,No.,1,"""No evidence""",2023,2023-10-01T19:32:14Z,,,
arXIv2023,Analyzing and Mitigating Object Hallucination in Large Vision-Language Models,No.,1,"""No evidence""",2023,2023-10-01T18:10:53Z,,,
arXIv2023,TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks,No.,1,"""No evidence""",2023,2023-10-01T18:01:51Z,,,
arXIv2023,SEED: Domain-Specific Data Curation With Large Language Models,No.,1,"""No evidence""",2023,2023-10-01T17:59:20Z,,,
arXIv2023,"RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models",No.,1,"""No evidence""",2023,2023-10-01T17:52:59Z,,,
arXIv2023,My Machine and I: ChatGPT and the Future of Human-Machine Collaboration in Africa,No.,1,"""No evidence""",2023,2023-10-01T17:27:51Z,,,
arXIv2023,Robust Sentiment Analysis for Low Resource languages Using Data Augmentation Approaches: A Case Study in Marathi,No.,1,"""No evidence""",2023,2023-10-01T17:09:31Z,,,
arXIv2023,Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips,No.,1,"""No evidence""",2023,2023-10-01T15:13:48Z,,,
arXIv2023,Meta Semantic Template for Evaluation of Large Language Models,No.,1,"""No evidence""",2023,2023-10-01T15:06:51Z,,,
arXIv2023,The Robots are Here: Navigating the Generative AI Revolution in Computing Education,No.,1,"""No evidence""",2023,2023-10-01T12:54:37Z,,,
arXIv2023,Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants,No.,1,"""No evidence""",2023,2023-10-01T12:35:18Z,,,
arXIv2023,Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning,No.,1,"""No evidence""",2023,2023-10-01T12:28:36Z,,,
arXIv2023,WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data,No.,1,"""No evidence""",2023,2023-10-01T12:02:57Z,,,
arXIv2023,Knowledge Engineering using Large Language Models,No.,1,"""No evidence""",2023,2023-10-01T10:26:25Z,,,
arXIv2023,Adapting LLM Agents Through Communication,No.,1,"""No evidence""",2023,2023-10-01T07:50:30Z,,,
arXIv2023,Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs,No.,1,"""No evidence""",2023,2023-10-01T05:53:15Z,,,
arXIv2023,Nine-year-old children outperformed ChatGPT in emotion: Evidence from Chinese writing,No.,1,"""No evidence""",2023,2023-10-01T05:37:55Z,,,
arXIv2023,"Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models",No.,1,"""No evidence""",2023,2023-10-01T03:50:34Z,,,
arXIv2023,SELF: Self-Evolution with Language Feedback,No.,1,"""No evidence""",2023,2023-10-01T00:52:24Z,,,
arXIv2023,From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning,No.,1,"""No evidence""",2023,2023-09-30T21:16:05Z,,,
arXIv2023,Prompting Code Interpreter to Write Better Unit Tests on Quixbugs Functions,No.,1,"""No evidence""",2023,2023-09-30T20:36:23Z,,,
arXIv2023,UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities,No.,1,"""No evidence""",2023,2023-09-30T20:18:50Z,,,
arXIv2023,Question-Answering Model for Schizophrenia Symptoms and Their Impact on Daily Life using Mental Health Forums Data,No.,1,"""No evidence""",2023,2023-09-30T17:50:50Z,,,
arXIv2023,PixArt-$?$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis,No.,1,"""No evidence""",2023,2023-09-30T16:18:00Z,,,
arXIv2023,InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists,No.,1,"""No evidence""",2023,2023-09-30T14:26:43Z,,,
arXIv2023,Dynamic Demonstrations Controller for In-Context Learning,No.,1,"""No evidence""",2023,2023-09-30T14:04:22Z,,,
arXIv2023,AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ,No.,1,"""No evidence""",2023,2023-09-30T13:15:49Z,,,
arXIv2023,Gaze-Driven Sentence Simplification for Language Learners: Enhancing Comprehension and Readability,No.,1,"""No evidence""",2023,2023-09-30T12:18:31Z,,,
arXIv2023,Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models,No.,1,"""No evidence""",2023,2023-09-30T09:35:50Z,,,
arXIv2023,Decoding In-Context Learning: Neuroscience-inspired Analysis of Representations in Large Language Models,No.,1,"""No evidence""",2023,2023-09-30T09:01:35Z,,,
arXIv2023,Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method,No.,1,"""No evidence""",2023,2023-09-30T08:33:04Z,,,
arXIv2023,RelBERT: Embedding Relations with Language Models,No.,1,"""No evidence""",2023,2023-09-30T08:15:36Z,,,
arXIv2023,Graph Neural Architecture Search with GPT-4,No.,1,"""No evidence""",2023,2023-09-30T08:05:59Z,,,
arXIv2023,Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,No.,1,"""No evidence""",2023,2023-09-30T07:11:39Z,,,
arXIv2023,NAYER: Noisy Layer Data Generation for Efficient and Effective Data-free Knowledge Distillation,No.,1,"""No evidence""",2023,2023-09-30T05:19:10Z,,,
arXIv2023,SLM: Bridge the thin gap between speech and text foundation models,No.,1,"""No evidence""",2023,2023-09-30T02:27:45Z,,,
arXIv2023,A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models,No.,1,"""No evidence""",2023,2023-09-30T00:10:14Z,,,
arXIv2023,Motif: Intrinsic Motivation from Artificial Intelligence Feedback,No.,1,"""No evidence""",2023,2023-09-29T22:10:01Z,,,
arXIv2023,Self-Specialization: Uncovering Latent Expertise within Large Language Models,No.,1,"""No evidence""",2023,2023-09-29T21:53:46Z,,,
arXIv2023,Learning to Rewrite Prompts for Personalized Text Generation,No.,1,"""No evidence""",2023,2023-09-29T21:15:49Z,,,
arXIv2023,One for All: Towards Training One Graph Model for All Classification Tasks,No.,1,"""No evidence""",2023,2023-09-29T21:15:26Z,,,
arXIv2023,ABScribe: Rapid Exploration & Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models,No.,1,"""No evidence""",2023,2023-09-29T20:11:15Z,,,
arXIv2023,Multilingual Natural Language Processing Model for Radiology Reports -- The Summary is all you need!,No.,1,"""No evidence""",2023,2023-09-29T19:20:27Z,,,
arXIv2023,Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality,No.,1,"""No evidence""",2023,2023-09-29T19:06:52Z,,,
arXIv2023,Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks,No.,1,"""No evidence""",2023,2023-09-29T18:35:52Z,,,
arXIv2023,Optimizing with Low Budgets: a Comparison on the Black-box Optimization Benchmarking Suite and OpenAI Gym,No.,1,"""No evidence""",2023,2023-09-29T18:33:10Z,,,
arXIv2023,ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving,No.,1,"""No evidence""",2023,2023-09-29T17:59:38Z,,,
arXIv2023,A Large Language Model Approach to Educational Survey Feedback Analysis,No.,1,"""No evidence""",2023,2023-09-29T17:57:23Z,,,
arXIv2023,CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets,No.,1,"""No evidence""",2023,2023-09-29T17:40:26Z,,,
arXIv2023,Data Filtering Networks,No.,1,"""No evidence""",2023,2023-09-29T17:37:29Z,,,
arXIv2023,The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision),No.,1,"""No evidence""",2023,2023-09-29T17:34:51Z,,,
arXIv2023,Intuitive or Dependent? Investigating LLMs' Behavior Style to Conflicting Prompts,No.,1,"""No evidence""",2023,2023-09-29T17:26:03Z,,,
arXIv2023,Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile,No.,1,"""No evidence""",2023,2023-09-29T16:30:49Z,,,
arXIv2023,AutoAgents: A Framework for Automatic Agent Generation,No.,1,"""No evidence""",2023,2023-09-29T14:46:30Z,,,
arXIv2023,PB-LLM: Partially Binarized Large Language Models,No.,1,"""No evidence""",2023,2023-09-29T14:35:27Z,,,
arXIv2023,Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency,No.,1,"""No evidence""",2023,2023-09-29T14:23:26Z,,,
arXIv2023,Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering,No.,1,"""No evidence""",2023,2023-09-29T13:55:45Z,,,
arXIv2023,AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback,No.,1,"""No evidence""",2023,2023-09-29T12:16:19Z,,,
arXIv2023,Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain,No.,1,"""No evidence""",2023,2023-09-29T12:09:36Z,,,
arXIv2023,An evaluation of GPT models for phenotype concept recognition,No.,1,"""No evidence""",2023,2023-09-29T12:06:55Z,,,
arXIv2023,DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks,No.,1,"""No evidence""",2023,2023-09-29T12:04:14Z,,,
arXIv2023,Guiding Instruction-based Image Editing via Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-09-29T10:01:50Z,,,
arXIv2023,SCALE: Synergized Collaboration of Asymmetric Language Translation Engines,No.,1,"""No evidence""",2023,2023-09-29T08:46:38Z,,,
arXIv2023,Tell Me a Story! Narrative-Driven XAI with Large Language Models,No.,1,"""No evidence""",2023,2023-09-29T08:40:08Z,,,
arXIv2023,Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-09-29T08:23:19Z,,,
arXIv2023,Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors,No.,1,"""No evidence""",2023,2023-09-29T04:49:35Z,,,
arXIv2023,"A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM",No.,1,"""No evidence""",2023,2023-09-28T23:54:41Z,,,
arXIv2023,Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data,No.,1,"""No evidence""",2023,2023-09-28T19:14:18Z,,,
arXIv2023,Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution,No.,1,"""No evidence""",2023,2023-09-28T19:01:07Z,,,
arXIv2023,How many words does ChatGPT know? The answer is ChatWords,No.,1,"""No evidence""",2023,2023-09-28T18:13:02Z,,,
arXIv2023,MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention,No.,1,"""No evidence""",2023,2023-09-28T17:49:03Z,,,
arXIv2023,Stress Testing Chain-of-Thought Prompting for Large Language Models,No.,1,"""No evidence""",2023,2023-09-28T17:21:33Z,,,
arXIv2023,Qwen Technical Report,No.,1,"""No evidence""",2023,2023-09-28T17:07:49Z,,,
arXIv2023,"Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why?",No.,1,"""No evidence""",2023,2023-09-28T16:58:37Z,,,
arXIv2023,A Benchmark for Learning to Translate a New Language from One Grammar Book,No.,1,"""No evidence""",2023,2023-09-28T16:32:28Z,,,
arXIv2023,Unsupervised Pretraining for Fact Verification by Language Model Distillation,No.,1,"""No evidence""",2023,2023-09-28T15:53:44Z,,,
arXIv2023,Chatmap : Large Language Model Interaction with Cartographic Data,No.,1,"""No evidence""",2023,2023-09-28T15:32:36Z,,,
arXIv2023,Augmenting LLMs with Knowledge: A survey on hallucination prevention,No.,1,"""No evidence""",2023,2023-09-28T14:09:58Z,,,
arXIv2023,AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models,No.,1,"""No evidence""",2023,2023-09-28T13:08:08Z,,,
arXIv2023,RLLTE: Long-Term Evolution Project of Reinforcement Learning,No.,1,"""No evidence""",2023,2023-09-28T12:30:37Z,,,
arXIv2023,At Which Training Stage Does Code Data Help LLMs Reasoning?,No.,1,"""No evidence""",2023,2023-09-28T09:50:27Z,,,
arXIv2023,DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models,No.,1,"""No evidence""",2023,2023-09-28T09:41:35Z,,,
arXIv2023,Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems,No.,1,"""No evidence""",2023,2023-09-28T08:41:08Z,,,
arXIv2023,Controllable Text Generation with Residual Memory Transformer,No.,1,"""No evidence""",2023,2023-09-28T08:13:33Z,,,
arXIv2023,VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-09-28T07:37:18Z,,,
arXIv2023,"Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities",No.,1,"""No evidence""",2023,2023-09-28T06:22:59Z,,,
arXIv2023,Large Language Models in Finance: A Survey,No.,1,"""No evidence""",2023,2023-09-28T06:04:04Z,,,
arXIv2023,Large Language Model Soft Ideologization via AI-Self-Consciousness,No.,1,"""No evidence""",2023,2023-09-28T04:47:58Z,,,
arXIv2023,T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems,No.,1,"""No evidence""",2023,2023-09-28T03:51:49Z,,,
arXIv2023,The Confidence-Competence Gap in Large Language Models: A Cognitive Study,No.,1,"""No evidence""",2023,2023-09-28T03:50:09Z,,,
arXIv2023,TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration,No.,1,"""No evidence""",2023,2023-09-28T01:18:53Z,,,
arXIv2023,"""ChatGPT, a Friend or Foe for Education?"" Analyzing the User's Perspectives on the Latest AI Chatbot Via Reddit",No.,1,"""No evidence""",2023,2023-09-27T23:59:44Z,,,
arXIv2023,AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model,No.,1,"""No evidence""",2023,2023-09-27T22:50:51Z,,,
arXIv2023,Effective Long-Context Scaling of Foundation Models,No.,1,"""No evidence""",2023,2023-09-27T21:41:49Z,,,
arXIv2023,One For All: Video Conversation is Feasible Without Video Instruction Tuning,No.,1,"""No evidence""",2023,2023-09-27T16:58:35Z,,,
arXIv2023,Borges and AI,No.,1,"""No evidence""",2023,2023-09-27T16:15:34Z,,,
arXIv2023,MindGPT: Interpreting What You See with Non-invasive Brain Recordings,No.,1,"""No evidence""",2023,2023-09-27T15:35:20Z,,,
arXIv2023,Brave new world: Artificial Intelligence in teaching and learning,No.,1,"""No evidence""",2023,2023-09-27T15:22:05Z,,,
arXIv2023,"Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level Neural State Classification in Semantic Inference Reading Comprehension",No.,1,"""No evidence""",2023,2023-09-27T15:12:08Z,,,
arXIv2023,HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models,No.,1,"""No evidence""",2023,2023-09-27T14:44:10Z,,,
arXIv2023,MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection,No.,1,"""No evidence""",2023,2023-09-27T14:10:57Z,,,
arXIv2023,Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis,No.,1,"""No evidence""",2023,2023-09-27T13:45:38Z,,,
arXIv2023,Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting,No.,1,"""No evidence""",2023,2023-09-27T13:36:03Z,,,
arXIv2023,An Empirical Study of AI Generated Text Detection Tools,No.,1,"""No evidence""",2023,2023-09-27T12:44:12Z,,,
arXIv2023,An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8,No.,1,"""No evidence""",2023,2023-09-27T11:23:15Z,,,
arXIv2023,Investigating the changes in BOLD responses during viewing of images with varied complexity: An fMRI time-series based analysis on human vision,No.,1,"""No evidence""",2023,2023-09-27T08:46:09Z,,,
arXIv2023,Tackling VQA with Pretrained Foundation Models without Further Training,No.,1,"""No evidence""",2023,2023-09-27T08:35:24Z,,,
arXIv2023,ChatCounselor: A Large Language Models for Mental Health Support,No.,1,"""No evidence""",2023,2023-09-27T07:57:21Z,,,
arXIv2023,Warfare:Breaking the Watermark Protection of AI-Generated Content,No.,1,"""No evidence""",2023,2023-09-27T06:32:00Z,,,
arXIv2023,Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems,No.,1,"""No evidence""",2023,2023-09-26T23:27:06Z,,,
arXIv2023,SEPT: Towards Efficient Scene Representation Learning for Motion Prediction,No.,1,"""No evidence""",2023,2023-09-26T21:56:03Z,,,
arXIv2023,ChatGPT & Mechanical Engineering: Examining performance on the FE Mechanical Engineering and Undergraduate Exams,No.,1,"""No evidence""",2023,2023-09-26T20:12:26Z,,,
arXIv2023,Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition,No.,1,"""No evidence""",2023,2023-09-26T19:41:34Z,,,
arXIv2023,VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning,No.,1,"""No evidence""",2023,2023-09-26T17:36:26Z,,,
arXIv2023,RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models,No.,1,"""No evidence""",2023,2023-09-26T17:31:57Z,,,
arXIv2023,Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding,No.,1,"""No evidence""",2023,2023-09-26T16:50:20Z,,,
arXIv2023,When Prolog meets generative models: a new approach for managing knowledge and planning in robotic applications,No.,1,"""No evidence""",2023,2023-09-26T16:26:17Z,,,
arXIv2023,"With ChatGPT, do we have to rewrite our learning objectives -- CASE study in Cybersecurity",No.,1,"""No evidence""",2023,2023-09-26T16:12:10Z,,,
arXIv2023,Question-Answering Approach to Evaluating Legal Summaries,No.,1,"""No evidence""",2023,2023-09-26T15:36:29Z,,,
arXIv2023,A Democratic Platform for Engaging with Disabled Community in Generative AI Development,No.,1,"""No evidence""",2023,2023-09-26T13:30:57Z,,,
arXIv2023,Robustness of the Random Language Model,No.,1,"""No evidence""",2023,2023-09-26T13:14:35Z,,,
arXIv2023,Supersonic: Learning to Generate Source Code Optimizations in C/C++,No.,1,"""No evidence""",2023,2023-09-26T11:21:46Z,,,
arXIv2023,Fine-tuning and aligning question answering models for complex information extraction tasks,No.,1,"""No evidence""",2023,2023-09-26T10:02:21Z,,,
arXIv2023,Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification,No.,1,"""No evidence""",2023,2023-09-26T09:24:46Z,,,
arXIv2023,Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning,No.,1,"""No evidence""",2023,2023-09-26T09:06:39Z,,,
arXIv2023,KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation,No.,1,"""No evidence""",2023,2023-09-26T09:03:25Z,,,
arXIv2023,"Legal Question-Answering in the Indian Context: Efficacy, Challenges, and Potential of Modern AI Models",No.,1,"""No evidence""",2023,2023-09-26T07:56:55Z,,,
arXIv2023,Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization,No.,1,"""No evidence""",2023,2023-09-26T07:38:19Z,,,
arXIv2023,PLMM: Personal Large Models on Mobile Devices,No.,1,"""No evidence""",2023,2023-09-26T07:36:20Z,,,
arXIv2023,XGV-BERT: Leveraging Contextualized Language Model and Graph Neural Network for Efficient Software Vulnerability Detection,No.,1,"""No evidence""",2023,2023-09-26T05:05:34Z,,,
arXIv2023,MSG-BART: Multi-granularity Scene Graph-Enhanced Encoder-Decoder Language Model for Video-grounded Dialogue Generation,No.,1,"""No evidence""",2023,2023-09-26T04:23:23Z,,,
arXIv2023,Text-to-Image Generation for Abstract Concepts,No.,1,"""No evidence""",2023,2023-09-26T02:22:39Z,,,
arXIv2023,Efficient Post-training Quantization with FP8 Formats,No.,1,"""No evidence""",2023,2023-09-26T00:58:36Z,,,
arXIv2023,Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew,No.,1,"""No evidence""",2023,2023-09-25T22:42:09Z,,,
arXIv2023,Art or Artifice? Large Language Models and the False Promise of Creativity,No.,1,"""No evidence""",2023,2023-09-25T22:02:46Z,,,
arXIv2023,Aligning Large Multimodal Models with Factually Augmented RLHF,No.,1,"""No evidence""",2023,2023-09-25T20:59:33Z,,,
arXIv2023,ChatGPT Performance on Standardized Testing Exam -- A Proposed Strategy for Learners,No.,1,"""No evidence""",2023,2023-09-25T20:25:29Z,,,
arXIv2023,Watch Your Language: Investigating Content Moderation with Large Language Models,No.,1,"""No evidence""",2023,2023-09-25T20:23:51Z,,,
arXIv2023,DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models,No.,1,"""No evidence""",2023,2023-09-25T20:15:57Z,,,
arXIv2023,When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs,No.,1,"""No evidence""",2023,2023-09-25T19:32:18Z,,,
arXIv2023,LogGPT: Log Anomaly Detection via GPT,No.,1,"""No evidence""",2023,2023-09-25T19:29:50Z,,,
arXIv2023,Joint Audio and Speech Understanding,No.,1,"""No evidence""",2023,2023-09-25T17:59:05Z,,,
arXIv2023,pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning,No.,1,"""No evidence""",2023,2023-09-25T17:57:39Z,,,
arXIv2023,DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention,No.,1,"""No evidence""",2023,2023-09-25T17:53:29Z,,,
arXIv2023,Towards General-Purpose Text-Instruction-Guided Voice Conversion,No.,1,"""No evidence""",2023,2023-09-25T17:52:09Z,,,
arXIv2023,"Physics of Language Models: Part 3.2, Knowledge Manipulation",No.,1,"""No evidence""",2023,2023-09-25T17:50:41Z,,,
arXIv2023,Rethinking Internet Communication Through LLMs: How Close Are We?,No.,1,"""No evidence""",2023,2023-09-25T16:07:07Z,,,
arXIv2023,Guess & Sketch: Language Model Guided Transpilation,No.,1,"""No evidence""",2023,2023-09-25T15:42:18Z,,,
arXIv2023,Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques,No.,1,"""No evidence""",2023,2023-09-25T15:33:08Z,,,
arXIv2023,Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision,No.,1,"""No evidence""",2023,2023-09-25T14:43:43Z,,,
arXIv2023,On the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers,No.,1,"""No evidence""",2023,2023-09-25T13:35:28Z,,,
arXIv2023,SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution,No.,1,"""No evidence""",2023,2023-09-25T13:20:15Z,,,
arXIv2023,"Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges",No.,1,"""No evidence""",2023,2023-09-25T12:23:37Z,,,
arXIv2023,LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression,No.,1,"""No evidence""",2023,2023-09-25T10:35:17Z,,,
arXIv2023,An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems,No.,1,"""No evidence""",2023,2023-09-25T09:05:36Z,,,
arXIv2023,Connecting Speech Encoder and Large Language Model for ASR,No.,1,"""No evidence""",2023,2023-09-25T08:57:07Z,,,
arXIv2023,SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems,No.,1,"""No evidence""",2023,2023-09-25T08:13:49Z,,,
arXIv2023,Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data,No.,1,"""No evidence""",2023,2023-09-25T05:01:34Z,,,
arXIv2023,"Does the ""most sinfully decadent cake ever"" taste good? Answering Yes/No Questions from Figurative Contexts",No.,1,"""No evidence""",2023,2023-09-24T20:38:48Z,,,
arXIv2023,Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification,No.,1,"""No evidence""",2023,2023-09-24T19:36:17Z,,,
arXIv2023,Skill Check: Some Considerations on the Evaluation of Gamemastering Models for Role-playing Games,No.,1,"""No evidence""",2023,2023-09-24T17:19:36Z,,,
arXIv2023,Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR),No.,1,"""No evidence""",2023,2023-09-24T16:08:21Z,,,
arXIv2023,Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence,No.,1,"""No evidence""",2023,2023-09-24T14:21:50Z,,,
arXIv2023,EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria,No.,1,"""No evidence""",2023,2023-09-24T13:19:38Z,,,
arXIv2023,A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes,No.,1,"""No evidence""",2023,2023-09-24T11:36:21Z,,,
arXIv2023,MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models,No.,1,"""No evidence""",2023,2023-09-24T06:46:08Z,,,
arXIv2023,Keeping in Time: Adding Temporal Context to Sentiment Analysis Models,No.,1,"""No evidence""",2023,2023-09-24T06:38:21Z,,,
arXIv2023,I-AI: A Controllable & Interpretable AI System for Decoding Radiologists' Intense Focus for Accurate CXR Diagnoses,No.,1,"""No evidence""",2023,2023-09-24T04:48:44Z,,,
arXIv2023,Substituting Data Annotation with Balanced Updates and Collective Loss in Multi-label Text Classification,No.,1,"""No evidence""",2023,2023-09-24T04:12:52Z,,,
arXIv2023,Natural Language based Context Modeling and Reasoning for Ubiquitous Computing with Large Language Models: A Tutorial,No.,1,"""No evidence""",2023,2023-09-24T00:15:39Z,,,
arXIv2023,Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy,No.,1,"""No evidence""",2023,2023-09-23T23:37:55Z,,,
arXIv2023,A Unified Scheme of ResNet and Softmax,No.,1,"""No evidence""",2023,2023-09-23T21:41:01Z,,,
arXIv2023,Probing the Moral Development of Large Language Models through Defining Issues Test,No.,1,"""No evidence""",2023,2023-09-23T12:17:10Z,,,
arXIv2023,Lexical Squad@Multimodal Hate Speech Event Detection 2023: Multimodal Hate Speech Detection using Fused Ensemble Approach,No.,1,"""No evidence""",2023,2023-09-23T12:06:05Z,,,
arXIv2023,BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models,No.,1,"""No evidence""",2023,2023-09-23T11:36:15Z,,,
arXIv2023,An In-depth Survey of Large Language Model-based Artificial Intelligence Agents,No.,1,"""No evidence""",2023,2023-09-23T11:25:45Z,,,
arXIv2023,From Text to Source: Results in Detecting Large Language Model-Generated Content,No.,1,"""No evidence""",2023,2023-09-23T09:51:37Z,,,
arXIv2023,GlotScript: A Resource and Tool for Low Resource Writing System Identification,No.,1,"""No evidence""",2023,2023-09-23T09:35:55Z,,,
arXIv2023,Calibrating LLM-Based Evaluator,No.,1,"""No evidence""",2023,2023-09-23T08:46:11Z,,,
arXIv2023,ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education,No.,1,"""No evidence""",2023,2023-09-23T03:28:25Z,,,
arXIv2023,User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue,No.,1,"""No evidence""",2023,2023-09-23T02:04:57Z,,,
arXIv2023,A Practical Survey on Zero-shot Prompt Design for In-context Learning,No.,1,"""No evidence""",2023,2023-09-22T23:00:34Z,,,
arXIv2023,Effective Distillation of Table-based Reasoning Ability from LLMs,No.,1,"""No evidence""",2023,2023-09-22T21:15:28Z,,,
arXIv2023,AI Risk Profiles: A Standards Proposal for Pre-Deployment AI Risk Disclosures,No.,1,"""No evidence""",2023,2023-09-22T20:45:15Z,,,
arXIv2023,Large Language Models Are Also Good Prototypical Commonsense Reasoners,No.,1,"""No evidence""",2023,2023-09-22T20:07:24Z,,,
arXIv2023,AntiBARTy Diffusion for Property Guided Antibody Design,No.,1,"""No evidence""",2023,2023-09-22T18:30:50Z,,,
arXIv2023,ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs,No.,1,"""No evidence""",2023,2023-09-22T17:12:45Z,,,
arXIv2023,Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models,No.,1,"""No evidence""",2023,2023-09-22T15:42:43Z,,,
arXIv2023,Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models,No.,1,"""No evidence""",2023,2023-09-22T15:41:34Z,,,
arXIv2023,Frustrated with Code Quality Issues? LLMs can Help!,No.,1,"""No evidence""",2023,2023-09-22T15:37:07Z,,,
arXIv2023,TOPFORMER: Topology-Aware Authorship Attribution of Deepfake Texts with Diverse Writing Styles,No.,1,"""No evidence""",2023,2023-09-22T15:32:49Z,,,
arXIv2023,"Topological Data Mapping of Online Hate Speech, Misinformation, and General Mental Health: A Large Language Model Based Study",No.,1,"""No evidence""",2023,2023-09-22T15:10:36Z,,,
arXIv2023,Zero-Shot Object Counting with Language-Vision Models,No.,1,"""No evidence""",2023,2023-09-22T14:48:42Z,,,
arXIv2023,Affect Recognition in Conversations Using Large Language Models,No.,1,"""No evidence""",2023,2023-09-22T14:11:23Z,,,
arXIv2023,AnglE-optimized Text Embeddings,No.,1,"""No evidence""",2023,2023-09-22T13:52:42Z,,,
arXIv2023,Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts,No.,1,"""No evidence""",2023,2023-09-22T13:37:19Z,,,
arXIv2023,Computational Natural Philosophy: A Thread from Presocratics through Turing to ChatGPT,No.,1,"""No evidence""",2023,2023-09-22T11:47:36Z,,,
arXIv2023,ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT,No.,1,"""No evidence""",2023,2023-09-22T11:46:44Z,,,
arXIv2023,Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-09-22T10:15:13Z,,,
arXIv2023,OpenAi's GPT4 as coding assistant,No.,1,"""No evidence""",2023,2023-09-22T09:31:39Z,,,
arXIv2023,Defeasible Reasoning with Knowledge Graphs,No.,1,"""No evidence""",2023,2023-09-22T09:27:26Z,,,
arXIv2023,Semantic similarity prediction is better than other semantic similarity measures,No.,1,"""No evidence""",2023,2023-09-22T08:11:01Z,,,
arXIv2023,AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer,No.,1,"""No evidence""",2023,2023-09-22T08:02:45Z,,,
arXIv2023,HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering,No.,1,"""No evidence""",2023,2023-09-22T07:26:17Z,,,
arXIv2023,Sequential Action-Induced Invariant Representation for Reinforcement Learning,No.,1,"""No evidence""",2023,2023-09-22T05:31:55Z,,,
arXIv2023,DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients,No.,1,"""No evidence""",2023,2023-09-22T05:18:54Z,,,
arXIv2023,Unlocking Model Insights: A Dataset for Automated Model Card Generation,No.,1,"""No evidence""",2023,2023-09-22T04:46:11Z,,,
arXIv2023,Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers,No.,1,"""No evidence""",2023,2023-09-22T01:49:36Z,,,
arXIv2023,PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models,No.,1,"""No evidence""",2023,2023-09-22T00:55:52Z,,,
arXIv2023,Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models,No.,1,"""No evidence""",2023,2023-09-22T00:47:18Z,,,
arXIv2023,Automatic Answerability Evaluation for Question Generation,No.,1,"""No evidence""",2023,2023-09-22T00:13:07Z,,,
arXIv2023,A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance,No.,1,"""No evidence""",2023,2023-09-21T23:06:19Z,,,
arXIv2023,"HANS, are you clever? Clever Hans Effect Analysis of Neural Systems",No.,1,"""No evidence""",2023,2023-09-21T20:52:18Z,,,
arXIv2023,Multimodal Deep Learning for Scientific Imaging Interpretation,No.,1,"""No evidence""",2023,2023-09-21T20:09:22Z,,,
arXIv2023,Constraints First: A New MDD-based Model to Generate Sentences Under Constraints,No.,1,"""No evidence""",2023,2023-09-21T18:29:52Z,,,
arXIv2023,TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance,No.,1,"""No evidence""",2023,2023-09-21T17:59:53Z,,,
arXIv2023,Rehearsal: Simulating Conflict to Teach Conflict Resolution,No.,1,"""No evidence""",2023,2023-09-21T17:59:20Z,,,
arXIv2023,LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models,No.,1,"""No evidence""",2023,2023-09-21T17:59:11Z,,,
arXIv2023,MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models,No.,1,"""No evidence""",2023,2023-09-21T17:45:42Z,,,
arXIv2023,LLMR: Real-time Prompting of Interactive Worlds using Large Language Models,No.,1,"""No evidence""",2023,2023-09-21T17:37:01Z,,,
arXIv2023,The Cambridge Law Corpus: A Dataset for Legal AI Research,No.,1,"""No evidence""",2023,2023-09-21T17:24:40Z,,,
arXIv2023,On the Relationship between Skill Neurons and Robustness in Prompt Tuning,No.,1,"""No evidence""",2023,2023-09-21T17:13:21Z,,,
arXIv2023,ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events,No.,1,"""No evidence""",2023,2023-09-21T16:43:17Z,,,
arXIv2023,SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap,No.,1,"""No evidence""",2023,2023-09-21T15:06:08Z,,,
arXIv2023,PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models,No.,1,"""No evidence""",2023,2023-09-21T14:29:23Z,,,
arXIv2023,Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam,No.,1,"""No evidence""",2023,2023-09-21T13:39:54Z,,,
arXIv2023,CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation,No.,1,"""No evidence""",2023,2023-09-21T12:51:24Z,,,
arXIv2023,LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset,No.,1,"""No evidence""",2023,2023-09-21T12:13:55Z,,,
arXIv2023,Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics,No.,1,"""No evidence""",2023,2023-09-21T11:34:52Z,,,
arXIv2023,Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT,No.,1,"""No evidence""",2023,2023-09-21T11:26:36Z,,,
arXIv2023,SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels,No.,1,"""No evidence""",2023,2023-09-21T10:55:26Z,,,
arXIv2023,InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework,No.,1,"""No evidence""",2023,2023-09-21T09:22:07Z,,,
arXIv2023,MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for Domain-specific Large Models,No.,1,"""No evidence""",2023,2023-09-21T09:02:28Z,,,
arXIv2023,Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection,No.,1,"""No evidence""",2023,2023-09-21T08:59:24Z,,,
arXIv2023,How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses,No.,1,"""No evidence""",2023,2023-09-21T07:54:25Z,,,
arXIv2023,Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues,No.,1,"""No evidence""",2023,2023-09-21T07:28:03Z,,,
arXIv2023,JobRecoGPT -- Explainable job recommendations using LLMs,No.,1,"""No evidence""",2023,2023-09-21T06:25:28Z,,,
arXIv2023,"SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features",No.,1,"""No evidence""",2023,2023-09-21T05:38:14Z,,,
arXIv2023,Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation,No.,1,"""No evidence""",2023,2023-09-21T03:59:00Z,,,
arXIv2023,How Robust is Google's Bard to Adversarial Image Attacks?,No.,1,"""No evidence""",2023,2023-09-21T03:24:30Z,,,
arXIv2023,Choice-75: A Dataset on Decision Branching in Script Learning,No.,1,"""No evidence""",2023,2023-09-21T02:23:44Z,,,
arXIv2023,Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening,No.,1,"""No evidence""",2023,2023-09-20T23:43:42Z,,,
arXIv2023,A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models,No.,1,"""No evidence""",2023,2023-09-20T22:53:15Z,,,
arXIv2023,Generative AI in Mafia-like Game Simulation,No.,1,"""No evidence""",2023,2023-09-20T22:38:34Z,,,
arXIv2023,BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model,No.,1,"""No evidence""",2023,2023-09-20T18:12:56Z,,,
arXIv2023,SignBank+: Preparing a Multilingual Sign Language Dataset for Machine Translation Using Large Language Models,No.,1,"""No evidence""",2023,2023-09-20T18:08:28Z,,,
arXIv2023,DreamLLM: Synergistic Multimodal Comprehension and Creation,No.,1,"""No evidence""",2023,2023-09-20T17:58:05Z,,,
arXIv2023,"Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs",No.,1,"""No evidence""",2023,2023-09-20T17:23:05Z,,,
arXIv2023,Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence,No.,1,"""No evidence""",2023,2023-09-20T16:43:05Z,,,
arXIv2023,You Only Look at Screens: Multimodal Chain-of-Action Agents,No.,1,"""No evidence""",2023,2023-09-20T16:12:32Z,,,
arXIv2023,Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing,No.,1,"""No evidence""",2023,2023-09-20T16:01:45Z,,,
arXIv2023,SCREWS: A Modular Framework for Reasoning with Revisions,No.,1,"""No evidence""",2023,2023-09-20T15:59:54Z,,,
arXIv2023,Kosmos-2.5: A Multimodal Literate Model,No.,1,"""No evidence""",2023,2023-09-20T15:50:08Z,,,
arXIv2023,Safurai 001: New Qualitative Approach for Code LLM Evaluation,No.,1,"""No evidence""",2023,2023-09-20T15:11:32Z,,,
arXIv2023,Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions,No.,1,"""No evidence""",2023,2023-09-20T15:04:49Z,,,
arXIv2023,Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG),No.,1,"""No evidence""",2023,2023-09-20T14:43:43Z,,,
arXIv2023,DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services,No.,1,"""No evidence""",2023,2023-09-20T13:50:26Z,,,
arXIv2023,Face Aging via Diffusion-based Editing,No.,1,"""No evidence""",2023,2023-09-20T13:47:10Z,,,
arXIv2023,CPLLM: Clinical Prediction with Large Language Models,No.,1,"""No evidence""",2023,2023-09-20T13:24:12Z,,,
arXIv2023,Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains,No.,1,"""No evidence""",2023,2023-09-20T13:10:06Z,,,
arXIv2023,"StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding",No.,1,"""No evidence""",2023,2023-09-20T12:51:13Z,,,
arXIv2023,Sequence-to-Sequence Spanish Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-09-20T12:35:19Z,,,
arXIv2023,OpenChat: Advancing Open-source Language Models with Mixed-Quality Data,No.,1,"""No evidence""",2023,2023-09-20T11:54:40Z,,,
arXIv2023,ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish,No.,1,"""No evidence""",2023,2023-09-20T11:44:45Z,,,
arXIv2023,The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute,No.,1,"""No evidence""",2023,2023-09-20T10:31:17Z,,,
arXIv2023,Assessment of Pre-Trained Models Across Languages and Grammars,No.,1,"""No evidence""",2023,2023-09-20T09:23:36Z,,,
arXIv2023,CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought,No.,1,"""No evidence""",2023,2023-09-20T08:42:06Z,,,
arXIv2023,Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation,No.,1,"""No evidence""",2023,2023-09-20T08:19:05Z,,,
arXIv2023,AttentionMix: Data augmentation method that relies on BERT attention mechanism,No.,1,"""No evidence""",2023,2023-09-20T07:18:53Z,,,
arXIv2023,Embed-Search-Align: DNA Sequence Alignment using Transformer Models,No.,1,"""No evidence""",2023,2023-09-20T06:30:39Z,,,
arXIv2023,XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates,No.,1,"""No evidence""",2023,2023-09-20T04:58:59Z,,,
arXIv2023,Design of Chain-of-Thought in Math Problem Solving,No.,1,"""No evidence""",2023,2023-09-20T04:17:28Z,,,
arXIv2023,Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks,No.,1,"""No evidence""",2023,2023-09-20T03:49:57Z,,,
arXIv2023,Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters,No.,1,"""No evidence""",2023,2023-09-20T03:39:56Z,,,
arXIv2023,Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model,No.,1,"""No evidence""",2023,2023-09-20T01:48:27Z,,,
arXIv2023,Is GPT4 a Good Trader?,No.,1,"""No evidence""",2023,2023-09-20T00:47:52Z,,,
arXIv2023,MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods,No.,1,"""No evidence""",2023,2023-09-19T23:39:07Z,,,
arXIv2023,Semi-Autoregressive Streaming ASR With Label Context,No.,1,"""No evidence""",2023,2023-09-19T20:55:58Z,,,
arXIv2023,End-to-End Speech Recognition Contextualization with Large Language Models,No.,1,"""No evidence""",2023,2023-09-19T20:28:57Z,,,
arXIv2023,Enhancing Health Data Interoperability with Large Language Models: A FHIR Study,No.,1,"""No evidence""",2023,2023-09-19T20:09:35Z,,,
arXIv2023,Generative AI in the Construction Industry: Opportunities & Challenges,No.,1,"""No evidence""",2023,2023-09-19T18:20:49Z,,,
arXIv2023,SlimPajama-DC: Understanding Data Combinations for LLM Training,No.,1,"""No evidence""",2023,2023-09-19T17:59:54Z,,,
arXIv2023,Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning,No.,1,"""No evidence""",2023,2023-09-19T17:54:21Z,,,
arXIv2023,Language as the Medium: Multimodal Video Classification through text only,No.,1,"""No evidence""",2023,2023-09-19T17:32:21Z,,,
arXIv2023,OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch,No.,1,"""No evidence""",2023,2023-09-19T15:46:40Z,,,
arXIv2023,"""With Great Power Comes Great Responsibility!"": Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education",No.,1,"""No evidence""",2023,2023-09-19T15:29:12Z,,,
arXIv2023,MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback,No.,1,"""No evidence""",2023,2023-09-19T15:25:42Z,,,
arXIv2023,Language Modeling Is Compression,No.,1,"""No evidence""",2023,2023-09-19T14:50:38Z,,,
arXIv2023,NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages,No.,1,"""No evidence""",2023,2023-09-19T14:42:33Z,,,
arXIv2023,CFGPT: Chinese Financial Assistant with Large Language Model,No.,1,"""No evidence""",2023,2023-09-19T14:34:01Z,,,
arXIv2023,Large language models can accurately predict searcher preferences,No.,1,"""No evidence""",2023,2023-09-19T13:55:39Z,,,
arXIv2023,A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents,No.,1,"""No evidence""",2023,2023-09-19T12:18:28Z,,,
arXIv2023,Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition,No.,1,"""No evidence""",2023,2023-09-19T11:10:50Z,,,
arXIv2023,An Evaluation of GPT-4 on the ETHICS Dataset,No.,1,"""No evidence""",2023,2023-09-19T10:01:50Z,,,
arXIv2023,Exploring the Dark Side of AI: Advanced Phishing Attack Design and Deployment Using ChatGPT,No.,1,"""No evidence""",2023,2023-09-19T09:31:39Z,,,
arXIv2023,Toward Unified Controllable Text Generation via Regular Expression Instruction,No.,1,"""No evidence""",2023,2023-09-19T09:05:14Z,,,
arXIv2023,Writer-Defined AI Personas for On-Demand Feedback Generation,No.,1,"""No evidence""",2023,2023-09-19T08:49:35Z,,,
arXIv2023,Learning from Teaching Assistants to Program with Subgoals: Exploring the Potential for AI Teaching Assistants,No.,1,"""No evidence""",2023,2023-09-19T08:30:58Z,,,
arXIv2023,PICK: Polished & Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems,No.,1,"""No evidence""",2023,2023-09-19T08:27:09Z,,,
arXIv2023,Pointing out Human Answer Mistakes in a Goal-Oriented Visual Dialogue,No.,1,"""No evidence""",2023,2023-09-19T07:22:05Z,,,
arXIv2023,Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs,No.,1,"""No evidence""",2023,2023-09-19T07:12:55Z,,,
arXIv2023,"Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning",No.,1,"""No evidence""",2023,2023-09-19T06:42:37Z,,,
arXIv2023,Explaining Agent Behavior with Large Language Models,No.,1,"""No evidence""",2023,2023-09-19T06:13:24Z,,,
arXIv2023,Rigorously Assessing Natural Language Explanations of Neurons,No.,1,"""No evidence""",2023,2023-09-19T04:49:45Z,,,
arXIv2023,Baichuan 2: Open Large-scale Language Models,No.,1,"""No evidence""",2023,2023-09-19T04:13:22Z,,,
arXIv2023,"Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition",No.,1,"""No evidence""",2023,2023-09-19T03:52:01Z,,,
arXIv2023,Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity,No.,1,"""No evidence""",2023,2023-09-19T03:20:02Z,,,
arXIv2023,"Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi",No.,1,"""No evidence""",2023,2023-09-19T02:59:41Z,,,
arXIv2023,What is the Best Automated Metric for Text to Motion Generation?,No.,1,"""No evidence""",2023,2023-09-19T01:59:54Z,,,
arXIv2023,PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models,No.,1,"""No evidence""",2023,2023-09-19T01:22:42Z,,,
arXIv2023,Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles,No.,1,"""No evidence""",2023,2023-09-19T00:47:13Z,,,
arXIv2023,Positive and Risky Message Assessment for Music Products,No.,1,"""No evidence""",2023,2023-09-18T22:20:13Z,,,
arXIv2023,"Generative modeling, design and analysis of spider silk protein sequences for enhanced mechanical properties",No.,1,"""No evidence""",2023,2023-09-18T21:38:40Z,,,
arXIv2023,Few-Shot Adaptation for Parsing Contextual Utterances with LLMs,No.,1,"""No evidence""",2023,2023-09-18T21:35:19Z,,,
arXIv2023,Understanding Catastrophic Forgetting in Language Models via Implicit Inference,No.,1,"""No evidence""",2023,2023-09-18T19:28:48Z,,,
arXIv2023,Reasoning about the Unseen for Efficient Outdoor Object Navigation,No.,1,"""No evidence""",2023,2023-09-18T19:24:21Z,,,
arXIv2023,Conformal Temporal Logic Planning using Large Language Models,No.,1,"""No evidence""",2023,2023-09-18T19:05:25Z,,,
arXIv2023,Automatic Personalized Impression Generation for PET Reports Using Large Language Models,No.,1,"""No evidence""",2023,2023-09-18T18:33:40Z,,,
arXIv2023,Multimodal Foundation Models: From Specialists to General-Purpose Assistants,No.,1,"""No evidence""",2023,2023-09-18T17:56:28Z,,,
arXIv2023,MindAgent: Emergent Gaming Interaction,No.,1,"""No evidence""",2023,2023-09-18T17:52:22Z,,,
arXIv2023,Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents,No.,1,"""No evidence""",2023,2023-09-18T16:33:30Z,,,
arXIv2023,Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction,No.,1,"""No evidence""",2023,2023-09-18T16:17:44Z,,,
arXIv2023,Speaker attribution in German parliamentary debates with QLoRA-adapted large language models,No.,1,"""No evidence""",2023,2023-09-18T16:06:16Z,,,
arXIv2023,Towards Ontology Construction with Language Models,No.,1,"""No evidence""",2023,2023-09-18T16:02:39Z,,,
arXIv2023,Context is Environment,No.,1,"""No evidence""",2023,2023-09-18T15:51:27Z,,,
arXIv2023,Corpus Synthesis for Zero-shot ASR domain Adaptation using Large Language Models,No.,1,"""No evidence""",2023,2023-09-18T15:43:08Z,,,
arXIv2023,SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback,No.,1,"""No evidence""",2023,2023-09-18T15:08:48Z,,,
arXIv2023,Instruction-Following Speech Recognition,No.,1,"""No evidence""",2023,2023-09-18T14:59:10Z,,,
arXIv2023,RECAP: Retrieval-Augmented Audio Captioning,No.,1,"""No evidence""",2023,2023-09-18T14:53:08Z,,,
arXIv2023,Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits,No.,1,"""No evidence""",2023,2023-09-18T14:51:51Z,,,
arXIv2023,Grasp-Anything: Large-scale Grasp Dataset from Foundation Models,No.,1,"""No evidence""",2023,2023-09-18T14:39:26Z,,,
arXIv2023,R2GenGPT: Radiology Report Generation with Frozen LLMs,No.,1,"""No evidence""",2023,2023-09-18T14:35:35Z,,,
arXIv2023,AMuRD: Annotated Arabic-English Receipt Dataset for Key Information Extraction and Classification,No.,1,"""No evidence""",2023,2023-09-18T14:18:19Z,,,
arXIv2023,The ParlaSent Multilingual Training Dataset for Sentiment Identification in Parliamentary Proceedings,No.,1,"""No evidence""",2023,2023-09-18T14:01:06Z,,,
arXIv2023,Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation,No.,1,"""No evidence""",2023,2023-09-18T13:24:44Z,,,
arXIv2023,When Large Language Models Meet Citation: A Survey,No.,1,"""No evidence""",2023,2023-09-18T12:48:48Z,,,
arXIv2023,LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models,No.,1,"""No evidence""",2023,2023-09-18T12:22:00Z,,,
arXIv2023,Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs,No.,1,"""No evidence""",2023,2023-09-18T08:45:47Z,,,
arXIv2023,Summarization is (Almost) Dead,No.,1,"""No evidence""",2023,2023-09-18T08:13:01Z,,,
arXIv2023,A Multitask Training Approach to Enhance Whisper with Contextual Biasing and Open-Vocabulary Keyword Spotting,No.,1,"""No evidence""",2023,2023-09-18T08:03:54Z,,,
arXIv2023,Adapting Large Language Models via Reading Comprehension,No.,1,"""No evidence""",2023,2023-09-18T07:17:52Z,,,
arXIv2023,Pruning Large Language Models via Accuracy Predictor,No.,1,"""No evidence""",2023,2023-09-18T06:38:24Z,,,
arXIv2023,LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models,No.,1,"""No evidence""",2023,2023-09-18T06:35:10Z,,,
arXIv2023,Enhancing Multilingual Speech Recognition through Language Prompt Tuning and Frame-Level Language Adapter,No.,1,"""No evidence""",2023,2023-09-18T02:51:59Z,,,
arXIv2023,ChatGPT Hallucinates when Attributing Answers,No.,1,"""No evidence""",2023,2023-09-17T23:49:12Z,,,
arXIv2023,Do Large GPT Models Discover Moral Dimensions in Language Representations? A Topological Study Of Sentence Embeddings,No.,1,"""No evidence""",2023,2023-09-17T23:38:39Z,,,
arXIv2023,Augmenting text for spoken language understanding with Large Language Models,No.,1,"""No evidence""",2023,2023-09-17T22:25:34Z,,,
arXIv2023,Mitigating Shortcuts in Language Models with Soft Label Encoding,No.,1,"""No evidence""",2023,2023-09-17T21:18:02Z,,,
arXIv2023,Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles,No.,1,"""No evidence""",2023,2023-09-17T20:28:17Z,,,
arXIv2023,Performance of the Pre-Trained Large Language Model GPT-4 on Automated Short Answer Grading,No.,1,"""No evidence""",2023,2023-09-17T18:04:34Z,,,
arXIv2023,A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models,No.,1,"""No evidence""",2023,2023-09-17T16:50:07Z,,,
arXIv2023,OWL: A Large Language Model for IT Operations,No.,1,"""No evidence""",2023,2023-09-17T15:19:29Z,,,
arXIv2023,Detecting covariate drift in text data using document embeddings and dimensionality reduction,No.,1,"""No evidence""",2023,2023-09-17T07:34:57Z,,,
arXIv2023,From Cooking Recipes to Robot Task Trees -- Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network,No.,1,"""No evidence""",2023,2023-09-17T07:09:16Z,,,
arXIv2023,ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing,No.,1,"""No evidence""",2023,2023-09-17T01:42:30Z,,,
arXIv2023,How much can ChatGPT really help Computational Biologists in Programming?,No.,1,"""No evidence""",2023,2023-09-17T01:36:02Z,,,
arXIv2023,Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT and Ernie,No.,1,"""No evidence""",2023,2023-09-17T00:53:34Z,,,
arXIv2023,Contrastive Decoding Improves Reasoning in Large Language Models,No.,1,"""No evidence""",2023,2023-09-17T00:29:32Z,,,
arXIv2023,"Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF",No.,1,"""No evidence""",2023,2023-09-16T17:31:36Z,,,
arXIv2023,Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors,No.,1,"""No evidence""",2023,2023-09-16T17:12:05Z,,,
arXIv2023,gym-saturation: Gymnasium environments for saturation provers (System description),No.,1,"""No evidence""",2023,2023-09-16T15:25:39Z,,,
arXIv2023,Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference,No.,1,"""No evidence""",2023,2023-09-16T11:58:34Z,,,
arXIv2023,Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca,No.,1,"""No evidence""",2023,2023-09-16T11:22:46Z,,,
arXIv2023,Cross-Lingual Knowledge Editing in Large Language Models,No.,1,"""No evidence""",2023,2023-09-16T11:07:52Z,,,
arXIv2023,Enhancing Large Language Model Induced Task-Oriented Dialogue Systems Through Look-Forward Motivated Goals,No.,1,"""No evidence""",2023,2023-09-16T10:56:00Z,,,
arXIv2023,An Unified Search and Recommendation Foundation Model for Cold-Start Scenario,No.,1,"""No evidence""",2023,2023-09-16T10:00:02Z,,,
arXIv2023,Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models,No.,1,"""No evidence""",2023,2023-09-16T08:22:22Z,,,
arXIv2023,"Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models",No.,1,"""No evidence""",2023,2023-09-16T07:07:04Z,,,
arXIv2023,ChatGPT-4 with Code Interpreter can be used to solve introductory college-level vector calculus and electromagnetism problems,No.,1,"""No evidence""",2023,2023-09-16T05:19:39Z,,,
arXIv2023,X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs,No.,1,"""No evidence""",2023,2023-09-16T04:34:55Z,,,
arXIv2023,Has Sentiment Returned to the Pre-pandemic Level? A Sentiment Analysis Using U.S. College Subreddit Data from 2019 to 2022,No.,1,"""No evidence""",2023,2023-09-16T02:57:30Z,,,
arXIv2023,Bias and Fairness in Chatbots: An Overview,No.,1,"""No evidence""",2023,2023-09-16T02:01:18Z,,,
arXIv2023,GPT as a Baseline for Recommendation Explanation Texts,No.,1,"""No evidence""",2023,2023-09-16T00:00:44Z,,,
arXIv2023,BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials,No.,1,"""No evidence""",2023,2023-09-15T22:12:44Z,,,
arXIv2023,Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata,No.,1,"""No evidence""",2023,2023-09-15T22:11:29Z,,,
arXIv2023,Self-training Strategies for Sentiment Analysis: An Empirical Study,No.,1,"""No evidence""",2023,2023-09-15T21:42:46Z,,,
arXIv2023,Enhance audio generation controllability through representation similarity regularization,No.,1,"""No evidence""",2023,2023-09-15T21:32:20Z,,,
arXIv2023,Mining Patents with Large Language Models Elucidates the Chemical Function Landscape,No.,1,"""No evidence""",2023,2023-09-15T21:08:41Z,,,
arXIv2023,AlbNER: A Corpus for Named Entity Recognition in Albanian,No.,1,"""No evidence""",2023,2023-09-15T20:03:19Z,,,
arXIv2023,"OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?",No.,1,"""No evidence""",2023,2023-09-15T20:00:27Z,,,
arXIv2023,MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response,No.,1,"""No evidence""",2023,2023-09-15T19:31:40Z,,,
arXIv2023,Sparse Autoencoders Find Highly Interpretable Features in Language Models,No.,1,"""No evidence""",2023,2023-09-15T17:56:55Z,,,
arXIv2023,"""Merge Conflicts!"" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs",No.,1,"""No evidence""",2023,2023-09-15T17:47:59Z,,,
arXIv2023,Neural Machine Translation Models Can Learn to be Few-shot Learners,No.,1,"""No evidence""",2023,2023-09-15T17:44:21Z,,,
arXIv2023,Chain-of-Thought Reasoning is a Policy Improvement Operator,No.,1,"""No evidence""",2023,2023-09-15T17:44:17Z,,,
arXIv2023,Compositional Foundation Models for Hierarchical Planning,No.,1,"""No evidence""",2023,2023-09-15T17:44:05Z,,,
arXIv2023,ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer,No.,1,"""No evidence""",2023,2023-09-15T17:41:14Z,,,
arXIv2023,"When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets",No.,1,"""No evidence""",2023,2023-09-15T17:05:43Z,,,
arXIv2023,Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers,No.,1,"""No evidence""",2023,2023-09-15T16:50:09Z,,,
arXIv2023,Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens,No.,1,"""No evidence""",2023,2023-09-15T16:48:34Z,,,
arXIv2023,VulnSense: Efficient Vulnerability Detection in Ethereum Smart Contracts by Multimodal Learning with Graph Neural Network and Language Model,No.,1,"""No evidence""",2023,2023-09-15T15:26:44Z,,,
arXIv2023,Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite,No.,1,"""No evidence""",2023,2023-09-15T14:52:23Z,,,
arXIv2023,Exploring Meta Information for Audio-based Zero-shot Bird Classification,No.,1,"""No evidence""",2023,2023-09-15T13:50:16Z,,,
arXIv2023,MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings,No.,1,"""No evidence""",2023,2023-09-15T13:15:54Z,,,
arXIv2023,Unleashing Potential of Evidence in Knowledge-Intensive Dialogue Generation,No.,1,"""No evidence""",2023,2023-09-15T13:13:30Z,,,
arXIv2023,GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab,No.,1,"""No evidence""",2023,2023-09-15T10:51:21Z,,,
arXIv2023,Self-Consistent Narrative Prompts on Abductive Natural Language Inference,No.,1,"""No evidence""",2023,2023-09-15T10:48:10Z,,,
arXIv2023,Structural Self-Supervised Objectives for Transformers,No.,1,"""No evidence""",2023,2023-09-15T09:30:45Z,,,
arXIv2023,Large Language Models for Failure Mode Classification: An Investigation,No.,1,"""No evidence""",2023,2023-09-15T06:13:01Z,,,
arXIv2023,Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments and Relationships in Online Comments,No.,1,"""No evidence""",2023,2023-09-15T04:57:23Z,,,
arXIv2023,Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation,No.,1,"""No evidence""",2023,2023-09-15T04:07:57Z,,,
arXIv2023,Detecting Relevant Information in High-Volume Chat Logs: Keyphrase Extraction for Grooming and Drug Dealing Forensic Analysis,No.,1,"""No evidence""",2023,2023-09-15T03:18:31Z,,,
arXIv2023,InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning,No.,1,"""No evidence""",2023,2023-09-15T02:59:31Z,,,
arXIv2023,Empowering Private Tutoring by Chaining Large Language Models,No.,1,"""No evidence""",2023,2023-09-15T02:42:03Z,,,
arXIv2023,Research on Joint Representation Learning Methods for Entity Neighborhood Information and Description Information,No.,1,"""No evidence""",2023,2023-09-15T01:38:07Z,,,
arXIv2023,"Gender Bias in News Summarization: Measures, Pitfalls and Corpora",No.,1,"""No evidence""",2023,2023-09-14T22:20:27Z,,,
arXIv2023,An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing,No.,1,"""No evidence""",2023,2023-09-14T19:35:00Z,,,
arXIv2023,Leveraging Contextual Information for Effective Entity Salience Detection,No.,1,"""No evidence""",2023,2023-09-14T19:04:40Z,,,
arXIv2023,Unified Human-Scene Interaction via Prompted Chain-of-Contacts,No.,1,"""No evidence""",2023,2023-09-14T17:59:49Z,,,
arXIv2023,Looking at words and points with attention: a benchmark for text-to-shape coherence,No.,1,"""No evidence""",2023,2023-09-14T17:59:48Z,,,
arXIv2023,Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning,No.,1,"""No evidence""",2023,2023-09-14T17:58:33Z,,,
arXIv2023,Anchor Points: Benchmarking Models with Much Fewer Examples,No.,1,"""No evidence""",2023,2023-09-14T17:45:51Z,,,
arXIv2023,Agents: An Open-source Framework for Autonomous Language Agents,No.,1,"""No evidence""",2023,2023-09-14T17:18:25Z,,,
arXIv2023,The Rise and Potential of Large Language Model Based Agents: A Survey,No.,1,"""No evidence""",2023,2023-09-14T17:12:03Z,,,
arXIv2023,ExpertQA: Expert-Curated Questions and Attributed Answers,No.,1,"""No evidence""",2023,2023-09-14T16:54:34Z,,,
arXIv2023,Two Timin': Repairing Smart Contracts With A Two-Layered Approach,No.,1,"""No evidence""",2023,2023-09-14T16:37:23Z,,,
arXIv2023,CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration,No.,1,"""No evidence""",2023,2023-09-14T16:16:40Z,,,
arXIv2023,Text Classification of Cancer Clinical Trial Eligibility Criteria,No.,1,"""No evidence""",2023,2023-09-14T15:59:16Z,,,
arXIv2023,Generative AI Text Classification using Ensemble LLM Approaches,No.,1,"""No evidence""",2023,2023-09-14T14:41:46Z,,,
arXIv2023,ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing?,No.,1,"""No evidence""",2023,2023-09-14T14:04:03Z,,,
arXIv2023,Assessing the nature of large language models: A caution against anthropocentrism,No.,1,"""No evidence""",2023,2023-09-14T12:58:30Z,,,
arXIv2023,A Conversation is Worth A Thousand Recommendations: A Survey of Holistic Conversational Recommender Systems,No.,1,"""No evidence""",2023,2023-09-14T12:55:23Z,,,
arXIv2023,Automatic Data Visualization Generation from Chinese Natural Language Questions,No.,1,"""No evidence""",2023,2023-09-14T12:16:21Z,,,
arXIv2023,Incorporating Class-based Language Model for Named Entity Recognition in Factorized Neural Transducer,No.,1,"""No evidence""",2023,2023-09-14T12:14:49Z,,,
arXIv2023,SwitchGPT: Adapting Large Language Models for Non-Text Outputs,No.,1,"""No evidence""",2023,2023-09-14T11:38:23Z,,,
arXIv2023,Feature Engineering in Learning-to-Rank for Community Question Answering Task,No.,1,"""No evidence""",2023,2023-09-14T11:18:26Z,,,
arXIv2023,Zero-shot Audio Topic Reranking using Large Language Models,No.,1,"""No evidence""",2023,2023-09-14T11:13:36Z,,,
arXIv2023,Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision,No.,1,"""No evidence""",2023,2023-09-14T11:06:51Z,,,
arXIv2023,Revisiting Supertagging for HPSG,No.,1,"""No evidence""",2023,2023-09-14T10:49:16Z,,,
arXIv2023,Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer,No.,1,"""No evidence""",2023,2023-09-14T09:52:08Z,,,
arXIv2023,VerilogEval: Evaluating Large Language Models for Verilog Code Generation,No.,1,"""No evidence""",2023,2023-09-14T09:15:34Z,,,
arXIv2023,"SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",No.,1,"""No evidence""",2023,2023-09-14T05:56:49Z,,,
arXIv2023,Adapted Large Language Models Can Outperform Medical Experts in Clinical Text Summarization,No.,1,"""No evidence""",2023,2023-09-14T05:15:01Z,,,
arXIv2023,"A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time",No.,1,"""No evidence""",2023,2023-09-14T04:23:40Z,,,
arXIv2023,PromptASR for contextualized ASR with controllable style,No.,1,"""No evidence""",2023,2023-09-14T03:43:07Z,,,
arXIv2023,CPPF: A contextual and post-processing-free model for automatic speech recognition,No.,1,"""No evidence""",2023,2023-09-14T03:40:14Z,,,
arXIv2023,Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks,No.,1,"""No evidence""",2023,2023-09-14T03:13:18Z,,,
arXIv2023,Multi-Grade Deep Learning for Partial Differential Equations with Applications to the Burgers Equation,No.,1,"""No evidence""",2023,2023-09-14T03:09:58Z,,,
arXIv2023,DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective,No.,1,"""No evidence""",2023,2023-09-14T02:43:34Z,,,
arXIv2023,EnCodecMAE: Leveraging neural codecs for universal audio representation learning,No.,1,"""No evidence""",2023,2023-09-14T02:21:53Z,,,
arXIv2023,An Interactive Framework for Profiling News Media Sources,No.,1,"""No evidence""",2023,2023-09-14T02:03:45Z,,,
arXIv2023,Hybrid Attention-based Encoder-decoder Model for Efficient Language Model Adaptation,No.,1,"""No evidence""",2023,2023-09-14T01:07:36Z,,,
arXIv2023,Traveling Words: A Geometric Interpretation of Transformers,No.,1,"""No evidence""",2023,2023-09-13T21:01:03Z,,,
arXIv2023,Pretraining on the Test Set Is All You Need,No.,1,"""No evidence""",2023,2023-09-13T19:47:33Z,,,
arXIv2023,Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement,No.,1,"""No evidence""",2023,2023-09-13T18:43:13Z,,,
arXIv2023,EarthPT: a time series foundation model for Earth Observation,No.,1,"""No evidence""",2023,2023-09-13T18:00:00Z,,,
arXIv2023,Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics,No.,1,"""No evidence""",2023,2023-09-13T17:57:21Z,,,
arXIv2023,Can Whisper perform speech-based in-context learning?,No.,1,"""No evidence""",2023,2023-09-13T16:46:27Z,,,
arXIv2023,"Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting",No.,1,"""No evidence""",2023,2023-09-13T15:42:06Z,,,
arXIv2023,OYXOY: A Modern NLP Test Suite for Modern Greek,No.,1,"""No evidence""",2023,2023-09-13T15:00:56Z,,,
arXIv2023,Unsupervised Contrast-Consistent Ranking with Language Models,No.,1,"""No evidence""",2023,2023-09-13T14:36:26Z,,,
arXIv2023,Auto-Regressive Next-Token Predictors are Universal Learners,No.,1,"""No evidence""",2023,2023-09-13T14:15:03Z,,,
arXIv2023,Dynamic Causal Disentanglement Model for Dialogue Emotion Detection,No.,1,"""No evidence""",2023,2023-09-13T12:58:09Z,,,
arXIv2023,TAP: Targeted Prompting for Task Adaptive Generation of Textual Training Instances for Visual Classification,No.,1,"""No evidence""",2023,2023-09-13T08:59:54Z,,,
arXIv2023,Cognitive Mirage: A Review of Hallucinations in Large Language Models,No.,1,"""No evidence""",2023,2023-09-13T08:33:09Z,,,
arXIv2023,CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation,No.,1,"""No evidence""",2023,2023-09-13T06:40:24Z,,,
arXIv2023,Simultaneous Machine Translation with Large Language Models,No.,1,"""No evidence""",2023,2023-09-13T04:06:47Z,,,
arXIv2023,Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish,No.,1,"""No evidence""",2023,2023-09-13T03:42:28Z,,,
arXIv2023,Attention Loss Adjusted Prioritized Experience Replay,No.,1,"""No evidence""",2023,2023-09-13T02:49:32Z,,,
arXIv2023,Large Language Models Can Infer Psychological Dispositions of Social Media Users,No.,1,"""No evidence""",2023,2023-09-13T01:27:48Z,,,
arXIv2023,Do Generative Large Language Models need billions of parameters?,No.,1,"""No evidence""",2023,2023-09-12T20:25:22Z,,,
arXIv2023,Commands as AI Conversations,No.,1,"""No evidence""",2023,2023-09-12T19:52:27Z,,,
arXIv2023,Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity,No.,1,"""No evidence""",2023,2023-09-12T19:32:45Z,,,
arXIv2023,Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed Hinglish Memes,No.,1,"""No evidence""",2023,2023-09-12T18:47:29Z,,,
arXIv2023,Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets,No.,1,"""No evidence""",2023,2023-09-12T18:18:23Z,,,
arXIv2023,Leveraging Large Language Models for Automated Dialogue Analysis,No.,1,"""No evidence""",2023,2023-09-12T18:03:55Z,,,
arXIv2023,Unveiling the potential of large language models in generating semantic and cross-language clones,No.,1,"""No evidence""",2023,2023-09-12T17:40:49Z,,,
arXIv2023,Recovering from Privacy-Preserving Masking with Large Language Models,No.,1,"""No evidence""",2023,2023-09-12T16:39:41Z,,,
arXIv2023,Learning to Predict Concept Ordering for Common Sense Generation,No.,1,"""No evidence""",2023,2023-09-12T16:27:18Z,,,
arXIv2023,Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors,No.,1,"""No evidence""",2023,2023-09-12T16:15:33Z,,,
arXIv2023,Re-Reading Improves Reasoning in Large Language Models,No.,1,"""No evidence""",2023,2023-09-12T14:36:23Z,,,
arXIv2023,Mitigating the Alignment Tax of RLHF,No.,1,"""No evidence""",2023,2023-09-12T14:16:54Z,,,
arXIv2023,Efficient Memory Management for Large Language Model Serving with PagedAttention,No.,1,"""No evidence""",2023,2023-09-12T12:50:04Z,,,
arXIv2023,Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO,No.,1,"""No evidence""",2023,2023-09-12T11:18:29Z,,,
arXIv2023,AstroLLaMA: Towards Specialized Foundation Models in Astronomy,No.,1,"""No evidence""",2023,2023-09-12T11:02:27Z,,,
arXIv2023,Characterizing Latent Perspectives of Media Houses Towards Public Figures,No.,1,"""No evidence""",2023,2023-09-12T10:27:39Z,,,
arXIv2023,Towards Visual Taxonomy Expansion,No.,1,"""No evidence""",2023,2023-09-12T10:17:28Z,,,
arXIv2023,Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies,No.,1,"""No evidence""",2023,2023-09-12T09:37:08Z,,,
arXIv2023,BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models,No.,1,"""No evidence""",2023,2023-09-12T09:31:25Z,,,
arXIv2023,RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair,No.,1,"""No evidence""",2023,2023-09-12T08:52:56Z,,,
arXIv2023,Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model,No.,1,"""No evidence""",2023,2023-09-12T08:16:58Z,,,
arXIv2023,Automatically Estimating the Effort Required to Repay Self-Admitted Technical Debt,No.,1,"""No evidence""",2023,2023-09-12T07:40:18Z,,,
arXIv2023,"Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method",No.,1,"""No evidence""",2023,2023-09-12T06:05:57Z,,,
arXIv2023,Circuit Breaking: Removing Model Behaviors with Targeted Ablation,No.,1,"""No evidence""",2023,2023-09-12T05:51:56Z,,,
arXIv2023,Language Models as Black-Box Optimizers for Vision-Language Models,No.,1,"""No evidence""",2023,2023-09-12T04:03:41Z,,,
arXIv2023,SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs,No.,1,"""No evidence""",2023,2023-09-12T02:24:16Z,,,
arXIv2023,Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation,No.,1,"""No evidence""",2023,2023-09-12T01:19:54Z,,,
arXIv2023,Uncovering mesa-optimization algorithms in Transformers,No.,1,"""No evidence""",2023,2023-09-11T22:42:50Z,,,
arXIv2023,ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation,No.,1,"""No evidence""",2023,2023-09-11T22:28:36Z,,,
arXIv2023,Challenges in Annotating Datasets to Quantify Bias in Under-represented Society,No.,1,"""No evidence""",2023,2023-09-11T22:24:39Z,,,
arXIv2023,Large Language Models for Compiler Optimization,No.,1,"""No evidence""",2023,2023-09-11T22:11:46Z,,,
arXIv2023,Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature,No.,1,"""No evidence""",2023,2023-09-11T18:05:12Z,,,
arXIv2023,Hypothesis Search: Inductive Reasoning with Language Models,No.,1,"""No evidence""",2023,2023-09-11T17:56:57Z,,,
arXIv2023,Large Language Model for Science: A Study on P vs. NP,No.,1,"""No evidence""",2023,2023-09-11T17:49:27Z,,,
arXIv2023,MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning,No.,1,"""No evidence""",2023,2023-09-11T17:47:22Z,,,
arXIv2023,Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial NLP,No.,1,"""No evidence""",2023,2023-09-11T17:07:01Z,,,
arXIv2023,An Empirical Study of NetOps Capability of Pre-Trained Large Language Models,No.,1,"""No evidence""",2023,2023-09-11T15:45:40Z,,,
arXIv2023,Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications,No.,1,"""No evidence""",2023,2023-09-11T15:27:59Z,,,
arXIv2023,NExT-GPT: Any-to-Any Multimodal LLM,No.,1,"""No evidence""",2023,2023-09-11T15:02:25Z,,,
arXIv2023,Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task,No.,1,"""No evidence""",2023,2023-09-11T14:43:54Z,,,
arXIv2023,CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts,No.,1,"""No evidence""",2023,2023-09-11T14:36:16Z,,,
arXIv2023,Zero-shot Learning with Minimum Instruction to Extract Social Determinants and Family History from Clinical Notes using GPT Model,No.,1,"""No evidence""",2023,2023-09-11T14:16:27Z,,,
arXIv2023,Textbooks Are All You Need II: phi-1.5 technical report,No.,1,"""No evidence""",2023,2023-09-11T14:01:45Z,,,
arXIv2023,Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer Review,No.,1,"""No evidence""",2023,2023-09-11T13:51:40Z,,,
arXIv2023,Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models,No.,1,"""No evidence""",2023,2023-09-11T13:50:38Z,,,
arXIv2023,TeGit: Generating High-Quality Instruction-Tuning Data with Text-Grounded Task Design,No.,1,"""No evidence""",2023,2023-09-11T13:41:18Z,,,
arXIv2023,Improving Information Extraction on Business Documents with Specific Pre-Training Tasks,No.,1,"""No evidence""",2023,2023-09-11T13:05:23Z,,,
arXIv2023,Unsupervised Bias Detection in College Student Newspapers,No.,1,"""No evidence""",2023,2023-09-11T06:51:09Z,,,
arXIv2023,Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation,No.,1,"""No evidence""",2023,2023-09-11T05:12:14Z,,,
arXIv2023,Detecting Natural Language Biases with Prompt-based Learning,No.,1,"""No evidence""",2023,2023-09-11T04:20:36Z,,,
arXIv2023,Understanding the Impact of Post-Training Quantization on Large Language Models,No.,1,"""No evidence""",2023,2023-09-11T02:58:32Z,,,
arXIv2023,From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery,No.,1,"""No evidence""",2023,2023-09-11T02:35:36Z,,,
arXIv2023,Graph-Aware Contrasting for Multivariate Time-Series Classification,No.,1,"""No evidence""",2023,2023-09-11T02:35:22Z,,,
arXIv2023,HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving,No.,1,"""No evidence""",2023,2023-09-11T01:24:13Z,,,
arXIv2023,Large Language Models for Difficulty Estimation of Foreign Language Content with Application to Language Learning,No.,1,"""No evidence""",2023,2023-09-10T21:23:09Z,,,
arXIv2023,"A compendium of data sources for data science, machine learning, and artificial intelligence",No.,1,"""No evidence""",2023,2023-09-10T19:15:22Z,,,
arXIv2023,Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler,No.,1,"""No evidence""",2023,2023-09-10T17:13:25Z,,,
arXIv2023,Generalization error bounds for iterative learning algorithms with bounded updates,No.,1,"""No evidence""",2023,2023-09-10T16:55:59Z,,,
arXIv2023,An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents,No.,1,"""No evidence""",2023,2023-09-10T16:55:49Z,,,
arXIv2023,Exploiting CLIP for Zero-shot HOI Detection Requires Knowledge Distillation at Multiple Levels,No.,1,"""No evidence""",2023,2023-09-10T16:27:54Z,,,
arXIv2023,Implementing Learning Principles with a Personal AI Tutor: A Case Study,No.,1,"""No evidence""",2023,2023-09-10T15:35:47Z,,,
arXIv2023,"Decolonial AI Alignment: Openness, Vi?e\d{s}a-Dharma, and Including Excluded Knowledges",No.,1,"""No evidence""",2023,2023-09-10T14:04:21Z,,,
arXIv2023,RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution,No.,1,"""No evidence""",2023,2023-09-10T09:46:38Z,,,
arXIv2023,Evaluating Chatbots to Promote Users' Trust -- Practices and Open Problems,No.,1,"""No evidence""",2023,2023-09-09T22:40:30Z,,,
arXIv2023,Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System,No.,1,"""No evidence""",2023,2023-09-09T18:19:47Z,,,
arXIv2023,"Neurons in Large Language Models: Dead, N-gram, Positional",No.,1,"""No evidence""",2023,2023-09-09T15:51:36Z,,,
arXIv2023,FaNS: a Facet-based Narrative Similarity Metric,No.,1,"""No evidence""",2023,2023-09-09T15:29:24Z,,,
arXIv2023,Beyond Traditional Teaching: The Potential of Large Language Models and Chatbots in Graduate Engineering Education,No.,1,"""No evidence""",2023,2023-09-09T13:37:22Z,,,
arXIv2023,"MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images",No.,1,"""No evidence""",2023,2023-09-09T13:35:01Z,,,
arXIv2023,EPA: Easy Prompt Augmentation on Large Language Models via Multiple Sources and Multiple Targets,No.,1,"""No evidence""",2023,2023-09-09T09:03:50Z,,,
arXIv2023,Toward Reproducing Network Research Results Using Large Language Models,No.,1,"""No evidence""",2023,2023-09-09T08:07:54Z,,,
arXIv2023,Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model,No.,1,"""No evidence""",2023,2023-09-09T07:10:19Z,,,
arXIv2023,Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization,No.,1,"""No evidence""",2023,2023-09-09T03:01:38Z,,,
arXIv2023,FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning,No.,1,"""No evidence""",2023,2023-09-09T02:43:48Z,,,
arXIv2023,MADLAD-400: A Multilingual And Document-Level Large Audited Dataset,No.,1,"""No evidence""",2023,2023-09-09T02:34:01Z,,,
arXIv2023,Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf,No.,1,"""No evidence""",2023,2023-09-09T01:56:40Z,,,
arXIv2023,Efficient Finetuning Large Language Models For Vietnamese Chatbot,No.,1,"""No evidence""",2023,2023-09-09T00:11:53Z,,,
arXIv2023,"Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't have a Definitive Answer?",No.,1,"""No evidence""",2023,2023-09-08T23:12:03Z,,,
arXIv2023,Knowledge Distillation-Empowered Digital Twin for Anomaly Detection,No.,1,"""No evidence""",2023,2023-09-08T22:13:03Z,,,
arXIv2023,Unleashing the Power of Graph Learning through LLM-based Autonomous Agents,No.,1,"""No evidence""",2023,2023-09-08T19:34:29Z,,,
arXIv2023,When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale,No.,1,"""No evidence""",2023,2023-09-08T19:34:05Z,,,
arXIv2023,End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining,No.,1,"""No evidence""",2023,2023-09-08T17:12:14Z,,,
arXIv2023,MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers,No.,1,"""No evidence""",2023,2023-09-08T15:06:05Z,,,
arXIv2023,Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation,No.,1,"""No evidence""",2023,2023-09-08T15:00:41Z,,,
arXIv2023,Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens,No.,1,"""No evidence""",2023,2023-09-08T14:00:29Z,,,
arXIv2023,Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models,No.,1,"""No evidence""",2023,2023-09-08T13:29:05Z,,,
arXIv2023,FIMO: A Challenge Formal Dataset for Automated Theorem Proving,No.,1,"""No evidence""",2023,2023-09-08T12:34:28Z,,,
arXIv2023,Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations,No.,1,"""No evidence""",2023,2023-09-08T12:26:01Z,,,
arXIv2023,From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting,No.,1,"""No evidence""",2023,2023-09-08T11:31:08Z,,,
arXIv2023,Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification,No.,1,"""No evidence""",2023,2023-09-08T07:42:29Z,,,
arXIv2023,Compositional Learning of Visually-Grounded Concepts Using Reinforcement,No.,1,"""No evidence""",2023,2023-09-08T07:26:49Z,,,
arXIv2023,Context-Aware Prompt Tuning for Vision-Language Model with Dual-Alignment,No.,1,"""No evidence""",2023,2023-09-08T06:51:15Z,,,
arXIv2023,NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus,No.,1,"""No evidence""",2023,2023-09-08T06:23:25Z,,,
arXIv2023,Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large Language Models,No.,1,"""No evidence""",2023,2023-09-08T03:59:02Z,,,
arXIv2023,Meta predictive learning model of languages in neural circuits,No.,1,"""No evidence""",2023,2023-09-08T03:58:05Z,,,
arXIv2023,SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments,No.,1,"""No evidence""",2023,2023-09-08T02:24:37Z,,,
arXIv2023,Data Commons,No.,1,"""No evidence""",2023,2023-09-08T00:14:09Z,,,
arXIv2023,Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems,No.,1,"""No evidence""",2023,2023-09-07T21:57:39Z,,,
arXIv2023,LanSER: Language-Model Supported Speech Emotion Recognition,No.,1,"""No evidence""",2023,2023-09-07T19:21:08Z,,,
arXIv2023,ImageBind-LLM: Multi-modality Instruction Tuning,No.,1,"""No evidence""",2023,2023-09-07T17:59:45Z,,,
arXIv2023,Zero-Shot Audio Captioning via Audibility Guidance,No.,1,"""No evidence""",2023,2023-09-07T17:45:58Z,,,
arXIv2023,FLM-101B: An Open LLM and How to Train It with $100K Budget,No.,1,"""No evidence""",2023,2023-09-07T17:07:36Z,,,
arXIv2023,USA: Universal Sentiment Analysis Model & Construction of Japanese Sentiment Text Classification and Part of Speech Dataset,No.,1,"""No evidence""",2023,2023-09-07T15:35:00Z,,,
arXIv2023,Enhancing Pipeline-Based Conversational Agents with Large Language Models,No.,1,"""No evidence""",2023,2023-09-07T14:43:17Z,,,
arXIv2023,"The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties",No.,1,"""No evidence""",2023,2023-09-07T14:42:35Z,,,
arXIv2023,Exploring an LM to generate Prolog Predicates from Mathematics Questions,No.,1,"""No evidence""",2023,2023-09-07T12:10:47Z,,,
arXIv2023,Learning of Generalizable and Interpretable Knowledge in Grid-Based Reinforcement Learning Environments,No.,1,"""No evidence""",2023,2023-09-07T11:46:57Z,,,
arXIv2023,VideolandGPT: A User Study on a Conversational Recommender System,No.,1,"""No evidence""",2023,2023-09-07T11:24:47Z,,,
arXIv2023,Automatically Testing Functional Properties of Code Translation Models,No.,1,"""No evidence""",2023,2023-09-07T11:00:15Z,,,
arXIv2023,Evaluating ChatGPT as a Recommender System: A Rigorous Approach,No.,1,"""No evidence""",2023,2023-09-07T10:13:09Z,,,
arXIv2023,Loquacity and Visible Emotion: ChatGPT as a Policy Advisor,No.,1,"""No evidence""",2023,2023-09-07T09:40:12Z,,,
arXIv2023,Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision,No.,1,"""No evidence""",2023,2023-09-07T09:31:27Z,,,
arXIv2023,An Anchor Learning Approach for Citation Field Learning,No.,1,"""No evidence""",2023,2023-09-07T08:42:40Z,,,
arXIv2023,Using Curriculum Theory to Inform Approaches to Generative AI in Schools,No.,1,"""No evidence""",2023,2023-09-07T05:38:36Z,,,
arXIv2023,From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models,No.,1,"""No evidence""",2023,2023-09-07T00:14:37Z,,,
arXIv2023,ETP: Learning Transferable ECG Representations via ECG-Text Pre-training,No.,1,"""No evidence""",2023,2023-09-06T19:19:26Z,,,
arXIv2023,Gender-specific Machine Translation with Large Language Models,No.,1,"""No evidence""",2023,2023-09-06T17:24:06Z,,,
arXIv2023,GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models,No.,1,"""No evidence""",2023,2023-09-06T17:18:55Z,,,
arXIv2023,J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News,No.,1,"""No evidence""",2023,2023-09-06T17:06:31Z,,,
arXIv2023,Everyone Deserves A Reward: Learning Customized Human Preferences,No.,1,"""No evidence""",2023,2023-09-06T16:03:59Z,,,
arXIv2023,Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs,No.,1,"""No evidence""",2023,2023-09-06T15:55:01Z,,,
arXIv2023,ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning,No.,1,"""No evidence""",2023,2023-09-06T15:28:43Z,,,
arXIv2023,Synthetic Text Generation using Hypergraph Representations,No.,1,"""No evidence""",2023,2023-09-06T14:14:37Z,,,
arXIv2023,ViCGCN: Graph Convolutional Network with Contextualized Language Models for Social Media Mining in Vietnamese,No.,1,"""No evidence""",2023,2023-09-06T10:51:34Z,,,
arXIv2023,AI for Investment: A Platform Disruption,No.,1,"""No evidence""",2023,2023-09-06T09:46:29Z,,,
arXIv2023,Promoting Open-domain Dialogue Generation through Learning Pattern Information between Contexts and Responses,No.,1,"""No evidence""",2023,2023-09-06T08:11:39Z,,,
arXIv2023,Automated Bioinformatics Analysis via AutoBA,No.,1,"""No evidence""",2023,2023-09-06T07:54:45Z,,,
arXIv2023,CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model,No.,1,"""No evidence""",2023,2023-09-06T06:53:45Z,,,
arXIv2023,Hot or Cold? Adaptive Temperature Sampling for Code Generation with Large Language Models,No.,1,"""No evidence""",2023,2023-09-06T06:27:33Z,,,
arXIv2023,GPT Can Solve Mathematical Problems Without a Calculator,No.,1,"""No evidence""",2023,2023-09-06T06:18:16Z,,,
arXIv2023,HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus,No.,1,"""No evidence""",2023,2023-09-06T05:33:57Z,,,
arXIv2023,Offensive Hebrew Corpus and Detection using BERT,No.,1,"""No evidence""",2023,2023-09-06T05:18:43Z,,,
arXIv2023,HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models,No.,1,"""No evidence""",2023,2023-09-06T04:38:16Z,,,
arXIv2023,Certifying LLM Safety against Adversarial Prompting,No.,1,"""No evidence""",2023,2023-09-06T04:37:20Z,,,
arXIv2023,Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning,No.,1,"""No evidence""",2023,2023-09-05T21:27:27Z,,,
arXIv2023,Do You Trust ChatGPT? -- Perceived Credibility of Human and AI-Generated Content,No.,1,"""No evidence""",2023,2023-09-05T18:29:29Z,,,
arXIv2023,Cognitive Architectures for Language Agents,No.,1,"""No evidence""",2023,2023-09-05T17:56:20Z,,,
arXIv2023,Exploiting Language Models as a Source of Knowledge for Cognitive Agents,No.,1,"""No evidence""",2023,2023-09-05T15:18:04Z,,,
arXIv2023,CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning,No.,1,"""No evidence""",2023,2023-09-05T15:06:37Z,,,
arXIv2023,Dialog Action-Aware Transformer for Dialog Policy Learning,No.,1,"""No evidence""",2023,2023-09-05T13:47:25Z,,,
arXIv2023,"AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models",No.,1,"""No evidence""",2023,2023-09-05T13:43:37Z,,,
arXIv2023,Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering,No.,1,"""No evidence""",2023,2023-09-05T13:39:38Z,,,
arXIv2023,Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification,No.,1,"""No evidence""",2023,2023-09-05T12:48:21Z,,,
arXIv2023,Incorporating Dictionaries into a Neural Network Architecture to Extract COVID-19 Medical Concepts From Social Media,No.,1,"""No evidence""",2023,2023-09-05T12:47:44Z,,,
arXIv2023,Making Large Language Models Better Reasoners with Alignment,No.,1,"""No evidence""",2023,2023-09-05T11:32:48Z,,,
arXIv2023,Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies,No.,1,"""No evidence""",2023,2023-09-05T08:44:23Z,,,
arXIv2023,"The Impact of Artificial Intelligence on the Evolution of Digital Education: A Comparative Study of OpenAI Text Generation Tools including ChatGPT, Bing Chat, Bard, and Ernie",No.,1,"""No evidence""",2023,2023-09-05T08:15:00Z,,,
arXIv2023,CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models,No.,1,"""No evidence""",2023,2023-09-05T04:12:01Z,,,
arXIv2023,QuantEase: Optimization-based Quantization for Language Models,No.,1,"""No evidence""",2023,2023-09-05T01:39:09Z,,,
arXIv2023,ChatGPT Assisting Diagnosis of Neuro-ophthalmology Diseases Based on Case Reports,No.,1,"""No evidence""",2023,2023-09-05T00:44:23Z,,,
arXIv2023,"Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension",No.,1,"""No evidence""",2023,2023-09-04T21:22:28Z,,,
arXIv2023,Are Emergent Abilities in Large Language Models just In-Context Learning?,No.,1,"""No evidence""",2023,2023-09-04T20:54:11Z,,,
arXIv2023,Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction,No.,1,"""No evidence""",2023,2023-09-04T16:53:17Z,,,
arXIv2023,MathAttack: Attacking Large Language Models Towards Math Solving Ability,No.,1,"""No evidence""",2023,2023-09-04T16:02:23Z,,,
arXIv2023,Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?,No.,1,"""No evidence""",2023,2023-09-04T15:34:02Z,,,
arXIv2023,Fine-grained Affective Processing Capabilities Emerging from Large Language Models,No.,1,"""No evidence""",2023,2023-09-04T15:32:47Z,,,
arXIv2023,Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain,No.,1,"""No evidence""",2023,2023-09-04T15:26:15Z,,,
arXIv2023,Evolving linguistic divergence on polarizing social media,No.,1,"""No evidence""",2023,2023-09-04T15:21:55Z,,,
arXIv2023,Exploring the effectiveness of ChatGPT-based feedback compared with teacher feedback and self-feedback: Evidence from Chinese to English translation,No.,1,"""No evidence""",2023,2023-09-04T14:54:39Z,,,
arXIv2023,Concepts is All You Need: A More Direct Path to AGI,No.,1,"""No evidence""",2023,2023-09-04T14:14:41Z,,,
arXIv2023,DeViL: Decoding Vision features into Language,No.,1,"""No evidence""",2023,2023-09-04T13:59:55Z,,,
arXIv2023,Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking,No.,1,"""No evidence""",2023,2023-09-04T13:44:50Z,,,
arXIv2023,ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning,No.,1,"""No evidence""",2023,2023-09-04T11:38:02Z,,,
arXIv2023,What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You,No.,1,"""No evidence""",2023,2023-09-04T11:05:10Z,,,
arXIv2023,MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval,No.,1,"""No evidence""",2023,2023-09-04T10:48:29Z,,,
arXIv2023,LLM and Infrastructure as a Code use case,No.,1,"""No evidence""",2023,2023-09-04T09:05:17Z,,,
arXIv2023,Do androids dream of fictional references? A bibliographic dialogue with ChatGPT3.5,No.,1,"""No evidence""",2023,2023-09-04T08:11:59Z,,,
arXIv2023,Zero-shot information extraction from radiological reports using ChatGPT,No.,1,"""No evidence""",2023,2023-09-04T07:00:26Z,,,
arXIv2023,Understanding Video Scenes through Text: Insights from Text-based Video Question Answering,No.,1,"""No evidence""",2023,2023-09-04T06:11:00Z,,,
arXIv2023,DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion,No.,1,"""No evidence""",2023,2023-09-04T05:43:48Z,,,
arXIv2023,Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning,No.,1,"""No evidence""",2023,2023-09-04T04:31:24Z,,,
arXIv2023,Code Representation Pre-training with Complements from Program Executions,No.,1,"""No evidence""",2023,2023-09-04T01:57:22Z,,,
arXIv2023,Generative Social Choice,No.,1,"""No evidence""",2023,2023-09-03T23:47:21Z,,,
arXIv2023,Large AI Model Empowered Multimodal Semantic Communications,No.,1,"""No evidence""",2023,2023-09-03T19:24:34Z,,,
arXIv2023,Saturn: An Optimized Data System for Large Model Deep Learning Workloads,No.,1,"""No evidence""",2023,2023-09-03T17:19:11Z,,,
arXIv2023,MAGMA: Music Aligned Generative Motion Autodecoder,No.,1,"""No evidence""",2023,2023-09-03T15:21:47Z,,,
arXIv2023,A Visual Interpretation-Based Self-Improved Classification System Using Virtual Adversarial Training,No.,1,"""No evidence""",2023,2023-09-03T15:07:24Z,,,
arXIv2023,LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection,No.,1,"""No evidence""",2023,2023-09-03T14:22:57Z,,,
arXIv2023,Large Language Models for Generative Recommendation: A Survey and Visionary Discussions,No.,1,"""No evidence""",2023,2023-09-03T12:33:47Z,,,
arXIv2023,LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models,No.,1,"""No evidence""",2023,2023-09-03T12:23:33Z,,,
arXIv2023,AutoML-GPT: Large Language Model for AutoML,No.,1,"""No evidence""",2023,2023-09-03T09:39:49Z,,,
arXIv2023,MedChatZH: a Better Medical Adviser Learns from Better Instructions,No.,1,"""No evidence""",2023,2023-09-03T08:08:15Z,,,
arXIv2023,A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture,No.,1,"""No evidence""",2023,2023-09-03T07:03:17Z,,,
arXIv2023,CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection,No.,1,"""No evidence""",2023,2023-09-03T06:18:39Z,,,
arXIv2023,Business Process Text Sketch Automation Generation Using Large Language Model,No.,1,"""No evidence""",2023,2023-09-03T04:19:02Z,,,
arXIv2023,Multidomain transformer-based deep learning for early detection of network intrusion,No.,1,"""No evidence""",2023,2023-09-03T04:18:08Z,,,
arXIv2023,Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering,No.,1,"""No evidence""",2023,2023-09-03T03:27:06Z,,,
arXIv2023,Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging,No.,1,"""No evidence""",2023,2023-09-02T21:29:53Z,,,
arXIv2023,A Critical Examination of the Ethics of AI-Mediated Peer Review,No.,1,"""No evidence""",2023,2023-09-02T18:14:10Z,,,
arXIv2023,ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models,No.,1,"""No evidence""",2023,2023-09-02T16:50:30Z,,,
arXIv2023,Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks,No.,1,"""No evidence""",2023,2023-09-02T12:56:15Z,,,
arXIv2023,BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing,No.,1,"""No evidence""",2023,2023-09-02T11:46:05Z,,,
arXIv2023,RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model,No.,1,"""No evidence""",2023,2023-09-02T03:27:20Z,,,
arXIv2023,LinkTransformer: A Unified Package for Record Linkage with Transformer Language Models,No.,1,"""No evidence""",2023,2023-09-02T01:45:27Z,,,
arXIv2023,"Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties",No.,1,"""No evidence""",2023,2023-09-02T01:24:59Z,,,
arXIv2023,Bias and Fairness in Large Language Models: A Survey,No.,1,"""No evidence""",2023,2023-09-02T00:32:55Z,,,
arXIv2023,Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence,No.,1,"""No evidence""",2023,2023-09-01T22:26:06Z,,,
arXIv2023,PathLDM: Text conditioned Latent Diffusion Model for Histopathology,No.,1,"""No evidence""",2023,2023-09-01T22:08:32Z,,,
arXIv2023,Contextual Biasing of Named-Entities with Large Language Models,No.,1,"""No evidence""",2023,2023-09-01T20:15:48Z,,,
arXIv2023,OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation,No.,1,"""No evidence""",2023,2023-09-01T17:59:56Z,,,
arXIv2023,"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",No.,1,"""No evidence""",2023,2023-09-01T17:59:47Z,,,
arXIv2023,Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair,No.,1,"""No evidence""",2023,2023-09-01T17:54:14Z,,,
arXIv2023,Long-Term Ad Memorability: Understanding and Generating Memorable Ads,No.,1,"""No evidence""",2023,2023-09-01T10:27:04Z,,,
arXIv2023,FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning,No.,1,"""No evidence""",2023,2023-09-01T09:40:36Z,,,
arXIv2023,"Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",No.,1,"""No evidence""",2023,2023-09-01T09:34:49Z,,,
arXIv2023,Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI's Whisper,No.,1,"""No evidence""",2023,2023-09-01T08:31:35Z,,,
arXIv2023,RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback,No.,1,"""No evidence""",2023,2023-09-01T05:53:33Z,,,
arXIv2023,"SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks",No.,1,"""No evidence""",2023,2023-09-01T05:12:25Z,,,
arXIv2023,Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes,No.,1,"""No evidence""",2023,2023-09-01T04:01:20Z,,,
arXIv2023,Image Hijacks: Adversarial Images can Control Generative Models at Runtime,No.,1,"""No evidence""",2023,2023-09-01T03:53:40Z,,,
arXIv2023,Large Language Models for Semantic Monitoring of Corporate Disclosures: A Case Study on Korea's Top 50 KOSPI Companies,No.,1,"""No evidence""",2023,2023-09-01T01:51:28Z,,,
arXIv2023,Subjectivity in Unsupervised Machine Learning Model Selection,No.,1,"""No evidence""",2023,2023-09-01T01:40:58Z,,,
arXIv2023,AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation,No.,1,"""No evidence""",2023,2023-08-31T19:34:09Z,,,
arXIv2023,Large language models in medicine: the potentials and pitfalls,No.,1,"""No evidence""",2023,2023-08-31T19:06:39Z,,,
arXIv2023,YaRN: Efficient Context Window Extension of Large Language Models,No.,1,"""No evidence""",2023,2023-08-31T18:18:07Z,,,
arXIv2023,PointLLM: Empowering Large Language Models to Understand Point Clouds,No.,1,"""No evidence""",2023,2023-08-31T17:59:46Z,,,
arXIv2023,TouchStone: Evaluating Vision-Language Models by Language Models,No.,1,"""No evidence""",2023,2023-08-31T17:52:04Z,,,
arXIv2023,The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants,No.,1,"""No evidence""",2023,2023-08-31T17:43:08Z,,,
arXIv2023,Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information,No.,1,"""No evidence""",2023,2023-08-31T16:12:01Z,,,
arXIv2023,Can Programming Languages Boost Each Other via Instruction Tuning?,No.,1,"""No evidence""",2023,2023-08-31T15:53:51Z,,,
arXIv2023,Exploring the Potential of Large Language Models to Generate Formative Programming Feedback,No.,1,"""No evidence""",2023,2023-08-31T15:22:11Z,,,
arXIv2023,Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation,No.,1,"""No evidence""",2023,2023-08-31T15:19:28Z,,,
arXIv2023,Socratis: Are large multimodal models emotionally aware?,No.,1,"""No evidence""",2023,2023-08-31T13:59:35Z,,,
arXIv2023,"Can humans help BERT gain ""confidence""?",No.,1,"""No evidence""",2023,2023-08-31T13:12:28Z,,,
arXIv2023,GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice,No.,1,"""No evidence""",2023,2023-08-31T12:53:52Z,,,
arXIv2023,SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models,No.,1,"""No evidence""",2023,2023-08-31T12:53:09Z,,,
arXIv2023,Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology,No.,1,"""No evidence""",2023,2023-08-31T12:45:53Z,,,
arXIv2023,DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew,No.,1,"""No evidence""",2023,2023-08-31T12:43:18Z,,,
arXIv2023,Developing Social Robots with Empathetic Non-Verbal Cues Using Large Language Models,No.,1,"""No evidence""",2023,2023-08-31T08:20:04Z,,,
arXIv2023,Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations,No.,1,"""No evidence""",2023,2023-08-31T07:36:44Z,,,
arXIv2023,Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception,No.,1,"""No evidence""",2023,2023-08-31T06:53:55Z,,,
arXIv2023,Transformer Compression via Subspace Projection,No.,1,"""No evidence""",2023,2023-08-31T05:40:14Z,,,
arXIv2023,Enhancing Subtask Performance of Multi-modal Large Language Model,No.,1,"""No evidence""",2023,2023-08-31T05:37:21Z,,,
arXIv2023,MaintainoMATE: A GitHub App for Intelligent Automation of Maintenance Activities,No.,1,"""No evidence""",2023,2023-08-31T05:15:42Z,,,
arXIv2023,Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models,No.,1,"""No evidence""",2023,2023-08-31T05:15:27Z,,,
arXIv2023,BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge,No.,1,"""No evidence""",2023,2023-08-31T04:52:58Z,,,
arXIv2023,ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding,No.,1,"""No evidence""",2023,2023-08-30T21:56:36Z,,,
arXIv2023,Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art,No.,1,"""No evidence""",2023,2023-08-30T20:46:45Z,,,
arXIv2023,Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction,No.,1,"""No evidence""",2023,2023-08-30T18:34:55Z,,,
arXIv2023,Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness,No.,1,"""No evidence""",2023,2023-08-30T17:53:25Z,,,
arXIv2023,Intelligence as a Measure of Consciousness,No.,1,"""No evidence""",2023,2023-08-30T17:15:04Z,,,
arXIv2023,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,No.,1,"""No evidence""",2023,2023-08-30T17:07:17Z,,,
arXIv2023,Response: Emergent analogical reasoning in large language models,No.,1,"""No evidence""",2023,2023-08-30T16:17:26Z,,,
arXIv2023,"Analyzing Character and Consciousness in AI-Generated Social Content: A Case Study of Chirper, the AI Social Network",No.,1,"""No evidence""",2023,2023-08-30T15:40:18Z,,,
arXIv2023,Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,No.,1,"""No evidence""",2023,2023-08-30T14:33:25Z,,,
arXIv2023,DTrOCR: Decoder-only Transformer for Optical Character Recognition,No.,1,"""No evidence""",2023,2023-08-30T12:37:03Z,,,
arXIv2023,Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting,No.,1,"""No evidence""",2023,2023-08-30T11:35:21Z,,,
arXIv2023,WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model,No.,1,"""No evidence""",2023,2023-08-30T11:35:21Z,,,
arXIv2023,LLaSM: Large Language and Speech Model,No.,1,"""No evidence""",2023,2023-08-30T10:12:39Z,,,
arXIv2023,Natlog: Embedding Logic Programming into the Python Deep-Learning Ecosystem,No.,1,"""No evidence""",2023,2023-08-30T09:05:13Z,,,
arXIv2023,On the Potential of CLIP for Compositional Logical Reasoning,No.,1,"""No evidence""",2023,2023-08-30T09:04:24Z,,,
arXIv2023,Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models,No.,1,"""No evidence""",2023,2023-08-30T07:35:32Z,,,
arXIv2023,HAlf-MAsked Model for Named Entity Sentiment analysis,No.,1,"""No evidence""",2023,2023-08-30T06:53:24Z,,,
arXIv2023,AskIt: Unified Programming Interface for Programming with Large Language Models,No.,1,"""No evidence""",2023,2023-08-29T21:44:27Z,,,
arXIv2023,Extracting Mathematical Concepts with Large Language Models,No.,1,"""No evidence""",2023,2023-08-29T20:54:50Z,,,
arXIv2023,InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning,No.,1,"""No evidence""",2023,2023-08-29T20:02:24Z,,,
arXIv2023,Benchmarks for Detecting Measurement Tampering,No.,1,"""No evidence""",2023,2023-08-29T19:54:37Z,,,
arXIv2023,Radiology-Llama2: Best-in-Class Large Language Model for Radiology,No.,1,"""No evidence""",2023,2023-08-29T17:44:28Z,,,
arXIv2023,ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer,No.,1,"""No evidence""",2023,2023-08-29T17:36:02Z,,,
arXIv2023,When Do Program-of-Thoughts Work for Reasoning?,No.,1,"""No evidence""",2023,2023-08-29T17:22:39Z,,,
arXIv2023,"Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",No.,1,"""No evidence""",2023,2023-08-29T16:24:09Z,,,
arXIv2023,Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?,No.,1,"""No evidence""",2023,2023-08-29T15:57:32Z,,,
arXIv2023,Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation,No.,1,"""No evidence""",2023,2023-08-29T14:59:54Z,,,
arXIv2023,A Framework for Responsible Development of Automated Student Feedback with Generative AI,No.,1,"""No evidence""",2023,2023-08-29T14:29:57Z,,,
arXIv2023,FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models,No.,1,"""No evidence""",2023,2023-08-29T14:20:17Z,,,
arXIv2023,TaskLAMA: Probing the Complex Task Understanding of Language Models,No.,1,"""No evidence""",2023,2023-08-29T13:36:45Z,,,
arXIv2023,AutoDroid: LLM-powered Task Automation in Android,No.,1,"""No evidence""",2023,2023-08-29T13:02:30Z,,,
arXIv2023,"FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions",No.,1,"""No evidence""",2023,2023-08-29T11:08:40Z,,,
arXIv2023,Where Would I Go Next? Large Language Models as Human Mobility Predictors,No.,1,"""No evidence""",2023,2023-08-29T10:24:23Z,,,
arXIv2023,Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals,No.,1,"""No evidence""",2023,2023-08-29T10:20:53Z,,,
arXIv2023,SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation,No.,1,"""No evidence""",2023,2023-08-29T08:41:16Z,,,
arXIv2023,Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills,No.,1,"""No evidence""",2023,2023-08-29T08:36:30Z,,,
arXIv2023,LAMBO: Large Language Model Empowered Edge Intelligence,No.,1,"""No evidence""",2023,2023-08-29T07:25:42Z,,,
arXIv2023,Large language models converge toward human-like concept organization,No.,1,"""No evidence""",2023,2023-08-29T06:09:47Z,,,
arXIv2023,Is it an i or an l: Test-time Adaptation of Text Line Recognition Models,No.,1,"""No evidence""",2023,2023-08-29T05:44:00Z,,,
arXIv2023,Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning,No.,1,"""No evidence""",2023,2023-08-28T20:46:07Z,,,
arXIv2023,Distributionally Robust Statistical Verification with Imprecise Neural Networks,No.,1,"""No evidence""",2023,2023-08-28T18:06:24Z,,,
arXIv2023,"AI Deception: A Survey of Examples, Risks, and Potential Solutions",No.,1,"""No evidence""",2023,2023-08-28T17:59:35Z,,,
arXIv2023,CoVR: Learning Composed Video Retrieval from Web Video Captions,No.,1,"""No evidence""",2023,2023-08-28T17:55:33Z,,,
arXIv2023,Bayesian artificial brain with ChatGPT,No.,1,"""No evidence""",2023,2023-08-28T17:34:24Z,,,
arXIv2023,Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts,No.,1,"""No evidence""",2023,2023-08-28T16:18:50Z,,,
arXIv2023,ANER: Arabic and Arabizi Named Entity Recognition using Transformer-Based Approach,No.,1,"""No evidence""",2023,2023-08-28T15:54:48Z,,,
arXIv2023,Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance,No.,1,"""No evidence""",2023,2023-08-28T15:04:16Z,,,
arXIv2023,AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics,No.,1,"""No evidence""",2023,2023-08-28T14:23:04Z,,,
arXIv2023,Spoken Language Intelligence of Large Language Models for Language Learning,No.,1,"""No evidence""",2023,2023-08-28T12:47:41Z,,,
arXIv2023,Graph Meets LLMs: Towards Large Graph Models,No.,1,"""No evidence""",2023,2023-08-28T12:17:51Z,,,
arXIv2023,Multimodal Detection of Social Spambots in Twitter using Transformers,No.,1,"""No evidence""",2023,2023-08-28T10:51:11Z,,,
arXIv2023,ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment,No.,1,"""No evidence""",2023,2023-08-28T09:35:13Z,,,
arXIv2023,Using ChatGPT as a Static Application Security Testing Tool,No.,1,"""No evidence""",2023,2023-08-28T09:21:37Z,,,
arXIv2023,TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models,No.,1,"""No evidence""",2023,2023-08-28T09:06:32Z,,,
arXIv2023,FIRE: Food Image to REcipe generation,No.,1,"""No evidence""",2023,2023-08-28T08:14:20Z,,,
arXIv2023,Target-independent XLA optimization using Reinforcement Learning,No.,1,"""No evidence""",2023,2023-08-28T07:23:03Z,,,
arXIv2023,Mobile Foundation Model as Firmware,No.,1,"""No evidence""",2023,2023-08-28T07:21:26Z,,,
arXIv2023,"ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",No.,1,"""No evidence""",2023,2023-08-28T06:56:44Z,,,
arXIv2023,DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation,No.,1,"""No evidence""",2023,2023-08-28T06:41:49Z,,,
arXIv2023,Cognitive Effects in Large Language Models,No.,1,"""No evidence""",2023,2023-08-28T06:30:33Z,,,
arXIv2023,Reinforcement Learning for Generative AI: A Survey,No.,1,"""No evidence""",2023,2023-08-28T06:15:14Z,,,
arXIv2023,Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction,No.,1,"""No evidence""",2023,2023-08-28T06:05:18Z,,,
arXIv2023,Solving Attention Kernel Regression Problem via Pre-conditioner,No.,1,"""No evidence""",2023,2023-08-28T04:37:38Z,,,
arXIv2023,Artificial Intelligence in Career Counseling: A Test Case with ResumAI,No.,1,"""No evidence""",2023,2023-08-28T04:35:20Z,,,
arXIv2023,RecMind: Large Language Model Powered Agent For Recommendation,No.,1,"""No evidence""",2023,2023-08-28T04:31:04Z,,,
arXIv2023,Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning,No.,1,"""No evidence""",2023,2023-08-28T03:49:13Z,,,
arXIv2023,FonMTL: Towards Multitask Learning for the Fon Language,No.,1,"""No evidence""",2023,2023-08-28T03:26:21Z,,,
arXIv2023,SalesBot 2.0: A Human-Like Intent-Guided Chit-Chat Dataset,No.,1,"""No evidence""",2023,2023-08-28T02:48:49Z,,,
arXIv2023,PeptideBERT: A Language Model based on Transformers for Peptide Property Prediction,No.,1,"""No evidence""",2023,2023-08-28T01:09:21Z,,,
arXIv2023,The Cultural Psychology of Large Language Models: Is ChatGPT a Holistic or Analytic Thinker?,No.,1,"""No evidence""",2023,2023-08-28T01:05:18Z,,,
arXIv2023,Generations of Knowledge Graphs: The Crazy Ideas and the Business Impact,No.,1,"""No evidence""",2023,2023-08-27T22:35:27Z,,,
arXIv2023,Generative AI for Business Strategy: Using Foundation Models to Create Business Strategy Tools,No.,1,"""No evidence""",2023,2023-08-27T19:03:12Z,,,
arXIv2023,Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP,No.,1,"""No evidence""",2023,2023-08-27T18:46:47Z,,,
arXIv2023,Large Language Models Streamline Automated Machine Learning for Clinical Studies,No.,1,"""No evidence""",2023,2023-08-27T14:28:38Z,,,
arXIv2023,MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records,No.,1,"""No evidence""",2023,2023-08-27T12:24:39Z,,,
arXIv2023,MM-AU:Towards Multimodal Understanding of Advertisement Videos,No.,1,"""No evidence""",2023,2023-08-27T09:11:46Z,,,
arXIv2023,Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum,No.,1,"""No evidence""",2023,2023-08-27T07:53:00Z,,,
arXIv2023,"Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language Models",No.,1,"""No evidence""",2023,2023-08-26T21:38:31Z,,,
arXIv2023,"Improving Knowledge Distillation for BERT Models: Loss Functions, Mapping Methods, and Weight Tuning",No.,1,"""No evidence""",2023,2023-08-26T20:59:21Z,,,
arXIv2023,Exploring Large Language Models for Knowledge Graph Completion,No.,1,"""No evidence""",2023,2023-08-26T16:51:17Z,,,
arXIv2023,A Wide Evaluation of ChatGPT on Affective Computing Tasks,No.,1,"""No evidence""",2023,2023-08-26T16:10:30Z,,,
arXIv2023,FwdLLM: Efficient FedLLM using Forward Gradient,No.,1,"""No evidence""",2023,2023-08-26T14:36:30Z,,,
arXIv2023,Towards Real Time Egocentric Segment Captioning for The Blind and Visually Impaired in RGB-D Theatre Images,No.,1,"""No evidence""",2023,2023-08-26T14:27:10Z,,,
arXIv2023,Solving Math Word Problem with Problem Type Classification,No.,1,"""No evidence""",2023,2023-08-26T10:35:16Z,,,
arXIv2023,Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs,No.,1,"""No evidence""",2023,2023-08-26T08:31:48Z,,,
arXIv2023,ORES: Open-vocabulary Responsible Visual Synthesis,No.,1,"""No evidence""",2023,2023-08-26T06:47:34Z,,,
arXIv2023,ZC3: Zero-Shot Cross-Language Code Clone Detection,No.,1,"""No evidence""",2023,2023-08-26T03:48:10Z,,,
arXIv2023,ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning,No.,1,"""No evidence""",2023,2023-08-26T01:31:35Z,,,
arXIv2023,1.5 million materials narratives generated by chatbots,No.,1,"""No evidence""",2023,2023-08-25T22:00:53Z,,,
arXIv2023,Go Beyond Imagination: Maximizing Episodic Reachability with World Models,No.,1,"""No evidence""",2023,2023-08-25T20:30:20Z,,,
arXIv2023,ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection,No.,1,"""No evidence""",2023,2023-08-25T17:51:23Z,,,
arXIv2023,Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages,No.,1,"""No evidence""",2023,2023-08-25T16:35:06Z,,,
arXIv2023,Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models,No.,1,"""No evidence""",2023,2023-08-25T16:11:08Z,,,
arXIv2023,Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models,No.,1,"""No evidence""",2023,2023-08-25T15:33:47Z,,,
arXIv2023,SoTaNa: The Open-Source Software Development Assistant,No.,1,"""No evidence""",2023,2023-08-25T14:56:21Z,,,
arXIv2023,EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression,No.,1,"""No evidence""",2023,2023-08-25T14:23:40Z,,,
arXIv2023,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,No.,1,"""No evidence""",2023,2023-08-25T14:02:12Z,,,
arXIv2023,Prompting Visual-Language Models for Dynamic Facial Expression Recognition,No.,1,"""No evidence""",2023,2023-08-25T13:52:05Z,,,
arXIv2023,From system models to class models: An in-context learning paradigm,No.,1,"""No evidence""",2023,2023-08-25T13:50:17Z,,,
arXIv2023,Text Style Transfer Evaluation Using Large Language Models,No.,1,"""No evidence""",2023,2023-08-25T13:07:33Z,,,
arXIv2023,An Ensemble Approach to Personalized Real Time Predictive Writing for Experts,No.,1,"""No evidence""",2023,2023-08-25T12:45:46Z,,,
arXIv2023,Decoupled Structure for Improved Adaptability of End-to-End Models,No.,1,"""No evidence""",2023,2023-08-25T12:31:12Z,,,
arXIv2023,Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics,No.,1,"""No evidence""",2023,2023-08-25T11:41:05Z,,,
arXIv2023,Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity,No.,1,"""No evidence""",2023,2023-08-25T10:00:06Z,,,
arXIv2023,LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models,No.,1,"""No evidence""",2023,2023-08-25T07:04:16Z,,,
arXIv2023,Discovering Mental Health Research Topics with Topic Modeling,No.,1,"""No evidence""",2023,2023-08-25T05:25:05Z,,,
arXIv2023,SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research,No.,1,"""No evidence""",2023,2023-08-25T03:05:33Z,,,
arXIv2023,A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions,No.,1,"""No evidence""",2023,2023-08-25T02:35:54Z,,,
arXIv2023,OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models,No.,1,"""No evidence""",2023,2023-08-25T02:28:35Z,,,
arXIv2023,MLLM-DataEngine: An Iterative Refinement Approach for MLLM,No.,1,"""No evidence""",2023,2023-08-25T01:41:04Z,,,
arXIv2023,DARWIN Series: Domain Specific Large Language Models for Natural Science,No.,1,"""No evidence""",2023,2023-08-25T01:40:48Z,,,
arXIv2023,Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation,No.,1,"""No evidence""",2023,2023-08-24T23:38:44Z,,,
arXIv2023,Bayesian Low-rank Adaptation for Large Language Models,No.,1,"""No evidence""",2023,2023-08-24T23:06:21Z,,,
arXIv2023,Multi-BERT for Embeddings for Recommendation System,No.,1,"""No evidence""",2023,2023-08-24T19:36:05Z,,,
arXIv2023,Financial News Analytics Using Fine-Tuned Llama 2 GPT Model,No.,1,"""No evidence""",2023,2023-08-24T18:58:10Z,,,
arXIv2023,"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",No.,1,"""No evidence""",2023,2023-08-24T17:59:17Z,,,
arXIv2023,Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment,No.,1,"""No evidence""",2023,2023-08-24T17:56:46Z,,,
arXIv2023,Code Llama: Open Foundation Models for Code,No.,1,"""No evidence""",2023,2023-08-24T17:39:13Z,,,
arXIv2023,Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI,No.,1,"""No evidence""",2023,2023-08-24T16:42:23Z,,,
arXIv2023,Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques,No.,1,"""No evidence""",2023,2023-08-24T15:06:04Z,,,
arXIv2023,PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning,No.,1,"""No evidence""",2023,2023-08-24T13:03:42Z,,,
arXIv2023,Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach,No.,1,"""No evidence""",2023,2023-08-24T12:49:21Z,,,
arXIv2023,Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models,No.,1,"""No evidence""",2023,2023-08-24T11:07:47Z,,,
arXIv2023,SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge,No.,1,"""No evidence""",2023,2023-08-24T09:47:28Z,,,
arXIv2023,Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models,No.,1,"""No evidence""",2023,2023-08-24T05:35:58Z,,,
arXIv2023,A Small and Fast BERT for Chinese Medical Punctuation Restoration,No.,1,"""No evidence""",2023,2023-08-24T05:15:43Z,,,
arXIv2023,Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions,No.,1,"""No evidence""",2023,2023-08-24T05:04:10Z,,,
arXIv2023,A Co-training Approach for Noisy Time Series Learning,No.,1,"""No evidence""",2023,2023-08-24T04:33:30Z,,,
arXIv2023,CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias,No.,1,"""No evidence""",2023,2023-08-24T03:53:55Z,,,
arXIv2023,HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks,No.,1,"""No evidence""",2023,2023-08-24T03:47:27Z,,,
arXIv2023,Rational Decision-Making Agent with Internalized Utility Judgment,No.,1,"""No evidence""",2023,2023-08-24T03:11:45Z,,,
arXIv2023,CGMI: Configurable General Multi-Agent Interaction Framework,No.,1,"""No evidence""",2023,2023-08-24T02:03:29Z,,,
arXIv2023,GPTEval: A Survey on Assessments of ChatGPT and GPT-4,No.,1,"""No evidence""",2023,2023-08-24T01:17:16Z,,,
arXIv2023,American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers,No.,1,"""No evidence""",2023,2023-08-24T00:24:42Z,,,
arXIv2023,Considerations for health care institutions training large language models on electronic health records,No.,1,"""No evidence""",2023,2023-08-24T00:09:01Z,,,
arXIv2023,Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis,No.,1,"""No evidence""",2023,2023-08-23T23:16:35Z,,,
arXIv2023,Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature,No.,1,"""No evidence""",2023,2023-08-23T20:42:32Z,,,
arXIv2023,How to Protect Copyright Data in Optimization of Large Language Models?,No.,1,"""No evidence""",2023,2023-08-23T16:48:04Z,,,
arXIv2023,LLMRec: Benchmarking Large Language Models on Recommendation Task,No.,1,"""No evidence""",2023,2023-08-23T16:32:54Z,,,
arXIv2023,Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning,No.,1,"""No evidence""",2023,2023-08-23T16:01:12Z,,,
arXIv2023,Instruction Position Matters in Sequence Generation with Large Language Models,No.,1,"""No evidence""",2023,2023-08-23T12:36:57Z,,,
arXIv2023,Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments,No.,1,"""No evidence""",2023,2023-08-23T12:11:27Z,,,
arXIv2023,InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4,No.,1,"""No evidence""",2023,2023-08-23T11:27:30Z,,,
arXIv2023,Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference,No.,1,"""No evidence""",2023,2023-08-23T11:25:37Z,,,
arXIv2023,FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering,No.,1,"""No evidence""",2023,2023-08-23T11:00:36Z,,,
arXIv2023,Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track,No.,1,"""No evidence""",2023,2023-08-23T09:56:59Z,,,
arXIv2023,Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages,No.,1,"""No evidence""",2023,2023-08-23T09:55:41Z,,,
arXIv2023,From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning,No.,1,"""No evidence""",2023,2023-08-23T09:45:29Z,,,
arXIv2023,Prompt-Based Length Controlled Generation with Reinforcement Learning,No.,1,"""No evidence""",2023,2023-08-23T09:43:10Z,,,
arXIv2023,LKPNR: LLM and KG for Personalized News Recommendation Framework,No.,1,"""No evidence""",2023,2023-08-23T09:39:18Z,,,
arXIv2023,Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network,No.,1,"""No evidence""",2023,2023-08-23T09:37:22Z,,,
arXIv2023,Audio Generation with Multiple Conditional Diffusion Model,No.,1,"""No evidence""",2023,2023-08-23T06:21:46Z,,,
arXIv2023,Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification,No.,1,"""No evidence""",2023,2023-08-23T05:04:01Z,,,
arXIv2023,Diagnosing Infeasible Optimization Problems Using Large Language Models,No.,1,"""No evidence""",2023,2023-08-23T04:34:05Z,,,
arXIv2023,Bridging the Gap: Deciphering Tabular Data Using Large Language Model,No.,1,"""No evidence""",2023,2023-08-23T03:38:21Z,,,
arXIv2023,Cabrita: closing the gap for foreign languages,No.,1,"""No evidence""",2023,2023-08-23T02:49:35Z,,,
arXIv2023,Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts,No.,1,"""No evidence""",2023,2023-08-22T21:18:54Z,,,
arXIv2023,Knowledge Graph Prompting for Multi-Document Question Answering,No.,1,"""No evidence""",2023,2023-08-22T18:41:31Z,,,
arXIv2023,"Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models",No.,1,"""No evidence""",2023,2023-08-22T17:48:24Z,,,
arXIv2023,SeamlessM4T: Massively Multilingual & Multimodal Machine Translation,No.,1,"""No evidence""",2023,2023-08-22T17:44:18Z,,,
arXIv2023,Using ChatGPT as a CAT tool in Easy Language translation,No.,1,"""No evidence""",2023,2023-08-22T16:59:31Z,,,
arXIv2023,Multi-event Video-Text Retrieval,No.,1,"""No evidence""",2023,2023-08-22T16:32:46Z,,,
arXIv2023,Furnishing Sound Event Detection with Language Model Abilities,No.,1,"""No evidence""",2023,2023-08-22T15:59:06Z,,,
arXIv2023,Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features,No.,1,"""No evidence""",2023,2023-08-22T15:03:16Z,,,
arXIv2023,A Survey on Large Language Model based Autonomous Agents,No.,1,"""No evidence""",2023,2023-08-22T13:30:37Z,,,
arXIv2023,AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block,No.,1,"""No evidence""",2023,2023-08-22T13:15:29Z,,,
arXIv2023,ProAgent: Building Proactive Cooperative Agents with Large Language Models,No.,1,"""No evidence""",2023,2023-08-22T10:36:56Z,,,
arXIv2023,From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow,No.,1,"""No evidence""",2023,2023-08-22T09:30:02Z,,,
arXIv2023,LEAP: Efficient and Automated Test Method for NLP Software,No.,1,"""No evidence""",2023,2023-08-22T08:51:10Z,,,
arXIv2023,Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning,No.,1,"""No evidence""",2023,2023-08-22T08:43:33Z,,,
arXIv2023,ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts,No.,1,"""No evidence""",2023,2023-08-22T07:21:24Z,,,
arXIv2023,Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries,No.,1,"""No evidence""",2023,2023-08-22T04:49:23Z,,,
arXIv2023,Is There Any Social Principle for LLM-Based Agents?,No.,1,"""No evidence""",2023,2023-08-22T02:32:14Z,,,
arXIv2023,ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation,No.,1,"""No evidence""",2023,2023-08-22T02:25:04Z,,,
arXIv2023,Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection,No.,1,"""No evidence""",2023,2023-08-22T01:55:03Z,,,
arXIv2023,"""Guinea Pig Trials"" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion",No.,1,"""No evidence""",2023,2023-08-21T18:42:17Z,,,
arXIv2023,Can Language Models Learn to Listen?,No.,1,"""No evidence""",2023,2023-08-21T17:59:02Z,,,
arXIv2023,LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles,No.,1,"""No evidence""",2023,2023-08-21T16:49:40Z,,,
arXIv2023,AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors,No.,1,"""No evidence""",2023,2023-08-21T16:47:11Z,,,
arXIv2023,Instruction Tuning for Large Language Models: A Survey,No.,1,"""No evidence""",2023,2023-08-21T15:35:16Z,,,
arXIv2023,Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis,No.,1,"""No evidence""",2023,2023-08-21T15:19:10Z,,,
arXIv2023,DepreSym: A Depression Symptom Annotated Corpus and the Role of LLMs as Assessors of Psychological Markers,No.,1,"""No evidence""",2023,2023-08-21T14:44:31Z,,,
arXIv2023,WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models,No.,1,"""No evidence""",2023,2023-08-21T14:40:48Z,,,
arXIv2023,DataVinci: Learning Syntactic and Semantic String Repairs,No.,1,"""No evidence""",2023,2023-08-21T14:09:16Z,,,
arXIv2023,On the Adversarial Robustness of Multi-Modal Foundation Models,No.,1,"""No evidence""",2023,2023-08-21T14:09:09Z,,,
arXIv2023,Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models,No.,1,"""No evidence""",2023,2023-08-21T13:14:32Z,,,
arXIv2023,RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-08-21T11:08:16Z,,,
arXIv2023,Large Language Models for Software Engineering: A Systematic Literature Review,No.,1,"""No evidence""",2023,2023-08-21T10:37:49Z,,,
arXIv2023,Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning,No.,1,"""No evidence""",2023,2023-08-21T09:35:33Z,,,
arXIv2023,PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator,No.,1,"""No evidence""",2023,2023-08-21T06:51:56Z,,,
arXIv2023,GradientCoin: A Peer-to-Peer Decentralized Large Language Models,No.,1,"""No evidence""",2023,2023-08-21T06:42:42Z,,,
arXIv2023,Elucidating STEM Concepts through Generative AI: A Multi-modal Exploration of Analogical Reasoning,No.,1,"""No evidence""",2023,2023-08-21T04:00:56Z,,,
arXIv2023,GPT-in-the-Loop: Adaptive Decision-Making for Multiagent Systems,No.,1,"""No evidence""",2023,2023-08-21T03:08:16Z,,,
arXIv2023,LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework,No.,1,"""No evidence""",2023,2023-08-20T23:47:23Z,,,
arXIv2023,Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models,No.,1,"""No evidence""",2023,2023-08-20T22:36:23Z,,,
arXIv2023,LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models,No.,1,"""No evidence""",2023,2023-08-20T22:08:03Z,,,
arXIv2023,cantnlp@LT-EDI-2023: Homophobia/Transphobia Detection in Social Media Comments using Spatio-Temporally Retrained Language Models,No.,1,"""No evidence""",2023,2023-08-20T21:30:34Z,,,
arXIv2023,Imaginations of WALL-E : Reconstructing Experiences with an Imagination-Inspired Module for Advanced AI Systems,No.,1,"""No evidence""",2023,2023-08-20T20:10:55Z,,,
arXIv2023,Can Large Language Models Find And Fix Vulnerable Software?,No.,1,"""No evidence""",2023,2023-08-20T19:33:12Z,,,
arXIv2023,Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting,No.,1,"""No evidence""",2023,2023-08-20T16:27:17Z,,,
arXIv2023,Scaling up Discovery of Latent Concepts in Deep NLP Models,No.,1,"""No evidence""",2023,2023-08-20T13:20:54Z,,,
arXIv2023,Large Transformers are Better EEG Learners,No.,1,"""No evidence""",2023,2023-08-20T12:54:17Z,,,
arXIv2023,StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data,No.,1,"""No evidence""",2023,2023-08-20T12:43:52Z,,,
arXIv2023,LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models,No.,1,"""No evidence""",2023,2023-08-20T12:42:19Z,,,
arXIv2023,Activation Addition: Steering Language Models Without Optimization,No.,1,"""No evidence""",2023,2023-08-20T12:21:05Z,,,
arXIv2023,Indonesian Automatic Speech Recognition with XLSR-53,No.,1,"""No evidence""",2023,2023-08-20T09:59:40Z,,,
arXIv2023,ChatEDA: A Large Language Model Powered Autonomous Agent for EDA,No.,1,"""No evidence""",2023,2023-08-20T08:32:13Z,,,
arXIv2023,FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt,No.,1,"""No evidence""",2023,2023-08-20T05:58:33Z,,,
arXIv2023,Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?,No.,1,"""No evidence""",2023,2023-08-20T05:31:03Z,,,
arXIv2023,ExpeL: LLM Agents Are Experiential Learners,No.,1,"""No evidence""",2023,2023-08-20T03:03:34Z,,,
arXIv2023,ASPIRE: Language-Guided Augmentation for Robust Image Classification,No.,1,"""No evidence""",2023,2023-08-19T20:18:15Z,,,
arXIv2023,"Open, Closed, or Small Language Models for Text Classification?",No.,1,"""No evidence""",2023,2023-08-19T18:58:32Z,,,
arXIv2023,PACE: Improving Prompt with Actor-Critic Editing for Large Language Model,No.,1,"""No evidence""",2023,2023-08-19T18:47:44Z,,,
arXIv2023,Large Language Models as Zero-Shot Conversational Recommenders,No.,1,"""No evidence""",2023,2023-08-19T15:29:45Z,,,
arXIv2023,ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval,No.,1,"""No evidence""",2023,2023-08-19T14:17:57Z,,,
arXIv2023,Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers,No.,1,"""No evidence""",2023,2023-08-19T13:29:15Z,,,
arXIv2023,Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes,No.,1,"""No evidence""",2023,2023-08-19T13:14:15Z,,,
arXIv2023,HICL: Hashtag-Driven In-Context Learning for Social Media Natural Language Understanding,No.,1,"""No evidence""",2023,2023-08-19T11:31:45Z,,,
arXIv2023,Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5: A Bit of Help Needed from Google Translate,No.,1,"""No evidence""",2023,2023-08-19T09:19:34Z,,,
arXIv2023,Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs,No.,1,"""No evidence""",2023,2023-08-19T09:17:19Z,,,
arXIv2023,BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions,No.,1,"""No evidence""",2023,2023-08-19T07:53:43Z,,,
arXIv2023,East: Efficient and Accurate Secure Transformer Framework for Inference,No.,1,"""No evidence""",2023,2023-08-19T06:26:14Z,,,
arXIv2023,RAH! RecSys-Assistant-Human: A Human-Centered Recommendation Framework with LLM Agents,No.,1,"""No evidence""",2023,2023-08-19T04:46:01Z,,,
arXIv2023,Inductive-bias Learning: Generating Code Models with Large Language Model,No.,1,"""No evidence""",2023,2023-08-19T03:01:45Z,,,
arXIv2023,DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization,No.,1,"""No evidence""",2023,2023-08-19T02:51:00Z,,,
arXIv2023,A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case,No.,1,"""No evidence""",2023,2023-08-19T02:30:35Z,,,
arXIv2023,How susceptible are LLMs to Logical Fallacies?,No.,1,"""No evidence""",2023,2023-08-18T23:07:29Z,,,
arXIv2023,YORC: Yoruba Reading Comprehension dataset,No.,1,"""No evidence""",2023,2023-08-18T18:46:47Z,,,
arXIv2023,Graph of Thoughts: Solving Elaborate Problems with Large Language Models,No.,1,"""No evidence""",2023,2023-08-18T17:29:23Z,,,
arXIv2023,OCR Language Models with Custom Vocabularies,No.,1,"""No evidence""",2023,2023-08-18T16:46:11Z,,,
arXIv2023,ChatHaruhi: Reviving Anime Character in Reality via Large Language Model,No.,1,"""No evidence""",2023,2023-08-18T14:50:25Z,,,
arXIv2023,WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct,No.,1,"""No evidence""",2023,2023-08-18T14:23:21Z,,,
arXIv2023,PUMGPT: A Large Vision-Language Model for Product Understanding,No.,1,"""No evidence""",2023,2023-08-18T14:01:37Z,,,
arXIv2023,Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication,No.,1,"""No evidence""",2023,2023-08-18T12:14:25Z,,,
arXIv2023,Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model,No.,1,"""No evidence""",2023,2023-08-18T10:34:46Z,,,
arXIv2023,Scope is all you need: Transforming LLMs for HPC Code,No.,1,"""No evidence""",2023,2023-08-18T10:12:03Z,,,
arXIv2023,A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages,No.,1,"""No evidence""",2023,2023-08-18T10:07:28Z,,,
arXIv2023,Leveraging Large Language Models for DRL-Based Anti-Jamming Strategies in Zero Touch Networks,No.,1,"""No evidence""",2023,2023-08-18T08:13:23Z,,,
arXIv2023,A tailored Handwritten-Text-Recognition System for Medieval Latin,No.,1,"""No evidence""",2023,2023-08-18T08:02:52Z,,,
arXIv2023,Accelerated materials language processing enabled by GPT,No.,1,"""No evidence""",2023,2023-08-18T07:31:13Z,,,
arXIv2023,Document Automation Architectures: Updated Survey in Light of Large Language Models,No.,1,"""No evidence""",2023,2023-08-18T06:59:55Z,,,
arXIv2023,Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach,No.,1,"""No evidence""",2023,2023-08-18T03:12:59Z,,,
arXIv2023,Conversational Ontology Alignment with ChatGPT,No.,1,"""No evidence""",2023,2023-08-18T00:26:05Z,,,
arXIv2023,A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports,No.,1,"""No evidence""",2023,2023-08-17T21:36:56Z,,,
arXIv2023,ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT,No.,1,"""No evidence""",2023,2023-08-17T20:50:46Z,,,
arXIv2023,Characterizing Information Seeking Events in Health-Related Social Discourse,No.,1,"""No evidence""",2023,2023-08-17T19:08:42Z,,,
arXIv2023,The Unreasonable Effectiveness of Large Language-Vision Models for Source-free Video Domain Adaptation,No.,1,"""No evidence""",2023,2023-08-17T18:12:05Z,,,
arXIv2023,MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models,No.,1,"""No evidence""",2023,2023-08-17T16:59:50Z,,,
arXIv2023,Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries,No.,1,"""No evidence""",2023,2023-08-17T16:38:30Z,,,
arXIv2023,Contrasting Linguistic Patterns in Human and LLM-Generated Text,No.,1,"""No evidence""",2023,2023-08-17T15:54:38Z,,,
arXIv2023,Uni-NLX: Unifying Textual Explanations for Vision and Vision-Language Tasks,No.,1,"""No evidence""",2023,2023-08-17T15:15:55Z,,,
arXIv2023,FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings,No.,1,"""No evidence""",2023,2023-08-17T14:30:26Z,,,
arXIv2023,Reinforced Self-Training (ReST) for Language Modeling,No.,1,"""No evidence""",2023,2023-08-17T14:12:48Z,,,
arXIv2023,Evaluation of really good grammatical error correction,No.,1,"""No evidence""",2023,2023-08-17T13:45:35Z,,,
arXIv2023,End-to-End Beam Retrieval for Multi-Hop Question Answering,No.,1,"""No evidence""",2023,2023-08-17T13:24:14Z,,,
arXIv2023,KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases,No.,1,"""No evidence""",2023,2023-08-17T13:07:00Z,,,
arXIv2023,Towards Automatically Addressing Self-Admitted Technical Debt: How Far Are We?,No.,1,"""No evidence""",2023,2023-08-17T12:27:32Z,,,
arXIv2023,Language-enhanced RNR-Map: Querying Renderable Neural Radiance Field maps with natural language,No.,1,"""No evidence""",2023,2023-08-17T08:27:01Z,,,
arXIv2023,"Approaches to Generative Artificial Intelligence, A Social Justice Perspective",No.,1,"""No evidence""",2023,2023-08-17T06:30:46Z,,,
arXIv2023,Chinese Spelling Correction as Rephrasing Language Model,No.,1,"""No evidence""",2023,2023-08-17T06:04:28Z,,,
arXIv2023,CodeCoT: Tackling Code Syntax Errors in CoT Reasoning for Code Generation,No.,1,"""No evidence""",2023,2023-08-17T04:58:51Z,,,
arXIv2023,Exploring Demonstration Ensembling for In-context Learning,No.,1,"""No evidence""",2023,2023-08-17T04:45:19Z,,,
arXIv2023,Large Language Models at Work in China's Labor Market,No.,1,"""No evidence""",2023,2023-08-17T04:20:36Z,,,
arXIv2023,Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes,No.,1,"""No evidence""",2023,2023-08-17T03:52:15Z,,,
arXIv2023,Fine-grained Text and Image Guided Point Cloud Completion with CLIP Model,No.,1,"""No evidence""",2023,2023-08-17T03:05:18Z,,,
arXIv2023,LLM-FuncMapper: Function Identification for Interpreting Complex Clauses in Building Codes via LLM,No.,1,"""No evidence""",2023,2023-08-17T01:58:04Z,,,
arXIv2023,A Preliminary Study on a Conceptual Game Feature Generation and Recommendation System,No.,1,"""No evidence""",2023,2023-08-16T21:20:50Z,,,
arXIv2023,Large Language Models for Granularized Barrett's Esophagus Diagnosis Classification,No.,1,"""No evidence""",2023,2023-08-16T20:17:46Z,,,
arXIv2023,BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition,No.,1,"""No evidence""",2023,2023-08-16T18:48:01Z,,,
arXIv2023,Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought,No.,1,"""No evidence""",2023,2023-08-16T18:13:27Z,,,
arXIv2023,FootGPT : A Large Language Model Development Experiment on a Minimal Setting,No.,1,"""No evidence""",2023,2023-08-16T18:03:22Z,,,
arXIv2023,Painter: Teaching Auto-regressive Language Models to Draw Sketches,No.,1,"""No evidence""",2023,2023-08-16T17:18:30Z,,,
arXIv2023,Time Travel in LLMs: Tracing Data Contamination in Large Language Models,No.,1,"""No evidence""",2023,2023-08-16T16:48:57Z,,,
arXIv2023,Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition,No.,1,"""No evidence""",2023,2023-08-16T13:40:58Z,,,
arXIv2023,Convergence of Two-Layer Regression with Nonlinear Units,No.,1,"""No evidence""",2023,2023-08-16T13:30:45Z,,,
arXIv2023,Visually-Aware Context Modeling for News Image Captioning,No.,1,"""No evidence""",2023,2023-08-16T12:39:39Z,,,
arXIv2023,CMD: a framework for Context-aware Model self-Detoxification,No.,1,"""No evidence""",2023,2023-08-16T11:50:38Z,,,
arXIv2023,Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval,No.,1,"""No evidence""",2023,2023-08-16T11:10:43Z,,,
arXIv2023,TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series,No.,1,"""No evidence""",2023,2023-08-16T09:16:02Z,,,
arXIv2023,MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation,No.,1,"""No evidence""",2023,2023-08-16T09:15:18Z,,,
arXIv2023,Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT,No.,1,"""No evidence""",2023,2023-08-16T07:44:06Z,,,
arXIv2023,PEvoLM: Protein Sequence Evolutionary Information Language Model,No.,1,"""No evidence""",2023,2023-08-16T06:46:28Z,,,
arXIv2023,AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,No.,1,"""No evidence""",2023,2023-08-16T05:57:52Z,,,
arXIv2023,ChatLogo: A Large Language Model-Driven Hybrid Natural-Programming Language Interface for Agent-based Modeling and Programming,No.,1,"""No evidence""",2023,2023-08-16T02:21:52Z,,,
arXIv2023,Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation,No.,1,"""No evidence""",2023,2023-08-16T01:46:01Z,,,
arXIv2023,"DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory",No.,1,"""No evidence""",2023,2023-08-16T01:43:41Z,,,
arXIv2023,Pro-Cap: Leveraging a Frozen Vision-Language Model for Hateful Meme Detection,No.,1,"""No evidence""",2023,2023-08-16T01:38:49Z,,,
arXIv2023,Detection of ChatGPT Fake Science with the xFakeSci Learning Algorithm,No.,1,"""No evidence""",2023,2023-08-15T23:22:37Z,,,
arXIv2023,"The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models",No.,1,"""No evidence""",2023,2023-08-15T22:26:58Z,,,
arXIv2023,Domain Adaptation for Deep Unit Test Case Generation,No.,1,"""No evidence""",2023,2023-08-15T20:48:50Z,,,
arXIv2023,Large Language Models in Introductory Programming Education: ChatGPT's Performance and Implications for Assessments,No.,1,"""No evidence""",2023,2023-08-15T19:48:31Z,,,
arXIv2023,$A^2$Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models,No.,1,"""No evidence""",2023,2023-08-15T19:01:19Z,,,
arXIv2023,MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction,No.,1,"""No evidence""",2023,2023-08-15T18:18:34Z,,,
arXIv2023,Teach LLMs to Personalize -- An Approach inspired by Writing Education,No.,1,"""No evidence""",2023,2023-08-15T18:06:23Z,,,
arXIv2023,Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification,No.,1,"""No evidence""",2023,2023-08-15T17:58:45Z,,,
arXIv2023,A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision,No.,1,"""No evidence""",2023,2023-08-15T17:39:52Z,,,
arXIv2023,A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data,No.,1,"""No evidence""",2023,2023-08-15T17:20:05Z,,,
arXIv2023,"Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT",No.,1,"""No evidence""",2023,2023-08-15T16:41:53Z,,,
arXIv2023,DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion,No.,1,"""No evidence""",2023,2023-08-15T14:07:59Z,,,
arXIv2023,Forward-Backward Reasoning in Large Language Models for Mathematical Verification,No.,1,"""No evidence""",2023,2023-08-15T13:19:59Z,,,
arXIv2023,Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model,No.,1,"""No evidence""",2023,2023-08-15T13:00:42Z,,,
arXIv2023,SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search,No.,1,"""No evidence""",2023,2023-08-15T11:45:34Z,,,
arXIv2023,Better Zero-Shot Reasoning with Role-Play Prompting,No.,1,"""No evidence""",2023,2023-08-15T11:08:30Z,,,
arXIv2023,Gradient-Based Post-Training Quantization: Challenging the Status Quo,No.,1,"""No evidence""",2023,2023-08-15T09:25:11Z,,,
arXIv2023,From Commit Message Generation to History-Aware Commit Message Completion,No.,1,"""No evidence""",2023,2023-08-15T09:10:49Z,,,
arXIv2023,Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping,No.,1,"""No evidence""",2023,2023-08-15T08:46:17Z,,,
arXIv2023,Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies,No.,1,"""No evidence""",2023,2023-08-15T07:40:21Z,,,
arXIv2023,A User-Centered Evaluation of Spanish Text Simplification,No.,1,"""No evidence""",2023,2023-08-15T03:49:59Z,,,
arXIv2023,CALYPSO: LLMs as Dungeon Masters' Assistants,No.,1,"""No evidence""",2023,2023-08-15T02:57:00Z,,,
arXIv2023,Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models,No.,1,"""No evidence""",2023,2023-08-15T01:25:34Z,,,
arXIv2023,Data Race Detection Using Large Language Models,No.,1,"""No evidence""",2023,2023-08-15T00:08:43Z,,,
arXIv2023,Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans,No.,1,"""No evidence""",2023,2023-08-14T21:19:44Z,,,
arXIv2023,Semantic Similarity Loss for Neural Source Code Summarization,No.,1,"""No evidence""",2023,2023-08-14T19:51:45Z,,,
arXIv2023,Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering,No.,1,"""No evidence""",2023,2023-08-14T18:58:00Z,,,
arXIv2023,Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders,No.,1,"""No evidence""",2023,2023-08-14T18:52:03Z,,,
arXIv2023,Text Injection for Capitalization and Turn-Taking Prediction in Speech Models,No.,1,"""No evidence""",2023,2023-08-14T18:28:04Z,,,
arXIv2023,"Platypus: Quick, Cheap, and Powerful Refinement of LLMs",No.,1,"""No evidence""",2023,2023-08-14T17:59:56Z,,,
arXIv2023,Neural Authorship Attribution: Stylometric Analysis on Large Language Models,No.,1,"""No evidence""",2023,2023-08-14T17:46:52Z,,,
arXIv2023,The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation,No.,1,"""No evidence""",2023,2023-08-14T17:17:21Z,,,
arXIv2023,Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Generation for Few-shot Learning,No.,1,"""No evidence""",2023,2023-08-14T16:58:50Z,,,
arXIv2023,EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models,No.,1,"""No evidence""",2023,2023-08-14T16:52:42Z,,,
arXIv2023,AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes,No.,1,"""No evidence""",2023,2023-08-14T15:47:25Z,,,
arXIv2023,Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data,No.,1,"""No evidence""",2023,2023-08-14T15:40:33Z,,,
arXIv2023,OctoPack: Instruction Tuning Code Large Language Models,No.,1,"""No evidence""",2023,2023-08-14T13:53:54Z,,,
arXIv2023,Pairing interacting protein sequences using masked language modeling,No.,1,"""No evidence""",2023,2023-08-14T13:42:09Z,,,
arXIv2023,Language is All a Graph Needs,No.,1,"""No evidence""",2023,2023-08-14T13:41:09Z,,,
arXIv2023,Large Language Models for Information Retrieval: A Survey,No.,1,"""No evidence""",2023,2023-08-14T12:47:22Z,,,
arXIv2023,#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models,No.,1,"""No evidence""",2023,2023-08-14T11:16:28Z,,,
arXIv2023,EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce,No.,1,"""No evidence""",2023,2023-08-14T06:49:53Z,,,
arXIv2023,Approximating Human-Like Few-shot Learning with GPT-based Compression,No.,1,"""No evidence""",2023,2023-08-14T05:22:33Z,,,
arXIv2023,ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots,No.,1,"""No evidence""",2023,2023-08-14T03:43:57Z,,,
arXIv2023,"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",No.,1,"""No evidence""",2023,2023-08-14T03:12:29Z,,,
arXIv2023,SpeechX: Neural Codec Language Model as a Versatile Speech Transformer,No.,1,"""No evidence""",2023,2023-08-14T01:01:19Z,,,
arXIv2023,Improving Face Recognition from Caption Supervision with Multi-Granular Contextual Feature Aggregation,No.,1,"""No evidence""",2023,2023-08-13T23:52:15Z,,,
arXIv2023,"Building Trust in Conversational AI: A Comprehensive Review and Solution Architecture for Explainable, Privacy-Aware Systems using LLMs and Knowledge Graph",No.,1,"""No evidence""",2023,2023-08-13T22:47:51Z,,,
arXIv2023,"An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM",No.,1,"""No evidence""",2023,2023-08-13T18:14:10Z,,,
arXIv2023,Ground Manipulator Primitive Tasks to Executable Actions using Large Language Models,No.,1,"""No evidence""",2023,2023-08-13T16:52:36Z,,,
arXIv2023,"A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations",No.,1,"""No evidence""",2023,2023-08-13T13:34:04Z,,,
arXIv2023,Transforming Sentiment Analysis in the Financial Domain with ChatGPT,No.,1,"""No evidence""",2023,2023-08-13T09:20:47Z,,,
arXIv2023,VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use,No.,1,"""No evidence""",2023,2023-08-12T15:27:51Z,,,
arXIv2023,MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction,No.,1,"""No evidence""",2023,2023-08-12T12:38:10Z,,,
arXIv2023,AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models,No.,1,"""No evidence""",2023,2023-08-12T08:52:40Z,,,
arXIv2023,Three Ways of Using Large Language Models to Evaluate Chat,No.,1,"""No evidence""",2023,2023-08-12T08:34:15Z,,,
arXIv2023,NewsDialogues: Towards Proactive News Grounded Conversation,No.,1,"""No evidence""",2023,2023-08-12T08:33:42Z,,,
arXIv2023,Generating Faithful Text From a Knowledge Graph with Noisy Reference Text,No.,1,"""No evidence""",2023,2023-08-12T07:12:45Z,,,
arXIv2023,Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation,No.,1,"""No evidence""",2023,2023-08-12T03:30:49Z,,,
arXIv2023,ZYN: Zero-Shot Reward Models with Yes-No Questions for RLAIF,No.,1,"""No evidence""",2023,2023-08-11T20:59:31Z,,,
arXIv2023,Large Language Models and Knowledge Graphs: Opportunities and Challenges,No.,1,"""No evidence""",2023,2023-08-11T20:16:57Z,,,
arXIv2023,Large Language Models to Identify Social Determinants of Health in Electronic Health Records,No.,1,"""No evidence""",2023,2023-08-11T19:18:35Z,,,
arXIv2023,Enhancing Network Management Using Code Generated by Large Language Models,No.,1,"""No evidence""",2023,2023-08-11T17:49:15Z,,,
arXIv2023,ChatGPT-based Investment Portfolio Selection,No.,1,"""No evidence""",2023,2023-08-11T17:48:17Z,,,
arXIv2023,Self-Alignment with Instruction Backtranslation,No.,1,"""No evidence""",2023,2023-08-11T17:47:54Z,,,
arXIv2023,A Large Language Model Enhanced Conversational Recommender System,No.,1,"""No evidence""",2023,2023-08-11T16:30:44Z,,,
arXIv2023,Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals,No.,1,"""No evidence""",2023,2023-08-11T16:13:04Z,,,
arXIv2023,Task Conditioned BERT for Joint Intent Detection and Slot-filling,No.,1,"""No evidence""",2023,2023-08-11T14:47:27Z,,,
arXIv2023,Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models,No.,1,"""No evidence""",2023,2023-08-11T14:06:41Z,,,
arXIv2023,Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models,No.,1,"""No evidence""",2023,2023-08-11T12:55:09Z,,,
arXIv2023,Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling,No.,1,"""No evidence""",2023,2023-08-11T11:29:51Z,,,
arXIv2023,Learning to Guide Human Experts via Personalized Large Language Models,No.,1,"""No evidence""",2023,2023-08-11T09:36:33Z,,,
arXIv2023,Large Language Models in Cryptocurrency Securities Cases: Can a GPT Model Meaningfully Assist Lawyers?,No.,1,"""No evidence""",2023,2023-08-11T09:23:11Z,,,
arXIv2023,Inappropriate Benefits and Identification of ChatGPT Misuse in Programming Tests: A Controlled Experiment,No.,1,"""No evidence""",2023,2023-08-11T06:42:29Z,,,
arXIv2023,Decentralised Governance-Driven Architecture for Designing Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI,No.,1,"""No evidence""",2023,2023-08-11T06:41:47Z,,,
arXIv2023,BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents,No.,1,"""No evidence""",2023,2023-08-11T06:37:54Z,,,
arXIv2023,LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts,No.,1,"""No evidence""",2023,2023-08-11T04:36:26Z,,,
arXIv2023,Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT,No.,1,"""No evidence""",2023,2023-08-11T03:40:22Z,,,
arXIv2023,Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception,No.,1,"""No evidence""",2023,2023-08-10T18:43:44Z,,,
arXIv2023,AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining,No.,1,"""No evidence""",2023,2023-08-10T17:55:13Z,,,
arXIv2023,A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment,No.,1,"""No evidence""",2023,2023-08-10T16:58:51Z,,,
arXIv2023,You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content,No.,1,"""No evidence""",2023,2023-08-10T14:14:13Z,,,
arXIv2023,Bringing order into the realm of Transformer-based language models for artificial intelligence and law,No.,1,"""No evidence""",2023,2023-08-10T11:14:22Z,,,
arXIv2023,LLM As DBA,No.,1,"""No evidence""",2023,2023-08-10T10:12:43Z,,,
arXIv2023,Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis,No.,1,"""No evidence""",2023,2023-08-10T10:07:00Z,,,
arXIv2023,Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection,No.,1,"""No evidence""",2023,2023-08-10T08:39:59Z,,,
arXIv2023,Enhancing Trust in LLM-Based AI Automation Agents: New Considerations and Future Challenges,No.,1,"""No evidence""",2023,2023-08-10T07:12:11Z,,,
arXIv2023,Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment,No.,1,"""No evidence""",2023,2023-08-10T06:43:44Z,,,
arXIv2023,"WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine",No.,1,"""No evidence""",2023,2023-08-10T06:08:20Z,,,
arXIv2023,RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model,No.,1,"""No evidence""",2023,2023-08-10T05:24:41Z,,,
arXIv2023,Metacognitive Prompting Improves Understanding in Large Language Models,No.,1,"""No evidence""",2023,2023-08-10T05:10:17Z,,,
arXIv2023,Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT,No.,1,"""No evidence""",2023,2023-08-10T05:09:42Z,,,
arXIv2023,Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season,No.,1,"""No evidence""",2023,2023-08-10T01:51:33Z,,,
arXIv2023,Decoding Layer Saliency in Language Transformers,No.,1,"""No evidence""",2023,2023-08-09T20:53:22Z,,,
arXIv2023,Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio,No.,1,"""No evidence""",2023,2023-08-09T20:51:54Z,,,
arXIv2023,"""Generate"" the Future of Work through AI: Empirical Evidence from Online Labor Markets",No.,1,"""No evidence""",2023,2023-08-09T19:45:00Z,,,
arXIv2023,LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation,No.,1,"""No evidence""",2023,2023-08-09T17:45:04Z,,,
arXIv2023,Fine-Tune Language Models as Multi-Modal Differential Equation Solvers,No.,1,"""No evidence""",2023,2023-08-09T16:44:25Z,,,
arXIv2023,An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures,No.,1,"""No evidence""",2023,2023-08-09T15:35:14Z,,,
arXIv2023,MetRoBERTa: Leveraging Traditional Customer Relationship Management Data to Develop a Transit-Topic-Aware Language Model,No.,1,"""No evidence""",2023,2023-08-09T15:11:37Z,,,
arXIv2023,"Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection",No.,1,"""No evidence""",2023,2023-08-09T13:33:27Z,,,
arXIv2023,Extrapolating Large Language Models to Non-English by Aligning Languages,No.,1,"""No evidence""",2023,2023-08-09T13:32:06Z,,,
arXIv2023,LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking,No.,1,"""No evidence""",2023,2023-08-09T13:22:37Z,,,
arXIv2023,Integrating large language models and active inference to understand eye movements in reading and dyslexia,No.,1,"""No evidence""",2023,2023-08-09T13:16:30Z,,,
arXIv2023,Emotion-Conditioned Text Generation through Automatic Prompt Optimization,No.,1,"""No evidence""",2023,2023-08-09T10:42:38Z,,,
arXIv2023,On the Unexpected Abilities of Large Language Models,No.,1,"""No evidence""",2023,2023-08-09T09:15:07Z,,,
arXIv2023,ADMUS: A Progressive Question Answering Framework Adaptable to Multiple Knowledge Sources,No.,1,"""No evidence""",2023,2023-08-09T08:46:39Z,,,
arXIv2023,Fuzz4All: Universal Fuzzing with Large Language Models,No.,1,"""No evidence""",2023,2023-08-09T07:36:21Z,,,
arXIv2023,Case Study: Using AI-Assisted Code Generation In Mobile Teams,No.,1,"""No evidence""",2023,2023-08-09T07:02:39Z,,,
arXIv2023,TextPainter: Multimodal Text Image Generation with Visual-harmony and Text-comprehension for Poster Design,No.,1,"""No evidence""",2023,2023-08-09T06:59:29Z,,,
arXIv2023,Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning,No.,1,"""No evidence""",2023,2023-08-09T05:08:57Z,,,
arXIv2023,Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval,No.,1,"""No evidence""",2023,2023-08-09T05:06:39Z,,,
arXIv2023,Benchmarking LLM powered Chatbots: Methods and Metrics,No.,1,"""No evidence""",2023,2023-08-08T23:30:20Z,,,
arXIv2023,Accelerating LLM Inference with Staged Speculative Decoding,No.,1,"""No evidence""",2023,2023-08-08T23:29:55Z,,,
arXIv2023,Shepherd: A Critic for Language Model Generation,No.,1,"""No evidence""",2023,2023-08-08T21:23:23Z,,,
arXIv2023,Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs,No.,1,"""No evidence""",2023,2023-08-08T21:14:21Z,,,
arXIv2023,Ahead of the Text: Leveraging Entity Preposition for Financial Relation Extraction,No.,1,"""No evidence""",2023,2023-08-08T18:56:52Z,,,
arXIv2023,SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore,No.,1,"""No evidence""",2023,2023-08-08T17:58:15Z,,,
arXIv2023,Learning Evaluation Models from Large Language Models for Sequence Generation,No.,1,"""No evidence""",2023,2023-08-08T16:41:16Z,,,
arXIv2023,Cumulative Reasoning with Large Language Models,No.,1,"""No evidence""",2023,2023-08-08T16:18:20Z,,,
arXIv2023,3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment,No.,1,"""No evidence""",2023,2023-08-08T15:59:17Z,,,
arXIv2023,In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning,No.,1,"""No evidence""",2023,2023-08-08T14:17:17Z,,,
arXIv2023,Assistive Chatbots for healthcare: a succinct review,No.,1,"""No evidence""",2023,2023-08-08T10:35:25Z,,,
arXIv2023,On Monotonic Aggregation for Open-domain QA,No.,1,"""No evidence""",2023,2023-08-08T10:23:04Z,,,
arXIv2023,Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions,No.,1,"""No evidence""",2023,2023-08-08T09:32:43Z,,,
arXIv2023,Large Language Model Prompt Chaining for Long Legal Document Classification,No.,1,"""No evidence""",2023,2023-08-08T08:57:01Z,,,
arXIv2023,OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation,No.,1,"""No evidence""",2023,2023-08-08T08:30:16Z,,,
arXIv2023,PTransIPs: Identification of phosphorylation sites enhanced by protein PLM embeddings,No.,1,"""No evidence""",2023,2023-08-08T07:50:38Z,,,
arXIv2023,I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection,No.,1,"""No evidence""",2023,2023-08-08T07:47:10Z,,,
arXIv2023,DataTales: Investigating the use of Large Language Models for Authoring Data-Driven Articles,No.,1,"""No evidence""",2023,2023-08-08T06:21:58Z,,,
arXIv2023,Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications,No.,1,"""No evidence""",2023,2023-08-08T04:21:14Z,,,
arXIv2023,Gentopia: A Collaborative Platform for Tool-Augmented LLMs,No.,1,"""No evidence""",2023,2023-08-08T04:12:29Z,,,
arXIv2023,Continual Pre-Training of Large Language Models: How to (re)warm your model?,No.,1,"""No evidence""",2023,2023-08-08T03:18:18Z,,,
arXIv2023,Few-shot medical image classification with simple shape and texture text descriptors using vision-language models,No.,1,"""No evidence""",2023,2023-08-08T02:48:46Z,,,
arXIv2023,"NEOLAF, an LLM-powered neural-symbolic cognitive architecture",No.,1,"""No evidence""",2023,2023-08-08T02:13:04Z,,,
arXIv2023,SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool,No.,1,"""No evidence""",2023,2023-08-08T02:00:43Z,,,
arXIv2023,Simple synthetic data reduces sycophancy in large language models,No.,1,"""No evidence""",2023,2023-08-07T23:48:36Z,,,
arXIv2023,Fact-Checking Generative AI: Ontology-Driven Biological Graphs for Disease-Gene Link Verification,No.,1,"""No evidence""",2023,2023-08-07T22:13:30Z,,,
arXIv2023,Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts,No.,1,"""No evidence""",2023,2023-08-07T21:54:58Z,,,
arXIv2023,"ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition",No.,1,"""No evidence""",2023,2023-08-07T20:50:54Z,,,
arXIv2023,A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction,No.,1,"""No evidence""",2023,2023-08-07T19:50:59Z,,,
arXIv2023,Generative Benchmark Creation for Table Union Search,No.,1,"""No evidence""",2023,2023-08-07T19:26:09Z,,,
arXIv2023,Evaluating and Explaining Large Language Models for Code Using Syntactic Structures,No.,1,"""No evidence""",2023,2023-08-07T18:50:57Z,,,
arXIv2023,Trusting Language Models in Education,No.,1,"""No evidence""",2023,2023-08-07T18:27:54Z,,,
arXIv2023,AI Text-to-Behavior: A Study In Steerability,No.,1,"""No evidence""",2023,2023-08-07T18:14:24Z,,,
arXIv2023,CORAL: Expert-Curated medical Oncology Reports to Advance Language Model Inference,No.,1,"""No evidence""",2023,2023-08-07T18:03:10Z,,,
arXIv2023,Training BERT Models to Carry Over a Coding System Developed on One Corpus to Another,No.,1,"""No evidence""",2023,2023-08-07T17:46:49Z,,,
arXIv2023,A Cost Analysis of Generative Language Models and Influence Operations,No.,1,"""No evidence""",2023,2023-08-07T17:38:41Z,,,
arXIv2023,Tiny LVLM-eHub: Early Multimodal Experiments with Bard,No.,1,"""No evidence""",2023,2023-08-07T17:17:05Z,,,
arXIv2023,Detecting Spells in Fantasy Literature with a Transformer Based Artificial Intelligence,No.,1,"""No evidence""",2023,2023-08-07T15:20:20Z,,,
arXIv2023,Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench,No.,1,"""No evidence""",2023,2023-08-07T15:18:30Z,,,
arXIv2023,KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering,No.,1,"""No evidence""",2023,2023-08-07T14:42:49Z,,,
arXIv2023,Segmentation Framework for Heat Loss Identification in Thermal Images: Empowering Scottish Retrofitting and Thermographic Survey Companies,No.,1,"""No evidence""",2023,2023-08-07T14:36:49Z,,,
arXIv2023,Why We Don't Have AGI Yet,No.,1,"""No evidence""",2023,2023-08-07T13:59:31Z,,,
arXIv2023,Topological Interpretations of GPT-3,No.,1,"""No evidence""",2023,2023-08-07T13:16:42Z,,,
arXIv2023,Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue,No.,1,"""No evidence""",2023,2023-08-07T12:56:13Z,,,
arXIv2023,Exploring ChatGPT's Empathic Abilities,No.,1,"""No evidence""",2023,2023-08-07T12:23:07Z,,,
arXIv2023,TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage,No.,1,"""No evidence""",2023,2023-08-07T09:22:03Z,,,
arXIv2023,RecycleGPT: An Autoregressive Language Model with Recyclable Module,No.,1,"""No evidence""",2023,2023-08-07T09:14:33Z,,,
arXIv2023,SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs,No.,1,"""No evidence""",2023,2023-08-07T07:03:49Z,,,
arXIv2023,Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM,No.,1,"""No evidence""",2023,2023-08-07T06:29:20Z,,,
arXIv2023,Generative AI trial for nonviolent communication mediation,No.,1,"""No evidence""",2023,2023-08-07T06:19:29Z,,,
arXIv2023,What has ChatGPT read? The origins of archaeological citations used by a generative artificial intelligence application,No.,1,"""No evidence""",2023,2023-08-07T05:06:35Z,,,
arXIv2023,SynJax: Structured Probability Distributions for JAX,No.,1,"""No evidence""",2023,2023-08-07T04:20:38Z,,,
arXIv2023,Towards General Text Embeddings with Multi-stage Contrastive Learning,No.,1,"""No evidence""",2023,2023-08-07T03:52:59Z,,,
arXIv2023,UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition,No.,1,"""No evidence""",2023,2023-08-07T03:39:52Z,,,
arXIv2023,Automated Distractor and Feedback Generation for Math Multiple-choice Questions via In-context Learning,No.,1,"""No evidence""",2023,2023-08-07T01:03:04Z,,,
arXIv2023,Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies,No.,1,"""No evidence""",2023,2023-08-06T18:38:52Z,,,
arXIv2023,Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation,No.,1,"""No evidence""",2023,2023-08-06T14:49:26Z,,,
arXIv2023,"""Kurosawa"": A Script Writer's Assistant",No.,1,"""No evidence""",2023,2023-08-06T14:09:02Z,,,
arXIv2023,PromptSum: Parameter-Efficient Controllable Abstractive Summarization,No.,1,"""No evidence""",2023,2023-08-06T13:54:14Z,,,
arXIv2023,Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data,No.,1,"""No evidence""",2023,2023-08-06T13:18:38Z,,,
arXIv2023,Pre-Trained Large Language Models for Industrial Control,No.,1,"""No evidence""",2023,2023-08-06T06:01:18Z,,,
arXIv2023,SAPIEN: Affective Virtual Agents Powered by Large Language Models,No.,1,"""No evidence""",2023,2023-08-06T05:13:16Z,,,
arXIv2023,Spanish Pre-trained BERT Model and Evaluation Data,No.,1,"""No evidence""",2023,2023-08-06T00:16:04Z,,,
arXIv2023,"A criterion for Artificial General Intelligence: hypothetic-deductive reasoning, tested on ChatGPT",No.,1,"""No evidence""",2023,2023-08-05T20:33:13Z,,,
arXIv2023,EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education,No.,1,"""No evidence""",2023,2023-08-05T02:55:35Z,,,
arXIv2023,DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation,No.,1,"""No evidence""",2023,2023-08-05T00:14:49Z,,,
arXIv2023,Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification,No.,1,"""No evidence""",2023,2023-08-04T23:50:58Z,,,
arXIv2023,How Good Are SOTA Fake News Detectors,No.,1,"""No evidence""",2023,2023-08-04T22:14:19Z,,,
arXIv2023,ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation,No.,1,"""No evidence""",2023,2023-08-04T18:11:40Z,,,
arXIv2023,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,No.,1,"""No evidence""",2023,2023-08-04T17:59:47Z,,,
arXIv2023,Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&3D Medical Data,No.,1,"""No evidence""",2023,2023-08-04T17:00:38Z,,,
arXIv2023,Legal Summarisation through LLMs: The PRODIGIT Project,No.,1,"""No evidence""",2023,2023-08-04T16:59:48Z,,,
arXIv2023,ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval,No.,1,"""No evidence""",2023,2023-08-04T14:50:37Z,,,
arXIv2023,Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text,No.,1,"""No evidence""",2023,2023-08-04T14:47:15Z,,,
arXIv2023,Is Stack Overflow Obsolete? An Empirical Study of the Characteristics of ChatGPT Answers to Stack Overflow Questions,No.,1,"""No evidence""",2023,2023-08-04T13:23:20Z,,,
arXIv2023,Learning to Paraphrase Sentences to Different Complexity Levels,No.,1,"""No evidence""",2023,2023-08-04T09:43:37Z,,,
arXIv2023,ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation,No.,1,"""No evidence""",2023,2023-08-04T09:35:45Z,,,
arXIv2023,GEMRec: Towards Generative Model Recommendation,No.,1,"""No evidence""",2023,2023-08-04T08:45:02Z,,,
arXIv2023,Explaining Relation Classification Models with Semantic Extents,No.,1,"""No evidence""",2023,2023-08-04T08:17:52Z,,,
arXIv2023,Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization,No.,1,"""No evidence""",2023,2023-08-04T06:14:23Z,,,
arXIv2023,ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP,No.,1,"""No evidence""",2023,2023-08-04T03:48:28Z,,,
arXIv2023,Multimodal machine learning for materials science: composition-structure bimodal learning for experimentally measured properties,No.,1,"""No evidence""",2023,2023-08-04T02:04:52Z,,,
arXIv2023,Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale,No.,1,"""No evidence""",2023,2023-08-03T22:42:30Z,,,
arXIv2023,Accurate Neural Network Pruning Requires Rethinking Sparse Optimization,No.,1,"""No evidence""",2023,2023-08-03T21:49:14Z,,,
arXIv2023,Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty,No.,1,"""No evidence""",2023,2023-08-03T20:20:01Z,,,
arXIv2023,Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces,No.,1,"""No evidence""",2023,2023-08-03T18:11:00Z,,,
arXIv2023,PARL: A Unified Framework for Policy Alignment in Reinforcement Learning,No.,1,"""No evidence""",2023,2023-08-03T18:03:44Z,,,
arXIv2023,Reasoning in Large Language Models Through Symbolic Math Word Problems,No.,1,"""No evidence""",2023,2023-08-03T17:59:27Z,,,
arXIv2023,How many preprints have actually been printed and why: a case study of computer science preprints on arXiv,No.,1,"""No evidence""",2023,2023-08-03T17:56:16Z,,,
arXIv2023,ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints,No.,1,"""No evidence""",2023,2023-08-03T17:04:41Z,,,
arXIv2023,Wider and Deeper LLM Networks are Fairer LLM Evaluators,No.,1,"""No evidence""",2023,2023-08-03T16:38:34Z,,,
arXIv2023,XNLP: An Interactive Demonstration System for Universal Structured NLP,No.,1,"""No evidence""",2023,2023-08-03T16:13:05Z,,,
arXIv2023,The Capability of Large Language Models to Measure Psychiatric Functioning,No.,1,"""No evidence""",2023,2023-08-03T15:52:27Z,,,
arXIv2023,RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension,No.,1,"""No evidence""",2023,2023-08-03T14:17:22Z,,,
arXIv2023,Supply chain emission estimation using large language models,No.,1,"""No evidence""",2023,2023-08-03T13:06:37Z,,,
arXIv2023,Ambient Adventures: Teaching ChatGPT on Developing Complex Stories,No.,1,"""No evidence""",2023,2023-08-03T12:52:49Z,,,
arXIv2023,Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings,No.,1,"""No evidence""",2023,2023-08-03T12:47:17Z,,,
arXIv2023,Local Large Language Models for Complex Structured Medical Tasks,No.,1,"""No evidence""",2023,2023-08-03T12:36:13Z,,,
arXIv2023,Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models,No.,1,"""No evidence""",2023,2023-08-03T10:52:52Z,,,
arXIv2023,Holy Grail 2.0: From Natural Language to Constraint Models,No.,1,"""No evidence""",2023,2023-08-03T07:48:02Z,,,
arXIv2023,InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent,No.,1,"""No evidence""",2023,2023-08-03T06:19:58Z,,,
arXIv2023,Comparing scalable strategies for generating numerical perspectives,No.,1,"""No evidence""",2023,2023-08-03T04:35:46Z,,,
arXIv2023,Food Classification using Joint Representation of Visual and Textual Data,No.,1,"""No evidence""",2023,2023-08-03T04:03:46Z,,,
arXIv2023,Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors,No.,1,"""No evidence""",2023,2023-08-03T01:46:27Z,,,
arXIv2023,Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?,No.,1,"""No evidence""",2023,2023-08-02T21:13:38Z,,,
arXIv2023,UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text using Transformer Ensembles,No.,1,"""No evidence""",2023,2023-08-02T20:08:59Z,,,
arXIv2023,"Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction",No.,1,"""No evidence""",2023,2023-08-02T20:01:38Z,,,
arXIv2023,The Paradigm Shifts in Artificial Intelligence,No.,1,"""No evidence""",2023,2023-08-02T19:38:24Z,,,
arXIv2023,Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability,No.,1,"""No evidence""",2023,2023-08-02T19:11:04Z,,,
arXIv2023,OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,No.,1,"""No evidence""",2023,2023-08-02T19:10:23Z,,,
arXIv2023,"DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales",No.,1,"""No evidence""",2023,2023-08-02T18:49:57Z,,,
arXIv2023,ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders,No.,1,"""No evidence""",2023,2023-08-02T17:59:45Z,,,
arXIv2023,Flows: Building Blocks of Reasoning and Collaborating AI,No.,1,"""No evidence""",2023,2023-08-02T17:14:22Z,,,
arXIv2023,Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?,No.,1,"""No evidence""",2023,2023-08-02T17:11:37Z,,,
arXIv2023,Exploring the psychology of LLMs' Moral and Legal Reasoning,No.,1,"""No evidence""",2023,2023-08-02T16:36:58Z,,,
arXIv2023,Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation,No.,1,"""No evidence""",2023,2023-08-02T15:54:22Z,,,
arXIv2023,Do Multilingual Language Models Think Better in English?,No.,1,"""No evidence""",2023,2023-08-02T15:29:22Z,,,
arXIv2023,Calibration in Deep Learning: A Survey of the State-of-the-Art,No.,1,"""No evidence""",2023,2023-08-02T15:28:10Z,,,
arXIv2023,"LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs",No.,1,"""No evidence""",2023,2023-08-02T13:59:35Z,,,
arXIv2023,Arithmetic with Language Models: from Memorization to Computation,No.,1,"""No evidence""",2023,2023-08-02T13:58:37Z,,,
arXIv2023,Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search,No.,1,"""No evidence""",2023,2023-08-02T12:05:01Z,,,
arXIv2023,Leveraging Few-Shot Data Augmentation and Waterfall Prompting for Response Generation,No.,1,"""No evidence""",2023,2023-08-02T11:04:27Z,,,
arXIv2023,A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards,No.,1,"""No evidence""",2023,2023-08-02T10:51:36Z,,,
arXIv2023,TS-RGBD Dataset: a Novel Dataset for Theatre Scenes Description for People with Visual Impairments,No.,1,"""No evidence""",2023,2023-08-02T09:28:35Z,,,
arXIv2023,Knowledge-aware Collaborative Filtering with Pre-trained Language Model for Personalized Review-based Rating Prediction,No.,1,"""No evidence""",2023,2023-08-02T07:28:08Z,,,
arXIv2023,Teaching Smaller Language Models To Generalise To Unseen Compositional Questions,No.,1,"""No evidence""",2023,2023-08-02T05:00:12Z,,,
arXIv2023,Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval,No.,1,"""No evidence""",2023,2023-08-01T18:01:21Z,,,
arXIv2023,LISA: Reasoning Segmentation via Large Language Model,No.,1,"""No evidence""",2023,2023-08-01T17:50:17Z,,,
arXIv2023,CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code,No.,1,"""No evidence""",2023,2023-08-01T17:40:48Z,,,
arXIv2023,Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models,No.,1,"""No evidence""",2023,2023-08-01T17:21:38Z,,,
arXIv2023,JIANG: Chinese Open Foundation Language Model,No.,1,"""No evidence""",2023,2023-08-01T15:51:41Z,,,
arXIv2023,Detecting Cloud Presence in Satellite Images Using the RGB-based CLIP Vision-Language Model,No.,1,"""No evidence""",2023,2023-08-01T13:36:46Z,,,
arXIv2023,Structural Embeddings of Tools for Large Language Models,No.,1,"""No evidence""",2023,2023-08-01T10:46:09Z,,,
arXIv2023,Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions,No.,1,"""No evidence""",2023,2023-08-01T10:10:59Z,,,
arXIv2023,qgym: A Gym for Training and Benchmarking RL-Based Quantum Compilation,No.,1,"""No evidence""",2023,2023-08-01T10:07:20Z,,,
arXIv2023,MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework,No.,1,"""No evidence""",2023,2023-08-01T07:49:10Z,,,
arXIv2023,Pixel to policy: DQN Encoders for within & cross-game reinforcement learning,No.,1,"""No evidence""",2023,2023-08-01T06:29:33Z,,,
arXIv2023,Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models,No.,1,"""No evidence""",2023,2023-08-01T05:54:12Z,,,
arXIv2023,"Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation",No.,1,"""No evidence""",2023,2023-08-01T02:43:27Z,,,
arXIv2023,ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks,No.,1,"""No evidence""",2023,2023-08-01T02:08:13Z,,,
arXIv2023,Experiments on Generative AI-Powered Parametric Modeling and BIM for Architectural Design,No.,1,"""No evidence""",2023,2023-08-01T01:51:59Z,,,
arXIv2023,Advancing Beyond Identification: Multi-bit Watermark for Large Language Models,No.,1,"""No evidence""",2023,2023-08-01T01:27:40Z,,,
arXIv2023,Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?,No.,1,"""No evidence""",2023,2023-07-31T22:58:41Z,,,
arXIv2023,Predictive Data Analytics with AI: assessing the need for post-editing of MT output by fine-tuning OpenAI LLMs,No.,1,"""No evidence""",2023,2023-07-31T21:13:30Z,,,
arXIv2023,DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms,No.,1,"""No evidence""",2023,2023-07-31T19:46:49Z,,,
arXIv2023,Towards Semantically Enriched Embeddings for Knowledge Graph Completion,No.,1,"""No evidence""",2023,2023-07-31T18:53:47Z,,,
arXIv2023,Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection,No.,1,"""No evidence""",2023,2023-07-31T17:56:00Z,,,
arXIv2023,Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment,No.,1,"""No evidence""",2023,2023-07-31T16:40:06Z,,,
arXIv2023,Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc,No.,1,"""No evidence""",2023,2023-07-31T16:29:28Z,,,
arXIv2023,ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs,No.,1,"""No evidence""",2023,2023-07-31T15:56:53Z,,,
arXIv2023,Ontology engineering with Large Language Models,No.,1,"""No evidence""",2023,2023-07-31T14:18:23Z,,,
arXIv2023,No that's not what I meant: Handling Third Position Repair in Conversational Question Answering,No.,1,"""No evidence""",2023,2023-07-31T14:02:45Z,,,
arXIv2023,On the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook,No.,1,"""No evidence""",2023,2023-07-31T13:57:05Z,,,
arXIv2023,LLMs4OL: Large Language Models for Ontology Learning,No.,1,"""No evidence""",2023,2023-07-31T13:27:21Z,,,
arXIv2023,Scaling Sentence Embeddings with Large Language Models,No.,1,"""No evidence""",2023,2023-07-31T13:26:03Z,,,
arXIv2023,VacancySBERT: the approach for representation of titles and skills for semantic similarity search in the recruitment domain,No.,1,"""No evidence""",2023,2023-07-31T13:21:15Z,,,
arXIv2023,Perceptions of the Fourth Industrial Revolution and Artificial Intelligence Impact on Society,No.,1,"""No evidence""",2023,2023-07-31T13:16:37Z,,,
arXIv2023,Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks,No.,1,"""No evidence""",2023,2023-07-31T12:35:54Z,,,
arXIv2023,NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?,No.,1,"""No evidence""",2023,2023-07-31T11:53:52Z,,,
arXIv2023,"Classifying multilingual party manifestos: Domain transfer across country, time, and genre",No.,1,"""No evidence""",2023,2023-07-31T09:16:13Z,,,
arXIv2023,BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models,No.,1,"""No evidence""",2023,2023-07-31T08:34:24Z,,,
arXIv2023,FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis,No.,1,"""No evidence""",2023,2023-07-31T07:44:15Z,,,
arXIv2023,Camoscio: an Italian Instruction-tuned LLaMA,No.,1,"""No evidence""",2023,2023-07-31T07:31:48Z,,,
arXIv2023,An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model,No.,1,"""No evidence""",2023,2023-07-31T07:23:11Z,,,
arXIv2023,MovieChat: From Dense Token to Sparse Memory for Long Video Understanding,No.,1,"""No evidence""",2023,2023-07-31T07:15:45Z,,,
arXIv2023,Visual Captioning at Will: Describing Images and Videos Guided by a Few Stylized Sentences,No.,1,"""No evidence""",2023,2023-07-31T04:26:01Z,,,
arXIv2023,Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks,No.,1,"""No evidence""",2023,2023-07-31T03:57:31Z,,,
arXIv2023,Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?,No.,1,"""No evidence""",2023,2023-07-31T03:17:51Z,,,
arXIv2023,When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities,No.,1,"""No evidence""",2023,2023-07-31T02:48:56Z,,,
arXIv2023,AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?,No.,1,"""No evidence""",2023,2023-07-31T02:14:19Z,,,
arXIv2023,Promptly: Using Prompt Problems to Teach Learners How to Effectively Utilize AI Code Generators,No.,1,"""No evidence""",2023,2023-07-31T01:46:42Z,,,
arXIv2023,Anatomy of an AI-powered malicious social botnet,No.,1,"""No evidence""",2023,2023-07-30T23:06:06Z,,,
arXIv2023,Evaluating ChatGPT and GPT-4 for Visual Programming,No.,1,"""No evidence""",2023,2023-07-30T22:13:20Z,,,
arXIv2023,LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning,No.,1,"""No evidence""",2023,2023-07-30T18:47:54Z,,,
arXIv2023,Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs,No.,1,"""No evidence""",2023,2023-07-30T12:49:54Z,,,
arXIv2023,"UnIVAL: Unified Model for Image, Video, Audio and Language Tasks",No.,1,"""No evidence""",2023,2023-07-30T09:48:36Z,,,
arXIv2023,Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models,No.,1,"""No evidence""",2023,2023-07-30T09:34:35Z,,,
arXIv2023,User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination,No.,1,"""No evidence""",2023,2023-07-30T06:06:35Z,,,
arXIv2023,Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System,No.,1,"""No evidence""",2023,2023-07-29T21:37:24Z,,,
arXIv2023,RoCar: A Relationship Network-based Evaluation Method to Large Language Models,No.,1,"""No evidence""",2023,2023-07-29T14:47:07Z,,,
arXIv2023,A Theory for Emergence of Complex Skills in Language Models,No.,1,"""No evidence""",2023,2023-07-29T09:22:54Z,,,
arXIv2023,GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning,No.,1,"""No evidence""",2023,2023-07-29T09:17:16Z,,,
arXIv2023,Holistic Survey of Privacy and Fairness in Machine Learning,No.,1,"""No evidence""",2023,2023-07-28T23:39:29Z,,,
arXIv2023,Dialogue Shaping: Empowering Agents through NPC Interaction,No.,1,"""No evidence""",2023,2023-07-28T22:44:54Z,,,
arXIv2023,Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning,No.,1,"""No evidence""",2023,2023-07-28T19:38:13Z,,,
arXIv2023,The Hydra Effect: Emergent Self-repair in Language Model Computations,No.,1,"""No evidence""",2023,2023-07-28T19:13:26Z,,,
arXIv2023,Robust Distortion-free Watermarks for Language Models,No.,1,"""No evidence""",2023,2023-07-28T14:52:08Z,,,
arXIv2023,Exploring Format Consistency for Instruction Tuning,No.,1,"""No evidence""",2023,2023-07-28T12:00:13Z,,,
arXIv2023,Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding,No.,1,"""No evidence""",2023,2023-07-28T11:20:23Z,,,
arXIv2023,Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking,No.,1,"""No evidence""",2023,2023-07-28T10:45:14Z,,,
arXIv2023,Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning,No.,1,"""No evidence""",2023,2023-07-28T09:03:19Z,,,
arXIv2023,"Multilingual Tourist Assistance using ChatGPT: Comparing Capabilities in Hindi, Telugu, and Kannada",No.,1,"""No evidence""",2023,2023-07-28T07:52:26Z,,,
arXIv2023,Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation,No.,1,"""No evidence""",2023,2023-07-28T06:31:34Z,,,
arXIv2023,Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models,No.,1,"""No evidence""",2023,2023-07-28T06:15:27Z,,,
arXIv2023,Beyond Reality: The Pivotal Role of Generative AI in the Metaverse,No.,1,"""No evidence""",2023,2023-07-28T05:44:20Z,,,
arXIv2023,TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety,No.,1,"""No evidence""",2023,2023-07-28T05:17:11Z,,,
arXIv2023,WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories,No.,1,"""No evidence""",2023,2023-07-28T04:17:41Z,,,
arXIv2023,ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation,No.,1,"""No evidence""",2023,2023-07-28T04:04:43Z,,,
arXIv2023,Multilingual Lexical Simplification via Paraphrase Generation,No.,1,"""No evidence""",2023,2023-07-28T03:47:44Z,,,
arXIv2023,VeriGen: A Large Language Model for Verilog Code Generation,No.,1,"""No evidence""",2023,2023-07-28T02:57:14Z,,,
arXIv2023,RSGPT: A Remote Sensing Vision Language Model and Benchmark,No.,1,"""No evidence""",2023,2023-07-28T02:23:35Z,,,
arXIv2023,Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation,No.,1,"""No evidence""",2023,2023-07-28T01:52:16Z,,,
arXIv2023,VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings,No.,1,"""No evidence""",2023,2023-07-27T19:42:22Z,,,
arXIv2023,Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation,No.,1,"""No evidence""",2023,2023-07-27T17:59:14Z,,,
arXIv2023,A Geometric Notion of Causal Probing,No.,1,"""No evidence""",2023,2023-07-27T17:57:57Z,,,
arXIv2023,A Transformer-based Approach for Arabic Offline Handwritten Text Recognition,No.,1,"""No evidence""",2023,2023-07-27T17:51:52Z,,,
arXIv2023,Universal and Transferable Adversarial Attacks on Aligned Language Models,No.,1,"""No evidence""",2023,2023-07-27T17:49:12Z,,,
arXIv2023,AI Literature Review Suite,No.,1,"""No evidence""",2023,2023-07-27T17:30:31Z,,,
arXIv2023,SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark,No.,1,"""No evidence""",2023,2023-07-27T17:24:09Z,,,
arXIv2023,How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges,No.,1,"""No evidence""",2023,2023-07-27T17:19:32Z,,,
arXIv2023,TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer,No.,1,"""No evidence""",2023,2023-07-27T16:45:33Z,,,
arXIv2023,Multilingual Code Co-Evolution Using Large Language Models,No.,1,"""No evidence""",2023,2023-07-27T16:37:30Z,,,
arXIv2023,Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs,No.,1,"""No evidence""",2023,2023-07-27T16:30:27Z,,,
arXIv2023,PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback,No.,1,"""No evidence""",2023,2023-07-27T15:28:29Z,,,
arXIv2023,Cascaded Cross-Modal Transformer for Request and Complaint Detection,No.,1,"""No evidence""",2023,2023-07-27T13:45:42Z,,,
arXIv2023,ArcGPT: A Large Language Model Tailored for Real-world Archival Applications,No.,1,"""No evidence""",2023,2023-07-27T13:31:45Z,,,
arXIv2023,LLMediator: GPT-4 Assisted Online Dispute Resolution,No.,1,"""No evidence""",2023,2023-07-27T10:25:29Z,,,
arXIv2023,New Interaction Paradigm for Complex EDA Software Leveraging GPT,No.,1,"""No evidence""",2023,2023-07-27T09:53:02Z,,,
arXIv2023,Evaluating Generative Models for Graph-to-Text Generation,No.,1,"""No evidence""",2023,2023-07-27T09:03:05Z,,,
arXIv2023,Metric-Based In-context Learning: A Case Study in Text Simplification,No.,1,"""No evidence""",2023,2023-07-27T05:45:35Z,,,
arXIv2023,TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation,No.,1,"""No evidence""",2023,2023-07-27T03:56:39Z,,,
arXIv2023,Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models,No.,1,"""No evidence""",2023,2023-07-26T23:11:15Z,,,
arXIv2023,CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions,No.,1,"""No evidence""",2023,2023-07-26T21:49:14Z,,,
arXIv2023,"A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing",No.,1,"""No evidence""",2023,2023-07-26T20:58:47Z,,,
arXIv2023,A Sentence is Worth a Thousand Pictures: Can Large Language Models Understand Human Language?,No.,1,"""No evidence""",2023,2023-07-26T18:58:53Z,,,
arXIv2023,Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking,No.,1,"""No evidence""",2023,2023-07-26T18:16:45Z,,,
arXIv2023,Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models,No.,1,"""No evidence""",2023,2023-07-26T18:01:49Z,,,
arXIv2023,Three Bricks to Consolidate Watermarks for Large Language Models,No.,1,"""No evidence""",2023,2023-07-26T17:56:36Z,,,
arXIv2023,Utilizing Large Language Models for Natural Interface to Pharmacology Databases,No.,1,"""No evidence""",2023,2023-07-26T17:50:11Z,,,
arXIv2023,Evaluating the Moral Beliefs Encoded in LLMs,No.,1,"""No evidence""",2023,2023-07-26T17:42:43Z,,,
arXIv2023,Comparative Analysis of Libraries for the Sentimental Analysis,No.,1,"""No evidence""",2023,2023-07-26T17:21:53Z,,,
arXIv2023,ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality,No.,1,"""No evidence""",2023,2023-07-26T16:58:10Z,,,
arXIv2023,Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences,No.,1,"""No evidence""",2023,2023-07-26T14:47:15Z,,,
arXIv2023,AI and Education: An Investigation into the Use of ChatGPT for Systems Thinking,No.,1,"""No evidence""",2023,2023-07-26T14:12:16Z,,,
arXIv2023,"Unveiling Security, Privacy, and Ethical Concerns of ChatGPT",No.,1,"""No evidence""",2023,2023-07-26T13:45:18Z,,,
arXIv2023,Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models,No.,1,"""No evidence""",2023,2023-07-26T12:02:30Z,,,
arXIv2023,"Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges, and Possible Future Directions",No.,1,"""No evidence""",2023,2023-07-26T11:10:04Z,,,
arXIv2023,DPBERT: Efficient Inference for BERT based on Dynamic Planning,No.,1,"""No evidence""",2023,2023-07-26T07:18:50Z,,,
arXIv2023,How User Language Affects Conflict Fatality Estimates in ChatGPT,No.,1,"""No evidence""",2023,2023-07-26T07:07:44Z,,,
arXIv2023,GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning,No.,1,"""No evidence""",2023,2023-07-26T02:45:38Z,,,
arXIv2023,Embedding Democratic Values into Social Media AIs via Societal Objective Functions,No.,1,"""No evidence""",2023,2023-07-26T02:27:24Z,,,
arXIv2023,Data Augmentation for Neural Machine Translation using Generative Language Model,No.,1,"""No evidence""",2023,2023-07-26T02:12:58Z,,,
arXIv2023,FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction,No.,1,"""No evidence""",2023,2023-07-26T01:48:52Z,,,
arXIv2023,WebArena: A Realistic Web Environment for Building Autonomous Agents,No.,1,"""No evidence""",2023,2023-07-25T22:59:32Z,,,
arXIv2023,Trustworthiness of Children Stories Generated by Large Language Models,No.,1,"""No evidence""",2023,2023-07-25T22:55:51Z,,,
arXIv2023,A large language model-assisted education tool to provide feedback on open-ended responses,No.,1,"""No evidence""",2023,2023-07-25T19:49:55Z,,,
arXIv2023,Diversity and Language Technology: How Techno-Linguistic Bias Can Cause Epistemic Injustice,No.,1,"""No evidence""",2023,2023-07-25T16:08:27Z,,,
arXIv2023,XDLM: Cross-lingual Diffusion Language Model for Machine Translation,No.,1,"""No evidence""",2023,2023-07-25T15:08:34Z,,,
arXIv2023,"Holistic Exploration on Universal Decompositional Semantic Parsing: Architecture, Data Augmentation, and LLM Paradigm",No.,1,"""No evidence""",2023,2023-07-25T11:44:28Z,,,
arXIv2023,Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers,No.,1,"""No evidence""",2023,2023-07-25T09:35:43Z,,,
arXIv2023,Empower Your Model with Longer and Better Context Comprehension,No.,1,"""No evidence""",2023,2023-07-25T09:34:42Z,,,
arXIv2023,Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions,No.,1,"""No evidence""",2023,2023-07-25T08:51:30Z,,,
arXIv2023,QuIP: 2-Bit Quantization of Large Language Models With Guarantees,No.,1,"""No evidence""",2023,2023-07-25T07:44:06Z,,,
arXIv2023,LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition,No.,1,"""No evidence""",2023,2023-07-25T05:39:21Z,,,
arXIv2023,Multi-Granularity Prediction with Learnable Fusion for Scene Text Recognition,No.,1,"""No evidence""",2023,2023-07-25T04:12:50Z,,,
arXIv2023,Fashion Matrix: Editing Photos by Just Talking,No.,1,"""No evidence""",2023,2023-07-25T04:06:25Z,,,
arXIv2023,Multilevel Large Language Models for Everyone,No.,1,"""No evidence""",2023,2023-07-25T03:18:04Z,,,
arXIv2023,Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI,No.,1,"""No evidence""",2023,2023-07-24T21:42:27Z,,,
arXIv2023,"A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe",No.,1,"""No evidence""",2023,2023-07-24T21:01:46Z,,,
arXIv2023,Getting pwn'd by AI: Penetration Testing with Large Language Models,No.,1,"""No evidence""",2023,2023-07-24T19:59:22Z,,,
arXIv2023,How to use LLMs for Text Analysis,No.,1,"""No evidence""",2023,2023-07-24T19:54:15Z,,,
arXIv2023,Comparative Analysis of Drug-GPT and ChatGPT LLMs for Healthcare Insights: Evaluating Accuracy and Relevance in Patient and HCP Contexts,No.,1,"""No evidence""",2023,2023-07-24T19:27:11Z,,,
arXIv2023,Making Metadata More FAIR Using Large Language Models,No.,1,"""No evidence""",2023,2023-07-24T19:14:38Z,,,
arXIv2023,LLM-Rec: Personalized Recommendation via Prompting Large Language Models,No.,1,"""No evidence""",2023,2023-07-24T18:47:38Z,,,
arXIv2023,3D-LLM: Injecting the 3D World into Large Language Models,No.,1,"""No evidence""",2023,2023-07-24T17:59:02Z,,,
arXIv2023,Leveraging Label Variation in Large Language Models for Zero-Shot Text Classification,No.,1,"""No evidence""",2023,2023-07-24T17:49:31Z,,,
arXIv2023,RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment,No.,1,"""No evidence""",2023,2023-07-24T17:23:22Z,,,
arXIv2023,EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge: Mixed Sequences Prediction,No.,1,"""No evidence""",2023,2023-07-24T14:35:46Z,,,
arXIv2023,RRAML: Reinforced Retrieval Augmented Machine Learning,No.,1,"""No evidence""",2023,2023-07-24T13:51:19Z,,,
arXIv2023,Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models,No.,1,"""No evidence""",2023,2023-07-24T03:44:17Z,,,
arXIv2023,HateModerate: Testing Hate Speech Detectors against Content Moderation Policies,No.,1,"""No evidence""",2023,2023-07-23T20:08:38Z,,,
arXIv2023,Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data,No.,1,"""No evidence""",2023,2023-07-23T17:52:28Z,,,
arXIv2023,Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education,No.,1,"""No evidence""",2023,2023-07-23T08:47:51Z,,,
arXIv2023,Security and Privacy Issues of Federated Learning,No.,1,"""No evidence""",2023,2023-07-22T22:51:07Z,,,
arXIv2023,How to Build Low-cost Networks for Large Language Models (without Sacrificing Performance)?,No.,1,"""No evidence""",2023,2023-07-22T21:18:41Z,,,
arXIv2023,Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models,No.,1,"""No evidence""",2023,2023-07-22T19:59:16Z,,,
arXIv2023,Explainable Topic-Enhanced Argument Mining from Heterogeneous Sources,No.,1,"""No evidence""",2023,2023-07-22T17:26:55Z,,,
arXIv2023,A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks,No.,1,"""No evidence""",2023,2023-07-22T15:58:17Z,,,
arXIv2023,How to Design and Deliver Courses for Higher Education in the AI Era: Insights from Exam Data Analysis,No.,1,"""No evidence""",2023,2023-07-22T08:33:41Z,,,
arXIv2023,Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering,No.,1,"""No evidence""",2023,2023-07-22T05:34:18Z,,,
arXIv2023,Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?,No.,1,"""No evidence""",2023,2023-07-22T04:20:30Z,,,
arXIv2023,LAMP: Leveraging Language Prompts for Multi-person Pose Estimation,No.,1,"""No evidence""",2023,2023-07-21T23:00:43Z,,,
arXIv2023,Bibliometric Analysis of Publisher and Journal Instructions to Authors on Generative-AI in Academic and Scientific Publishing,No.,1,"""No evidence""",2023,2023-07-21T21:47:51Z,,,
arXIv2023,CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots,No.,1,"""No evidence""",2023,2023-07-21T19:09:37Z,,,
arXIv2023,Multimodal Document Analytics for Banking Process Automation,No.,1,"""No evidence""",2023,2023-07-21T18:29:04Z,,,
arXIv2023,Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts,No.,1,"""No evidence""",2023,2023-07-21T15:49:59Z,,,
arXIv2023,Assessing Large Language Models' ability to predict how humans balance self-interest and the interest of others,No.,1,"""No evidence""",2023,2023-07-21T13:23:31Z,,,
arXIv2023,AIGC Empowering Telecom Sector White Paper_chinese,No.,1,"""No evidence""",2023,2023-07-21T09:30:08Z,,,
arXIv2023,Prompting Large Language Models with Speech Recognition Abilities,No.,1,"""No evidence""",2023,2023-07-21T08:39:15Z,,,
arXIv2023,Large Language Model-based System to Provide Immediate Feedback to Students in Flipped Classroom Preparation Learning,No.,1,"""No evidence""",2023,2023-07-21T06:59:53Z,,,
arXIv2023,Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text,No.,1,"""No evidence""",2023,2023-07-21T06:38:37Z,,,
arXIv2023,CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study,No.,1,"""No evidence""",2023,2023-07-21T04:43:00Z,,,
arXIv2023,DEFTri: A Few-Shot Label Fused Contextual Representation Learning For Product Defect Triage in e-Commerce,No.,1,"""No evidence""",2023,2023-07-21T04:22:43Z,,,
arXIv2023,A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI,No.,1,"""No evidence""",2023,2023-07-21T04:15:36Z,,,
arXIv2023,GIST: Generating Image-Specific Text for Fine-grained Object Classification,No.,1,"""No evidence""",2023,2023-07-21T02:47:18Z,,,
arXIv2023,Kernelized Offline Contextual Dueling Bandits,No.,1,"""No evidence""",2023,2023-07-21T01:17:31Z,,,
arXIv2023,Generator-Retriever-Generator Approach for Open-Domain Question Answering,No.,1,"""No evidence""",2023,2023-07-21T00:34:38Z,,,
arXIv2023,An In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing,No.,1,"""No evidence""",2023,2023-07-20T22:10:04Z,,,
arXIv2023,UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models,No.,1,"""No evidence""",2023,2023-07-20T20:45:13Z,,,
arXIv2023,Applying QNLP to sentiment analysis in finance,No.,1,"""No evidence""",2023,2023-07-20T18:30:35Z,,,
arXIv2023,UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition,No.,1,"""No evidence""",2023,2023-07-20T18:08:34Z,,,
arXIv2023,L-Eval: Instituting Standardized Evaluation for Long Context Language Models,No.,1,"""No evidence""",2023,2023-07-20T17:59:41Z,,,
arXIv2023,A LLM Assisted Exploitation of AI-Guardian,No.,1,"""No evidence""",2023,2023-07-20T17:33:25Z,,,
arXIv2023,Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent Problems in AI Alignment using Large-Language Models,No.,1,"""No evidence""",2023,2023-07-20T17:19:15Z,,,
arXIv2023,Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification,No.,1,"""No evidence""",2023,2023-07-20T17:07:28Z,,,
arXIv2023,"""It Felt Like Having a Second Mind"": Investigating Human-AI Co-creativity in Prewriting with Large Language Models",No.,1,"""No evidence""",2023,2023-07-20T16:55:25Z,,,
arXIv2023,Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation,No.,1,"""No evidence""",2023,2023-07-20T16:46:10Z,,,
arXIv2023,LLM Cognitive Judgements Differ From Human,No.,1,"""No evidence""",2023,2023-07-20T16:22:36Z,,,
arXIv2023,MediaGPT : A Large Language Model For Chinese Media,No.,1,"""No evidence""",2023,2023-07-20T14:59:02Z,,,
arXIv2023,On Combining Expert Demonstrations in Imitation Learning via Optimal Transport,No.,1,"""No evidence""",2023,2023-07-20T12:20:18Z,,,
arXIv2023,Extreme Multi-Label Skill Extraction Training using Large Language Models,No.,1,"""No evidence""",2023,2023-07-20T11:29:15Z,,,
arXIv2023,"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers",No.,1,"""No evidence""",2023,2023-07-20T08:45:00Z,,,
arXIv2023,Generative Language Models on Nucleotide Sequences of Human Genes,No.,1,"""No evidence""",2023,2023-07-20T06:59:02Z,,,
arXIv2023,"Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa",No.,1,"""No evidence""",2023,2023-07-20T06:58:55Z,,,
arXIv2023,A Deep Dive into the Disparity of Word Error Rates Across Thousands of NPTEL MOOC Videos,No.,1,"""No evidence""",2023,2023-07-20T05:03:00Z,,,
arXIv2023,"Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting",No.,1,"""No evidence""",2023,2023-07-20T04:28:53Z,,,
arXIv2023,Instruction-following Evaluation through Verbalizer Manipulation,No.,1,"""No evidence""",2023,2023-07-20T03:54:24Z,,,
arXIv2023,SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer,No.,1,"""No evidence""",2023,2023-07-20T03:28:06Z,,,
arXIv2023,Dynamic Large Language Models on Blockchains,No.,1,"""No evidence""",2023,2023-07-20T03:26:57Z,,,
arXIv2023,"The Extractive-Abstractive Axis: Measuring Content ""Borrowing"" in Generative Language Models",No.,1,"""No evidence""",2023,2023-07-20T02:12:00Z,,,
arXIv2023,IvyGPT: InteractiVe Chinese pathwaY language model in medical domain,No.,1,"""No evidence""",2023,2023-07-20T01:11:14Z,,,
arXIv2023,Transsion TSUP's speech recognition system for ASRU 2023 MADASR Challenge,No.,1,"""No evidence""",2023,2023-07-20T00:55:01Z,,,
arXIv2023,Identifying Interpretable Subspaces in Image Representations,No.,1,"""No evidence""",2023,2023-07-20T00:02:24Z,,,
arXIv2023,Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs,No.,1,"""No evidence""",2023,2023-07-19T23:03:20Z,,,
arXIv2023,SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval,No.,1,"""No evidence""",2023,2023-07-19T22:48:02Z,,,
arXIv2023,FinGPT: Democratizing Internet-scale Data for Financial Large Language Models,No.,1,"""No evidence""",2023,2023-07-19T22:43:57Z,,,
arXIv2023,Thrust: Adaptively Propels Large Language Models with External Knowledge,No.,1,"""No evidence""",2023,2023-07-19T20:16:46Z,,,
arXIv2023,DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI,No.,1,"""No evidence""",2023,2023-07-19T17:57:53Z,,,
arXIv2023,Challenges and Applications of Large Language Models,No.,1,"""No evidence""",2023,2023-07-19T17:55:13Z,,,
arXIv2023,LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs,No.,1,"""No evidence""",2023,2023-07-19T17:54:43Z,,,
arXIv2023,VITS : Variational Inference Thomson Sampling for contextual bandits,No.,1,"""No evidence""",2023,2023-07-19T17:53:22Z,,,
arXIv2023,ProtiGeno: a prokaryotic short gene finder using protein language models,No.,1,"""No evidence""",2023,2023-07-19T16:46:42Z,,,
arXIv2023,Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK,No.,1,"""No evidence""",2023,2023-07-19T15:05:55Z,,,
arXIv2023,GUIDO: A Hybrid Approach to Guideline Discovery & Ordering from Natural Language Texts,No.,1,"""No evidence""",2023,2023-07-19T13:01:03Z,,,
arXIv2023,Large Language Models can accomplish Business Process Management Tasks,No.,1,"""No evidence""",2023,2023-07-19T11:54:46Z,,,
arXIv2023,Chit-Chat or Deep Talk: Prompt Engineering for Process Mining,No.,1,"""No evidence""",2023,2023-07-19T11:25:12Z,,,
arXIv2023,DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification,No.,1,"""No evidence""",2023,2023-07-19T08:02:20Z,,,
arXIv2023,"On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",No.,1,"""No evidence""",2023,2023-07-19T07:17:43Z,,,
arXIv2023,Longitudinal Data and a Semantic Similarity Reward for Chest X-Ray Report Generation,No.,1,"""No evidence""",2023,2023-07-19T05:41:14Z,,,
arXIv2023,Generative Prompt Model for Weakly Supervised Object Localization,No.,1,"""No evidence""",2023,2023-07-19T05:40:38Z,,,
arXIv2023,Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community,No.,1,"""No evidence""",2023,2023-07-19T05:23:43Z,,,
arXIv2023,Mood Classification of Bangla Songs Based on Lyrics,No.,1,"""No evidence""",2023,2023-07-19T03:31:41Z,,,
arXIv2023,RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap,No.,1,"""No evidence""",2023,2023-07-19T01:37:31Z,,,
arXIv2023,Efficient Guided Generation for Large Language Models,No.,1,"""No evidence""",2023,2023-07-19T01:14:49Z,,,
arXIv2023,PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence,No.,1,"""No evidence""",2023,2023-07-18T23:35:53Z,,,
arXIv2023,ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning,No.,1,"""No evidence""",2023,2023-07-18T17:56:06Z,,,
arXIv2023,SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs,No.,1,"""No evidence""",2023,2023-07-18T17:09:15Z,,,
arXIv2023,Scaling Laws for Imitation Learning in Single-Agent Games,No.,1,"""No evidence""",2023,2023-07-18T16:43:03Z,,,
arXIv2023,Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation,No.,1,"""No evidence""",2023,2023-07-18T16:33:30Z,,,
arXIv2023,Llama 2: Open Foundation and Fine-Tuned Chat Models,No.,1,"""No evidence""",2023,2023-07-18T14:31:57Z,,,
arXIv2023,Application of BERT in Wind Power Forecasting-Teletraan's Solution in Baidu KDD Cup 2022,No.,1,"""No evidence""",2023,2023-07-18T13:28:30Z,,,
arXIv2023,Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models,No.,1,"""No evidence""",2023,2023-07-18T12:45:54Z,,,
arXIv2023,Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature,No.,1,"""No evidence""",2023,2023-07-18T07:59:37Z,,,
arXIv2023,Emotional Intelligence of Large Language Models,No.,1,"""No evidence""",2023,2023-07-18T07:49:38Z,,,
arXIv2023,Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey,No.,1,"""No evidence""",2023,2023-07-18T07:20:43Z,,,
arXIv2023,Multimodal LLMs for health grounded in individual-specific data,No.,1,"""No evidence""",2023,2023-07-18T07:12:46Z,,,
arXIv2023,How is ChatGPT's behavior changing over time?,No.,1,"""No evidence""",2023,2023-07-18T06:56:08Z,,,
arXIv2023,PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM,No.,1,"""No evidence""",2023,2023-07-18T05:51:00Z,,,
arXIv2023,"Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines",No.,1,"""No evidence""",2023,2023-07-18T05:12:52Z,,,
arXIv2023,AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models,No.,1,"""No evidence""",2023,2023-07-18T04:43:24Z,,,
arXIv2023,NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning,No.,1,"""No evidence""",2023,2023-07-18T03:12:51Z,,,
arXIv2023,Federated Large Language Model: A Position Paper,No.,1,"""No evidence""",2023,2023-07-18T02:09:14Z,,,
arXIv2023,Large Language Models Perform Diagnostic Reasoning,No.,1,"""No evidence""",2023,2023-07-18T01:43:00Z,,,
arXIv2023,AI for the Generation and Testing of Ideas Towards an AI Supported Knowledge Development Environment,No.,1,"""No evidence""",2023,2023-07-17T22:17:40Z,,,
arXIv2023,Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge,No.,1,"""No evidence""",2023,2023-07-17T20:01:11Z,,,
arXIv2023,Creating Image Datasets in Agricultural Environments using DALL.E: Generative AI-Powered Large Language Model,No.,1,"""No evidence""",2023,2023-07-17T19:17:10Z,,,
arXIv2023,GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution,No.,1,"""No evidence""",2023,2023-07-17T18:42:05Z,,,
arXIv2023,A mixed policy to improve performance of language models on math problems,No.,1,"""No evidence""",2023,2023-07-17T18:27:49Z,,,
arXIv2023,AlpaGasus: Training A Better Alpaca with Fewer Data,No.,1,"""No evidence""",2023,2023-07-17T17:59:40Z,,,
arXIv2023,FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning,No.,1,"""No evidence""",2023,2023-07-17T17:50:36Z,,,
arXIv2023,COLLIE: Systematic Construction of Constrained Text Generation Tasks,No.,1,"""No evidence""",2023,2023-07-17T17:48:51Z,,,
arXIv2023,Retentive Network: A Successor to Transformer for Large Language Models,No.,1,"""No evidence""",2023,2023-07-17T16:40:01Z,,,
arXIv2023,BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs,No.,1,"""No evidence""",2023,2023-07-17T15:51:47Z,,,
arXIv2023,A Study on the Performance of Generative Pre-trained Transformer (GPT) in Simulating Depressed Individuals on the Standardized Depressive Symptom Scale,No.,1,"""No evidence""",2023,2023-07-17T15:44:13Z,,,
arXIv2023,Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models,No.,1,"""No evidence""",2023,2023-07-17T13:49:52Z,,,
arXIv2023,Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain,No.,1,"""No evidence""",2023,2023-07-17T13:34:31Z,,,
arXIv2023,"Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems",No.,1,"""No evidence""",2023,2023-07-17T12:14:14Z,,,
arXIv2023,On the application of Large Language Models for language teaching and assessment technology,No.,1,"""No evidence""",2023,2023-07-17T11:12:56Z,,,
arXIv2023,Gender mobility in the labor market with skills-based matching models,No.,1,"""No evidence""",2023,2023-07-17T10:06:21Z,,,
arXIv2023,Zero-th Order Algorithm for Softmax Attention Optimization,No.,1,"""No evidence""",2023,2023-07-17T09:43:50Z,,,
arXIv2023,M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,No.,1,"""No evidence""",2023,2023-07-17T09:38:41Z,,,
arXIv2023,Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction,No.,1,"""No evidence""",2023,2023-07-17T08:38:46Z,,,
arXIv2023,Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models,No.,1,"""No evidence""",2023,2023-07-17T07:55:47Z,,,
arXIv2023,"Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research",No.,1,"""No evidence""",2023,2023-07-17T07:48:31Z,,,
arXIv2023,ChatGPT is Good but Bing Chat is Better for Vietnamese Students,No.,1,"""No evidence""",2023,2023-07-17T06:36:53Z,,,
arXIv2023,Extending the Frontier of ChatGPT: Code Generation and Debugging,No.,1,"""No evidence""",2023,2023-07-17T06:06:58Z,,,
arXIv2023,Harnessing Scalable Transactional Stream Processing for Managing Large Language Models [Vision],No.,1,"""No evidence""",2023,2023-07-17T04:01:02Z,,,
arXIv2023,A Lightweight Framework for High-Quality Code Generation,No.,1,"""No evidence""",2023,2023-07-17T03:45:00Z,,,
arXIv2023,"Mini-Giants: ""Small"" Language Models and Open Source Win-Win",No.,1,"""No evidence""",2023,2023-07-17T01:35:56Z,,,
arXIv2023,Zero-Shot Image Harmonization with Generative Model Prior,No.,1,"""No evidence""",2023,2023-07-17T00:56:21Z,,,
arXIv2023,Using an LLM to Help With Code Understanding,No.,1,"""No evidence""",2023,2023-07-17T00:49:06Z,,,
arXIv2023,Assessing the Quality of Multiple-Choice Questions Using GPT-4 and Rule-Based Methods,No.,1,"""No evidence""",2023,2023-07-16T22:12:10Z,,,
arXIv2023,Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling,No.,1,"""No evidence""",2023,2023-07-16T15:18:25Z,,,
arXIv2023,Fast Quantum Algorithm for Attention Computation,No.,1,"""No evidence""",2023,2023-07-16T14:00:42Z,,,
arXIv2023,Planting a SEED of Vision in Large Language Model,No.,1,"""No evidence""",2023,2023-07-16T13:41:39Z,,,
arXIv2023,A Survey of Techniques for Optimizing Transformer Inference,No.,1,"""No evidence""",2023,2023-07-16T08:50:50Z,,,
arXIv2023,MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning,No.,1,"""No evidence""",2023,2023-07-16T05:41:53Z,,,
arXIv2023,SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning,No.,1,"""No evidence""",2023,2023-07-16T05:33:35Z,,,
arXIv2023,Language Conditioned Traffic Generation,No.,1,"""No evidence""",2023,2023-07-16T05:10:32Z,,,
arXIv2023,GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT,No.,1,"""No evidence""",2023,2023-07-16T03:03:59Z,,,
arXIv2023,Communicative Agents for Software Development,No.,1,"""No evidence""",2023,2023-07-16T02:11:34Z,,,
arXIv2023,Recognition of Mental Adjectives in An Efficient and Automatic Style,No.,1,"""No evidence""",2023,2023-07-16T01:27:08Z,,,
arXIv2023,Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages,No.,1,"""No evidence""",2023,2023-07-16T00:45:42Z,,,
arXIv2023,LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models,No.,1,"""No evidence""",2023,2023-07-15T22:02:12Z,,,
arXIv2023,The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents,No.,1,"""No evidence""",2023,2023-07-15T19:05:56Z,,,
arXIv2023,Large Language Models as Superpositions of Cultural Perspectives,No.,1,"""No evidence""",2023,2023-07-15T19:04:33Z,,,
arXIv2023,SINC: Self-Supervised In-Context Learning for Vision-Language Tasks,No.,1,"""No evidence""",2023,2023-07-15T08:33:08Z,,,
arXIv2023,Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning,No.,1,"""No evidence""",2023,2023-07-15T08:18:37Z,,,
arXIv2023,CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models,No.,1,"""No evidence""",2023,2023-07-15T04:37:11Z,,,
arXIv2023,Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph,No.,1,"""No evidence""",2023,2023-07-15T03:31:38Z,,,
arXIv2023,Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text,No.,1,"""No evidence""",2023,2023-07-15T03:29:59Z,,,
arXIv2023,Creating a Dataset for High-Performance Computing Code Translation using LLMs: A Bridge Between OpenMP Fortran and C++,No.,1,"""No evidence""",2023,2023-07-15T02:35:51Z,,,
arXIv2023,Othering and low status framing of immigrant cuisines in US restaurant reviews and large language models,No.,1,"""No evidence""",2023,2023-07-14T22:25:39Z,,,
arXIv2023,Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT,No.,1,"""No evidence""",2023,2023-07-14T17:24:15Z,,,
arXIv2023,Towards spoken dialect identification of Irish,No.,1,"""No evidence""",2023,2023-07-14T16:03:09Z,,,
arXIv2023,Gloss Attention for Gloss-free Sign Language Translation,No.,1,"""No evidence""",2023,2023-07-14T14:07:55Z,,,
arXIv2023,Time for aCTIon: Automated Analysis of Cyber Threat Intelligence in the Wild,No.,1,"""No evidence""",2023,2023-07-14T13:43:16Z,,,
arXIv2023,PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting,No.,1,"""No evidence""",2023,2023-07-14T13:43:04Z,,,
arXIv2023,Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs,No.,1,"""No evidence""",2023,2023-07-14T12:45:03Z,,,
arXIv2023,C3: Zero-shot Text-to-SQL with ChatGPT,No.,1,"""No evidence""",2023,2023-07-14T12:30:41Z,,,
arXIv2023,MorphPiece : A Linguistic Tokenizer for Large Language Models,No.,1,"""No evidence""",2023,2023-07-14T10:35:04Z,,,
arXIv2023,Improving BERT with Hybrid Pooling Network and Drop Mask,No.,1,"""No evidence""",2023,2023-07-14T10:20:08Z,,,
arXIv2023,Fairness of ChatGPT and the Role Of Explainable-Guided Prompts,No.,1,"""No evidence""",2023,2023-07-14T09:20:16Z,,,
arXIv2023,TVPR: Text-to-Video Person Retrieval and a New Benchmark,No.,1,"""No evidence""",2023,2023-07-14T06:34:00Z,,,
arXIv2023,HYTREL: Hypergraph-enhanced Tabular Data Representation Learning,No.,1,"""No evidence""",2023,2023-07-14T05:41:22Z,,,
arXIv2023,Learning to Retrieve In-Context Examples for Large Language Models,No.,1,"""No evidence""",2023,2023-07-14T05:23:08Z,,,
arXIv2023,Drive Like a Human: Rethinking Autonomous Driving with Large Language Models,No.,1,"""No evidence""",2023,2023-07-14T05:18:34Z,,,
arXIv2023,Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords,No.,1,"""No evidence""",2023,2023-07-14T05:09:04Z,,,
arXIv2023,Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots,No.,1,"""No evidence""",2023,2023-07-14T03:58:42Z,,,
arXIv2023,Large Language Models Understand and Can be Enhanced by Emotional Stimuli,No.,1,"""No evidence""",2023,2023-07-14T00:57:12Z,,,
arXIv2023,Bootstrapping Vision-Language Learning with Decoupled Language Pre-training,No.,1,"""No evidence""",2023,2023-07-13T21:08:15Z,,,
arXIv2023,Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section,No.,1,"""No evidence""",2023,2023-07-13T20:04:05Z,,,
arXIv2023,Electoral Agitation Data Set: The Use Case of the Polish Election,No.,1,"""No evidence""",2023,2023-07-13T18:14:43Z,,,
arXIv2023,In-context Autoencoder for Context Compression in a Large Language Model,No.,1,"""No evidence""",2023,2023-07-13T17:59:21Z,,,
arXIv2023,InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation,No.,1,"""No evidence""",2023,2023-07-13T17:58:32Z,,,
arXIv2023,mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs,No.,1,"""No evidence""",2023,2023-07-13T17:51:58Z,,,
arXIv2023,LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT,No.,1,"""No evidence""",2023,2023-07-13T17:31:41Z,,,
arXIv2023,Retrieval Augmented Generation using Engineering Design Knowledge,No.,1,"""No evidence""",2023,2023-07-13T17:25:28Z,,,
arXIv2023,Generating Benchmarks for Factuality Evaluation of Language Models,No.,1,"""No evidence""",2023,2023-07-13T17:14:38Z,,,
arXIv2023,Effective Prompt Extraction from Language Models,No.,1,"""No evidence""",2023,2023-07-13T16:15:08Z,,,
arXIv2023,Personalization for BERT-based Discriminative Speech Recognition Rescoring,No.,1,"""No evidence""",2023,2023-07-13T15:54:32Z,,,
arXIv2023,Negated Complementary Commonsense using Large Language Models,No.,1,"""No evidence""",2023,2023-07-13T15:03:48Z,,,
arXIv2023,Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models,No.,1,"""No evidence""",2023,2023-07-13T14:50:55Z,,,
arXIv2023,ChatGPT and Bard Responses to Polarizing Questions,No.,1,"""No evidence""",2023,2023-07-13T14:45:47Z,,,
arXIv2023,Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models,No.,1,"""No evidence""",2023,2023-07-13T12:11:36Z,,,
arXIv2023,SecureFalcon: The Next Cyber Reasoning System for Cyber Security,No.,1,"""No evidence""",2023,2023-07-13T08:34:09Z,,,
arXIv2023,Convolutional Neural Networks for Sentiment Analysis on Weibo Data: A Natural Language Processing Approach,No.,1,"""No evidence""",2023,2023-07-13T03:02:56Z,,,
arXIv2023,Agreement Tracking for Multi-Issue Negotiation Dialogues,No.,1,"""No evidence""",2023,2023-07-13T02:00:27Z,,,
arXIv2023,Assessing the Ability of ChatGPT to Screen Articles for Systematic Reviews,No.,1,"""No evidence""",2023,2023-07-12T21:39:42Z,,,
arXIv2023,ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task,No.,1,"""No evidence""",2023,2023-07-12T20:33:30Z,,,
arXIv2023,No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models,No.,1,"""No evidence""",2023,2023-07-12T20:10:14Z,,,
arXIv2023,Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events,No.,1,"""No evidence""",2023,2023-07-12T20:08:48Z,,,
arXIv2023,A Comprehensive Overview of Large Language Models,No.,1,"""No evidence""",2023,2023-07-12T20:01:52Z,,,
arXIv2023,Instruction Mining: When Data Mining Meets Large Language Model Finetuning,No.,1,"""No evidence""",2023,2023-07-12T16:37:31Z,,,
arXIv2023,MMBench: Is Your Multi-modal Model an All-around Player?,No.,1,"""No evidence""",2023,2023-07-12T16:23:09Z,,,
arXIv2023,Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches,No.,1,"""No evidence""",2023,2023-07-12T15:07:16Z,,,
arXIv2023,Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems,No.,1,"""No evidence""",2023,2023-07-12T14:26:46Z,,,
arXIv2023,Detecting the Presence of COVID-19 Vaccination Hesitancy from South African Twitter Data Using Machine Learning,No.,1,"""No evidence""",2023,2023-07-12T13:28:37Z,,,
arXIv2023,NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services,No.,1,"""No evidence""",2023,2023-07-12T13:10:08Z,,,
arXIv2023,SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,No.,1,"""No evidence""",2023,2023-07-12T12:37:55Z,,,
arXIv2023,VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View,No.,1,"""No evidence""",2023,2023-07-12T11:08:24Z,,,
arXIv2023,PolyLM: An Open Source Polyglot Large Language Model,No.,1,"""No evidence""",2023,2023-07-12T09:00:37Z,,,
arXIv2023,VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models,No.,1,"""No evidence""",2023,2023-07-12T07:40:48Z,,,
arXIv2023,Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval Augmented Generation Models for Open Book Question-Answering,No.,1,"""No evidence""",2023,2023-07-12T04:44:31Z,,,
arXIv2023,Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding,No.,1,"""No evidence""",2023,2023-07-12T04:28:41Z,,,
arXIv2023,Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing,No.,1,"""No evidence""",2023,2023-07-11T22:58:53Z,,,
arXIv2023,Large Language Models,No.,1,"""No evidence""",2023,2023-07-11T20:21:02Z,,,
arXIv2023,Neural Machine Translation Data Generation and Augmentation using ChatGPT,No.,1,"""No evidence""",2023,2023-07-11T20:15:47Z,,,
arXIv2023,Lightweight reranking for language model generations,No.,1,"""No evidence""",2023,2023-07-11T17:51:48Z,,,
arXIv2023,Named entity recognition using GPT for identifying comparable companies,No.,1,"""No evidence""",2023,2023-07-11T16:48:16Z,,,
arXIv2023,GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts,No.,1,"""No evidence""",2023,2023-07-11T15:44:01Z,,,
arXIv2023,Explaining Competitive-Level Programming Solutions using LLMs,No.,1,"""No evidence""",2023,2023-07-11T15:26:49Z,,,
arXIv2023,Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration,No.,1,"""No evidence""",2023,2023-07-11T14:45:19Z,,,
arXIv2023,SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization,No.,1,"""No evidence""",2023,2023-07-11T10:38:58Z,,,
arXIv2023,Piecing Together Clues: A Benchmark for Evaluating the Detective Skills of Large Language Models,No.,1,"""No evidence""",2023,2023-07-11T08:45:46Z,,,
arXIv2023,Vacaspati: A Diverse Corpus of Bangla Literature,No.,1,"""No evidence""",2023,2023-07-11T07:32:12Z,,,
arXIv2023,OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning,No.,1,"""No evidence""",2023,2023-07-11T07:31:58Z,,,
arXIv2023,Argumentative Segmentation Enhancement for Legal Summarization,No.,1,"""No evidence""",2023,2023-07-11T07:29:18Z,,,
arXIv2023,Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain,No.,1,"""No evidence""",2023,2023-07-11T07:16:22Z,,,
arXIv2023,Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps,No.,1,"""No evidence""",2023,2023-07-11T07:03:29Z,,,
arXIv2023,DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence Analysis Tasks,No.,1,"""No evidence""",2023,2023-07-11T06:30:43Z,,,
arXIv2023,Improving RNN-Transducers with Acoustic LookAhead,No.,1,"""No evidence""",2023,2023-07-11T03:57:00Z,,,
arXIv2023,Epidemic Modeling with Generative Agents,No.,1,"""No evidence""",2023,2023-07-11T02:52:32Z,,,
arXIv2023,KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization,No.,1,"""No evidence""",2023,2023-07-10T21:18:01Z,,,
arXIv2023,SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation,No.,1,"""No evidence""",2023,2023-07-10T21:16:46Z,,,
arXIv2023,"ChatGPT for Digital Forensic Investigation: The Good, The Bad, and The Unknown",No.,1,"""No evidence""",2023,2023-07-10T20:07:30Z,,,
arXIv2023,Linear Alignment of Vision-language Models for Image Captioning,No.,1,"""No evidence""",2023,2023-07-10T17:59:21Z,,,
arXIv2023,RoCo: Dialectic Multi-Robot Collaboration with Large Language Models,No.,1,"""No evidence""",2023,2023-07-10T17:52:01Z,,,
arXIv2023,Large Language Models as General Pattern Machines,No.,1,"""No evidence""",2023,2023-07-10T17:32:13Z,,,
arXIv2023,COMEX: A Tool for Generating Customized Source Code Representations,No.,1,"""No evidence""",2023,2023-07-10T16:46:34Z,,,
arXIv2023,A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI/ML in Oncology,No.,1,"""No evidence""",2023,2023-07-10T14:07:28Z,,,
arXIv2023,Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases,No.,1,"""No evidence""",2023,2023-07-10T12:18:34Z,,,
arXIv2023,Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations,No.,1,"""No evidence""",2023,2023-07-10T11:29:41Z,,,
arXIv2023,PapagAI:Automated Feedback for Reflective Essays,No.,1,"""No evidence""",2023,2023-07-10T11:05:51Z,,,
arXIv2023,Some Preliminary Steps Towards Metaverse Logic,No.,1,"""No evidence""",2023,2023-07-10T09:13:22Z,,,
arXIv2023,Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures,No.,1,"""No evidence""",2023,2023-07-10T08:20:34Z,,,
arXIv2023,RLTF: Reinforcement Learning from Unit Test Feedback,No.,1,"""No evidence""",2023,2023-07-10T05:18:18Z,,,
arXIv2023,Text Descriptions are Compressive and Invariant Representations for Visual Learning,No.,1,"""No evidence""",2023,2023-07-10T03:06:45Z,,,
arXIv2023,Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion,No.,1,"""No evidence""",2023,2023-07-09T22:39:37Z,,,
arXIv2023,Assessing the efficacy of large language models in generating accurate teacher responses,No.,1,"""No evidence""",2023,2023-07-09T22:32:46Z,,,
arXIv2023,The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence,No.,1,"""No evidence""",2023,2023-07-09T21:16:56Z,,,
arXIv2023,ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey,No.,1,"""No evidence""",2023,2023-07-09T19:28:46Z,,,
arXIv2023,Natural Language Instructions for Intuitive Human Interaction with Robotic Assistants in Field Construction Work,No.,1,"""No evidence""",2023,2023-07-09T15:02:34Z,,,
arXIv2023,Can Generative Large Language Models Perform ASR Error Correction?,No.,1,"""No evidence""",2023,2023-07-09T13:38:25Z,,,
arXIv2023,FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?,No.,1,"""No evidence""",2023,2023-07-09T08:07:43Z,,,
arXIv2023,SVIT: Scaling up Visual Instruction Tuning,No.,1,"""No evidence""",2023,2023-07-09T03:25:14Z,,,
arXIv2023,Bidirectional Attention as a Mixture of Continuous Word Experts,No.,1,"""No evidence""",2023,2023-07-08T23:25:55Z,,,
arXIv2023,Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes,No.,1,"""No evidence""",2023,2023-07-08T17:21:55Z,,,
arXIv2023,Is ChatGPT a Good Personality Recognizer? A Preliminary Study,No.,1,"""No evidence""",2023,2023-07-08T11:02:02Z,,,
arXIv2023,Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models,No.,1,"""No evidence""",2023,2023-07-08T09:11:19Z,,,
arXIv2023,"Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators",No.,1,"""No evidence""",2023,2023-07-08T07:08:20Z,,,
arXIv2023,On decoder-only architecture for speech-to-text and large language model integration,No.,1,"""No evidence""",2023,2023-07-08T06:47:58Z,,,
arXIv2023,ScriptWorld: Text Based Environment For Learning Procedural Knowledge,No.,1,"""No evidence""",2023,2023-07-08T05:43:03Z,,,
arXIv2023,Large Language Models for Supply Chain Optimization,No.,1,"""No evidence""",2023,2023-07-08T01:42:22Z,,,
arXIv2023,How does AI chat change search behaviors?,No.,1,"""No evidence""",2023,2023-07-07T20:41:26Z,,,
arXIv2023,Exploring and Characterizing Large Language Models For Embedded System Development and Debugging,No.,1,"""No evidence""",2023,2023-07-07T20:14:22Z,,,
arXIv2023,QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models,No.,1,"""No evidence""",2023,2023-07-07T17:46:08Z,,,
arXIv2023,INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers,No.,1,"""No evidence""",2023,2023-07-07T16:54:53Z,,,
arXIv2023,LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad,No.,1,"""No evidence""",2023,2023-07-07T16:25:59Z,,,
arXIv2023,Discovering Variable Binding Circuitry with Desiderata,No.,1,"""No evidence""",2023,2023-07-07T14:51:30Z,,,
arXIv2023,GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest,No.,1,"""No evidence""",2023,2023-07-07T13:43:44Z,,,
arXIv2023,Text Simplification of Scientific Texts for Non-Expert Readers,No.,1,"""No evidence""",2023,2023-07-07T13:05:11Z,,,
arXIv2023,DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling,No.,1,"""No evidence""",2023,2023-07-07T12:34:57Z,,,
arXIv2023,Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers,No.,1,"""No evidence""",2023,2023-07-07T12:04:12Z,,,
arXIv2023,Procedurally generating rules to adapt difficulty for narrative puzzle games,No.,1,"""No evidence""",2023,2023-07-07T11:14:53Z,,,
arXIv2023,MultiQG-TI: Towards Question Generation from Multi-modal Sources,No.,1,"""No evidence""",2023,2023-07-07T08:14:15Z,,,
arXIv2023,Goal-Conditioned Predictive Coding for Offline Reinforcement Learning,No.,1,"""No evidence""",2023,2023-07-07T06:12:14Z,,,
arXIv2023,Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs,No.,1,"""No evidence""",2023,2023-07-07T05:31:31Z,,,
arXIv2023,AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime,No.,1,"""No evidence""",2023,2023-07-07T04:49:26Z,,,
arXIv2023,Teaching Arithmetic to Small Transformers,No.,1,"""No evidence""",2023,2023-07-07T04:33:31Z,,,
arXIv2023,A Side-by-side Comparison of Transformers for English Implicit Discourse Relation Classification,No.,1,"""No evidence""",2023,2023-07-07T04:12:25Z,,,
arXIv2023,S2vNTM: Semi-supervised vMF Neural Topic Modeling,No.,1,"""No evidence""",2023,2023-07-06T21:44:31Z,,,
arXIv2023,Distilling Large Vision-Language Model with Out-of-Distribution Generalizability,No.,1,"""No evidence""",2023,2023-07-06T17:05:26Z,,,
arXIv2023,KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding,No.,1,"""No evidence""",2023,2023-07-06T16:35:25Z,,,
arXIv2023,A Survey on Evaluation of Large Language Models,No.,1,"""No evidence""",2023,2023-07-06T16:28:35Z,,,
arXIv2023,Can ChatGPT's Responses Boost Traditional Natural Language Processing?,No.,1,"""No evidence""",2023,2023-07-06T15:42:05Z,,,
arXIv2023,Generalizing Backpropagation for Gradient-Based Interpretability,No.,1,"""No evidence""",2023,2023-07-06T15:19:53Z,,,
arXIv2023,Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain,No.,1,"""No evidence""",2023,2023-07-06T15:06:41Z,,,
arXIv2023,Improving Retrieval-Augmented Large Language Models via Data Importance Learning,No.,1,"""No evidence""",2023,2023-07-06T14:44:07Z,,,
arXIv2023,"CORE-GPT: Combining Open Access research and large language models for credible, trustworthy question answering",No.,1,"""No evidence""",2023,2023-07-06T13:41:36Z,,,
arXIv2023,Agentivit e telicit in GilBERTo: implicazioni cognitive,No.,1,"""No evidence""",2023,2023-07-06T10:52:22Z,,,
arXIv2023,Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation,No.,1,"""No evidence""",2023,2023-07-06T08:13:53Z,,,
arXIv2023,Policy Contrastive Imitation Learning,No.,1,"""No evidence""",2023,2023-07-06T07:52:50Z,,,
arXIv2023,What Should Data Science Education Do with Large Language Models?,No.,1,"""No evidence""",2023,2023-07-06T06:07:29Z,,,
arXIv2023,UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language,No.,1,"""No evidence""",2023,2023-07-06T05:26:49Z,,,
arXIv2023,UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering,No.,1,"""No evidence""",2023,2023-07-06T05:22:20Z,,,
arXIv2023,Large Language Models Empowered Autonomous Edge AI for Connected Intelligence,No.,1,"""No evidence""",2023,2023-07-06T05:16:55Z,,,
arXIv2023,Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships,No.,1,"""No evidence""",2023,2023-07-06T04:06:05Z,,,
arXIv2023,RecallM: An Adaptable Memory Mechanism with Temporal Understanding for Large Language Models,No.,1,"""No evidence""",2023,2023-07-06T02:51:54Z,,,
arXIv2023,Text Alignment Is An Efficient Unified Model for Massive NLP Tasks,No.,1,"""No evidence""",2023,2023-07-06T02:28:31Z,,,
arXIv2023,Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment,No.,1,"""No evidence""",2023,2023-07-05T23:01:26Z,,,
arXIv2023,SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference,No.,1,"""No evidence""",2023,2023-07-05T19:59:09Z,,,
arXIv2023,Learning when to observe: A frugal reinforcement learning framework for a high-cost world,No.,1,"""No evidence""",2023,2023-07-05T19:48:03Z,,,
arXIv2023,Evade ChatGPT Detectors via A Single Space,No.,1,"""No evidence""",2023,2023-07-05T18:48:28Z,,,
arXIv2023,Artificial Intelligence in archival and historical scholarship workflow: HTS and ChatGPT,No.,1,"""No evidence""",2023,2023-07-05T18:32:28Z,,,
arXIv2023,Several categories of Large Language Models (LLMs): A Short Survey,No.,1,"""No evidence""",2023,2023-07-05T18:18:23Z,,,
arXIv2023,"LongNet: Scaling Transformers to 1,000,000,000 Tokens",No.,1,"""No evidence""",2023,2023-07-05T17:59:38Z,,,
arXIv2023,Building Cooperative Embodied Agents Modularly with Large Language Models,No.,1,"""No evidence""",2023,2023-07-05T17:59:27Z,,,
arXIv2023,Deductive Additivity for Planning of Natural Language Proofs,No.,1,"""No evidence""",2023,2023-07-05T17:45:48Z,,,
arXIv2023,What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?,No.,1,"""No evidence""",2023,2023-07-05T17:44:28Z,,,
arXIv2023,Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models,No.,1,"""No evidence""",2023,2023-07-05T17:22:09Z,,,
arXIv2023,An Exploratory Literature Study on Sharing and Energy Use of Language Models for Source Code,No.,1,"""No evidence""",2023,2023-07-05T17:13:00Z,,,
arXIv2023,Exploring Continual Learning for Code Generation Models,No.,1,"""No evidence""",2023,2023-07-05T16:58:39Z,,,
arXIv2023,In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick,No.,1,"""No evidence""",2023,2023-07-05T16:41:01Z,,,
arXIv2023,Causal Discovery with Language Models as Imperfect Experts,No.,1,"""No evidence""",2023,2023-07-05T16:01:38Z,,,
arXIv2023,Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media,No.,1,"""No evidence""",2023,2023-07-05T14:15:15Z,,,
arXIv2023,Sumformer: Universal Approximation for Efficient Transformers,No.,1,"""No evidence""",2023,2023-07-05T13:59:35Z,,,
arXIv2023,"Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard",No.,1,"""No evidence""",2023,2023-07-05T13:40:57Z,,,
arXIv2023,Exploring Multimodal Approaches for Alzheimer's Disease Detection Using Patient Speech Transcript and Audio Data,No.,1,"""No evidence""",2023,2023-07-05T12:40:11Z,,,
arXIv2023,Power-up! What Can Generative Models Do for Human Computation Workflows?,No.,1,"""No evidence""",2023,2023-07-05T12:35:29Z,,,
arXIv2023,The FormAI Dataset: Generative AI in Software Security Through the Lens of Formal Verification,No.,1,"""No evidence""",2023,2023-07-05T10:39:58Z,,,
arXIv2023,Citation: A Key to Building Responsible and Accountable Large Language Models,No.,1,"""No evidence""",2023,2023-07-05T10:25:45Z,,,
arXIv2023,Open-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks,No.,1,"""No evidence""",2023,2023-07-05T10:15:07Z,,,
arXIv2023,Generative Job Recommendations with Large Language Model,No.,1,"""No evidence""",2023,2023-07-05T09:58:08Z,,,
arXIv2023,Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning,No.,1,"""No evidence""",2023,2023-07-05T09:42:25Z,,,
arXIv2023,Multilingual Controllable Transformer-Based Lexical Simplification,No.,1,"""No evidence""",2023,2023-07-05T08:48:19Z,,,
arXIv2023,Emoji Prediction in Tweets using BERT,No.,1,"""No evidence""",2023,2023-07-05T06:38:52Z,,,
arXIv2023,Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning,No.,1,"""No evidence""",2023,2023-07-05T06:36:54Z,,,
arXIv2023,CAME: Confidence-guided Adaptive Memory Efficient Optimization,No.,1,"""No evidence""",2023,2023-07-05T06:05:36Z,,,
arXIv2023,Recommender Systems in the Era of Large Language Models (LLMs),No.,1,"""No evidence""",2023,2023-07-05T06:03:40Z,,,
arXIv2023,PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records,No.,1,"""No evidence""",2023,2023-07-05T03:31:12Z,,,
arXIv2023,A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis,No.,1,"""No evidence""",2023,2023-07-05T01:45:19Z,,,
arXIv2023,Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review,No.,1,"""No evidence""",2023,2023-07-04T21:26:51Z,,,
arXIv2023,Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners,No.,1,"""No evidence""",2023,2023-07-04T21:25:12Z,,,
arXIv2023,"Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics",No.,1,"""No evidence""",2023,2023-07-04T20:16:32Z,,,
arXIv2023,KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation,No.,1,"""No evidence""",2023,2023-07-04T18:49:19Z,,,
arXIv2023,Embodied Task Planning with Large Language Models,No.,1,"""No evidence""",2023,2023-07-04T17:58:25Z,,,
arXIv2023,The Inner Sentiments of a Thought,No.,1,"""No evidence""",2023,2023-07-04T15:44:37Z,,,
arXIv2023,mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding,No.,1,"""No evidence""",2023,2023-07-04T11:28:07Z,,,
arXIv2023,Insert-expansions for Tool-enabled Conversational Agents,No.,1,"""No evidence""",2023,2023-07-04T10:57:31Z,,,
arXIv2023,Chain of Thought Prompting Elicits Knowledge Augmentation,No.,1,"""No evidence""",2023,2023-07-04T10:51:16Z,,,
arXIv2023,A Language Model for Grammatical Error Correction in L2 Russian,No.,1,"""No evidence""",2023,2023-07-04T09:50:13Z,,,
arXIv2023,On Conditional and Compositional Language Model Differentiable Prompting,No.,1,"""No evidence""",2023,2023-07-04T02:47:42Z,,,
arXIv2023,"Garbage in, garbage out: Zero-shot detection of crime using Large Language Models",No.,1,"""No evidence""",2023,2023-07-04T01:29:15Z,,,
arXIv2023,"ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis",No.,1,"""No evidence""",2023,2023-07-03T22:50:53Z,,,
arXIv2023,Translating Latin with Artificial Intelligence,No.,1,"""No evidence""",2023,2023-07-03T16:27:32Z,,,
arXIv2023,SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions,No.,1,"""No evidence""",2023,2023-07-03T16:25:49Z,,,
arXIv2023,Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking,No.,1,"""No evidence""",2023,2023-07-03T16:19:50Z,,,
arXIv2023,ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience,No.,1,"""No evidence""",2023,2023-07-03T16:15:34Z,,,
arXIv2023,Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction,No.,1,"""No evidence""",2023,2023-07-03T16:01:45Z,,,
arXIv2023,Visual Instruction Tuning with Polite Flamingo,No.,1,"""No evidence""",2023,2023-07-03T13:37:00Z,,,
arXIv2023,Large Language and Text-to-3D Models for Engineering Design Optimization,No.,1,"""No evidence""",2023,2023-07-03T07:54:09Z,,,
arXIv2023,CollabKG: A Learnable Human-Machine-Cooperative Information Extraction Toolkit for (Event) Knowledge Graph Construction,No.,1,"""No evidence""",2023,2023-07-03T06:18:13Z,,,
arXIv2023,Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT),No.,1,"""No evidence""",2023,2023-07-03T03:17:20Z,,,
arXIv2023,From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy,No.,1,"""No evidence""",2023,2023-07-03T00:36:57Z,,,
arXIv2023,MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval,No.,1,"""No evidence""",2023,2023-07-02T15:11:59Z,,,
arXIv2023,TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition,No.,1,"""No evidence""",2023,2023-07-02T09:33:09Z,,,
arXIv2023,Large Language Models Enable Few-Shot Clustering,No.,1,"""No evidence""",2023,2023-07-02T09:17:11Z,,,
arXIv2023,PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation,No.,1,"""No evidence""",2023,2023-07-02T04:32:41Z,,,
arXIv2023,Conformer LLMs -- Convolution Augmented Large Language Models,No.,1,"""No evidence""",2023,2023-07-02T03:05:41Z,,,
arXIv2023,GenRec: Large Language Model for Generative Recommendation,No.,1,"""No evidence""",2023,2023-07-02T02:37:07Z,,,
arXIv2023,Understanding Counterspeech for Online Harm Mitigation,No.,1,"""No evidence""",2023,2023-07-01T20:54:01Z,,,
arXIv2023,CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model,No.,1,"""No evidence""",2023,2023-07-01T15:41:12Z,,,
arXIv2023,BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer,No.,1,"""No evidence""",2023,2023-07-01T15:10:01Z,,,
arXIv2023,DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment,No.,1,"""No evidence""",2023,2023-07-01T12:51:02Z,,,
arXIv2023,Let Me Teach You: Pedagogical Foundations of Feedback for Language Models,No.,1,"""No evidence""",2023,2023-07-01T09:18:24Z,,,
arXIv2023,InstructEval: Systematic Evaluation of Instruction Selection Methods,No.,1,"""No evidence""",2023,2023-07-01T07:45:38Z,,,
arXIv2023,THUIR2 at NTCIR-16 Session Search (SS) Task,No.,1,"""No evidence""",2023,2023-07-01T06:55:06Z,,,
arXIv2023,Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search,No.,1,"""No evidence""",2023,2023-07-01T02:26:34Z,,,
arXIv2023,How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain,No.,1,"""No evidence""",2023,2023-07-01T01:18:09Z,,,
arXIv2023,Personality Traits in Large Language Models,No.,1,"""No evidence""",2023,2023-07-01T00:58:51Z,,,
arxiv2024,Knowledge-Centric Templatic Views of Documents,Yes.,1,"""we consider each of these documents to be templatic views of the same underlying knowledge, and we aim to unify the generation and evaluation of these templatic views of documents. We begin by introducing an LLM-powered method to extract the most important information from an input document and represent this information in a structured format.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions,Yes.,2,"""The zero-shot performance of existing vision-language models (VLMs) such as CLIP is limited by the availability of large-scale, aligned image and text datasets in specific domains.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads,Yes.,1,"""The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview,Yes.,1,"""Most of the runs leveraged Large Language Models (LLMs) in their pipelines, with a few focusing on a generate-then-retrieve approach.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization,Yes.,1,"""Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Fast and Optimal Weight Update for Pruned Large Language Models,Yes.,1,"""Pruning large language models (LLMs) is a challenging task due to their enormous size.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray Report Labeling,Yes.,1,"""Traditional rule-based labeling methods fall short of capturing the nuances of diverse free-text patterns. Moreover, models using expert-annotated data are limited by data scarcity and pre-defined classes, impacting their performance, flexibility and scalability.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,A Survey of Resource-efficient LLM and Multimodal Foundation Models,Yes.,3,"""the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models,Yes.,3,"""almost all models perform poorly in complex subjects such as mathematics"" and ""most Chinese-dominant LLMs did not achieve higher scores at the primary school level compared to the middle school level"" and ""the mastery of higher-order knowledge by the model does not necessarily imply the mastery of lower-order knowledge as well.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Monte Carlo Tree Search for Recipe Generation using GPT-2,Yes.,3,"""Existing research on using LLMs to generate recipes has shown that LLMs can be finetuned to generate realistic-sounding recipes. However, on close examination, these generated recipes often fail to meet basic requirements like including chicken as an ingredient in chicken dishes.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,The Impact of Reasoning Step Length on Large Language Models,Yes.,3,"""However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"A Survey on the Applications of Frontier AI, Foundation Models, and Large Language Models to Intelligent Transportation Systems",Yes.,1,"""The paper further surveys interactions between LLMs and various aspects of ITS, exploring roles in traffic management, facilitating autonomous vehicles, and contributing to smart city development, while addressing challenges brought by frontier AI and foundation models.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model,Yes.,1,"""With recent notable successes, large language models (LLMs) demonstrate significant potential in attaining human-like intelligence and there has been a growing research area that employs LLMs as autonomous agents to obtain human-like decision-making capabilities.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models,Yes.,3,"""Further investigation into the effects of these methods on both model robustness and code security reveals that larger models tend to demonstrate reduced robustness and less security.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",Yes.,2,"""We begin (Part I) with a sustained defense of bibliotechnism against this challenge showing how even entirely novel text may be meaningful only in a derivative sense, and arguing that, in particular, much novel text generated by LLMs is only derivatively meaningful.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Assessing and Understanding Creativity in Large Language Models,Yes.,3,"""We found that the creativity of LLMs primarily falls short in originality, while excelling in elaboration.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Large Language Model Evaluation via Matrix Entropy,Yes.,1,"""Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,CharPoet: A Chinese Classical Poetry Generation System Based on Token-free LLM,Yes.,3,"""Large language models (LLMs) improve content control by allowing unrestricted user instructions, but the token-by-token generation process frequently makes format errors.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,Yes.,1,"""one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data,Yes.,2,"""utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models,Yes.,2,"""However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,A Preliminary Study on Using Large Language Models in Software Pentesting,Yes.,1,"""Large language models (LLM) are perceived to offer promising potentials for automating security tasks, such as those found in security operation centers (SOCs).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Large Language Model Adaptation for Financial Sentiment Analysis,Yes.,3,"""Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification,Yes.,2,"""However, some tasks still pose challenges for these models, including text classification with limited labels. This can result in a cold-start problem.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model,Yes.,1,"""By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models,Yes.,2,"""there is a large gap between the performance of LLMs on English and other languages"" and ""finetuning sometimes improves performance on low-resource languages, while degrading performance on high-resource languages.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Knowledge Distillation for Closed-Source Language Models,Yes.,3,"""due to the incapability to directly access the weights, hidden states, and output distributions of these closed-source models, the distillation can only be performed by fine-tuning smaller models with data samples generated by closed-source language models, which constrains the effectiveness of knowledge distillation.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,DataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure,Yes.,1,"""This paper introduces DataFrame question answering (QA), a novel task that utilizes large language models (LLMs) to generate Pandas queries for information retrieval and data analysis on dataframes, emphasizing safe and non-revealing data handling.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment,Yes.,1,"""we present the first comprehensive cross-supervision alignment experiment in the role-play domain, revealing that the intrinsic capabilities of LLMs confine the knowledge within role-play.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,AugSumm: towards generalizable speech summarization using synthetic labels from large language model,Yes.,1,"""We tackle this challenge by proposing AugSumm, a method to leverage large language models (LLMs) as a proxy for human annotators to generate augmented summaries for training and evaluation.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models,Yes.,1,"""Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,WARM: On the Benefits of Weight Averaged Reward Models,Yes.,3,"""Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives."" and ""We identify two primary challenges when designing RMs to mitigate reward hacking",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"A Fast, Performant, Secure Distributed Training Framework For Large Language Model",Yes.,3,"""maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Transfer Learning for Text Diffusion Models,Yes.,1,"""We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call 'AR2Diff'.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks,Yes.,3,"""Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems,Yes.,2,"""Large Language Models (LLMs) has shown exceptional capabilities in many natural language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs,Yes.,3,"""we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Cheetah: Natural Language Generation for 517 African Languages,Yes.,1,"""In this paper, we develop Cheetah, a massively multilingual NLG language model for African languages.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,A Comprehensive Study of Knowledge Editing for Large Language Models,Yes.,3,"""a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization"" and ""necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers,No.,1,"The paper discusses adversarial examples and text classifiers, but does not mention language models (LLMs or LMs).",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Contextualization Distillation from Large Language Model for Knowledge Graph Completion,Yes.,2,"""the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning,Yes.,3,"""Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,A Dataset and Benchmark for Copyright Protection from Text-to-Image Diffusion Models,No.,1,"The abstract focuses on text-to-image generation techniques and copyright protection, without mentioning language models.",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Few-Shot Detection of Machine-Generated Text using Style Representations,Yes.,3,"""model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of newer language models producing still more fluent text than the models used to train the detectors.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education,Yes.,2,"""Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC) Detectors for academic misconduct.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Reinforcement Learning for Optimizing RAG for Domain Chatbots,Yes.,1,"""With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models,Yes.,3,"""The experimental results show that although some models perform well in some tasks, there is still much room for improvement overall.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Parameter-Efficient Detoxification with Contrastive Decoding,Yes.,1,"""We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Zero Resource Cross-Lingual Part Of Speech Tagging,Yes.,1,"""Existing systems use two main techniques for POS tagging i.e. pretrained multilingual large language models(LLM) or project the source language labels into the zero resource target language and train a sequence labeling model on it.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction,Yes.,3,"""it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control,Yes.,3,"""it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Language Detection for Transliterated Content,Yes.,1,"""Emphasizing the pivotal role of comprehensive datasets for training Large Language Models LLMs like BERT.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Comparing Template-based and Template-free Language Model Probing,Yes.,1,"""The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Multi-Candidate Speculative Decoding,Yes.,3,"""the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection,Yes.,1,"""we propose AnomalyLLM, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models,Yes.,3,"""existing research typically focuses on limited tasks and often omits key multi-view and temporal information which is crucial for robust autonomous driving.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning,Yes.,3,"""However, the current LLMs' perceiving tool-use ability is limited to a single text query, which may result in ambiguity in understanding the users' real intentions.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series,Yes.,3,"""Large pre-trained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pre-training data."" and ""However, these models are typically very slow and large (~billion parameters) and do not consider cross-channel correlations.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese,Yes.,3,"""While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives,Yes.,1,"""Large Language Models increasingly rely on distributed techniques for their training and inference.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Towards Language-Driven Video Inpainting via Multimodal Large Language Models,Yes.,1,"""This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",Yes.,2,"""Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-based embedding representations for specialized domains remains uncertain."" and ""Despite challenges like content structuring and response latency, the advancements in LLMs are expected to encourage the use of Prompt-RAG.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions,Yes.,1,"""This work presents an unsupervised method for automatically constructing and expanding topic taxonomies using instruction-based fine-tuned LLMs (Large Language Models).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Training microrobots to swim by a large language model,Yes.,1,"""We discuss the nuanced aspects of prompt design, particularly emphasizing the reduction of monetary expenses of using GPT-4.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models",Yes.,1,"""In this paper, we introduce Uni3D-LLM, a unified framework that leverages a Large Language Model (LLM) to integrate tasks of 3D perception, generation, and editing within point cloud scenes.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges",Yes.,2,"""Some challenges still remain such as ensuring physical interpretation, nefarious use cases, and trustworthiness.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning,Yes.,2,"""existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption,Yes.,3,"""However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries."" and ""This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Analyzing the Effectiveness of Large Language Models on Text-to-SQL Synthesis,Yes.,3,"""Most if not all of the queries fall into these categories and it is insightful to understanding where the faults still lie with LLM program synthesis and where they can be improved.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,TeleChat Technical Report,Yes.,1,"""It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens,Yes.,3,"""we also observe irregularities in the machine--$\infty$-gram agreement level with respect to the suffix length, which indicates deficiencies in neural LLM pretraining and the positional embeddings of Transformers.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning,Yes.,1,"""However, the reasoning abilities of MLLMs have not been systematically investigated.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?,Yes.,3,"""We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not during the final step which relies on the problem's arithmetic expressions (solution execution).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping,Yes.,1,"""Built upon the tremendous success achieved by Large Language Models (LLMs) as the foundation models for language tasks, this paper discusses the challenges of building foundation models for geospatial artificial intelligence (GeoAI) vision tasks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Cross-target Stance Detection by Exploiting Target Analytical Perspectives,Yes.,1,"""formulating instructions based on large language model (LLM).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk,Yes.,3,"""specializing them towards fulfilling a specific function can be challenging"" and ""requires a number of data samples that a) might not be available or b) costly to generate.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Hierarchical Continual Reinforcement Learning via Large Language Model,Yes.,1,"""Hierarchical Continual reinforcement learning via large language model (Hi-Core), designed to facilitate the transfer of high-level knowledge.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study,Yes.,2,"""The transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods,Yes.,3,"""Previous subjective evaluation methods mainly rely on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation,Yes.,1,"""Only 33% of LLM-generated raw assertions had errors.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,DeepEdit: Knowledge Editing as Decoding with Constraints,Yes.,1,"""We propose a new perspective of knowledge editing (KE) for large language models (LLMs) that treats it as a constrained decoding problem.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Reinforcement learning for question answering in programming domain using public community scoring as a human feedback,Yes.,3,"""demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning,Yes.,2,"""Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Can Large Language Models Replace Economic Choice Prediction Labs?,Yes.,1,"""The AI community has recently contributed to that effort in two ways",2024,2024-01-01T00:00:00Z,,,
arxiv2024,TQCompressor: improving tensor decomposition methods in neural networks via permutations,Yes.,1,"""We explore the challenges posed by the computational and storage demands of pre-trained language models in NLP tasks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Evolving Code with A Large Language Model,Yes.,1,"""We cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models,Yes.,1,"""Large Language Models (LLMs) have upended decades of pedagogy in computing education.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media,Yes.,2,"""Recent advances in Large Language Models (LLMs) may pave the way to fabricate indistinguishable fake generated content at a much lower cost.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning,Yes.,1,"""The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization,Yes.,1,"""LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners,Yes.,2,"""highlighting the potential limitations of general LLMs as intelligent teaching assistants in computer programming courses.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Scientific Large Language Models: A Survey on Biological & Chemical Domains,Yes.,2,"""Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System,Yes.,3,"""prevailing models, both commercial and open-source, confront notable challenges",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task,Yes.,3,"""LLMs trained on multilingual datasets normally struggle to respond to prompts in Portuguese satisfactorily, presenting, for example, code switching in their responses.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Fine-tuning and Utilization Methods of Domain-specific LLMs,Yes.,2,"""The study explores the potential of LLMs in the financial domain, identifies limitations, and proposes directions for improvement.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation,Yes.,1,"""This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models,Yes.,1,"""Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark,Yes.,1,"""We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%, indicating a large space for improvement.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Prompting open-source and commercial language models for grammatical error correction of English learner text,Yes.,3,"""Our results indicate that LLMs do not always outperform supervised English GEC models except in specific contexts"" and ""We find that several open-source models outperform commercial ones on minimal edit benchmarks, and that in some settings zero-shot prompting is just as competitive as few-shot prompting.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis,Yes.,1,"""propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education,Yes.,2,"""While AI's promise in education, assessment, and tutoring is clear, challenges remain.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation,Yes.,1,"""Despite significant advancements in text-to-image models for generating high-quality images, these methods still struggle to ensure the controllability of text prompts over images in the context of complex text prompts, especially when it comes to retaining object attributes and relationships.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models,Yes.,1,"""This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Tuning Language Models by Proxy,Yes.,2,"""tuning these models has become increasingly resource-intensive, or impossible when model weights are private.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",Yes.,2,"""the potential misuse of this intelligence for malicious purposes presents significant risks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",Yes.,3,"""Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,General Flow as Foundation Affordance for Scalable Robot Learning,Yes.,1,"""Inspired by the success of large-scale auto-regressive prediction in Large Language Models (LLMs), we hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks,Yes.,1,"""This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues,Yes.,1,"""To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Pre-trained Large Language Models for Financial Sentiment Analysis,Yes.,1,"""In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples. To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs) [1, 2, 3] to solve this problem.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,A Vision Check-up for Language Models,Yes.,1,"""Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data,Yes.,2,"""Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Improving Domain Adaptation through Extended-Text Reading Comprehension,Yes.,3,"""regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access,Yes.,3,"""Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM",Yes.,3,"""While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,A match made in consistency heaven: when large language models meet evolutionary algorithms,Yes.,1,"""Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Towards Goal-oriented Large Language Model Prompting: A Survey,Yes.,2,"""highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study,Yes.,2,"""However, the problems of LLMs and RL model collaboration still need to be solved.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis,Yes.,2,"""While highlighting the limitations and the need for real-world testing, this study opens new avenues for AI applications in physical system control and sets a foundation for future advancements in AI-driven automation technologies.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents,Yes.,3,"""However, their capability in handling complex, multi-character social interactions has yet to be fully explored, primarily due to the absence of robust, quantitative evaluation methods.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Adaptive Text Watermark for Large Language Models,Yes.,2,"""it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty,Yes.,3,"""the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation,Yes.,3,"""even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4."" and ""we first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data,",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Identifying and Analyzing Task-Encoding Tokens in Large Language Models,Yes.,3,"""unexpectedly large changes in performance can arise from small changes in the prompt, leaving prompt design a largely empirical endeavour.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM,Yes.,3,"""LLM-generated testsuites still suffer from low coverage.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,ReGAL: Refactoring Programs to Discover Generalizable Abstractions,Yes.,3,"""While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity",Yes.,3,"""the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance"" and ""The paper identifies potential gaps and shortcomings in the legislative framework.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain,Yes.,1,"""Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Detecting mental disorder on social media: a ChatGPT-augmented explainable approach,Yes.,1,"""This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",Yes.,1,"""This data, comprising poisoning images equipped with prompts, is generated by leveraging the powerful capabilities of multimodal large language models and text-guided image inpainting techniques.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference,Yes.,2,"""Fine-tuning and inference with large Language Models (LM) are generally known to be expensive.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models,Yes.,1,"""This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters,Yes.,3,"""decisions around what data is retained or removed during this initial stage is under-scrutinized"" and ""we conduct the first study investigating how ten 'quality' and English language identification (langID) filters affect webpages that vary along these social dimensions.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior,Yes.,3,"""Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",Yes.,3,"""existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Transformers and Cortical Waves: Encoders for Pulling In Context Across Time,Yes.,1,"""The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,-CAUSAL: Exploring Defeasibility in Causal Reasoning,Yes.,2,"""We further demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and 10.7 points behind humans in generating supporters and defeaters, emphasizing the challenge posed by {\delta}-CAUSAL.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Arrows of Time for Large Language Models,Yes.,3,"""We empirically find a time asymmetry exhibited by such models in their ability to model natural language",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Leveraging Print Debugging to Improve Code Generation in Large Language Models,Yes.,3,"""Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes,Yes.,2,"""However, their performance in actual clinical applications has been underexplored."" and ""This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Improving Classification Performance With Human Feedback: Label a few, we label the rest",Yes.,1,"""By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Entity Recognition from Colloquial Text,Yes.,3,"""Despite the recent advances in training large language models for a variety of natural language processing tasks, the developed models and techniques have mainly focused on formal texts and do not perform as well on colloquial data, which is characterized by a number of distinct challenges.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",Yes.,1,"""Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs,Yes.,1,"""Recent prompting techniques, such as chain of thought, have consistently improved LLMs' performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs,Yes.,3,"""LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,A Study on Training and Developing Large Language Models for Behavior Tree Generation,Yes.,1,"""we propose a novel methodology that leverages the robust representation and reasoning abilities of LLMs.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search,Yes.,1,"""AEL combines the power of a large language model and the paradigm of evolutionary computation to design, combine, and modify algorithms automatically.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Multilingual Instruction Tuning With Just a Pinch of Multilinguality,Yes.,1,"""As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Empirical Study of Large Language Models as Automated Essay Scoring Tools in English Composition__Taking TOEFL Independent Writing Task for Example,Yes.,3,"""The primary objective is to assess the capabilities and constraints of ChatGPT, a prominent representative of large language models, within the context of automated essay scoring.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,DiffusionGPT: LLM-Driven Text-to-Image Generation System,Yes.,1,"""However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges,Yes.,3,"""These results underscore INACIA's potential in complex legal task handling while also acknowledging the current limitations.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Cross-lingual Editing in Multilingual Language Models,Yes.,3,"""The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distinct script families.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Scaling Sparse Fine-Tuning to Large Language Models,Yes.,3,"""Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters."" and ""their memory requirements increase proportionally to the size of the LLMs.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Wordflow: Social Prompt Engineering for Large Language Models,Yes.,1,"""Large language models (LLMs) require well-crafted prompts for effective use.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,De-identification is not always enough,Yes.,3,"""We observed that when synthetically generated notes closely match the performance of real data, they also exhibit similar privacy concerns to the real data.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Transformers are Multi-State RNNs,Yes.,3,"""They also lay out the option of mitigating one of their most painful computational bottlenecks - the size of their cache memory.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,DevEval: Evaluating Code Generation in Practical Software Projects,Yes.,3,"""Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Question Translation Training for Better Multilingual Reasoning,Yes.,3,"""Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT,Yes.,1,"""To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",Yes.,2,"""Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization,Yes.,1,"""Large language models (LLMs) such as ChatGPT and GPT-4, have established new benchmarks in a variety of NLP tasks, fundamentally altering the landscape of the field.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Agent Alignment in Evolving Social Norms,Yes.,2,"""The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Enhancing Recommendation Diversity by Re-ranking with Large Language Models,Yes.,2,"""We find that LLM-based re-ranking outperforms random re-ranking across all the metrics that we use but does not perform as well as the traditional re-ranking methods.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs,Yes.,2,"""Fine-tuning large pre-trained language models (LLMs) on particular datasets is a commonly employed strategy in Natural Language Processing (NLP) classification tasks. However, this approach usually results in a loss of models generalizability.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages",Yes.,2,"""One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages."" and ""MTInstruct is limited by weak cross-lingual signals inherent in the second challenge.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning,Yes.,3,"""recent attempts to use LLMs for vulnerability detection are still preliminary, as they lack an in-depth understanding of a subject LLM's vulnerability reasoning capability -- whether it originates from the model itself or from external assistance, such as invoking tool support and retrieving vulnerability knowledge.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models,Yes.,3,"""Traditional evaluation methods, such as ROUGE and BERTScore, which measure token similarity, often fail to capture the holistic semantic equivalence. This results in a low correlation with human judgments and intuition, which is especially problematic in high-stakes applications like healthcare and finance where reliability, safety, and robust decision-making are highly critical.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Towards Optimizing the Costs of LLM Usage,Yes.,2,"""enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets,Yes.,3,"""We also investigate claims that Large Language Models (LLMs) are effective as MT evaluators by evaluating on ACES. Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods fail to demonstrate reliable performance.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications,Yes.,1,"""Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,ChatGraph: Chat with Your Graphs,Yes.,1,"""To address the limitations, we propose a large language model (LLM)-based framework called ChatGraph.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Do We Need Language-Specific Fact-Checking Models? The Case of Chinese,Yes.,3,"""We first demonstrate the limitations of translation-based methods and multilingual large language models (e.g., GPT-4), highlighting the need for language-specific systems.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Large language model empowered participatory urban planning,Yes.,1,"""This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer,Yes.,1,"""CAT-LLM incorporates a bespoke, pluggable Text Style Definition (TSD) module aimed at comprehensively analyzing text features in articles, prompting LLMs to efficiently transfer Chinese article-style.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding,Yes.,1,"""the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Large Language Models Are Neurosymbolic Reasoners,Yes.,1,"""This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Extreme Compression of Large Language Models via Additive Quantization,Yes.,1,"""The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,An Exploratory Study on Automatic Identification of Assumptions in the Development of Deep Learning Frameworks,Yes.,3,"""Though ChatGPT is the most popular large language model, we do not recommend using it to identify assumptions in DL framework development because of its low performance on the task.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,InFoBench: Evaluating Instruction Following Ability in Large Language Models,Yes.,3,"""The evaluation of several advanced LLMs using this framework reveals their strengths and areas needing improvement, particularly in complex instruction-following.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment,Yes.,1,"""Despite recent advances, field experiments applying LLM-based tools in realistic settings are limited.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Towards Conversational Diagnostic AI,Yes.,2,"""Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,SliceGPT: Compress Large Language Models by Deleting Rows and Columns,Yes.,3,"""Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios",Yes.,1,"""existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection,Yes.,1,"""we propose a novel framework that leverages advanced AI techniques, including large language models and multi-prompt engineering.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Video Anomaly Detection and Explanation via Large Language Models,Yes.,3,"""We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Finetuning Large Language Models for Vulnerability Detection,Yes.,1,"""This demonstrates the potential for transfer learning by finetuning large pretrained language models for specialized source code analysis tasks.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Efficient Tool Use with Chain-of-Abstraction Reasoning,Yes.,2,"""there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models,Yes.,3,"""the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive"" and ""its high computational cost remains a barrier to its widespread applicability in the context of LLMs.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models,Yes.,3,"""To address challenges that still cannot be handled with the encoded knowledge of LLMs,"" and ""poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding,Yes.,1,"""The findings reveal GPT-4's superior performance over the other two LLMs and human cohorts in answering questions across various mechanics topics, except for Continuum Mechanics. This signals the potential future improvements for GPT models in handling symbolic calculations and tensor analyses.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning,Yes.,3,"""CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Generative Large Language Models are autonomous practitioners of evidence-based medicine,Yes.,3,"""Limitations were observed in terms of model ability to handle complex guidelines and diagnostic nuances.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis,Yes.,3,"""To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model's ability to capture the true semantic representation of visual scenes.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,YODA: Teacher-Student Progressive Learning for Language Models,Yes.,3,"""Although large language models (LLMs) have demonstrated adeptness in a range of tasks, they still lag behind human learning efficiency.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,ChemDFM: Dialogue Foundation Model for Chemistry,Yes.,3,"""the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Harnessing Large Language Models Over Transformer Models for Detecting Bengali Depressive Social Media Text: A Comprehensive Study,Yes.,1,"""The work emphasizes the effectiveness and flexibility of LLMs in a variety of linguistic circumstances, providing insightful information about the complex field of depression detection models.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"""You tell me"": A Dataset of GPT-4-Based Behaviour Change Support Conversations",Yes.,1,"""Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Automated Fact-Checking of Climate Change Claims with Large Language Models,Yes.,1,"""While our research is subject to certain limitations and necessitates careful interpretation, our approach holds significant potential.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing,Yes.,3,"""However, this approach requires a large memory and/or takes into the consideration the specific LM architecture. Moreover, due to the causal nature between the key-values in prior context and the queries at present, this approach cannot be extended to bidirectional attention such as in an encoder-decoder or PrefixLM decoder-only architecture.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,PRE: A Peer Review Based Large Language Model Evaluator,Yes.,3,"""Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware Debugging,Yes.,2,"""Despite the success of LLMs in automating various software development tasks, their application in the hardware security domain has been limited due to the constraints of commercial LLMs and the scarcity of domain specific data.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Misconfidence-based Demonstration Selection for LLM In-Context Learning,Yes.,3,"""However, its success hinges on carefully selecting demonstrations, which remains an obstacle in practice.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,On Detecting Cherry-picking in News Coverage Using Large Language Models,Yes.,1,"""Our approach relies on language models that consider contextual information from other news sources to classify statements based on their importance to the event covered in the target news story.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition,Yes.,3,"""Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Generating Zero-shot Abstractive Explanations for Rumour Verification,Yes.,1,"""To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia,Yes.,2,"""Overall, three LLM chatbots identify AD vs CN surpassing chance-levels but do not currently satisfy clinical application.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Critical Data Size of Language Models from a Grokking Perspective,Yes.,1,"""Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Veagle: Advancements in Multimodal Representation Learning,Yes.,3,"""While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Enhanced Automated Code Vulnerability Repair using Large Language Models,Yes.,2,"""The research also offers a critical assessment of current evaluation metrics, such as perfect predictions, and their limitations in reflecting the true capabilities of automated repair models in real-world scenarios.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems,Yes.,2,"""However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,TP-Aware Dequantization,Yes.,1,"""Our contribution is an optimized inference deployment scheme that address the current limitations of state-of-the-art quantization kernels when used in conjunction with Tensor Parallel (TP).""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries,Yes.,1,"""utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,"""Which LLM should I use?"": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students",Yes.,2,"""Evaluation for these tasks was carried out by pre-final year and final year undergraduate computer science students and provides insights into the models' strengths and limitations.""",2024,2024-01-01T00:00:00Z,,,
arxiv2024,Multi-hop Question Answering under Temporal Knowledge Editing,Yes.,3,"""existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts.""",2024,2024-03-30T23:22:51Z,,,
arxiv2024,PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression,Yes.,2,"""While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts.""",2024,2024-03-30T23:07:58Z,,,
arxiv2024,Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App,Yes.,1,"""MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being.""",2024,2024-03-30T23:01:34Z,,,
arxiv2024,Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4,Yes.,2,"""However, our novel methods did not produce more accurate results than GPT-4 in terms of faithfulness and consistency.""",2024,2024-03-30T22:27:21Z,,,
arxiv2024,MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks,Yes.,3,"""prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets.""",2024,2024-03-30T19:43:45Z,,,
arxiv2024,Do Vision-Language Models Understand Compound Nouns?,Yes.,3,"""Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs.""",2024,2024-03-30T16:54:45Z,,,
arxiv2024,CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP,Yes.,1,"""Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints.""",2024,2024-03-30T16:47:06Z,,,
arxiv2024,Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks,Yes.,3,"""While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field."" and ""Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems.""",2024,2024-03-30T14:09:00Z,,,
arxiv2024,Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation,Yes.,3,"""Recently, large language models (LLM) have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues.""",2024,2024-03-30T13:28:51Z,,,
arxiv2024,Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,Yes.,2,"""addresses the issue of class imbalance encountered in LLM-based annotations.""",2024,2024-03-30T12:13:57Z,,,
arxiv2024,Exploring Unseen Environments with Robots using Large Language and Vision Models through a Procedurally Generated 3D Scene Representation,Yes.,3,"""A challenging task in using LLMs to generate high level sub-goals is to efficiently represent the environment around the robot."" and ""But providing the LLM with a mass of contextual information (rich 3D scene semantic representation), can lead to redundant and inefficient plans.""",2024,2024-03-30T10:54:59Z,,,
arxiv2024,ST-LLM: Large Language Models Are Effective Temporal Learners,Yes.,2,"""to address the overhead and stability issues introduced by uncompressed video tokens within LLMs.""",2024,2024-03-30T10:11:26Z,,,
arxiv2024,Instruction-Driven Game Engines on Large Language Models,Yes.,1,"""The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game rules and autonomously generate game-play processes.""",2024,2024-03-30T08:02:16Z,,,
arxiv2024,Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits,Yes.,3,"""We also note that some theoretically established lexical-based linguistic markers lose their reliability as predictors when LLMs are used in the writing process.""",2024,2024-03-30T06:49:17Z,,,
arxiv2024,DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference,Yes.,3,"""current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference.""",2024,2024-03-30T04:34:54Z,,,
arxiv2024,A Survey of using Large Language Models for Generating Infrastructure as Code,Yes.,2,"""Finally, we conclude by presenting the challenges in this area and highlighting the scope for future research.""",2024,2024-03-30T02:57:55Z,,,
arxiv2024,Multi-Conditional Ranking with Large Language Models,Yes.,3,"""Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow.""",2024,2024-03-30T01:26:05Z,,,
arxiv2024,EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs,Yes.,3,"""Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability.""",2024,2024-03-30T01:16:37Z,,,
arxiv2024,GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs,Yes.,2,"""By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training.""",2024,2024-03-29T23:04:04Z,,,
arxiv2024,On-the-fly Definition Augmentation of LLMs for Biomedical NER,Yes.,3,"""Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data.""",2024,2024-03-29T20:59:27Z,,,
arxiv2024,ReALM: Reference Resolution As Language Modeling,Yes.,1,"""While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized.""",2024,2024-03-29T17:59:06Z,,,
arxiv2024,Gecko: Versatile Text Embeddings Distilled from Large Language Models,Yes.,1,"""distilling knowledge from large language models (LLMs) into a retriever.""",2024,2024-03-29T17:56:40Z,,,
arxiv2024,Convolutional Prompting meets Language Models for Continual Learning,Yes.,2,"""We further leverage Large Language Models to generate fine-grained text descriptions of each category which are used to get task similarity and dynamically decide the number of prompts to be learned.""",2024,2024-03-29T17:40:37Z,,,
arxiv2024,Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference,Yes.,3,"""Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models.""",2024,2024-03-29T17:22:48Z,,,
arxiv2024,"Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain",Yes.,3,"""Our analysis also uncovers the challenges of ensuring that LLM-generated suggestions are pertinent and useful, emphasizing the need for further research in this area.""",2024,2024-03-29T16:59:13Z,,,
arxiv2024,LayerNorm: A key component in parameter-efficient fine-tuning,No.,1,"The abstract does not mention LLMs or any limitations associated with them. It focuses on the fine-tuning of BERT, which is a general NLP model.",2024,2024-03-29T16:53:11Z,,,
arxiv2024,Using LLMs to Model the Beliefs and Preferences of Targeted Populations,Yes.,3,"""Existing work has had mixed success using LLMs to accurately model human behavior in different contexts.""",2024,2024-03-29T15:58:46Z,,,
arxiv2024,Shallow Cross-Encoders for Low-Latency Retrieval,Yes.,3,"""Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window.""",2024,2024-03-29T15:07:21Z,,,
arxiv2024,Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science,Yes.,3,"""Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training.""",2024,2024-03-29T14:41:21Z,,,
arxiv2024,"The Future of Combating Rumors? Retrieval, Discrimination, and Generation",Yes.,1,"""By using prompt engineering techniques, we feed results and knowledge into a LLM (Large Language Model), achieving satisfactory discrimination and explanatory effects while eliminating the need for fine-tuning, saving computational costs, and contributing to debunking efforts.""",2024,2024-03-29T14:32:41Z,,,
arxiv2024,Measuring Taiwanese Mandarin Language Understanding,Yes.,3,"""The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multilingual proprietary ones, and open-weight models tailored for Taiwanese Mandarin lag behind the Simplified-Chinese counterparts.""",2024,2024-03-29T13:56:21Z,,,
arxiv2024,ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models,Yes.,3,"""ChatGPT performs at par with fine-tuned models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, fake news, racial, gender, and cognitive biases.""",2024,2024-03-29T13:12:09Z,,,
arxiv2024,Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries,Yes.,1,"""In this work, we evaluate several state-of-the-art Large Language Models (LLMs), with and without fine-tuning, on our custom dataset for generating concise summaries from mental state examinations.""",2024,2024-03-29T12:25:37Z,,,
arxiv2024,User Modeling Challenges in Interactive AI Assistant Systems,Yes.,3,"""One of the remaining challenges is to understand user's mental states during the task for more personalized guidance."" and ""investigate the capabilities and challenges for large language models to interpret user profiles for more personalized user guidance.""",2024,2024-03-29T11:54:13Z,,,
arxiv2024,The Impact of Prompts on Zero-Shot Detection of AI-Generated Text,Yes.,2,"""While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns.""",2024,2024-03-29T11:33:34Z,,,
arxiv2024,PURPLE: Making a Large Language Model a Better SQL Writer,Yes.,3,"""However, LLMs sometimes fail to generate appropriate SQL due to their lack of knowledge in organizing complex logical operator composition.""",2024,2024-03-29T07:01:29Z,,,
arxiv2024,Large Language Model based Situational Dialogues for Second Language Learning,Yes.,2,"""Additionally, research in the field of dialogue systems still lacks reliable automatic evaluation metrics, leading to human evaluation as the gold standard (Smith et al., 2022), which is typically expensive.""",2024,2024-03-29T06:43:55Z,,,
arxiv2024,DiJiang: Efficient Large Language Models through Compact Kernelization,Yes.,2,"""the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters.""",2024,2024-03-29T02:32:15Z,,,
arxiv2024,Towards a Robust Retrieval-Based Summarization System,Yes.,3,"""While LLMs provide summarization capabilities, their performance in complex, real-world scenarios remains under-explored.""",2024,2024-03-29T00:14:46Z,,,
arxiv2024,Localizing Paragraph Memorization in Language Models,Yes.,3,"""memorized continuations are not only harder to unlearn, but also to corrupt than non-memorized ones.""",2024,2024-03-28T21:53:24Z,,,
arxiv2024,Target Span Detection for Implicit Harmful Content,Yes.,1,"""The collection is achieved using an innovative pooling method with matching scores based on human annotations and Large Language Models (LLMs).""",2024,2024-03-28T21:15:15Z,,,
arxiv2024,Developing Healthcare Language Model Embedding Spaces,Yes.,3,"""Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text.""",2024,2024-03-28T19:31:32Z,,,
arxiv2024,Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care,Yes.,1,"""We present and evaluate three different approaches for LLM-based, end-to-end ingestion of variable-length clinical EHR data to assist clinicians when triaging referrals.""",2024,2024-03-28T19:17:07Z,,,
arxiv2024,GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation,Yes.,3,"""We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability).""",2024,2024-03-28T18:08:22Z,,,
arxiv2024,MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions,Yes.,1,"""we can bring those implicit relations explicit by synthesizing instructions via large multimodal models (LMMs) and large language models (LLMs).""",2024,2024-03-28T17:59:20Z,,,
arxiv2024,Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis,Yes.,1,"""The Change-Agent integrates a multi-level change interpretation (MCI) model as the eyes and a large language model (LLM) as the brain.""",2024,2024-03-28T17:55:42Z,,,
arxiv2024,Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,Yes.,3,"""Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses.""",2024,2024-03-28T17:47:19Z,,,
arxiv2024,TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes,Yes.,1,"""integrates Relation Q-Former with LLaMA-Adapter to generate rich captions for these objects.""",2024,2024-03-28T17:12:55Z,,,
arxiv2024,WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models,Yes.,3,"""there has been little work in analyzing the impact that these perturbations have on the quality of generated texts.""",2024,2024-03-28T16:28:38Z,,,
arxiv2024,JDocQA: Japanese Document Question Answering Dataset for Generative Language Models,Yes.,1,"""We empirically evaluate the effectiveness of our dataset with text-based large language models (LLMs) and multimodal models.""",2024,2024-03-28T14:22:54Z,,,
arxiv2024,Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,Yes.,3,"""LLMs can inherit harmful biases and produce outputs that are not aligned with human values.""",2024,2024-03-28T14:15:10Z,,,
arxiv2024,BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation,Yes.,3,"""While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation.""",2024,2024-03-28T13:38:13Z,,,
arxiv2024,Checkpoint Merging via Bayesian Optimization in LLM Pretraining,Yes.,2,"""The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs.""",2024,2024-03-28T13:01:18Z,,,
arxiv2024,IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation,Yes.,3,"""However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object.""",2024,2024-03-28T11:52:42Z,,,
arxiv2024,Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation,Yes.,2,"""How to select informative examples remains an open issue.""",2024,2024-03-28T10:13:34Z,,,
arxiv2024,Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction,Yes.,3,"""However, applying LLMs to grammatical error correction (GEC) is still a challenging task.""",2024,2024-03-28T10:05:57Z,,,
arxiv2024,sDPO: Don't Use Your Data All at Once,Yes.,1,"""As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important.""",2024,2024-03-28T09:56:04Z,,,
arxiv2024,Dual-Personalizing Adapter for Federated Foundation Models,Yes.,3,"""However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications.""",2024,2024-03-28T08:19:33Z,,,
arxiv2024,Text Data-Centric Image Captioning with Interactive Prompts,Yes.,3,"""However, the current methods still face several challenges in adapting to the diversity of data configurations in a unified solution, accurately estimating image-text embedding bias, and correcting unsatisfactory prediction results in the inference stage.""",2024,2024-03-28T07:43:49Z,,,
arxiv2024,MUGC: Machine Generated versus User Generated Content Detection,Yes.,2,"""While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Language Models), may contribute to this high detection accuracy.""",2024,2024-03-28T07:33:53Z,,,
arxiv2024,Make Large Language Model a Better Ranker,Yes.,3,"""These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models"" and ""This shortfall is attributed to the misalignment between the objectives of ranking and language generation.""",2024,2024-03-28T07:22:16Z,,,
arxiv2024,Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering,Yes.,3,"""Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models.""",2024,2024-03-28T06:28:35Z,,,
arxiv2024,Disentangling Length from Quality in Direct Preference Optimization,Yes.,3,"""RLHF is known to exploit biases in human preferences, such as verbosity.""",2024,2024-03-28T06:03:47Z,,,
arxiv2024,Compressing Large Language Models by Streamlining the Unimportant Layer,Yes.,3,"""Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models.""",2024,2024-03-28T04:12:13Z,,,
arxiv2024,Code Comparison Tuning for Code Large Language Models,Yes.,1,"""We present Code Comparison Tuning (CCT), a simple and effective tuning method for code large language models (Code LLMs) to better handle subtle code errors.""",2024,2024-03-28T03:25:23Z,,,
arxiv2024,MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering,Yes.,1,"""Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts.""",2024,2024-03-28T03:14:18Z,,,
arxiv2024,Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation,Yes.,1,"""Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images.""",2024,2024-03-28T02:35:53Z,,,
arxiv2024,CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems,Yes.,1,"""In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection.""",2024,2024-03-27T23:45:31Z,,,
arxiv2024,Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data,Yes.,2,"""LLM-annotated data without human guidance for training light-weight supervised classification models is an ineffective strategy.""",2024,2024-03-27T22:05:10Z,,,
arxiv2024,Towards LLM-RecSys Alignment with Textual ID Learning,Yes.,3,"""current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations.""",2024,2024-03-27T21:22:37Z,,,
arxiv2024,TextCraftor: Your Text Encoder Can be Image Quality Controller,No.,1,The abstract does not mention large language models (LLMs) or their limitations. It focuses on diffusion-based text-to-image generative models and the text encoder in Stable Diffusion.,2024,2024-03-27T19:52:55Z,,,
arxiv2024,A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products,Yes.,3,"""Our systematic review of grey literature identifies common challenges in deploying LLMs, ranging from pre-training and fine-tuning to user experience considerations.""",2024,2024-03-27T19:02:56Z,,,
arxiv2024,Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment,No.,1,The abstract focuses on a GPT-based model for dance accompaniment and does not discuss language models or their limitations.,2024,2024-03-27T17:57:02Z,,,
arxiv2024,Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation,Yes.,3,"""This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve.""",2024,2024-03-27T17:50:00Z,,,
arxiv2024,CheckEval: Robust Evaluation Framework using Large Language Model via Checklist,Yes.,2,"""addressing the challenges of ambiguity and inconsistency in current evaluation methods"" and ""CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation.""",2024,2024-03-27T17:20:39Z,,,
arxiv2024,Understanding the Learning Dynamics of Alignment with Human Feedback,Yes.,3,"""theoretically understanding how these methods affect model behavior remains an open question"" and ""the optimization is prone to prioritizing certain behaviors with higher preference distinguishability.""",2024,2024-03-27T16:39:28Z,,,
arxiv2024,The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian,Yes.,3,"""We show that this is a challenging benchmark where current language models are bound by 60% accuracy.""",2024,2024-03-27T15:46:25Z,,,
arxiv2024,SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens,Yes.,1,"""We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT).""",2024,2024-03-27T14:54:27Z,,,
arxiv2024,"A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks",Yes.,1,"""In this paper, we sketch a proof of principle for such a method using large language models (LLMs), expert legal systems known as legal decision paths, and Bayesian networks.""",2024,2024-03-27T13:12:57Z,,,
arxiv2024,Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval,Yes.,2,"""the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored.""",2024,2024-03-27T09:46:56Z,,,
arxiv2024,Improving Attributed Text Generation of Large Language Models via Preference Learning,Yes.,3,"""Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content.""",2024,2024-03-27T09:19:13Z,,,
arxiv2024,LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models,Yes.,1,"""we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs).""",2024,2024-03-27T08:34:55Z,,,
arxiv2024,IterAlign: Iterative Constitutional Alignment of Large Language Models,Yes.,3,"""However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming.""",2024,2024-03-27T08:32:19Z,,,
arxiv2024,Toward Interactive Regional Understanding in Vision-Large Language Models,Yes.,3,"""these models heavily rely on image-text pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability.""",2024,2024-03-27T05:22:06Z,,,
arxiv2024,Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models,Yes.,3,"""the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning.""",2024,2024-03-27T04:49:23Z,,,
arxiv2024,Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges,Yes.,3,"""Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency.""",2024,2024-03-27T04:39:18Z,,,
arxiv2024,Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,Yes.,1,"""Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge.""",2024,2024-03-27T04:20:18Z,,,
arxiv2024,Leveraging Large Language Models for Fuzzy String Matching in Political Science,Yes.,1,"""we propose to use large language models to entirely sidestep this problem in an easy and intuitive manner.""",2024,2024-03-27T03:04:21Z,,,
arxiv2024,Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models,Yes.,3,"""However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices.""",2024,2024-03-26T23:51:44Z,,,
arxiv2024,Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency,Yes.,2,"""We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers."" and ""Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings.""",2024,2024-03-26T23:32:52Z,,,
arxiv2024,Large Language Models Produce Responses Perceived to be Empathic,Yes.,1,"""Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy.""",2024,2024-03-26T23:14:34Z,,,
arxiv2024,Juru: Legal Brazilian Large Language Model from Reputable Sources,Yes.,3,"""However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language.""",2024,2024-03-26T22:54:12Z,,,
arxiv2024,For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers,Yes.,3,"""it is not well understood how unclear or nonstandard language queries affect the model outputs.""",2024,2024-03-26T22:08:33Z,,,
arxiv2024,Large Language Models for Education: A Survey and Outlook,Yes.,3,"""identify the risks and challenges associated with deploying LLMs in education.""",2024,2024-03-26T21:04:29Z,,,
arxiv2024,Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models,Yes.,2,"""error analysis reveals several existing issues in the retrieval system that still need resolution.""",2024,2024-03-26T20:25:53Z,,,
arxiv2024,PerOS: Personalized Self-Adapting Operating Systems in the Cloud,Yes.,1,"""the rise of large language models (LLMs) in ML has introduced transformative capabilities, reshaping user interactions and software development paradigms.""",2024,2024-03-26T20:10:31Z,,,
arxiv2024,COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning,Yes.,3,"""However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks.""",2024,2024-03-26T19:24:18Z,,,
arxiv2024,Supervisory Prompt Training,Yes.,2,"""The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable.""",2024,2024-03-26T19:08:20Z,,,
arxiv2024,Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER,Yes.,3,"""Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains.""",2024,2024-03-26T18:23:16Z,,,
arxiv2024,LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning,Yes.,2,"""their huge memory consumption has become a major roadblock to large-scale training.""",2024,2024-03-26T17:55:02Z,,,
arxiv2024,Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications,Yes.,2,"""we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks.""",2024,2024-03-26T16:49:25Z,,,
arxiv2024,ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages,Yes.,1,"""Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models.""",2024,2024-03-26T16:48:13Z,,,
arxiv2024,Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs,Yes.,3,"""little work has been done to establish the degree to which language models capture this type of generalization"" and ""We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7B parameter Mistral displays as little difference between its baseline performance on the natural",2024,2024-03-26T16:45:27Z,,,
arxiv2024,ArabicaQA: A Comprehensive Dataset for Arabic Question Answering,Yes.,1,"""our study includes extensive benchmarking of large language models (LLMs) for Arabic question answering, critically evaluating their performance in the Arabic language context.""",2024,2024-03-26T16:37:54Z,,,
arxiv2024,Assessment of Multimodal Large Language Models in Alignment with Human Values,Yes.,3,"""despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations.""",2024,2024-03-26T16:10:21Z,,,
arxiv2024,Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs),Yes.,2,"""We explore various roles that LLMs can play in this context while identifying some of the challenges to address.""",2024,2024-03-26T15:54:48Z,,,
arxiv2024,Are Compressed Language Models Less Subgroup Robust?,Yes.,3,"""To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset.""",2024,2024-03-26T15:50:37Z,,,
arxiv2024,Improving Text-to-Image Consistency via Automatic Prompt Optimization,Yes.,1,"""introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models.""",2024,2024-03-26T15:42:01Z,,,
arxiv2024,Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement,Yes.,1,"""we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed 'Topic Refinement'.""",2024,2024-03-26T13:50:34Z,,,
arxiv2024,ExpressEdit: Video Editing with Natural Language and Sketching,Yes.,1,"""Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching.""",2024,2024-03-26T13:34:21Z,,,
arxiv2024,Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes,Yes.,1,"""The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains.""",2024,2024-03-26T13:32:32Z,,,
arxiv2024,Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games,Yes.,1,"""Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations.""",2024,2024-03-26T13:02:46Z,,,
arxiv2024,Language Models for Text Classification: Is In-Context Learning Enough?,Yes.,3,"""In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.""",2024,2024-03-26T12:47:39Z,,,
arxiv2024,"""You are an expert annotator"": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling",Yes.,2,"""Regression is considered more challenging than classification",2024,2024-03-26T11:45:22Z,,,
arxiv2024,Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction,Yes.,1,"""Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC).""",2024,2024-03-26T09:43:15Z,,,
arxiv2024,ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler,Yes.,1,"""Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks.""",2024,2024-03-26T09:41:21Z,,,
arxiv2024,DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation,Yes.,3,"""However, due to the hallucination problem of LLM, it is often necessary to improve the reliability of the results through multi-round query prompt approach such as Graph of Thoughts (GoT), which also brings additional reasoning costs.""",2024,2024-03-26T08:47:23Z,,,
arxiv2024,LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction,Yes.,1,"""we find that LM-Combiner still has a good rewriting performance even with small parameters and few training data, and thus can cost-effectively mitigate the over-correction of black-box GEC systems (e.g., ChatGPT).""",2024,2024-03-26T06:12:21Z,,,
arxiv2024,Disambiguate Entity Matching through Relation Discovery with Large Language Models,Yes.,1,"""Entity matching is a critical challenge in data integration and cleaning, central to tasks like fuzzy joins and deduplication. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT.""",2024,2024-03-26T03:07:32Z,,,
arxiv2024,The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge,Yes.,2,"""we recognize a discrepancy between the primary use of maximum likelihood estimation during text generation and the evaluation metrics such as ROUGE employed to assess the quality of generated captions.""",2024,2024-03-26T03:03:50Z,,,
arxiv2024,JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset,Yes.,3,"""we identified limitations in the task completion capabilities of LLMs in Japanese.""",2024,2024-03-26T02:01:18Z,,,
arxiv2024,ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching,Yes.,3,"""Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature.""",2024,2024-03-26T01:46:34Z,,,
arxiv2024,Automate Knowledge Concept Tagging on Math Questions with LLMs,Yes.,1,"""In this paper, we explore automating the tagging task using Large Language Models (LLMs), in response to the inability of prior manual methods to meet the rapidly growing demand for concept tagging in questions posed by advanced educational applications.""",2024,2024-03-26T00:09:38Z,,,
arxiv2024,TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models,Yes.,3,"""large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences.""",2024,2024-03-25T22:47:13Z,,,
arxiv2024,SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies,Yes.,3,"""We also observed that model performance with simulated data was on par with using the real data for training in most evaluation scenarios. We conclude by discussing the potential implications of SeSaMe in addressing some challenges researchers face with ground-truth collection in passive sensing studies.""",2024,2024-03-25T21:48:22Z,,,
arxiv2024,Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis,Yes.,3,"""We also find that the task is highly challenging for Large Language Models, even after fine-tuning.""",2024,2024-03-25T21:46:35Z,,,
arxiv2024,Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation,Yes.,3,"""Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code.""",2024,2024-03-25T21:41:31Z,,,
arxiv2024,Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node,Yes.,1,"""a system powered by large language models is designed and implemented to process 'semantic node' and generate AAS instance models from textual technical data.""",2024,2024-03-25T21:37:30Z,,,
arxiv2024,Outcome-Constrained Large Language Models for Countering Hate Speech,Yes.,2,"""However, it remains unclear what impact the counterspeech might have in an online environment.""",2024,2024-03-25T19:44:06Z,,,
arxiv2024,MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models,Yes.,3,"""existing methods are parameter-adherent to the policy model, leading to two key limitations",2024,2024-03-25T19:28:10Z,,,
arxiv2024,"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair",Yes.,1,"""This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM).""",2024,2024-03-25T19:17:43Z,,,
arxiv2024,"Attribute First, then Generate: Locally-attributable Grounded Text Generation",Yes.,3,"""Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections.""",2024,2024-03-25T18:41:47Z,,,
arxiv2024,DreamLIP: Language-Image Pre-training with Long Captions,Yes.,1,"""we first re-caption 30M images with detailed descriptions using a pre-trained Multi-modality Large Language Model (MLLM)""",2024,2024-03-25T17:59:42Z,,,
arxiv2024,Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows,No.,1,The abstract does not mention LLMs or their limitations. It focuses on diffusion language models and a proposed method called Language Rectified Flow.,2024,2024-03-25T17:58:22Z,,,
arxiv2024,Comp4D: LLM-Guided Compositional 4D Scene Generation,Yes.,1,"""Utilizing Large Language Models (LLMs), the framework begins by decomposing an input text prompt into distinct entities and maps out their trajectories.""",2024,2024-03-25T17:55:52Z,,,
arxiv2024,AIOS: LLM Agent Operating System,Yes.,3,"""Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations.""",2024,2024-03-25T17:32:23Z,,,
arxiv2024,PropTest: Automatic Property Testing for Improved Visual Programming,Yes.,1,"""This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data.""",2024,2024-03-25T16:39:15Z,,,
arxiv2024,Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data,Yes.,3,"""we conduct analyzes to uncover the types of stressors it assigns to demographic groups, which could be used to test the limitations of LLMs for synthetic data generation for depression data.""",2024,2024-03-25T16:21:25Z,,,
arxiv2024,Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making,Yes.,1,"""To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision.""",2024,2024-03-25T14:34:06Z,,,
arxiv2024,An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems,Yes.,1,"""We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization.""",2024,2024-03-25T14:32:28Z,,,
arxiv2024,CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment,Yes.,2,"""a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training.""",2024,2024-03-25T11:37:15Z,,,
arxiv2024,Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units,Yes.,3,"""Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities."" and ""substantial progress, especially in the realm of Large Language Models, remains lacking.""",2024,2024-03-25T10:39:18Z,,,
arxiv2024,NSINA: A News Corpus for Sinhala,Yes.,2,"""their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges",2024,2024-03-25T09:36:51Z,,,
arxiv2024,Harnessing the power of LLMs for normative reasoning in MASs,Yes.,2,"""We also highlight challenges in this emerging field.""",2024,2024-03-25T08:09:01Z,,,
arxiv2024,LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,Yes.,1,"""Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks.""",2024,2024-03-25T07:38:40Z,,,
arxiv2024,"Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm",Yes.,3,"""Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations.""",2024,2024-03-25T06:17:54Z,,,
arxiv2024,CodeS: Natural Language to Code Repository via Multi-Layer Sketch,Yes.,1,"""The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development.""",2024,2024-03-25T06:09:55Z,,,
arxiv2024,InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models,Yes.,1,"""This paper introduces InstUPR, an unsupervised passage reranking method based on large language models (LLMs).""",2024,2024-03-25T05:31:22Z,,,
arxiv2024,$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models,Yes.,2,"""raising concerns about the adversarial vulnerability of this paradigm"" and ""Extensive results demonstrate the effectiveness of $\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo.""",2024,2024-03-25T05:27:35Z,,,
arxiv2024,Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation,Yes.,3,"""However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations. Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as",2024,2024-03-25T05:12:18Z,,,
arxiv2024,An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations,Yes.,3,"""The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness.""",2024,2024-03-25T05:04:52Z,,,
arxiv2024,Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases,Yes.,3,"""discover the limitations of unified information extraction and large language models in solving definition bias.""",2024,2024-03-25T03:19:20Z,,,
arxiv2024,Concurrent Linguistic Error Detection (CLED) for Large Language Models,Yes.,3,"""Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue.""",2024,2024-03-25T03:17:27Z,,,
arxiv2024,Dia-LLaMA: Towards Large Language Model-driven CT Report Generation,Yes.,1,"""Recently, LLM has shown a great ability to generate reliable answers with appropriate prompts, which shed light on addressing the aforementioned challenges.""",2024,2024-03-25T03:02:51Z,,,
arxiv2024,"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",Yes.,1,"""We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images.""",2024,2024-03-25T03:02:27Z,,,
arxiv2024,ChatDBG: An AI-Powered Debugging Assistant,Yes.,1,"""ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers.""",2024,2024-03-25T01:12:57Z,,,
arxiv2024,Enhanced Facet Generation with LLM Editing,Yes.,1,"""The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model.""",2024,2024-03-25T00:43:44Z,,,
arxiv2024,Is Watermarking LLM-Generated Code Robust?,Yes.,3,"""we show that it is easy to remove these watermarks on code by semantic-preserving transformations.""",2024,2024-03-24T21:41:29Z,,,
arxiv2024,Large Language Models in Biomedical and Health Informatics: A Bibliometric Review,Yes.,3,"""Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations.""",2024,2024-03-24T21:29:39Z,,,
arxiv2024,Engineering Safety Requirements for Autonomous Driving with Large Language Models,Yes.,1,"""Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update.""",2024,2024-03-24T20:40:51Z,,,
arxiv2024,AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue,Yes.,1,"""While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited.""",2024,2024-03-24T19:50:49Z,,,
arxiv2024,Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling,Yes.,2,"""Through in-depth experiments and evaluation, we summarise the advantages and constraints of employing LLMs in topic extraction.""",2024,2024-03-24T17:39:51Z,,,
arxiv2024,CoverUp: Coverage-Guided LLM-Based Test Generation,Yes.,1,"""This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs).""",2024,2024-03-24T16:18:27Z,,,
arxiv2024,ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models,Yes.,2,"""Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks.""",2024,2024-03-24T15:09:55Z,,,
arxiv2024,Opportunities and challenges in the application of large artificial intelligence models in radiology,Yes.,2,"""Finally, this paper also summarizes some of the challenges of large AI models in radiology.""",2024,2024-03-24T12:05:23Z,,,
arxiv2024,Can Language Models Pretend Solvers? Logic Code Simulation with LLMs,Yes.,3,"""What strength arises along with logic code simulation? And what pitfalls?""",2024,2024-03-24T11:27:16Z,,,
arxiv2024,LLMs as Compiler for Arabic Programming Language,Yes.,1,"""In this paper we introduce APL (Arabic Programming Language) that uses Large language models (LLM) as semi-compiler to covert Arabic text code to python code then run the code.""",2024,2024-03-24T10:57:08Z,,,
arxiv2024,Argument Quality Assessment in the Age of Instruction-Following Large Language Models,Yes.,2,"""We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems.""",2024,2024-03-24T10:43:21Z,,,
arxiv2024,Qibo: A Large Language Model for Traditional Chinese Medicine,Yes.,3,"""the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources.""",2024,2024-03-24T07:48:05Z,,,
arxiv2024,Monotonic Paraphrasing Improves Generalization of Language Model Prompting,Yes.,2,"""Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity.""",2024,2024-03-24T06:49:07Z,,,
arxiv2024,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,Yes.,3,"""While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques.""",2024,2024-03-24T04:34:34Z,,,
arxiv2024,LlamBERT: Large-scale low-cost data annotation in NLP,Yes.,2,"""Despite their effectiveness, the high costs associated with their use pose a challenge.""",2024,2024-03-23T21:54:34Z,,,
arxiv2024,Leveraging Zero-Shot Prompting for Efficient Language Model Distillation,Yes.,1,"""This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor.""",2024,2024-03-23T16:51:52Z,,,
arxiv2024,Using Large Language Models for OntoClean-based Ontology Refinement,Yes.,1,"""This paper explores the integration of Large Language Models (LLMs) such as GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing on the OntoClean methodology.""",2024,2024-03-23T15:09:50Z,,,
arxiv2024,When LLM-based Code Generation Meets the Software Development Process,Yes.,1,"""This paper introduces LCG, a code generation framework inspired by established software engineering practices. LCG leverages multiple Large Language Model (LLM) agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum.""",2024,2024-03-23T14:04:48Z,,,
arxiv2024,Computational Sentence-level Metrics Predicting Human Sentence Comprehension,Yes.,1,"""This study introduces innovative methods for computing sentence-level metrics using multilingual large language models.""",2024,2024-03-23T12:19:49Z,,,
arxiv2024,AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving,Yes.,3,"""existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs.""",2024,2024-03-23T10:42:49Z,,,
arxiv2024,Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study,Yes.,1,"""A large language model can quickly summarise information in less time than a human.""",2024,2024-03-23T07:59:30Z,,,
arxiv2024,Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models,Yes.,2,"""How can everyday web users confirm if LLMs misuse their data without permission?"" and ""To explore the effectiveness and usage of this copyrighting tool, we define the user training data identification task with ghost sentences.""",2024,2024-03-23T06:36:32Z,,,
arxiv2024,LLMs Instruct LLMs:An Extraction and Editing Method,Yes.,3,"""The interest in updating Large Language Models (LLMs) without retraining from scratch is substantial, yet it comes with some challenges. This is especially true for situations demanding complex reasoning with limited samples, a scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation for LLM",2024,2024-03-23T06:03:36Z,,,
arxiv2024,Towards a RAG-based Summarization Agent for the Electron-Ion Collider,Yes.,1,"""second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data.""",2024,2024-03-23T05:32:46Z,,,
arxiv2024,Contact-aware Human Motion Generation from Textual Descriptions,No.,1,The abstract does not mention LLMs or any limitations related to them.,2024,2024-03-23T04:08:39Z,,,
arxiv2024,SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models,Yes.,1,"""To address these issues, we resort to using the large language model (LLM) to drive the procedural modeling.""",2024,2024-03-23T03:23:29Z,,,
arxiv2024,MixRED: A Mix-lingual Relation Extraction Dataset,Yes.,3,"""we evaluate both state-of-the-art supervised models and large language models (LLMs) on MixRED, revealing their respective advantages and limitations in the mix-lingual scenario.""",2024,2024-03-23T03:18:14Z,,,
arxiv2024,EAGLE: A Domain Generalization Framework for AI-generated Text Detection,Yes.,2,"""With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models."" and ""building supervised detectors for identifying text from such new models would require new labeled training data, which",2024,2024-03-23T02:44:20Z,,,
arxiv2024,Differentially Private Next-Token Prediction of Large Language Models,Yes.,3,"""DP-SGD overestimates an adversary's capabilities in having white box access to the model and, as a result, causes longer training times and larger memory usage than SGD.""",2024,2024-03-22T22:27:44Z,,,
arxiv2024,Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers,Yes.,2,"""Concerns about the security implications of this trend have been raised"" and ""Our findings suggest developers are under-educated on insecure code propagation from both platforms.""",2024,2024-03-22T20:06:41Z,,,
arxiv2024,"Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors",Yes.,1,"""This study delves into university instructors' experiences and attitudes toward AI language models, filling a gap in the literature by analyzing educators' perspectives on AI's role in the classroom and its potential impacts on teaching and learning.""",2024,2024-03-22T19:21:29Z,,,
arxiv2024,MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis,Yes.,3,"""Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance.""",2024,2024-03-22T19:19:51Z,,,
arxiv2024,LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers,Yes.,2,"""We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities.""",2024,2024-03-22T17:31:43Z,,,
arxiv2024,Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization,Yes.,1,"""SID is generated utilizing multimodal GPT-4 and can be seamlessly integrated into optimization-based models.""",2024,2024-03-22T16:35:38Z,,,
arxiv2024,Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review,Yes.,3,"""Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future development.""",2024,2024-03-22T15:16:23Z,,,
arxiv2024,Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models,Yes.,3,"""Retrieval-Augmented-Generation and Generation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results",2024,2024-03-22T15:06:45Z,,,
arxiv2024,Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach,Yes.,3,"""targeting the inadequacies in current evaluation methods"" and ""challenge assumptions about emergent abilities and the influence of given training types and architectures in LLMs.""",2024,2024-03-22T14:47:35Z,,,
arxiv2024,FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions,Yes.,3,"""Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information.""",2024,2024-03-22T14:42:29Z,,,
arxiv2024,Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models,Yes.,1,"""we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs),"" and ""The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed.""",2024,2024-03-22T14:20:34Z,,,
arxiv2024,InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection,Yes.,3,"""Large Language Models (LLMs) raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media."" and ""Our investigations show that the objectives of fidelity and utility may conflict and that prompt engineering is a useful but insufficient strategy. Additionally, we find that while individual synthetic posts may appear realistic, collectively they lack diversity, topic connectivity",2024,2024-03-22T13:58:42Z,,,
arxiv2024,MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection,Yes.,1,"""we propose a novel Multispectral Chain-of-Thought Detection (MSCoTDet) framework, which incorporates Large Language Models (LLMs) to understand the complementary information at the semantic level and further enhance the fusion process.""",2024,2024-03-22T13:50:27Z,,,
arxiv2024,CACA Agent: Capability Collaboration based AI Agent,Yes.,3,"""Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality.""",2024,2024-03-22T11:42:47Z,,,
arxiv2024,Text clustering with LLM embeddings,Yes.,2,"""we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models.""",2024,2024-03-22T11:08:48Z,,,
arxiv2024,Construction of a Japanese Financial Benchmark for Large Language Models,Yes.,1,"""With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity.""",2024,2024-03-22T09:40:27Z,,,
arxiv2024,LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement,Yes.,2,"""While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging.""",2024,2024-03-22T08:57:07Z,,,
arxiv2024,MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts,Yes.,1,"""We employ large language models (LLMs) to solve this task through several prompting techniques.""",2024,2024-03-22T06:31:49Z,,,
arxiv2024,Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation,Yes.,2,"""However, our study acknowledges that there are limitations to the proposed approach.""",2024,2024-03-22T05:37:52Z,,,
arxiv2024,Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices,No.,1,The abstract does not mention LLMs or any specific limitations of language models.,2024,2024-03-22T05:23:31Z,,,
arxiv2024,Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation,Yes.,3,"""Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses.""",2024,2024-03-22T05:05:45Z,,,
arxiv2024,AutoRE: Document-Level Relation Extraction with Large Language Models,Yes.,3,"""Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple",2024,2024-03-21T23:48:21Z,,,
arxiv2024,VidLA: Video-Language Alignment at Scale,Yes.,1,"""we leverage recent LLMs to curate the largest video-language dataset to date with better visual grounding.""",2024,2024-03-21T22:36:24Z,,,
arxiv2024,Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models,Yes.,3,"""Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-",2024,2024-03-21T22:08:44Z,,,
arxiv2024,VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding,Yes.,1,"""Recent studies have demonstrated the effectiveness of Large Language Models (LLMs) as reasoning modules that can deconstruct complex tasks into more manageable sub-tasks, particularly when applied to visual reasoning tasks for images.""",2024,2024-03-21T18:00:00Z,,,
arxiv2024,MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?,Yes.,3,"""However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood.""",2024,2024-03-21T17:59:50Z,,,
arxiv2024,Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey,Yes.,3,"""However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities.""",2024,2024-03-21T17:55:50Z,,,
arxiv2024,A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,Yes.,2,"""A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.""",2024,2024-03-21T17:09:08Z,,,
arxiv2024,The Era of Semantic Decoding,Yes.,2,"""Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs.""",2024,2024-03-21T17:06:17Z,,,
arxiv2024,EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling,Yes.,2,"""However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity.""",2024,2024-03-21T16:41:12Z,,,
arxiv2024,ChatGPT Alternative Solutions: Large Language Models Survey,Yes.,3,"""our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories.""",2024,2024-03-21T15:16:50Z,,,
arxiv2024,gTBLS: Generating Tables from Text by Conditional Question Answering,Yes.,1,"""Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible.""",2024,2024-03-21T15:04:32Z,,,
arxiv2024,Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning,Yes.,3,"""off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models.""",2024,2024-03-21T13:47:40Z,,,
arxiv2024,From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision,Yes.,3,"""However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight.""",2024,2024-03-21T13:29:54Z,,,
arxiv2024,Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics,Yes.,3,"""Considering the information asymmetry problem caused by redundant class semantics annotated with large language models (LLMs).""",2024,2024-03-21T12:45:01Z,,,
arxiv2024,Exploring the Potential of Large Language Models in Graph Generation,Yes.,3,"""We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance.""",2024,2024-03-21T12:37:54Z,,,
arxiv2024,Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives,Yes.,3,"""We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts.""",2024,2024-03-21T12:17:59Z,,,
arxiv2024,ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting,Yes.,3,"""Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts."" and ""To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method.""",2024,2024-03-21T11:34:26Z,,,
arxiv2024,From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora,Yes.,3,"""users' reliance on AI-generated content, such as the ones produced by Large Language Models, which can manifest both human biases hidden in training sets and non-human biases that emerge from their artificial neural architectures.""",2024,2024-03-21T11:04:41Z,,,
arxiv2024,Multi-role Consensus through LLMs Discussions for Vulnerability Detection,Yes.,2,"""Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers.""",2024,2024-03-21T10:28:18Z,,,
arxiv2024,LLM-based Extraction of Contradictions from Patents,Yes.,3,"""While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models.""",2024,2024-03-21T09:36:36Z,,,
arxiv2024,ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification,Yes.,3,"""it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.""",2024,2024-03-21T09:28:38Z,,,
arxiv2024,LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding,Yes.,2,"""Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate.""",2024,2024-03-21T09:25:24Z,,,
arxiv2024,Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology,Yes.,1,"""The workflow integrates large language, transformer-based vision models and sophisticated machine learning tools.""",2024,2024-03-21T09:02:17Z,,,
arxiv2024,PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning,Yes.,3,"""We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children's creative thinking but may not consistently provide timely feedback.""",2024,2024-03-21T08:37:15Z,,,
arxiv2024,Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering,Yes.,2,"""Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality.""",2024,2024-03-21T07:47:57Z,,,
arxiv2024,MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation,Yes.,2,"""However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored."" and ""how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question.""",2024,2024-03-21T06:47:28Z,,,
arxiv2024,Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond,Yes.,1,"""Notably, we encapsulate recent advancements in Large Language Models (LLMs) that hold the potential to augment trajectory computing.""",2024,2024-03-21T05:57:27Z,,,
arxiv2024,Empowering Segmentation Ability to Multi-modal Large Language Models,Yes.,3,"""Although they achieve superior segmentation performance, we observe that the dialogue ability decreases by a large margin compared to the original MLLMs.""",2024,2024-03-21T05:36:25Z,,,
arxiv2024,Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors,Yes.,2,"""We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualitative and quantitative evaluation with domain experts, we demonstrate that our method minimizes the risk of potentially harmful and low-quality feedback generation which is desirable in such high-stakes scenarios.""",2024,2024-03-21T04:23:56Z,,,
arxiv2024,From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation,Yes.,3,"""The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained language models (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches.""",2024,2024-03-21T04:07:40Z,,,
arxiv2024,Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics,Yes.,3,"""We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.""",2024,2024-03-21T01:57:30Z,,,
arxiv2024,LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning,Yes.,2,"""using writing assistants introduces a mental dilemma, as some content isn't directly our creation.""",2024,2024-03-20T21:06:42Z,,,
arxiv2024,Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases,Yes.,1,"""We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs)""",2024,2024-03-20T18:13:17Z,,,
arxiv2024,RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition,Yes.,3,"""the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size.""",2024,2024-03-20T17:59:55Z,,,
arxiv2024,Bridge the Modality and Capacity Gaps in Vision-Language Model Selection,Yes.,3,"""we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection",2024,2024-03-20T17:54:58Z,,,
arxiv2024,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,Yes.,1,"""To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions.""",2024,2024-03-20T17:47:49Z,,,
arxiv2024,Information-Theoretic Distillation for Reference-less Summarization,Yes.,3,"""While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer.""",2024,2024-03-20T17:42:08Z,,,
arxiv2024,EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation,Yes.,3,"""However, low-resource languages are still lagging behind current state-of-the-art (SOTA) developments in the field of NLP due to insufficient resources to train LLMs.""",2024,2024-03-20T16:43:42Z,,,
arxiv2024,Large Language Models meet Network Slicing Management and Orchestration,Yes.,2,"""We also discuss the challenges associated with implementing this framework and potential solutions to mitigate them.""",2024,2024-03-20T16:29:52Z,,,
arxiv2024,RoleInteract: Evaluating the Social Interaction of Role-Playing Agents,Yes.,2,"""While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence.""",2024,2024-03-20T15:38:36Z,,,
arxiv2024,No more optimization rules: LLM-enabled policy-based multi-modal query optimizer,Yes.,3,"""to prevent LLM from making mistakes or negative optimization.""",2024,2024-03-20T13:44:30Z,,,
arxiv2024,Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models,Yes.,3,"""Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where LLMs' outputs may significantly vary depending on the order of the input options.""",2024,2024-03-20T13:38:07Z,,,
arxiv2024,A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation,Yes.,1,"""Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender.""",2024,2024-03-20T13:14:29Z,,,
arxiv2024,Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach,Yes.,1,"""The primary contribution of this research is in the demonstration of how Large Language Models can be integrated into machine learning workflows for incident management, thereby simplifying feature extraction from unstructured text and enhancing or matching the precision of severity predictions using conventional machine learning pipeline.""",2024,2024-03-20T12:33:51Z,,,
arxiv2024,Motion Generation from Fine-grained Textual Descriptions,Yes.,1,"""by feeding GPT-3.5-turbo with step-by-step instructions with pseudo-code compulsory checks.""",2024,2024-03-20T11:38:30Z,,,
arxiv2024,FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs,Yes.,3,"""Despite the remarkable performance of video-based large language models (LLMs), their adversarial threat remains unexplored.""",2024,2024-03-20T11:05:07Z,,,
arxiv2024,VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis,Yes.,2,"""automatic generation of a video synopsis based on the original single prompt leveraging LLMs,"" and ""open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content.""",2024,2024-03-20T10:58:58Z,,,
arxiv2024,An Entropy-based Text Watermarking Detection Method,Yes.,2,"""Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved.""",2024,2024-03-20T10:40:01Z,,,
arxiv2024,Vi-Mistral-X: Building a Vietnamese Language Model with Advanced Continual Pre-training,Yes.,1,"""The advancement of Large Language Models (LLMs) has significantly transformed the field of natural language processing, although the focus on English-centric models has created a noticeable research gap for specific languages, including Vietnamese.""",2024,2024-03-20T10:14:13Z,,,
arxiv2024,LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models,Yes.,1,"""Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks.""",2024,2024-03-20T08:08:54Z,,,
arxiv2024,ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics,Yes.,1,"""A baseline for POM, leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the relationship between 6D pose and task-specific requirements, offers enhanced pose-aware grasp prediction and motion planning capabilities.""",2024,2024-03-20T07:48:32Z,,,
arxiv2024,BadEdit: Backdooring large language models by model editing,Yes.,2,"""Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs).""",2024,2024-03-20T07:34:18Z,,,
arxiv2024,Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection,Yes.,3,"""single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset.""",2024,2024-03-20T06:38:13Z,,,
arxiv2024,Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model,Yes.,1,"""a novel method called ODPC is proposed, in which specific prompts to generate OOD peer classes of ID semantics are designed by a large language model as an auxiliary modality to facilitate detection.""",2024,2024-03-20T06:04:05Z,,,
arxiv2024,Polaris: A Safety-focused LLM Constellation Architecture for Healthcare,Yes.,1,"""Unlike prior LLM works in healthcare focusing on tasks like question answering, our work specifically focuses on long multi-turn voice conversations.""",2024,2024-03-20T05:34:03Z,,,
arxiv2024,Reading Users' Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference,Yes.,1,"""This paper investigates the use of Large Language Models (LLMs) for performing mental inference tasks, specifically inferring users' underlying goals and fundamental psychological needs (FPNs).""",2024,2024-03-20T04:57:32Z,,,
arxiv2024,Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models,Yes.,1,"""We propose utilizing knowledge distillation of large language models to annotate the dataset.""",2024,2024-03-20T02:29:09Z,,,
arxiv2024,Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model,Yes.,1,"""Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'.""",2024,2024-03-20T02:15:55Z,,,
arxiv2024,SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization,Yes.,3,"""the scarcity of fine-tuning samples makes this approach challenging in some cases.""",2024,2024-03-20T02:04:42Z,,,
arxiv2024,Technical Report: Competition Solution For BetterMixture,Yes.,1,"""the challenge of selecting and optimizing datasets from the vast and complex sea of data, to enhance the performance of large language models within the constraints of limited computational resources.""",2024,2024-03-20T01:46:06Z,,,
arxiv2024,A Study of Vulnerability Repair in JavaScript Programs with Large Language Models,Yes.,3,"""while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.""",2024,2024-03-19T23:04:03Z,,,
arxiv2024,Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning,Yes.,1,"""This study presents an approach to summarize doctor-patient dialogues using generative large language models (LLMs).""",2024,2024-03-19T18:37:05Z,,,
arxiv2024,LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction,Yes.,1,"""We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies.""",2024,2024-03-19T18:10:13Z,,,
arxiv2024,LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression,Yes.,3,"""The challenge is that information entropy may be a suboptimal compression metric",2024,2024-03-19T17:59:56Z,,,
arxiv2024,Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models,Yes.,3,"""it is challenging for the visual encoder in Large Vision-Language Models (LVLMs) to extract useful features tailored to questions that aid the language model's response"" and ""a common practice among existing LVLMs is to utilize lower-resolution images, which restricts the ability for visual recognition.""",2024,2024-03-19T17:59:52Z,,,
arxiv2024,Bypassing LLM Watermarks with Color-Aware Substitutions,Yes.,2,"""Existing attack methods fail to evade detection for longer text segments.""",2024,2024-03-19T17:54:39Z,,,
arxiv2024,Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models,Yes.,1,"""This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases.""",2024,2024-03-19T17:43:08Z,,,
arxiv2024,Supporting Energy Policy Research with Large Language Models,Yes.,1,"""This paper presents a method for harnessing Large Language Models (LLMs) to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape.""",2024,2024-03-19T17:28:51Z,,,
arxiv2024,Semantic Layering in Room Segmentation via LLMs,Yes.,1,"""By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation.""",2024,2024-03-19T17:23:44Z,,,
arxiv2024,Yell At Your Robot: Improving On-the-Fly from Language Corrections,Yes.,2,"""for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail.""",2024,2024-03-19T17:08:24Z,,,
arxiv2024,Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference,Yes.,1,"""This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services.""",2024,2024-03-19T16:53:53Z,,,
arxiv2024,HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning,Yes.,3,"""Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors",2024,2024-03-19T16:31:30Z,,,
arxiv2024,Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models,Yes.,3,"""the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data;"" and ""current approaches have side-effects when improving agent abilities by introducing hallucinations.""",2024,2024-03-19T16:26:10Z,,,
arxiv2024,Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation,Yes.,1,"""large language model is utilized to explicitly aggregate the global graph features with local relationship features.""",2024,2024-03-19T15:54:48Z,,,
arxiv2024,Contextual Moral Value Alignment Through Context-Based Aggregation,Yes.,1,"""Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance.""",2024,2024-03-19T15:06:53Z,,,
arxiv2024,RelationVLM: Making Large Vision-Language Models Understand Visual Relations,Yes.,3,"""current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data.""",2024,2024-03-19T15:01:19Z,,,
arxiv2024,Investigating Text Shortening Strategy in BERT: Truncation vs Summarization,Yes.,3,"""The parallelism of Transformer-based models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative.""",2024,2024-03-19T15:01:14Z,,,
arxiv2024,Automated Data Curation for Robust Language Model Fine-Tuning,Yes.,3,"""for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses.""",2024,2024-03-19T14:44:45Z,,,
arxiv2024,Instructing Large Language Models to Identify and Ignore Irrelevant Conditions,Yes.,3,"""Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs. However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.""",2024,2024-03-19T14:07:28Z,,,
arxiv2024,Pragmatic Competence Evaluation of Large Language Models for Korean,Yes.,3,"""Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference.""",2024,2024-03-19T12:21:20Z,,,
arxiv2024,Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code,Yes.,3,"""Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs).""",2024,2024-03-19T10:53:40Z,,,
arxiv2024,LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models,Yes.,3,"""However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture."" and ""Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types.""",2024,2024-03-19T10:11:14Z,,,
arxiv2024,Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs,Yes.,1,"""reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements.""",2024,2024-03-19T10:03:07Z,,,
arxiv2024,Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation,Yes.,3,"""Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve.""",2024,2024-03-19T09:00:23Z,,,
arxiv2024,AffineQuant: Affine Transformation Quantization for Large Language Models,Yes.,1,"""Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights.""",2024,2024-03-19T08:40:21Z,,,
arxiv2024,To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions,Yes.,1,"""It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs).""",2024,2024-03-19T08:09:44Z,,,
arxiv2024,UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All,Yes.,1,"""UniBind is superior in its flexible application to all CLIP-style models and delivers remarkable performance boosts.""",2024,2024-03-19T08:09:27Z,,,
arxiv2024,DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM,Yes.,1,"""We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini.""",2024,2024-03-19T06:54:33Z,,,
arxiv2024,Embodied LLM Agents Learn to Cooperate in Organized Teams,Yes.,3,"""However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation.""",2024,2024-03-19T06:39:47Z,,,
arxiv2024,WoLF: Wide-scope Large Language Model Framework for CXR Understanding,Yes.,3,"""existing CXR understanding frameworks still possess several procedural caveats... While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness.""",2024,2024-03-19T06:39:23Z,,,
arxiv2024,CrossTune: Black-Box Few-Shot Classification with Label Enhancement,Yes.,2,"""Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks."" and ""Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task",2024,2024-03-19T05:52:56Z,,,
arxiv2024,Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales,Yes.,1,"""we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design.""",2024,2024-03-19T03:22:35Z,,,
arxiv2024,Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models,Yes.,1,"""Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret.""",2024,2024-03-19T02:57:07Z,,,
arxiv2024,Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning,Yes.,3,"""However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications.""",2024,2024-03-19T02:34:33Z,,,
arxiv2024,RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners,Yes.,3,"""even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes.""",2024,2024-03-19T02:34:18Z,,,
arxiv2024,Advancing Time Series Classification with Multimodal Language Modeling,Yes.,1,"""Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task.""",2024,2024-03-19T02:32:24Z,,,
arxiv2024,Characteristic AI Agents via Large Language Models,Yes.,1,"""The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems.""",2024,2024-03-19T02:25:29Z,,,
arxiv2024,Improving LoRA in Privacy-preserving Federated Learning,Yes.,3,"""LoRA may become unstable due to the following facts",2024,2024-03-18T23:20:08Z,,,
arxiv2024,Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach,Yes.,1,"""Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns.""",2024,2024-03-18T22:39:03Z,,,
arxiv2024,FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications,Yes.,3,"""Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources.""",2024,2024-03-18T22:11:00Z,,,
arxiv2024,Reference-based Metrics Disprove Themselves in Question Generation,Yes.,1,"""utilizing large language models.""",2024,2024-03-18T20:47:10Z,,,
arxiv2024,Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models,Yes.,1,"""The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of large language models (LLMs) to detect potential malware in the npm ecosystem.""",2024,2024-03-18T19:10:12Z,,,
arxiv2024,TnT-LLM: Text Mining at Scale with Large Language Models,Yes.,2,"""We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications.""",2024,2024-03-18T18:45:28Z,,,
arxiv2024,MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control,Yes.,1,"""Specifically, MineDreamer is developed on top of recent advances in Multimodal Large Language Models (MLLMs) and diffusion models.""",2024,2024-03-18T17:59:42Z,,,
arxiv2024,RouterBench: A Benchmark for Multi-LLM Routing System,Yes.,3,"""no single model can optimally address all tasks and applications, particularly when balancing performance with cost."" and ""highlighting their potentials and limitations within our evaluation framework.""",2024,2024-03-18T17:59:04Z,,,
arxiv2024,Supervised Fine-Tuning as Inverse Reinforcement Learning,Yes.,3,"""Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.""",2024,2024-03-18T17:52:57Z,,,
arxiv2024,EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents,Yes.,2,"""Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive.""",2024,2024-03-18T17:51:16Z,,,
arxiv2024,Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching,Yes.,1,"""In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs).""",2024,2024-03-18T17:21:35Z,,,
arxiv2024,Towards Enabling FAIR Dataspaces Using Large Language Models,Yes.,1,"""The advent of Large Language Models (LLMs) raises the question of how these models can support the adoption of FAIR dataspaces.""",2024,2024-03-18T16:46:00Z,,,
arxiv2024,A Closer Look at Claim Decomposition,Yes.,3,"""We investigate how various methods of claim decomposition -- especially LLM-based methods -- affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used.""",2024,2024-03-18T16:03:45Z,,,
arxiv2024,From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?,Yes.,2,"""The major challenges identified are that most XIAI do not explore 'global' modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks.""",2024,2024-03-18T15:53:33Z,,,
arxiv2024,QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction,Yes.,3,"""we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered.""",2024,2024-03-18T15:39:14Z,,,
arxiv2024,GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture,Yes.,2,"""Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge.""",2024,2024-03-18T15:08:01Z,,,
arxiv2024,Agent3D-Zero: An Agent for Zero-shot 3D Understanding,Yes.,3,"""Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data.""",2024,2024-03-18T14:47:03Z,,,
arxiv2024,"SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator",Yes.,1,"""two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics.""",2024,2024-03-18T14:45:20Z,,,
arxiv2024,Metaphor Understanding Challenge Dataset for LLMs,Yes.,3,"""Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs.""",2024,2024-03-18T14:08:59Z,,,
arxiv2024,How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments,Yes.,3,"""Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited.""",2024,2024-03-18T14:04:47Z,,,
arxiv2024,"Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models",Yes.,3,"""relatively little is known about how well the long-context capability and performance of leading LLMs (e.g., GPT-4 Turbo and Kimi Chat).""",2024,2024-03-18T14:01:45Z,,,
arxiv2024,Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models,Yes.,2,"""Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area.""",2024,2024-03-18T13:44:48Z,,,
arxiv2024,Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs,Yes.,2,"""However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest.""",2024,2024-03-18T13:03:24Z,,,
arxiv2024,Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems,Yes.,1,"""Gender stereotypes were rectified using a Large Language Model (LLM) and its effectiveness was evaluated in a comparative survey against human educator rectifications.""",2024,2024-03-18T13:02:02Z,,,
arxiv2024,LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images,Yes.,3,"""we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy.""",2024,2024-03-18T12:04:11Z,,,
arxiv2024,HDLdebugger: Streamlining HDL debugging with Large Language Models,Yes.,3,"""Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results.""",2024,2024-03-18T11:19:37Z,,,
arxiv2024,Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model,Yes.,2,"""Traditional fine-tuning methods engage all parameters of LLMs, which is computationally expensive and may not be necessary.""",2024,2024-03-18T09:55:01Z,,,
arxiv2024,Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines,Yes.,1,"""The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task.""",2024,2024-03-18T08:58:47Z,,,
arxiv2024,Reinforcement Learning with Token-level Feedback for Controllable Text Generation,Yes.,3,"""Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods).""",2024,2024-03-18T08:18:37Z,,,
arxiv2024,LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning,Yes.,1,"""we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning.""",2024,2024-03-18T08:03:47Z,,,
arxiv2024,DEE: Dual-stage Explainable Evaluation Method for Text Generation,Yes.,2,"""Recent advancements have sought to mitigate this limitation by incorporating large language models (LLMs) to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount.""",2024,2024-03-18T06:30:41Z,,,
arxiv2024,VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding,Yes.,1,"""We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos.""",2024,2024-03-18T05:07:59Z,,,
arxiv2024,Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V,Yes.,2,"""Despite its impressive capabilities, the financial cost associated with GPT-4V's inference presents a substantial barrier for its wide use.""",2024,2024-03-18T04:41:38Z,,,
arxiv2024,HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models,Yes.,2,"""While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how 'offensive content' is conceptualized, and the resulting differences in how these datasets are labeled.""",2024,2024-03-18T04:12:35Z,,,
arxiv2024,LLM Guided Evolution -- The Automation of Models Advancing Models,Yes.,1,"""GE leverages LLMs for a more intelligent, supervised evolutionary process, guiding mutations and crossovers.""",2024,2024-03-18T03:44:55Z,,,
arxiv2024,StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation,Yes.,3,"""However the ability of LLMs is data-driven and limited by data bias, leading to poor performance on specific tasks.""",2024,2024-03-18T03:26:18Z,,,
arxiv2024,InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions,Yes.,3,"""LLMs necessitate continual task-specific adaptation without catastrophic forgetting"" and ""traditional replay-based methods do not fully utilize instructions to customize the replay strategy.""",2024,2024-03-18T03:10:36Z,,,
arxiv2024,A Novel Paradigm Boosting Translation Capabilities of Large Language Models,Yes.,1,"""Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited.""",2024,2024-03-18T02:53:49Z,,,
arxiv2024,Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning,Yes.,1,"""This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs).""",2024,2024-03-18T01:18:48Z,,,
arxiv2024,X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment,Yes.,3,"""However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity.""",2024,2024-03-18T01:14:47Z,,,
arxiv2024,"Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot",Yes.,3,"""Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures.""",2024,2024-03-18T00:13:43Z,,,
arxiv2024,JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning,Yes.,3,"""The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences.""",2024,2024-03-17T23:02:04Z,,,
arxiv2024,ConvSDG: Session Data Generation for Conversational Search,Yes.,1,"""Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation.""",2024,2024-03-17T20:34:40Z,,,
arxiv2024,Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback,Yes.,1,"""We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals.""",2024,2024-03-17T20:21:26Z,,,
arxiv2024,StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows,Yes.,1,"""It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and external environments.""",2024,2024-03-17T19:54:16Z,,,
arxiv2024,Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches,Yes.,2,"""Our findings indicate that for Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to the LLM embedding space does not guarantee improved performance over using image captions.""",2024,2024-03-17T19:44:05Z,,,
arxiv2024,SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant,Yes.,3,"""bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck.""",2024,2024-03-17T18:42:38Z,,,
arxiv2024,Cheap Ways of Extracting Clinical Markers from Texts,Yes.,1,"""Our work focuses on evaluating Large Language Models (LLM) as opposed to an alternative method that is much more memory and resource efficient.""",2024,2024-03-17T14:21:42Z,,,
arxiv2024,Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework,Yes.,3,"""However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers.""",2024,2024-03-17T13:01:03Z,,,
arxiv2024,Evaluation Ethics of LLMs in Legal Domain,Yes.,3,"""However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny."" and ""The incorporation of legal ethics into the model has been overlooked by researchers.""",2024,2024-03-17T09:05:13Z,,,
arxiv2024,Training A Small Emotional Vision Language Model for Visual Art Comprehension,Yes.,3,"""While small models are computationally efficient, their capacity is much limited compared with large models.""",2024,2024-03-17T09:01:02Z,,,
arxiv2024,Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering,Yes.,3,"""these methods are prone to the errors of sequential generation due to multiple events in a document.""",2024,2024-03-17T07:41:58Z,,,
arxiv2024,Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities,Yes.,3,"""Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases.""",2024,2024-03-17T07:34:12Z,,,
arxiv2024,Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment,Yes.,2,"""Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback.""",2024,2024-03-17T07:08:55Z,,,
arxiv2024,m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks,Yes.,3,"""the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions.""",2024,2024-03-17T04:36:18Z,,,
arxiv2024,Large Language Models Powered Context-aware Motion Prediction,Yes.,2,"""considering the cost associated with LLMs, we propose a cost-effective deployment strategy.""",2024,2024-03-17T02:06:49Z,,,
arxiv2024,SelfIE: Self-Interpretation of Large Language Model Embeddings,Yes.,3,"""The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments.""",2024,2024-03-16T15:30:34Z,,,
arxiv2024,Human Centered AI for Indian Legal Text Analytics,Yes.,3,"""Recent boom in generative AI has not translated to proportionate rise in impactful legal applications, because of low trustworthiness and the scarcity of specialized datasets for training Large Language Models (LLMs).""",2024,2024-03-16T15:17:13Z,,,
arxiv2024,Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization,No.,1,The abstract does not mention LLMs or their limitations.,2024,2024-03-16T11:09:27Z,,,
arxiv2024,Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean,Yes.,2,"""their expansion requires significant computing resources"" and ""overlooking less-resourced languages (LRLs).""",2024,2024-03-16T10:26:38Z,,,
arxiv2024,Efficient Pruning of Large Language Model with Adaptive Estimation Fusion,Yes.,3,"""Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices.""",2024,2024-03-16T04:12:50Z,,,
arxiv2024,From Words to Routes: Applying Large Language Models to Vehicle Routing,Yes.,3,"""We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency."" and ""our proposed framework achieves a 16% increase in feasibility, a 7",2024,2024-03-16T03:54:38Z,,,
arxiv2024,LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices,Yes.,3,"""we experiment and microbenchmark various LLMs' performance in tasks along CaiTI's conversation flow and discuss their strengths and weaknesses.""",2024,2024-03-16T02:48:50Z,,,
arxiv2024,Depression Detection on Social Media with Large Language Models,Yes.,1,"""combining medical knowledge and the recent advances in large language models (LLMs)""",2024,2024-03-16T01:01:16Z,,,
arxiv2024,Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns,Yes.,1,"""To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs).""",2024,2024-03-15T21:54:00Z,,,
arxiv2024,PERL: Parameter Efficient Reinforcement Learning from Human Feedback,Yes.,3,"""training models with RLHF is computationally expensive, and an overall complex process"" and ""reducing the computational burden that limits its adoption as an alignment technique for Large Language Models.""",2024,2024-03-15T21:43:46Z,,,
arxiv2024,Apriori Knowledge in an Era of Computational Opacity: The Role of AI in Mathematical Discovery,Yes.,3,"""Modern LLMs / DNNs are, by contrast, opaque to us in significant ways, and this creates obstacles in obtaining mathematical knowledge from them.""",2024,2024-03-15T21:38:26Z,,,
arxiv2024,Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems,Yes.,3,"""This deliberate erosion involves ablating synapses or neurons, or adding Gaussian noise during or after training, resulting in a controlled progressive decline in the LLMs' performance.""",2024,2024-03-15T18:00:00Z,,,
arxiv2024,"S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document",Yes.,1,"""The emergence of generative AI, specifically large language models (LLMs), provides novel pathways for understanding such complex scientific codes.""",2024,2024-03-15T17:04:27Z,,,
arxiv2024,Using an LLM to Turn Sign Spottings into Spoken Language Sentences,Yes.,1,"""In this paper, we introduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and a pretrained large language model to improve SLT performance.""",2024,2024-03-15T16:14:34Z,,,
arxiv2024,SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores,Yes.,2,"""Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address.""",2024,2024-03-15T15:43:02Z,,,
arxiv2024,TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale,Yes.,3,"""However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings.""",2024,2024-03-15T14:36:38Z,,,
arxiv2024,CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model,Yes.,1,"""In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation.""",2024,2024-03-15T14:14:26Z,,,
arxiv2024,Read between the lines -- Functionality Extraction From READMEs,Yes.,2,"""making existing text2text generation systems not very useful"" and ""small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard.""",2024,2024-03-15T11:11:57Z,,,
arxiv2024,Generative Region-Language Pretraining for Open-Ended Object Detection,Yes.,1,"""we employ Deformable DETR as a region proposal generator with a language model translating visual regions to object names.""",2024,2024-03-15T10:52:39Z,,,
arxiv2024,ChatPattern: Layout Pattern Customization via Natural Language,Yes.,1,"""In this paper, we propose ChatPattern, a novel Large-Language-Model (LLM) powered framework for flexible pattern customization.""",2024,2024-03-15T09:15:22Z,,,
arxiv2024,Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning,Yes.,1,"""we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs).""",2024,2024-03-15T08:51:15Z,,,
arxiv2024,Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF,Yes.,3,"""These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts.""",2024,2024-03-15T08:03:49Z,,,
arxiv2024,Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties,Yes.,1,"""This paper proposes Large Language Models (LLMs) to generate test programs.""",2024,2024-03-15T08:01:02Z,,,
arxiv2024,CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner,Yes.,1,"""Most existing one-shot skeleton-based action recognition focuses on raw low-level information (e.g., joint location), and may suffer from local information loss and low generalization ability. To alleviate these, we propose to leverage text description generated from large language models (LL",2024,2024-03-15T07:51:35Z,,,
arxiv2024,DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models,Yes.,3,"""current dynamic RAG methods fall short in both aspects"" and ""the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context.""",2024,2024-03-15T07:45:37Z,,,
arxiv2024,Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning,Yes.,3,"""the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded.""",2024,2024-03-15T06:54:20Z,,,
arxiv2024,TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model,Yes.,1,"""Additionally, we attempt to spot texts directly from an entire scene image to demonstrate the potential of PLMs, even Large Language Models (LLMs).""",2024,2024-03-15T06:38:25Z,,,
arxiv2024,Knowledge Condensation and Reasoning for Knowledge-based VQA,Yes.,3,"""However, these retrieved knowledge passages often contain irrelevant or noisy information, which limits the performance of the model.""",2024,2024-03-15T06:06:06Z,,,
arxiv2024,Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain,Yes.,2,"""cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies.""",2024,2024-03-15T05:35:02Z,,,
arxiv2024,Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers,Yes.,3,"""Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM.""",2024,2024-03-15T02:38:26Z,,,
arxiv2024,ViTCN: Vision Transformer Contrastive Network For Reasoning,Yes.,2,"""However, abstract reasoning remains a challenge for these models, Can AI really thinking like a human? still be a question yet to be answered.""",2024,2024-03-15T02:01:14Z,,,
arxiv2024,"Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals",Yes.,1,"""We present a novel approach to tackle the ObjectNav task for non-stationary and potentially occluded targets in an indoor environment. We refer to this task Portable ObjectNav (or P-ObjectNav), and in this work, present its formulation, feasibility, and a navigation benchmark using a",2024,2024-03-14T22:33:22Z,,,
arxiv2024,3D-VLA: A 3D Vision-Language-Action Generative World Model,Yes.,1,"""Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM).""",2024,2024-03-14T17:58:41Z,,,
arxiv2024,Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking,Yes.,3,"""We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens.""",2024,2024-03-14T17:58:16Z,,,
arxiv2024,Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models,Yes.,1,"""a component in charge of generating natural language explanations by harnessing the capabilities of Large Language Models (LLMs) over the data contained within the previously mentioned black box.""",2024,2024-03-14T16:57:18Z,,,
arxiv2024,Less is More: Data Value Estimation for Visual Instruction Tuning,Yes.,2,"""However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy.""",2024,2024-03-14T16:47:25Z,,,
arxiv2024,VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding,Yes.,2,"""However, the mismatching between the algorithms with the problem could lead to undesired results.""",2024,2024-03-14T16:13:00Z,,,
arxiv2024,MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation,Yes.,3,"""Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency.""",2024,2024-03-14T16:07:39Z,,,
arxiv2024,LLM-based agents for automating the enhancement of user story quality: An early report,Yes.,1,"""This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams.""",2024,2024-03-14T14:35:53Z,,,
arxiv2024,"""Like a Nesting Doll"": Analyzing Recursion Analogies Generated by CS Students using Large Language Models",Yes.,1,"""We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand.""",2024,2024-03-14T14:01:26Z,,,
arxiv2024,GiT: Towards Generalist Vision Transformer through Universal Language Interface,Yes.,1,"""Motivated by the universality of the Multi-layer Transformer architecture (e.g, GPT) widely used in large language models (LLMs),""",2024,2024-03-14T13:47:41Z,,,
arxiv2024,Komodo: A Linguistic Expedition into Indonesia's Regional Languages,Yes.,1,"""The recent breakthroughs in Large Language Models (LLMs) have mostly focused on languages with easily available and sufficient resources, such as English.""",2024,2024-03-14T13:12:21Z,,,
arxiv2024,BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences,Yes.,3,"""the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences.""",2024,2024-03-14T12:51:58Z,,,
arxiv2024,What Was Your Prompt? A Remote Keylogging Attack on AI Assistants,Yes.,2,"""we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences,"" and ""we were able to accurately reconstruct 29% of an AI assistant's responses and successfully infer the topic from 55% of them.""",2024,2024-03-14T09:38:12Z,,,
arxiv2024,Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse,Yes.,3,"""A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model's generalization capabilities.""",2024,2024-03-14T08:27:32Z,,,
arxiv2024,Unveiling the Generalization Power of Fine-Tuned Large Language Models,Yes.,3,"""the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood.""",2024,2024-03-14T08:18:59Z,,,
arxiv2024,USimAgent: Large Language Models for Simulating Search Users,Yes.,1,"""Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks.""",2024,2024-03-14T07:40:54Z,,,
arxiv2024,ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text,Yes.,2,"""Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored.""",2024,2024-03-14T06:49:16Z,,,
arxiv2024,Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models,Yes.,3,"""However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment.""",2024,2024-03-14T05:34:35Z,,,
arxiv2024,Large Language Models are Parallel Multilingual Learners,Yes.,1,The abstract discusses enhancing comprehension abilities of multilingual LLMs using Parallel Input in Multiple Languages (PiM) but does not mention any explicit limitations of the models.,2024,2024-03-14T03:33:46Z,,,
arxiv2024,UniCode: Learning a Unified Codebook for Multimodal Large Language Models,Yes.,3,"""This innovation addresses a critical limitation in existing MLLMs",2024,2024-03-14T03:29:58Z,,,
arxiv2024,Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate,Yes.,3,"""Two primary barriers impede the straightforward application of LLMs to circuits",2024,2024-03-14T03:24:14Z,,,
arxiv2024,LAMP: A Language Model on the Map,Yes.,3,"""nonetheless, their utility is hindered when it comes to answering fine-grained questions about specific places, such as grocery stores or restaurants, which constitute essential aspects of people's everyday lives. This is mainly because the places in our cities haven't been systematically fed into LLMs, so as to understand and memorize them.""",2024,2024-03-14T02:56:38Z,,,
arxiv2024,Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference,Yes.,3,"""This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.""",2024,2024-03-14T02:42:42Z,,,
arxiv2024,CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences,Yes.,2,"""By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment.""",2024,2024-03-14T01:51:35Z,,,
arxiv2024,ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning,Yes.,1,"""We then present two distinct systems for instruction tuning on such datasets",2024,2024-03-14T01:40:23Z,,,
arxiv2024,VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework,Yes.,1,"""With the emergence of large language models (LLMs) and vision foundation models, how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world visual perception remains an open question.""",2024,2024-03-14T01:39:40Z,,,
arxiv2024,AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic,Yes.,3,"""there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic.""",2024,2024-03-14T00:45:24Z,,,
arxiv2024,Evaluating the Application of Large Language Models to Generate Feedback in Programming Education,Yes.,3,"""However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.""",2024,2024-03-13T23:14:35Z,,,
arxiv2024,Exploring Prompt Engineering Practices in the Enterprise,Yes.,3,"""Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal.""",2024,2024-03-13T20:32:32Z,,,
arxiv2024,Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era,Yes.,3,"""First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities."" and ""We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges.""",2024,2024-03-13T20:25:27Z,,,
arxiv2024,LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots,Yes.,1,"""This paper aims to address this issue by proposing the LMStyle Benchmark, a novel evaluation framework applicable to chat-style text style transfer (C-TST), that can measure the quality of style transfer for LLMs in an automated and scalable manner.""",2024,2024-03-13T20:19:30Z,,,
arxiv2024,Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models,Yes.,3,"""various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored.""",2024,2024-03-13T18:57:30Z,,,
arxiv2024,Teaching Machines to Code: Smart Contract Translation with LLMs,Yes.,1,"""Despite the advancements in utilizing LLMs for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the LLM, remains largely unexplored.""",2024,2024-03-13T18:55:20Z,,,
arxiv2024,Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics,Yes.,3,"""We frame this as retrieval augmented generation, where perspectives are retrieved from a knowledge base and the LLM is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic retrieval system and then focus on common LLM failure modes that arise during this approach to text generation, namely hallucination and coverage errors.""",2024,2024-03-13T18:47:00Z,,,
arxiv2024,Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation,Yes.,3,"""Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.""",2024,2024-03-13T18:16:21Z,,,
arxiv2024,Cultural evolution in populations of Large Language Models,Yes.,1,"""We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap.""",2024,2024-03-13T18:11:17Z,,,
arxiv2024,DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation,Yes.,3,"""However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper.""",2024,2024-03-13T18:00:01Z,,,
arxiv2024,Simple and Scalable Strategies to Continually Pre-train Large Language Models,Yes.,3,"""the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data.""",2024,2024-03-13T17:58:57Z,,,
arxiv2024,SOTOPIA-$$: Interactive Learning of Socially Intelligent Language Agents,Yes.,3,"""We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence",2024,2024-03-13T17:17:48Z,,,
arxiv2024,TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning,Yes.,3,"""The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm.""",2024,2024-03-13T16:57:57Z,,,
arxiv2024,Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records,Yes.,1,"""The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge.""",2024,2024-03-13T16:17:09Z,,,
arxiv2024,MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models,Yes.,3,"""Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses. However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital.""",2024,2024-03-13T15:20:30Z,,,
arxiv2024,Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments,Yes.,1,"""We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments.""",2024,2024-03-13T14:59:07Z,,,
arxiv2024,Language models scale reliably with over-training and on downstream tasks,Yes.,1,"""Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated.""",2024,2024-03-13T13:54:00Z,,,
arxiv2024,Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator,Yes.,3,"""Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks.""",2024,2024-03-13T13:04:58Z,,,
arxiv2024,Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking,Yes.,3,"""limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications.""",2024,2024-03-13T12:55:43Z,,,
arxiv2024,Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning,Yes.,3,"""In view of the huge number of parameters of Large language models (LLMs), tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible.""",2024,2024-03-13T12:50:23Z,,,
arxiv2024,Software Vulnerability and Functionality Assessment using LLMs,Yes.,1,"""In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews.""",2024,2024-03-13T11:29:13Z,,,
arxiv2024,SMART: Submodular Data Mixture Strategy for Instruction Tuning,Yes.,1,"""Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks.""",2024,2024-03-13T09:31:50Z,,,
arxiv2024,From human experts to machines: An LLM supported approach to ontology and knowledge graph construction,Yes.,2,"""Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs.""",2024,2024-03-13T08:50:15Z,,,
arxiv2024,LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments,Yes.,1,"""This work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties.""",2024,2024-03-13T08:41:55Z,,,
arxiv2024,OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models,Yes.,2,"""To achieve the above goal, there are three challenges",2024,2024-03-13T07:52:31Z,,,
arxiv2024,Is Context Helpful for Chat Translation Evaluation?,Yes.,1,"""Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics.""",2024,2024-03-13T07:49:50Z,,,
arxiv2024,HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback,Yes.,3,"""highlighting practical limitations of basic RLAIF"" and ""the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness.""",2024,2024-03-13T07:38:20Z,,,
arxiv2024,Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform,Yes.,3,"""Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models."" and ""Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated.""",2024,2024-03-13T07:31:20Z,,,
arxiv2024,Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale,Yes.,1,"""GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training.""",2024,2024-03-13T06:54:47Z,,,
arxiv2024,CleanAgent: Automating Data Standardization with LLM-based Agents,Yes.,3,"""Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement.""",2024,2024-03-13T06:54:15Z,,,
arxiv2024,Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation,Yes.,1,"""To address this issue, we design the HAS framework to auto-organize groups of LLM-based agents to complete navigation tasks.""",2024,2024-03-13T06:22:17Z,,,
arxiv2024,"Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",Yes.,3,"""Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously.""",2024,2024-03-13T06:18:48Z,,,
arxiv2024,RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education,Yes.,1,"""We further illustrate potential applications of RECIPE4U dataset for enhancing the incorporation of LLMs in educational frameworks.""",2024,2024-03-13T05:51:57Z,,,
arxiv2024,Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification,Yes.,3,"""Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features.""",2024,2024-03-13T05:48:58Z,,,
arxiv2024,"TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation",Yes.,3,"""Existing supervised learning-based models, trained using annotated data through reinforcement learning, exhibit limitations in generalization capabilities."" and ""To compensate for the shortcomings of LLMs in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework.""",2024,2024-03-13T05:22:39Z,,,
arxiv2024,Boosting Disfluency Detection with Large Language Model as Disfluency Generator,Yes.,1,"""We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM.""",2024,2024-03-13T04:14:33Z,,,
arxiv2024,Learning to Watermark LLM-generated Text via Reinforcement Learning,Yes.,1,"""We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse.""",2024,2024-03-13T03:43:39Z,,,
arxiv2024,Can Large Language Models Identify Authorship?,Yes.,3,"""These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability.""",2024,2024-03-13T03:22:02Z,,,
arxiv2024,Large Language Models are Contrastive Reasoners,Yes.,1,"""Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs).""",2024,2024-03-13T03:15:05Z,,,
arxiv2024,AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models,Yes.,1,"""This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology.""",2024,2024-03-13T02:53:36Z,,,
arxiv2024,PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency,Yes.,3,"""Nevertheless, they face challenges when dealing with verbose database information and complex user intentions.""",2024,2024-03-13T02:32:41Z,,,
arxiv2024,TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection,Yes.,3,"""Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases.""",2024,2024-03-12T22:33:02Z,,,
arxiv2024,Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems,Yes.,3,"""connecting FN models with RAG can cause a decrease in performance"" and ""This shows the significant advantage of RAG over FN in terms of hallucination, which is not offset by the fact that the average 8% better METEOR score of FN models indicates greater creativity compared to RAG.""",2024,2024-03-12T21:06:31Z,,,
arxiv2024,CHAI: Clustered Head Attention for Efficient LLM Inference,Yes.,2,"""However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory.""",2024,2024-03-12T20:10:04Z,,,
arxiv2024,Generating Clarification Questions for Disambiguating Contracts,No.,1,The abstract does not mention LLMs or their limitations.,2024,2024-03-12T19:57:39Z,,,
arxiv2024,Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection,Yes.,3,"""Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content.""",2024,2024-03-12T19:12:28Z,,,
arxiv2024,LG-Traj: LLM Guided Pedestrian Trajectory Prediction,Yes.,1,"""This paper investigates the possibilities of using Large Language Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing motion cues.""",2024,2024-03-12T19:06:23Z,,,
arxiv2024,Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM,Yes.,1,"""In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences.""",2024,2024-03-12T18:19:47Z,,,
arxiv2024,Beyond Text: Frozen Large Language Models in Visual Signal Comprehension,Yes.,1,"""In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multi-modal datasets.""",2024,2024-03-12T17:59:51Z,,,
arxiv2024,Rethinking Generative Large Language Model Evaluation for Semantic Comprehension,Yes.,3,"""we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios.""",2024,2024-03-12T17:59:48Z,,,
arxiv2024,LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code,Yes.,2,"""However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities.""",2024,2024-03-12T17:58:04Z,,,
arxiv2024,Duwak: Dual Watermarks in Large Language Models,Yes.,1,"""As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms.""",2024,2024-03-12T16:25:38Z,,,
arxiv2024,Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations,Yes.,3,"""Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving.""",2024,2024-03-12T15:56:10Z,,,
arxiv2024,Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings,Yes.,1,"""We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training.""",2024,2024-03-12T15:36:42Z,,,
arxiv2024,Multi-modal Auto-regressive Modeling via Visual Words,Yes.,3,"""there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification.""",2024,2024-03-12T14:58:52Z,,,
arxiv2024,WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?,Yes.,3,"""Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs.""",2024,2024-03-12T14:58:45Z,,,
arxiv2024,StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models,Yes.,2,"""Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status.""",2024,2024-03-12T14:57:40Z,,,
arxiv2024,KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,Yes.,1,"""In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation.""",2024,2024-03-12T14:56:34Z,,,
arxiv2024,Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards,Yes.,3,"""existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile.""",2024,2024-03-12T14:51:57Z,,,
arxiv2024,"Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization",Yes.,3,"""However, data augmentation based on large language models faces two disadvantages",2024,2024-03-12T14:37:03Z,,,
arxiv2024,Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Pre-training Framework,Yes.,1,"""This is achieved by consulting a large language model and medical experts.""",2024,2024-03-12T13:18:22Z,,,
arxiv2024,generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation,Yes.,3,"""the considered output candidates of the underlying search algorithm are under-explored and under-explained.""",2024,2024-03-12T13:09:15Z,,,
arxiv2024,Couler: Unified Machine Learning Workflow Optimization in Cloud,Yes.,1,"""We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines.""",2024,2024-03-12T12:47:32Z,,,
arxiv2024,LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model,Yes.,3,"""Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance."" and ""we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even",2024,2024-03-12T12:10:18Z,,,
arxiv2024,MoAI: Mixture of All Intelligence for Large Language and Vision Models,Yes.,3,"""current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene graph generation (SGG), and optical character recognition (OCR).""",2024,2024-03-12T10:44:13Z,,,
arxiv2024,SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models,Yes.,1,"""Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due",2024,2024-03-12T07:45:33Z,,,
arxiv2024,NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning,Yes.,3,"""their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus.""",2024,2024-03-12T07:27:02Z,,,
arxiv2024,Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking,Yes.,2,"""these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues"" and ""no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience.""",2024,2024-03-12T07:17:01Z,,,
arxiv2024,Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation,Yes.,3,"""Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data.""",2024,2024-03-12T04:04:38Z,,,
arxiv2024,A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism,Yes.,2,"""there are still concerns regarding cost and trade-offs between privacy issues and accuracy.""",2024,2024-03-12T03:30:04Z,,,
arxiv2024,CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation,Yes.,2,"""However, the state-of-the-art method (instructERC) solely identifying speaker, and ignores commonsense knowledge(i.e., reaction of the listeners and intention of the speaker, etc.) behind speakers during a conversation, which can deeply mine speaker information.""",2024,2024-03-12T02:37:11Z,,,
arxiv2024,AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production,Yes.,1,"""optimizing the LLM prompts and utilities usage.""",2024,2024-03-12T02:30:50Z,,,
arxiv2024,Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits,Yes.,2,"""Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs.""",2024,2024-03-11T23:52:46Z,,,
arxiv2024,Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions,Yes.,1,"""First, an LLM is utilized initially to obtain a plausible answer for the instruction or question.""",2024,2024-03-11T22:46:46Z,,,
arxiv2024,Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews,Yes.,2,"""We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review.""",2024,2024-03-11T21:51:39Z,,,
arxiv2024,Narrating Causal Graphs with Large Language Models,Yes.,3,"""Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings.""",2024,2024-03-11T19:19:59Z,,,
arxiv2024,MRL Parsing Without Tears: The Case of Hebrew,No.,1,"""Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking.""",2024,2024-03-11T17:54:33Z,,,
arxiv2024,SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees,Yes.,3,"""the deployment of high-performance LLMs incurs substantial costs, primarily due to the increased number of parameters aimed at enhancing model performance.""",2024,2024-03-11T17:45:47Z,,,
arxiv2024,SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data,Yes.,2,"""SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills"" and ""Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect",2024,2024-03-11T17:35:33Z,,,
arxiv2024,"Naming, Describing, and Quantifying Visual Objects in Humans and LLMs",Yes.,3,"""Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences, with all models failing in tasks that require high-level reasoning such as assigning quantifiers.""",2024,2024-03-11T17:20:12Z,,,
arxiv2024,ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis,Yes.,3,"""However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning.""",2024,2024-03-11T17:18:53Z,,,
arxiv2024,MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning,Yes.,3,"""Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism."" and ""Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance.""",2024,2024-03-11T17:03:04Z,,,
arxiv2024,Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents,Yes.,1,"""We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts.""",2024,2024-03-11T16:24:08Z,,,
arxiv2024,Development of a Reliable and Accessible Caregiving Language Model (CaLM),Yes.,1,"""Large language models can potentially be used as a foundation technology for supporting caregivers as educational tools or as adjunct to care.""",2024,2024-03-11T16:12:34Z,,,
arxiv2024,DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation,Yes.,1,"""DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos.""",2024,2024-03-11T16:03:35Z,,,
arxiv2024,The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework,Yes.,3,"""This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations.""",2024,2024-03-11T15:48:43Z,,,
arxiv2024,ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model,Yes.,1,"""However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions).""",2024,2024-03-11T14:35:45Z,,,
arxiv2024,ALaRM: Align Language Models via Hierarchical Rewards Modeling,Yes.,3,"""The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals.""",2024,2024-03-11T14:28:40Z,,,
arxiv2024,Real-Time Multimodal Cognitive Assistant for Emergency Medical Services,Yes.,1,"""augmented with synthetic data generated by large language models (LLMs).""",2024,2024-03-11T13:56:57Z,,,
arxiv2024,Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning,Yes.,1,"""This paper introduces a novel RRG method, LM-RRG, that integrates large models (LMs) with clinical quality reinforcement learning to generate accurate and comprehensive chest X-ray radiology reports.""",2024,2024-03-11T13:47:11Z,,,
arxiv2024,Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System,Yes.,2,"""substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity"" and ""storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories.""",2024,2024-03-11T12:32:14Z,,,
arxiv2024,FashionReGen: LLM-Empowered Fashion Report Generation,Yes.,1,"""In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based on the advanced Large Language Models (LLMs), debbed as GPT-FAR.""",2024,2024-03-11T12:29:35Z,,,
arxiv2024,Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement,Yes.,2,"""generating more descriptive prompts and reducing hallucinations in LLM-generated content to boost zero-shot classification.""",2024,2024-03-11T12:28:55Z,,,
arxiv2024,KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation,Yes.,3,"""However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination.""",2024,2024-03-11T12:04:20Z,,,
arxiv2024,ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models,Yes.,1,"""Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities.""",2024,2024-03-11T10:32:23Z,,,
arxiv2024,AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models,Yes.,2,"""Our extensive evaluation of top-performing LLMs, tailored for both English and Chinese, reveals a substantial potential for enhancing ancient text comprehension. By highlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote their development and application forward in the realms of ancient Chinese language education and scholarly research.""",2024,2024-03-11T10:24:37Z,,,
arxiv2024,Unraveling the Mystery of Scaling Laws: Part I,Yes.,3,"""However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters."" and ""they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for",2024,2024-03-11T10:05:29Z,,,
arxiv2024,On the Consideration of AI Openness: Can Good Intent Be Abused?,Yes.,3,"""We found that a widely accepted open-source LLM, which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities.""",2024,2024-03-11T09:24:06Z,,,
arxiv2024,QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning,No.,1,"The paper discusses transformer-based models in general and focuses on model quantization and fine-tuning methods, but does not specifically address LLMs or their limitations.",2024,2024-03-11T08:09:30Z,,,
arxiv2024,Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach,Yes.,1,"""We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation.""",2024,2024-03-11T07:48:35Z,,,
arxiv2024,CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation,Yes.,3,"""However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset.""",2024,2024-03-11T05:49:34Z,,,
arxiv2024,RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models,Yes.,1,"""In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations.""",2024,2024-03-11T04:13:26Z,,,
arxiv2024,Evolving Knowledge Distillation with Large Language Models and Active Learning,Yes.,3,"""Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high.""",2024,2024-03-11T03:55:24Z,,,
arxiv2024,What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation,Yes.,3,"""Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization.""",2024,2024-03-11T03:42:51Z,,,
arxiv2024,A Knowledge-Injected Curriculum Pretraining Framework for Question Answering,Yes.,3,"""However, these methods often depend on specific techniques and resources to work, which may not always be available and restrict its application. Moreover, existing methods focus more on improving language understanding with KGs, while neglect the more important human-like complex reasoning.""",2024,2024-03-11T03:42:03Z,,,
arxiv2024,Can LLMs' Tuning Methods Work in Medical Multimodal Domain?,Yes.,2,"""Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization.""",2024,2024-03-11T03:38:48Z,,,
arxiv2024,DivCon: Divide and Conquer for Progressive Text-to-Image Generation,Yes.,2,"""However, these methods still struggle with generating images from textural prompts with multiple objects and complicated spatial relationships.""",2024,2024-03-11T03:24:44Z,,,
arxiv2024,Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages,Yes.,3,"""Despite their success, LLMs often struggle to perform well on low-resource languages because there is so little training data available. This shortcoming is especially prevalent with open source models.""",2024,2024-03-11T01:04:36Z,,,
arxiv2024,From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification,Yes.,1,"""User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints.""",2024,2024-03-10T22:14:54Z,,,
arxiv2024,LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition,Yes.,3,"""We find evidence that state-of-the-art large language models exhibit sensitivity to all of these properties except novelty, which demonstrates that they have yet to reach human-level language understanding abilities.""",2024,2024-03-10T20:20:16Z,,,
arxiv2024,TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision,Yes.,3,"""However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context.""",2024,2024-03-10T13:58:38Z,,,
arxiv2024,Personalized LoRA for Human-Centered Text Understanding,Yes.,1,"""Effectively and efficiently adapting a pre-trained language model (PLM) for human-centered text understanding (HCTU) is challenging since user tokens are million-level in most personalized applications and do not have concrete explicit semantics.""",2024,2024-03-10T13:04:54Z,,,
arxiv2024,Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models,Yes.,2,"""Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities.""",2024,2024-03-10T12:43:27Z,,,
arxiv2024,Can Large Language Models Automatically Score Proficiency of Written Essays?,Yes.,2,"""Finally, despite the performance gap between the two LLMs and SOTA models in terms of predictions, they provide feedback to enhance the quality of the essays, which can potentially help both teachers and students.""",2024,2024-03-10T09:39:00Z,,,
arxiv2024,Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity,Yes.,1,"""Recently, the emergence of LLMs has introduced new solutions to such problems by leveraging graph structures to generate supplementary user profiles.""",2024,2024-03-10T08:59:04Z,,,
arxiv2024,FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning,Yes.,2,"""Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks.""",2024,2024-03-10T08:41:22Z,,,
arxiv2024,Low-dose CT Denoising with Language-engaged Dual-space Alignment,Yes.,1,"""While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability. To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models.""",2024,2024-03-10T08:21:50Z,,,
arxiv2024,FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language,Yes.,1,"""we propose the Fine-Grained Monetary Policy Analysis Framework (FMPAF), a novel approach that integrates large language models (LLMs) with regression analysis to provide a comprehensive analysis of the impact of the press-conference communications of chairs of the Federal",2024,2024-03-10T07:21:31Z,,,
arxiv2024,Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning,No.,1,The abstract does not mention LLMs or any limitations related to them.,2024,2024-03-10T06:30:54Z,,,
arxiv2024,Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery,Yes.,1,"""The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and Large Language Model annotation.""",2024,2024-03-10T05:12:16Z,,,
arxiv2024,Reframe Anything: LLM Agent for Open World Video Reframing,Yes.,1,"""The advent of powerful large language models (LLMs) open new avenues for AI capabilities.""",2024,2024-03-10T03:29:56Z,,,
arxiv2024,A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation,Yes.,1,"""Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs).""",2024,2024-03-09T23:22:56Z,,,
arxiv2024,Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages,Yes.,2,"""Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind.""",2024,2024-03-09T21:36:13Z,,,
arxiv2024,Calibrating Large Language Models Using Their Generations Only,Yes.,3,"""finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge.""",2024,2024-03-09T17:46:24Z,,,
arxiv2024,Thread Detection and Response Generation using Transformers with Prompt Optimisation,Yes.,1,"""Llama2 7b is used due to its high level of generalisation but the system can be updated with any open source Large Language Model(LLM).""",2024,2024-03-09T14:50:20Z,,,
arxiv2024,GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing,Yes.,3,"""We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of GPT for handling advanced tasks in emotion recognition and related fields.""",2024,2024-03-09T13:56:25Z,,,
arxiv2024,KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques,Yes.,3,"""Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases.""",2024,2024-03-09T11:23:38Z,,,
arxiv2024,LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content,Yes.,1,"""inspired by the rich implicit knowledge in large-scale models (e.g., large language models, LLMs), LTGC leverages the power of these models to parse and reason over the original tail data to produce diverse tail-class content.""",2024,2024-03-09T09:52:15Z,,,
arxiv2024,Optimizing LLM Queries in Relational Workloads,Yes.,3,"""However, LLM inference is highly expensive in both computational and economic terms",2024,2024-03-09T07:01:44Z,,,
arxiv2024,$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting,Yes.,2,"""the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting.""",2024,2024-03-09T05:20:48Z,,,
arxiv2024,ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes,Yes.,3,"""most earlier clinical language models were pretrained with a context length limited to roughly one clinical document.""",2024,2024-03-09T04:58:25Z,,,
arxiv2024,ItD: Large Language Models Can Teach Themselves Induction through Deduction,Yes.,3,"""researchers have found that they still have limited ability to conduct induction."" and ""their performance is still constrained by the inherent inductive capability of the LLMs.""",2024,2024-03-09T04:20:46Z,,,
arxiv2024,FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs,Yes.,3,"""the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs.""",2024,2024-03-09T02:27:45Z,,,
arxiv2024,A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health,Yes.,2,"""Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question.""",2024,2024-03-08T23:46:37Z,,,
arxiv2024,A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries,Yes.,1,"""While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown.""",2024,2024-03-08T23:17:55Z,,,
arxiv2024,Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?,Yes.,3,"""Moreover, we show that vision models fail to capture the essence of video stimuli and that LLMs tend to rate different communicative acts and behavior desirability higher than people.""",2024,2024-03-08T22:23:23Z,,,
arxiv2024,Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations,Yes.,2,"""The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts.""",2024,2024-03-08T21:26:49Z,,,
arxiv2024,DP-TabICL: In-Context Learning with Differentially Private Tabular Data,Yes.,3,"""it has been shown that LLMs can leak information contained in prompts, and since tabular data often contain sensitive information, understanding how to protect the underlying tabular data used in ICL is a critical area of research.""",2024,2024-03-08T21:19:01Z,,,
arxiv2024,Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4,Yes.,3,"""Although GPT-4V outperformed other models in our evaluation, it still requires overall improvement.""",2024,2024-03-08T21:16:28Z,,,
arxiv2024,PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design,Yes.,3,"""retrievals from large databases can constitute a substantial portion of the overall generation time,"" and ""to reduce generation latency and enhance generation quality.""",2024,2024-03-08T21:09:20Z,,,
arxiv2024,GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM,Yes.,3,"""the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput.""",2024,2024-03-08T18:48:30Z,,,
arxiv2024,Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation,Yes.,1,"""We introduce a novel approach to domain extrapolation that leverages reasoning ability and the extensive knowledge encapsulated within large language models (LLMs) to synthesize entirely new domains.""",2024,2024-03-08T18:44:23Z,,,
arxiv2024,VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model,Yes.,1,"""To overcome this, we introduce a new approach called Vision-Language Model assisted Pseudo-Labeling (VLM-PL).""",2024,2024-03-08T14:23:00Z,,,
arxiv2024,ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues,Yes.,3,"""limiting the ASU performance"" and ""alleviate the LLMs-intrinsic factual hallucination problem in TSA.""",2024,2024-03-08T14:05:36Z,,,
arxiv2024,Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents,Yes.,2,"""The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task.""",2024,2024-03-08T13:34:20Z,,,
arxiv2024,Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering,Yes.,1,"""Existing methods follow two main paradigms to collect evidence",2024,2024-03-08T11:09:13Z,,,
arxiv2024,"Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge",Yes.,3,"""Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge"" and ""highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.""",2024,2024-03-08T10:09:57Z,,,
arxiv2024,Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation,Yes.,2,"""We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs).""",2024,2024-03-08T09:20:12Z,,,
arxiv2024,Towards a Psychology of Machines: Large Language Models Predict Human Memory,Yes.,1,"""Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition.""",2024,2024-03-08T08:41:14Z,,,
arxiv2024,Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem,No.,1,"The abstract discusses the use of reinforcement learning and Transformer architecture for the inverse design of PCSEL, but it does not mention LLMs or their limitations.",2024,2024-03-08T08:38:50Z,,,
arxiv2024,Med3DInsight: Enhancing 3D Medical Image Understanding with 2D Multi-Modal Large Language Models,Yes.,2,"""Recent advances in multi-modal large language models (MLLMs) provide a new and promising way to understand images with the help of text descriptions. However, most current MLLMs are designed for 2D natural images.""",2024,2024-03-08T08:15:53Z,,,
arxiv2024,ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment,Yes.,3,"""However, most widely used models still employ CLIP as their text encoder, which constrains their ability to comprehend dense prompts, encompassing multiple objects, detailed attributes, complex relationships, long-text alignment, etc.""",2024,2024-03-08T08:08:10Z,,,
arxiv2024,Can we obtain significant success in RST discourse parsing by using Large Language Models?,Yes.,1,"""While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question.""",2024,2024-03-08T05:34:29Z,,,
arxiv2024,DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation,Yes.,1,"""We present DiffChat, a novel method to align Large Language Models (LLMs) to 'chat' with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable Diffusion) for interactive image creation.""",2024,2024-03-08T02:24:27Z,,,
arxiv2024,An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment,Yes.,3,"""However, LLMs have their limitations, as seen in GPT-4's struggles with lexical paraphrasing.""",2024,2024-03-08T00:19:24Z,,,
arxiv2024,Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering,Yes.,1,"""Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support.""",2024,2024-03-07T20:48:40Z,,,
arxiv2024,iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries,Yes.,3,"""their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform.""",2024,2024-03-07T18:56:39Z,,,
arxiv2024,ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes,No.,1,The abstract does not mention LLMs or any related limitations. It focuses on evaluating the robustness of vision-based models against object-to-background compositional changes.,2024,2024-03-07T17:48:48Z,,,
arxiv2024,Teaching Large Language Models to Reason with Reinforcement Learning,Yes.,3,"""during RL training models fail to explore significantly beyond solutions already produced by SFT models.""",2024,2024-03-07T16:36:29Z,,,
arxiv2024,CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios,Yes.,3,"""Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events.""",2024,2024-03-07T16:31:02Z,,,
arxiv2024,Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition,Yes.,3,"""Additionally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed dataset.""",2024,2024-03-07T15:22:07Z,,,
arxiv2024,GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability,Yes.,1,"""Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic.""",2024,2024-03-07T13:36:08Z,,,
arxiv2024,Do Large Language Model Understand Multi-Intent Spoken Language ?,Yes.,1,"""Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models.""",2024,2024-03-07T13:30:52Z,,,
arxiv2024,Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset,Yes.,2,"""Despite the progress, the field has many aspects left to explore.""",2024,2024-03-07T12:57:16Z,,,
arxiv2024,Low-Resource Court Judgment Summarization for Common Law Systems,Yes.,1,"""Besides, this is the first court judgment summarization work adopting large language models (LLMs) in data augmentation, summary generation, and evaluation.""",2024,2024-03-07T12:47:42Z,,,
arxiv2024,Membership Inference Attacks and Privacy in Topic Modeling,No.,1,The abstract discusses privacy attacks against topic models and does not mention large language models (LLMs) or their limitations.,2024,2024-03-07T12:43:42Z,,,
arxiv2024,Feedback-Generation for Programming Exercises With GPT-4,Yes.,3,"""At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed.""",2024,2024-03-07T12:37:52Z,,,
arxiv2024,Discriminative Probing and Tuning for Text-to-Image Generation,Yes.,3,"""Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning."" and ""However, the inherent alignment capabilities of T2I models are still inadequate.""",2024,2024-03-07T08:37:33Z,,,
arxiv2024,Online Adaptation of Language Models with a Memory of Amortized Contexts,Yes.,3,"""large language models (LLMs) quickly run out of date despite enormous development costs"" and ""given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential.""",2024,2024-03-07T08:34:57Z,,,
arxiv2024,Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders,Yes.,3,"""We observe that current embedding models fare poorly in semantic understanding of these concepts.""",2024,2024-03-07T08:32:17Z,,,
arxiv2024,Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy,Yes.,3,"""existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously.""",2024,2024-03-07T07:31:00Z,,,
arxiv2024,Advancing Biomedical Text Mining with Community Challenges,Yes.,2,"""Finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of large language models.""",2024,2024-03-07T06:52:51Z,,,
arxiv2024,Federated Recommendation via Hybrid Retrieval Augmented Generation,Yes.,3,"""LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios.""",2024,2024-03-07T06:38:41Z,,,
arxiv2024,UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities,Yes.,3,"""Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a large space for improvement in Ultra-ESE.""",2024,2024-03-07T06:10:02Z,,,
arxiv2024,DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning,Yes.,3,"""We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning."" and ""overcoming pretraining sequence length limitations.""",2024,2024-03-07T05:26:41Z,,,
arxiv2024,Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks,Yes.,1,"""This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination.""",2024,2024-03-07T05:05:56Z,,,
arxiv2024,Aligners: Decoupling LLMs and Alignment,Yes.,3,"""Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion.""",2024,2024-03-07T04:54:56Z,,,
arxiv2024,Self-Evaluation of Large Language Model based on Glass-box Features,Yes.,1,"""The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods.""",2024,2024-03-07T04:50:38Z,,,
arxiv2024,Large Language Models are In-Context Molecule Learners,Yes.,3,"""previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs.""",2024,2024-03-07T03:58:28Z,,,
arxiv2024,"Generative AI for Synthetic Data Generation: Methods, Challenges and the Future",Yes.,3,"""discuss the current limitations, and suggest potential pathways for future research.""",2024,2024-03-07T03:38:44Z,,,
arxiv2024,Metric-aware LLM inference for regression and scoring,Yes.,3,"""Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. Building on prior work on Minimum Bayes Risk Decoding, we show that this inference strategy can be suboptimal for a range of regression and scoring tasks, and associated evaluation metrics.""",2024,2024-03-07T03:24:34Z,,,
arxiv2024,Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy,Yes.,3,"""However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents.""",2024,2024-03-07T02:34:54Z,,,
arxiv2024,Privacy-preserving Fine-tuning of Large Language Models through Flatness,Yes.,3,"""The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT. Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation.""",2024,2024-03-07T00:44:11Z,,,
arxiv2024,Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification,Yes.,1,"""a novel approach leveraging a locally executable Large Language Model (LLM) to extract and enhance findings labels on CXR reports.""",2024,2024-03-06T20:10:41Z,,,
arxiv2024,Guiding Enumerative Program Synthesis with Large Language Models,Yes.,3,"""We find that GPT-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms.""",2024,2024-03-06T19:13:53Z,,,
arxiv2024,FaaF: Facts as a Function for the evaluation of generated text,Yes.,3,"""this method of prompting is unreliable when faced with incomplete or inaccurate reference information.""",2024,2024-03-06T17:48:06Z,,,
arxiv2024,SaulLM-7B: A pioneering Large Language Model for Law,Yes.,1,"""In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain.""",2024,2024-03-06T17:42:16Z,,,
arxiv2024,The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported,Yes.,3,"""Recent work arXiv.2310.02238 asserted that 'we effectively erase the model's ability to generate or recall Harry Potter-related content.' This claim is shown to be overbroad.""",2024,2024-03-06T16:39:50Z,,,
arxiv2024,Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery,Yes.,1,"""considering that the large language models (LLMs) emerge the powerful generalization ability, a novel unified visual-language model called Popeye is proposed for multi-source ship detection from RS imagery.""",2024,2024-03-06T15:35:53Z,,,
arxiv2024,Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese,Yes.,1,"""The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation.""",2024,2024-03-06T13:17:07Z,,,
arxiv2024,General2Specialized LLMs Translation for E-commerce,Yes.,1,"""The paradigm can be used for the NMT models based on Large language models (LLMs).""",2024,2024-03-06T13:15:21Z,,,
arxiv2024,K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data,Yes.,1,"""leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias.""",2024,2024-03-06T12:08:14Z,,,
arxiv2024,SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models,Yes.,3,"""Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements).""",2024,2024-03-06T11:48:08Z,,,
arxiv2024,GPTopic: Dynamic and Interactive Topic Representations,Yes.,1,"""we introduce GPTopic, a software package that leverages Large Language Models (LLMs) to create dynamic, interactive topic representations.""",2024,2024-03-06T11:34:20Z,,,
arxiv2024,Multimodal Large Language Models to Support Real-World Fact-Checking,Yes.,3,"""While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied."" and ""existing open-source models exhibit strong biases and are highly sensitive to the prompt.""",2024,2024-03-06T11:32:41Z,,,
arxiv2024,WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off,Yes.,2,"""Watermarking is a technical means to dissuade malfeasant usage of Large Language Models."" and ""WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness.""",2024,2024-03-06T10:55:30Z,,,
arxiv2024,Prompt Mining for Language-based Human Mobility Forecasting,Yes.,3,"""using fixed templates for prompting may limit the forecasting capability of language models.""",2024,2024-03-06T08:43:30Z,,,
arxiv2024,GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection,Yes.,3,"""Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states.""",2024,2024-03-06T07:29:57Z,,,
arxiv2024,A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation,Yes.,2,"""Even in the era of large language models, response generation grounded in knowledge retrieved from additional up-to-date sources remains a practically important approach.""",2024,2024-03-06T06:54:02Z,,,
arxiv2024,TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs,No.,1,"The abstract discusses Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) and their application in model order reduction of partial differential equations, but it does not mention language models (LLMs or LMs).",2024,2024-03-06T04:49:18Z,,,
arxiv2024,Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models,Yes.,3,"""However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks.""",2024,2024-03-06T03:33:48Z,,,
arxiv2024,Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization,Yes.,3,"""existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data.""",2024,2024-03-06T03:02:38Z,,,
arxiv2024,Human vs. Machine: Language Models and Wargames,Yes.,3,"""We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.""",2024,2024-03-06T02:23:32Z,,,
arxiv2024,Japanese-English Sentence Translation Exercises Dataset for Automatic Grading,Yes.,3,"""the GPT models with few-shot learning show poorer results than finetuned BERT, indicating that our newly proposed task presents a challenging issue, even for the state-of-the-art large language models.""",2024,2024-03-06T01:37:03Z,,,
arxiv2024,Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation,Yes.,2,"""We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements.""",2024,2024-03-05T22:12:01Z,,,
arxiv2024,Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse,Yes.,3,"""We then perform thorough LLM model diagnostics, identifying the role of claim type (i.e. implicit vs explicit claims) and comment length as sources of model error.""",2024,2024-03-05T21:38:19Z,,,
arxiv2024,Guardrail Baselines for Unlearning in LLMs,Yes.,3,"""fine-tuning can be expensive, as it requires both generating a set of examples and running iterations of fine-tuning to update the model.""",2024,2024-03-05T21:19:06Z,,,
arxiv2024,MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets,Yes.,1,"""Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs.""",2024,2024-03-05T18:31:28Z,,,
arxiv2024,Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement,Yes.,1,"""a customized AI Assistant powered by the GPT-4 Large Language Model.""",2024,2024-03-05T18:24:52Z,,,
arxiv2024,SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection,Yes.,3,"""While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences.""",2024,2024-03-05T18:04:59Z,,,
arxiv2024,PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset,Yes.,3,"""most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities."" and ""Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance.""",2024,2024-03-05T18:01:59Z,,,
arxiv2024,Language Guided Exploration for RL Agents in Text Environments,Yes.,1,"""Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts.""",2024,2024-03-05T17:26:41Z,,,
arxiv2024,Learning to Use Tools via Cooperative and Interactive Agents,Yes.,3,"""they still suffer from potential performance degradation when addressing complex tasks due to",2024,2024-03-05T15:08:16Z,,,
arxiv2024,Word Importance Explains How Prompts Affect Language Model Outputs,Yes.,3,"""However, their 'black box' nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use.""",2024,2024-03-05T15:04:18Z,,,
arxiv2024,OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,Yes.,3,"""identify visual perception and low-level action execution as critical bottlenecks.""",2024,2024-03-05T14:53:53Z,,,
arxiv2024,Localized Zeroth-Order Prompt Optimization,Yes.,3,"""Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks.""",2024,2024-03-05T14:18:15Z,,,
arxiv2024,"Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges",Yes.,3,"""this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation.""",2024,2024-03-05T14:11:54Z,,,
arxiv2024,Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering,Yes.,3,"""Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence.""",2024,2024-03-05T13:43:58Z,,,
arxiv2024,Multi-Scale Protein Language Model for Unified Molecular Modeling,Yes.,3,"""However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules.""",2024,2024-03-05T13:35:41Z,,,
arxiv2024,WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction,Yes.,3,"""Existing research mainly focuses on regular-shaped tables... editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code.""",2024,2024-03-05T13:33:12Z,,,
arxiv2024,Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation,Yes.,3,"""Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.""",2024,2024-03-05T13:23:48Z,,,
arxiv2024,A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods,Yes.,1,"""the advent of Large Language Models (LLMs) has altered conventional ATS methods.""",2024,2024-03-05T12:11:07Z,,,
arxiv2024,MathScale: Scaling Instruction Tuning for Mathematical Reasoning,Yes.,3,"""However, their proficiency in solving mathematical problems remains inadequate.""",2024,2024-03-05T11:42:59Z,,,
arxiv2024,DPPA: Pruning Method for Large Language Model to Model Merging,No.,1,The abstract does not mention LLMs or any limitations related to them.,2024,2024-03-05T09:12:49Z,,,
arxiv2024,Evaluating and Optimizing Educational Content with Large Language Model Judgments,Yes.,3,"""We conclude by discussing potential divergences between human and LM opinions and the resulting pitfalls of automating instructional design.""",2024,2024-03-05T09:09:15Z,,,
arxiv2024,Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations,Yes.,1,"""the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understanding and generation, and their impressive generalization and reasoning capabilities.""",2024,2024-03-05T08:31:00Z,,,
arxiv2024,CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models,Yes.,3,"""This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets.""",2024,2024-03-05T07:58:12Z,,,
arxiv2024,Towards Training A Chinese Large Language Model for Anesthesiology,Yes.,2,"""The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies.""",2024,2024-03-05T07:53:49Z,,,
arxiv2024,Android in the Zoo: Chain-of-Action-Thought for GUI Agents,Yes.,1,"""Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API.""",2024,2024-03-05T07:09:35Z,,,
arxiv2024,Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment,Yes.,1,"""an adversarial multi-hop fact verification dataset and a symmetric multi-hop fact verification dataset are proposed with the help of the large language model.""",2024,2024-03-05T06:28:02Z,,,
arxiv2024,Privacy-Aware Semantic Cache for Large Language Models,Yes.,3,"""these models incur exceptionally high computational costs"" and ""Existing caching solutions for LLMs raise privacy and scalability concerns and perform wasteful query requests.""",2024,2024-03-05T06:23:50Z,,,
arxiv2024,Revisiting Meta-evaluation for Grammatical Error Correction,Yes.,3,"""These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques."" and ""covering 12 state-of-the-art systems including large language models (LLMs).""",2024,2024-03-05T05:53:09Z,,,
arxiv2024,Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use,Yes.,1,"""Our framework leverages recent advances in foundation models, both large language models and vision-language models, to carve out the concept space through conversation and by automatically labeling training data points.""",2024,2024-03-05T03:34:11Z,,,
arxiv2024,ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary,Yes.,3,"""simple CoT method often lacks the ability to provide extensive comparative summary.""",2024,2024-03-05T01:13:56Z,,,
arxiv2024,Eliciting Better Multilingual Structured Reasoning from LLMs through Code,Yes.,3,"""studies have been limited to English or simple reasoning tasks,"" and ""xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks.""",2024,2024-03-05T00:48:56Z,,,
arxiv2024,Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research,Yes.,2,"""scaling and evaluating their usage presents new challenges not addressed in previous frameworks.""",2024,2024-03-05T00:27:43Z,,,
arxiv2024,Wukong: Towards a Scaling Law for Large-Scale Recommendation,No.,1,"The abstract primarily discusses recommendation models and their scaling laws, mentioning LLMs only in a comparative context to highlight differences in scalability.",2024,2024-03-04T23:40:20Z,,,
arxiv2024,DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation,Yes.,1,"""We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique.""",2024,2024-03-04T22:47:58Z,,,
arxiv2024,Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents,Yes.,1,"""In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents.""",2024,2024-03-04T21:50:29Z,,,
arxiv2024,Enhancing LLM Safety via Constrained Direct Preference Optimization,Yes.,3,"""This approach, however, is computationally expensive and often unstable.""",2024,2024-03-04T20:39:24Z,,,
arxiv2024,OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering,Yes.,2,"""Despite limitations in generating offensive texts using ChatGPT due to ethical constraints.""",2024,2024-03-04T20:34:58Z,,,
arxiv2024,RegionGPT: Towards Region Understanding Vision Language Model,Yes.,3,"""they struggle with detailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse-grained training data that lacks detailed, region-specific captions.""",2024,2024-03-04T18:58:08Z,,,
arxiv2024,Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve,Yes.,1,"""Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time.""",2024,2024-03-04T18:47:08Z,,,
arxiv2024,KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection,Yes.,2,"""To utilize this textual information, we propose a Large Language Model (LLM)-based approach to extract brand information of webpages from text.""",2024,2024-03-04T17:38:32Z,,,
arxiv2024,Non-autoregressive Sequence-to-Sequence Vision-Language Models,Yes.,3,"""their applicability is limited by their inference latency due to their autoregressive way of generating predictions.""",2024,2024-03-04T17:34:59Z,,,
arxiv2024,Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks,Yes.,1,"""This paper presents the development of a custom Large Language Model (LLM) for 5G and next-generation intent-based networking and provides insights into future LLM developments and integrations to realize end-to-end intent-based networking for fully automated network intelligence.""",2024,2024-03-04T17:29:57Z,,,
arxiv2024,TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models,Yes.,1,"""It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learning.""",2024,2024-03-04T17:08:57Z,,,
arxiv2024,Not all Layers of LLMs are Necessary during Inference,Yes.,3,"""The inference phase of Large Language Models (LLMs) is very expensive.""",2024,2024-03-04T16:23:58Z,,,
arxiv2024,Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models,Yes.,3,"""In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains.""",2024,2024-03-04T16:21:54Z,,,
arxiv2024,Using LLMs for the Extraction and Normalization of Product Attribute Values,Yes.,1,"""This paper explores the potential of using large language models (LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and product descriptions.""",2024,2024-03-04T15:39:59Z,,,
arxiv2024,Large language models surpass human experts in predicting neuroscience results,Yes.,1,"""Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts.""",2024,2024-03-04T15:27:59Z,,,
arxiv2024,adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds,Yes.,2,"""Despite the exciting potential of this technology, its impact on developing high-quality Machine Translation (MT) outputs for low-resource languages remains relatively under-explored.""",2024,2024-03-04T14:49:18Z,,,
arxiv2024,Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5,Yes.,3,"""Post-hoc qualitative analysis reveals several points for improvement in future work including cross-referencing part-of-speech tagging, better sentence validation, and improving GPT prompts.""",2024,2024-03-04T14:24:47Z,,,
arxiv2024,Unveiling Hidden Links Between Unseen Security Entities,Yes.,1,"""Leveraging ULTRA, a knowledge graph foundation model, combined with a Large Language Model (LLM), VulnScopper effectively handles unseen entities, overcoming the limitations of previous KG approaches.""",2024,2024-03-04T13:14:39Z,,,
arxiv2024,LLM-Oriented Retrieval Tuner,Yes.,3,"""However, due to the paradigm discrepancy between text generation of LLM and DR, it is still an open challenge to integrate the retrieval and generation tasks in a shared LLM.""",2024,2024-03-04T12:50:25Z,,,
arxiv2024,Vanilla Transformers are Transfer Capability Teachers,No.,1,"The abstract discusses Mixture of Experts (MoE) Transformers and vanilla Transformers, but does not mention language models (LLMs or LMs) or their limitations.",2024,2024-03-04T12:40:28Z,,,
arxiv2024,SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis,Yes.,2,"""Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in scientific literature analysis, especially in scenarios involving complex comprehension and multimodal data.""",2024,2024-03-04T12:19:28Z,,,
arxiv2024,Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models,Yes.,3,"""However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results.""",2024,2024-03-04T12:16:15Z,,,
arxiv2024,AS-ES Learning: Towards Efficient CoT Learning in Small Models,Yes.,3,"""existing methods often simply generate and incorporate more data from LLMs and fail to note the importance of efficiently utilizing existing CoT data,"" and ""we explore the reason behind the inefficiency of small models in learning CoT.""",2024,2024-03-04T12:13:59Z,,,
arxiv2024,To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering,Yes.,1,"""an alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting.""",2024,2024-03-04T10:41:52Z,,,
arxiv2024,LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset,Yes.,1,"""an innovative application of the Claude 2 large language model to classify cases based on content-specific prompts.""",2024,2024-03-04T10:13:30Z,,,
arxiv2024,Online Training of Large Language Models: Learn while chatting,Yes.,3,"""existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning.""",2024,2024-03-04T10:00:55Z,,,
arxiv2024,Predicting Learning Performance with Large Language Models: A Study in Adult Literacy,Yes.,2,"""While XGBoost (trained on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior performance compared to local machine execution. Moreover, our investigation into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable performance, albeit with less stability in the automated approach, using",2024,2024-03-04T08:14:07Z,,,
arxiv2024,CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text,Yes.,2,"""current evaluation methods are either limited in task coverage or lack standardization.""",2024,2024-03-04T07:26:07Z,,,
arxiv2024,Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models,Yes.,1,"""Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters.""",2024,2024-03-04T06:20:31Z,,,
arxiv2024,Differentially Private Synthetic Data via Foundation Model APIs 2: Text,Yes.,2,"""existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs.""",2024,2024-03-04T05:57:50Z,,,
arxiv2024,Decode Neural signal as Speech,Yes.,2,"""However, the exploration is not adequate in three aspects",2024,2024-03-04T05:55:01Z,,,
arxiv2024,Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study,Yes.,3,"""state-of-the-art models such as GPT-4 generate relevant and accurate Design Decisions, although they fall short of human-level performance.""",2024,2024-03-04T03:56:14Z,,,
arxiv2024,Improving LLM Code Generation with Grammar Augmentation,Yes.,1,"""We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs).""",2024,2024-03-03T22:38:35Z,,,
arxiv2024,Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models,Yes.,1,"""This paper presents our contributions towards advancing the state of Vietnamese language understanding and generation through the development and dissemination of open datasets and pre-trained models for Vietnamese Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).""",2024,2024-03-03T21:24:35Z,,,
arxiv2024,SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos,Yes.,1,"""we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thought prompting.""",2024,2024-03-03T19:53:06Z,,,
arxiv2024,ReMatch: Retrieval Enhanced Schema Matching with LLMs,Yes.,1,"""In this paper we present a novel method, named ReMatch, for matching schemas using retrieval-enhanced Large Language Models (LLMs).""",2024,2024-03-03T17:14:40Z,,,
arxiv2024,Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics,Yes.,3,"""the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures.""",2024,2024-03-03T13:14:47Z,,,
arxiv2024,Infusing Knowledge into Large Language Models with Contextual Prompts,Yes.,1,"""Knowledge infusion is a promising method for enhancing Large Language Models for domain-specific NLP tasks rather than pre-training models over large data from scratch.""",2024,2024-03-03T11:19:26Z,,,
arxiv2024,Logic Rules as Explanations for Legal Case Retrieval,Yes.,1,"""introducing a novel explainability metric using Large Language Models (LLMs).""",2024,2024-03-03T09:22:21Z,,,
arxiv2024,GuardT2I: Defending Text-to-Image Models from Adversarial Prompts,Yes.,1,"""GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance.""",2024,2024-03-03T09:04:34Z,,,
arxiv2024,MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies,Yes.,3,"""These models have shown promise in analyzing short video clips. However, when it comes to longer formats like movies, they often fall short.""",2024,2024-03-03T07:43:39Z,,,
arxiv2024,The Implicit Bias of Heterogeneity towards Invariance and Causality,Yes.,3,"""It is a mystery why causality, in a higher layer of understanding, can emerge from the regression task that pursues associations.""",2024,2024-03-03T07:38:24Z,,,
arxiv2024,On the Compressibility of Quantized Large Language Models,Yes.,3,"""However, it also faces critical challenges due to the substantial memory requirement of LLMs"" and ""even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference.""",2024,2024-03-03T03:27:07Z,,,
arxiv2024,Automatic Question-Answer Generation for Long-Tail Knowledge,Yes.,3,"""While they exhibit high accuracy in answering questions related to common knowledge, LLMs encounter difficulties in learning about uncommon long-tail knowledge (tail entities).""",2024,2024-03-03T03:06:31Z,,,
arxiv2024,LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems,Yes.,3,"""However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts.""",2024,2024-03-02T23:32:33Z,,,
arxiv2024,A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition,Yes.,1,"""Additionally, our introduction of Large Language Model (LLM) Integrated Scoring Adjustment (LISA) significantly improves recognition accuracy.""",2024,2024-03-02T21:15:24Z,,,
arxiv2024,Improving the Validity of Automatically Generated Feedback via Reinforcement Learning,Yes.,2,"""However, both feedback generation and evaluation are challenging",2024,2024-03-02T20:25:50Z,,,
arxiv2024,NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention,Yes.,2,"""Large language model inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations.""",2024,2024-03-02T17:29:22Z,,,
arxiv2024,Dissecting Language Models: Machine Unlearning via Selective Pruning,Yes.,1,"""This paper introduces a machine unlearning method specifically designed for LLMs.""",2024,2024-03-02T17:10:44Z,,,
arxiv2024,AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks,Yes.,3,"""Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks.""",2024,2024-03-02T16:52:22Z,,,
arxiv2024,Accelerating Greedy Coordinate Gradient via Probe Sampling,Yes.,3,"""Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications."" and ""Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming",2024,2024-03-02T16:23:44Z,,,
arxiv2024,SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code,Yes.,1,"""This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets.""",2024,2024-03-02T16:16:26Z,,,
arxiv2024,IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact,Yes.,3,"""Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance.""",2024,2024-03-02T16:05:26Z,,,
arxiv2024,API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access,Yes.,3,"""This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access."" and ""However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance.""",2024,2024-03-02T14:14:45Z,,,
arxiv2024,Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning,Yes.,1,"""This paper proposes a novel framework for multi-label image recognition without any training data, called data-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt pretrained Vision-Language Model (VLM) like CLIP to multilabel classification.""",2024,2024-03-02T13:43:32Z,,,
arxiv2024,The Case for Animal-Friendly AI,Yes.,2,"""Preliminary results suggest that the outcomes of the tested models can be benchmarked regarding the consideration they give to animals, and that generated positions and biases might be addressed and mitigated with more developed and validated systems.""",2024,2024-03-02T12:41:11Z,,,
arxiv2024,DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling,Yes.,2,"""There remain two challenges in RM training",2024,2024-03-02T12:31:22Z,,,
arxiv2024,Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding,Yes.,3,"""Finetuning approaches in NLP often focus on exploitation rather than exploration, which may lead to suboptimal models. Given the vast search space of natural language, this limited exploration can restrict their performance in complex, high-stakes domains, where accurate negation understanding and logical reasoning abilities are crucial.""",2024,2024-03-02T11:54:55Z,,,
arxiv2024,MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining,Yes.,1,"""Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored.""",2024,2024-03-02T09:27:32Z,,,
arxiv2024,ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies,Yes.,3,"""We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (~13% gap) after a light supervision. ... Lastly, we show challenging distractors confuse LLMs, but not humans.""",2024,2024-03-02T08:53:40Z,,,
arxiv2024,LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization,Yes.,3,"""The immense sizes of LLMs have led to very high resource demand and cost for running the models.""",2024,2024-03-02T08:40:07Z,,,
arxiv2024,Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data,Yes.,3,"""In the first phase, we investigate the challenges an LLM like GPT-4 faces in comprehending raw sensor data.""",2024,2024-03-02T08:29:08Z,,,
arxiv2024,LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation,Yes.,3,"""However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge.""",2024,2024-03-02T08:21:59Z,,,
arxiv2024,OpenGraph: Towards Open Graph Foundation Models,Yes.,2,"""we introduce a data augmentation mechanism enhanced by a LLM to alleviate the limitations of data scarcity in real-world scenarios.""",2024,2024-03-02T08:05:03Z,,,
arxiv2024,Towards Accurate Lip-to-Speech Synthesis in-the-Wild,No.,1,The abstract does not mention LLMs or their limitations.,2024,2024-03-02T04:07:24Z,,,
arxiv2024,LLMCRIT: Teaching Large Language Models to Use Criteria,Yes.,2,"""However, existing research in this field tends to consider only a limited set of criteria or quality assessment aspects.""",2024,2024-03-02T02:25:55Z,,,
arxiv2024,FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis,Yes.,3,"""it is difficult to integrate effectively with established techniques, including graph-based models and linguistics, because modifying their internal architecture is not easy.""",2024,2024-03-02T02:00:51Z,,,
arxiv2024,Towards Full Authorship with AI: Supporting Revision with AI-Generated Views,Yes.,3,"""This paradigm shifts some creative control from the user to the system, thereby diminishing the user's authorship and autonomy in the writing process."" and ""However, the study also showed interaction design challenges related to document navigation and scoping, prompt engineering, and context management.""",2024,2024-03-02T01:11:35Z,,,
arxiv2024,AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks,Yes.,2,"""However, so far there lacks a comprehensive study regarding whether LLM-based systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or 'hands-on-keyboard' attacks, under various attack techniques and environments.""",2024,2024-03-02T00:10:45Z,,,
arxiv2024,Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks,Yes.,2,"""However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic.""",2024,2024-03-01T23:38:02Z,,,
arxiv2024,ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys,Yes.,1,"""Our evaluations on various models (e.g., BERT and Llama) demonstrate that ATP achieves comparable accuracy with much lower computation and memory complexity than the standard attention mechanism.""",2024,2024-03-01T19:24:37Z,,,
arxiv2024,Differentially Private Knowledge Distillation via Synthetic Text Generation,Yes.,3,"""Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss.""",2024,2024-03-01T19:22:24Z,,,
arxiv2024,An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce,Yes.,3,"""Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare",2024,2024-03-01T19:08:25Z,,,
arxiv2024,AtP*: An efficient and scalable method for localizing LLM behaviour to components,Yes.,3,"""We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives.""",2024,2024-03-01T18:43:51Z,,,
arxiv2024,Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction,Yes.,1,"""A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories."" and ""We fine-tune two BERT LMs as baselines, as well as eight open-source",2024,2024-03-01T13:36:04Z,,,
arxiv2024,Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition,Yes.,1,"""Recent advances in LLMs have sparked a debate on whether they understand text.""",2024,2024-03-01T12:42:47Z,,,
arxiv2024,LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues,Yes.,2,"""Yet a major bottleneck to achieving genuinely transformative task-oriented dialogue capabilities remains the scarcity of high quality and linguistically sophisticated data.""",2024,2024-03-01T11:33:53Z,,,
arxiv2024,Hierarchical Indexing for Retrieval-Augmented Opinion Summarization,Yes.,1,"""We propose a method for unsupervised abstractive opinion summarization, that combines the attributability and scalability of extractive approaches with the coherence and fluency of Large Language Models (LLMs).""",2024,2024-03-01T10:38:07Z,,,
arxiv2024,LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness,Yes.,3,"""their performance is heavily influenced by prompt design"" and ""Recognizing the subjective nature of TSA, we evaluate the ability of LLMs to quantify predictive uncertainty via calibration error and correlation to human inter-annotator agreement.""",2024,2024-03-01T10:10:34Z,,,
arxiv2024,Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models,Yes.,3,"""Despite subword tokenizers like Byte Pair Encoding (BPE) overcoming many word tokenizer limitations, they encounter difficulties in handling non-Latin languages and depend heavily on extensive training data and computational resources to grasp the nuances of multiword expressions (MWEs).""",2024,2024-03-01T10:03:07Z,,,
arxiv2024,Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment,Yes.,1,"""We propose a method to support metadata enrichment with topic annotations of column headers using three Large Language Models (LLMs)",2024,2024-03-01T10:01:36Z,,,
arxiv2024,Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models,Yes.,1,"""We further enhance LLMs' reasoning abilities through a novel Retrieval-Aware Training (RAT) process and its refined iteration, RAT-R.""",2024,2024-03-01T08:43:43Z,,,
arxiv2024,Never-Ending Embodied Robot Learning,Yes.,3,"""However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks.""",2024,2024-03-01T07:51:29Z,,,
arxiv2024,SoftTiger: A Clinical Foundation Model for Healthcare Workflows,Yes.,3,"""Moreover, we address several modeling challenges in the healthcare context, e.g., extra long context window.""",2024,2024-03-01T04:39:16Z,,,
arxiv2024,Improving Socratic Question Generation using Data Augmentation and Preference Optimization,Yes.,3,"""existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions.""",2024,2024-03-01T00:08:20Z,,,
arxiv2024,LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction,Yes.,2,"""Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters.""",2024,2024-02-29T23:03:19Z,,,
arxiv2024,TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision,Yes.,3,"""Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt.""",2024,2024-02-29T22:26:07Z,,,
arxiv2024,LLMs in Political Science: Heralding a New Era of Visual Analysis,Yes.,1,"""This landscape could potentially change thanks to the rise of large language models (LLMs).""",2024,2024-02-29T22:11:20Z,,,
arxiv2024,UniTS: Building a Unified Time Series Model,Yes.,3,"""current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models.""",2024,2024-02-29T21:25:58Z,,,
arxiv2024,LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario,Yes.,3,"""such a handy share-and-play setting opens up new attack surfaces, that the attacker can render LoRA as an attacker, such as backdoor injection, and widely distribute the adversarial LoRA to the community easily. This can result in detrimental outcomes.""",2024,2024-02-29T20:25:16Z,,,
arxiv2024,The All-Seeing Project V2: Towards General Relation Comprehension of the Open World,Yes.,3,"""diminishing the relation hallucination often encountered by Multi-modal Large Language Models (MLLMs).""",2024,2024-02-29T18:59:17Z,,,
arxiv2024,Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling,Yes.,3,"""In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines.""",2024,2024-02-29T18:58:15Z,,,
arxiv2024,Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models,Yes.,3,"""We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics.""",2024,2024-02-29T18:47:52Z,,,
arxiv2024,ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL,Yes.,3,"""current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks.""",2024,2024-02-29T18:45:56Z,,,
arxiv2024,Compositional API Recommendation for Library-Oriented Code Generation,Yes.,3,"""However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs.""",2024,2024-02-29T18:27:27Z,,,
arxiv2024,Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines,Yes.,3,"""the intricate nature of LLMs renders their 'cognitive' processes opaque, challenging even their designers' understanding.""",2024,2024-02-29T18:20:37Z,,,
arxiv2024,Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy,Yes.,2,"""Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate.""",2024,2024-02-29T17:27:59Z,,,
arxiv2024,OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models,Yes.,2,"""Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups.""",2024,2024-02-29T17:19:39Z,,,
arxiv2024,SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency,Yes.,3,"""The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs.""",2024,2024-02-29T17:13:44Z,,,
arxiv2024,"Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook",Yes.,1,"""Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field.""",2024,2024-02-29T16:56:23Z,,,
arxiv2024,Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction,Yes.,2,"""existing methods leverage coarse-grained pathogenetic descriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks.""",2024,2024-02-29T16:29:53Z,,,
arxiv2024,RL-GPT: Integrating Reinforcement Learning and Code-as-policy,Yes.,3,"""Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control.""",2024,2024-02-29T16:07:22Z,,,
arxiv2024,PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval,Yes.,3,"""general-purpose large language models often struggle to meet the specific needs of planners.""",2024,2024-02-29T15:41:20Z,,,
arxiv2024,Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark,Yes.,3,"""Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well.""",2024,2024-02-29T15:22:13Z,,,
arxiv2024,RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks,Yes.,1,"""Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to help unveil the hidden code of RNA.""",2024,2024-02-29T14:50:58Z,,,
arxiv2024,PeLLE: Encoder-based language models for Brazilian Portuguese based on open data,Yes.,1,"""We also evaluate PeLLE models against a set of existing multilingual and PT-BR refined pretrained Transformer-based LLM encoders, contrasting performance of large versus smaller-but-curated pretrained models in several downstream tasks.""",2024,2024-02-29T14:34:03Z,,,
arxiv2024,PRSA: Prompt Reverse Stealing Attacks against Large Language Models,Yes.,2,"""this problem still has not been comprehensively explored yet"" and ""PRSA poses a severe threat in real world scenarios.""",2024,2024-02-29T14:30:28Z,,,
arxiv2024,Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning,Yes.,2,"""the model's pre-training may not naturally encompass these principles, necessitating the need to inspire or teach the model.""",2024,2024-02-29T13:49:56Z,,,
arxiv2024,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study,Yes.,3,"""However, these models share the same weakness by demonstrating a potential for improvement in the opportunity costs and perceived effectiveness metrics.""",2024,2024-02-29T11:29:47Z,,,
arxiv2024,"FhGenie: A Custom, Confidentiality-preserving Chat AI for Corporate and Scientific Use",Yes.,2,"""However, the use of free public services poses a risk of data leakage, as service providers may exploit user input for additional training and optimization without clear boundaries.""",2024,2024-02-29T09:43:50Z,,,
arxiv2024,EyeGPT: Ophthalmic Assistant with Large Language Models,Yes.,3,"""large language models (LLM) trained with general world knowledge might not possess the capability to tackle medical-related tasks at an expert level.""",2024,2024-02-29T09:35:41Z,,,
arxiv2024,How do Large Language Models Handle Multilingualism?,Yes.,1,"""Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages.""",2024,2024-02-29T02:55:26Z,,,
arxiv2024,On the Decision-Making Abilities in Role-Playing using Large Language Models,Yes.,1,"""Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks.""",2024,2024-02-29T02:22:23Z,,,
arxiv2024,FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning,Yes.,1,"""Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks.""",2024,2024-02-29T01:33:08Z,,,
arxiv2024,NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models,Yes.,3,"""expensive training as well as inference remains a significant impediment to their widespread applicability.""",2024,2024-02-28T22:21:47Z,,,
arxiv2024,Commonsense Ontology Micropatterns,Yes.,1,"""Large Language Models have quickly become a source of common knowledge and, in some cases, replacing search engines for questions.""",2024,2024-02-28T21:23:54Z,,,
arxiv2024,CLLMs: Consistency Large Language Models,Yes.,3,"""Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step.""",2024,2024-02-28T20:17:04Z,,,
arxiv2024,Data Interpreter: An LLM Agent For Data Science,Yes.,3,"""However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning.""",2024,2024-02-28T19:49:55Z,,,
arxiv2024,Simple linear attention language models balance the recall-throughput tradeoff,Yes.,3,"""the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption"" and ""efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall.""",2024,2024-02-28T19:28:27Z,,,
arxiv2024,Large Language Models and Games: A Survey and Roadmap,Yes.,2,"""we reconcile the potential and limitations of LLMs within the games domain.""",2024,2024-02-28T19:09:08Z,,,
arxiv2024,Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards,Yes.,3,"""Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications.""",2024,2024-02-28T18:58:25Z,,,
arxiv2024,Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification,Yes.,2,"""it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness.""",2024,2024-02-28T17:29:27Z,,,
arxiv2024,Language Models Represent Beliefs of Self and Others,Yes.,1,"""While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive.""",2024,2024-02-28T17:25:59Z,,,
arxiv2024,The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA,Yes.,1,"""which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs).""",2024,2024-02-28T15:05:43Z,,,
arxiv2024,Large Language Models As Evolution Strategies,Yes.,1,"""Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms.""",2024,2024-02-28T15:02:17Z,,,
arxiv2024,How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning,Yes.,2,"""a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation.""",2024,2024-02-28T13:14:20Z,,,
arxiv2024,Retrieval-based Full-length Wikipedia Generation for Emergent Events,Yes.,2,"""To ensure that Large Language Models (LLMs) are not trained on corpora related to recently occurred events"" and ""evaluate the capability of LLMs in generating factual full-length Wikipedia documents.""",2024,2024-02-28T11:51:56Z,,,
arxiv2024,Towards Generalist Prompting for Large Language Models by Mental Models,Yes.,3,"""However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks.""",2024,2024-02-28T11:29:09Z,,,
arxiv2024,Prospect Personalized Recommendation on Large Language Model-based Agent Platform,Yes.,2,"""Lastly, we discuss potential issues and promising directions for future research.""",2024,2024-02-28T11:12:17Z,,,
arxiv2024,CogBench: a large language model walks into a psychology lab,Yes.,2,"""evaluating them comprehensively remains challenging"" and ""Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior.""",2024,2024-02-28T10:43:54Z,,,
arxiv2024,Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging,Yes.,1,"""to enhance the merging of log templates, we design a chain-of-thought method for large language models (LLMs).""",2024,2024-02-28T09:51:55Z,,,
arxiv2024,MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery,Yes.,1,"""we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions.""",2024,2024-02-28T08:57:42Z,,,
arxiv2024,From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs,Yes.,1,"""Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence.""",2024,2024-02-28T08:42:23Z,,,
arxiv2024,MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices,Yes.,3,"""However, deploying LLMs in resource-constrained edge computing and embedded systems presents significant challenges.""",2024,2024-02-28T08:30:49Z,,,
arxiv2024,Cause and Effect: Can Large Language Models Truly Understand Causality?,Yes.,3,"""With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails.""",2024,2024-02-28T08:02:14Z,,,
arxiv2024,Small But Funny: A Feedback-Driven Approach to Humor Distillation,Yes.,3,"""While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation.""",2024,2024-02-28T07:02:38Z,,,
arxiv2024,Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models,Yes.,3,"""current model editing methods struggle with the specialization and complexity of medical knowledge.""",2024,2024-02-28T06:40:57Z,,,
arxiv2024,Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models,Yes.,3,"""Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other.""",2024,2024-02-28T05:43:22Z,,,
arxiv2024,Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension,Yes.,3,"""these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs.""",2024,2024-02-28T04:56:21Z,,,
arxiv2024,Datasets for Large Language Models: A Comprehensive Survey,Yes.,3,"""The survey sheds light on the prevailing challenges and points out potential avenues for future investigation.""",2024,2024-02-28T04:35:51Z,,,
arxiv2024,ResLoRA: Identity Residual Mapping in Low-Rank Adaption,Yes.,1,"""updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model.""",2024,2024-02-28T04:33:20Z,,,
arxiv2024,Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions,Yes.,3,"""Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages.""",2024,2024-02-28T03:44:01Z,,,
arxiv2024,Merino: Entropy-driven Design for Generative Language Models on IoT Devices,Yes.,3,"""However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost.""",2024,2024-02-28T03:20:27Z,,,
arxiv2024,A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems,Yes.,2,"""discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.""",2024,2024-02-28T03:16:44Z,,,
arxiv2024,Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization,Yes.,3,"""Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information.""",2024,2024-02-28T02:40:09Z,,,
arxiv2024,Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars,Yes.,3,"""Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct.""",2024,2024-02-28T02:12:47Z,,,
arxiv2024,EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models,Yes.,1,"""This paper introduces EmMark, a novel watermarking framework for protecting the intellectual property (IP) of embedded large language models deployed on resource-constrained edge devices.""",2024,2024-02-27T23:30:17Z,,,
arxiv2024,Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures,Yes.,2,"""Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive.""",2024,2024-02-27T23:12:45Z,,,
arxiv2024,Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning,Yes.,1,"""using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan.""",2024,2024-02-27T23:06:53Z,,,
arxiv2024,A Language Model based Framework for New Concept Placement in Ontologies,Yes.,3,"""Zero-shot prompting of LLMs is still not adequate for the task, and we propose explainable instruction tuning of LLMs for improved performance.""",2024,2024-02-27T21:27:35Z,,,
arxiv2024,"Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents",Yes.,3,"""Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs)."" and ""good performance on these benchmarks provides a false sense of security.""",2024,2024-02-27T21:27:16Z,,,
arxiv2024,JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability,Yes.,1,"""This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks.""",2024,2024-02-27T21:01:41Z,,,
arxiv2024,Deep Learning Detection Method for Large Language Models-Generated Scientific Content,Yes.,2,"""LLMs carry severe consequences for the scientific community, which relies on the integrity and reliability of publications.""",2024,2024-02-27T19:16:39Z,,,
arxiv2024,Self-Refinement of Language Models from External Proxy Metrics Feedback,Yes.,1,"""In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response.""",2024,2024-02-27T19:13:01Z,,,
arxiv2024,The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits,Yes.,1,"""Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs).""",2024,2024-02-27T18:56:19Z,,,
arxiv2024,Massive Activations in Large Language Models,Yes.,3,"""We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others... these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output.""",2024,2024-02-27T18:55:17Z,,,
arxiv2024,Tower: An Open Multilingual Large Language Model for Translation-Related Tasks,Yes.,1,"""While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task.""",2024,2024-02-27T18:09:36Z,,,
arxiv2024,Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs,Yes.,3,"""Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning.""",2024,2024-02-27T16:19:37Z,,,
arxiv2024,SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation,Yes.,1,"""We present SongComposer, an innovative LLM designed for song composition.""",2024,2024-02-27T16:15:28Z,,,
arxiv2024,Variational Learning is Effective for Large Deep Networks,Yes.,1,"""We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data.""",2024,2024-02-27T16:11:05Z,,,
arxiv2024,Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization,Yes.,3,"""Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games.""",2024,2024-02-27T15:09:20Z,,,
arxiv2024,DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation,Yes.,1,"""DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropagation.""",2024,2024-02-27T14:51:11Z,,,
arxiv2024,Retrieval is Accurate Generation,Yes.,1,"""Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation.""",2024,2024-02-27T14:16:19Z,,,
arxiv2024,BASES: Large-scale Web Search User Simulation with Large Language Model based Agents,Yes.,1,"""Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation.""",2024,2024-02-27T13:44:09Z,,,
arxiv2024,Intensive Care as One Big Sequence Modeling Problem,Yes.,1,"""previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning.""",2024,2024-02-27T13:36:55Z,,,
arxiv2024,Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?,Yes.,1,"""Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care.""",2024,2024-02-27T13:18:00Z,,,
arxiv2024,DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning,Yes.,3,"""existing LLM agents are hindered by generating unreasonable experiment plans within this scenario.""",2024,2024-02-27T12:26:07Z,,,
arxiv2024,Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder,Yes.,2,"""EEG-based language decoding is still in its nascent stages, facing several technical issues such as",2024,2024-02-27T11:45:21Z,,,
arxiv2024,Investigating Continual Pretraining in Large Language Models: Insights and Implications,Yes.,3,"""Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting,"" and ""smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning.""",2024,2024-02-27T10:47:24Z,,,
arxiv2024,Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies,Yes.,3,"""At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution.""",2024,2024-02-27T10:44:52Z,,,
arxiv2024,Determinants of LLM-assisted Decision-Making,Yes.,2,"""understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions.""",2024,2024-02-27T10:24:50Z,,,
arxiv2024,RECOST: External Knowledge Guided Data-efficient Instruction Tuning,Yes.,3,"""Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples.""",2024,2024-02-27T09:47:36Z,,,
arxiv2024,Probing Multimodal Large Language Models for Global and Local Semantic Representations,Yes.,3,"""We find that the topmost layers may excessively focus on local information, leading to a diminished ability to encode global information.""",2024,2024-02-27T08:27:15Z,,,
arxiv2024,Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning,Yes.,3,"""However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning.""",2024,2024-02-27T07:14:12Z,,,
arxiv2024,MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning,Yes.,3,"""We further observe that TALMs are not as effective for simpler math word problems (in GSM-8K), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH).""",2024,2024-02-27T05:50:35Z,,,
arxiv2024,"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",Yes.,3,"""our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited.""",2024,2024-02-27T04:18:49Z,,,
arxiv2024,"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",No.,1,"The abstract focuses on Sora, a text-to-video generative AI model, and does not discuss language models (LLMs or LMs).",2024,2024-02-27T03:30:58Z,,,
arxiv2024,Benchmarking Data Science Agents,Yes.,3,"""Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process.""",2024,2024-02-27T03:03:06Z,,,
arxiv2024,Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation,Yes.,3,"""While these translation models have greatly improved the overall translation accuracy, surpassing 70% on NLIDB benchmarks, the use of auto-regressive decoding to generate single SQL queries may result in sub-optimal outputs, potentially leading to erroneous translations.""",2024,2024-02-27T02:16:07Z,,,
arxiv2024,Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models,Yes.,3,"""we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances.""",2024,2024-02-27T01:37:23Z,,,
arxiv2024,Creating Suspenseful Stories: Iterative Planning with Large Language Models,Yes.,3,"""state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation.""",2024,2024-02-27T01:25:52Z,,,
arxiv2024,Sinkhorn Distance Minimization for Knowledge Distillation,Yes.,3,"""However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student."" and ""we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and",2024,2024-02-27T01:13:58Z,,,
arxiv2024,Adapting to Teammates in a Cooperative Language Game,Yes.,2,"""Previous approaches to designing agents for this game have utilized a single internal language model to determine action choices. This often leads to good performance with some teammates and inferior performance with other teammates, as the agent cannot adapt to any specific teammate.""",2024,2024-02-26T23:15:07Z,,,
arxiv2024,Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling,Yes.,1,"""In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts.""",2024,2024-02-26T20:56:06Z,,,
arxiv2024,Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset,No.,1,The abstract does not mention LLMs or any kind of language models.,2024,2024-02-26T20:42:40Z,,,
arxiv2024,Can Large Language Models Recall Reference Location Like Humans?,Yes.,1,"""This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position.""",2024,2024-02-26T20:35:32Z,,,
arxiv2024,Benchmarking LLMs on the Semantic Overlap Summarization Task,Yes.,3,"""We conclude the paper by analyzing the strengths and limitations of various LLMs in terms of their capabilities in capturing overlapping information.""",2024,2024-02-26T20:33:50Z,,,
arxiv2024,WIPI: A New Web Threat for LLM-Driven Web Agents,Yes.,2,"""an essential and pressing question arises",2024,2024-02-26T19:01:54Z,,,
arxiv2024,Do Large Language Models Latently Perform Multi-Hop Reasoning?,Yes.,3,"""However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop.""",2024,2024-02-26T18:57:54Z,,,
arxiv2024,Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections,Yes.,2,"""off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications.""",2024,2024-02-26T18:56:48Z,,,
arxiv2024,A Survey on Data Selection for Language Models,Yes.,2,"""naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary.""",2024,2024-02-26T18:54:35Z,,,
arxiv2024,Language Agents as Optimizable Graphs,Yes.,1,"""Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases.""",2024,2024-02-26T18:48:27Z,,,
arxiv2024,Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts,Yes.,3,"""Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations.""",2024,2024-02-26T18:47:27Z,,,
arxiv2024,OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA),Yes.,1,"""However, there is limited research on LLMs specifically addressing oncology-related queries.""",2024,2024-02-26T18:33:13Z,,,
arxiv2024,"If in a Crowdsourced Data Annotation Pipeline, a GPT-4",Yes.,1,"""Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy.""",2024,2024-02-26T18:08:52Z,,,
arxiv2024,CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models,Yes.,2,"""Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs).""",2024,2024-02-26T16:35:59Z,,,
arxiv2024,SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection,Yes.,1,"""Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions.""",2024,2024-02-26T16:21:53Z,,,
arxiv2024,Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models,Yes.,3,"""the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets.""",2024,2024-02-26T16:11:03Z,,,
arxiv2024,HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization,Yes.,2,"""These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs.""",2024,2024-02-26T16:09:00Z,,,
arxiv2024,ESG Sentiment Analysis: comparing human and language model performance including GPT,Yes.,1,"""Our study seeks to compare human performance with the cutting edge in machine performance in the measurement of ESG related sentiment.""",2024,2024-02-26T15:22:30Z,,,
arxiv2024,LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language,Yes.,3,"""Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts.""",2024,2024-02-26T15:05:16Z,,,
arxiv2024,Multi-Bit Distortion-Free Watermarking for Large Language Models,Yes.,1,"""Methods for watermarking large language models have been proposed that distinguish AI-generated text from human-generated text by slightly altering the model output distribution, but they also distort the quality of the text, exposing the watermark to adversarial detection.""",2024,2024-02-26T14:01:34Z,,,
arxiv2024,Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models,Yes.,1,"""Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area.""",2024,2024-02-26T13:47:09Z,,,
arxiv2024,Aligning Large Language Models to a Domain-specific Graph Database,Yes.,3,"""Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB.""",2024,2024-02-26T13:46:51Z,,,
arxiv2024,"Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup",Yes.,1,"""Retrieval Augmented Generation (RAG) systems have seen huge popularity in augmenting Large-Language Model (LLM) outputs with domain specific and time sensitive data.""",2024,2024-02-26T12:56:17Z,,,
arxiv2024,Integrating Large Language Models with Graphical Session-Based Recommendation,Yes.,3,"""The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs.""",2024,2024-02-26T12:55:51Z,,,
arxiv2024,LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification,Yes.,2,"""Differential privacy (DP) learning methods theoretically bound the protection but are not skilled at generating pseudo text samples with large models.""",2024,2024-02-26T11:52:55Z,,,
arxiv2024,On Languaging a Simulation Engine,Yes.,1,"""Depending on its functionalized type, each language model features a distinct processing of chat history to best balance its memory limit and information completeness, thus leveraging the model intelligence to unstructured nature of human request.""",2024,2024-02-26T11:01:54Z,,,
arxiv2024,RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering,Yes.,3,"""The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly.""",2024,2024-02-26T09:59:04Z,,,
arxiv2024,Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models,Yes.,1,"""Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora.""",2024,2024-02-26T09:36:05Z,,,
arxiv2024,RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions,Yes.,3,"""recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples.""",2024,2024-02-26T09:30:55Z,,,
arxiv2024,DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models,Yes.,3,"""Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture.""",2024,2024-02-26T09:21:59Z,,,
arxiv2024,HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy,Yes.,3,"""Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives.""",2024,2024-02-26T09:10:34Z,,,
arxiv2024,MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property,Yes.,3,"""Notably, the performance of current LLMs on the MoZIP benchmark has much room for improvement, and even the most powerful ChatGPT does not reach the passing level.""",2024,2024-02-26T08:27:50Z,,,
arxiv2024,Immunization against harmful fine-tuning attacks,Yes.,3,"""However, this focus overlooks another source of misalignment",2024,2024-02-26T08:08:03Z,,,
arxiv2024,Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models,Yes.,1,"""Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic.""",2024,2024-02-26T07:44:56Z,,,
arxiv2024,LLM Inference Unveiled: Survey and Roofline Model Insights,Yes.,3,"""This framework identifies the bottlenecks when deploying LLMs on hardware devices and provides a clear understanding of practical problems, such as why LLMs are memory-bound, how much memory and computation they need, and how to choose the right hardware.""",2024,2024-02-26T07:33:05Z,,,
arxiv2024,Language-guided Skill Learning with Temporal Variational Inference,Yes.,1,"""The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories.""",2024,2024-02-26T07:19:23Z,,,
arxiv2024,Personalized Federated Instruction Tuning via Neural Architecture Search,Yes.,2,"""The evaluation with multiple LLMs non-IID scenarios demonstrates that compared to the state-of-the-art FIT methods, our approach can achieve up to a 23% decrease in perplexity.""",2024,2024-02-26T06:29:05Z,,,
arxiv2024,Data-freeWeight Compress and Denoise for Large Language Models,Yes.,3,"""Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed.""",2024,2024-02-26T05:51:47Z,,,
arxiv2024,Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering,Yes.,3,"""open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis.""",2024,2024-02-26T05:31:34Z,,,
arxiv2024,"PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering",Yes.,1,"""Experimental results demonstrate that BERT-based classification models significantly outperform LLMs such as ChatGLM3 and ChatGPT in the memory classification task.""",2024,2024-02-26T04:09:53Z,,,
arxiv2024,From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto,Yes.,3,"""We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research challenges remain to be addressed.""",2024,2024-02-26T03:10:11Z,,,
arxiv2024,CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering,Yes.,3,"""the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer.""",2024,2024-02-26T01:17:50Z,,,
arxiv2024,Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing,Yes.,3,"""Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content.""",2024,2024-02-25T20:36:03Z,,,
arxiv2024,Attacking LLM Watermarks by Exploiting Their Strengths,Yes.,3,"""However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks.""",2024,2024-02-25T20:24:07Z,,,
arxiv2024,How Can LLM Guide RL? A Value-Based Approach,Yes.,3,"""recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback.""",2024,2024-02-25T20:07:13Z,,,
arxiv2024,ChatMusician: Understanding and Generating Music Intrinsically with LLM,Yes.,3,"""While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language."" and ""but there remains significant territory to be conquered.""",2024,2024-02-25T17:19:41Z,,,
arxiv2024,PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization,Yes.,3,"""Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices.""",2024,2024-02-25T16:43:41Z,,,
arxiv2024,What Generative Artificial Intelligence Means for Terminological Definitions,No.,1,The abstract discusses Generative Artificial Intelligence (GenAI) tools like ChatGPT in the context of terminological definitions but does not specifically mention LLMs or their limitations.,2024,2024-02-25T16:36:51Z,,,
arxiv2024,AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation,Yes.,1,"""This system harnesses the robust contextual reasoning and hallucination capability offered by Large Language Models (LLMs) to instruct the realistic synthesis of 3D talking faces.""",2024,2024-02-25T15:51:05Z,,,
arxiv2024,NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification,Yes.,3,"""We also show how generating controlled synthetic data using this workflow fixes some of the notable weaknesses of LLM-based generation.""",2024,2024-02-25T13:20:13Z,,,
arxiv2024,UrbanGPT: Spatio-Temporal Large Language Models,Yes.,1,"""Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is to create a spatio-temporal LLM that can exhibit exceptional generalization capabilities across a wide range of downstream urban tasks.""",2024,2024-02-25T12:37:29Z,,,
arxiv2024,How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study,Yes.,3,"""only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms."" and ""gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence.""",2024,2024-02-25T11:15:42Z,,,
arxiv2024,Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression,Yes.,3,"""Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference.""",2024,2024-02-25T11:07:08Z,,,
arxiv2024,LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding,Yes.,1,"""Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed",2024,2024-02-25T10:27:46Z,,,
arxiv2024,LLMs with Chain-of-Thought Are Non-Causal Reasoners,Yes.,3,"""Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa."" and ""highlight discrepancies between LLM and human reasoning processes.""",2024,2024-02-25T10:13:04Z,,,
arxiv2024,Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy,Yes.,3,"""machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues.""",2024,2024-02-25T09:44:56Z,,,
arxiv2024,EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings,Yes.,1,"""This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments.""",2024,2024-02-25T09:41:50Z,,,
arxiv2024,Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations,Yes.,3,"""In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle large-scale data sets, and technical strategies to protect user privacy.""",2024,2024-02-25T09:19:11Z,,,
arxiv2024,Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration,Yes.,3,"""recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives.""",2024,2024-02-25T08:45:10Z,,,
arxiv2024,GraphWiz: An Instruction-Following Language Model for Graph Problems,Yes.,2,"""Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored."" and ""highlighting the potential for overfitting with increased data.""",2024,2024-02-25T08:41:32Z,,,
arxiv2024,LoRA Meets Dropout under a Unified Framework,Yes.,2,"""a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked.""",2024,2024-02-25T07:09:10Z,,,
arxiv2024,Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale,Yes.,2,"""Recent large language models require considerable resources to train and deploy, resulting in significant energy usage, potential carbon emissions, and massive demand for GPUs and other hardware accelerators.""",2024,2024-02-25T02:22:34Z,,,
arxiv2024,GreenLLaMA: A Framework for Detoxification with Explanations,Yes.,2,"""Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified",2024,2024-02-25T01:56:47Z,,,
arxiv2024,Bootstrapping Cognitive Agents with a Large Language Model,Yes.,3,"""Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune.""",2024,2024-02-25T01:40:30Z,,,
arxiv2024,LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step,Yes.,3,"""However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations.""",2024,2024-02-25T00:56:27Z,,,
arxiv2024,QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs,Yes.,2,"""However, traditional studies do not provide formal guarantees on the performance of LLMs.""",2024,2024-02-24T23:16:57Z,,,
arxiv2024,Multimodal Instruction Tuning with Conditional Mixture of LoRA,Yes.,3,"""However, applying LoRA in multimodal instruction tuning presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of multimodal tasks.""",2024,2024-02-24T20:15:31Z,,,
arxiv2024,SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection,Yes.,1,"""Detection of machine-generated text is becoming an increasingly important task, with the advent of large language models (LLMs).""",2024,2024-02-24T17:44:56Z,,,
arxiv2024,SportQA: A Benchmark for Sports Understanding in Large Language Models,Yes.,3,"""Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise.""",2024,2024-02-24T17:12:10Z,,,
arxiv2024,MATHWELL: Generating Educational Math Word Problems at Scale,Yes.,1,"""We suggest that language models can support K-8 math education by automatically generating problems at scale.""",2024,2024-02-24T17:08:45Z,,,
arxiv2024,NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation,Yes.,1,"""we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap.""",2024,2024-02-24T16:39:16Z,,,
arxiv2024,Linguistic Intelligence in Large Language Models for Telecommunications,Yes.,2,"""We also observe that no single LLM consistently outperforms others, and the performance of different LLMs can fluctuate. Although their performance lags behind fine-tuned models, our findings underscore the potential of LLMs as a valuable resource for understanding various aspects of this field that lack large annotated data.""",2024,2024-02-24T14:01:07Z,,,
arxiv2024,PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA,Yes.,1,"""With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods.""",2024,2024-02-24T13:39:05Z,,,
arxiv2024,Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method,Yes.,3,"""We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance.""",2024,2024-02-24T13:36:58Z,,,
arxiv2024,OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining,Yes.,3,"""Extensive experiments reveal that even advanced algorithms like large language models (LLMs) encounter difficulties in addressing key challenges in certain tasks, such as paper source tracing and scholar profiling.""",2024,2024-02-24T13:15:54Z,,,
arxiv2024,Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models,Yes.,2,"""LLMs promise to revolutionize society, yet training these foundational models poses immense challenges.""",2024,2024-02-24T12:31:22Z,,,
arxiv2024,Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation,Yes.,1,"""TV-SAM incorporates and integrates large language model GPT-4, Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously generate descriptive text prompts and visual bounding box prompts from medical images.""",2024,2024-02-24T08:10:54Z,,,
arxiv2024,Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens,Yes.,3,"""their widespread application is hindered by the resource-intensive decoding process"" and ""the accuracy of these decoding heads falls short of the auto-regressive decoding approach.""",2024,2024-02-24T08:10:39Z,,,
arxiv2024,Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning,Yes.,3,"""However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks.""",2024,2024-02-24T07:22:04Z,,,
arxiv2024,MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation,Yes.,1,"""we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements.""",2024,2024-02-24T06:14:34Z,,,
arxiv2024,How Do Humans Write Code? Large Models Do It the Same Way Too,Yes.,3,"""Large Language Models (LLMs) often make errors when performing numerical calculations"" and ""we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language.""",2024,2024-02-24T05:40:01Z,,,
arxiv2024,LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper,Yes.,2,"""the defensive side has been relatively less explored.""",2024,2024-02-24T05:34:43Z,,,
arxiv2024,Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models,Yes.,3,"""Large Vision Language Models exhibit remarkable capabilities but struggle with hallucinations inconsistencies between images and their descriptions.""",2024,2024-02-24T05:14:52Z,,,
arxiv2024,Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors,Yes.,3,"""The primary challenges are catastrophic forgetting and overfitting.""",2024,2024-02-24T04:32:44Z,,,
arxiv2024,On Trojan Signatures in Large Language Models of Code,Yes.,3,"""Our results suggest that trojan signatures could not generalize to LLMs of code.""",2024,2024-02-23T22:48:29Z,,,
arxiv2024,Fine-Grained Self-Endorsement Improves Factuality and Reasoning,Yes.,2,"""This work studies improving large language model (LLM) generations at inference time by mitigating fact-conflicting hallucinations.""",2024,2024-02-23T22:24:40Z,,,
arxiv2024,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",Yes.,2,"""Training LLMs at this scale brings unprecedented challenges to training efficiency and stability.""",2024,2024-02-23T22:10:59Z,,,
arxiv2024,Towards Efficient Active Learning in NLP via Pretrained Representations,Yes.,1,"""Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications.""",2024,2024-02-23T21:28:59Z,,,
arxiv2024,"Selective ""Selective Prediction"": Reducing Unnecessary Abstention in Vision-Language Reasoning",Yes.,2,"""Prior work on selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain."" and ""ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if",2024,2024-02-23T21:16:52Z,,,
arxiv2024,Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis,Yes.,1,"""Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers.""",2024,2024-02-23T21:07:20Z,,,
arxiv2024,Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts,Yes.,1,"""Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting.""",2024,2024-02-23T20:14:16Z,,,
arxiv2024,AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning,Yes.,3,"""However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories.""",2024,2024-02-23T18:56:26Z,,,
arxiv2024,Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts,No.,1,The abstract does not mention LLMs or any specific language models.,2024,2024-02-23T18:56:11Z,,,
arxiv2024,Self-Retrieval: Building an Information Retrieval System with One Large Language Model,Yes.,1,"""Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models.""",2024,2024-02-23T18:45:35Z,,,
arxiv2024,API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs,Yes.,1,"""There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks.""",2024,2024-02-23T18:30:49Z,,,
arxiv2024,Repetition Improves Language Model Embeddings,Yes.,3,"""we address an architectural limitation of autoregressive models",2024,2024-02-23T17:25:10Z,,,
arxiv2024,PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning,Yes.,1,"""we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans.""",2024,2024-02-23T16:30:05Z,,,
arxiv2024,Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction,Yes.,3,"""However, these approaches lack mission performance and safety guarantees.""",2024,2024-02-23T15:02:44Z,,,
arxiv2024,Farsight: Fostering Responsible AI Awareness During AI Application Prototyping,Yes.,3,"""However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping.""",2024,2024-02-23T14:38:05Z,,,
arxiv2024,Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies,Yes.,3,"""Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare.""",2024,2024-02-23T14:17:01Z,,,
arxiv2024,GPTVQ: The Blessing of Dimensionality for LLM Quantization,Yes.,1,"""GPTVQ establishes a new state-of-the-art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral.""",2024,2024-02-23T13:39:16Z,,,
arxiv2024,ArabianGPT: Native Arabic GPT-based Large Language Model,Yes.,3,"""The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax.""",2024,2024-02-23T13:32:47Z,,,
arxiv2024,Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models,Yes.,1,"""The advance of large language models (LLMs) provides opportunities to address these problems.""",2024,2024-02-23T13:02:10Z,,,
arxiv2024,CFIR: Fast and Effective Long-Text To Image Retrieval for Large Corpora,Yes.,3,"""Although Multimodal Large Language Models (MLLMs) demonstrate state-of-the-art performance, they exhibit limitations in handling large-scale, diverse, and ambiguous real-world needs of retrieval, due to the computation cost and the injective embeddings they produce.""",2024,2024-02-23T11:47:16Z,,,
arxiv2024,"CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models",Yes.,3,"""However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs.""",2024,2024-02-23T11:25:17Z,,,
arxiv2024,DEEM: Dynamic Experienced Expert Modeling for Stance Detection,Yes.,3,"""considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis.""",2024,2024-02-23T11:24:00Z,,,
arxiv2024,Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues,Yes.,2,"""We assess the impact of this addition by testing two models",2024,2024-02-23T10:27:42Z,,,
arxiv2024,GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?,Yes.,2,"""Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training."" and ""To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs).""",2024,2024-02-23T10:02:01Z,,,
arxiv2024,ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition,Yes.,2,"""Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences.""",2024,2024-02-23T09:29:19Z,,,
arxiv2024,Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing,Yes.,2,"""However, realizing this vision involves addressing several socio-technical and practical research challenges.""",2024,2024-02-23T09:06:25Z,,,
arxiv2024,DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators,Yes.,3,"""This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts.""",2024,2024-02-23T09:01:00Z,,,
arxiv2024,GraphEdit: Large Language Models for Graph Structure Learning,Yes.,1,"""By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning.""",2024,2024-02-23T08:29:42Z,,,
arxiv2024,Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer,Yes.,3,"""Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process.""",2024,2024-02-23T08:11:55Z,,,
arxiv2024,Machine Unlearning of Pre-trained Large Language Models,Yes.,2,"""We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area.""",2024,2024-02-23T07:43:26Z,,,
arxiv2024,Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models,Yes.,3,"""fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge.""",2024,2024-02-23T06:32:18Z,,,
arxiv2024,AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System,Yes.,2,"""However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into LLM agents.""",2024,2024-02-23T06:25:20Z,,,
arxiv2024,Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,Yes.,3,"""However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications.""",2024,2024-02-23T06:11:50Z,,,
arxiv2024,Large Multimodal Agents: A Survey,Yes.,2,"""One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs.""",2024,2024-02-23T06:04:23Z,,,
arxiv2024,Executing Natural Language-Described Algorithms with Large Language Models: An Investigation,Yes.,3,"""Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved.""",2024,2024-02-23T05:31:36Z,,,
arxiv2024,Evaluating the Performance of ChatGPT for Spam Email Detection,Yes.,3,"""Though extensive experiments, the performance of ChatGPT is significantly worse than deep supervised learning methods in the large English dataset, while it presents superior performance on the low-resourced Chinese dataset, even outperforming BERT in this case.""",2024,2024-02-23T04:52:08Z,,,
arxiv2024,Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models,Yes.,1,"""We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection.""",2024,2024-02-23T02:58:12Z,,,
arxiv2024,On the Multi-turn Instruction Following for Conversational Web Agents,Yes.,3,"""To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks.""",2024,2024-02-23T02:18:12Z,,,
arxiv2024,Unlocking the Power of Large Language Models for Entity Alignment,Yes.,1,"""To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy.""",2024,2024-02-23T01:55:35Z,,,
arxiv2024,CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models,Yes.,3,"""Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways.""",2024,2024-02-22T23:42:25Z,,,
arxiv2024,Divide-or-Conquer? Which Part Should You Distill Your LLM?,Yes.,3,"""However, it is harder to distill the problem solving capability without losing performance and the resulting distilled model struggles with generalization.""",2024,2024-02-22T22:28:46Z,,,
arxiv2024,tinyBenchmarks: evaluating LLMs with fewer examples,Yes.,1,"""The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities.""",2024,2024-02-22T22:05:23Z,,,
arxiv2024,Optimizing Language Models for Human Preferences is a Causal Inference Problem,Yes.,1,"""As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences.""",2024,2024-02-22T21:36:07Z,,,
arxiv2024,AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation,Yes.,1,"""This paper explores twofold aspects of integrating LLMs into the creative process - the divergence stage of idea generation, and the convergence stage of evaluation and selection of ideas.""",2024,2024-02-22T21:34:52Z,,,
arxiv2024,GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data,Yes.,2,"""These benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation.""",2024,2024-02-22T21:22:04Z,,,
arxiv2024,Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment,Yes.,3,"""these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases"" and ""this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack).""",2024,2024-02-22T21:05:18Z,,,
arxiv2024,CriticBench: Benchmarking LLMs for Critique-Correct Reasoning,Yes.,3,"""Our findings reveal",2024,2024-02-22T18:59:02Z,,,
arxiv2024,MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,Yes.,1,"""This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns.""",2024,2024-02-22T18:58:55Z,,,
arxiv2024,Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models,Yes.,1,"""Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes.""",2024,2024-02-22T18:56:07Z,,,
arxiv2024,Watermarking Makes Language Models Radioactive,Yes.,1,"This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data.",2024,2024-02-22T18:55:22Z,,,
arxiv2024,Zero-shot cross-lingual transfer in instruction tuning of large language model,Yes.,3,"""English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.""",2024,2024-02-22T18:37:33Z,,,
arxiv2024,DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models,Yes.,3,"""Current MLLMs typically singularly focus on inputs at a predefined resolution, resulting in deficiencies in detailed questions involving local regions.""",2024,2024-02-22T18:26:02Z,,,
arxiv2024,Generalizing Reward Modeling for Out-of-Distribution Preference Learning,Yes.,3,"""Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging.""",2024,2024-02-22T18:20:33Z,,,
arxiv2024,Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs,Yes.,3,"""Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases."" and ""We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias.""",2024,2024-02-22T18:14:09Z,,,
arxiv2024,Scaling Efficient LLMs,Yes.,1,"""Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency.""",2024,2024-02-22T18:06:19Z,,,
arxiv2024,Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation,Yes.,1,"""LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks.""",2024,2024-02-22T18:03:14Z,,,
arxiv2024,Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs,Yes.,3,"""Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning.""",2024,2024-02-22T17:52:34Z,,,
arxiv2024,IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus,Yes.,3,"""Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE).""",2024,2024-02-22T17:11:38Z,,,
arxiv2024,An LLM-Enhanced Adversarial Editing System for Lexical Simplification,Yes.,1,"""Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system.""",2024,2024-02-22T17:04:30Z,,,
arxiv2024,Unveiling Linguistic Regions in Large Language Models,Yes.,3,"""freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common occurrence observed during further pre-training of LLMs.""",2024,2024-02-22T16:56:13Z,,,
arxiv2024,Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality,Yes.,3,"""Our goal is to evaluate the consistency between LLMs' professed personality inclinations and their actual 'behavior', examining the extent to which these models can emulate human-like personality patterns.""",2024,2024-02-22T16:32:08Z,,,
arxiv2024,LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition,Yes.,3,"""their performance on information extraction tasks is still not entirely satisfactory"" and ""to overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text.""",2024,2024-02-22T14:19:56Z,,,
arxiv2024,LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey,Yes.,2,"""our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context.""",2024,2024-02-22T13:52:02Z,,,
arxiv2024,"Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard",Yes.,1,"""Large Language Models (LLMs) are capable of generating text that is similar to or surpasses human quality.""",2024,2024-02-22T13:25:17Z,,,
arxiv2024,Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance,Yes.,1,"""We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs).""",2024,2024-02-22T13:24:10Z,,,
arxiv2024,Balanced Data Sampling for Language Model Training with Clustering,Yes.,2,"""Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal.""",2024,2024-02-22T13:20:53Z,,,
arxiv2024,Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond,Yes.,3,"""However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner.""",2024,2024-02-22T13:13:31Z,,,
arxiv2024,LLMBind: A Unified Modality-Task Integration Framework,Yes.,3,"""they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field.""",2024,2024-02-22T12:36:31Z,,,
arxiv2024,Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?,Yes.,1,"""This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text.""",2024,2024-02-22T11:16:23Z,,,
arxiv2024,Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph,Yes.,1,"""Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses.""",2024,2024-02-22T10:12:16Z,,,
arxiv2024,Uncertainty-Aware Evaluation for Vision-Language Models,Yes.,3,"""Our empirical findings also reveal a correlation between model uncertainty and its language model part.""",2024,2024-02-22T10:04:17Z,,,
arxiv2024,Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching,Yes.,2,"""a significant challenge remains for low-resource languages, where limited data hinders the effective training of such models.""",2024,2024-02-22T09:49:26Z,,,
arxiv2024,Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction,Yes.,2,"""An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches.""",2024,2024-02-22T08:26:56Z,,,
arxiv2024,OpenTab: Advancing Large Language Models as Open-domain Table Reasoners,Yes.,3,"""Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously."" and ""existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes.""",2024,2024-02-22T08:01:01Z,,,
arxiv2024,Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark,Yes.,3,"""LLMs have limitations in learning from in-context information in scientific domains.""",2024,2024-02-22T07:58:29Z,,,
arxiv2024,"Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",Yes.,3,"""Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning.""",2024,2024-02-22T07:55:26Z,,,
arxiv2024,Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering,Yes.,2,"""However, their use in answering questions from knowledge bases remains largely unexplored.""",2024,2024-02-22T06:23:37Z,,,
arxiv2024,Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge,Yes.,3,"""Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes.""",2024,2024-02-22T05:58:03Z,,,
arxiv2024,Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education,Yes.,1,"""This study investigates LLMs' capabilities in educational scenarios, focusing on concept graph recovery and question-answering (QA).""",2024,2024-02-22T05:15:27Z,,,
arxiv2024,CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations,Yes.,1,"""Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text.""",2024,2024-02-22T05:07:31Z,,,
arxiv2024,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,Yes.,2,"""However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners.""",2024,2024-02-22T04:55:14Z,,,
arxiv2024,Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization,Yes.,2,"""a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data.""",2024,2024-02-22T04:10:57Z,,,
arxiv2024,Can Large Language Models Detect Misinformation in Scientific News Reporting?,Yes.,1,"""The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting.""",2024,2024-02-22T04:07:00Z,,,
arxiv2024,Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming,Yes.,3,"""utilizing LLMs out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance.""",2024,2024-02-22T03:51:34Z,,,
arxiv2024,Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond,Yes.,1,"""we achieve a significant enhancement in the performance of LLMs when employing sequences with lower uncertainty, identified by WSE, as final answers.""",2024,2024-02-22T03:46:08Z,,,
arxiv2024,Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models,Yes.,3,"""However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details.""",2024,2024-02-22T03:14:03Z,,,
arxiv2024,COPR: Continual Human Preference Learning via Optimal Policy Regularization,Yes.,2,"""Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs.""",2024,2024-02-22T02:20:08Z,,,
arxiv2024,Content Conditional Debiasing for Fair Text Embedding,Yes.,2,"""we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups.""",2024,2024-02-22T01:20:51Z,,,
arxiv2024,Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models,Yes.,3,"""The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.""",2024,2024-02-22T01:20:17Z,,,
arxiv2024,Understanding the Dataset Practitioners Behind Large Language Model Development,Yes.,2,"""We find that although data quality is a top priority, there is little consensus around what data quality is and how to evaluate it.""",2024,2024-02-21T23:50:37Z,,,
arxiv2024,Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media,Yes.,2,"""Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of LLM and MMT in news translation procedures.""",2024,2024-02-21T23:43:04Z,,,
arxiv2024,Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement,Yes.,1,"""Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel.""",2024,2024-02-21T22:57:49Z,,,
arxiv2024,TOOLVERIFIER: Generalization to New Tools via Self-Verification,Yes.,3,"""language models still struggle with learning how to robustly use new tools from only a few demonstrations.""",2024,2024-02-21T22:41:38Z,,,
arxiv2024,Automatic Histograms: Leveraging Language Models for Text Dataset Exploration,Yes.,1,"""We present AutoHistograms, a visualization tool leveraging LLMs.""",2024,2024-02-21T22:29:16Z,,,
arxiv2024,BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives,Yes.,3,"""No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.""",2024,2024-02-21T22:22:30Z,,,
arxiv2024,Driving Generative Agents With Their Personality,Yes.,1,"""The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters.""",2024,2024-02-21T21:29:57Z,,,
arxiv2024,EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy,Yes.,1,"""the extracted information from EHRs using a pre-trained large language model (LLM),""",2024,2024-02-21T19:49:12Z,,,
arxiv2024,Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns,Yes.,1,"""we exploit an LLM by utilizing the knowledge obtained from the graph learning model for interpretation.""",2024,2024-02-21T19:36:24Z,,,
arxiv2024,Analysing The Impact of Sequence Composition on Language Model Pre-Training,Yes.,3,"""applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks.""",2024,2024-02-21T18:23:16Z,,,
arxiv2024,Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation,Yes.,2,"""This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands.""",2024,2024-02-21T17:20:38Z,,,
arxiv2024,Exploring ChatGPT and its Impact on Society,Yes.,2,"""However, the use of ChatGPT has also raised several concerns, including ethical, social, and employment challenges, which must be carefully considered to ensure the responsible use of this technology.""",2024,2024-02-21T16:44:35Z,,,
arxiv2024,Could We Have Had Better Multilingual LLMs If English Was Not the Central Language?,Yes.,3,"""However, the impact of factors beyond training data size on translation performance remains a topic of debate, especially concerning languages not directly encountered during training.""",2024,2024-02-21T16:32:38Z,,,
arxiv2024,Calibrating Large Language Models with Sample Consistency,Yes.,3,"""However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale.""",2024,2024-02-21T16:15:20Z,,,
arxiv2024,$Se^2$: Sequential Example Selection for In-Context Learning,Yes.,3,"""such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference.""",2024,2024-02-21T15:35:04Z,,,
arxiv2024,An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach,Yes.,1,"""Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges.""",2024,2024-02-21T15:23:21Z,,,
arxiv2024,Kuaiji: the First Chinese Accounting Large Language Model,Yes.,3,"""Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting.""",2024,2024-02-21T15:14:20Z,,,
arxiv2024,Large Language Models are Advanced Anonymizers,Yes.,2,"""existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats.""",2024,2024-02-21T14:44:00Z,,,
arxiv2024,LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation,Yes.,2,"""However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR.""",2024,2024-02-21T14:38:02Z,,,
arxiv2024,LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain,Yes.,1,"""The recent introduction of Large Language Models (LLMs) has enabled the creation of customized text output satisfying user requests.""",2024,2024-02-21T13:54:53Z,,,
arxiv2024,CriticBench: Evaluating Large Language Models as Critic,Yes.,2,"""how to comprehensively and reliably measure the critique abilities of LLMs is under-explored.""",2024,2024-02-21T12:38:59Z,,,
arxiv2024,Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph,Yes.,1,"""In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape.""",2024,2024-02-21T12:22:01Z,,,
arxiv2024,Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction,Yes.,3,"""Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs).""",2024,2024-02-21T12:12:16Z,,,
arxiv2024,From Text to CQL: Bridging Natural Language and Corpus Search Engine,Yes.,1,"""This paper presents the first text-to-CQL task that aims to automate the translation of natural language into CQL. We present a comprehensive framework for this task, including a specifically curated large-scale dataset and methodologies leveraging large language models (LLMs) for effective text-to-CQL task.""",2024,2024-02-21T12:11:28Z,,,
arxiv2024,Ouroboros: Speculative Decoding with Large Model Enhanced Drafting,Yes.,3,"""Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration.""",2024,2024-02-21T11:31:28Z,,,
arxiv2024,Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent,Yes.,3,"""Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios.""",2024,2024-02-21T11:30:20Z,,,
arxiv2024,An Evaluation of Large Language Models in Bioinformatics Research,Yes.,3,"""In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks.""",2024,2024-02-21T11:27:31Z,,,
arxiv2024,Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?,Yes.,1,"""The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions.""",2024,2024-02-21T11:07:07Z,,,
arxiv2024,KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection,Yes.,1,"""Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts.""",2024,2024-02-21T10:09:56Z,,,
arxiv2024,Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning,Yes.,3,"""fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities"" and ""the distribution gap between task datasets and the LLMs serves as the primary underlying cause.""",2024,2024-02-21T10:06:08Z,,,
arxiv2024,GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model,Yes.,3,"""Large language models(LLM) such as ChatGPT have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge.""",2024,2024-02-21T09:59:20Z,,,
arxiv2024,Privacy-Preserving Instructions for Aligning Large Language Models,Yes.,3,"""These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization.""",2024,2024-02-21T09:45:08Z,,,
arxiv2024,Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions,Yes.,3,"""Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively.""",2024,2024-02-21T09:28:02Z,,,
arxiv2024,UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language,Yes.,1,"""Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives.""",2024,2024-02-21T09:06:31Z,,,
arxiv2024,FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models,Yes.,2,"""Furthermore, we elucidate the strengths and weaknesses of FLAME through an extensive case study, error analysis and ablation studies on the benchmarks.""",2024,2024-02-21T08:50:40Z,,,
arxiv2024,KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge,Yes.,3,"""The experiment results of seven LLMs reveal that only a few models met our reference score, indicating a potential for further enhancement.""",2024,2024-02-21T08:12:26Z,,,
arxiv2024,Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving,Yes.,2,"""However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration.""",2024,2024-02-21T08:09:05Z,,,
arxiv2024,User-LLM: Efficient LLM Contextualization with User Embeddings,Yes.,2,"""effectively incorporating complex and potentially noisy user interaction data remains a challenge.""",2024,2024-02-21T08:03:27Z,,,
arxiv2024,Knowledge Graph Enhanced Large Language Model Editing,Yes.,3,"""Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge.""",2024,2024-02-21T07:52:26Z,,,
arxiv2024,APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models,Yes.,2,"""the high computational load and huge model sizes pose a grand challenge for deployment on edge devices.""",2024,2024-02-21T07:45:22Z,,,
arxiv2024,A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation,Yes.,3,"""However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features.""",2024,2024-02-21T07:38:29Z,,,
arxiv2024,WinoViz: Probing Visual Properties of Objects Under Different States,Yes.,3,"""Large language models such as GPT-4 demonstrate effective performance, but when it comes to multi-hop data, their performance is significantly degraded."" and ""Large models perform well on pragmatic reasoning, but visual knowledge reasoning is a bottleneck in our task.""",2024,2024-02-21T07:31:47Z,,,
arxiv2024,BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models,Yes.,3,"""the vanilla Chain-of-Thought (CoT) prompting method faces challenges in effectively leveraging the unique strengths of visual and DSL representations, primarily due to their differing reasoning mechanisms. Additionally, it often falls short in addressing critical steps in multi-step reasoning tasks.""",2024,2024-02-21T07:16:29Z,,,
arxiv2024,Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment,Yes.,2,"""we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment,"" and ""Ablation studies also verify the effectiveness of VKA and FKA,",2024,2024-02-21T06:34:46Z,,,
arxiv2024,ActiveRAG: Revealing the Treasures of Knowledge via Active Learning,Yes.,3,"""current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge.""",2024,2024-02-21T06:04:53Z,,,
arxiv2024,ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling,Yes.,3,"""existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs.""",2024,2024-02-21T05:41:34Z,,,
arxiv2024,Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel,Yes.,2,"""we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations of current technology.""",2024,2024-02-21T05:14:30Z,,,
arxiv2024,OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models,Yes.,2,"""Modern large language models (LLMs) should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation benchmarks tailed for LLMs mainly focus on English.""",2024,2024-02-21T04:42:41Z,,,
arxiv2024,AgentScope: A Flexible yet Robust Multi-Agent Platform,Yes.,3,"""However, the complexities in coordinating agents' cooperation and LLMs' erratic performance pose notable challenges in developing robust and efficient multi-agent applications.""",2024,2024-02-21T04:11:28Z,,,
arxiv2024,ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models,Yes.,1,"""Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish).""",2024,2024-02-21T03:58:49Z,,,
arxiv2024,The Lay Person's Guide to Biomedicine: Orchestrating Large Language Models,Yes.,2,"""Existing approaches using pre-trained language models, possibly augmented with external background knowledge, tend to struggle with effective simplification and explanation.""",2024,2024-02-21T03:21:14Z,,,
arxiv2024,GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis,Yes.,3,"""Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes.""",2024,2024-02-21T03:09:21Z,,,
arxiv2024,ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding,Yes.,3,"""However, their efficiency is hampered by the inherent limitations in autoregressive token generation."" and ""it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing.""",2024,2024-02-21T02:51:07Z,,,
arxiv2024,STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning,Yes.,2,"""As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models.""",2024,2024-02-21T01:54:58Z,,,
arxiv2024,Ranking Large Language Models without Ground Truth,Yes.,1,"""Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable.""",2024,2024-02-21T00:49:43Z,,,
arxiv2024,DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain,Yes.,2,"""Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive.""",2024,2024-02-20T23:54:02Z,,,
arxiv2024,Explaining Relationships Among Research Papers,Yes.,1,"""In this work, we explore a feature-based, LLM-prompting approach to generate richer citation texts, as well as generating multiple citations at once to capture the complex relationships among research papers.""",2024,2024-02-20T23:38:39Z,,,
arxiv2024,Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems,Yes.,2,"""Notably, we have observed that fine-tuning enhances the simulator's coherence with user goals, effectively mitigating hallucinations -- a major source of inconsistencies in simulator responses.""",2024,2024-02-20T20:57:47Z,,,
arxiv2024,ChatEL: Entity Linking with Chatbots,Yes.,2,"""Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced solution to the problems inherent in EL models, but simply naive prompts to LLMs do not work well.""",2024,2024-02-20T20:52:57Z,,,
arxiv2024,Investigating Cultural Alignment of Large Language Models,Yes.,3,"""Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values.""",2024,2024-02-20T18:47:28Z,,,
arxiv2024,RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian,Yes.,3,"""Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs.""",2024,2024-02-20T18:32:47Z,,,
arxiv2024,Soft Self-Consistency Improves Language Model Agents,Yes.,3,"""Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current 'sample and select' methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for",2024,2024-02-20T18:22:38Z,,,
arxiv2024,Defending Jailbreak Prompts via In-Context Adversarial Game,Yes.,3,"""However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist.""",2024,2024-02-20T17:04:06Z,,,
arxiv2024,Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity,Yes.,3,"""While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings.""",2024,2024-02-20T16:43:20Z,,,
arxiv2024,Event-level Knowledge Editing,Yes.,3,"""We find that ELKEN poses significant challenges to existing knowledge editing approaches.""",2024,2024-02-20T15:36:41Z,,,
arxiv2024,Slot-VLM: SlowFast Slots for Video-Language Modeling,Yes.,1,"""A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs.""",2024,2024-02-20T15:30:09Z,,,
arxiv2024,Identifying Semantic Induction Heads to Understand In-Context Learning,Yes.,3,"""the lack of transparency in their inference logic raises concerns about their trustworthiness.""",2024,2024-02-20T14:43:39Z,,,
arxiv2024,Stable Knowledge Editing in Large Language Models,Yes.,3,"""previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge.""",2024,2024-02-20T14:36:23Z,,,
arxiv2024,Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries,Yes.,1,"""Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning.""",2024,2024-02-20T14:31:17Z,,,
arxiv2024,Text-Guided Molecule Generation with Diffusion Language Model,Yes.,1,"""a novel approach that leverages diffusion models to address the limitations of autoregressive methods.""",2024,2024-02-20T14:29:02Z,,,
arxiv2024,Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models,Yes.,3,"""recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction.""",2024,2024-02-20T14:23:23Z,,,
arxiv2024,SoMeLVLM: A Large Vision Language Model for Social Media Processing,Yes.,2,"""the general domain models often fall short in aligning with the unique speaking style and context of social media tasks.""",2024,2024-02-20T14:02:45Z,,,
arxiv2024,Code Needs Comments: Enhancing Code LLMs with Comment Augmentation,Yes.,1,"""We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment.""",2024,2024-02-20T13:56:38Z,,,
arxiv2024,An Autonomous Large Language Model Agent for Chemical Literature Data Mining,Yes.,1,"""This AI agent employs large language models (LLMs) for prompt generation and iterative optimization.""",2024,2024-02-20T13:21:46Z,,,
arxiv2024,TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification,Yes.,1,"""Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them.""",2024,2024-02-20T13:20:39Z,,,
arxiv2024,Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning,Yes.,3,"""Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior."" and ""we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process.""",2024,2024-02-20T12:58:14Z,,,
arxiv2024,GlrIA -- A Generative and Open Large Language Model for Portuguese,Yes.,1,"""Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs).""",2024,2024-02-20T12:36:40Z,,,
arxiv2024,GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick,Yes.,3,"""Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty."" and ""GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience.""",2024,2024-02-20T12:05:47Z,,,
arxiv2024,A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence,Yes.,2,"""The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in multiple aspects.""",2024,2024-02-20T11:28:50Z,,,
arxiv2024,OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data,Yes.,2,"""This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data.""",2024,2024-02-20T11:01:39Z,,,
arxiv2024,Chain of Thought Empowers Transformers to Solve Inherently Serial Problems,Yes.,3,"""CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low.""",2024,2024-02-20T10:11:03Z,,,
arxiv2024,Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data,Yes.,1,"""Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention.""",2024,2024-02-20T10:00:58Z,,,
arxiv2024,MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models,Yes.,3,"""the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios.""",2024,2024-02-20T09:30:48Z,,,
arxiv2024,Instruction-tuned Language Models are Better Knowledge Learners,Yes.,3,"""we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized.""",2024,2024-02-20T09:20:32Z,,,
arxiv2024,PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning,Yes.,2,"""Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression.""",2024,2024-02-20T09:10:08Z,,,
arxiv2024,PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs,Yes.,3,"""While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models.""",2024,2024-02-20T09:02:55Z,,,
arxiv2024,"Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?",Yes.,2,"""When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many",2024,2024-02-20T08:38:24Z,,,
arxiv2024,Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models,Yes.,3,"""previous research found that LLMs sometimes struggle with adhering to specific constraints (e.g., in specific place or at specific time), at times even overlooking them, which leads to responses that are either too generic or not fully satisfactory.""",2024,2024-02-20T08:03:05Z,,,
arxiv2024,Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations,Yes.,3,"""When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn.""",2024,2024-02-20T07:51:43Z,,,
arxiv2024,Me LLaMA: Foundation Large Language Models for Medical Applications,Yes.,3,"""However, their performance on medical tasks is suboptimal and can be improved by training on extensive domain-specific datasets.""",2024,2024-02-20T06:37:31Z,,,
arxiv2024,MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion,Yes.,2,"""Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings.""",2024,2024-02-20T06:14:30Z,,,
arxiv2024,Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues,Yes.,1,"""there is a need for more evaluations of the performance of counseling dialogue systems that use large language models.""",2024,2024-02-20T06:05:36Z,,,
arxiv2024,Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering,Yes.,3,"""While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations.""",2024,2024-02-20T05:32:24Z,,,
arxiv2024,SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning,Yes.,3,"""Modern LLMs have become increasingly powerful, but they are still facing challenges in specialized tasks such as Text-to-SQL.""",2024,2024-02-20T03:57:55Z,,,
arxiv2024,FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning,Yes.,1,"""We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database.""",2024,2024-02-20T03:39:49Z,,,
arxiv2024,CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management,Yes.,2,"""We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative questions).""",2024,2024-02-20T01:59:11Z,,,
arxiv2024,Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM,Yes.,1,"""In this work, we propose a new framework called MINGLE, which integrates both structures and semantics in EHR effectively.""",2024,2024-02-19T23:48:40Z,,,
arxiv2024,Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation,Yes.,1,"""current works in controllable text generation have yet to explore using these standards as references for control.""",2024,2024-02-19T23:18:18Z,,,
arxiv2024,Detecting misinformation through Framing Theory: the Frame Element-based Model,Yes.,1,"""To tackle this challenge, we propose an innovative approach leveraging the power of pre-trained Large Language Models and deep neural networks to detect misinformation originating from accurate facts portrayed under different frames.""",2024,2024-02-19T21:50:42Z,,,
arxiv2024,Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models,Yes.,3,"""Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction.""",2024,2024-02-19T21:38:02Z,,,
arxiv2024,Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection,Yes.,1,"""Data selection in instruction tuning emerges as a pivotal process for acquiring high-quality data and training instruction-following large language models (LLMs), but it is still a new and unexplored research area for vision-language models (VLMs).""",2024,2024-02-19T20:08:48Z,,,
arxiv2024,Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?,Yes.,3,"""To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices."" and ""We hope to motivate the use of stronger baselines in MCQA benchmarks, the design of robust MCQA datasets, and further efforts to explain LLM decision-making.""",2024,2024-02-19T19:38:58Z,,,
arxiv2024,"Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding",Yes.,1,"""As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important.""",2024,2024-02-19T18:58:32Z,,,
arxiv2024,A Critical Evaluation of AI Feedback for Aligning Large Language Models,Yes.,2,"""we question whether the complexity of this RL step is truly warranted for AI feedback"" and ""we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models.""",2024,2024-02-19T18:53:54Z,,,
arxiv2024,DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models,Yes.,3,"""We show that the task is difficult as it requires the model to learn long-range code relationships, a task that inherently relies on extensive amounts of training data. At the same time, creating a large, clean dataset for complex program bugs and their corresponding fixes is non-trivial.""",2024,2024-02-19T18:35:40Z,,,
arxiv2024,Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models,Yes.,3,"""Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem.""",2024,2024-02-19T18:09:48Z,,,
arxiv2024,Query-Based Adversarial Prompt Generation,Yes.,3,"""Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior.""",2024,2024-02-19T18:01:36Z,,,
arxiv2024,LLM Agents for Psychology: A Study on Gamified Assessments,Yes.,3,"""While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability.""",2024,2024-02-19T18:00:30Z,,,
arxiv2024,ARKS: Active Retrieval in Knowledge Soup for Code Generation,Yes.,1,"""Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training.""",2024,2024-02-19T17:37:28Z,,,
arxiv2024,Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports,Yes.,1,"""In this paper, we show that while GPT-4 is superior to open-source models in zero-shot report labeling, the implementation of few-shot prompting can bring open-source models on par with GPT-4.""",2024,2024-02-19T17:23:10Z,,,
arxiv2024,Adaptive Skeleton Graph Decoding,Yes.,3,"""LLM inference incurs significant computation and memory costs"" and ""they often suffer from reduced response quality.""",2024,2024-02-19T16:47:04Z,,,
arxiv2024,"WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",Yes.,1,"""We do this by extending work on program synthesis via LLMs.""",2024,2024-02-19T16:39:18Z,,,
arxiv2024,Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data,Yes.,3,"""We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance.""",2024,2024-02-19T16:34:50Z,,,
arxiv2024,Uncertainty quantification in fine-tuned LLMs using LoRA ensembles,Yes.,3,"""a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing.""",2024,2024-02-19T16:26:00Z,,,
arxiv2024,Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships,Yes.,1,"""predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context.""",2024,2024-02-19T16:15:03Z,,,
arxiv2024,CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation,Yes.,1,"""Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem.""",2024,2024-02-19T15:30:40Z,,,
arxiv2024,Reformatted Alignment,Yes.,3,"""Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations.""",2024,2024-02-19T15:21:58Z,,,
arxiv2024,Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages,Yes.,3,"""their performance still lags behind in most languages compared to a few resource-rich languages"" and ""using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer.""",2024,2024-02-19T15:07:32Z,,,
arxiv2024,Stick to your Role! Stability of Personal Values Expressed in Large Language Models,Yes.,3,"""We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size."" and ""When instructed to simulate particular personas, LLMs exhibit low Rank-Order stability, and this stability further diminishes with conversation length.""",2024,2024-02-19T14:53:01Z,,,
arxiv2024,Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning,Yes.,2,"""Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs).""",2024,2024-02-19T14:33:24Z,,,
arxiv2024,Transformer-based Causal Language Models Perform Clustering,Yes.,3,"""the capability of an LLM to follow human instructions is still a concern"" and ""the mechanisms responsible for effective instruction-following capabilities remain inadequately understood.""",2024,2024-02-19T14:02:31Z,,,
arxiv2024,End-to-end multilingual fact-checking at scale,Yes.,1,"""We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.""",2024,2024-02-19T14:00:35Z,,,
arxiv2024,Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement,Yes.,3,"""Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination.""",2024,2024-02-19T13:57:55Z,,,
arxiv2024,LVCHAT: Facilitating Long Video Comprehension,Yes.,3,"""Existing works show promise on short videos whereas long video (longer than e.g.~1 minute) comprehension remains challenging. The major problem lies in the over-compression of videos, i.e., the encoded video representations are not enough to represent the whole video.""",2024,2024-02-19T11:59:14Z,,,
arxiv2024,WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More,Yes.,3,"""Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process.""",2024,2024-02-19T11:33:21Z,,,
arxiv2024,All Language Models Large and Small,Yes.,1,"""Many leading language models (LMs) use high-intensity computational resources both during training and execution.""",2024,2024-02-19T11:28:20Z,,,
arxiv2024,Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations,Yes.,2,"""rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales.""",2024,2024-02-19T10:47:09Z,,,
arxiv2024,Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space,Yes.,3,"""the reliability of LMs is susceptible to backdoor attacks"" and ""Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios.""",2024,2024-02-19T10:34:48Z,,,
arxiv2024,Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?,Yes.,3,"""we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.""",2024,2024-02-19T10:34:13Z,,,
arxiv2024,Distilling Large Language Models for Text-Attributed Graph Learning,Yes.,3,"""Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues.""",2024,2024-02-19T10:31:53Z,,,
arxiv2024,EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs,Yes.,3,"""Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs. Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions.""",2024,2024-02-19T09:55:32Z,,,
arxiv2024,Structure Guided Large Language Model for SQL Generation,Yes.,3,"""Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation.""",2024,2024-02-19T09:07:59Z,,,
arxiv2024,DB-LLM: Accurate Dual-Binarization for Efficient LLMs,Yes.,3,"""the expensive memory and computation consumption impede their practical deployment"" and ""existing ultra-low-bit quantization always causes severe accuracy drops.""",2024,2024-02-19T09:04:30Z,,,
arxiv2024,Automatic Evaluation for Mental Health Counseling using LLMs,Yes.,1,"""This paper proposes an innovative and efficient automatic approach using large language models (LLMs) to evaluate the working alliance in counseling conversations.""",2024,2024-02-19T09:00:10Z,,,
arxiv2024,LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation,Yes.,3,"""even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence.""",2024,2024-02-19T08:32:27Z,,,
arxiv2024,Comprehensive Cognitive LLM Agent for Smartphone GUI Automation,Yes.,3,"""However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response.""",2024,2024-02-19T08:29:03Z,,,
arxiv2024,Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation,Yes.,1,"""Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem.""",2024,2024-02-19T07:46:40Z,,,
arxiv2024,Learning to Edit: Aligning LLMs with Knowledge Editing,Yes.,3,"""existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions.""",2024,2024-02-19T07:45:17Z,,,
arxiv2024,SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning,Yes.,3,"""Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning.""",2024,2024-02-19T07:38:57Z,,,
arxiv2024,SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning,Yes.,3,"""Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time."" and ""the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks.""",2024,2024-02-19T07:22:29Z,,,
arxiv2024,Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation,Yes.,3,"""Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges."" and ""the data leakage issue cause over-estimation on existing benchmarks.""",2024,2024-02-19T07:15:59Z,,,
arxiv2024,ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding,Yes.,2,"""the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient.""",2024,2024-02-19T06:58:42Z,,,
arxiv2024,NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization,Yes.,2,"""large language models' application programming interfaces (LLMs' APIs) are widely available, but importing and exporting medical data presents significant challenges due to privacy protection policies in healthcare institutions.""",2024,2024-02-19T06:43:25Z,,,
arxiv2024,LoRA Training in the NTK Regime has No Spurious Local Minima,Yes.,1,"""Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited.""",2024,2024-02-19T06:22:09Z,,,
arxiv2024,Modularized Networks for Few-shot Hateful Meme Detection,Yes.,1,"""We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules.""",2024,2024-02-19T05:15:13Z,,,
arxiv2024,UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction,Yes.,1,"""Drawing inspiration from large language models, UniST achieves success through",2024,2024-02-19T05:04:11Z,,,
arxiv2024,Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search,Yes.,3,"""most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results.""",2024,2024-02-19T04:41:31Z,,,
arxiv2024,HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?,Yes.,3,"""Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials."" and ""Nonetheless, the majority of these systems rely on the text-generating model. This limitation is impractical in real-world scenarios, as it's often impossible to know which specific",2024,2024-02-19T04:11:34Z,,,
arxiv2024,FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema,Yes.,1,"""In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user.""",2024,2024-02-19T03:56:44Z,,,
arxiv2024,LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs,Yes.,1,"""In this paper, we attempt to address this challenge with Large Language Models (LLMs).""",2024,2024-02-19T03:21:19Z,,,
arxiv2024,RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts,Yes.,2,"""However, an important issue is how we can detect AI-generated texts from human-written ones.""",2024,2024-02-19T00:40:17Z,,,
arxiv2024,In-Context Learning Demonstration Selection via Influence Analysis,Yes.,3,"""ICL generalization performance is sensitive to the selected demonstrations. Selecting effective demonstrations for ICL is still an open research challenge.""",2024,2024-02-19T00:39:31Z,,,
arxiv2024,"Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges",Yes.,2,"""Despite the popularity of LLMs, we find that for specific tasks, finely tuned BERT encoders still outperform, and at a lower deployment cost.""",2024,2024-02-18T23:22:40Z,,,
arxiv2024,Solving Data-centric Tasks using Large Language Models,Yes.,3,"""Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.""",2024,2024-02-18T23:19:21Z,,,
arxiv2024,Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models,Yes.,1,"""Advances in language modeling have paved the way for novel human-AI co-writing experiences.""",2024,2024-02-18T22:27:42Z,,,
arxiv2024,Modelling Political Coalition Negotiations Using LLM-based Agents,Yes.,1,"""We evaluate the performance of state-of-the-art large language models (LLMs) as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling.""",2024,2024-02-18T21:28:06Z,,,
arxiv2024,MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization,Yes.,1,"""RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions.""",2024,2024-02-18T21:25:09Z,,,
arxiv2024,GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network,Yes.,2,"""fine-tuning still remains crucial to further enhance their adaptability"" and ""high demands on computing resources limit its practicality.""",2024,2024-02-18T21:13:05Z,,,
arxiv2024,One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation,Yes.,1,"""Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation.""",2024,2024-02-18T19:13:52Z,,,
arxiv2024,Autocorrect for Estonian texts: final report from project EKTB25,Yes.,2,"""The final results show that the approach we have developed provides better scores than GPT4 and the result is usable but not entirely reliable yet.""",2024,2024-02-18T18:20:57Z,,,
arxiv2024,Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models,Yes.,1,"""This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the 'where' and 'how' perspectives.""",2024,2024-02-18T16:43:21Z,,,
arxiv2024,Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks,Yes.,3,"""Our attacks assume limited access to the generator LLMs,"" and ""Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes.""",2024,2024-02-18T16:36:00Z,,,
arxiv2024,Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs,Yes.,1,"""While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs.""",2024,2024-02-18T16:20:43Z,,,
arxiv2024,On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs,Yes.,1,"""Intrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.""",2024,2024-02-18T15:53:32Z,,,
arxiv2024,SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models,Yes.,1,"""In this paper we introduce SpeCrawler, a comprehensive system that utilizes large language models (LLMs) to generate OpenAPI Specifications from diverse API documentation through a carefully crafted pipeline.""",2024,2024-02-18T15:33:24Z,,,
arxiv2024,Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection,Yes.,3,"""Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distinguishing between reporting genuine emotional expression and intentionally use framing bias in news headlines.""",2024,2024-02-18T15:27:48Z,,,
arxiv2024,Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?,Yes.,1,"""Large language models (LLMs) are typically prompted to follow a single instruction per inference call.""",2024,2024-02-18T14:25:19Z,,,
arxiv2024,BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation,Yes.,3,"""While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive.""",2024,2024-02-18T12:44:15Z,,,
arxiv2024,ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation,Yes.,2,"""However, they still struggle to accommodate the diverse and specific needs of users and simplify the utilization of AI models for the average user.""",2024,2024-02-18T11:24:34Z,,,
arxiv2024,Deciphering the Impact of Pretraining Data on Large Language Models through Machine Unlearning,Yes.,3,"""the impact of each component of the pretraining corpus remains opaque"" and ""the organization of the pretraining corpus is still empirical and may deviate from the optimal.""",2024,2024-02-18T10:36:05Z,,,
arxiv2024,Ploutos: Towards interpretable stock movement prediction with financial large language model,Yes.,3,"""First, they struggle to fuse textual and numerical information flexibly for stock movement prediction. Second, traditional methods lack clarity and interpretability, which impedes their application in scenarios where the justification for predictions is essential.""",2024,2024-02-18T10:28:18Z,,,
arxiv2024,Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models,Yes.,3,"""However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks (Wang et al., 2023a).""",2024,2024-02-18T10:10:40Z,,,
arxiv2024,Efficient Multimodal Learning from Data-centric Perspective,Yes.,3,"""their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities.""",2024,2024-02-18T10:09:10Z,,,
arxiv2024,Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network,Yes.,1,"""We propose a novel REasoning meta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into the evolutionary procedure.""",2024,2024-02-18T09:21:12Z,,,
arxiv2024,Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM,Yes.,3,"""there is some necessary knowledge that is not explicitly included in the database schema or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models.""",2024,2024-02-18T09:10:04Z,,,
arxiv2024,Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources,Yes.,3,"""it raises significant challenges due to the heterogeneous resources and data distributions of clients.""",2024,2024-02-18T08:32:59Z,,,
arxiv2024,ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework,Yes.,2,"""While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization.""",2024,2024-02-18T06:07:17Z,,,
arxiv2024,scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation,Yes.,3,"""Despite the inherent limitations of existing Large Language Models in directly reading and interpreting single-cell omics data, they demonstrate significant potential and flexibility as the Foundation Model.""",2024,2024-02-18T05:39:00Z,,,
arxiv2024,LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks,Yes.,1,"""LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain.""",2024,2024-02-18T04:41:25Z,,,
arxiv2024,MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization,Yes.,1,"""Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored.""",2024,2024-02-18T04:28:28Z,,,
arxiv2024,AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition,Yes.,3,"""Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment.""",2024,2024-02-18T04:28:16Z,,,
arxiv2024,SciAgent: Tool-augmented Language Models for Scientific Reasoning,Yes.,3,"""Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs).""",2024,2024-02-18T04:19:44Z,,,
arxiv2024,In-Context Example Ordering Guided by Label Distributions,Yes.,3,"""However, a number of problems persist in ICL. In particular, its performance is sensitive to the choice and order of in-context examples.""",2024,2024-02-18T04:08:10Z,,,
arxiv2024,InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration,Yes.,3,"""Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks."" and ""Injecting new knowledge poses the risk of forgetting previously acquired knowledge.""",2024,2024-02-18T03:36:26Z,,,
arxiv2024,"Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning",Yes.,1,"""This dataset can also be used to evaluate the complex reasoning capability of current large language models and serve as a reasoning benchmark for further research.""",2024,2024-02-18T02:52:54Z,,,
arxiv2024,Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction,Yes.,3,"""Previous studies have shown that LLMs' performance as correctors on CGEC remains unsatisfactory due to its challenging task focus.""",2024,2024-02-18T01:40:34Z,,,
arxiv2024,LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models,Yes.,3,"""However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs).""",2024,2024-02-18T01:20:00Z,,,
arxiv2024,Multi-dimensional Evaluation of Empathetic Dialog Responses,Yes.,3,"""measuring conversational empathy remains a challenging task for prompting frozen LLMs, reflected by less satisfying performance of GPT-4 and Flan family models.""",2024,2024-02-18T00:32:33Z,,,
arxiv2024,Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis,Yes.,1,"""In this study, we leverage LLM to enhance the semantic analysis and develop similarity metrics for texts, addressing the limitations of traditional unsupervised NLP metrics like ROUGE and BLEU.""",2024,2024-02-17T22:46:44Z,,,
arxiv2024,CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness,Yes.,3,"""LLMs still require substantial resources,"" and ""Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem.""",2024,2024-02-17T22:37:17Z,,,
arxiv2024,Training Language Model Agents without Modifying Language Models,Yes.,1,"""we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications.""",2024,2024-02-17T18:31:21Z,,,
arxiv2024,Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention,Yes.,3,"""However, we also observed challenges in promoting self-disclosure through LTM, particularly around addressing chronic health conditions and privacy concerns.""",2024,2024-02-17T18:05:53Z,,,
arxiv2024,PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models,Yes.,3,"""Crafting an ideal prompt for Large Language Models (LLMs) is a challenging task that demands significant resources and expert human input."" and ""formulating such optimization in the discrete and high-dimensional natural language space introduces challenges in terms of convergence and computational efficiency.""",2024,2024-02-17T17:47:10Z,,,
arxiv2024,EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries,Yes.,3,"""current KE approaches, which typically operate on (subject, relation, object) triples, ignore the contextual information and the relation among different knowledge. Such editing methods could thus encounter an uncertain editing boundary, leaving a lot of relevant knowledge in ambiguity.""",2024,2024-02-17T16:34:50Z,,,
arxiv2024,Dissecting Human and LLM Preferences,Yes.,3,"""Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks.""",2024,2024-02-17T14:34:31Z,,,
arxiv2024,OneBit: Towards Extremely Low-bit Large Language Models,Yes.,3,"""existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced.""",2024,2024-02-17T14:26:57Z,,,
arxiv2024,Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models,Yes.,3,"""However, existing methods often fail to overcome the issue of overconfidence on incorrect answers.""",2024,2024-02-17T13:37:39Z,,,
arxiv2024,MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning,Yes.,1,"""Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge.""",2024,2024-02-17T12:25:31Z,,,
arxiv2024,C-ICL: Contrastive In-context Learning for Information Extraction,Yes.,2,"""Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process.""",2024,2024-02-17T11:28:08Z,,,
arxiv2024,Aligning Large Language Models by On-Policy Self-Judgment,Yes.,1,"""Existing approaches for aligning large language models with human preferences face a trade-off that requires a separate reward model (RM) for on-policy learning.""",2024,2024-02-17T11:25:26Z,,,
arxiv2024,LLM can Achieve Self-Regulation via Hyperparameter Aware Generation,Yes.,3,"""The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample.""",2024,2024-02-17T11:18:22Z,,,
arxiv2024,Can Large Language Models perform Relation-based Argument Mining?,Yes.,1,"""In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline.""",2024,2024-02-17T10:37:51Z,,,
arxiv2024,When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection,Yes.,3,"""However, their primary limitation arises from their exclusive dependence on textual input, which constrains their overall capabilities.""",2024,2024-02-17T09:39:46Z,,,
arxiv2024,Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs,Yes.,1,"""This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs.""",2024,2024-02-17T08:14:37Z,,,
arxiv2024,Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models,Yes.,3,"""Moreover, these benchmarks are susceptible to data leakage, since Med-MLLMs are trained on large assemblies of publicly available data.""",2024,2024-02-17T08:04:23Z,,,
arxiv2024,I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments,Yes.,3,"""The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills.""",2024,2024-02-17T05:05:31Z,,,
arxiv2024,LaCo: Large Language Model Pruning via Layer Collapse,Yes.,3,"""Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model.""",2024,2024-02-17T04:16:30Z,,,
arxiv2024,M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection,Yes.,3,"""Human evaluation for Task 2 shows less than random guess performance, demonstrating the challenges to distinguish unique LLMs.""",2024,2024-02-17T02:50:33Z,,,
arxiv2024,KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph,Yes.,1,"""In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions.""",2024,2024-02-17T02:07:49Z,,,
arxiv2024,PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation,Yes.,3,"""current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM)."" and ""There are two challenges",2024,2024-02-17T01:56:19Z,,,
arxiv2024,Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction,Yes.,3,"""existing models typically rely on extensive annotated data for training, which can be both costly and time-consuming to acquire"" and ""these models often struggle to adapt to new or unseen relationships.""",2024,2024-02-17T00:20:06Z,,,
arxiv2024,Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models,Yes.,3,"""Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving."" and ""uses error analysis obtained from the LLM on them to explicitly revise prompting.""",2024,2024-02-17T00:13:36Z,,,
arxiv2024,TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks,No.,1,The abstract does not mention LLMs or any other language models. It focuses on prior-data fitted networks (PFNs) and their limitations.,2024,2024-02-17T00:02:23Z,,,
arxiv2024,Orca-Math: Unlocking the potential of SLMs in Grade School Math,No.,1,The paper focuses on small language models (SLMs) and does not mention large language models (LLMs) or their limitations.,2024,2024-02-16T23:44:38Z,,,
arxiv2024,BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering,Yes.,3,"""However, these methods often face challenges with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness.""",2024,2024-02-16T23:28:02Z,,,
arxiv2024,Word Embeddings Revisited: Do LLMs Offer Something New?,Yes.,2,"""it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE).""",2024,2024-02-16T21:47:30Z,,,
arxiv2024,AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators,Yes.,2,"""To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs).""",2024,2024-02-16T20:59:57Z,,,
arxiv2024,Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions,Yes.,3,"""Our analysis reveals the strengths and potential of LLMs in both enhancing traditional CD methods and as an imperfect expert, alongside the challenges and limitations inherent in current practices.""",2024,2024-02-16T20:48:53Z,,,
arxiv2024,Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement,Yes.,2,"""Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization.""",2024,2024-02-16T20:20:43Z,,,
arxiv2024,PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter,Yes.,3,"""we observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited scalability with a lack of direct supervision.""",2024,2024-02-16T18:54:47Z,,,
arxiv2024,Proving membership in LLM pretraining data via data watermarks,Yes.,1,"""Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem.""",2024,2024-02-16T18:49:27Z,,,
arxiv2024,Instruction Diversity Drives Generalization To Unseen Tasks,Yes.,2,"""Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such unseen tasks are not well understood.""",2024,2024-02-16T18:47:21Z,,,
arxiv2024,EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models,Yes.,3,"""A limitation of these ranking strategies with LLMs is their cost",2024,2024-02-16T18:03:42Z,,,
arxiv2024,Quantifying the Persona Effect in LLM Simulations,Yes.,3,"""Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in the current NLP landscape.""",2024,2024-02-16T16:35:35Z,,,
arxiv2024,EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge,Yes.,3,"""the wide applications of LLMs on edge devices are limited due to their massive parameters and computations"" and ""Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache",2024,2024-02-16T16:10:38Z,,,
arxiv2024,A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models,Yes.,3,"""Even though Large Language Models (LLMs) offer a promising solution to predict unobserved relations between the head and tail entity in a zero-shot manner, their performance is still restricted due to the inability to leverage all the (exponentially many) paths' information between two entities, which",2024,2024-02-16T16:02:33Z,,,
arxiv2024,AutoGPT+P: Affordance-based Task Planning with Large Language Models,Yes.,3,"""Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities.""",2024,2024-02-16T16:00:50Z,,,
arxiv2024,How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?,Yes.,3,"""we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type"" and ""their reliability is highly context-dependent.""",2024,2024-02-16T15:48:33Z,,,
arxiv2024,Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning,Yes.,3,"""LLMs exhibit a weaker capacity compared to humans in discerning the difficulty levels of demonstrations.""",2024,2024-02-16T14:55:33Z,,,
arxiv2024,A Novel BERT-based Classifier to Detect Political Leaning of YouTube Videos based on their Titles,Yes.,1,"""we propose a novel classifier based on Bert -- a language model from Google -- to classify YouTube videos merely based on their titles into six categories""",2024,2024-02-16T14:44:30Z,,,
arxiv2024,AutoSAT: Automatically Optimize SAT Solvers via Large Language Models,Yes.,1,"""AutoSAT is based on Large Large Models (LLMs) which is able to autonomously generate code, conduct evaluation, then utilize the feedback to further optimize heuristics, thereby reducing human intervention and enhancing solver capabilities.""",2024,2024-02-16T14:04:56Z,,,
arxiv2024,Multi-Cultural Commonsense Knowledge Distillation,Yes.,3,"""Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions.""",2024,2024-02-16T13:46:38Z,,,
arxiv2024,German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data,Yes.,2,"""This paper employs various methodologies for evaluation and demonstrates the limitations of currently used rule-based metrics.""",2024,2024-02-16T13:28:44Z,,,
arxiv2024,OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models,Yes.,1,"""We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand.""",2024,2024-02-16T13:21:33Z,,,
arxiv2024,Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL,Yes.,2,"""human labeling suffers from the limitations of insufficient diversity and high labeling overhead.""",2024,2024-02-16T13:13:18Z,,,
arxiv2024,Network Formation and Dynamics Among Multi-LLMs,Yes.,1,"""Our study analyzes LLMs' network formation behavior to examine whether the dynamics of multiple LLMs are similar to or different from human social dynamics.""",2024,2024-02-16T13:10:14Z,,,
arxiv2024,AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation,Yes.,3,"""Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored.""",2024,2024-02-16T12:47:11Z,,,
arxiv2024,Can Separators Improve Chain-of-Thought Prompting?,Yes.,3,"""the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs.""",2024,2024-02-16T12:46:16Z,,,
arxiv2024,BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation,Yes.,2,"""The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges.""",2024,2024-02-16T12:27:15Z,,,
arxiv2024,Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks,Yes.,3,"""The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability, followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods.""",2024,2024-02-16T11:30:11Z,,,
arxiv2024,Do Llamas Work in English? On the Latent Language of Multilingual Transformers,Yes.,3,"""our evidence suggests that the abstract 'concept space' lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models.""",2024,2024-02-16T11:21:28Z,,,
arxiv2024,Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs,Yes.,1,"""With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred.""",2024,2024-02-16T11:20:30Z,,,
arxiv2024,LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty,Yes.,3,"""Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks. Furthermore, non-public and large-scale weights make tuning LLMs difficult.""",2024,2024-02-16T11:02:29Z,,,
arxiv2024,LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine,Yes.,3,"""We experimented with four different LLMs, two medical rule engine implementations, and 58 real medical rules to investigate the hallucination, success, time efficiency, and robustness of the LLMs to generate tests.""",2024,2024-02-16T10:56:15Z,,,
arxiv2024,SPAR: Personalized Content-Based Recommendation via Long Engagement Attention,Yes.,2,"""existing works still struggle with processing very long user historical text and insufficient user-item interaction.""",2024,2024-02-16T10:36:38Z,,,
arxiv2024,Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts,Yes.,2,"""our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary summarization models, including state-of-the-art language models such as GPT-3.5.""",2024,2024-02-16T10:35:18Z,,,
arxiv2024,Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models,Yes.,3,"""Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency.""",2024,2024-02-16T10:32:16Z,,,
arxiv2024,Can We Verify Step by Step for Incorrect Answer Detection?,Yes.,3,"""This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps.""",2024,2024-02-16T09:29:50Z,,,
arxiv2024,LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models,Yes.,2,"""analyzing the results from this evaluation approach raises scalability and interpretability challenges.""",2024,2024-02-16T09:14:49Z,,,
arxiv2024,"Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",Yes.,2,"""considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes.""",2024,2024-02-16T09:06:06Z,,,
arxiv2024,Provably Sample Efficient RLHF via Active Preference Optimization,Yes.,3,"""the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF.""",2024,2024-02-16T08:19:34Z,,,
arxiv2024,Comparing Hallucination Detection Metrics for Multilingual Generation,Yes.,3,"""Our findings highlight existing gaps in multilingual hallucination detection and motivate future research to develop more robust detection methods for LLM hallucination in other languages.""",2024,2024-02-16T08:10:34Z,,,
arxiv2024,Large Language Models as Zero-shot Dialogue State Tracker through Function Calling,Yes.,3,"""However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying.""",2024,2024-02-16T06:13:18Z,,,
arxiv2024,QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning,Yes.,3,"""Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models."" and ""finding the efficient LoRA rank is still challenging.""",2024,2024-02-16T05:42:17Z,,,
arxiv2024,Steering Conversational Large Language Models for Long Emotional Support Conversations,Yes.,3,"""In this study, we address the challenge of consistently following emotional support strategies in long conversations by large language models (LLMs).""",2024,2024-02-16T05:03:01Z,,,
arxiv2024,Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models,Yes.,1,"""Instruction-tuning language models has become a crucial step in aligning them for general use.""",2024,2024-02-16T03:39:37Z,,,
arxiv2024,Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning,Yes.,1,"""leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs.""",2024,2024-02-16T02:21:59Z,,,
arxiv2024,Chain of Logic: Rule-Based Reasoning with Large Language Models,Yes.,2,"""Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements.""",2024,2024-02-16T01:54:43Z,,,
arxiv2024,BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains,Yes.,2,"""Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges.""",2024,2024-02-15T23:39:04Z,,,
arxiv2024,Can we Soft Prompt LLMs for Graph Learning Tasks?,Yes.,3,"""However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities.""",2024,2024-02-15T23:09:42Z,,,
arxiv2024,Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models,Yes.,3,"""Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning.""",2024,2024-02-15T22:54:24Z,,,
arxiv2024,How to Discern Important Urgent News?,Yes.,1,"""The found correlation should allow using clustering (as an alternative to LLM) for identifying the most important urgent news, or for filtering out unimportant articles.""",2024,2024-02-15T20:08:07Z,,,
arxiv2024,LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing,Yes.,1,"""we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers.""",2024,2024-02-15T19:53:11Z,,,
arxiv2024,Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation,No.,1,"The abstract focuses on diffusion models and their fine-tuning techniques, without mentioning LLMs or their limitations.",2024,2024-02-15T18:59:18Z,,,
arxiv2024,A StrongREJECT for Empty Jailbreaks,Yes.,3,"""there is no standard benchmark for measuring the severity of a jailbreak,"" and ""Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions.""",2024,2024-02-15T18:58:09Z,,,
arxiv2024,Chain-of-Thought Reasoning Without Prompting,Yes.,3,"""These methods, while effective, often involve manually intensive prompt engineering.""",2024,2024-02-15T18:55:41Z,,,
arxiv2024,BitDelta: Your Fine-Tune May Only Be Worth One Bit,Yes.,3,"""This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models.""",2024,2024-02-15T18:50:06Z,,,
arxiv2024,OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset,Yes.,2,"""A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs.""",2024,2024-02-15T18:26:11Z,,,
arxiv2024,Generative AI and Process Systems Engineering: The Next Frontier,Yes.,2,"""Furthermore, the article identifies and discusses potential challenges in fully leveraging GenAI within PSE, including multiscale modeling, data requirements, evaluation metrics and benchmarks, and trust and safety.""",2024,2024-02-15T18:20:42Z,,,
arxiv2024,OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models,Yes.,1,"""This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions.""",2024,2024-02-15T18:19:18Z,,,
arxiv2024,Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients,Yes.,3,"""current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses.""",2024,2024-02-15T18:00:02Z,,,
arxiv2024,TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles,Yes.,2,"""the creation of high-quality annotated data for Task-Oriented Dialog (TOD) is recognized to be slow and costly.""",2024,2024-02-15T17:40:02Z,,,
arxiv2024,Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning,Yes.,2,"""its success heavily relies on the training data quality"" and ""many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned.""",2024,2024-02-15T17:06:21Z,,,
arxiv2024,Towards Reducing Diagnostic Errors with Interpretable Risk Prediction,Yes.,1,"""We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model.""",2024,2024-02-15T17:05:48Z,,,
arxiv2024,Quantized Embedding Vectors for Controllable Diffusion Language Models,Yes.,3,"""the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models.""",2024,2024-02-15T17:02:48Z,,,
arxiv2024,GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving,Yes.,3,"""Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67% accuracy rate on the main subset but only a 6.00% accuracy on the challenging subset. This highlights the",2024,2024-02-15T16:59:41Z,,,
arxiv2024,Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4,Yes.,3,"""Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical inaccuracies in the LLM-generated responses, which were appropriately identified by the GPT-4 evaluation.""",2024,2024-02-15T16:43:41Z,,,
arxiv2024,Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence,Yes.,3,"""Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI).""",2024,2024-02-15T16:36:04Z,,,
arxiv2024,Towards Safer Large Language Models through Machine Unlearning,Yes.,3,"""LLMs often encounter challenges in generating harmful content when faced with problematic prompts.""",2024,2024-02-15T16:28:34Z,,,
arxiv2024,SwissNYF: Tool Grounded LLM Agents for Black Box Setting,Yes.,3,"""This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges.""",2024,2024-02-15T16:15:38Z,,,
arxiv2024,Self-Augmented In-Context Learning for Unsupervised Word Translation,Yes.,3,"""they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages"" and ""we also conduct comprehensive analyses on SAIL and discuss its limitations.""",2024,2024-02-15T15:43:05Z,,,
arxiv2024,LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition,Yes.,1,"""In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge.""",2024,2024-02-15T14:54:33Z,,,
arxiv2024,Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation,Yes.,3,"""Previous explanations of the ICL mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon.""",2024,2024-02-15T14:03:33Z,,,
arxiv2024,Generative AI in the Construction Industry: A State-of-the-art Analysis,Yes.,1,"""However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry.""",2024,2024-02-15T13:39:55Z,,,
arxiv2024,DE-COP: Detecting Copyrighted Content in Language Models Training Data,Yes.,1,"""We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text.""",2024,2024-02-15T12:17:15Z,,,
arxiv2024,Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks,Yes.,3,"""This study undertakes a systematic exploration of this challenge in two distinct phases",2024,2024-02-15T10:58:22Z,,,
arxiv2024,MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music,Yes.,1,"""we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music.""",2024,2024-02-15T10:55:01Z,,,
arxiv2024,LAPDoc: Layout-Aware Prompting for Documents,Yes.,3,"""In addition, we study the impact of noisy OCR and layout errors, as well as the limitations of LLMs when it comes to utilizing document layout.""",2024,2024-02-15T10:00:49Z,,,
arxiv2024,EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models,Yes.,3,"""Multimodal large language models (MLLMs) ... may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination.""",2024,2024-02-15T08:58:03Z,,,
arxiv2024,NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models,Yes.,3,"""The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware."" and ""Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints.""",2024,2024-02-15T08:03:12Z,,,
arxiv2024,Aligning Crowd Feedback via Distributional Preference Reward Modeling,Yes.,3,"""the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately.""",2024,2024-02-15T07:29:43Z,,,
arxiv2024,Grounding Language Model with Chunking-Free In-Context Retrieval,Yes.,3,"""Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations.""",2024,2024-02-15T07:22:04Z,,,
arxiv2024,Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish,Yes.,1,"""This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text.""",2024,2024-02-15T07:17:10Z,,,
arxiv2024,Model Compression and Efficient Inference for Large Language Models: A Survey,Yes.,3,"""However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices.""",2024,2024-02-15T06:58:30Z,,,
arxiv2024,AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis,Yes.,3,"""the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential.""",2024,2024-02-15T06:46:48Z,,,
arxiv2024,Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data,Yes.,1,"""TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM).""",2024,2024-02-15T06:30:12Z,,,
arxiv2024,AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns,Yes.,2,"""We have found strong empirical evidences to show that attackers can exploit ethical standards in the existing generative AI-based chatbot services by crafting prompt injection attacks to create newer smishing campaigns.""",2024,2024-02-15T05:49:22Z,,,
arxiv2024,Best Arm Identification for Prompt Learning under a Limited Budget,Yes.,2,"""the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered.""",2024,2024-02-15T05:31:13Z,,,
arxiv2024,How to Train Data-Efficient LLMs,Yes.,1,"""The training of large language models (LLMs) is expensive.""",2024,2024-02-15T02:27:57Z,,,
arxiv2024,ProtChatGPT: Towards Understanding Proteins with Large Language Models,Yes.,1,"""Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research.""",2024,2024-02-15T01:22:30Z,,,
arxiv2024,Answer is All You Need: Instruction-following Text Embedding via Answering the Question,Yes.,1,"""InBedder demonstrates significantly improved instruction-following capabilities according to our proposed instruction awareness tests and instruction robustness tests, when applied to both large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based LMs (e.g., roberta",2024,2024-02-15T01:02:41Z,,,
arxiv2024,LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations,Yes.,3,"""A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis.""",2024,2024-02-14T23:12:09Z,,,
arxiv2024,Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications,Yes.,3,"""We also emphasize the current limitations and discuss future work that has the potential to improve the performance on this task.""",2024,2024-02-14T21:33:13Z,,,
arxiv2024,Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems,Yes.,1,"""combining them, LLM further packages these insights into a coherent, human-understandable narrative.""",2024,2024-02-14T21:19:33Z,,,
arxiv2024,Rationality Report Cards: Assessing the Economic Rationality of Large Language Models,Yes.,2,"""determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality.""",2024,2024-02-14T20:05:26Z,,,
arxiv2024,"Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls",Yes.,3,"""While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks.""",2024,2024-02-14T19:00:01Z,,,
arxiv2024,AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability,Yes.,3,"""Our investigations reveal several interesting findings",2024,2024-02-14T18:59:33Z,,,
arxiv2024,Reinforcement Learning from Human Feedback with Active Queries,Yes.,2,"""Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect.""",2024,2024-02-14T18:58:40Z,,,
arxiv2024,"LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",Yes.,3,"""existing research indicates that their performance on chemistry tasks is discouragingly low.""",2024,2024-02-14T18:42:25Z,,,
arxiv2024,Copyright Traps for Large Language Models,Yes.,3,"""SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize a lot, we hypothesize--and later confirm--that they will not work against models that do not naturally memorize, e.g. medium-size 1B models.""",2024,2024-02-14T18:09:53Z,,,
arxiv2024,HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference,Yes.,3,"""Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache.""",2024,2024-02-14T18:04:36Z,,,
arxiv2024,ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization,Yes.,2,"""Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself.""",2024,2024-02-14T17:14:34Z,,,
arxiv2024,Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies,Yes.,1,"""Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural Language Processing (NLP), showing potential in traditional tasks such as Named Entity Recognition (NER).""",2024,2024-02-14T16:10:45Z,,,
arxiv2024,SyntaxShap: Syntax-aware Explainability Method for Text Generation,Yes.,1,"""To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions.""",2024,2024-02-14T15:45:56Z,,,
arxiv2024,Attacking Large Language Models with Projected Gradient Descent,Yes.,3,"""Current LLM alignment methods are readily broken through specifically crafted adversarial prompts.""",2024,2024-02-14T13:13:26Z,,,
arxiv2024,MPIrigen: MPI Code Generation through Domain-Specific Language Models,Yes.,3,"""Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs.""",2024,2024-02-14T12:24:21Z,,,
arxiv2024,L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects,Yes.,3,"""Yet, these models are not robust in precisely reasoning about physical and spatial configurations of objects, especially when instructed with unconventional, thereby out-of-distribution descriptions, such as 'a chair with five legs'.""",2024,2024-02-14T09:51:05Z,,,
arxiv2024,FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems,Yes.,1,"""we introduced FGeo-TP (Theorem Predictor), which utilizes the language model to predict theorem sequences for solving geometry problems.""",2024,2024-02-14T09:44:28Z,,,
arxiv2024,Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications,Yes.,2,"""a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency.""",2024,2024-02-14T08:46:15Z,,,
arxiv2024,Multi-Query Focused Disaster Summarization via Instruction-Based Prompting,Yes.,1,"""The summarizer module is based on the open-source Large Language Model (LLM) LLaMA-13b.""",2024,2024-02-14T08:22:58Z,,,
arxiv2024,Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision,Yes.,1,"""we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities.""",2024,2024-02-14T06:01:44Z,,,
arxiv2024,MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data,Yes.,3,"""As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges.""",2024,2024-02-14T05:57:58Z,,,
arxiv2024,MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences,Yes.,3,"""we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences.""",2024,2024-02-14T03:56:27Z,,,
arxiv2024,Tree-Based Hard Attention with Self-Motivation for Large Language Models,Yes.,3,"""While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps.""",2024,2024-02-14T00:40:51Z,,,
arxiv2024,Large Language Model with Graph Convolution for Recommendation,Yes.,3,"""existing ways of prompting LLMs with raw texts ignore structured knowledge of user-item interactions, which may lead to hallucination problems like inconsistent description generation.""",2024,2024-02-14T00:04:33Z,,,
arxiv2024,"eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",Yes.,1,"""large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields.""",2024,2024-02-13T22:26:24Z,,,
arxiv2024,Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy,Yes.,3,"""However, even LLMs specifically trained on medical topics may lack sufficient diagnostic accuracy for real-life applications.""",2024,2024-02-13T21:24:21Z,,,
arxiv2024,Rethinking Machine Unlearning for Large Language Models,Yes.,3,"""We highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment.""",2024,2024-02-13T20:51:58Z,,,
arxiv2024,JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models,Yes.,2,"""Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM's APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement.""",2024,2024-02-13T19:54:29Z,,,
arxiv2024,LLM-driven Imitation of Subrational Behavior : Illusion or Reality?,Yes.,3,"""We conclude by discussing the potential benefits, challenges and limitations of our framework.""",2024,2024-02-13T19:46:39Z,,,
arxiv2024,COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability,Yes.,3,"""Jailbreaks on Large language models (LLMs) have recently received increasing attention... Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability.""",2024,2024-02-13T18:58:48Z,,,
arxiv2024,Human Curriculum Effects Emerge with In-Context Learning in Neural Networks,Yes.,1,"""Here we show that this same tradeoff spontaneously emerges with 'in-context learning' (ICL) both in neural networks trained with metalearning and in large language models (LLMs).""",2024,2024-02-13T18:55:27Z,,,
arxiv2024,Improving Generalization in Semantic Parsing by Increasing Natural Language Variation,Yes.,3,"""However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions.""",2024,2024-02-13T18:48:23Z,,,
arxiv2024,The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting,Yes.,1,"""We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health.""",2024,2024-02-13T18:39:36Z,,,
arxiv2024,PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs,Yes.,3,"""Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding.""",2024,2024-02-13T18:39:18Z,,,
arxiv2024,Tandem Transformers for Inference Efficient LLMs,Yes.,3,"""The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations",2024,2024-02-13T18:24:08Z,,,
arxiv2024,SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages,Yes.,1,"""offering insights into the capabilities and performance of Large Language Models (LLMs).""",2024,2024-02-13T18:04:53Z,,,
arxiv2024,Knowledge Editing on Black-box Large Language Models,Yes.,3,"""To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses.""",2024,2024-02-13T17:59:34Z,,,
arxiv2024,PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment,Yes.,3,"""realistic tasks for agents are multi-step and introduce new challenges",2024,2024-02-13T16:38:01Z,,,
arxiv2024,Test-Time Backdoor Attacks on Multimodal Large Language Models,Yes.,1,"""In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images.""",2024,2024-02-13T16:28:28Z,,,
arxiv2024,Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style,Yes.,1,"""We audited counter-arguments generated by large language models (LLMs), focusing on their ability to generate evidence-based and stylistic counter-arguments to posts from the Reddit ChangeMyView dataset.""",2024,2024-02-13T14:53:12Z,,,
arxiv2024,Large Language Models as Minecraft Agents,Yes.,2,"""examining the challenges and opportunities for improvement.""",2024,2024-02-13T11:37:30Z,,,
arxiv2024,Punctuation Restoration Improves Structure Understanding without Supervision,Yes.,3,"""despite impressive generative capabilities of recent large language models, their abilities to capture syntactic or semantic structure within text lag behind.""",2024,2024-02-13T11:22:52Z,,,
arxiv2024,Unsupervised Evaluation of Code LLMs with Round-Trip Correctness,Yes.,2,"""To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains.""",2024,2024-02-13T11:08:08Z,,,
arxiv2024,Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries,Yes.,2,"""a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing"" and ""we explore the performance of representative Text-to-SQL systems and language models. We further quantify the impact of training data size, pre-, and post-processing steps as well as language model inference time.""",2024,2024-02-13T10:28:57Z,,,
arxiv2024,Eliciting Personality Traits in Large Language Models,Yes.,2,"""However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these 'black-box' models.""",2024,2024-02-13T10:09:00Z,,,
arxiv2024,Prompted Contextual Vectors for Spear-Phishing Detection,Yes.,1,"""Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance.""",2024,2024-02-13T09:12:55Z,,,
arxiv2024,ChatCell: Facilitating Single-Cell Analysis with Natural Language,Yes.,3,"""High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration.""",2024,2024-02-13T09:06:14Z,,,
arxiv2024,A Survey of Table Reasoning with Large Language Models,Yes.,3,"""Due to the existing lack of research, questions about which techniques can improve table reasoning performance in the era of LLMs, why LLMs excel at table reasoning, and how to enhance table reasoning abilities in the future, remain largely unexplored. This gap significantly limits progress in research.""",2024,2024-02-13T07:17:52Z,,,
arxiv2024,BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT,Yes.,1,"""To address this limitation, we propose an approach using BERT, which can learn more information from the maximal bi-cliques extracted by FCA and use them to make link prediction.""",2024,2024-02-13T06:02:05Z,,,
arxiv2024,Privacy-Preserving Language Model Inference with Instance Obfuscation,Yes.,3,"""Recent studies have started tackling the privacy issue by transforming input data into privacy-preserving representation from the user-end with the techniques such as noise addition and content perturbation, while the exploration of inference result protection, namely decision privacy, is still a blank page.""",2024,2024-02-13T05:36:54Z,,,
arxiv2024,Improving Black-box Robustness with In-Context Rewriting,Yes.,1,"""We propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function.""",2024,2024-02-13T05:33:35Z,,,
arxiv2024,BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models,Yes.,3,"""Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost.""",2024,2024-02-13T05:15:46Z,,,
arxiv2024,LLaGA: Large Language and Graph Assistant,Yes.,2,"""However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language.""",2024,2024-02-13T02:03:26Z,,,
arxiv2024,Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search,Yes.,1,"""We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq.""",2024,2024-02-13T00:55:14Z,,,
arxiv2024,Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts,Yes.,3,"""However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions.""",2024,2024-02-12T22:47:57Z,,,
arxiv2024,Grounding Data Science Code Generation with Input-Output Specifications,Yes.,3,"""However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification.""",2024,2024-02-12T21:32:49Z,,,
arxiv2024,Lumos : Empowering Multimodal LLMs with Scene Text Recognition,Yes.,2,"""While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference.""",2024,2024-02-12T19:27:26Z,,,
arxiv2024,Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs,Yes.,1,"""In this paper, we introduce refined Direct Preference Optimization (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data.""",2024,2024-02-12T19:10:13Z,,,
arxiv2024,WildfireGPT: Tailored Large Language Model for Wildfire Analysis,Yes.,3,"""LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change.""",2024,2024-02-12T18:41:55Z,,,
arxiv2024,Policy Improvement using Language Feedback Models,Yes.,1,"""To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions.""",2024,2024-02-12T18:41:34Z,,,
arxiv2024,AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy,Yes.,1,"""This study explores the potential of LLMs to augment judgement in forecasting tasks.""",2024,2024-02-12T18:14:43Z,,,
arxiv2024,Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning,Yes.,3,"""DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD.""",2024,2024-02-12T17:24:15Z,,,
arxiv2024,"TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection",Yes.,3,"""their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs).""",2024,2024-02-12T16:41:54Z,,,
arxiv2024,Quantitative knowledge retrieval from large language models,Yes.,2,"""Implications and challenges of using LLMs as 'experts' are discussed.""",2024,2024-02-12T16:32:37Z,,,
arxiv2024,CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity,Yes.,1,"""Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics.""",2024,2024-02-12T14:53:28Z,,,
arxiv2024,"Large Language Models ""Ad Referendum"": How Good Are They at Machine Translation in the Legal Domain?",Yes.,2,"""The results indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations.""",2024,2024-02-12T14:40:54Z,,,
arxiv2024,The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models,Yes.,1,"""This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription.""",2024,2024-02-12T14:01:12Z,,,
arxiv2024,Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models,Yes.,1,"""We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD.""",2024,2024-02-12T13:34:33Z,,,
arxiv2024,G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,Yes.,3,"""To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem.""",2024,2024-02-12T13:13:04Z,,,
arxiv2024,Anchor-based Large Language Models,Yes.,3,"""However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing.""",2024,2024-02-12T12:48:02Z,,,
arxiv2024,Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping,Yes.,1,"""our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models.""",2024,2024-02-12T12:30:42Z,,,
arxiv2024,BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection,Yes.,3,"""Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection.""",2024,2024-02-12T10:04:07Z,,,
arxiv2024,Secret Collusion Among Generative AI Agents,Yes.,3,"""While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities.""",2024,2024-02-12T09:31:21Z,,,
arxiv2024,T-RAG: Lessons from the LLM Trenches,Yes.,2,"""making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain.""",2024,2024-02-12T08:45:08Z,,,
arxiv2024,Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm,Yes.,2,"""However, a general-purpose Recommendation as Language Processing (RLP) approach lacks the critical components necessary for effective food recommendations.""",2024,2024-02-12T08:32:29Z,,,
arxiv2024,Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch,Yes.,1,"""The proposed system uses a large language model (LLM) for code generation to interpret and transform natural language commands into behavior branch, a proposed knowledge expression based on behavior trees, which facilitates execution by the game agent.""",2024,2024-02-12T06:49:48Z,,,
arxiv2024,Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples,Yes.,1,"""we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models.""",2024,2024-02-12T04:59:58Z,,,
arxiv2024,Differentially Private Training of Mixture of Experts Models,Yes.,2,"""However, this growth raises significant computational and privacy concerns.""",2024,2024-02-11T23:57:09Z,,,
arxiv2024,Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,Yes.,1,"""We find that the mechanistic story behind factual recall is more complex than previously thought.""",2024,2024-02-11T22:58:49Z,,,
arxiv2024,ODIN: Disentangled Reward Mitigates Hacking in RLHF,Yes.,3,"""A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores.""",2024,2024-02-11T22:40:12Z,,,
arxiv2024,A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference,Yes.,3,"""the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.""",2024,2024-02-11T21:44:21Z,,,
arxiv2024,How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?,Yes.,3,"""We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty.""",2024,2024-02-11T19:13:26Z,,,
arxiv2024,TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation,Yes.,1,"""This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants",2024,2024-02-11T15:50:35Z,,,
arxiv2024,Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models,Yes.,2,"""This means that for example, if conversational LLMs do not use a word it may become less and less frequent and eventually stop being used altogether.""",2024,2024-02-11T13:41:17Z,,,
arxiv2024,GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,Yes.,1,"""Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor.""",2024,2024-02-11T13:24:13Z,,,
arxiv2024,Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy,Yes.,1,"""facilitated by a large-language model (LLM) to enhance the planning quality"" and ""the LLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans through interaction with clinicians using natural language.""",2024,2024-02-11T11:24:09Z,,,
arxiv2024,Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces,Yes.,2,"""inclusion of LLM-based AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts.""",2024,2024-02-11T11:03:08Z,,,
arxiv2024,Natural Language Reinforcement Learning,Yes.,1,"""We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4.""",2024,2024-02-11T11:03:04Z,,,
arxiv2024,Graph Descriptive Order Improves Reasoning with Large Language Model,Yes.,3,"""However, the progress in the field of graph reasoning with LLM remains limited."" and ""We discover that the graph reasoning performance of LLMs does not monotonically decrease with the increase in graph size.""",2024,2024-02-11T09:46:24Z,,,
arxiv2024,Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education,Yes.,1,"""In this work, we propose a large language model-based approach to automatically generate test cases and show that they are good measures of student knowledge.""",2024,2024-02-11T01:37:48Z,,,
arxiv2024,Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models,Yes.,2,"""running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes.""",2024,2024-02-10T19:54:08Z,,,
arxiv2024,REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models,Yes.,2,"""address these limitations"" and ""eliminates hallucinations and ensures consistency.""",2024,2024-02-10T18:27:28Z,,,
arxiv2024,DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting,Yes.,3,"""Generic language models may not capture the complex clinical dimensions while specific clinical or biomedical models may not perform well on lay reports.""",2024,2024-02-10T16:48:45Z,,,
arxiv2024,A Thorough Examination of Decoding Methods in the Era of LLMs,Yes.,2,"""Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.""",2024,2024-02-10T11:14:53Z,,,
arxiv2024,UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction,Yes.,1,"""UrbanKGent-13B not only can significantly outperform 21 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost.""",2024,2024-02-10T01:50:19Z,,,
arxiv2024,"History, Development, and Principles of Large Language Models-An Introductory Survey",Yes.,3,"""The survey also highlights the limitations of existing work and points out promising future directions.""",2024,2024-02-10T01:18:15Z,,,
arxiv2024,ChemLLM: A Chemical Large Language Model,Yes.,3,"""the direct use of these structured data compromises the model's ability to maintain coherent dialogue.""",2024,2024-02-10T01:11:59Z,,,
arxiv2024,Forecasting Events in Soccer Matches Through Language,Yes.,1,"""a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs).""",2024,2024-02-09T23:02:57Z,,,
arxiv2024,The Unreasonable Effectiveness of Eccentric Automatic Prompts,Yes.,3,"""Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt.""",2024,2024-02-09T22:48:45Z,,,
arxiv2024,Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models,No.,1,The abstract discusses Large Event Models (LEMs) in the context of soccer analytics and does not mention language models (LLMs).,2024,2024-02-09T22:47:25Z,,,
arxiv2024,Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing,Yes.,1,"""This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques.""",2024,2024-02-09T21:37:13Z,,,
arxiv2024,Debating with More Persuasive LLMs Leads to More Truthful Answers,Yes.,2,"""Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data.""",2024,2024-02-09T21:05:01Z,,,
arxiv2024,EntGPT: Linking Generative Large Language Models with Knowledge Bases,Yes.,3,"""The ability of Large Language Models (LLMs) to generate factually correct output remains relatively unexplored due to the lack of fact-checking and knowledge grounding during training and inference.""",2024,2024-02-09T19:16:27Z,,,
arxiv2024,NICE: To Optimize In-Context Examples or Not?,Yes.,3,"""We challenge this consensus by investigating the necessity of optimizing ICE when task-specific instructions are provided and find that there are tasks for which it yields diminishing returns.""",2024,2024-02-09T19:09:19Z,,,
arxiv2024,If Turing played piano with an artificial partner,No.,1,The abstract discusses neural network architectures and generative models for producing musical scores but does not specifically mention language models (LLMs or LMs).,2024,2024-02-09T18:43:48Z,,,
arxiv2024,TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations,Yes.,3,"""LLMs excel at natural language processing but do not perform well on planning.""",2024,2024-02-09T18:39:13Z,,,
arxiv2024,G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German,Yes.,1,"""Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles.""",2024,2024-02-09T18:05:03Z,,,
arxiv2024,Calibrating Long-form Generations from Large Language Models,Yes.,3,"""larger models don't necessarily guarantee better calibration,"" and ""calibration performance is found to be metric-dependent.""",2024,2024-02-09T17:00:32Z,,,
arxiv2024,Large Language Models for Captioning and Retrieving Remote Sensing Images,Yes.,1,"""the development and application of vision and language models to the remote sensing domain have been hindered by the relatively small size of the available datasets and models used in previous studies.""",2024,2024-02-09T15:31:01Z,,,
arxiv2024,V-STaR: Training Verifiers for Self-Taught Reasoners,Yes.,3,"""These approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions.""",2024,2024-02-09T15:02:56Z,,,
arxiv2024,CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models,Yes.,1,"""In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents.""",2024,2024-02-09T12:10:00Z,,,
arxiv2024,RareBench: Can LLMs Serve as Rare Diseases Specialists?,Yes.,1,"""Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis."" and ""Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases.""",2024,2024-02-09T11:34:16Z,,,
arxiv2024,ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs,Yes.,2,"""However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy.""",2024,2024-02-09T11:23:14Z,,,
arxiv2024,InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning,Yes.,1,"""The math abilities of large language models can represent their abstract reasoning ability.""",2024,2024-02-09T11:22:08Z,,,
arxiv2024,Zero-shot Explainable Mental Health Analysis on Social Media by Incorporating Mental Scales,Yes.,3,"""The generative approaches, such as those based on large language models (LLMs), have the potential to get rid of heavy annotations and provide explanations but their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is",2024,2024-02-09T09:44:06Z,,,
arxiv2024,Entropy-Regularized Token-Level Policy Optimization for Large Language Models,Yes.,3,"""Nonetheless, it faces significant hurdles",2024,2024-02-09T07:45:26Z,,,
arxiv2024,Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants,Yes.,3,"""LLMs often leap to action without sufficient context, giving rise to implicit assumptions and inaccurate responses.""",2024,2024-02-09T07:44:27Z,,,
arxiv2024,ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement,Yes.,1,"""We propose ResumeFlow",2024,2024-02-09T07:13:44Z,,,
arxiv2024,Large Language Models: A Survey,Yes.,3,"""We review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations.""",2024,2024-02-09T05:37:09Z,,,
arxiv2024,CultureLLM: Incorporating Cultural Differences into Large Language Models,Yes.,3,"""Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora."" and ""Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources.""",2024,2024-02-09T04:02:43Z,,,
arxiv2024,Learn To be Efficient: Build Structured Sparsity in Large Language Models,Yes.,3,"""Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads.""",2024,2024-02-09T01:18:16Z,,,
arxiv2024,ContPhy: Continuum Physical Concept Learning and Reasoning from Videos,Yes.,1,"""We also introduce an oracle model (ContPRO) that marries the particle-based physical dynamic models with the recent large language models, which enjoy the advantages of both models, precise dynamic predictions, and interpretable reasoning.""",2024,2024-02-09T01:09:21Z,,,
arxiv2024,LLMs for Coding and Robotics Education,Yes.,2,"""Our results show that GPT-4V outperforms other models in all of our tests but struggles with generating block diagram images.""",2024,2024-02-09T00:58:57Z,,,
arxiv2024,Large Language Model Augmented Exercise Retrieval for Personalized Language Learning,Yes.,3,"""vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO.""",2024,2024-02-08T20:35:31Z,,,
arxiv2024,A Prompt Response to the Demand for Automatic Gender-Neutral Translation,Yes.,3,"""Through extensive manual analyses, our study empirically reveals the inherent limitations of current MT systems in generating GNTs and provides valuable insights into the potential and challenges associated with prompting for neutrality.""",2024,2024-02-08T20:24:44Z,,,
arxiv2024,Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing,Yes.,3,"""Experimental results indicate that GPT-4V excels at identifying cultural concepts but still exhibits weaker performance in low-resource languages, such as Tamil and Swahili.""",2024,2024-02-08T19:25:40Z,,,
arxiv2024,On the Convergence of Zeroth-Order Federated Tuning for Large Language Models,Yes.,3,"""the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on clients with limited computational resources.""",2024,2024-02-08T18:56:40Z,,,
arxiv2024,Efficient Stagewise Pretraining via Progressive Subnetworks,Yes.,3,"""it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages.""",2024,2024-02-08T18:49:09Z,,,
arxiv2024,FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs,Yes.,1,"""FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims.""",2024,2024-02-08T18:43:05Z,,,
arxiv2024,CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion,Yes.,1,"""It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation.""",2024,2024-02-08T18:27:22Z,,,
arxiv2024,How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis,Yes.,3,"""We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans.""",2024,2024-02-08T17:51:48Z,,,
arxiv2024,Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning,Yes.,3,"""The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation.""",2024,2024-02-08T16:46:26Z,,,
arxiv2024,Text-to-Code Generation with Modality-relative Pre-training,Yes.,3,"""programming language keywords (e.g. 'while') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa.""",2024,2024-02-08T16:17:24Z,,,
arxiv2024,Unified Speech-Text Pretraining for Spoken Dialog Modeling,Yes.,1,"""While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation.""",2024,2024-02-08T14:35:09Z,,,
arxiv2024,Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation,Yes.,2,"""Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse.""",2024,2024-02-08T14:21:03Z,,,
arxiv2024,Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design,Yes.,2,"""We also address a critical limitation of non-autoregressive models -- namely, that they tend to generate unrealistic sequences with overly repeating tokens.""",2024,2024-02-08T13:02:05Z,,,
arxiv2024,The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment,Yes.,1,"""The increasing popularity of AI, particularly Large Language Models (LLMs), has significantly impacted various domains, including Software Engineering.""",2024,2024-02-08T12:47:57Z,,,
arxiv2024,Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset,Yes.,1,"""To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess their effectiveness in communicative medical coaching tasks.""",2024,2024-02-08T10:32:06Z,,,
arxiv2024,Question Aware Vision Transformer for Multimodal Reasoning,Yes.,3,"""Despite their success, a critical limitation persists",2024,2024-02-08T08:03:39Z,,,
arxiv2024,It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition,Yes.,3,"""GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal.""",2024,2024-02-08T07:21:45Z,,,
arxiv2024,Large Language Models for Psycholinguistic Plausibility Pretesting,Yes.,3,"""We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.""",2024,2024-02-08T07:20:02Z,,,
arxiv2024,Accurate LoRA-Finetuning Quantization of LLMs via Information Retention,Yes.,3,"""However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA.""",2024,2024-02-08T06:53:31Z,,,
arxiv2024,GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study,Yes.,3,"""All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid.""",2024,2024-02-08T06:20:01Z,,,
arxiv2024,In-Context Principle Learning from Mistakes,Yes.,3,"""Nonetheless, all ICL-based approaches only learn from correct input-output pairs.""",2024,2024-02-08T04:42:29Z,,,
arxiv2024,Enhancing Zero-shot Counting via Language-guided Exemplar Learning,Yes.,1,"""inheriting rich semantic priors from the prevailing pre-trained Large Language Models (LLMs)""",2024,2024-02-08T04:07:38Z,,,
arxiv2024,CIC: A framework for Culturally-aware Image Captioning,Yes.,3,"""However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups.""",2024,2024-02-08T03:12:25Z,,,
arxiv2024,Scaling Up LLM Reviews for Google Ads Content Moderation,Yes.,3,"""Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository.""",2024,2024-02-07T23:47:02Z,,,
arxiv2024,Using text embedding models and vector databases as text classifiers with the example of medical data,Yes.,3,"""Using various LLMs to generate the medical data, we also understand the limitations of the medical knowledge of these models and encourage further expert medical review of our testing data.""",2024,2024-02-07T22:15:15Z,,,
arxiv2024,$$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space,Yes.,2,"""Predominantly, contemporary approaches, involving the training of Hypernetworks and Multimodal Large Language Models (MLLMs), require heavy computing resources that range from 600 to 12300 GPU hours of training.""",2024,2024-02-07T19:07:10Z,,,
arxiv2024,InCoRo: In-Context Learning for Robotics Control with Feedback Loops,Yes.,1,"""Recent advances in LLMs have positioned them as go-to tools for simple reasoning tasks, motivating the pioneering work of Liang et al. [35] that uses an LLM to translate natural language commands into low-level static execution plans for robotic units.""",2024,2024-02-07T19:01:11Z,,,
arxiv2024,Opening the AI black box: program synthesis via mechanistic interpretability,Yes.,3,"""As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub.""",2024,2024-02-07T18:59:12Z,,,
arxiv2024,Hydragen: High-Throughput LLM Inference with Shared Prefixes,Yes.,1,"""Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users.""",2024,2024-02-07T18:53:01Z,,,
arxiv2024,Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation,No.,1,The abstract does not mention LLMs or any specific limitations related to them.,2024,2024-02-07T18:44:27Z,,,
arxiv2024,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Yes.,3,"""Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics.""",2024,2024-02-07T17:33:54Z,,,
arxiv2024,A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?,Yes.,3,"""However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs.""",2024,2024-02-07T16:32:58Z,,,
arxiv2024,Pedagogical Alignment of Large Language Models,Yes.,1,"""In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts.""",2024,2024-02-07T16:15:59Z,,,
arxiv2024,ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12,Yes.,1,"""leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance.""",2024,2024-02-07T15:55:51Z,,,
arxiv2024,Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems,Yes.,2,"""However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial.""",2024,2024-02-07T15:39:07Z,,,
arxiv2024,L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ,Yes.,3,"""Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs)."" and ""However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-",2024,2024-02-07T14:35:05Z,,,
arxiv2024,Detecting Generated Native Ads in Conversational Search,Yes.,3,"""In our experiments sentence transformers achieve detection precision and recall values above 0.9, while the investigated LLMs struggle with the task.""",2024,2024-02-07T14:22:51Z,,,
arxiv2024,"Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation",Yes.,3,"""not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints.""",2024,2024-02-07T13:36:02Z,,,
arxiv2024,Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning,Yes.,1,"""We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses, thus ruling out any artificial improvement.""",2024,2024-02-07T13:32:11Z,,,
arxiv2024,Direct Language Model Alignment from Online AI Feedback,Yes.,3,"""the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy.""",2024,2024-02-07T12:31:13Z,,,
arxiv2024,ApiQ: Finetuning of 2-Bit Quantized Large Language Model,Yes.,3,"""current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of",2024,2024-02-07T09:36:54Z,,,
arxiv2024,LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views,Yes.,3,"""potential limitations in the pre-training data and models are often ignored.""",2024,2024-02-07T08:16:40Z,,,
arxiv2024,The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends,Yes.,2,"""Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion.""",2024,2024-02-07T07:28:34Z,,,
arxiv2024,SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph,Yes.,2,"""one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs.""",2024,2024-02-07T07:24:01Z,,,
arxiv2024,CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients,Yes.,1,"""To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs).""",2024,2024-02-07T07:07:02Z,,,
arxiv2024,TinyLLM: Learning a Small Student from Multiple Large Language Models,Yes.,3,"""However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information.""",2024,2024-02-07T06:48:24Z,,,
arxiv2024,Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,Yes.,3,"""However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains.""",2024,2024-02-07T06:13:14Z,,,
arxiv2024,"The Role of LLMs in Sustainable Smart Cities: Applications, Challenges, and Future Directions",Yes.,3,"""Our discourse culminates with an exploration of the formidable challenges that DL, FL, IoT, Blockchain, NLP, and LLMs face within these contexts, and we offer insights into potential future directions.""",2024,2024-02-07T05:22:10Z,,,
arxiv2024,Can Large Language Model Agents Simulate Human Trust Behaviors?,Yes.,3,"""We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations.""",2024,2024-02-07T03:37:19Z,,,
arxiv2024,An Artificial Intelligence (AI) workflow for catalyst design and optimization,Yes.,1,"""this study proposes an innovative Artificial Intelligence (AI) workflow that integrates Large Language Models (LLMs), Bayesian optimization, and an active learning loop to expedite and enhance catalyst optimization.""",2024,2024-02-07T03:25:08Z,,,
arxiv2024,RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation,Yes.,3,"""Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness.""",2024,2024-02-07T02:14:58Z,,,
arxiv2024,The Fine-Grained Complexity of Gradient Computation for Training Large Language Models,Yes.,2,"""there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false.""",2024,2024-02-07T00:45:31Z,,,
arxiv2024,Grandmaster-Level Chess Without Search,No.,1,The paper focuses on a transformer model trained for chess and does not discuss language models.,2024,2024-02-07T00:36:24Z,,,
arxiv2024,Structured Entity Extraction Using Large Language Models,Yes.,3,"""This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues.""",2024,2024-02-06T22:15:09Z,,,
arxiv2024,Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton,Yes.,3,"""Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service.""",2024,2024-02-06T21:14:45Z,,,
arxiv2024,Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning,Yes.,3,"""However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic.""",2024,2024-02-06T21:03:52Z,,,
arxiv2024,Fine-Tuned Language Models Generate Stable Inorganic Materials as Text,Yes.,1,"""We propose fine-tuning large language models for generation of stable materials.""",2024,2024-02-06T20:35:28Z,,,
arxiv2024,Monitoring the evolution of antisemitic discourse on extremist social media using BERT,Yes.,1,"""we created an unsupervised online machine learning approach that uses large language models to assess the contextual similarity of posts.""",2024,2024-02-06T20:34:49Z,,,
arxiv2024,The World of Generative AI: Deepfakes and Large Language Models,Yes.,3,"""LLMs are powerful language models that generate general-purpose language. However due to its generative aspect, it can also be a risk for people if used with ill intentions.""",2024,2024-02-06T20:18:32Z,,,
arxiv2024,Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains,Yes.,3,"""However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences.""",2024,2024-02-06T20:11:54Z,,,
arxiv2024,The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry,Yes.,3,"""However, linear attentions often underperform standard softmax attention in quality.""",2024,2024-02-06T19:31:26Z,,,
arxiv2024,LESS: Selecting Influential Data for Targeted Instruction Tuning,Yes.,3,"""The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning.""",2024,2024-02-06T19:18:04Z,,,
arxiv2024,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Yes.,3,"""Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs),"" and ""We identify several desirable properties previously unaccounted for in red teaming evaluations.""",2024,2024-02-06T18:59:08Z,,,
arxiv2024,Can Generative Agents Predict Emotion?,Yes.,3,"""The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary.""",2024,2024-02-06T18:39:43Z,,,
arxiv2024,Scaling Laws for Downstream Task Performance of Large Language Models,Yes.,2,"""However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves.""",2024,2024-02-06T17:31:20Z,,,
arxiv2024,Harnessing the Plug-and-Play Controller by Prompting,Yes.,3,"""Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation.""",2024,2024-02-06T17:18:25Z,,,
arxiv2024,Multi-line AI-assisted Code Authoring,Yes.,3,"""First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction."" and ""multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived",2024,2024-02-06T16:48:50Z,,,
arxiv2024,Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs),Yes.,1,"""By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs)",2024,2024-02-06T16:47:34Z,,,
arxiv2024,Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science,Yes.,3,"""two key issues remain",2024,2024-02-06T16:12:36Z,,,
arxiv2024,The Use of a Large Language Model for Cyberbullying Detection,Yes.,2,"""However, their performances are not consistent due to high class imbalance and generalisation issues.""",2024,2024-02-06T15:46:31Z,,,
arxiv2024,Provably learning a multi-head attention layer,Yes.,1,"""We focus on Boolean $\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian.""",2024,2024-02-06T15:39:09Z,,,
arxiv2024,LLM Agents can Autonomously Hack Websites,Yes.,2,"""Our findings raise questions about the widespread deployment of LLMs.""",2024,2024-02-06T14:46:08Z,,,
arxiv2024,Enhancing Retrieval Processes for Language Generation with Augmented Queries,Yes.,3,"""These models sometimes face difficulties, like providing inaccurate information, commonly known as 'hallucination.'""",2024,2024-02-06T13:19:53Z,,,
arxiv2024,Discovery of the Hidden World with Large Language Models,Yes.,1,"""The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data.""",2024,2024-02-06T12:18:54Z,,,
arxiv2024,Large Language Models to Enhance Bayesian Optimization,Yes.,1,"""we present LLAMBO, a novel approach that integrates the capabilities of Large Language Models (LLM) within BO.""",2024,2024-02-06T11:44:06Z,,,
arxiv2024,"Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy",Yes.,3,"""Lastly, we speculate that combining the information provided to LLM-powered environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions. While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated.""",2024,2024-02-06T11:19:40Z,,,
arxiv2024,DistiLLM: Towards Streamlined Distillation for Large Language Models,Yes.,3,"""current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs.""",2024,2024-02-06T11:10:35Z,,,
arxiv2024,ANLS* -- A Universal Document Processing Metric for Generative Large Language Models,Yes.,3,"""However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs.""",2024,2024-02-06T09:50:08Z,,,
arxiv2024,BiLLM: Pushing the Limit of Post-Training Quantization for LLMs,Yes.,3,"""Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources."" and ""existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths.""",2024,2024-02-06T09:26:34Z,,,
arxiv2024,Rethinking Skill Extraction in the Job Market Domain using Large Language Models,Yes.,3,"""However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions.""",2024,2024-02-06T09:23:26Z,,,
arxiv2024,ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs,Yes.,1,"""Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons.""",2024,2024-02-06T08:45:51Z,,,
arxiv2024,Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning,Yes.,2,"""existing pre-trained vision-language models require domain experts to carefully design the medical prompts, which greatly increases the burden on clinicians.""",2024,2024-02-06T07:53:23Z,,,
arxiv2024,MolTC: Towards Molecular Relational Modeling In Language Models,Yes.,3,"""Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs.""",2024,2024-02-06T07:51:56Z,,,
arxiv2024,Large Language Models As MOOCs Graders,Yes.,3,"""However, the History and Philosophy of Astronomy course proves to be more challenging in terms of grading as opposed to other courses.""",2024,2024-02-06T07:43:07Z,,,
arxiv2024,INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection,Yes.,3,"""Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs.""",2024,2024-02-06T06:23:12Z,,,
arxiv2024,Similarity-based Neighbor Selection for Graph LLMs,Yes.,3,"""Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs.""",2024,2024-02-06T05:29:05Z,,,
arxiv2024,Automatic Robotic Development through Collaborative Framework by Large Language Models,Yes.,3,"""Despite the remarkable code generation abilities of large language models LLMs, they still face challenges in complex task handling.""",2024,2024-02-06T04:40:27Z,,,
arxiv2024,Personalized Language Modeling from Personalized Human Feedback,Yes.,3,"""However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse."" and ""explain why vanilla RLHF can be problematic in this context.""",2024,2024-02-06T04:18:58Z,,,
arxiv2024,Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning,Yes.,3,"""previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR.""",2024,2024-02-06T03:41:12Z,,,
arxiv2024,Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models,Yes.,3,"""the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others.""",2024,2024-02-06T03:18:58Z,,,
arxiv2024,Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue,Yes.,1,"""Although existing studies have achieved great success based on the generative pretrained language model BART, they overlook exploiting the sentiments residing in the utterance, video and audio, which are vital clues for sarcasm explanation.""",2024,2024-02-06T03:14:46Z,,,
arxiv2024,Partially Recentralization Softmax Loss for Vision-Language Models Robustness,Yes.,3,"""it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input.""",2024,2024-02-06T01:44:38Z,,,
arxiv2024,Self-Discover: Large Language Models Self-Compose Reasoning Structures,Yes.,1,"""We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods.""",2024,2024-02-06T01:13:53Z,,,
arxiv2024,Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning,Yes.,3,"""large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships.""",2024,2024-02-06T00:51:27Z,,,
arxiv2024,Distinguishing the Knowable from the Unknowable with Language Models,Yes.,2,"""We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text.""",2024,2024-02-05T22:22:49Z,,,
arxiv2024,Neural networks for abstraction and reasoning: Towards broad generalization in machines,Yes.,3,"""LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches.""",2024,2024-02-05T20:48:57Z,,,
arxiv2024,Arabic Synonym BERT-based Adversarial Examples for Text Classification,Yes.,2,"""We find that fine-tuned BERT models were more susceptible to our synonym attacks than the other Deep Neural Networks (DNN) models like WordCNN and WordLSTM we trained.""",2024,2024-02-05T19:39:07Z,,,
arxiv2024,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,Yes.,3,"""Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature.""",2024,2024-02-05T18:55:32Z,,,
arxiv2024,"Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models",Yes.,3,"""How well can language models represent inherent uncertainty in conversations?"" and ""Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies... can calibrate smaller open-source models to compete with pre-trained models 10x their size.""",2024,2024-02-05T18:39:47Z,,,
arxiv2024,A Framework for Partially Observed Reward-States in RLHF,Yes.,3,"""Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment.""",2024,2024-02-05T18:38:55Z,,,
arxiv2024,MobilityGPT: Enhanced Human Mobility Modeling with a GPT model,Yes.,1,"""Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits.""",2024,2024-02-05T18:22:21Z,,,
arxiv2024,English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts,Yes.,1,"""Our experiments with natural language inference-based language models show that it is consistently better to use English prompts even if the data is in a different language.""",2024,2024-02-05T17:36:19Z,,,
arxiv2024,LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System,Yes.,1,"""One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods.""",2024,2024-02-05T16:47:17Z,,,
arxiv2024,Empowering Time Series Analysis with Large Language Models: A Survey,Yes.,3,"""completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training.""",2024,2024-02-05T16:46:35Z,,,
arxiv2024,CIDAR: Culturally Relevant Instruction Dataset For Arabic,Yes.,3,"""current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture.""",2024,2024-02-05T16:44:17Z,,,
arxiv2024,The Matrix: A Bayesian learning model for LLMs,Yes.,1,"""We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle.""",2024,2024-02-05T16:42:10Z,,,
arxiv2024,MULTI: Multimodal Understanding Leaderboard with Text and Images,Yes.,3,"""existing benchmarks primarily focus on understanding simple natural images and short context"" and ""Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on MULTI, in contrast to other MLLMs scoring between 28.5% and",2024,2024-02-05T16:41:02Z,,,
arxiv2024,Homograph Attacks on Maghreb Sentiment Analyzers,No.,1,The abstract does not mention language models (LLMs) explicitly.,2024,2024-02-05T16:39:15Z,,,
arxiv2024,Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization,Yes.,3,"""In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions.""",2024,2024-02-05T16:30:49Z,,,
arxiv2024,Constrained Decoding for Cross-lingual Label Projection,Yes.,2,"""However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods.""",2024,2024-02-05T15:57:32Z,,,
arxiv2024,Evaluation of ChatGPT Usability as A Code Generation Tool,Yes.,3,"""Our experiments demonstrated that ChatGPT is highly useful for generating R program code although it may fail on hard programming tasks."" and ""Our experiment also shows that it is hard for human developers to learn from experiences to improve the skill of using ChatGPT to generate code.""",2024,2024-02-05T15:56:19Z,,,
arxiv2024,Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases,Yes.,2,"""Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction.""",2024,2024-02-05T15:28:43Z,,,
arxiv2024,Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing,Yes.,1,"""a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images.""",2024,2024-02-05T13:16:12Z,,,
arxiv2024,LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models,Yes.,2,"""highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.""",2024,2024-02-05T11:05:20Z,,,
arxiv2024,Shortened LLaMA: A Simple Depth Pruning for Large Language Models,Yes.,1,"""Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs.""",2024,2024-02-05T09:44:49Z,,,
arxiv2024,KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models,Yes.,1,The abstract discusses a method for fine-tuning LLMs and finding effective subsets of parameters but does not mention any explicit limitations of the models.,2024,2024-02-05T08:19:56Z,,,
arxiv2024,Rethinking Optimization and Architecture for Tiny Language Models,Yes.,3,"""However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required.""",2024,2024-02-05T07:59:38Z,,,
arxiv2024,List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation,Yes.,1,"""retrieval-augmented generation for large language models (LLMs).""",2024,2024-02-05T06:52:53Z,,,
arxiv2024,Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering,Yes.,1,"""These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression.""",2024,2024-02-05T06:08:06Z,,,
arxiv2024,KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache,Yes.,3,"""the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage.""",2024,2024-02-05T06:06:47Z,,,
arxiv2024,Understanding the planning of LLM agents: A survey,Yes.,2,"""further challenges for the field of research are discussed.""",2024,2024-02-05T04:25:24Z,,,
arxiv2024,Adversarial Text Purification: A Large Language Model Approach for Defense,Yes.,1,"""We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations.""",2024,2024-02-05T02:36:41Z,,,
arxiv2024,Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases,Yes.,1,"""Instead, it leverages the world knowledge encoded in pre-trained large language models (LLMs) to synthesize programs in a domain-specific layout language that describe objects and spatial relations between them.""",2024,2024-02-05T01:59:31Z,,,
arxiv2024,RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews,Yes.,3,"""Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements. Our study highlights the opportunities and challenges in using LLMs to improve research efficiency.""",2024,2024-02-05T00:56:30Z,,,
arxiv2024,Zero-Shot Clinical Trial Patient Matching with LLMs,Yes.,1,"""Large language models (LLMs) offer a promising solution.""",2024,2024-02-05T00:06:08Z,,,
arxiv2024,Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity,Yes.,3,"""Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs).""",2024,2024-02-04T22:56:56Z,,,
arxiv2024,UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing,Yes.,3,"""existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code.""",2024,2024-02-04T22:48:05Z,,,
arxiv2024,A Truly Joint Neural Architecture for Segmentation and Parsing,Yes.,1,"""This proposed architecture is LLM-based and language agnostic, providing a solid foundation for MRLs to obtain further performance improvements and bridge the gap with other languages.""",2024,2024-02-04T16:56:08Z,,,
arxiv2024,DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models,Yes.,3,"""Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems."" and ""Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predomin",2024,2024-02-04T16:45:01Z,,,
arxiv2024,Are Large Language Models Table-based Fact-Checkers?,Yes.,2,"""Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability."" and ""We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples.""",2024,2024-02-04T15:52:59Z,,,
arxiv2024,Knowledge Generation for Zero-shot Knowledge-based VQA,Yes.,2,"""However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability.""",2024,2024-02-04T15:41:35Z,,,
arxiv2024,CompeteSMoE -- Effective Training of Sparse Mixture of Experts via Competition,Yes.,1,"""In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse.""",2024,2024-02-04T15:17:09Z,,,
arxiv2024,Conversational Crowdsensing: A Parallel Intelligence Powered Novel Sensing Approach,Yes.,1,"""new requirements and opportunities to current sensing approaches, especially in light of recent progress in Chatbots and Large Language Models (LLMs).""",2024,2024-02-04T15:10:11Z,,,
arxiv2024,GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering,Yes.,3,"""However, such conversion may introduce irrelevant information, which causes the LLM to misinterpret images and ignore visual details crucial for accurate knowledge.""",2024,2024-02-04T14:28:23Z,,,
arxiv2024,BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback,Yes.,2,"""However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward",2024,2024-02-04T13:16:29Z,,,
arxiv2024,A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer,No.,1,The abstract does not mention LLMs or their limitations.,2024,2024-02-04T12:29:40Z,,,
arxiv2024,FoldToken: Learning Protein Language via Vector Quantization and Beyond,No.,1,The abstract does not mention language models or their limitations.,2024,2024-02-04T12:18:51Z,,,
arxiv2024,Breaking MLPerf Training: A Case Study on Optimizing BERT,Yes.,1,"""We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance.""",2024,2024-02-04T11:12:17Z,,,
arxiv2024,LQER: Low-Rank Quantization Error Reconstruction for LLMs,Yes.,3,"""Post-training quantization of Large Language Models (LLMs) is challenging.""",2024,2024-02-04T10:59:52Z,,,
arxiv2024,Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction,Yes.,3,"""RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters.""",2024,2024-02-04T09:24:51Z,,,
arxiv2024,GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model,Yes.,3,"""Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design.""",2024,2024-02-04T08:57:54Z,,,
arxiv2024,KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion,Yes.,3,"""Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency.""",2024,2024-02-04T08:01:07Z,,,
arxiv2024,Evaluating Large Language Models in Analysing Classroom Dialogue,Yes.,2,"""Results indicate substantial time savings with GPT-4, and a high degree of consistency in coding between the model and human coders, with some discrepancies in specific codes.""",2024,2024-02-04T07:39:06Z,,,
arxiv2024,AutoTimes: Autoregressive Time Series Forecasters via Large Language Models,Yes.,1,"""increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series"" and ""we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters.""",2024,2024-02-04T06:59:21Z,,,
arxiv2024,Timer: Transformers for Time Series Analysis at Scale,No.,1,"The abstract discusses the development of large time series models (LTSM) and their applications in time series analysis, but it does not mention language models (LLMs or LLMs) specifically.",2024,2024-02-04T06:55:55Z,,,
arxiv2024,Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques,Yes.,1,"""Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking.""",2024,2024-02-04T05:51:14Z,,,
arxiv2024,Large Language Model Adaptation for Networking,Yes.,1,"""Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy.""",2024,2024-02-04T04:21:34Z,,,
arxiv2024,Enhance Reasoning for Large Language Models in the Game Werewolf,Yes.,1,"""This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents.""",2024,2024-02-04T03:47:10Z,,,
arxiv2024,Selecting Large Language Model to Fine-tune via Rectified Scaling Law,Yes.,2,"""We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically.""",2024,2024-02-04T01:55:00Z,,,
arxiv2024,Jailbreaking Attack against Multimodal Large Language Model,Yes.,3,"""seeking to elicit MLLMs to generate objectionable responses to harmful user queries,"" and ""we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks.""",2024,2024-02-04T01:29:24Z,,,
arxiv2024,Large Language Model for Table Processing: A Survey,Yes.,2,"""Finally, we highlight several challenges, ranging from private deployment and efficient inference to the development of extensive benchmarks for table manipulation and advanced data analysis.""",2024,2024-02-04T00:47:53Z,,,
arxiv2024,SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking,Yes.,2,"""However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible.""",2024,2024-02-03T22:49:00Z,,,
arxiv2024,Analyzing Sentiment Polarity Reduction in News Presentation through Contextual Perturbation and Large Language Models,Yes.,1,"""a context-aware masked language model""",2024,2024-02-03T13:27:32Z,,,
arxiv2024,Rendering Graphs for Graph Reasoning in Multimodal Large Language Models,Yes.,1,"""Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning.""",2024,2024-02-03T12:19:47Z,,,
arxiv2024,Break the Sequential Dependency of LLM Inference Using Lookahead Decoding,Yes.,3,"""Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators.""",2024,2024-02-03T06:37:50Z,,,
arxiv2024,Panacea: Pareto Alignment via Preference Adaptation for LLMs,Yes.,3,"""Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment.""",2024,2024-02-03T05:01:04Z,,,
arxiv2024,"PresAIse, A Prescriptive AI Solution for Enterprises",Yes.,1,"""the integration of large language models (LLMs) to bridge communication gaps via a conversation agent.""",2024,2024-02-03T03:23:08Z,,,
arxiv2024,SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks,Yes.,2,"""However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured.""",2024,2024-02-03T01:33:16Z,,,
arxiv2024,"A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions",Yes.,2,"""However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments.""",2024,2024-02-03T00:27:22Z,,,
arxiv2024,MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles,Yes.,1,"""We use an XLM-roBERTa-large model for sub-task A and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and BERT-base for sub-task B.""",2024,2024-02-03T00:23:36Z,,,
arxiv2024,Large Language Model Agent for Hyper-Parameter Optimization,Yes.,1,"""To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks.""",2024,2024-02-02T20:12:05Z,,,
arxiv2024,The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models,Yes.,3,"""We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.""",2024,2024-02-02T20:01:15Z,,,
arxiv2024,Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision,Yes.,1,"""We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision",2024,2024-02-02T19:45:39Z,,,
arxiv2024,Cross-modality debiasing: using language to mitigate sub-population shifts in imaging,Yes.,2,"""Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning.""",2024,2024-02-02T18:54:48Z,,,
arxiv2024,Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment,Yes.,1,"""Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations.""",2024,2024-02-02T18:49:26Z,,,
arxiv2024,Stochastic Two Points Method for Deep Model Zeroth-order Optimization,Yes.,2,"""Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation.""",2024,2024-02-02T18:39:40Z,,,
arxiv2024,MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models,Yes.,3,"""However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference.""",2024,2024-02-02T18:35:14Z,,,
arxiv2024,KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases,Yes.,2,"""PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data.""",2024,2024-02-02T18:32:24Z,,,
arxiv2024,Style Vectors for Steering Generative Large Language Model,Yes.,1,"""This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation.""",2024,2024-02-02T18:31:15Z,,,
arxiv2024,Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature,Yes.,1,"""In this work, we employed GPT-35-turbo, a large language model (LLM), to automatically extract the mean and standard deviation values of BP for both males and females from a dataset comprising 25 million abstracts sourced from PubMed.""",2024,2024-02-02T18:15:51Z,,,
arxiv2024,Ecologically rational meta-learned inference explains human category learning,Yes.,1,"""In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge.""",2024,2024-02-02T16:32:04Z,,,
arxiv2024,An Empirical Analysis of Diversity in Argument Summarization,Yes.,3,"""We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths.""",2024,2024-02-02T16:26:52Z,,,
arxiv2024,Decoding Speculative Decoding,Yes.,2,"""The speedup provided by speculative decoding heavily depends on the choice of the draft model."" and ""our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases.""",2024,2024-02-02T16:15:24Z,,,
arxiv2024,K-Level Reasoning with Large Language Models,Yes.,3,"""existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works.""",2024,2024-02-02T16:07:05Z,,,
arxiv2024,A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation,Yes.,3,"""large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information.""",2024,2024-02-02T15:26:39Z,,,
arxiv2024,AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback,Yes.,1,"""The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks.""",2024,2024-02-02T14:56:48Z,,,
arxiv2024,Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach,Yes.,2,"""The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.""",2024,2024-02-02T14:43:19Z,,,
arxiv2024,LLM-based NLG Evaluation: Current Status and Challenges,Yes.,3,"""we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively.""",2024,2024-02-02T13:06:35Z,,,
arxiv2024,Preference-free Alignment Learning with Regularized Relevance Reward,Yes.,3,"""our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones"" and ""the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts.""",2024,2024-02-02T11:58:08Z,,,
arxiv2024,Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion,Yes.,1,"""However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices.""",2024,2024-02-02T11:57:50Z,,,
arxiv2024,KTO: Model Alignment as Prospect Theoretic Optimization,Yes.,1,"""We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs).""",2024,2024-02-02T10:53:36Z,,,
arxiv2024,Efficient Causal Graph Discovery Using Large Language Models,Yes.,1,"""While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs.""",2024,2024-02-02T08:25:32Z,,,
arxiv2024,Large Language Models for Time Series: A Survey,Yes.,3,"""We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis.""",2024,2024-02-02T07:24:35Z,,,
arxiv2024,Efficient Prompt Caching via Embedding Similarity,Yes.,3,"""However, it faces the challenge of significant resource consumption during inference.""",2024,2024-02-02T06:34:11Z,,,
arxiv2024,Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward,Yes.,3,"""Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference.""",2024,2024-02-02T06:29:34Z,,,
arxiv2024,LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning,Yes.,3,"""Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance.""",2024,2024-02-02T05:54:12Z,,,
arxiv2024,CABINET: Content Relevance based Noise Reduction for Table Question Answering,Yes.,3,"""The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise.""",2024,2024-02-02T05:48:39Z,,,
arxiv2024,ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution,Yes.,1,"""This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces.""",2024,2024-02-02T05:04:51Z,,,
arxiv2024,PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models,Yes.,1,"""The design of PokeLLMon incorporates three key strategies",2024,2024-02-02T03:22:12Z,,,
arxiv2024,DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models,Yes.,2,"""Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy.""",2024,2024-02-02T03:21:00Z,,,
arxiv2024,Vaccine: Perturbation-aware Alignment for Large Language Model,Yes.,3,"""The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs)",2024,2024-02-02T02:56:50Z,,,
arxiv2024,"Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions",Yes.,3,"""Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings."" and ""Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time.""",2024,2024-02-02T02:53:11Z,,,
arxiv2024,Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs,Yes.,1,"""we propose a novel framework LVA-COG that leverages Large Language Models (LLMs) to extract insights from customer interests (e.g., character information) and employ prompt engineering techniques for accurate understanding of customer preferences.""",2024,2024-02-02T02:11:31Z,,,
arxiv2024,Specialized Language Models with Cheap Inference from Limited Domain Data,Yes.,3,"""Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets.""",2024,2024-02-02T01:45:18Z,,,
arxiv2024,Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer,Yes.,1,"""With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models.""",2024,2024-02-01T23:46:05Z,,,
arxiv2024,Plan-Grounded Large Language Models for Dual Goal Conversational Settings,Yes.,3,"""Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another.""",2024,2024-02-01T22:56:39Z,,,
arxiv2024,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",Yes.,3,"""Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership.""",2024,2024-02-01T22:54:31Z,,,
arxiv2024,IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition,Yes.,1,"""With the emergence of generative AI models such as large language models (LLMs) and text-driven motion synthesis models, language has become a promising source data modality as well as shown in proof of concepts such as IMUGPT.""",2024,2024-02-01T22:37:33Z,,,
arxiv2024,COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations,Yes.,1,"""This study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs.""",2024,2024-02-01T21:51:09Z,,,
arxiv2024,Getting the most out of your tokenizer for pre-training and domain adaptation,Yes.,2,"""Tokenization is an understudied and often neglected component of modern LLMs.""",2024,2024-02-01T21:49:34Z,,,
arxiv2024,Repeat After Me: Transformers are Better than State Space Models at Copying,Yes.,3,"""Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context.""",2024,2024-02-01T21:44:11Z,,,
arxiv2024,Executable Code Actions Elicit Better LLM Agents,Yes.,3,"""LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools).""",2024,2024-02-01T21:38:58Z,,,
arxiv2024,HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent,Yes.,2,"""However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset.""",2024,2024-02-01T21:10:44Z,,,
arxiv2024,Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?,Yes.,3,"""However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources."" and ""we observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets.""",2024,2024-02-01T18:31:34Z,,,
arxiv2024,"LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",Yes.,1,"""Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting.""",2024,2024-02-01T17:28:10Z,,,
arxiv2024,Unlearnable Algorithms for In-context Learning,Yes.,1,"""In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM).""",2024,2024-02-01T16:43:04Z,,,
arxiv2024,Health-LLM: Personalized Retrieval-Augmented Disease Prediction System,Yes.,2,"""However, there are conspicuous challenges such as vast data volumes and inconsistent symptom characterization standards, preventing full integration of healthcare AI systems with individual patients' needs.""",2024,2024-02-01T16:40:32Z,,,
arxiv2024,Transforming and Combining Rewards for Aligning Large Language Models,Yes.,2,"""This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model).""",2024,2024-02-01T16:39:28Z,,,
arxiv2024,Intent Assurance using LLMs guided by Intent Drift,Yes.,1,"""we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.""",2024,2024-02-01T16:09:19Z,,,
arxiv2024,Ocassionally Secure: A Comparative Analysis of Code Generation Assistants,Yes.,3,"""These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation.""",2024,2024-02-01T15:49:47Z,,,
arxiv2024,Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning,Yes.,2,"""Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data."" and ""But it also leads to extra cost and computation due to the involvement of LLMs in this process.""",2024,2024-02-01T11:57:53Z,,,
arxiv2024,EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models,Yes.,1,"""This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs).""",2024,2024-02-01T11:39:04Z,,,
arxiv2024,SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models,Yes.,3,"""However, their effective application in the medical domain is hampered by a lack of medical domain knowledge.""",2024,2024-02-01T10:26:27Z,,,
arxiv2024,From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models,Yes.,1,"""To bridge this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model (LLM) Enhanced PARIS (LE-PARIS).""",2024,2024-02-01T08:37:13Z,,,
arxiv2024,Prompt-Time Symbolic Knowledge Capture with Large Language Models,Yes.,2,"""LLMs inherently lack mechanisms for prompt-driven knowledge capture.""",2024,2024-02-01T08:15:28Z,,,
arxiv2024,What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection,Yes.,3,"""we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection"" and ""LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.""",2024,2024-02-01T06:21:19Z,,,
arxiv2024,Large Language Models Based Fuzzing Techniques: A Survey,Yes.,1,"""Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models.""",2024,2024-02-01T05:34:03Z,,,
arxiv2024,IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators,Yes.,1,"""we introduce a general bias detection framework, IndiVec, built upon large language models.""",2024,2024-02-01T05:20:07Z,,,
arxiv2024,PAP-REC: Personalized Automatic Prompt for Recommendation Language Model,Yes.,3,"""handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes.""",2024,2024-02-01T02:29:16Z,,,
arxiv2024,HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA,Yes.,3,"""However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application.""",2024,2024-02-01T02:24:15Z,,,
arxiv2024,Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs,Yes.,1,"""This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline.""",2024,2024-02-01T01:09:00Z,,,
arxiv2024,Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning,Yes.,3,"""This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models.""",2024,2024-02-01T00:23:31Z,,,
arxiv2024,PID Control-Based Self-Healing to Improve the Robustness of Large Language Models,No.,1,"""No evidence""",2024,2024-03-31T23:46:51Z,,,
arxiv2024,Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,No.,1,"""No evidence""",2024,2024-03-31T23:37:18Z,,,
arxiv2024,Training-Free Semantic Segmentation via LLM-Supervision,No.,1,"""No evidence""",2024,2024-03-31T14:37:25Z,,,
arxiv2024,Scaling Properties of Speech Language Models,No.,1,"""No evidence""",2024,2024-03-31T13:30:12Z,,,
arxiv2024,WavLLM: Towards Robust and Adaptive Speech Large Language Model,No.,1,"""No evidence""",2024,2024-03-31T12:01:32Z,,,
arxiv2024,Learning to Plan for Language Modeling from Unlabeled Data,No.,1,"""No evidence""",2024,2024-03-31T09:04:01Z,,,
arxiv2024,Extensive Self-Contrast Enables Feedback-Free Language Model Alignment,No.,1,"""No evidence""",2024,2024-03-31T08:30:15Z,,,
arxiv2024,Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing,No.,1,"""No evidence""",2024,2024-03-31T07:38:39Z,,,
arxiv2024,CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,No.,1,"""No evidence""",2024,2024-03-31T07:11:48Z,,,
arxiv2024,DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model,No.,1,"""No evidence""",2024,2024-03-31T06:28:15Z,,,
arxiv2024,ParaICL: Towards Robust Parallel In-Context Learning,No.,1,"""No evidence""",2024,2024-03-31T05:56:15Z,,,
arxiv2024,CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks,No.,1,"""No evidence""",2024,2024-03-31T05:20:53Z,,,
arxiv2024,Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization,No.,1,"""No evidence""",2024,2024-03-31T02:05:40Z,,,
arxiv2024,Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App,No.,1,"""No evidence""",2024,2024-03-30T23:01:34Z,,,
arxiv2024,Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4,No.,1,"""No evidence""",2024,2024-03-30T22:27:21Z,,,
arxiv2024,Linguistic Calibration of Language Models,No.,1,"""No evidence""",2024,2024-03-30T20:47:55Z,,,
arxiv2024,NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,No.,1,"""No evidence""",2024,2024-03-30T19:46:59Z,,,
arxiv2024,MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks,No.,1,"""No evidence""",2024,2024-03-30T19:43:45Z,,,
arxiv2024,Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order,No.,1,"""No evidence""",2024,2024-03-30T15:38:54Z,,,
arxiv2024,Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks,No.,1,"""No evidence""",2024,2024-03-30T14:09:00Z,,,
arxiv2024,Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,No.,1,"""No evidence""",2024,2024-03-30T12:13:57Z,,,
arxiv2024,ST-LLM: Large Language Models Are Effective Temporal Learners,No.,1,"""No evidence""",2024,2024-03-30T10:11:26Z,,,
arxiv2024,Instruction-Driven Game Engines on Large Language Models,No.,1,"""No evidence""",2024,2024-03-30T08:02:16Z,,,
arxiv2024,DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference,No.,1,"""No evidence""",2024,2024-03-30T04:34:54Z,,,
arxiv2024,A Survey of using Large Language Models for Generating Infrastructure as Code,No.,1,"""No evidence""",2024,2024-03-30T02:57:55Z,,,
arxiv2024,Conceptual and Unbiased Reasoning in Language Models,No.,1,"""No evidence""",2024,2024-03-30T00:53:53Z,,,
arxiv2024,GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs,No.,1,"""No evidence""",2024,2024-03-29T23:04:04Z,,,
arxiv2024,ReALM: Reference Resolution As Language Modeling,No.,1,"""No evidence""",2024,2024-03-29T17:59:06Z,,,
arxiv2024,Gecko: Versatile Text Embeddings Distilled from Large Language Models,No.,1,"""No evidence""",2024,2024-03-29T17:56:40Z,,,
arxiv2024,"Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain",No.,1,"""No evidence""",2024,2024-03-29T16:59:13Z,,,
arxiv2024,LayerNorm: A key component in parameter-efficient fine-tuning,No.,1,"""No evidence""",2024,2024-03-29T16:53:11Z,,,
arxiv2024,ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models,No.,1,"""No evidence""",2024,2024-03-29T16:13:31Z,,,
arxiv2024,Using LLMs to Model the Beliefs and Preferences of Targeted Populations,No.,1,"""No evidence""",2024,2024-03-29T15:58:46Z,,,
arxiv2024,H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model,No.,1,"""No evidence""",2024,2024-03-29T14:50:43Z,,,
arxiv2024,"The Future of Combating Rumors? Retrieval, Discrimination, and Generation",No.,1,"""No evidence""",2024,2024-03-29T14:32:41Z,,,
arxiv2024,Measuring Taiwanese Mandarin Language Understanding,No.,1,"""No evidence""",2024,2024-03-29T13:56:21Z,,,
arxiv2024,ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models,No.,1,"""No evidence""",2024,2024-03-29T13:12:09Z,,,
arxiv2024,Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries,No.,1,"""No evidence""",2024,2024-03-29T12:25:37Z,,,
arxiv2024,User Modeling Challenges in Interactive AI Assistant Systems,No.,1,"""No evidence""",2024,2024-03-29T11:54:13Z,,,
arxiv2024,The Impact of Prompts on Zero-Shot Detection of AI-Generated Text,No.,1,"""No evidence""",2024,2024-03-29T11:33:34Z,,,
arxiv2024,PURPLE: Making a Large Language Model a Better SQL Writer,No.,1,"""No evidence""",2024,2024-03-29T07:01:29Z,,,
arxiv2024,Large Language Model based Situational Dialogues for Second Language Learning,No.,1,"""No evidence""",2024,2024-03-29T06:43:55Z,,,
arxiv2024,Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning,No.,1,"""No evidence""",2024,2024-03-29T03:48:12Z,,,
arxiv2024,DiJiang: Efficient Large Language Models through Compact Kernelization,No.,1,"""No evidence""",2024,2024-03-29T02:32:15Z,,,
arxiv2024,MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models,No.,1,"""No evidence""",2024,2024-03-29T01:53:24Z,,,
arxiv2024,LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces,No.,1,"""No evidence""",2024,2024-03-28T22:06:04Z,,,
arxiv2024,Localizing Paragraph Memorization in Language Models,No.,1,"""No evidence""",2024,2024-03-28T21:53:24Z,,,
arxiv2024,"Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",No.,1,"""No evidence""",2024,2024-03-28T21:18:33Z,,,
arxiv2024,Target Span Detection for Implicit Harmful Content,No.,1,"""No evidence""",2024,2024-03-28T21:15:15Z,,,
arxiv2024,Developing Healthcare Language Model Embedding Spaces,No.,1,"""No evidence""",2024,2024-03-28T19:31:32Z,,,
arxiv2024,Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care,No.,1,"""No evidence""",2024,2024-03-28T19:17:07Z,,,
arxiv2024,InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction,No.,1,"""No evidence""",2024,2024-03-28T17:59:30Z,,,
arxiv2024,MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions,No.,1,"""No evidence""",2024,2024-03-28T17:59:20Z,,,
arxiv2024,Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis,No.,1,"""No evidence""",2024,2024-03-28T17:55:42Z,,,
arxiv2024,Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,No.,1,"""No evidence""",2024,2024-03-28T17:47:19Z,,,
arxiv2024,TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes,No.,1,"""No evidence""",2024,2024-03-28T17:12:55Z,,,
arxiv2024,WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models,No.,1,"""No evidence""",2024,2024-03-28T16:28:38Z,,,
arxiv2024,JDocQA: Japanese Document Question Answering Dataset for Generative Language Models,No.,1,"""No evidence""",2024,2024-03-28T14:22:54Z,,,
arxiv2024,Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,No.,1,"""No evidence""",2024,2024-03-28T14:15:10Z,,,
arxiv2024,Checkpoint Merging via Bayesian Optimization in LLM Pretraining,No.,1,"""No evidence""",2024,2024-03-28T13:01:18Z,,,
arxiv2024,Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models,No.,1,"""No evidence""",2024,2024-03-28T11:26:30Z,,,
arxiv2024,Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation,No.,1,"""No evidence""",2024,2024-03-28T10:13:34Z,,,
arxiv2024,sDPO: Don't Use Your Data All at Once,No.,1,"""No evidence""",2024,2024-03-28T09:56:04Z,,,
arxiv2024,Dual-Personalizing Adapter for Federated Foundation Models,No.,1,"""No evidence""",2024,2024-03-28T08:19:33Z,,,
arxiv2024,Text Data-Centric Image Captioning with Interactive Prompts,No.,1,"""No evidence""",2024,2024-03-28T07:43:49Z,,,
arxiv2024,MUGC: Machine Generated versus User Generated Content Detection,No.,1,"""No evidence""",2024,2024-03-28T07:33:53Z,,,
arxiv2024,Make Large Language Model a Better Ranker,No.,1,"""No evidence""",2024,2024-03-28T07:22:16Z,,,
arxiv2024,Code Comparison Tuning for Code Large Language Models,No.,1,"""No evidence""",2024,2024-03-28T03:25:23Z,,,
arxiv2024,MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering,No.,1,"""No evidence""",2024,2024-03-28T03:14:18Z,,,
arxiv2024,Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation,No.,1,"""No evidence""",2024,2024-03-28T02:35:53Z,,,
arxiv2024,CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems,No.,1,"""No evidence""",2024,2024-03-27T23:45:31Z,,,
arxiv2024,Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data,No.,1,"""No evidence""",2024,2024-03-27T22:05:10Z,,,
arxiv2024,Towards LLM-RecSys Alignment with Textual ID Learning,No.,1,"""No evidence""",2024,2024-03-27T21:22:37Z,,,
arxiv2024,TextCraftor: Your Text Encoder Can be Image Quality Controller,No.,1,"""No evidence""",2024,2024-03-27T19:52:55Z,,,
arxiv2024,Measuring Political Bias in Large Language Models: What Is Said and How It Is Said,No.,1,"""No evidence""",2024,2024-03-27T18:22:48Z,,,
arxiv2024,Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment,No.,1,"""No evidence""",2024,2024-03-27T17:57:02Z,,,
arxiv2024,Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation,No.,1,"""No evidence""",2024,2024-03-27T17:50:00Z,,,
arxiv2024,Projective Methods for Mitigating Gender Bias in Pre-trained Language Models,No.,1,"""No evidence""",2024,2024-03-27T17:49:31Z,,,
arxiv2024,Long-form factuality in large language models,No.,1,"""No evidence""",2024,2024-03-27T17:48:55Z,,,
arxiv2024,Understanding the Learning Dynamics of Alignment with Human Feedback,No.,1,"""No evidence""",2024,2024-03-27T16:39:28Z,,,
arxiv2024,The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian,No.,1,"""No evidence""",2024,2024-03-27T15:46:25Z,,,
arxiv2024,SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens,No.,1,"""No evidence""",2024,2024-03-27T14:54:27Z,,,
arxiv2024,Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval,No.,1,"""No evidence""",2024,2024-03-27T09:46:56Z,,,
arxiv2024,Improving Attributed Text Generation of Large Language Models via Preference Learning,No.,1,"""No evidence""",2024,2024-03-27T09:19:13Z,,,
arxiv2024,BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models,No.,1,"""No evidence""",2024,2024-03-27T08:57:21Z,,,
arxiv2024,Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective,No.,1,"""No evidence""",2024,2024-03-27T08:38:49Z,,,
arxiv2024,IterAlign: Iterative Constitutional Alignment of Large Language Models,No.,1,"""No evidence""",2024,2024-03-27T08:32:19Z,,,
arxiv2024,Toward Interactive Regional Understanding in Vision-Large Language Models,No.,1,"""No evidence""",2024,2024-03-27T05:22:06Z,,,
arxiv2024,Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models,No.,1,"""No evidence""",2024,2024-03-27T04:49:23Z,,,
arxiv2024,Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,No.,1,"""No evidence""",2024,2024-03-27T04:20:18Z,,,
arxiv2024,Large Language Models Produce Responses Perceived to be Empathic,No.,1,"""No evidence""",2024,2024-03-26T23:14:34Z,,,
arxiv2024,Juru: Legal Brazilian Large Language Model from Reputable Sources,No.,1,"""No evidence""",2024,2024-03-26T22:54:12Z,,,
arxiv2024,Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization,No.,1,"""No evidence""",2024,2024-03-26T22:01:13Z,,,
arxiv2024,Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models,No.,1,"""No evidence""",2024,2024-03-26T20:25:53Z,,,
arxiv2024,PerOS: Personalized Self-Adapting Operating Systems in the Cloud,No.,1,"""No evidence""",2024,2024-03-26T20:10:31Z,,,
arxiv2024,COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning,No.,1,"""No evidence""",2024,2024-03-26T19:24:18Z,,,
arxiv2024,Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER,No.,1,"""No evidence""",2024,2024-03-26T18:23:16Z,,,
arxiv2024,The Unreasonable Ineffectiveness of the Deeper Layers,No.,1,"""No evidence""",2024,2024-03-26T17:20:04Z,,,
arxiv2024,Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs,No.,1,"""No evidence""",2024,2024-03-26T16:45:27Z,,,
arxiv2024,ArabicaQA: A Comprehensive Dataset for Arabic Question Answering,No.,1,"""No evidence""",2024,2024-03-26T16:37:54Z,,,
arxiv2024,Are Compressed Language Models Less Subgroup Robust?,No.,1,"""No evidence""",2024,2024-03-26T15:50:37Z,,,
arxiv2024,Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons,No.,1,"""No evidence""",2024,2024-03-26T14:51:12Z,,,
arxiv2024,ExpressEdit: Video Editing with Natural Language and Sketching,No.,1,"""No evidence""",2024,2024-03-26T13:34:21Z,,,
arxiv2024,Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes,No.,1,"""No evidence""",2024,2024-03-26T13:32:32Z,,,
arxiv2024,Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games,No.,1,"""No evidence""",2024,2024-03-26T13:02:46Z,,,
arxiv2024,Targeted Visualization of the Backbone of Encoder LLMs,No.,1,"""No evidence""",2024,2024-03-26T12:51:02Z,,,
arxiv2024,Language Models for Text Classification: Is In-Context Learning Enough?,No.,1,"""No evidence""",2024,2024-03-26T12:47:39Z,,,
arxiv2024,"""You are an expert annotator"": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling",No.,1,"""No evidence""",2024,2024-03-26T11:45:22Z,,,
arxiv2024,Naive Bayes-based Context Extension for Large Language Models,No.,1,"""No evidence""",2024,2024-03-26T09:59:45Z,,,
arxiv2024,Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction,No.,1,"""No evidence""",2024,2024-03-26T09:43:15Z,,,
arxiv2024,ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler,No.,1,"""No evidence""",2024,2024-03-26T09:41:21Z,,,
arxiv2024,Robust and Scalable Model Editing for Large Language Models,No.,1,"""No evidence""",2024,2024-03-26T06:57:23Z,,,
arxiv2024,LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction,No.,1,"""No evidence""",2024,2024-03-26T06:12:21Z,,,
arxiv2024,Disambiguate Entity Matching through Relation Discovery with Large Language Models,No.,1,"""No evidence""",2024,2024-03-26T03:07:32Z,,,
arxiv2024,Residual-based Language Models are Free Boosters for Biomedical Imaging,No.,1,"""No evidence""",2024,2024-03-26T03:05:20Z,,,
arxiv2024,The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge,No.,1,"""No evidence""",2024,2024-03-26T03:03:50Z,,,
arxiv2024,Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models,No.,1,"""No evidence""",2024,2024-03-26T02:47:42Z,,,
arxiv2024,"Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",No.,1,"""No evidence""",2024,2024-03-26T01:28:42Z,,,
arxiv2024,Automate Knowledge Concept Tagging on Math Questions with LLMs,No.,1,"""No evidence""",2024,2024-03-26T00:09:38Z,,,
arxiv2024,TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models,No.,1,"""No evidence""",2024,2024-03-25T22:47:13Z,,,
arxiv2024,Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis,No.,1,"""No evidence""",2024,2024-03-25T21:46:35Z,,,
arxiv2024,Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node,No.,1,"""No evidence""",2024,2024-03-25T21:37:30Z,,,
arxiv2024,Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model,No.,1,"""No evidence""",2024,2024-03-25T21:19:50Z,,,
arxiv2024,Outcome-Constrained Large Language Models for Countering Hate Speech,No.,1,"""No evidence""",2024,2024-03-25T19:44:06Z,,,
arxiv2024,MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models,No.,1,"""No evidence""",2024,2024-03-25T19:28:10Z,,,
arxiv2024,The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition,No.,1,"""No evidence""",2024,2024-03-25T19:07:32Z,,,
arxiv2024,DreamLIP: Language-Image Pre-training with Long Captions,No.,1,"""No evidence""",2024,2024-03-25T17:59:42Z,,,
arxiv2024,Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows,No.,1,"""No evidence""",2024,2024-03-25T17:58:22Z,,,
arxiv2024,Comp4D: LLM-Guided Compositional 4D Scene Generation,No.,1,"""No evidence""",2024,2024-03-25T17:55:52Z,,,
arxiv2024,PropTest: Automatic Property Testing for Improved Visual Programming,No.,1,"""No evidence""",2024,2024-03-25T16:39:15Z,,,
arxiv2024,Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data,No.,1,"""No evidence""",2024,2024-03-25T16:21:25Z,,,
arxiv2024,Do LLM Agents Have Regret? A Case Study in Online Learning and Games,No.,1,"""No evidence""",2024,2024-03-25T15:04:11Z,,,
arxiv2024,"All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification",No.,1,"""No evidence""",2024,2024-03-25T13:23:24Z,,,
arxiv2024,CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment,No.,1,"""No evidence""",2024,2024-03-25T11:37:15Z,,,
arxiv2024,Elysium: Exploring Object-level Perception in Videos via MLLM,No.,1,"""No evidence""",2024,2024-03-25T09:17:15Z,,,
arxiv2024,CodeS: Natural Language to Code Repository via Multi-Layer Sketch,No.,1,"""No evidence""",2024,2024-03-25T06:09:55Z,,,
arxiv2024,Evaluating Large Language Models with Runtime Behavior of Program Execution,No.,1,"""No evidence""",2024,2024-03-25T05:37:16Z,,,
arxiv2024,InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models,No.,1,"""No evidence""",2024,2024-03-25T05:31:22Z,,,
arxiv2024,$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models,No.,1,"""No evidence""",2024,2024-03-25T05:27:35Z,,,
arxiv2024,Dia-LLaMA: Towards Large Language Model-driven CT Report Generation,No.,1,"""No evidence""",2024,2024-03-25T03:02:51Z,,,
arxiv2024,Enhanced Facet Generation with LLM Editing,No.,1,"""No evidence""",2024,2024-03-25T00:43:44Z,,,
arxiv2024,Is Watermarking LLM-Generated Code Robust?,No.,1,"""No evidence""",2024,2024-03-24T21:41:29Z,,,
arxiv2024,Large Language Models in Biomedical and Health Informatics: A Bibliometric Review,No.,1,"""No evidence""",2024,2024-03-24T21:29:39Z,,,
arxiv2024,Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling,No.,1,"""No evidence""",2024,2024-03-24T17:39:51Z,,,
arxiv2024,CoverUp: Coverage-Guided LLM-Based Test Generation,No.,1,"""No evidence""",2024,2024-03-24T16:18:27Z,,,
arxiv2024,ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models,No.,1,"""No evidence""",2024,2024-03-24T15:09:55Z,,,
arxiv2024,Opportunities and challenges in the application of large artificial intelligence models in radiology,No.,1,"""No evidence""",2024,2024-03-24T12:05:23Z,,,
arxiv2024,Can Language Models Pretend Solvers? Logic Code Simulation with LLMs,No.,1,"""No evidence""",2024,2024-03-24T11:27:16Z,,,
arxiv2024,LLMs as Compiler for Arabic Programming Language,No.,1,"""No evidence""",2024,2024-03-24T10:57:08Z,,,
arxiv2024,Monotonic Paraphrasing Improves Generalization of Language Model Prompting,No.,1,"""No evidence""",2024,2024-03-24T06:49:07Z,,,
arxiv2024,Leveraging Zero-Shot Prompting for Efficient Language Model Distillation,No.,1,"""No evidence""",2024,2024-03-23T16:51:52Z,,,
arxiv2024,Using Large Language Models for OntoClean-based Ontology Refinement,No.,1,"""No evidence""",2024,2024-03-23T15:09:50Z,,,
arxiv2024,When LLM-based Code Generation Meets the Software Development Process,No.,1,"""No evidence""",2024,2024-03-23T14:04:48Z,,,
arxiv2024,Computational Sentence-level Metrics Predicting Human Sentence Comprehension,No.,1,"""No evidence""",2024,2024-03-23T12:19:49Z,,,
arxiv2024,AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving,No.,1,"""No evidence""",2024,2024-03-23T10:42:49Z,,,
arxiv2024,The Frontier of Data Erasure: Machine Unlearning for Large Language Models,No.,1,"""No evidence""",2024,2024-03-23T09:26:15Z,,,
arxiv2024,Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study,No.,1,"""No evidence""",2024,2024-03-23T07:59:30Z,,,
arxiv2024,Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models,No.,1,"""No evidence""",2024,2024-03-23T06:36:32Z,,,
arxiv2024,LLMs Instruct LLMs:An Extraction and Editing Method,No.,1,"""No evidence""",2024,2024-03-23T06:03:36Z,,,
arxiv2024,Towards a RAG-based Summarization Agent for the Electron-Ion Collider,No.,1,"""No evidence""",2024,2024-03-23T05:32:46Z,,,
arxiv2024,Contact-aware Human Motion Generation from Textual Descriptions,No.,1,"""No evidence""",2024,2024-03-23T04:08:39Z,,,
arxiv2024,FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models,No.,1,"""No evidence""",2024,2024-03-23T03:32:26Z,,,
arxiv2024,SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models,No.,1,"""No evidence""",2024,2024-03-23T03:23:29Z,,,
arxiv2024,MixRED: A Mix-lingual Relation Extraction Dataset,No.,1,"""No evidence""",2024,2024-03-23T03:18:14Z,,,
arxiv2024,EAGLE: A Domain Generalization Framework for AI-generated Text Detection,No.,1,"""No evidence""",2024,2024-03-23T02:44:20Z,,,
arxiv2024,AI for Biomedicine in the Era of Large Language Models,No.,1,"""No evidence""",2024,2024-03-23T01:40:22Z,,,
arxiv2024,SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning,No.,1,"""No evidence""",2024,2024-03-22T23:12:28Z,,,
arxiv2024,"Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors",No.,1,"""No evidence""",2024,2024-03-22T19:21:29Z,,,
arxiv2024,MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis,No.,1,"""No evidence""",2024,2024-03-22T19:19:51Z,,,
arxiv2024,Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs,No.,1,"""No evidence""",2024,2024-03-22T17:27:18Z,,,
arxiv2024,CoLLEGe: Concept Embedding Generation for Large Language Models,No.,1,"""No evidence""",2024,2024-03-22T17:26:05Z,,,
arxiv2024,Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization,No.,1,"""No evidence""",2024,2024-03-22T16:35:38Z,,,
arxiv2024,Sphere Neural-Networks for Rational Reasoning,No.,1,"""No evidence""",2024,2024-03-22T15:44:59Z,,,
arxiv2024,Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review,No.,1,"""No evidence""",2024,2024-03-22T15:16:23Z,,,
arxiv2024,Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs,No.,1,"""No evidence""",2024,2024-03-22T15:16:10Z,,,
arxiv2024,Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models,No.,1,"""No evidence""",2024,2024-03-22T15:06:45Z,,,
arxiv2024,Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach,No.,1,"""No evidence""",2024,2024-03-22T14:47:35Z,,,
arxiv2024,FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions,No.,1,"""No evidence""",2024,2024-03-22T14:42:29Z,,,
arxiv2024,An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets,No.,1,"""No evidence""",2024,2024-03-22T14:23:21Z,,,
arxiv2024,Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models,No.,1,"""No evidence""",2024,2024-03-22T14:20:34Z,,,
arxiv2024,Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study,No.,1,"""No evidence""",2024,2024-03-22T13:13:13Z,,,
arxiv2024,CACA Agent: Capability Collaboration based AI Agent,No.,1,"""No evidence""",2024,2024-03-22T11:42:47Z,,,
arxiv2024,Construction of a Japanese Financial Benchmark for Large Language Models,No.,1,"""No evidence""",2024,2024-03-22T09:40:27Z,,,
arxiv2024,LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement,No.,1,"""No evidence""",2024,2024-03-22T08:57:07Z,,,
arxiv2024,Risk and Response in Large Language Models: Evaluating Key Threat Categories,No.,1,"""No evidence""",2024,2024-03-22T06:46:40Z,,,
arxiv2024,MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts,No.,1,"""No evidence""",2024,2024-03-22T06:31:49Z,,,
arxiv2024,Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices,No.,1,"""No evidence""",2024,2024-03-22T05:23:31Z,,,
arxiv2024,Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation,No.,1,"""No evidence""",2024,2024-03-22T05:05:45Z,,,
arxiv2024,Evaluating the Performance of LLMs on Technical Language Processing tasks,No.,1,"""No evidence""",2024,2024-03-21T23:40:42Z,,,
arxiv2024,The opportunities and risks of large language models in mental health,No.,1,"""No evidence""",2024,2024-03-21T19:59:52Z,,,
arxiv2024,VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding,No.,1,"""No evidence""",2024,2024-03-21T18:00:00Z,,,
arxiv2024,MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?,No.,1,"""No evidence""",2024,2024-03-21T17:59:50Z,,,
arxiv2024,Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey,No.,1,"""No evidence""",2024,2024-03-21T17:55:50Z,,,
arxiv2024,A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,No.,1,"""No evidence""",2024,2024-03-21T17:09:08Z,,,
arxiv2024,The Era of Semantic Decoding,No.,1,"""No evidence""",2024,2024-03-21T17:06:17Z,,,
arxiv2024,EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling,No.,1,"""No evidence""",2024,2024-03-21T16:41:12Z,,,
arxiv2024,ChatGPT Alternative Solutions: Large Language Models Survey,No.,1,"""No evidence""",2024,2024-03-21T15:16:50Z,,,
arxiv2024,gTBLS: Generating Tables from Text by Conditional Question Answering,No.,1,"""No evidence""",2024,2024-03-21T15:04:32Z,,,
arxiv2024,Locating and Mitigating Gender Bias in Large Language Models,No.,1,"""No evidence""",2024,2024-03-21T13:57:43Z,,,
arxiv2024,Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning,No.,1,"""No evidence""",2024,2024-03-21T13:47:40Z,,,
arxiv2024,From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision,No.,1,"""No evidence""",2024,2024-03-21T13:29:54Z,,,
arxiv2024,"WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",No.,1,"""No evidence""",2024,2024-03-21T12:45:12Z,,,
arxiv2024,Exploring the Potential of Large Language Models in Graph Generation,No.,1,"""No evidence""",2024,2024-03-21T12:37:54Z,,,
arxiv2024,ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting,No.,1,"""No evidence""",2024,2024-03-21T11:34:26Z,,,
arxiv2024,From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora,No.,1,"""No evidence""",2024,2024-03-21T11:04:41Z,,,
arxiv2024,Multi-role Consensus through LLMs Discussions for Vulnerability Detection,No.,1,"""No evidence""",2024,2024-03-21T10:28:18Z,,,
arxiv2024,ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification,No.,1,"""No evidence""",2024,2024-03-21T09:28:38Z,,,
arxiv2024,Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology,No.,1,"""No evidence""",2024,2024-03-21T09:02:17Z,,,
arxiv2024,PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning,No.,1,"""No evidence""",2024,2024-03-21T08:37:15Z,,,
arxiv2024,Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering,No.,1,"""No evidence""",2024,2024-03-21T07:47:57Z,,,
arxiv2024,MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation,No.,1,"""No evidence""",2024,2024-03-21T06:47:28Z,,,
arxiv2024,Empowering Segmentation Ability to Multi-modal Large Language Models,No.,1,"""No evidence""",2024,2024-03-21T05:36:25Z,,,
arxiv2024,Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors,No.,1,"""No evidence""",2024,2024-03-21T04:23:56Z,,,
arxiv2024,From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation,No.,1,"""No evidence""",2024,2024-03-21T04:07:40Z,,,
arxiv2024,"Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection",No.,1,"""No evidence""",2024,2024-03-21T02:12:03Z,,,
arxiv2024,LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning,No.,1,"""No evidence""",2024,2024-03-20T21:06:42Z,,,
arxiv2024,Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification,No.,1,"""No evidence""",2024,2024-03-20T18:59:18Z,,,
arxiv2024,Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases,No.,1,"""No evidence""",2024,2024-03-20T18:13:17Z,,,
arxiv2024,Bridge the Modality and Capacity Gaps in Vision-Language Model Selection,No.,1,"""No evidence""",2024,2024-03-20T17:54:58Z,,,
arxiv2024,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,No.,1,"""No evidence""",2024,2024-03-20T17:47:49Z,,,
arxiv2024,Large Language Models meet Network Slicing Management and Orchestration,No.,1,"""No evidence""",2024,2024-03-20T16:29:52Z,,,
arxiv2024,RoleInteract: Evaluating the Social Interaction of Role-Playing Agents,No.,1,"""No evidence""",2024,2024-03-20T15:38:36Z,,,
arxiv2024,No more optimization rules: LLM-enabled policy-based multi-modal query optimizer,No.,1,"""No evidence""",2024,2024-03-20T13:44:30Z,,,
arxiv2024,CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing,No.,1,"""No evidence""",2024,2024-03-20T13:33:55Z,,,
arxiv2024,A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation,No.,1,"""No evidence""",2024,2024-03-20T13:14:29Z,,,
arxiv2024,Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach,No.,1,"""No evidence""",2024,2024-03-20T12:33:51Z,,,
arxiv2024,Motion Generation from Fine-grained Textual Descriptions,No.,1,"""No evidence""",2024,2024-03-20T11:38:30Z,,,
arxiv2024,FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs,No.,1,"""No evidence""",2024,2024-03-20T11:05:07Z,,,
arxiv2024,VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis,No.,1,"""No evidence""",2024,2024-03-20T10:58:58Z,,,
arxiv2024,Vi-Mistral-X: Building a Vietnamese Language Model with Advanced Continual Pre-training,No.,1,"""No evidence""",2024,2024-03-20T10:14:13Z,,,
arxiv2024,LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models,No.,1,"""No evidence""",2024,2024-03-20T08:08:54Z,,,
arxiv2024,ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics,No.,1,"""No evidence""",2024,2024-03-20T07:48:32Z,,,
arxiv2024,Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection,No.,1,"""No evidence""",2024,2024-03-20T06:38:13Z,,,
arxiv2024,Polaris: A Safety-focused LLM Constellation Architecture for Healthcare,No.,1,"""No evidence""",2024,2024-03-20T05:34:03Z,,,
arxiv2024,Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal,No.,1,"""No evidence""",2024,2024-03-20T05:17:22Z,,,
arxiv2024,Reading Users' Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference,No.,1,"""No evidence""",2024,2024-03-20T04:57:32Z,,,
arxiv2024,Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models,No.,1,"""No evidence""",2024,2024-03-20T02:29:09Z,,,
arxiv2024,Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model,No.,1,"""No evidence""",2024,2024-03-20T02:15:55Z,,,
arxiv2024,SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization,No.,1,"""No evidence""",2024,2024-03-20T02:04:42Z,,,
arxiv2024,Technical Report: Competition Solution For BetterMixture,No.,1,"""No evidence""",2024,2024-03-20T01:46:06Z,,,
arxiv2024,A Study of Vulnerability Repair in JavaScript Programs with Large Language Models,No.,1,"""No evidence""",2024,2024-03-19T23:04:03Z,,,
arxiv2024,Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning,No.,1,"""No evidence""",2024,2024-03-19T18:37:05Z,,,
arxiv2024,LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction,No.,1,"""No evidence""",2024,2024-03-19T18:10:13Z,,,
arxiv2024,Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models,No.,1,"""No evidence""",2024,2024-03-19T17:59:52Z,,,
arxiv2024,Bypassing LLM Watermarks with Color-Aware Substitutions,No.,1,"""No evidence""",2024,2024-03-19T17:54:39Z,,,
arxiv2024,Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models,No.,1,"""No evidence""",2024,2024-03-19T17:43:08Z,,,
arxiv2024,Supporting Energy Policy Research with Large Language Models,No.,1,"""No evidence""",2024,2024-03-19T17:28:51Z,,,
arxiv2024,Yell At Your Robot: Improving On-the-Fly from Language Corrections,No.,1,"""No evidence""",2024,2024-03-19T17:08:24Z,,,
arxiv2024,Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation,No.,1,"""No evidence""",2024,2024-03-19T15:54:48Z,,,
arxiv2024,MELTing point: Mobile Evaluation of Language Transformers,No.,1,"""No evidence""",2024,2024-03-19T15:51:21Z,,,
arxiv2024,RelationVLM: Making Large Vision-Language Models Understand Visual Relations,No.,1,"""No evidence""",2024,2024-03-19T15:01:19Z,,,
arxiv2024,Investigating Text Shortening Strategy in BERT: Truncation vs Summarization,No.,1,"""No evidence""",2024,2024-03-19T15:01:14Z,,,
arxiv2024,LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models,No.,1,"""No evidence""",2024,2024-03-19T10:11:14Z,,,
arxiv2024,Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs,No.,1,"""No evidence""",2024,2024-03-19T10:03:07Z,,,
arxiv2024,AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework,No.,1,"""No evidence""",2024,2024-03-19T09:45:33Z,,,
arxiv2024,AffineQuant: Affine Transformation Quantization for Large Language Models,No.,1,"""No evidence""",2024,2024-03-19T08:40:21Z,,,
arxiv2024,To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions,No.,1,"""No evidence""",2024,2024-03-19T08:09:44Z,,,
arxiv2024,RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content,No.,1,"""No evidence""",2024,2024-03-19T07:25:02Z,,,
arxiv2024,DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM,No.,1,"""No evidence""",2024,2024-03-19T06:54:33Z,,,
arxiv2024,WoLF: Wide-scope Large Language Model Framework for CXR Understanding,No.,1,"""No evidence""",2024,2024-03-19T06:39:23Z,,,
arxiv2024,CrossTune: Black-Box Few-Shot Classification with Label Enhancement,No.,1,"""No evidence""",2024,2024-03-19T05:52:56Z,,,
arxiv2024,VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation,No.,1,"""No evidence""",2024,2024-03-19T03:55:39Z,,,
arxiv2024,Characteristic AI Agents via Large Language Models,No.,1,"""No evidence""",2024,2024-03-19T02:25:29Z,,,
arxiv2024,Improving LoRA in Privacy-preserving Federated Learning,No.,1,"""No evidence""",2024,2024-03-18T23:20:08Z,,,
arxiv2024,Zero-Shot Multi-task Hallucination Detection,No.,1,"""No evidence""",2024,2024-03-18T20:50:26Z,,,
arxiv2024,Reference-based Metrics Disprove Themselves in Question Generation,No.,1,"""No evidence""",2024,2024-03-18T20:47:10Z,,,
arxiv2024,Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models,No.,1,"""No evidence""",2024,2024-03-18T19:10:12Z,,,
arxiv2024,EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models,No.,1,"""No evidence""",2024,2024-03-18T18:39:53Z,,,
arxiv2024,MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control,No.,1,"""No evidence""",2024,2024-03-18T17:59:42Z,,,
arxiv2024,RouterBench: A Benchmark for Multi-LLM Routing System,No.,1,"""No evidence""",2024,2024-03-18T17:59:04Z,,,
arxiv2024,Supervised Fine-Tuning as Inverse Reinforcement Learning,No.,1,"""No evidence""",2024,2024-03-18T17:52:57Z,,,
arxiv2024,EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents,No.,1,"""No evidence""",2024,2024-03-18T17:51:16Z,,,
arxiv2024,Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching,No.,1,"""No evidence""",2024,2024-03-18T17:21:35Z,,,
arxiv2024,Towards Enabling FAIR Dataspaces Using Large Language Models,No.,1,"""No evidence""",2024,2024-03-18T16:46:00Z,,,
arxiv2024,Investigating Markers and Drivers of Gender Bias in Machine Translations,No.,1,"""No evidence""",2024,2024-03-18T15:54:46Z,,,
arxiv2024,From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?,No.,1,"""No evidence""",2024,2024-03-18T15:53:33Z,,,
arxiv2024,QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction,No.,1,"""No evidence""",2024,2024-03-18T15:39:14Z,,,
arxiv2024,Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models,No.,1,"""No evidence""",2024,2024-03-18T14:48:29Z,,,
arxiv2024,"SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator",No.,1,"""No evidence""",2024,2024-03-18T14:45:20Z,,,
arxiv2024,How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments,No.,1,"""No evidence""",2024,2024-03-18T14:04:47Z,,,
arxiv2024,"Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models",No.,1,"""No evidence""",2024,2024-03-18T14:01:45Z,,,
arxiv2024,Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models,No.,1,"""No evidence""",2024,2024-03-18T13:44:48Z,,,
arxiv2024,Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs,No.,1,"""No evidence""",2024,2024-03-18T13:03:24Z,,,
arxiv2024,Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems,No.,1,"""No evidence""",2024,2024-03-18T13:02:02Z,,,
arxiv2024,LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images,No.,1,"""No evidence""",2024,2024-03-18T12:04:11Z,,,
arxiv2024,Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model,No.,1,"""No evidence""",2024,2024-03-18T09:55:01Z,,,
arxiv2024,Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines,No.,1,"""No evidence""",2024,2024-03-18T08:58:47Z,,,
arxiv2024,DEE: Dual-stage Explainable Evaluation Method for Text Generation,No.,1,"""No evidence""",2024,2024-03-18T06:30:41Z,,,
arxiv2024,VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding,No.,1,"""No evidence""",2024,2024-03-18T05:07:59Z,,,
arxiv2024,Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V,No.,1,"""No evidence""",2024,2024-03-18T04:41:38Z,,,
arxiv2024,HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models,No.,1,"""No evidence""",2024,2024-03-18T04:12:35Z,,,
arxiv2024,StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation,No.,1,"""No evidence""",2024,2024-03-18T03:26:18Z,,,
arxiv2024,InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions,No.,1,"""No evidence""",2024,2024-03-18T03:10:36Z,,,
arxiv2024,A Novel Paradigm Boosting Translation Capabilities of Large Language Models,No.,1,"""No evidence""",2024,2024-03-18T02:53:49Z,,,
arxiv2024,Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning,No.,1,"""No evidence""",2024,2024-03-18T01:18:48Z,,,
arxiv2024,X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment,No.,1,"""No evidence""",2024,2024-03-18T01:14:47Z,,,
arxiv2024,JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning,No.,1,"""No evidence""",2024,2024-03-17T23:02:04Z,,,
arxiv2024,Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback,No.,1,"""No evidence""",2024,2024-03-17T20:21:26Z,,,
arxiv2024,StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows,No.,1,"""No evidence""",2024,2024-03-17T19:54:16Z,,,
arxiv2024,SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant,No.,1,"""No evidence""",2024,2024-03-17T18:42:38Z,,,
arxiv2024,Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs,No.,1,"""No evidence""",2024,2024-03-17T17:01:45Z,,,
arxiv2024,Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework,No.,1,"""No evidence""",2024,2024-03-17T13:01:03Z,,,
arxiv2024,Evaluation Ethics of LLMs in Legal Domain,No.,1,"""No evidence""",2024,2024-03-17T09:05:13Z,,,
arxiv2024,Training A Small Emotional Vision Language Model for Visual Art Comprehension,No.,1,"""No evidence""",2024,2024-03-17T09:01:02Z,,,
arxiv2024,m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks,No.,1,"""No evidence""",2024,2024-03-17T04:36:18Z,,,
arxiv2024,GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment,No.,1,"""No evidence""",2024,2024-03-17T03:52:52Z,,,
arxiv2024,SelfIE: Self-Interpretation of Large Language Model Embeddings,No.,1,"""No evidence""",2024,2024-03-16T15:30:34Z,,,
arxiv2024,Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization,No.,1,"""No evidence""",2024,2024-03-16T11:09:27Z,,,
arxiv2024,Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean,No.,1,"""No evidence""",2024,2024-03-16T10:26:38Z,,,
arxiv2024,A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment,No.,1,"""No evidence""",2024,2024-03-16T08:30:45Z,,,
arxiv2024,Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning,No.,1,"""No evidence""",2024,2024-03-16T07:18:29Z,,,
arxiv2024,From Words to Routes: Applying Large Language Models to Vehicle Routing,No.,1,"""No evidence""",2024,2024-03-16T03:54:38Z,,,
arxiv2024,Depression Detection on Social Media with Large Language Models,No.,1,"""No evidence""",2024,2024-03-16T01:01:16Z,,,
arxiv2024,PERL: Parameter Efficient Reinforcement Learning from Human Feedback,No.,1,"""No evidence""",2024,2024-03-15T21:43:46Z,,,
arxiv2024,Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems,No.,1,"""No evidence""",2024,2024-03-15T18:00:00Z,,,
arxiv2024,Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?,No.,1,"""No evidence""",2024,2024-03-15T17:12:57Z,,,
arxiv2024,Using an LLM to Turn Sign Spottings into Spoken Language Sentences,No.,1,"""No evidence""",2024,2024-03-15T16:14:34Z,,,
arxiv2024,SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores,No.,1,"""No evidence""",2024,2024-03-15T15:43:02Z,,,
arxiv2024,TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale,No.,1,"""No evidence""",2024,2024-03-15T14:36:38Z,,,
arxiv2024,CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model,No.,1,"""No evidence""",2024,2024-03-15T14:14:26Z,,,
arxiv2024,Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction,No.,1,"""No evidence""",2024,2024-03-15T13:25:09Z,,,
arxiv2024,A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption,No.,1,"""No evidence""",2024,2024-03-15T13:15:23Z,,,
arxiv2024,Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models,No.,1,"""No evidence""",2024,2024-03-15T12:47:39Z,,,
arxiv2024,Read between the lines -- Functionality Extraction From READMEs,No.,1,"""No evidence""",2024,2024-03-15T11:11:57Z,,,
arxiv2024,Generative Region-Language Pretraining for Open-Ended Object Detection,No.,1,"""No evidence""",2024,2024-03-15T10:52:39Z,,,
arxiv2024,Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning,No.,1,"""No evidence""",2024,2024-03-15T08:51:15Z,,,
arxiv2024,Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF,No.,1,"""No evidence""",2024,2024-03-15T08:03:49Z,,,
arxiv2024,CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner,No.,1,"""No evidence""",2024,2024-03-15T07:51:35Z,,,
arxiv2024,Are LLMs Good Cryptic Crossword Solvers?,No.,1,"""No evidence""",2024,2024-03-15T06:57:08Z,,,
arxiv2024,Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning,No.,1,"""No evidence""",2024,2024-03-15T06:54:20Z,,,
arxiv2024,TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model,No.,1,"""No evidence""",2024,2024-03-15T06:38:25Z,,,
arxiv2024,Knowledge Condensation and Reasoning for Knowledge-based VQA,No.,1,"""No evidence""",2024,2024-03-15T06:06:06Z,,,
arxiv2024,Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain,No.,1,"""No evidence""",2024,2024-03-15T05:35:02Z,,,
arxiv2024,Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals,No.,1,"""No evidence""",2024,2024-03-15T04:04:45Z,,,
arxiv2024,Whose Side Are You On? Investigating the Political Stance of Large Language Models,No.,1,"""No evidence""",2024,2024-03-15T04:02:24Z,,,
arxiv2024,"Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals",No.,1,"""No evidence""",2024,2024-03-14T22:33:22Z,,,
arxiv2024,Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference,No.,1,"""No evidence""",2024,2024-03-14T17:59:26Z,,,
arxiv2024,3D-VLA: A 3D Vision-Language-Action Generative World Model,No.,1,"""No evidence""",2024,2024-03-14T17:58:41Z,,,
arxiv2024,Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey,No.,1,"""No evidence""",2024,2024-03-14T17:47:20Z,,,
arxiv2024,Less is More: Data Value Estimation for Visual Instruction Tuning,No.,1,"""No evidence""",2024,2024-03-14T16:47:25Z,,,
arxiv2024,VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding,No.,1,"""No evidence""",2024,2024-03-14T16:13:00Z,,,
arxiv2024,MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation,No.,1,"""No evidence""",2024,2024-03-14T16:07:39Z,,,
arxiv2024,AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,No.,1,"""No evidence""",2024,2024-03-14T15:57:13Z,,,
arxiv2024,LLM-based agents for automating the enhancement of user story quality: An early report,No.,1,"""No evidence""",2024,2024-03-14T14:35:53Z,,,
arxiv2024,"""Like a Nesting Doll"": Analyzing Recursion Analogies Generated by CS Students using Large Language Models",No.,1,"""No evidence""",2024,2024-03-14T14:01:26Z,,,
arxiv2024,Komodo: A Linguistic Expedition into Indonesia's Regional Languages,No.,1,"""No evidence""",2024,2024-03-14T13:12:21Z,,,
arxiv2024,AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions,No.,1,"""No evidence""",2024,2024-03-14T12:51:07Z,,,
arxiv2024,Unveiling the Generalization Power of Fine-Tuned Large Language Models,No.,1,"""No evidence""",2024,2024-03-14T08:18:59Z,,,
arxiv2024,ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text,No.,1,"""No evidence""",2024,2024-03-14T06:49:16Z,,,
arxiv2024,Large Language Models are Parallel Multilingual Learners,No.,1,"""No evidence""",2024,2024-03-14T03:33:46Z,,,
arxiv2024,UniCode: Learning a Unified Codebook for Multimodal Large Language Models,No.,1,"""No evidence""",2024,2024-03-14T03:29:58Z,,,
arxiv2024,Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate,No.,1,"""No evidence""",2024,2024-03-14T03:24:14Z,,,
arxiv2024,LAMP: A Language Model on the Map,No.,1,"""No evidence""",2024,2024-03-14T02:56:38Z,,,
arxiv2024,CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences,No.,1,"""No evidence""",2024,2024-03-14T01:51:35Z,,,
arxiv2024,ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning,No.,1,"""No evidence""",2024,2024-03-14T01:40:23Z,,,
arxiv2024,VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework,No.,1,"""No evidence""",2024,2024-03-14T01:39:40Z,,,
arxiv2024,Evaluating the Application of Large Language Models to Generate Feedback in Programming Education,No.,1,"""No evidence""",2024,2024-03-13T23:14:35Z,,,
arxiv2024,Exploring Prompt Engineering Practices in the Enterprise,No.,1,"""No evidence""",2024,2024-03-13T20:32:32Z,,,
arxiv2024,LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots,No.,1,"""No evidence""",2024,2024-03-13T20:19:30Z,,,
arxiv2024,Bugs in Large Language Models Generated Code: An Empirical Study,No.,1,"""No evidence""",2024,2024-03-13T20:12:01Z,,,
arxiv2024,Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models,No.,1,"""No evidence""",2024,2024-03-13T18:57:30Z,,,
arxiv2024,Teaching Machines to Code: Smart Contract Translation with LLMs,No.,1,"""No evidence""",2024,2024-03-13T18:55:20Z,,,
arxiv2024,Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation,No.,1,"""No evidence""",2024,2024-03-13T18:16:21Z,,,
arxiv2024,Cultural evolution in populations of Large Language Models,No.,1,"""No evidence""",2024,2024-03-13T18:11:17Z,,,
arxiv2024,DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation,No.,1,"""No evidence""",2024,2024-03-13T18:00:01Z,,,
arxiv2024,Simple and Scalable Strategies to Continually Pre-train Large Language Models,No.,1,"""No evidence""",2024,2024-03-13T17:58:57Z,,,
arxiv2024,Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,No.,1,"""No evidence""",2024,2024-03-13T17:29:45Z,,,
arxiv2024,SOTOPIA-$?$: Interactive Learning of Socially Intelligent Language Agents,No.,1,"""No evidence""",2024,2024-03-13T17:17:48Z,,,
arxiv2024,TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning,No.,1,"""No evidence""",2024,2024-03-13T16:57:57Z,,,
arxiv2024,Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments,No.,1,"""No evidence""",2024,2024-03-13T14:59:07Z,,,
arxiv2024,Non-discrimination Criteria for Generative Language Models,No.,1,"""No evidence""",2024,2024-03-13T14:19:08Z,,,
arxiv2024,Language models scale reliably with over-training and on downstream tasks,No.,1,"""No evidence""",2024,2024-03-13T13:54:00Z,,,
arxiv2024,Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator,No.,1,"""No evidence""",2024,2024-03-13T13:04:58Z,,,
arxiv2024,Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning,No.,1,"""No evidence""",2024,2024-03-13T12:50:23Z,,,
arxiv2024,SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks,No.,1,"""No evidence""",2024,2024-03-13T12:46:51Z,,,
arxiv2024,Software Vulnerability and Functionality Assessment using LLMs,No.,1,"""No evidence""",2024,2024-03-13T11:29:13Z,,,
arxiv2024,Tastle: Distract Large Language Models for Automatic Jailbreak Attack,No.,1,"""No evidence""",2024,2024-03-13T11:16:43Z,,,
arxiv2024,SMART: Submodular Data Mixture Strategy for Instruction Tuning,No.,1,"""No evidence""",2024,2024-03-13T09:31:50Z,,,
arxiv2024,OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models,No.,1,"""No evidence""",2024,2024-03-13T07:52:31Z,,,
arxiv2024,Is Context Helpful for Chat Translation Evaluation?,No.,1,"""No evidence""",2024,2024-03-13T07:49:50Z,,,
arxiv2024,StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses,No.,1,"""No evidence""",2024,2024-03-13T07:44:14Z,,,
arxiv2024,Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale,No.,1,"""No evidence""",2024,2024-03-13T06:54:47Z,,,
arxiv2024,Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation,No.,1,"""No evidence""",2024,2024-03-13T06:22:17Z,,,
arxiv2024,RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education,No.,1,"""No evidence""",2024,2024-03-13T05:51:57Z,,,
arxiv2024,Learning to Watermark LLM-generated Text via Reinforcement Learning,No.,1,"""No evidence""",2024,2024-03-13T03:43:39Z,,,
arxiv2024,Can Large Language Models Identify Authorship?,No.,1,"""No evidence""",2024,2024-03-13T03:22:02Z,,,
arxiv2024,Large Language Models are Contrastive Reasoners,No.,1,"""No evidence""",2024,2024-03-13T03:15:05Z,,,
arxiv2024,AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models,No.,1,"""No evidence""",2024,2024-03-13T02:53:36Z,,,
arxiv2024,TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection,No.,1,"""No evidence""",2024,2024-03-12T22:33:02Z,,,
arxiv2024,Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems,No.,1,"""No evidence""",2024,2024-03-12T21:06:31Z,,,
arxiv2024,CHAI: Clustered Head Attention for Efficient LLM Inference,No.,1,"""No evidence""",2024,2024-03-12T20:10:04Z,,,
arxiv2024,Generating Clarification Questions for Disambiguating Contracts,No.,1,"""No evidence""",2024,2024-03-12T19:57:39Z,,,
arxiv2024,LG-Traj: LLM Guided Pedestrian Trajectory Prediction,No.,1,"""No evidence""",2024,2024-03-12T19:06:23Z,,,
arxiv2024,Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging,No.,1,"""No evidence""",2024,2024-03-12T18:12:02Z,,,
arxiv2024,Beyond Text: Frozen Large Language Models in Visual Signal Comprehension,No.,1,"""No evidence""",2024,2024-03-12T17:59:51Z,,,
arxiv2024,Rethinking Generative Large Language Model Evaluation for Semantic Comprehension,No.,1,"""No evidence""",2024,2024-03-12T17:59:48Z,,,
arxiv2024,LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code,No.,1,"""No evidence""",2024,2024-03-12T17:58:04Z,,,
arxiv2024,Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings,No.,1,"""No evidence""",2024,2024-03-12T15:36:42Z,,,
arxiv2024,FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models,No.,1,"""No evidence""",2024,2024-03-12T15:32:39Z,,,
arxiv2024,WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?,No.,1,"""No evidence""",2024,2024-03-12T14:58:45Z,,,
arxiv2024,StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models,No.,1,"""No evidence""",2024,2024-03-12T14:57:40Z,,,
arxiv2024,KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,No.,1,"""No evidence""",2024,2024-03-12T14:56:34Z,,,
arxiv2024,Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards,No.,1,"""No evidence""",2024,2024-03-12T14:51:57Z,,,
arxiv2024,Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Pre-training Framework,No.,1,"""No evidence""",2024,2024-03-12T13:18:22Z,,,
arxiv2024,generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation,No.,1,"""No evidence""",2024,2024-03-12T13:09:15Z,,,
arxiv2024,Couler: Unified Machine Learning Workflow Optimization in Cloud,No.,1,"""No evidence""",2024,2024-03-12T12:47:32Z,,,
arxiv2024,LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model,No.,1,"""No evidence""",2024,2024-03-12T12:10:18Z,,,
arxiv2024,MoAI: Mixture of All Intelligence for Large Language and Vision Models,No.,1,"""No evidence""",2024,2024-03-12T10:44:13Z,,,
arxiv2024,SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models,No.,1,"""No evidence""",2024,2024-03-12T07:45:33Z,,,
arxiv2024,Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking,No.,1,"""No evidence""",2024,2024-03-12T07:17:01Z,,,
arxiv2024,A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism,No.,1,"""No evidence""",2024,2024-03-12T03:30:04Z,,,
arxiv2024,CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation,No.,1,"""No evidence""",2024,2024-03-12T02:37:11Z,,,
arxiv2024,AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production,No.,1,"""No evidence""",2024,2024-03-12T02:30:50Z,,,
arxiv2024,Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits,No.,1,"""No evidence""",2024,2024-03-11T23:52:46Z,,,
arxiv2024,Narrating Causal Graphs with Large Language Models,No.,1,"""No evidence""",2024,2024-03-11T19:19:59Z,,,
arxiv2024,MRL Parsing Without Tears: The Case of Hebrew,No.,1,"""No evidence""",2024,2024-03-11T17:54:33Z,,,
arxiv2024,SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data,No.,1,"""No evidence""",2024,2024-03-11T17:35:33Z,,,
arxiv2024,ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis,No.,1,"""No evidence""",2024,2024-03-11T17:18:53Z,,,
arxiv2024,MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning,No.,1,"""No evidence""",2024,2024-03-11T17:03:04Z,,,
arxiv2024,Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents,No.,1,"""No evidence""",2024,2024-03-11T16:24:08Z,,,
arxiv2024,Development of a Reliable and Accessible Caregiving Language Model (CaLM),No.,1,"""No evidence""",2024,2024-03-11T16:12:34Z,,,
arxiv2024,DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation,No.,1,"""No evidence""",2024,2024-03-11T16:03:35Z,,,
arxiv2024,The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework,No.,1,"""No evidence""",2024,2024-03-11T15:48:43Z,,,
arxiv2024,ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation,No.,1,"""No evidence""",2024,2024-03-11T14:10:57Z,,,
arxiv2024,Real-Time Multimodal Cognitive Assistant for Emergency Medical Services,No.,1,"""No evidence""",2024,2024-03-11T13:56:57Z,,,
arxiv2024,Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning,No.,1,"""No evidence""",2024,2024-03-11T13:47:11Z,,,
arxiv2024,Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code,No.,1,"""No evidence""",2024,2024-03-11T12:47:04Z,,,
arxiv2024,Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System,No.,1,"""No evidence""",2024,2024-03-11T12:32:14Z,,,
arxiv2024,FashionReGen: LLM-Empowered Fashion Report Generation,No.,1,"""No evidence""",2024,2024-03-11T12:29:35Z,,,
arxiv2024,Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement,No.,1,"""No evidence""",2024,2024-03-11T12:28:55Z,,,
arxiv2024,KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation,No.,1,"""No evidence""",2024,2024-03-11T12:04:20Z,,,
arxiv2024,Academically intelligent LLMs are not necessarily socially intelligent,No.,1,"""No evidence""",2024,2024-03-11T10:35:53Z,,,
arxiv2024,AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models,No.,1,"""No evidence""",2024,2024-03-11T10:24:37Z,,,
arxiv2024,Unraveling the Mystery of Scaling Laws: Part I,No.,1,"""No evidence""",2024,2024-03-11T10:05:29Z,,,
arxiv2024,QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning,No.,1,"""No evidence""",2024,2024-03-11T08:09:30Z,,,
arxiv2024,RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models,No.,1,"""No evidence""",2024,2024-03-11T04:13:26Z,,,
arxiv2024,Evolving Knowledge Distillation with Large Language Models and Active Learning,No.,1,"""No evidence""",2024,2024-03-11T03:55:24Z,,,
arxiv2024,What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation,No.,1,"""No evidence""",2024,2024-03-11T03:42:51Z,,,
arxiv2024,A Knowledge-Injected Curriculum Pretraining Framework for Question Answering,No.,1,"""No evidence""",2024,2024-03-11T03:42:03Z,,,
arxiv2024,Can LLMs' Tuning Methods Work in Medical Multimodal Domain?,No.,1,"""No evidence""",2024,2024-03-11T03:38:48Z,,,
arxiv2024,DivCon: Divide and Conquer for Progressive Text-to-Image Generation,No.,1,"""No evidence""",2024,2024-03-11T03:24:44Z,,,
arxiv2024,From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification,No.,1,"""No evidence""",2024,2024-03-10T22:14:54Z,,,
arxiv2024,LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition,No.,1,"""No evidence""",2024,2024-03-10T20:20:16Z,,,
arxiv2024,TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision,No.,1,"""No evidence""",2024,2024-03-10T13:58:38Z,,,
arxiv2024,Personalized LoRA for Human-Centered Text Understanding,No.,1,"""No evidence""",2024,2024-03-10T13:04:54Z,,,
arxiv2024,Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models,No.,1,"""No evidence""",2024,2024-03-10T12:43:27Z,,,
arxiv2024,Can Large Language Models Automatically Score Proficiency of Written Essays?,No.,1,"""No evidence""",2024,2024-03-10T09:39:00Z,,,
arxiv2024,FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning,No.,1,"""No evidence""",2024,2024-03-10T08:41:22Z,,,
arxiv2024,Low-dose CT Denoising with Language-engaged Dual-space Alignment,No.,1,"""No evidence""",2024,2024-03-10T08:21:50Z,,,
arxiv2024,FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language,No.,1,"""No evidence""",2024,2024-03-10T07:21:31Z,,,
arxiv2024,Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning,No.,1,"""No evidence""",2024,2024-03-10T06:30:54Z,,,
arxiv2024,Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery,No.,1,"""No evidence""",2024,2024-03-10T05:12:16Z,,,
arxiv2024,RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion,No.,1,"""No evidence""",2024,2024-03-10T05:10:34Z,,,
arxiv2024,A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation,No.,1,"""No evidence""",2024,2024-03-09T23:22:56Z,,,
arxiv2024,Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages,No.,1,"""No evidence""",2024,2024-03-09T21:36:13Z,,,
arxiv2024,Thread Detection and Response Generation using Transformers with Prompt Optimisation,No.,1,"""No evidence""",2024,2024-03-09T14:50:20Z,,,
arxiv2024,GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing,No.,1,"""No evidence""",2024,2024-03-09T13:56:25Z,,,
arxiv2024,Optimizing LLM Queries in Relational Workloads,No.,1,"""No evidence""",2024,2024-03-09T07:01:44Z,,,
arxiv2024,$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting,No.,1,"""No evidence""",2024,2024-03-09T05:20:48Z,,,
arxiv2024,ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes,No.,1,"""No evidence""",2024,2024-03-09T04:58:25Z,,,
arxiv2024,A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health,No.,1,"""No evidence""",2024,2024-03-08T23:46:37Z,,,
arxiv2024,A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries,No.,1,"""No evidence""",2024,2024-03-08T23:17:55Z,,,
arxiv2024,Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?,No.,1,"""No evidence""",2024,2024-03-08T22:23:23Z,,,
arxiv2024,Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations,No.,1,"""No evidence""",2024,2024-03-08T21:26:49Z,,,
arxiv2024,Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4,No.,1,"""No evidence""",2024,2024-03-08T21:16:28Z,,,
arxiv2024,PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design,No.,1,"""No evidence""",2024,2024-03-08T21:09:20Z,,,
arxiv2024,GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM,No.,1,"""No evidence""",2024,2024-03-08T18:48:30Z,,,
arxiv2024,Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs,No.,1,"""No evidence""",2024,2024-03-08T16:37:36Z,,,
arxiv2024,VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model,No.,1,"""No evidence""",2024,2024-03-08T14:23:00Z,,,
arxiv2024,ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues,No.,1,"""No evidence""",2024,2024-03-08T14:05:36Z,,,
arxiv2024,Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents,No.,1,"""No evidence""",2024,2024-03-08T13:34:20Z,,,
arxiv2024,ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models,No.,1,"""No evidence""",2024,2024-03-08T12:42:36Z,,,
arxiv2024,Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering,No.,1,"""No evidence""",2024,2024-03-08T11:09:13Z,,,
arxiv2024,"Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge",No.,1,"""No evidence""",2024,2024-03-08T10:09:57Z,,,
arxiv2024,Towards a Psychology of Machines: Large Language Models Predict Human Memory,No.,1,"""No evidence""",2024,2024-03-08T08:41:14Z,,,
arxiv2024,Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem,No.,1,"""No evidence""",2024,2024-03-08T08:38:50Z,,,
arxiv2024,Med3DInsight: Enhancing 3D Medical Image Understanding with 2D Multi-Modal Large Language Models,No.,1,"""No evidence""",2024,2024-03-08T08:15:53Z,,,
arxiv2024,ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment,No.,1,"""No evidence""",2024,2024-03-08T08:08:10Z,,,
arxiv2024,Are Human Conversations Special? A Large Language Model Perspective,No.,1,"""No evidence""",2024,2024-03-08T04:44:25Z,,,
arxiv2024,Can't Remember Details in Long Documents? You Need Some R&R,No.,1,"""No evidence""",2024,2024-03-08T03:03:20Z,,,
arxiv2024,DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation,No.,1,"""No evidence""",2024,2024-03-08T02:24:27Z,,,
arxiv2024,SecGPT: An Execution Isolation Architecture for LLM-Based Systems,No.,1,"""No evidence""",2024,2024-03-08T00:02:30Z,,,
arxiv2024,Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering,No.,1,"""No evidence""",2024,2024-03-07T20:48:40Z,,,
arxiv2024,Evaluating Biases in Context-Dependent Health Questions,No.,1,"""No evidence""",2024,2024-03-07T19:15:40Z,,,
arxiv2024,SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM,No.,1,"""No evidence""",2024,2024-03-07T18:38:17Z,,,
arxiv2024,ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes,No.,1,"""No evidence""",2024,2024-03-07T17:48:48Z,,,
arxiv2024,QAQ: Quality Adaptive Quantization for LLM KV Cache,No.,1,"""No evidence""",2024,2024-03-07T16:42:37Z,,,
arxiv2024,Teaching Large Language Models to Reason with Reinforcement Learning,No.,1,"""No evidence""",2024,2024-03-07T16:36:29Z,,,
arxiv2024,Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition,No.,1,"""No evidence""",2024,2024-03-07T15:22:07Z,,,
arxiv2024,GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability,No.,1,"""No evidence""",2024,2024-03-07T13:36:08Z,,,
arxiv2024,Do Large Language Model Understand Multi-Intent Spoken Language ?,No.,1,"""No evidence""",2024,2024-03-07T13:30:52Z,,,
arxiv2024,Low-Resource Court Judgment Summarization for Common Law Systems,No.,1,"""No evidence""",2024,2024-03-07T12:47:42Z,,,
arxiv2024,Membership Inference Attacks and Privacy in Topic Modeling,No.,1,"""No evidence""",2024,2024-03-07T12:43:42Z,,,
arxiv2024,Acceleron: A Tool to Accelerate Research Ideation,No.,1,"""No evidence""",2024,2024-03-07T10:20:06Z,,,
arxiv2024,Discriminative Probing and Tuning for Text-to-Image Generation,No.,1,"""No evidence""",2024,2024-03-07T08:37:33Z,,,
arxiv2024,Online Adaptation of Language Models with a Memory of Amortized Contexts,No.,1,"""No evidence""",2024,2024-03-07T08:34:57Z,,,
arxiv2024,Effectiveness Assessment of Recent Large Vision-Language Models,No.,1,"""No evidence""",2024,2024-03-07T08:25:27Z,,,
arxiv2024,Advancing Biomedical Text Mining with Community Challenges,No.,1,"""No evidence""",2024,2024-03-07T06:52:51Z,,,
arxiv2024,Federated Recommendation via Hybrid Retrieval Augmented Generation,No.,1,"""No evidence""",2024,2024-03-07T06:38:41Z,,,
arxiv2024,Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks,No.,1,"""No evidence""",2024,2024-03-07T05:05:56Z,,,
arxiv2024,Aligners: Decoupling LLMs and Alignment,No.,1,"""No evidence""",2024,2024-03-07T04:54:56Z,,,
arxiv2024,Large Language Models are In-Context Molecule Learners,No.,1,"""No evidence""",2024,2024-03-07T03:58:28Z,,,
arxiv2024,Metric-aware LLM inference for regression and scoring,No.,1,"""No evidence""",2024,2024-03-07T03:24:34Z,,,
arxiv2024,Can Large Language Models Reason and Plan?,No.,1,"""No evidence""",2024,2024-03-07T00:36:32Z,,,
arxiv2024,Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models,No.,1,"""No evidence""",2024,2024-03-06T21:45:35Z,,,
arxiv2024,Can Large Language Models do Analytical Reasoning?,No.,1,"""No evidence""",2024,2024-03-06T20:22:08Z,,,
arxiv2024,Guiding Enumerative Program Synthesis with Large Language Models,No.,1,"""No evidence""",2024,2024-03-06T19:13:53Z,,,
arxiv2024,FaaF: Facts as a Function for the evaluation of generated text,No.,1,"""No evidence""",2024,2024-03-06T17:48:06Z,,,
arxiv2024,SaulLM-7B: A pioneering Large Language Model for Law,No.,1,"""No evidence""",2024,2024-03-06T17:42:16Z,,,
arxiv2024,The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported,No.,1,"""No evidence""",2024,2024-03-06T16:39:50Z,,,
arxiv2024,Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ,No.,1,"""No evidence""",2024,2024-03-06T16:01:44Z,,,
arxiv2024,Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery,No.,1,"""No evidence""",2024,2024-03-06T15:35:53Z,,,
arxiv2024,Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese,No.,1,"""No evidence""",2024,2024-03-06T13:17:07Z,,,
arxiv2024,General2Specialized LLMs Translation for E-commerce,No.,1,"""No evidence""",2024,2024-03-06T13:15:21Z,,,
arxiv2024,Multimodal Large Language Models to Support Real-World Fact-Checking,No.,1,"""No evidence""",2024,2024-03-06T11:32:41Z,,,
arxiv2024,WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off,No.,1,"""No evidence""",2024,2024-03-06T10:55:30Z,,,
arxiv2024,Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem,No.,1,"""No evidence""",2024,2024-03-06T09:06:34Z,,,
arxiv2024,CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models,No.,1,"""No evidence""",2024,2024-03-06T07:43:43Z,,,
arxiv2024,A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation,No.,1,"""No evidence""",2024,2024-03-06T06:54:02Z,,,
arxiv2024,TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs,No.,1,"""No evidence""",2024,2024-03-06T04:49:18Z,,,
arxiv2024,Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization,No.,1,"""No evidence""",2024,2024-03-06T03:02:38Z,,,
arxiv2024,Human vs. Machine: Language Models and Wargames,No.,1,"""No evidence""",2024,2024-03-06T02:23:32Z,,,
arxiv2024,Japanese-English Sentence Translation Exercises Dataset for Automatic Grading,No.,1,"""No evidence""",2024,2024-03-06T01:37:03Z,,,
arxiv2024,Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation,No.,1,"""No evidence""",2024,2024-03-05T22:12:01Z,,,
arxiv2024,Guardrail Baselines for Unlearning in LLMs,No.,1,"""No evidence""",2024,2024-03-05T21:19:06Z,,,
arxiv2024,Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data,No.,1,"""No evidence""",2024,2024-03-05T20:07:42Z,,,
arxiv2024,MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets,No.,1,"""No evidence""",2024,2024-03-05T18:31:28Z,,,
arxiv2024,Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement,No.,1,"""No evidence""",2024,2024-03-05T18:24:52Z,,,
arxiv2024,SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection,No.,1,"""No evidence""",2024,2024-03-05T18:04:59Z,,,
arxiv2024,Word Importance Explains How Prompts Affect Language Model Outputs,No.,1,"""No evidence""",2024,2024-03-05T15:04:18Z,,,
arxiv2024,OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,No.,1,"""No evidence""",2024,2024-03-05T14:53:53Z,,,
arxiv2024,Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations,No.,1,"""No evidence""",2024,2024-03-05T14:41:12Z,,,
arxiv2024,Localized Zeroth-Order Prompt Optimization,No.,1,"""No evidence""",2024,2024-03-05T14:18:15Z,,,
arxiv2024,Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering,No.,1,"""No evidence""",2024,2024-03-05T13:43:58Z,,,
arxiv2024,Multi-Scale Protein Language Model for Unified Molecular Modeling,No.,1,"""No evidence""",2024,2024-03-05T13:35:41Z,,,
arxiv2024,Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation,No.,1,"""No evidence""",2024,2024-03-05T13:23:48Z,,,
arxiv2024,ImgTrojan: Jailbreaking Vision-Language Models with ONE Image,No.,1,"""No evidence""",2024,2024-03-05T12:21:57Z,,,
arxiv2024,MathScale: Scaling Instruction Tuning for Mathematical Reasoning,No.,1,"""No evidence""",2024,2024-03-05T11:42:59Z,,,
arxiv2024,DPPA: Pruning Method for Large Language Model to Model Merging,No.,1,"""No evidence""",2024,2024-03-05T09:12:49Z,,,
arxiv2024,Evaluating and Optimizing Educational Content with Large Language Model Judgments,No.,1,"""No evidence""",2024,2024-03-05T09:09:15Z,,,
arxiv2024,Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations,No.,1,"""No evidence""",2024,2024-03-05T08:31:00Z,,,
arxiv2024,"Towards Measuring and Modeling ""Culture"" in LLMs: A Survey",No.,1,"""No evidence""",2024,2024-03-05T08:29:36Z,,,
arxiv2024,Towards Training A Chinese Large Language Model for Anesthesiology,No.,1,"""No evidence""",2024,2024-03-05T07:53:49Z,,,
arxiv2024,Android in the Zoo: Chain-of-Action-Thought for GUI Agents,No.,1,"""No evidence""",2024,2024-03-05T07:09:35Z,,,
arxiv2024,Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment,No.,1,"""No evidence""",2024,2024-03-05T06:28:02Z,,,
arxiv2024,Revisiting Meta-evaluation for Grammatical Error Correction,No.,1,"""No evidence""",2024,2024-03-05T05:53:09Z,,,
arxiv2024,Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use,No.,1,"""No evidence""",2024,2024-03-05T03:34:11Z,,,
arxiv2024,Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research,No.,1,"""No evidence""",2024,2024-03-05T00:27:43Z,,,
arxiv2024,DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation,No.,1,"""No evidence""",2024,2024-03-04T22:47:58Z,,,
arxiv2024,"Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",No.,1,"""No evidence""",2024,2024-03-04T22:02:12Z,,,
arxiv2024,Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents,No.,1,"""No evidence""",2024,2024-03-04T21:50:29Z,,,
arxiv2024,Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems,No.,1,"""No evidence""",2024,2024-03-04T19:12:48Z,,,
arxiv2024,RegionGPT: Towards Region Understanding Vision Language Model,No.,1,"""No evidence""",2024,2024-03-04T18:58:08Z,,,
arxiv2024,Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve,No.,1,"""No evidence""",2024,2024-03-04T18:47:08Z,,,
arxiv2024,Non-autoregressive Sequence-to-Sequence Vision-Language Models,No.,1,"""No evidence""",2024,2024-03-04T17:34:59Z,,,
arxiv2024,Birbal: An efficient 7B instruct-model fine-tuned with curated datasets,No.,1,"""No evidence""",2024,2024-03-04T17:34:46Z,,,
arxiv2024,PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models,No.,1,"""No evidence""",2024,2024-03-04T17:34:34Z,,,
arxiv2024,Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks,No.,1,"""No evidence""",2024,2024-03-04T17:29:57Z,,,
arxiv2024,TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models,No.,1,"""No evidence""",2024,2024-03-04T17:08:57Z,,,
arxiv2024,Not all Layers of LLMs are Necessary during Inference,No.,1,"""No evidence""",2024,2024-03-04T16:23:58Z,,,
arxiv2024,Using LLMs for the Extraction and Normalization of Product Attribute Values,No.,1,"""No evidence""",2024,2024-03-04T15:39:59Z,,,
arxiv2024,Large language models surpass human experts in predicting neuroscience results,No.,1,"""No evidence""",2024,2024-03-04T15:27:59Z,,,
arxiv2024,adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds,No.,1,"""No evidence""",2024,2024-03-04T14:49:18Z,,,
arxiv2024,Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5,No.,1,"""No evidence""",2024,2024-03-04T14:24:47Z,,,
arxiv2024,Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?,No.,1,"""No evidence""",2024,2024-03-04T14:01:11Z,,,
arxiv2024,Unveiling Hidden Links Between Unseen Security Entities,No.,1,"""No evidence""",2024,2024-03-04T13:14:39Z,,,
arxiv2024,Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?,No.,1,"""No evidence""",2024,2024-03-04T13:10:08Z,,,
arxiv2024,Vanilla Transformers are Transfer Capability Teachers,No.,1,"""No evidence""",2024,2024-03-04T12:40:28Z,,,
arxiv2024,Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models,No.,1,"""No evidence""",2024,2024-03-04T12:16:15Z,,,
arxiv2024,Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?,No.,1,"""No evidence""",2024,2024-03-04T10:48:13Z,,,
arxiv2024,To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering,No.,1,"""No evidence""",2024,2024-03-04T10:41:52Z,,,
arxiv2024,LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset,No.,1,"""No evidence""",2024,2024-03-04T10:13:30Z,,,
arxiv2024,Predicting Learning Performance with Large Language Models: A Study in Adult Literacy,No.,1,"""No evidence""",2024,2024-03-04T08:14:07Z,,,
arxiv2024,CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text,No.,1,"""No evidence""",2024,2024-03-04T07:26:07Z,,,
arxiv2024,NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models,No.,1,"""No evidence""",2024,2024-03-04T07:10:31Z,,,
arxiv2024,Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models,No.,1,"""No evidence""",2024,2024-03-04T06:20:31Z,,,
arxiv2024,Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study,No.,1,"""No evidence""",2024,2024-03-04T03:56:14Z,,,
arxiv2024,Improving LLM Code Generation with Grammar Augmentation,No.,1,"""No evidence""",2024,2024-03-03T22:38:35Z,,,
arxiv2024,Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models,No.,1,"""No evidence""",2024,2024-03-03T21:24:35Z,,,
arxiv2024,SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos,No.,1,"""No evidence""",2024,2024-03-03T19:53:06Z,,,
arxiv2024,ReMatch: Retrieval Enhanced Schema Matching with LLMs,No.,1,"""No evidence""",2024,2024-03-03T17:14:40Z,,,
arxiv2024,Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics,No.,1,"""No evidence""",2024,2024-03-03T13:14:47Z,,,
arxiv2024,Infusing Knowledge into Large Language Models with Contextual Prompts,No.,1,"""No evidence""",2024,2024-03-03T11:19:26Z,,,
arxiv2024,Logic Rules as Explanations for Legal Case Retrieval,No.,1,"""No evidence""",2024,2024-03-03T09:22:21Z,,,
arxiv2024,GuardT2I: Defending Text-to-Image Models from Adversarial Prompts,No.,1,"""No evidence""",2024,2024-03-03T09:04:34Z,,,
arxiv2024,Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge,No.,1,"""No evidence""",2024,2024-03-03T08:07:55Z,,,
arxiv2024,MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies,No.,1,"""No evidence""",2024,2024-03-03T07:43:39Z,,,
arxiv2024,The Implicit Bias of Heterogeneity towards Invariance and Causality,No.,1,"""No evidence""",2024,2024-03-03T07:38:24Z,,,
arxiv2024,Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering,No.,1,"""No evidence""",2024,2024-03-03T04:22:13Z,,,
arxiv2024,Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data,No.,1,"""No evidence""",2024,2024-03-02T22:33:17Z,,,
arxiv2024,A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition,No.,1,"""No evidence""",2024,2024-03-02T21:15:24Z,,,
arxiv2024,NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention,No.,1,"""No evidence""",2024,2024-03-02T17:29:22Z,,,
arxiv2024,Accelerating Greedy Coordinate Gradient via Probe Sampling,No.,1,"""No evidence""",2024,2024-03-02T16:23:44Z,,,
arxiv2024,IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact,No.,1,"""No evidence""",2024,2024-03-02T16:05:26Z,,,
arxiv2024,Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning,No.,1,"""No evidence""",2024,2024-03-02T13:43:32Z,,,
arxiv2024,Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding,No.,1,"""No evidence""",2024,2024-03-02T11:54:55Z,,,
arxiv2024,STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models,No.,1,"""No evidence""",2024,2024-03-02T10:38:10Z,,,
arxiv2024,MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining,No.,1,"""No evidence""",2024,2024-03-02T09:27:32Z,,,
arxiv2024,LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization,No.,1,"""No evidence""",2024,2024-03-02T08:40:07Z,,,
arxiv2024,OpenGraph: Towards Open Graph Foundation Models,No.,1,"""No evidence""",2024,2024-03-02T08:05:03Z,,,
arxiv2024,Towards Accurate Lip-to-Speech Synthesis in-the-Wild,No.,1,"""No evidence""",2024,2024-03-02T04:07:24Z,,,
arxiv2024,LAB: Large-Scale Alignment for ChatBots,No.,1,"""No evidence""",2024,2024-03-02T03:48:37Z,,,
arxiv2024,LLMCRIT: Teaching Large Language Models to Use Criteria,No.,1,"""No evidence""",2024,2024-03-02T02:25:55Z,,,
arxiv2024,FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis,No.,1,"""No evidence""",2024,2024-03-02T02:00:51Z,,,
arxiv2024,Towards Full Authorship with AI: Supporting Revision with AI-Generated Views,No.,1,"""No evidence""",2024,2024-03-02T01:11:35Z,,,
arxiv2024,AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks,No.,1,"""No evidence""",2024,2024-03-02T00:10:45Z,,,
arxiv2024,Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks,No.,1,"""No evidence""",2024,2024-03-01T23:38:02Z,,,
arxiv2024,Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries,No.,1,"""No evidence""",2024,2024-03-01T21:59:03Z,,,
arxiv2024,ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys,No.,1,"""No evidence""",2024,2024-03-01T19:24:37Z,,,
arxiv2024,An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce,No.,1,"""No evidence""",2024,2024-03-01T19:08:25Z,,,
arxiv2024,Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction,No.,1,"""No evidence""",2024,2024-03-01T13:36:04Z,,,
arxiv2024,"ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models",No.,1,"""No evidence""",2024,2024-03-01T13:15:30Z,,,
arxiv2024,Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition,No.,1,"""No evidence""",2024,2024-03-01T12:42:47Z,,,
arxiv2024,LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues,No.,1,"""No evidence""",2024,2024-03-01T11:33:53Z,,,
arxiv2024,Hierarchical Indexing for Retrieval-Augmented Opinion Summarization,No.,1,"""No evidence""",2024,2024-03-01T10:38:07Z,,,
arxiv2024,LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness,No.,1,"""No evidence""",2024,2024-03-01T10:10:34Z,,,
arxiv2024,Invariant Test-Time Adaptation for Vision-Language Model Generalization,No.,1,"""No evidence""",2024,2024-03-01T09:01:53Z,,,
arxiv2024,Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models,No.,1,"""No evidence""",2024,2024-03-01T08:43:43Z,,,
arxiv2024,Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models,No.,1,"""No evidence""",2024,2024-03-01T08:05:44Z,,,
arxiv2024,Teach LLMs to Phish: Stealing Private Information from Language Models,No.,1,"""No evidence""",2024,2024-03-01T06:15:07Z,,,
arxiv2024,DPP-Based Adversarial Prompt Searching for Lanugage Models,No.,1,"""No evidence""",2024,2024-03-01T05:28:06Z,,,
arxiv2024,Gender Bias in Large Language Models across Multiple Languages,No.,1,"""No evidence""",2024,2024-03-01T04:47:16Z,,,
arxiv2024,SoftTiger: A Clinical Foundation Model for Healthcare Workflows,No.,1,"""No evidence""",2024,2024-03-01T04:39:16Z,,,
arxiv2024,Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,No.,1,"""No evidence""",2024,2024-03-01T02:21:30Z,,,
arxiv2024,LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction,No.,1,"""No evidence""",2024,2024-02-29T23:03:19Z,,,
arxiv2024,UniTS: Building a Unified Time Series Model,No.,1,"""No evidence""",2024,2024-02-29T21:25:58Z,,,
arxiv2024,FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition,No.,1,"""No evidence""",2024,2024-02-29T21:05:37Z,,,
arxiv2024,NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications,No.,1,"""No evidence""",2024,2024-02-29T21:05:14Z,,,
arxiv2024,LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario,No.,1,"""No evidence""",2024,2024-02-29T20:25:16Z,,,
arxiv2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,No.,1,"""No evidence""",2024,2024-02-29T19:02:03Z,,,
arxiv2024,The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?,No.,1,"""No evidence""",2024,2024-02-29T18:59:25Z,,,
arxiv2024,The All-Seeing Project V2: Towards General Relation Comprehension of the Open World,No.,1,"""No evidence""",2024,2024-02-29T18:59:17Z,,,
arxiv2024,Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models,No.,1,"""No evidence""",2024,2024-02-29T18:55:06Z,,,
arxiv2024,ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL,No.,1,"""No evidence""",2024,2024-02-29T18:45:56Z,,,
arxiv2024,Compositional API Recommendation for Library-Oriented Code Generation,No.,1,"""No evidence""",2024,2024-02-29T18:27:27Z,,,
arxiv2024,On the Scaling Laws of Geographical Representation in Language Models,No.,1,"""No evidence""",2024,2024-02-29T18:04:11Z,,,
arxiv2024,Entity-Aware Multimodal Alignment Framework for News Image Captioning,No.,1,"""No evidence""",2024,2024-02-29T18:03:00Z,,,
arxiv2024,Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy,No.,1,"""No evidence""",2024,2024-02-29T17:27:59Z,,,
arxiv2024,OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models,No.,1,"""No evidence""",2024,2024-02-29T17:19:39Z,,,
arxiv2024,"Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook",No.,1,"""No evidence""",2024,2024-02-29T16:56:23Z,,,
arxiv2024,Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction,No.,1,"""No evidence""",2024,2024-02-29T16:29:53Z,,,
arxiv2024,RL-GPT: Integrating Reinforcement Learning and Code-as-policy,No.,1,"""No evidence""",2024,2024-02-29T16:07:22Z,,,
arxiv2024,Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark,No.,1,"""No evidence""",2024,2024-02-29T15:22:13Z,,,
arxiv2024,RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks,No.,1,"""No evidence""",2024,2024-02-29T14:50:58Z,,,
arxiv2024,PeLLE: Encoder-based language models for Brazilian Portuguese based on open data,No.,1,"""No evidence""",2024,2024-02-29T14:34:03Z,,,
arxiv2024,PRSA: Prompt Reverse Stealing Attacks against Large Language Models,No.,1,"""No evidence""",2024,2024-02-29T14:30:28Z,,,
arxiv2024,Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning,No.,1,"""No evidence""",2024,2024-02-29T13:49:56Z,,,
arxiv2024,Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment,No.,1,"""No evidence""",2024,2024-02-29T12:12:30Z,,,
arxiv2024,"FhGenie: A Custom, Confidentiality-preserving Chat AI for Corporate and Scientific Use",No.,1,"""No evidence""",2024,2024-02-29T09:43:50Z,,,
arxiv2024,EyeGPT: Ophthalmic Assistant with Large Language Models,No.,1,"""No evidence""",2024,2024-02-29T09:35:41Z,,,
arxiv2024,Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning,No.,1,"""No evidence""",2024,2024-02-29T05:27:45Z,,,
arxiv2024,How do Large Language Models Handle Multilingualism?,No.,1,"""No evidence""",2024,2024-02-29T02:55:26Z,,,
arxiv2024,On the Decision-Making Abilities in Role-Playing using Large Language Models,No.,1,"""No evidence""",2024,2024-02-29T02:22:23Z,,,
arxiv2024,FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning,No.,1,"""No evidence""",2024,2024-02-29T01:33:08Z,,,
arxiv2024,NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models,No.,1,"""No evidence""",2024,2024-02-28T22:21:47Z,,,
arxiv2024,Commonsense Ontology Micropatterns,No.,1,"""No evidence""",2024,2024-02-28T21:23:54Z,,,
arxiv2024,CLLMs: Consistency Large Language Models,No.,1,"""No evidence""",2024,2024-02-28T20:17:04Z,,,
arxiv2024,Simple linear attention language models balance the recall-throughput tradeoff,No.,1,"""No evidence""",2024,2024-02-28T19:28:27Z,,,
arxiv2024,A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems,No.,1,"""No evidence""",2024,2024-02-28T19:00:12Z,,,
arxiv2024,Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification,No.,1,"""No evidence""",2024,2024-02-28T17:29:27Z,,,
arxiv2024,Language Models Represent Beliefs of Self and Others,No.,1,"""No evidence""",2024,2024-02-28T17:25:59Z,,,
arxiv2024,The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA,No.,1,"""No evidence""",2024,2024-02-28T15:05:43Z,,,
arxiv2024,Large Language Models As Evolution Strategies,No.,1,"""No evidence""",2024,2024-02-28T15:02:17Z,,,
arxiv2024,Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,No.,1,"""No evidence""",2024,2024-02-28T14:09:02Z,,,
arxiv2024,Retrieval-based Full-length Wikipedia Generation for Emergent Events,No.,1,"""No evidence""",2024,2024-02-28T11:51:56Z,,,
arxiv2024,Towards Generalist Prompting for Large Language Models by Mental Models,No.,1,"""No evidence""",2024,2024-02-28T11:29:09Z,,,
arxiv2024,CogBench: a large language model walks into a psychology lab,No.,1,"""No evidence""",2024,2024-02-28T10:43:54Z,,,
arxiv2024,Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging,No.,1,"""No evidence""",2024,2024-02-28T09:51:55Z,,,
arxiv2024,MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery,No.,1,"""No evidence""",2024,2024-02-28T08:57:42Z,,,
arxiv2024,From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs,No.,1,"""No evidence""",2024,2024-02-28T08:42:23Z,,,
arxiv2024,MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices,No.,1,"""No evidence""",2024,2024-02-28T08:30:49Z,,,
arxiv2024,Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information,No.,1,"""No evidence""",2024,2024-02-28T08:09:14Z,,,
arxiv2024,"Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",No.,1,"""No evidence""",2024,2024-02-28T07:18:39Z,,,
arxiv2024,Small But Funny: A Feedback-Driven Approach to Humor Distillation,No.,1,"""No evidence""",2024,2024-02-28T07:02:38Z,,,
arxiv2024,Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction,No.,1,"""No evidence""",2024,2024-02-28T06:50:14Z,,,
arxiv2024,Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models,No.,1,"""No evidence""",2024,2024-02-28T05:43:22Z,,,
arxiv2024,MEGAnno+: A Human-LLM Collaborative Annotation System,No.,1,"""No evidence""",2024,2024-02-28T04:58:07Z,,,
arxiv2024,Datasets for Large Language Models: A Comprehensive Survey,No.,1,"""No evidence""",2024,2024-02-28T04:35:51Z,,,
arxiv2024,ResLoRA: Identity Residual Mapping in Low-Rank Adaption,No.,1,"""No evidence""",2024,2024-02-28T04:33:20Z,,,
arxiv2024,Merino: Entropy-driven Design for Generative Language Models on IoT Devices,No.,1,"""No evidence""",2024,2024-02-28T03:20:27Z,,,
arxiv2024,Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization,No.,1,"""No evidence""",2024,2024-02-28T02:40:09Z,,,
arxiv2024,EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models,No.,1,"""No evidence""",2024,2024-02-27T23:30:17Z,,,
arxiv2024,Acquiring Linguistic Knowledge from Multimodal Input,No.,1,"""No evidence""",2024,2024-02-27T23:29:10Z,,,
arxiv2024,Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures,No.,1,"""No evidence""",2024,2024-02-27T23:12:45Z,,,
arxiv2024,Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning,No.,1,"""No evidence""",2024,2024-02-27T23:06:53Z,,,
arxiv2024,A Language Model based Framework for New Concept Placement in Ontologies,No.,1,"""No evidence""",2024,2024-02-27T21:27:35Z,,,
arxiv2024,"Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents",No.,1,"""No evidence""",2024,2024-02-27T21:27:16Z,,,
arxiv2024,JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability,No.,1,"""No evidence""",2024,2024-02-27T21:01:41Z,,,
arxiv2024,Deep Learning Detection Method for Large Language Models-Generated Scientific Content,No.,1,"""No evidence""",2024,2024-02-27T19:16:39Z,,,
arxiv2024,Self-Refinement of Language Models from External Proxy Metrics Feedback,No.,1,"""No evidence""",2024,2024-02-27T19:13:01Z,,,
arxiv2024,Prediction-Powered Ranking of Large Language Models,No.,1,"""No evidence""",2024,2024-02-27T19:00:01Z,,,
arxiv2024,The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits,No.,1,"""No evidence""",2024,2024-02-27T18:56:19Z,,,
arxiv2024,Tower: An Open Multilingual Large Language Model for Translation-Related Tasks,No.,1,"""No evidence""",2024,2024-02-27T18:09:36Z,,,
arxiv2024,NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents,No.,1,"""No evidence""",2024,2024-02-27T16:56:30Z,,,
arxiv2024,Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs,No.,1,"""No evidence""",2024,2024-02-27T16:19:37Z,,,
arxiv2024,SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation,No.,1,"""No evidence""",2024,2024-02-27T16:15:28Z,,,
arxiv2024,Variational Learning is Effective for Large Deep Networks,No.,1,"""No evidence""",2024,2024-02-27T16:11:05Z,,,
arxiv2024,Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization,No.,1,"""No evidence""",2024,2024-02-27T15:09:20Z,,,
arxiv2024,DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation,No.,1,"""No evidence""",2024,2024-02-27T14:51:11Z,,,
arxiv2024,OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web,No.,1,"""No evidence""",2024,2024-02-27T14:47:53Z,,,
arxiv2024,Retrieval is Accurate Generation,No.,1,"""No evidence""",2024,2024-02-27T14:16:19Z,,,
arxiv2024,Predict the Next Word: Humans exhibit uncertainty in this task and language models _____,No.,1,"""No evidence""",2024,2024-02-27T14:11:32Z,,,
arxiv2024,BASES: Large-scale Web Search User Simulation with Large Language Model based Agents,No.,1,"""No evidence""",2024,2024-02-27T13:44:09Z,,,
arxiv2024,Intensive Care as One Big Sequence Modeling Problem,No.,1,"""No evidence""",2024,2024-02-27T13:36:55Z,,,
arxiv2024,Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?,No.,1,"""No evidence""",2024,2024-02-27T13:18:00Z,,,
arxiv2024,Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles,No.,1,"""No evidence""",2024,2024-02-27T13:02:19Z,,,
arxiv2024,DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning,No.,1,"""No evidence""",2024,2024-02-27T12:26:07Z,,,
arxiv2024,Deep Learning Based Named Entity Recognition Models for Recipes,No.,1,"""No evidence""",2024,2024-02-27T12:03:56Z,,,
arxiv2024,Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective,No.,1,"""No evidence""",2024,2024-02-27T11:02:12Z,,,
arxiv2024,Investigating Continual Pretraining in Large Language Models: Insights and Implications,No.,1,"""No evidence""",2024,2024-02-27T10:47:24Z,,,
arxiv2024,Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies,No.,1,"""No evidence""",2024,2024-02-27T10:44:52Z,,,
arxiv2024,Determinants of LLM-assisted Decision-Making,No.,1,"""No evidence""",2024,2024-02-27T10:24:50Z,,,
arxiv2024,RECOST: External Knowledge Guided Data-efficient Instruction Tuning,No.,1,"""No evidence""",2024,2024-02-27T09:47:36Z,,,
arxiv2024,Probing Multimodal Large Language Models for Global and Local Semantic Representations,No.,1,"""No evidence""",2024,2024-02-27T08:27:15Z,,,
arxiv2024,Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese,No.,1,"""No evidence""",2024,2024-02-27T08:24:32Z,,,
arxiv2024,Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning,No.,1,"""No evidence""",2024,2024-02-27T07:14:12Z,,,
arxiv2024,MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning,No.,1,"""No evidence""",2024,2024-02-27T05:50:35Z,,,
arxiv2024,Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models,No.,1,"""No evidence""",2024,2024-02-27T05:37:10Z,,,
arxiv2024,Measuring Vision-Language STEM Skills of Neural Models,No.,1,"""No evidence""",2024,2024-02-27T04:55:03Z,,,
arxiv2024,"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",No.,1,"""No evidence""",2024,2024-02-27T03:30:58Z,,,
arxiv2024,Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation,No.,1,"""No evidence""",2024,2024-02-27T02:16:07Z,,,
arxiv2024,Sinkhorn Distance Minimization for Knowledge Distillation,No.,1,"""No evidence""",2024,2024-02-27T01:13:58Z,,,
arxiv2024,Adapting to Teammates in a Cooperative Language Game,No.,1,"""No evidence""",2024,2024-02-26T23:15:07Z,,,
arxiv2024,Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling,No.,1,"""No evidence""",2024,2024-02-26T20:56:06Z,,,
arxiv2024,Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset,No.,1,"""No evidence""",2024,2024-02-26T20:42:40Z,,,
arxiv2024,Can Large Language Models Recall Reference Location Like Humans?,No.,1,"""No evidence""",2024,2024-02-26T20:35:32Z,,,
arxiv2024,Algorithmic Arbitrariness in Content Moderation,No.,1,"""No evidence""",2024,2024-02-26T19:27:00Z,,,
arxiv2024,WIPI: A New Web Threat for LLM-Driven Web Agents,No.,1,"""No evidence""",2024,2024-02-26T19:01:54Z,,,
arxiv2024,"Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding",No.,1,"""No evidence""",2024,2024-02-26T18:59:28Z,,,
arxiv2024,Do Large Language Models Latently Perform Multi-Hop Reasoning?,No.,1,"""No evidence""",2024,2024-02-26T18:57:54Z,,,
arxiv2024,Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections,No.,1,"""No evidence""",2024,2024-02-26T18:56:48Z,,,
arxiv2024,A Survey on Data Selection for Language Models,No.,1,"""No evidence""",2024,2024-02-26T18:54:35Z,,,
arxiv2024,Language Agents as Optimizable Graphs,No.,1,"""No evidence""",2024,2024-02-26T18:48:27Z,,,
arxiv2024,A Surprising Failure? Multimodal LLMs and the NLVR Challenge,No.,1,"""No evidence""",2024,2024-02-26T18:37:18Z,,,
arxiv2024,OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA),No.,1,"""No evidence""",2024,2024-02-26T18:33:13Z,,,
arxiv2024,"If in a Crowdsourced Data Annotation Pipeline, a GPT-4",No.,1,"""No evidence""",2024,2024-02-26T18:08:52Z,,,
arxiv2024,Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models,No.,1,"""No evidence""",2024,2024-02-26T18:00:49Z,,,
arxiv2024,SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection,No.,1,"""No evidence""",2024,2024-02-26T16:21:53Z,,,
arxiv2024,Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models,No.,1,"""No evidence""",2024,2024-02-26T16:11:03Z,,,
arxiv2024,HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization,No.,1,"""No evidence""",2024,2024-02-26T16:09:00Z,,,
arxiv2024,Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study,No.,1,"""No evidence""",2024,2024-02-26T16:05:33Z,,,
arxiv2024,ESG Sentiment Analysis: comparing human and language model performance including GPT,No.,1,"""No evidence""",2024,2024-02-26T15:22:30Z,,,
arxiv2024,LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language,No.,1,"""No evidence""",2024,2024-02-26T15:05:16Z,,,
arxiv2024,Long-Context Language Modeling with Parallel Context Encoding,No.,1,"""No evidence""",2024,2024-02-26T14:47:35Z,,,
arxiv2024,Aligning Large Language Models to a Domain-specific Graph Database,No.,1,"""No evidence""",2024,2024-02-26T13:46:51Z,,,
arxiv2024,Integrating Large Language Models with Graphical Session-Based Recommendation,No.,1,"""No evidence""",2024,2024-02-26T12:55:51Z,,,
arxiv2024,LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments,No.,1,"""No evidence""",2024,2024-02-26T11:31:48Z,,,
arxiv2024,On Languaging a Simulation Engine,No.,1,"""No evidence""",2024,2024-02-26T11:01:54Z,,,
arxiv2024,Defending LLMs against Jailbreaking Attacks via Backtranslation,No.,1,"""No evidence""",2024,2024-02-26T10:03:33Z,,,
arxiv2024,"ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",No.,1,"""No evidence""",2024,2024-02-26T09:43:02Z,,,
arxiv2024,Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models,No.,1,"""No evidence""",2024,2024-02-26T09:36:05Z,,,
arxiv2024,DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models,No.,1,"""No evidence""",2024,2024-02-26T09:21:59Z,,,
arxiv2024,Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models,No.,1,"""No evidence""",2024,2024-02-26T09:19:46Z,,,
arxiv2024,HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy,No.,1,"""No evidence""",2024,2024-02-26T09:10:34Z,,,
arxiv2024,MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property,No.,1,"""No evidence""",2024,2024-02-26T08:27:50Z,,,
arxiv2024,Improving LLM-based Machine Translation with Systematic Self-Correction,No.,1,"""No evidence""",2024,2024-02-26T07:58:12Z,,,
arxiv2024,Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models,No.,1,"""No evidence""",2024,2024-02-26T07:44:56Z,,,
arxiv2024,LLM Inference Unveiled: Survey and Roofline Model Insights,No.,1,"""No evidence""",2024,2024-02-26T07:33:05Z,,,
arxiv2024,Personalized Federated Instruction Tuning via Neural Architecture Search,No.,1,"""No evidence""",2024,2024-02-26T06:29:05Z,,,
arxiv2024,Data-freeWeight Compress and Denoise for Large Language Models,No.,1,"""No evidence""",2024,2024-02-26T05:51:47Z,,,
arxiv2024,Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models,No.,1,"""No evidence""",2024,2024-02-26T05:43:51Z,,,
arxiv2024,"PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering",No.,1,"""No evidence""",2024,2024-02-26T04:09:53Z,,,
arxiv2024,CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering,No.,1,"""No evidence""",2024,2024-02-26T01:17:50Z,,,
arxiv2024,How Can LLM Guide RL? A Value-Based Approach,No.,1,"""No evidence""",2024,2024-02-25T20:07:13Z,,,
arxiv2024,ChatMusician: Understanding and Generating Music Intrinsically with LLM,No.,1,"""No evidence""",2024,2024-02-25T17:19:41Z,,,
arxiv2024,PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization,No.,1,"""No evidence""",2024,2024-02-25T16:43:41Z,,,
arxiv2024,What Generative Artificial Intelligence Means for Terminological Definitions,No.,1,"""No evidence""",2024,2024-02-25T16:36:51Z,,,
arxiv2024,AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation,No.,1,"""No evidence""",2024,2024-02-25T15:51:05Z,,,
arxiv2024,UrbanGPT: Spatio-Temporal Large Language Models,No.,1,"""No evidence""",2024,2024-02-25T12:37:29Z,,,
arxiv2024,LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding,No.,1,"""No evidence""",2024,2024-02-25T10:27:46Z,,,
arxiv2024,LLMs with Chain-of-Thought Are Non-Causal Reasoners,No.,1,"""No evidence""",2024,2024-02-25T10:13:04Z,,,
arxiv2024,Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations,No.,1,"""No evidence""",2024,2024-02-25T09:19:11Z,,,
arxiv2024,GraphWiz: An Instruction-Following Language Model for Graph Problems,No.,1,"""No evidence""",2024,2024-02-25T08:41:32Z,,,
arxiv2024,LoRA Meets Dropout under a Unified Framework,No.,1,"""No evidence""",2024,2024-02-25T07:09:10Z,,,
arxiv2024,GreenLLaMA: A Framework for Detoxification with Explanations,No.,1,"""No evidence""",2024,2024-02-25T01:56:47Z,,,
arxiv2024,Bootstrapping Cognitive Agents with a Large Language Model,No.,1,"""No evidence""",2024,2024-02-25T01:40:30Z,,,
arxiv2024,LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step,No.,1,"""No evidence""",2024,2024-02-25T00:56:27Z,,,
arxiv2024,Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency,No.,1,"""No evidence""",2024,2024-02-24T23:17:56Z,,,
arxiv2024,Multimodal Instruction Tuning with Conditional Mixture of LoRA,No.,1,"""No evidence""",2024,2024-02-24T20:15:31Z,,,
arxiv2024,MATHWELL: Generating Educational Math Word Problems at Scale,No.,1,"""No evidence""",2024,2024-02-24T17:08:45Z,,,
arxiv2024,NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation,No.,1,"""No evidence""",2024,2024-02-24T16:39:16Z,,,
arxiv2024,Prompt Perturbation Consistency Learning for Robust Language Models,No.,1,"""No evidence""",2024,2024-02-24T15:00:58Z,,,
arxiv2024,Linguistic Intelligence in Large Language Models for Telecommunications,No.,1,"""No evidence""",2024,2024-02-24T14:01:07Z,,,
arxiv2024,Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method,No.,1,"""No evidence""",2024,2024-02-24T13:36:58Z,,,
arxiv2024,Empowering Large Language Model Agents through Action Learning,No.,1,"""No evidence""",2024,2024-02-24T13:13:04Z,,,
arxiv2024,Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models,No.,1,"""No evidence""",2024,2024-02-24T12:31:22Z,,,
arxiv2024,"From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models",No.,1,"""No evidence""",2024,2024-02-24T09:06:25Z,,,
arxiv2024,Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation,No.,1,"""No evidence""",2024,2024-02-24T08:10:54Z,,,
arxiv2024,Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning,No.,1,"""No evidence""",2024,2024-02-24T07:22:04Z,,,
arxiv2024,MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation,No.,1,"""No evidence""",2024,2024-02-24T06:14:34Z,,,
arxiv2024,How Do Humans Write Code? Large Models Do It the Same Way Too,No.,1,"""No evidence""",2024,2024-02-24T05:40:01Z,,,
arxiv2024,LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper,No.,1,"""No evidence""",2024,2024-02-24T05:34:43Z,,,
arxiv2024,Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors,No.,1,"""No evidence""",2024,2024-02-24T04:32:44Z,,,
arxiv2024,Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology,No.,1,"""No evidence""",2024,2024-02-24T02:27:55Z,,,
arxiv2024,Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study,No.,1,"""No evidence""",2024,2024-02-24T00:38:29Z,,,
arxiv2024,On Trojan Signatures in Large Language Models of Code,No.,1,"""No evidence""",2024,2024-02-23T22:48:29Z,,,
arxiv2024,Fine-Grained Self-Endorsement Improves Factuality and Reasoning,No.,1,"""No evidence""",2024,2024-02-23T22:24:40Z,,,
arxiv2024,Towards Efficient Active Learning in NLP via Pretrained Representations,No.,1,"""No evidence""",2024,2024-02-23T21:28:59Z,,,
arxiv2024,"Selective ""Selective Prediction"": Reducing Unnecessary Abstention in Vision-Language Reasoning",No.,1,"""No evidence""",2024,2024-02-23T21:16:52Z,,,
arxiv2024,Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis,No.,1,"""No evidence""",2024,2024-02-23T21:07:20Z,,,
arxiv2024,DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures,No.,1,"""No evidence""",2024,2024-02-23T20:10:18Z,,,
arxiv2024,CI w/o TN: Context Injection without Task Name for Procedure Planning,No.,1,"""No evidence""",2024,2024-02-23T19:34:47Z,,,
arxiv2024,AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning,No.,1,"""No evidence""",2024,2024-02-23T18:56:26Z,,,
arxiv2024,Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts,No.,1,"""No evidence""",2024,2024-02-23T18:56:11Z,,,
arxiv2024,API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs,No.,1,"""No evidence""",2024,2024-02-23T18:30:49Z,,,
arxiv2024,PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning,No.,1,"""No evidence""",2024,2024-02-23T16:30:05Z,,,
arxiv2024,Explorations of Self-Repair in Language Models,No.,1,"""No evidence""",2024,2024-02-23T15:42:12Z,,,
arxiv2024,Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction,No.,1,"""No evidence""",2024,2024-02-23T15:02:44Z,,,
arxiv2024,Farsight: Fostering Responsible AI Awareness During AI Application Prototyping,No.,1,"""No evidence""",2024,2024-02-23T14:38:05Z,,,
arxiv2024,GPTVQ: The Blessing of Dimensionality for LLM Quantization,No.,1,"""No evidence""",2024,2024-02-23T13:39:16Z,,,
arxiv2024,ArabianGPT: Native Arabic GPT-based Large Language Model,No.,1,"""No evidence""",2024,2024-02-23T13:32:47Z,,,
arxiv2024,"CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models",No.,1,"""No evidence""",2024,2024-02-23T11:25:17Z,,,
arxiv2024,DEEM: Dynamic Experienced Expert Modeling for Stance Detection,No.,1,"""No evidence""",2024,2024-02-23T11:24:00Z,,,
arxiv2024,Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues,No.,1,"""No evidence""",2024,2024-02-23T10:27:42Z,,,
arxiv2024,DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators,No.,1,"""No evidence""",2024,2024-02-23T09:01:00Z,,,
arxiv2024,Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer,No.,1,"""No evidence""",2024,2024-02-23T08:11:55Z,,,
arxiv2024,Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,No.,1,"""No evidence""",2024,2024-02-23T07:21:32Z,,,
arxiv2024,AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System,No.,1,"""No evidence""",2024,2024-02-23T06:25:20Z,,,
arxiv2024,Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,No.,1,"""No evidence""",2024,2024-02-23T06:11:50Z,,,
arxiv2024,Large Multimodal Agents: A Survey,No.,1,"""No evidence""",2024,2024-02-23T06:04:23Z,,,
arxiv2024,Executing Natural Language-Described Algorithms with Large Language Models: An Investigation,No.,1,"""No evidence""",2024,2024-02-23T05:31:36Z,,,
arxiv2024,A First Look at GPT Apps: Landscape and Vulnerability,No.,1,"""No evidence""",2024,2024-02-23T05:30:32Z,,,
arxiv2024,Studying LLM Performance on Closed- and Open-source Data,No.,1,"""No evidence""",2024,2024-02-23T05:17:28Z,,,
arxiv2024,Evaluating the Performance of ChatGPT for Spam Email Detection,No.,1,"""No evidence""",2024,2024-02-23T04:52:08Z,,,
arxiv2024,On the Multi-turn Instruction Following for Conversational Web Agents,No.,1,"""No evidence""",2024,2024-02-23T02:18:12Z,,,
arxiv2024,Unlocking the Power of Large Language Models for Entity Alignment,No.,1,"""No evidence""",2024,2024-02-23T01:55:35Z,,,
arxiv2024,CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models,No.,1,"""No evidence""",2024,2024-02-22T23:42:25Z,,,
arxiv2024,Divide-or-Conquer? Which Part Should You Distill Your LLM?,No.,1,"""No evidence""",2024,2024-02-22T22:28:46Z,,,
arxiv2024,tinyBenchmarks: evaluating LLMs with fewer examples,No.,1,"""No evidence""",2024,2024-02-22T22:05:23Z,,,
arxiv2024,Optimizing Language Models for Human Preferences is a Causal Inference Problem,No.,1,"""No evidence""",2024,2024-02-22T21:36:07Z,,,
arxiv2024,GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data,No.,1,"""No evidence""",2024,2024-02-22T21:22:04Z,,,
arxiv2024,MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,No.,1,"""No evidence""",2024,2024-02-22T18:58:55Z,,,
arxiv2024,Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models,No.,1,"""No evidence""",2024,2024-02-22T18:56:07Z,,,
arxiv2024,Zero-shot cross-lingual transfer in instruction tuning of large language model,No.,1,"""No evidence""",2024,2024-02-22T18:37:33Z,,,
arxiv2024,DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models,No.,1,"""No evidence""",2024,2024-02-22T18:26:02Z,,,
arxiv2024,Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs,No.,1,"""No evidence""",2024,2024-02-22T18:14:09Z,,,
arxiv2024,Scaling Efficient LLMs,No.,1,"""No evidence""",2024,2024-02-22T18:06:19Z,,,
arxiv2024,Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation,No.,1,"""No evidence""",2024,2024-02-22T18:03:14Z,,,
arxiv2024,Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs,No.,1,"""No evidence""",2024,2024-02-22T17:52:34Z,,,
arxiv2024,Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images,No.,1,"""No evidence""",2024,2024-02-22T17:36:34Z,,,
arxiv2024,Chain-of-Thought Unfaithfulness as Disguised Accuracy,No.,1,"""No evidence""",2024,2024-02-22T17:23:53Z,,,
arxiv2024,Unveiling Linguistic Regions in Large Language Models,No.,1,"""No evidence""",2024,2024-02-22T16:56:13Z,,,
arxiv2024,UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models,No.,1,"""No evidence""",2024,2024-02-22T16:45:32Z,,,
arxiv2024,Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality,No.,1,"""No evidence""",2024,2024-02-22T16:32:08Z,,,
arxiv2024,OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement,No.,1,"""No evidence""",2024,2024-02-22T16:06:23Z,,,
arxiv2024,LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey,No.,1,"""No evidence""",2024,2024-02-22T13:52:02Z,,,
arxiv2024,"Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard",No.,1,"""No evidence""",2024,2024-02-22T13:25:17Z,,,
arxiv2024,Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance,No.,1,"""No evidence""",2024,2024-02-22T13:24:10Z,,,
arxiv2024,LLMBind: A Unified Modality-Task Integration Framework,No.,1,"""No evidence""",2024,2024-02-22T12:36:31Z,,,
arxiv2024,Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?,No.,1,"""No evidence""",2024,2024-02-22T11:16:23Z,,,
arxiv2024,Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph,No.,1,"""No evidence""",2024,2024-02-22T10:12:16Z,,,
arxiv2024,Uncertainty-Aware Evaluation for Vision-Language Models,No.,1,"""No evidence""",2024,2024-02-22T10:04:17Z,,,
arxiv2024,Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction,No.,1,"""No evidence""",2024,2024-02-22T08:26:56Z,,,
arxiv2024,OpenTab: Advancing Large Language Models as Open-domain Table Reasoners,No.,1,"""No evidence""",2024,2024-02-22T08:01:01Z,,,
arxiv2024,"Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",No.,1,"""No evidence""",2024,2024-02-22T07:55:26Z,,,
arxiv2024,INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models,No.,1,"""No evidence""",2024,2024-02-22T06:59:50Z,,,
arxiv2024,Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering,No.,1,"""No evidence""",2024,2024-02-22T06:23:37Z,,,
arxiv2024,Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education,No.,1,"""No evidence""",2024,2024-02-22T05:15:27Z,,,
arxiv2024,CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations,No.,1,"""No evidence""",2024,2024-02-22T05:07:31Z,,,
arxiv2024,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,No.,1,"""No evidence""",2024,2024-02-22T04:55:14Z,,,
arxiv2024,Can Large Language Models Detect Misinformation in Scientific News Reporting?,No.,1,"""No evidence""",2024,2024-02-22T04:07:00Z,,,
arxiv2024,Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming,No.,1,"""No evidence""",2024,2024-02-22T03:51:34Z,,,
arxiv2024,Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond,No.,1,"""No evidence""",2024,2024-02-22T03:46:08Z,,,
arxiv2024,Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models,No.,1,"""No evidence""",2024,2024-02-22T03:14:03Z,,,
arxiv2024,COPR: Continual Human Preference Learning via Optimal Policy Regularization,No.,1,"""No evidence""",2024,2024-02-22T02:20:08Z,,,
arxiv2024,Content Conditional Debiasing for Fair Text Embedding,No.,1,"""No evidence""",2024,2024-02-22T01:20:51Z,,,
arxiv2024,Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models,No.,1,"""No evidence""",2024,2024-02-22T01:20:17Z,,,
arxiv2024,Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization,No.,1,"""No evidence""",2024,2024-02-22T00:01:02Z,,,
arxiv2024,Understanding the Dataset Practitioners Behind Large Language Model Development,No.,1,"""No evidence""",2024,2024-02-21T23:50:37Z,,,
arxiv2024,Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media,No.,1,"""No evidence""",2024,2024-02-21T23:43:04Z,,,
arxiv2024,Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement,No.,1,"""No evidence""",2024,2024-02-21T22:57:49Z,,,
arxiv2024,TOOLVERIFIER: Generalization to New Tools via Self-Verification,No.,1,"""No evidence""",2024,2024-02-21T22:41:38Z,,,
arxiv2024,Automatic Histograms: Leveraging Language Models for Text Dataset Exploration,No.,1,"""No evidence""",2024,2024-02-21T22:29:16Z,,,
arxiv2024,MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms,No.,1,"""No evidence""",2024,2024-02-21T22:27:40Z,,,
arxiv2024,BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives,No.,1,"""No evidence""",2024,2024-02-21T22:22:30Z,,,
arxiv2024,Driving Generative Agents With Their Personality,No.,1,"""No evidence""",2024,2024-02-21T21:29:57Z,,,
arxiv2024,EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy,No.,1,"""No evidence""",2024,2024-02-21T19:49:12Z,,,
arxiv2024,Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns,No.,1,"""No evidence""",2024,2024-02-21T19:36:24Z,,,
arxiv2024,Coercing LLMs to do and reveal (almost) anything,No.,1,"""No evidence""",2024,2024-02-21T18:59:13Z,,,
arxiv2024,OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,No.,1,"""No evidence""",2024,2024-02-21T18:49:26Z,,,
arxiv2024,Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models,No.,1,"""No evidence""",2024,2024-02-21T18:40:24Z,,,
arxiv2024,Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning,No.,1,"""No evidence""",2024,2024-02-21T17:23:59Z,,,
arxiv2024,Exploring ChatGPT and its Impact on Society,No.,1,"""No evidence""",2024,2024-02-21T16:44:35Z,,,
arxiv2024,SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization,No.,1,"""No evidence""",2024,2024-02-21T16:33:22Z,,,
arxiv2024,Could We Have Had Better Multilingual LLMs If English Was Not the Central Language?,No.,1,"""No evidence""",2024,2024-02-21T16:32:38Z,,,
arxiv2024,$Se^2$: Sequential Example Selection for In-Context Learning,No.,1,"""No evidence""",2024,2024-02-21T15:35:04Z,,,
arxiv2024,Large Language Models are Advanced Anonymizers,No.,1,"""No evidence""",2024,2024-02-21T14:44:00Z,,,
arxiv2024,Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language,No.,1,"""No evidence""",2024,2024-02-21T13:57:36Z,,,
arxiv2024,LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain,No.,1,"""No evidence""",2024,2024-02-21T13:54:53Z,,,
arxiv2024,CriticBench: Evaluating Large Language Models as Critic,No.,1,"""No evidence""",2024,2024-02-21T12:38:59Z,,,
arxiv2024,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,No.,1,"""No evidence""",2024,2024-02-21T12:30:33Z,,,
arxiv2024,From Text to CQL: Bridging Natural Language and Corpus Search Engine,No.,1,"""No evidence""",2024,2024-02-21T12:11:28Z,,,
arxiv2024,Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent,No.,1,"""No evidence""",2024,2024-02-21T11:30:20Z,,,
arxiv2024,Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?,No.,1,"""No evidence""",2024,2024-02-21T11:07:07Z,,,
arxiv2024,UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language,No.,1,"""No evidence""",2024,2024-02-21T09:06:31Z,,,
arxiv2024,FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models,No.,1,"""No evidence""",2024,2024-02-21T08:50:40Z,,,
arxiv2024,A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models,No.,1,"""No evidence""",2024,2024-02-21T08:20:06Z,,,
arxiv2024,Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving,No.,1,"""No evidence""",2024,2024-02-21T08:09:05Z,,,
arxiv2024,User-LLM: Efficient LLM Contextualization with User Embeddings,No.,1,"""No evidence""",2024,2024-02-21T08:03:27Z,,,
arxiv2024,Knowledge Graph Enhanced Large Language Model Editing,No.,1,"""No evidence""",2024,2024-02-21T07:52:26Z,,,
arxiv2024,APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models,No.,1,"""No evidence""",2024,2024-02-21T07:45:22Z,,,
arxiv2024,A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation,No.,1,"""No evidence""",2024,2024-02-21T07:38:29Z,,,
arxiv2024,WinoViz: Probing Visual Properties of Objects Under Different States,No.,1,"""No evidence""",2024,2024-02-21T07:31:47Z,,,
arxiv2024,BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models,No.,1,"""No evidence""",2024,2024-02-21T07:16:29Z,,,
arxiv2024,Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment,No.,1,"""No evidence""",2024,2024-02-21T06:34:46Z,,,
arxiv2024,Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues,No.,1,"""No evidence""",2024,2024-02-21T06:11:03Z,,,
arxiv2024,ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling,No.,1,"""No evidence""",2024,2024-02-21T05:41:34Z,,,
arxiv2024,Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel,No.,1,"""No evidence""",2024,2024-02-21T05:14:30Z,,,
arxiv2024,OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models,No.,1,"""No evidence""",2024,2024-02-21T04:42:41Z,,,
arxiv2024,Round Trip Translation Defence against Large Language Model Jailbreaking Attacks,No.,1,"""No evidence""",2024,2024-02-21T03:59:52Z,,,
arxiv2024,ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models,No.,1,"""No evidence""",2024,2024-02-21T03:58:49Z,,,
arxiv2024,From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers,No.,1,"""No evidence""",2024,2024-02-21T03:51:34Z,,,
arxiv2024,Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models,No.,1,"""No evidence""",2024,2024-02-21T03:05:50Z,,,
arxiv2024,STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning,No.,1,"""No evidence""",2024,2024-02-21T01:54:58Z,,,
arxiv2024,CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory,No.,1,"""No evidence""",2024,2024-02-21T01:00:17Z,,,
arxiv2024,DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain,No.,1,"""No evidence""",2024,2024-02-20T23:54:02Z,,,
arxiv2024,Explaining Relationships Among Research Papers,No.,1,"""No evidence""",2024,2024-02-20T23:38:39Z,,,
arxiv2024,The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative,No.,1,"""No evidence""",2024,2024-02-20T23:08:21Z,,,
arxiv2024,Harnessing Large Language Models as Post-hoc Correctors,No.,1,"""No evidence""",2024,2024-02-20T22:50:41Z,,,
arxiv2024,Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems,No.,1,"""No evidence""",2024,2024-02-20T20:57:47Z,,,
arxiv2024,A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction,No.,1,"""No evidence""",2024,2024-02-20T20:42:02Z,,,
arxiv2024,Investigating Cultural Alignment of Large Language Models,No.,1,"""No evidence""",2024,2024-02-20T18:47:28Z,,,
arxiv2024,Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive,No.,1,"""No evidence""",2024,2024-02-20T18:42:34Z,,,
arxiv2024,RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian,No.,1,"""No evidence""",2024,2024-02-20T18:32:47Z,,,
arxiv2024,Benchmarking Retrieval-Augmented Generation for Medicine,No.,1,"""No evidence""",2024,2024-02-20T17:44:06Z,,,
arxiv2024,Is the System Message Really Important to Jailbreaks in Large Language Models?,No.,1,"""No evidence""",2024,2024-02-20T17:39:40Z,,,
arxiv2024,Defending Jailbreak Prompts via In-Context Adversarial Game,No.,1,"""No evidence""",2024,2024-02-20T17:04:06Z,,,
arxiv2024,Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity,No.,1,"""No evidence""",2024,2024-02-20T16:43:20Z,,,
arxiv2024,ELAD: Explanation-Guided Large Language Models Active Distillation,No.,1,"""No evidence""",2024,2024-02-20T15:47:59Z,,,
arxiv2024,Slot-VLM: SlowFast Slots for Video-Language Modeling,No.,1,"""No evidence""",2024,2024-02-20T15:30:09Z,,,
arxiv2024,Stable Knowledge Editing in Large Language Models,No.,1,"""No evidence""",2024,2024-02-20T14:36:23Z,,,
arxiv2024,Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries,No.,1,"""No evidence""",2024,2024-02-20T14:31:17Z,,,
arxiv2024,Text-Guided Molecule Generation with Diffusion Language Model,No.,1,"""No evidence""",2024,2024-02-20T14:29:02Z,,,
arxiv2024,Code Needs Comments: Enhancing Code LLMs with Comment Augmentation,No.,1,"""No evidence""",2024,2024-02-20T13:56:38Z,,,
arxiv2024,An Autonomous Large Language Model Agent for Chemical Literature Data Mining,No.,1,"""No evidence""",2024,2024-02-20T13:21:46Z,,,
arxiv2024,TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification,No.,1,"""No evidence""",2024,2024-02-20T13:20:39Z,,,
arxiv2024,Can GNN be Good Adapter for LLMs?,No.,1,"""No evidence""",2024,2024-02-20T13:13:13Z,,,
arxiv2024,Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning,No.,1,"""No evidence""",2024,2024-02-20T12:58:14Z,,,
arxiv2024,GlrIA -- A Generative and Open Large Language Model for Portuguese,No.,1,"""No evidence""",2024,2024-02-20T12:36:40Z,,,
arxiv2024,A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence,No.,1,"""No evidence""",2024,2024-02-20T11:28:50Z,,,
arxiv2024,Large Language Model-based Human-Agent Collaboration for Complex Task Solving,No.,1,"""No evidence""",2024,2024-02-20T11:03:36Z,,,
arxiv2024,OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data,No.,1,"""No evidence""",2024,2024-02-20T11:01:39Z,,,
arxiv2024,Chain of Thought Empowers Transformers to Solve Inherently Serial Problems,No.,1,"""No evidence""",2024,2024-02-20T10:11:03Z,,,
arxiv2024,Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data,No.,1,"""No evidence""",2024,2024-02-20T10:00:58Z,,,
arxiv2024,MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models,No.,1,"""No evidence""",2024,2024-02-20T09:30:48Z,,,
arxiv2024,Instruction-tuned Language Models are Better Knowledge Learners,No.,1,"""No evidence""",2024,2024-02-20T09:20:32Z,,,
arxiv2024,PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning,No.,1,"""No evidence""",2024,2024-02-20T09:10:08Z,,,
arxiv2024,"Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?",No.,1,"""No evidence""",2024,2024-02-20T08:38:24Z,,,
arxiv2024,Me LLaMA: Foundation Large Language Models for Medical Applications,No.,1,"""No evidence""",2024,2024-02-20T06:37:31Z,,,
arxiv2024,MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion,No.,1,"""No evidence""",2024,2024-02-20T06:14:30Z,,,
arxiv2024,Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues,No.,1,"""No evidence""",2024,2024-02-20T06:05:36Z,,,
arxiv2024,Are Large Language Models Rational Investors?,No.,1,"""No evidence""",2024,2024-02-20T04:26:08Z,,,
arxiv2024,FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning,No.,1,"""No evidence""",2024,2024-02-20T03:39:49Z,,,
arxiv2024,The FinBen: An Holistic Financial Benchmark for Large Language Models,No.,1,"""No evidence""",2024,2024-02-20T02:16:16Z,,,
arxiv2024,CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management,No.,1,"""No evidence""",2024,2024-02-20T01:59:11Z,,,
arxiv2024,Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation,No.,1,"""No evidence""",2024,2024-02-20T01:49:15Z,,,
arxiv2024,Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM,No.,1,"""No evidence""",2024,2024-02-19T23:48:40Z,,,
arxiv2024,Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation,No.,1,"""No evidence""",2024,2024-02-19T23:18:18Z,,,
arxiv2024,Detecting misinformation through Framing Theory: the Frame Element-based Model,No.,1,"""No evidence""",2024,2024-02-19T21:50:42Z,,,
arxiv2024,Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection,No.,1,"""No evidence""",2024,2024-02-19T20:08:48Z,,,
arxiv2024,Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?,No.,1,"""No evidence""",2024,2024-02-19T19:38:58Z,,,
arxiv2024,"Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding",No.,1,"""No evidence""",2024,2024-02-19T18:58:32Z,,,
arxiv2024,AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies,No.,1,"""No evidence""",2024,2024-02-19T18:56:44Z,,,
arxiv2024,A Critical Evaluation of AI Feedback for Aligning Large Language Models,No.,1,"""No evidence""",2024,2024-02-19T18:53:54Z,,,
arxiv2024,GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations,No.,1,"""No evidence""",2024,2024-02-19T18:23:36Z,,,
arxiv2024,Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models,No.,1,"""No evidence""",2024,2024-02-19T18:09:48Z,,,
arxiv2024,Query-Based Adversarial Prompt Generation,No.,1,"""No evidence""",2024,2024-02-19T18:01:36Z,,,
arxiv2024,LLM Agents for Psychology: A Study on Gamified Assessments,No.,1,"""No evidence""",2024,2024-02-19T18:00:30Z,,,
arxiv2024,ARKS: Active Retrieval in Knowledge Soup for Code Generation,No.,1,"""No evidence""",2024,2024-02-19T17:37:28Z,,,
arxiv2024,Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports,No.,1,"""No evidence""",2024,2024-02-19T17:23:10Z,,,
arxiv2024,Adaptive Skeleton Graph Decoding,No.,1,"""No evidence""",2024,2024-02-19T16:47:04Z,,,
arxiv2024,"WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",No.,1,"""No evidence""",2024,2024-02-19T16:39:18Z,,,
arxiv2024,Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data,No.,1,"""No evidence""",2024,2024-02-19T16:34:50Z,,,
arxiv2024,High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models,No.,1,"""No evidence""",2024,2024-02-19T16:29:40Z,,,
arxiv2024,Uncertainty quantification in fine-tuned LLMs using LoRA ensembles,No.,1,"""No evidence""",2024,2024-02-19T16:26:00Z,,,
arxiv2024,Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships,No.,1,"""No evidence""",2024,2024-02-19T16:15:03Z,,,
arxiv2024,Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition,No.,1,"""No evidence""",2024,2024-02-19T16:14:04Z,,,
arxiv2024,CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation,No.,1,"""No evidence""",2024,2024-02-19T15:30:40Z,,,
arxiv2024,Polarization of Autonomous Generative AI Agents Under Echo Chambers,No.,1,"""No evidence""",2024,2024-02-19T15:14:15Z,,,
arxiv2024,A Chinese Dataset for Evaluating the Safeguards in Large Language Models,No.,1,"""No evidence""",2024,2024-02-19T14:56:18Z,,,
arxiv2024,Stick to your Role! Stability of Personal Values Expressed in Large Language Models,No.,1,"""No evidence""",2024,2024-02-19T14:53:01Z,,,
arxiv2024,BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence,No.,1,"""No evidence""",2024,2024-02-19T14:28:31Z,,,
arxiv2024,Transformer-based Causal Language Models Perform Clustering,No.,1,"""No evidence""",2024,2024-02-19T14:02:31Z,,,
arxiv2024,End-to-end multilingual fact-checking at scale,No.,1,"""No evidence""",2024,2024-02-19T14:00:35Z,,,
arxiv2024,Can LLMs Compute with Reasons?,No.,1,"""No evidence""",2024,2024-02-19T12:04:25Z,,,
arxiv2024,All Language Models Large and Small,No.,1,"""No evidence""",2024,2024-02-19T11:28:20Z,,,
arxiv2024,Are LLM-based Evaluators Confusing NLG Quality Criteria?,No.,1,"""No evidence""",2024,2024-02-19T11:19:02Z,,,
arxiv2024,Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations,No.,1,"""No evidence""",2024,2024-02-19T10:47:09Z,,,
arxiv2024,Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space,No.,1,"""No evidence""",2024,2024-02-19T10:34:48Z,,,
arxiv2024,Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?,No.,1,"""No evidence""",2024,2024-02-19T10:34:13Z,,,
arxiv2024,Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought,No.,1,"""No evidence""",2024,2024-02-19T10:33:29Z,,,
arxiv2024,EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs,No.,1,"""No evidence""",2024,2024-02-19T09:55:32Z,,,
arxiv2024,Structure Guided Large Language Model for SQL Generation,No.,1,"""No evidence""",2024,2024-02-19T09:07:59Z,,,
arxiv2024,Automatic Evaluation for Mental Health Counseling using LLMs,No.,1,"""No evidence""",2024,2024-02-19T09:00:10Z,,,
arxiv2024,Comprehensive Cognitive LLM Agent for Smartphone GUI Automation,No.,1,"""No evidence""",2024,2024-02-19T08:29:03Z,,,
arxiv2024,Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation,No.,1,"""No evidence""",2024,2024-02-19T07:46:40Z,,,
arxiv2024,Learning to Edit: Aligning LLMs with Knowledge Editing,No.,1,"""No evidence""",2024,2024-02-19T07:45:17Z,,,
arxiv2024,Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models,No.,1,"""No evidence""",2024,2024-02-19T07:34:10Z,,,
arxiv2024,SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning,No.,1,"""No evidence""",2024,2024-02-19T07:22:29Z,,,
arxiv2024,Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint,No.,1,"""No evidence""",2024,2024-02-19T07:10:30Z,,,
arxiv2024,ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding,No.,1,"""No evidence""",2024,2024-02-19T06:58:42Z,,,
arxiv2024,The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth,No.,1,"""No evidence""",2024,2024-02-19T06:54:55Z,,,
arxiv2024,NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization,No.,1,"""No evidence""",2024,2024-02-19T06:43:25Z,,,
arxiv2024,UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction,No.,1,"""No evidence""",2024,2024-02-19T05:04:11Z,,,
arxiv2024,Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search,No.,1,"""No evidence""",2024,2024-02-19T04:41:31Z,,,
arxiv2024,FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema,No.,1,"""No evidence""",2024,2024-02-19T03:56:44Z,,,
arxiv2024,LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs,No.,1,"""No evidence""",2024,2024-02-19T03:21:19Z,,,
arxiv2024,What Evidence Do Language Models Find Convincing?,No.,1,"""No evidence""",2024,2024-02-19T02:15:34Z,,,
arxiv2024,MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs,No.,1,"""No evidence""",2024,2024-02-19T01:04:22Z,,,
arxiv2024,SPML: A DSL for Defending Language Models Against Prompt Attacks,No.,1,"""No evidence""",2024,2024-02-19T00:53:48Z,,,
arxiv2024,RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts,No.,1,"""No evidence""",2024,2024-02-19T00:40:17Z,,,
arxiv2024,"Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges",No.,1,"""No evidence""",2024,2024-02-18T23:22:40Z,,,
arxiv2024,Solving Data-centric Tasks using Large Language Models,No.,1,"""No evidence""",2024,2024-02-18T23:19:21Z,,,
arxiv2024,Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models,No.,1,"""No evidence""",2024,2024-02-18T22:27:42Z,,,
arxiv2024,Modelling Political Coalition Negotiations Using LLM-based Agents,No.,1,"""No evidence""",2024,2024-02-18T21:28:06Z,,,
arxiv2024,MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization,No.,1,"""No evidence""",2024,2024-02-18T21:25:09Z,,,
arxiv2024,GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network,No.,1,"""No evidence""",2024,2024-02-18T21:13:05Z,,,
arxiv2024,Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable,No.,1,"""No evidence""",2024,2024-02-18T21:10:18Z,,,
arxiv2024,A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models,No.,1,"""No evidence""",2024,2024-02-18T18:56:07Z,,,
arxiv2024,Autocorrect for Estonian texts: final report from project EKTB25,No.,1,"""No evidence""",2024,2024-02-18T18:20:57Z,,,
arxiv2024,Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models,No.,1,"""No evidence""",2024,2024-02-18T16:43:21Z,,,
arxiv2024,On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs,No.,1,"""No evidence""",2024,2024-02-18T15:53:32Z,,,
arxiv2024,Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection,No.,1,"""No evidence""",2024,2024-02-18T15:27:48Z,,,
arxiv2024,Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?,No.,1,"""No evidence""",2024,2024-02-18T14:25:19Z,,,
arxiv2024,Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark,No.,1,"""No evidence""",2024,2024-02-18T14:08:48Z,,,
arxiv2024,ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation,No.,1,"""No evidence""",2024,2024-02-18T11:24:34Z,,,
arxiv2024,Deciphering the Impact of Pretraining Data on Large Language Models through Machine Unlearning,No.,1,"""No evidence""",2024,2024-02-18T10:36:05Z,,,
arxiv2024,Ploutos: Towards interpretable stock movement prediction with financial large language model,No.,1,"""No evidence""",2024,2024-02-18T10:28:18Z,,,
arxiv2024,Efficient Multimodal Learning from Data-centric Perspective,No.,1,"""No evidence""",2024,2024-02-18T10:09:10Z,,,
arxiv2024,From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings,No.,1,"""No evidence""",2024,2024-02-18T08:53:41Z,,,
arxiv2024,Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources,No.,1,"""No evidence""",2024,2024-02-18T08:32:59Z,,,
arxiv2024,MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing,No.,1,"""No evidence""",2024,2024-02-18T07:15:03Z,,,
arxiv2024,ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework,No.,1,"""No evidence""",2024,2024-02-18T06:07:17Z,,,
arxiv2024,LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks,No.,1,"""No evidence""",2024,2024-02-18T04:41:25Z,,,
arxiv2024,AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition,No.,1,"""No evidence""",2024,2024-02-18T04:28:16Z,,,
arxiv2024,In-Context Example Ordering Guided by Label Distributions,No.,1,"""No evidence""",2024,2024-02-18T04:08:10Z,,,
arxiv2024,Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning,No.,1,"""No evidence""",2024,2024-02-18T03:04:38Z,,,
arxiv2024,"Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning",No.,1,"""No evidence""",2024,2024-02-18T02:52:54Z,,,
arxiv2024,Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction,No.,1,"""No evidence""",2024,2024-02-18T01:40:34Z,,,
arxiv2024,LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models,No.,1,"""No evidence""",2024,2024-02-18T01:20:00Z,,,
arxiv2024,Aligning Modalities in Vision Large Language Models via Preference Fine-tuning,No.,1,"""No evidence""",2024,2024-02-18T00:56:16Z,,,
arxiv2024,Multi-dimensional Evaluation of Empathetic Dialog Responses,No.,1,"""No evidence""",2024,2024-02-18T00:32:33Z,,,
arxiv2024,CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness,No.,1,"""No evidence""",2024,2024-02-17T22:37:17Z,,,
arxiv2024,Training Language Model Agents without Modifying Language Models,No.,1,"""No evidence""",2024,2024-02-17T18:31:21Z,,,
arxiv2024,Dissecting Human and LLM Preferences,No.,1,"""No evidence""",2024,2024-02-17T14:34:31Z,,,
arxiv2024,Can Large Multimodal Models Uncover Deep Semantics Behind Images?,No.,1,"""No evidence""",2024,2024-02-17T13:41:44Z,,,
arxiv2024,Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models,No.,1,"""No evidence""",2024,2024-02-17T13:37:39Z,,,
arxiv2024,MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning,No.,1,"""No evidence""",2024,2024-02-17T12:25:31Z,,,
arxiv2024,C-ICL: Contrastive In-context Learning for Information Extraction,No.,1,"""No evidence""",2024,2024-02-17T11:28:08Z,,,
arxiv2024,Aligning Large Language Models by On-Policy Self-Judgment,No.,1,"""No evidence""",2024,2024-02-17T11:25:26Z,,,
arxiv2024,Can Large Language Models perform Relation-based Argument Mining?,No.,1,"""No evidence""",2024,2024-02-17T10:37:51Z,,,
arxiv2024,Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs,No.,1,"""No evidence""",2024,2024-02-17T08:14:37Z,,,
arxiv2024,Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models,No.,1,"""No evidence""",2024,2024-02-17T08:04:23Z,,,
arxiv2024,Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,No.,1,"""No evidence""",2024,2024-02-17T06:48:45Z,,,
arxiv2024,Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection,No.,1,"""No evidence""",2024,2024-02-17T02:25:57Z,,,
arxiv2024,KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph,No.,1,"""No evidence""",2024,2024-02-17T02:07:49Z,,,
arxiv2024,Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction,No.,1,"""No evidence""",2024,2024-02-17T00:20:06Z,,,
arxiv2024,Orca-Math: Unlocking the potential of SLMs in Grade School Math,No.,1,"""No evidence""",2024,2024-02-16T23:44:38Z,,,
arxiv2024,BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering,No.,1,"""No evidence""",2024,2024-02-16T23:28:02Z,,,
arxiv2024,Word Embeddings Revisited: Do LLMs Offer Something New?,No.,1,"""No evidence""",2024,2024-02-16T21:47:30Z,,,
arxiv2024,AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators,No.,1,"""No evidence""",2024,2024-02-16T20:59:57Z,,,
arxiv2024,Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement,No.,1,"""No evidence""",2024,2024-02-16T20:20:43Z,,,
arxiv2024,PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter,No.,1,"""No evidence""",2024,2024-02-16T18:54:47Z,,,
arxiv2024,Proving membership in LLM pretraining data via data watermarks,No.,1,"""No evidence""",2024,2024-02-16T18:49:27Z,,,
arxiv2024,Instruction Diversity Drives Generalization To Unseen Tasks,No.,1,"""No evidence""",2024,2024-02-16T18:47:21Z,,,
arxiv2024,Multi-modal preference alignment remedies regression of visual instruction tuning on language model,No.,1,"""No evidence""",2024,2024-02-16T18:42:08Z,,,
arxiv2024,Quantifying the Persona Effect in LLM Simulations,No.,1,"""No evidence""",2024,2024-02-16T16:35:35Z,,,
arxiv2024,Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond,No.,1,"""No evidence""",2024,2024-02-16T16:31:46Z,,,
arxiv2024,In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss,No.,1,"""No evidence""",2024,2024-02-16T16:15:01Z,,,
arxiv2024,A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models,No.,1,"""No evidence""",2024,2024-02-16T16:02:33Z,,,
arxiv2024,AutoGPT+P: Affordance-based Task Planning with Large Language Models,No.,1,"""No evidence""",2024,2024-02-16T16:00:50Z,,,
arxiv2024,How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?,No.,1,"""No evidence""",2024,2024-02-16T15:48:33Z,,,
arxiv2024,Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification,No.,1,"""No evidence""",2024,2024-02-16T14:52:05Z,,,
arxiv2024,A Novel BERT-based Classifier to Detect Political Leaning of YouTube Videos based on their Titles,No.,1,"""No evidence""",2024,2024-02-16T14:44:30Z,,,
arxiv2024,An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference,No.,1,"""No evidence""",2024,2024-02-16T14:15:15Z,,,
arxiv2024,Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability,No.,1,"""No evidence""",2024,2024-02-16T13:46:06Z,,,
arxiv2024,German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data,No.,1,"""No evidence""",2024,2024-02-16T13:28:44Z,,,
arxiv2024,Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm,No.,1,"""No evidence""",2024,2024-02-16T13:24:05Z,,,
arxiv2024,OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models,No.,1,"""No evidence""",2024,2024-02-16T13:21:33Z,,,
arxiv2024,Network Formation and Dynamics Among Multi-LLMs,No.,1,"""No evidence""",2024,2024-02-16T13:10:14Z,,,
arxiv2024,AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation,No.,1,"""No evidence""",2024,2024-02-16T12:47:11Z,,,
arxiv2024,BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation,No.,1,"""No evidence""",2024,2024-02-16T12:27:15Z,,,
arxiv2024,Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement,No.,1,"""No evidence""",2024,2024-02-16T12:12:05Z,,,
arxiv2024,Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements,No.,1,"""No evidence""",2024,2024-02-16T12:00:34Z,,,
arxiv2024,Jailbreaking Proprietary Large Language Models using Word Substitution Cipher,No.,1,"""No evidence""",2024,2024-02-16T11:37:05Z,,,
arxiv2024,Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks,No.,1,"""No evidence""",2024,2024-02-16T11:30:11Z,,,
arxiv2024,Do Llamas Work in English? On the Latent Language of Multilingual Transformers,No.,1,"""No evidence""",2024,2024-02-16T11:21:28Z,,,
arxiv2024,Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs,No.,1,"""No evidence""",2024,2024-02-16T11:20:30Z,,,
arxiv2024,LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty,No.,1,"""No evidence""",2024,2024-02-16T11:02:29Z,,,
arxiv2024,LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine,No.,1,"""No evidence""",2024,2024-02-16T10:56:15Z,,,
arxiv2024,SPAR: Personalized Content-Based Recommendation via Long Engagement Attention,No.,1,"""No evidence""",2024,2024-02-16T10:36:38Z,,,
arxiv2024,Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts,No.,1,"""No evidence""",2024,2024-02-16T10:35:18Z,,,
arxiv2024,Properties and Challenges of LLM-Generated Explanations,No.,1,"""No evidence""",2024,2024-02-16T09:37:54Z,,,
arxiv2024,Can We Verify Step by Step for Incorrect Answer Detection?,No.,1,"""No evidence""",2024,2024-02-16T09:29:50Z,,,
arxiv2024,"Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",No.,1,"""No evidence""",2024,2024-02-16T09:06:06Z,,,
arxiv2024,Comparing Hallucination Detection Metrics for Multilingual Generation,No.,1,"""No evidence""",2024,2024-02-16T08:10:34Z,,,
arxiv2024,Large Language Models as Zero-shot Dialogue State Tracker through Function Calling,No.,1,"""No evidence""",2024,2024-02-16T06:13:18Z,,,
arxiv2024,I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models,No.,1,"""No evidence""",2024,2024-02-16T03:54:48Z,,,
arxiv2024,Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models,No.,1,"""No evidence""",2024,2024-02-16T03:39:37Z,,,
arxiv2024,Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning,No.,1,"""No evidence""",2024,2024-02-16T02:21:59Z,,,
arxiv2024,Chain of Logic: Rule-Based Reasoning with Large Language Models,No.,1,"""No evidence""",2024,2024-02-16T01:54:43Z,,,
arxiv2024,BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains,No.,1,"""No evidence""",2024,2024-02-15T23:39:04Z,,,
arxiv2024,Can we Soft Prompt LLMs for Graph Learning Tasks?,No.,1,"""No evidence""",2024,2024-02-15T23:09:42Z,,,
arxiv2024,Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models,No.,1,"""No evidence""",2024,2024-02-15T22:54:24Z,,,
arxiv2024,How to Discern Important Urgent News?,No.,1,"""No evidence""",2024,2024-02-15T20:08:07Z,,,
arxiv2024,LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing,No.,1,"""No evidence""",2024,2024-02-15T19:53:11Z,,,
arxiv2024,Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation,No.,1,"""No evidence""",2024,2024-02-15T18:59:18Z,,,
arxiv2024,A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents,No.,1,"""No evidence""",2024,2024-02-15T18:51:32Z,,,
arxiv2024,BitDelta: Your Fine-Tune May Only Be Worth One Bit,No.,1,"""No evidence""",2024,2024-02-15T18:50:06Z,,,
arxiv2024,Language Models with Conformal Factuality Guarantees,No.,1,"""No evidence""",2024,2024-02-15T18:31:53Z,,,
arxiv2024,Generative AI and Process Systems Engineering: The Next Frontier,No.,1,"""No evidence""",2024,2024-02-15T18:20:42Z,,,
arxiv2024,OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models,No.,1,"""No evidence""",2024,2024-02-15T18:19:18Z,,,
arxiv2024,Quantized Embedding Vectors for Controllable Diffusion Language Models,No.,1,"""No evidence""",2024,2024-02-15T17:02:48Z,,,
arxiv2024,GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving,No.,1,"""No evidence""",2024,2024-02-15T16:59:41Z,,,
arxiv2024,Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4,No.,1,"""No evidence""",2024,2024-02-15T16:43:41Z,,,
arxiv2024,Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence,No.,1,"""No evidence""",2024,2024-02-15T16:36:04Z,,,
arxiv2024,RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models,No.,1,"""No evidence""",2024,2024-02-15T16:00:58Z,,,
arxiv2024,LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition,No.,1,"""No evidence""",2024,2024-02-15T14:54:33Z,,,
arxiv2024,Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation,No.,1,"""No evidence""",2024,2024-02-15T14:03:33Z,,,
arxiv2024,Generative AI in the Construction Industry: A State-of-the-art Analysis,No.,1,"""No evidence""",2024,2024-02-15T13:39:55Z,,,
arxiv2024,MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music,No.,1,"""No evidence""",2024,2024-02-15T10:55:01Z,,,
arxiv2024,EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models,No.,1,"""No evidence""",2024,2024-02-15T08:58:03Z,,,
arxiv2024,Grounding Language Model with Chunking-Free In-Context Retrieval,No.,1,"""No evidence""",2024,2024-02-15T07:22:04Z,,,
arxiv2024,Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish,No.,1,"""No evidence""",2024,2024-02-15T07:17:10Z,,,
arxiv2024,AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis,No.,1,"""No evidence""",2024,2024-02-15T06:46:48Z,,,
arxiv2024,Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data,No.,1,"""No evidence""",2024,2024-02-15T06:30:12Z,,,
arxiv2024,ProtChatGPT: Towards Understanding Proteins with Large Language Models,No.,1,"""No evidence""",2024,2024-02-15T01:22:30Z,,,
arxiv2024,Answer is All You Need: Instruction-following Text Embedding via Answering the Question,No.,1,"""No evidence""",2024,2024-02-15T01:02:41Z,,,
arxiv2024,Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems,No.,1,"""No evidence""",2024,2024-02-14T21:19:33Z,,,
arxiv2024,Rationality Report Cards: Assessing the Economic Rationality of Large Language Models,No.,1,"""No evidence""",2024,2024-02-14T20:05:26Z,,,
arxiv2024,Reinforcement Learning from Human Feedback with Active Queries,No.,1,"""No evidence""",2024,2024-02-14T18:58:40Z,,,
arxiv2024,"LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",No.,1,"""No evidence""",2024,2024-02-14T18:42:25Z,,,
arxiv2024,HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation,No.,1,"""No evidence""",2024,2024-02-14T18:41:19Z,,,
arxiv2024,HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference,No.,1,"""No evidence""",2024,2024-02-14T18:04:36Z,,,
arxiv2024,ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization,No.,1,"""No evidence""",2024,2024-02-14T17:14:34Z,,,
arxiv2024,Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies,No.,1,"""No evidence""",2024,2024-02-14T16:10:45Z,,,
arxiv2024,SyntaxShap: Syntax-aware Explainability Method for Text Generation,No.,1,"""No evidence""",2024,2024-02-14T15:45:56Z,,,
arxiv2024,Scaling the Authoring of AutoTutors with Large Language Models,No.,1,"""No evidence""",2024,2024-02-14T14:53:56Z,,,
arxiv2024,MPIrigen: MPI Code Generation through Domain-Specific Language Models,No.,1,"""No evidence""",2024,2024-02-14T12:24:21Z,,,
arxiv2024,Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues,No.,1,"""No evidence""",2024,2024-02-14T11:11:51Z,,,
arxiv2024,L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects,No.,1,"""No evidence""",2024,2024-02-14T09:51:05Z,,,
arxiv2024,FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems,No.,1,"""No evidence""",2024,2024-02-14T09:44:28Z,,,
arxiv2024,SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks,No.,1,"""No evidence""",2024,2024-02-14T09:01:13Z,,,
arxiv2024,Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications,No.,1,"""No evidence""",2024,2024-02-14T08:46:15Z,,,
arxiv2024,Multi-Query Focused Disaster Summarization via Instruction-Based Prompting,No.,1,"""No evidence""",2024,2024-02-14T08:22:58Z,,,
arxiv2024,Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision,No.,1,"""No evidence""",2024,2024-02-14T06:01:44Z,,,
arxiv2024,MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data,No.,1,"""No evidence""",2024,2024-02-14T05:57:58Z,,,
arxiv2024,MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences,No.,1,"""No evidence""",2024,2024-02-14T03:56:27Z,,,
arxiv2024,Large Language Model with Graph Convolution for Recommendation,No.,1,"""No evidence""",2024,2024-02-14T00:04:33Z,,,
arxiv2024,GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency,No.,1,"""No evidence""",2024,2024-02-13T23:48:59Z,,,
arxiv2024,Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy,No.,1,"""No evidence""",2024,2024-02-13T21:24:21Z,,,
arxiv2024,"ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions",No.,1,"""No evidence""",2024,2024-02-13T21:15:33Z,,,
arxiv2024,Rethinking Machine Unlearning for Large Language Models,No.,1,"""No evidence""",2024,2024-02-13T20:51:58Z,,,
arxiv2024,"GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",No.,1,"""No evidence""",2024,2024-02-13T20:16:29Z,,,
arxiv2024,Measuring and Controlling Instruction (In)Stability in Language Model Dialogs,No.,1,"""No evidence""",2024,2024-02-13T20:10:29Z,,,
arxiv2024,JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models,No.,1,"""No evidence""",2024,2024-02-13T19:54:29Z,,,
arxiv2024,Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance,No.,1,"""No evidence""",2024,2024-02-13T18:59:05Z,,,
arxiv2024,COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability,No.,1,"""No evidence""",2024,2024-02-13T18:58:48Z,,,
arxiv2024,Human Curriculum Effects Emerge with In-Context Learning in Neural Networks,No.,1,"""No evidence""",2024,2024-02-13T18:55:27Z,,,
arxiv2024,Improving Generalization in Semantic Parsing by Increasing Natural Language Variation,No.,1,"""No evidence""",2024,2024-02-13T18:48:23Z,,,
arxiv2024,The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting,No.,1,"""No evidence""",2024,2024-02-13T18:39:36Z,,,
arxiv2024,SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages,No.,1,"""No evidence""",2024,2024-02-13T18:04:53Z,,,
arxiv2024,PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment,No.,1,"""No evidence""",2024,2024-02-13T16:38:01Z,,,
arxiv2024,Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast,No.,1,"""No evidence""",2024,2024-02-13T16:06:17Z,,,
arxiv2024,The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale,No.,1,"""No evidence""",2024,2024-02-13T14:38:12Z,,,
arxiv2024,Large Language Models as Minecraft Agents,No.,1,"""No evidence""",2024,2024-02-13T11:37:30Z,,,
arxiv2024,Unsupervised Evaluation of Code LLMs with Round-Trip Correctness,No.,1,"""No evidence""",2024,2024-02-13T11:08:08Z,,,
arxiv2024,Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries,No.,1,"""No evidence""",2024,2024-02-13T10:28:57Z,,,
arxiv2024,Visually Dehallucinative Instruction Generation,No.,1,"""No evidence""",2024,2024-02-13T10:25:45Z,,,
arxiv2024,Eliciting Personality Traits in Large Language Models,No.,1,"""No evidence""",2024,2024-02-13T10:09:00Z,,,
arxiv2024,Improving Black-box Robustness with In-Context Rewriting,No.,1,"""No evidence""",2024,2024-02-13T05:33:35Z,,,
arxiv2024,LLaGA: Large Language and Graph Assistant,No.,1,"""No evidence""",2024,2024-02-13T02:03:26Z,,,
arxiv2024,Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search,No.,1,"""No evidence""",2024,2024-02-13T00:55:14Z,,,
arxiv2024,On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era,No.,1,"""No evidence""",2024,2024-02-12T23:55:55Z,,,
arxiv2024,Beyond LLMs: Advancing the Landscape of Complex Reasoning,No.,1,"""No evidence""",2024,2024-02-12T21:14:45Z,,,
arxiv2024,Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs,No.,1,"""No evidence""",2024,2024-02-12T19:10:13Z,,,
arxiv2024,WildfireGPT: Tailored Large Language Model for Wildfire Analysis,No.,1,"""No evidence""",2024,2024-02-12T18:41:55Z,,,
arxiv2024,Policy Improvement using Language Feedback Models,No.,1,"""No evidence""",2024,2024-02-12T18:41:34Z,,,
arxiv2024,AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy,No.,1,"""No evidence""",2024,2024-02-12T18:14:43Z,,,
arxiv2024,Retrieval-Augmented Thought Process as Sequential Decision Making,No.,1,"""No evidence""",2024,2024-02-12T17:17:50Z,,,
arxiv2024,Quantitative knowledge retrieval from large language models,No.,1,"""No evidence""",2024,2024-02-12T16:32:37Z,,,
arxiv2024,AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension,No.,1,"""No evidence""",2024,2024-02-12T15:41:22Z,,,
arxiv2024,CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity,No.,1,"""No evidence""",2024,2024-02-12T14:53:28Z,,,
arxiv2024,The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models,No.,1,"""No evidence""",2024,2024-02-12T14:01:12Z,,,
arxiv2024,Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models,No.,1,"""No evidence""",2024,2024-02-12T13:34:33Z,,,
arxiv2024,G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,No.,1,"""No evidence""",2024,2024-02-12T13:13:04Z,,,
arxiv2024,Anchor-based Large Language Models,No.,1,"""No evidence""",2024,2024-02-12T12:48:02Z,,,
arxiv2024,Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping,No.,1,"""No evidence""",2024,2024-02-12T12:30:42Z,,,
arxiv2024,BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection,No.,1,"""No evidence""",2024,2024-02-12T10:04:07Z,,,
arxiv2024,T-RAG: Lessons from the LLM Trenches,No.,1,"""No evidence""",2024,2024-02-12T08:45:08Z,,,
arxiv2024,Pushing The Limit of LLM Capacity for Text Classification,No.,1,"""No evidence""",2024,2024-02-12T08:14:03Z,,,
arxiv2024,Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT,No.,1,"""No evidence""",2024,2024-02-12T06:43:52Z,,,
arxiv2024,Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples,No.,1,"""No evidence""",2024,2024-02-12T04:59:58Z,,,
arxiv2024,Dlares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English,No.,1,"""No evidence""",2024,2024-02-12T04:50:31Z,,,
arxiv2024,Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate,No.,1,"""No evidence""",2024,2024-02-12T04:32:33Z,,,
arxiv2024,Differentially Private Training of Mixture of Experts Models,No.,1,"""No evidence""",2024,2024-02-11T23:57:09Z,,,
arxiv2024,Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,No.,1,"""No evidence""",2024,2024-02-11T22:58:49Z,,,
arxiv2024,How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?,No.,1,"""No evidence""",2024,2024-02-11T19:13:26Z,,,
arxiv2024,TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation,No.,1,"""No evidence""",2024,2024-02-11T15:50:35Z,,,
arxiv2024,Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models,No.,1,"""No evidence""",2024,2024-02-11T13:41:17Z,,,
arxiv2024,GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,No.,1,"""No evidence""",2024,2024-02-11T13:24:13Z,,,
arxiv2024,Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models,No.,1,"""No evidence""",2024,2024-02-11T12:25:41Z,,,
arxiv2024,Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy,No.,1,"""No evidence""",2024,2024-02-11T11:24:09Z,,,
arxiv2024,Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias,No.,1,"""No evidence""",2024,2024-02-11T11:23:28Z,,,
arxiv2024,Natural Language Reinforcement Learning,No.,1,"""No evidence""",2024,2024-02-11T11:03:04Z,,,
arxiv2024,Graph Descriptive Order Improves Reasoning with Large Language Model,No.,1,"""No evidence""",2024,2024-02-11T09:46:24Z,,,
arxiv2024,Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education,No.,1,"""No evidence""",2024,2024-02-11T01:37:48Z,,,
arxiv2024,A Tale of Tails: Model Collapse as a Change of Scaling Laws,No.,1,"""No evidence""",2024,2024-02-10T21:06:34Z,,,
arxiv2024,Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models,No.,1,"""No evidence""",2024,2024-02-10T19:54:08Z,,,
arxiv2024,REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models,No.,1,"""No evidence""",2024,2024-02-10T18:27:28Z,,,
arxiv2024,DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting,No.,1,"""No evidence""",2024,2024-02-10T16:48:45Z,,,
arxiv2024,A Thorough Examination of Decoding Methods in the Era of LLMs,No.,1,"""No evidence""",2024,2024-02-10T11:14:53Z,,,
arxiv2024,Whispers in the Machine: Confidentiality in LLM-integrated Systems,No.,1,"""No evidence""",2024,2024-02-10T11:07:24Z,,,
arxiv2024,UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction,No.,1,"""No evidence""",2024,2024-02-10T01:50:19Z,,,
arxiv2024,"History, Development, and Principles of Large Language Models-An Introductory Survey",No.,1,"""No evidence""",2024,2024-02-10T01:18:15Z,,,
arxiv2024,ChemLLM: A Chemical Large Language Model,No.,1,"""No evidence""",2024,2024-02-10T01:11:59Z,,,
arxiv2024,The Unreasonable Effectiveness of Eccentric Automatic Prompts,No.,1,"""No evidence""",2024,2024-02-09T22:48:45Z,,,
arxiv2024,Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models,No.,1,"""No evidence""",2024,2024-02-09T22:47:25Z,,,
arxiv2024,Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing,No.,1,"""No evidence""",2024,2024-02-09T21:37:13Z,,,
arxiv2024,Debating with More Persuasive LLMs Leads to More Truthful Answers,No.,1,"""No evidence""",2024,2024-02-09T21:05:01Z,,,
arxiv2024,EntGPT: Linking Generative Large Language Models with Knowledge Bases,No.,1,"""No evidence""",2024,2024-02-09T19:16:27Z,,,
arxiv2024,If Turing played piano with an artificial partner,No.,1,"""No evidence""",2024,2024-02-09T18:43:48Z,,,
arxiv2024,Understanding the Weakness of Large Language Model Agents within a Complex Android Environment,No.,1,"""No evidence""",2024,2024-02-09T18:19:25Z,,,
arxiv2024,G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German,No.,1,"""No evidence""",2024,2024-02-09T18:05:03Z,,,
arxiv2024,Large Language Models for Captioning and Retrieving Remote Sensing Images,No.,1,"""No evidence""",2024,2024-02-09T15:31:01Z,,,
arxiv2024,CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models,No.,1,"""No evidence""",2024,2024-02-09T12:10:00Z,,,
arxiv2024,RareBench: Can LLMs Serve as Rare Diseases Specialists?,No.,1,"""No evidence""",2024,2024-02-09T11:34:16Z,,,
arxiv2024,ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs,No.,1,"""No evidence""",2024,2024-02-09T11:23:14Z,,,
arxiv2024,InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning,No.,1,"""No evidence""",2024,2024-02-09T11:22:08Z,,,
arxiv2024,LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education,No.,1,"""No evidence""",2024,2024-02-09T09:25:18Z,,,
arxiv2024,Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning,No.,1,"""No evidence""",2024,2024-02-09T09:09:39Z,,,
arxiv2024,Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants,No.,1,"""No evidence""",2024,2024-02-09T07:44:27Z,,,
arxiv2024,Large Language Models: A Survey,No.,1,"""No evidence""",2024,2024-02-09T05:37:09Z,,,
arxiv2024,ContPhy: Continuum Physical Concept Learning and Reasoning from Videos,No.,1,"""No evidence""",2024,2024-02-09T01:09:21Z,,,
arxiv2024,ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling,No.,1,"""No evidence""",2024,2024-02-09T01:00:14Z,,,
arxiv2024,LLMs for Coding and Robotics Education,No.,1,"""No evidence""",2024,2024-02-09T00:58:57Z,,,
arxiv2024,Large Language Model Augmented Exercise Retrieval for Personalized Language Learning,No.,1,"""No evidence""",2024,2024-02-08T20:35:31Z,,,
arxiv2024,A Prompt Response to the Demand for Automatic Gender-Neutral Translation,No.,1,"""No evidence""",2024,2024-02-08T20:24:44Z,,,
arxiv2024,LLMs Among Us: Generative AI Participating in Digital Discourse,No.,1,"""No evidence""",2024,2024-02-08T19:21:33Z,,,
arxiv2024,WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,No.,1,"""No evidence""",2024,2024-02-08T18:58:02Z,,,
arxiv2024,On the Convergence of Zeroth-Order Federated Tuning for Large Language Models,No.,1,"""No evidence""",2024,2024-02-08T18:56:40Z,,,
arxiv2024,CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion,No.,1,"""No evidence""",2024,2024-02-08T18:27:22Z,,,
arxiv2024,How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis,No.,1,"""No evidence""",2024,2024-02-08T17:51:48Z,,,
arxiv2024,Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning,No.,1,"""No evidence""",2024,2024-02-08T16:46:26Z,,,
arxiv2024,Text-to-Code Generation with Modality-relative Pre-training,No.,1,"""No evidence""",2024,2024-02-08T16:17:24Z,,,
arxiv2024,Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images,No.,1,"""No evidence""",2024,2024-02-08T16:11:23Z,,,
arxiv2024,Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation,No.,1,"""No evidence""",2024,2024-02-08T14:21:03Z,,,
arxiv2024,"Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks",No.,1,"""No evidence""",2024,2024-02-08T13:07:31Z,,,
arxiv2024,Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design,No.,1,"""No evidence""",2024,2024-02-08T13:02:05Z,,,
arxiv2024,The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment,No.,1,"""No evidence""",2024,2024-02-08T12:47:57Z,,,
arxiv2024,Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset,No.,1,"""No evidence""",2024,2024-02-08T10:32:06Z,,,
arxiv2024,Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia,No.,1,"""No evidence""",2024,2024-02-08T07:56:49Z,,,
arxiv2024,It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition,No.,1,"""No evidence""",2024,2024-02-08T07:21:45Z,,,
arxiv2024,Large Language Models for Psycholinguistic Plausibility Pretesting,No.,1,"""No evidence""",2024,2024-02-08T07:20:02Z,,,
arxiv2024,Accurate LoRA-Finetuning Quantization of LLMs via Information Retention,No.,1,"""No evidence""",2024,2024-02-08T06:53:31Z,,,
arxiv2024,Do Large Code Models Understand Programming Concepts? A Black-box Approach,No.,1,"""No evidence""",2024,2024-02-08T06:48:01Z,,,
arxiv2024,GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study,No.,1,"""No evidence""",2024,2024-02-08T06:20:01Z,,,
arxiv2024,In-Context Principle Learning from Mistakes,No.,1,"""No evidence""",2024,2024-02-08T04:42:29Z,,,
arxiv2024,Enhancing Zero-shot Counting via Language-guided Exemplar Learning,No.,1,"""No evidence""",2024,2024-02-08T04:07:38Z,,,
arxiv2024,$?$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space,No.,1,"""No evidence""",2024,2024-02-07T19:07:10Z,,,
arxiv2024,InCoRo: In-Context Learning for Robotics Control with Feedback Loops,No.,1,"""No evidence""",2024,2024-02-07T19:01:11Z,,,
arxiv2024,Opening the AI black box: program synthesis via mechanistic interpretability,No.,1,"""No evidence""",2024,2024-02-07T18:59:12Z,,,
arxiv2024,Hydragen: High-Throughput LLM Inference with Shared Prefixes,No.,1,"""No evidence""",2024,2024-02-07T18:53:01Z,,,
arxiv2024,Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation,No.,1,"""No evidence""",2024,2024-02-07T18:44:27Z,,,
arxiv2024,SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,No.,1,"""No evidence""",2024,2024-02-07T17:33:54Z,,,
arxiv2024,A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?,No.,1,"""No evidence""",2024,2024-02-07T16:32:58Z,,,
arxiv2024,Pedagogical Alignment of Large Language Models,No.,1,"""No evidence""",2024,2024-02-07T16:15:59Z,,,
arxiv2024,ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12,No.,1,"""No evidence""",2024,2024-02-07T15:55:51Z,,,
arxiv2024,Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems,No.,1,"""No evidence""",2024,2024-02-07T15:39:07Z,,,
arxiv2024,Prompting Implicit Discourse Relation Annotation,No.,1,"""No evidence""",2024,2024-02-07T14:44:42Z,,,
arxiv2024,Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning,No.,1,"""No evidence""",2024,2024-02-07T13:32:11Z,,,
arxiv2024,Direct Language Model Alignment from Online AI Feedback,No.,1,"""No evidence""",2024,2024-02-07T12:31:13Z,,,
arxiv2024,A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models,No.,1,"""No evidence""",2024,2024-02-07T12:26:12Z,,,
arxiv2024,ApiQ: Finetuning of 2-Bit Quantized Large Language Model,No.,1,"""No evidence""",2024,2024-02-07T09:36:54Z,,,
arxiv2024,The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends,No.,1,"""No evidence""",2024,2024-02-07T07:28:34Z,,,
arxiv2024,MEMORYLLM: Towards Self-Updatable Large Language Models,No.,1,"""No evidence""",2024,2024-02-07T07:14:11Z,,,
arxiv2024,CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients,No.,1,"""No evidence""",2024,2024-02-07T07:07:02Z,,,
arxiv2024,"The Role of LLMs in Sustainable Smart Cities: Applications, Challenges, and Future Directions",No.,1,"""No evidence""",2024,2024-02-07T05:22:10Z,,,
arxiv2024,Can Large Language Model Agents Simulate Human Trust Behaviors?,No.,1,"""No evidence""",2024,2024-02-07T03:37:19Z,,,
arxiv2024,Grandmaster-Level Chess Without Search,No.,1,"""No evidence""",2024,2024-02-07T00:36:24Z,,,
arxiv2024,Detecting Mode Collapse in Language Models via Narration,No.,1,"""No evidence""",2024,2024-02-06T23:52:58Z,,,
arxiv2024,Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton,No.,1,"""No evidence""",2024,2024-02-06T21:14:45Z,,,
arxiv2024,Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning,No.,1,"""No evidence""",2024,2024-02-06T21:03:52Z,,,
arxiv2024,Fine-Tuned Language Models Generate Stable Inorganic Materials as Text,No.,1,"""No evidence""",2024,2024-02-06T20:35:28Z,,,
arxiv2024,Monitoring the evolution of antisemitic discourse on extremist social media using BERT,No.,1,"""No evidence""",2024,2024-02-06T20:34:49Z,,,
arxiv2024,The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry,No.,1,"""No evidence""",2024,2024-02-06T19:31:26Z,,,
arxiv2024,Can Generative Agents Predict Emotion?,No.,1,"""No evidence""",2024,2024-02-06T18:39:43Z,,,
arxiv2024,Scaling Laws for Downstream Task Performance of Large Language Models,No.,1,"""No evidence""",2024,2024-02-06T17:31:20Z,,,
arxiv2024,Harnessing the Plug-and-Play Controller by Prompting,No.,1,"""No evidence""",2024,2024-02-06T17:18:25Z,,,
arxiv2024,Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs),No.,1,"""No evidence""",2024,2024-02-06T16:47:34Z,,,
arxiv2024,Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science,No.,1,"""No evidence""",2024,2024-02-06T16:12:36Z,,,
arxiv2024,The Use of a Large Language Model for Cyberbullying Detection,No.,1,"""No evidence""",2024,2024-02-06T15:46:31Z,,,
arxiv2024,Provably learning a multi-head attention layer,No.,1,"""No evidence""",2024,2024-02-06T15:39:09Z,,,
arxiv2024,Enhancing Retrieval Processes for Language Generation with Augmented Queries,No.,1,"""No evidence""",2024,2024-02-06T13:19:53Z,,,
arxiv2024,Discovery of the Hidden World with Large Language Models,No.,1,"""No evidence""",2024,2024-02-06T12:18:54Z,,,
arxiv2024,Large Language Models to Enhance Bayesian Optimization,No.,1,"""No evidence""",2024,2024-02-06T11:44:06Z,,,
arxiv2024,DistiLLM: Towards Streamlined Distillation for Large Language Models,No.,1,"""No evidence""",2024,2024-02-06T11:10:35Z,,,
arxiv2024,ANLS* -- A Universal Document Processing Metric for Generative Large Language Models,No.,1,"""No evidence""",2024,2024-02-06T09:50:08Z,,,
arxiv2024,RevOrder: A Novel Method for Enhanced Arithmetic in Language Models,No.,1,"""No evidence""",2024,2024-02-06T09:10:35Z,,,
arxiv2024,ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs,No.,1,"""No evidence""",2024,2024-02-06T08:45:51Z,,,
arxiv2024,Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning,No.,1,"""No evidence""",2024,2024-02-06T07:53:23Z,,,
arxiv2024,MolTC: Towards Molecular Relational Modeling In Language Models,No.,1,"""No evidence""",2024,2024-02-06T07:51:56Z,,,
arxiv2024,Large Language Models As MOOCs Graders,No.,1,"""No evidence""",2024,2024-02-06T07:43:07Z,,,
arxiv2024,The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs,No.,1,"""No evidence""",2024,2024-02-06T06:48:46Z,,,
arxiv2024,Similarity-based Neighbor Selection for Graph LLMs,No.,1,"""No evidence""",2024,2024-02-06T05:29:05Z,,,
arxiv2024,Personalized Language Modeling from Personalized Human Feedback,No.,1,"""No evidence""",2024,2024-02-06T04:18:58Z,,,
arxiv2024,Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning,No.,1,"""No evidence""",2024,2024-02-06T03:41:12Z,,,
arxiv2024,Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue,No.,1,"""No evidence""",2024,2024-02-06T03:14:46Z,,,
arxiv2024,Partially Recentralization Softmax Loss for Vision-Language Models Robustness,No.,1,"""No evidence""",2024,2024-02-06T01:44:38Z,,,
arxiv2024,Self-Discover: Large Language Models Self-Compose Reasoning Structures,No.,1,"""No evidence""",2024,2024-02-06T01:13:53Z,,,
arxiv2024,Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning,No.,1,"""No evidence""",2024,2024-02-06T00:51:27Z,,,
arxiv2024,A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications,No.,1,"""No evidence""",2024,2024-02-05T19:49:13Z,,,
arxiv2024,Arabic Synonym BERT-based Adversarial Examples for Text Classification,No.,1,"""No evidence""",2024,2024-02-05T19:39:07Z,,,
arxiv2024,Nevermind: Instruction Override and Moderation in Large Language Models,No.,1,"""No evidence""",2024,2024-02-05T18:58:19Z,,,
arxiv2024,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,No.,1,"""No evidence""",2024,2024-02-05T18:55:32Z,,,
arxiv2024,GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models,No.,1,"""No evidence""",2024,2024-02-05T18:54:43Z,,,
arxiv2024,"Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models",No.,1,"""No evidence""",2024,2024-02-05T18:39:47Z,,,
arxiv2024,A Framework for Partially Observed Reward-States in RLHF,No.,1,"""No evidence""",2024,2024-02-05T18:38:55Z,,,
arxiv2024,MobilityGPT: Enhanced Human Mobility Modeling with a GPT model,No.,1,"""No evidence""",2024,2024-02-05T18:22:21Z,,,
arxiv2024,English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts,No.,1,"""No evidence""",2024,2024-02-05T17:36:19Z,,,
arxiv2024,C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models,No.,1,"""No evidence""",2024,2024-02-05T16:46:16Z,,,
arxiv2024,The Matrix: A Bayesian learning model for LLMs,No.,1,"""No evidence""",2024,2024-02-05T16:42:10Z,,,
arxiv2024,MULTI: Multimodal Understanding Leaderboard with Text and Images,No.,1,"""No evidence""",2024,2024-02-05T16:41:02Z,,,
arxiv2024,Homograph Attacks on Maghreb Sentiment Analyzers,No.,1,"""No evidence""",2024,2024-02-05T16:39:15Z,,,
arxiv2024,Constrained Decoding for Cross-lingual Label Projection,No.,1,"""No evidence""",2024,2024-02-05T15:57:32Z,,,
arxiv2024,Evaluation of ChatGPT Usability as A Code Generation Tool,No.,1,"""No evidence""",2024,2024-02-05T15:56:19Z,,,
arxiv2024,Best Practices for Text Annotation with Large Language Models,No.,1,"""No evidence""",2024,2024-02-05T15:43:50Z,,,
arxiv2024,Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases,No.,1,"""No evidence""",2024,2024-02-05T15:28:43Z,,,
arxiv2024,UniMem: Towards a Unified View of Long-Context Large Language Models,No.,1,"""No evidence""",2024,2024-02-05T13:47:53Z,,,
arxiv2024,Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing,No.,1,"""No evidence""",2024,2024-02-05T13:16:12Z,,,
arxiv2024,LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models,No.,1,"""No evidence""",2024,2024-02-05T11:05:20Z,,,
arxiv2024,Shortened LLaMA: A Simple Depth Pruning for Large Language Models,No.,1,"""No evidence""",2024,2024-02-05T09:44:49Z,,,
arxiv2024,Large Language Model Distilling Medication Recommendation Model,No.,1,"""No evidence""",2024,2024-02-05T08:25:22Z,,,
arxiv2024,KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models,No.,1,"""No evidence""",2024,2024-02-05T08:19:56Z,,,
arxiv2024,Rethinking Optimization and Architecture for Tiny Language Models,No.,1,"""No evidence""",2024,2024-02-05T07:59:38Z,,,
arxiv2024,List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation,No.,1,"""No evidence""",2024,2024-02-05T06:52:53Z,,,
arxiv2024,Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering,No.,1,"""No evidence""",2024,2024-02-05T06:08:06Z,,,
arxiv2024,Understanding the planning of LLM agents: A survey,No.,1,"""No evidence""",2024,2024-02-05T04:25:24Z,,,
arxiv2024,Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases,No.,1,"""No evidence""",2024,2024-02-05T01:59:31Z,,,
arxiv2024,Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting,No.,1,"""No evidence""",2024,2024-02-05T00:44:28Z,,,
arxiv2024,UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing,No.,1,"""No evidence""",2024,2024-02-04T22:48:05Z,,,
arxiv2024,A Truly Joint Neural Architecture for Segmentation and Parsing,No.,1,"""No evidence""",2024,2024-02-04T16:56:08Z,,,
arxiv2024,DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models,No.,1,"""No evidence""",2024,2024-02-04T16:45:01Z,,,
arxiv2024,Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials,No.,1,"""No evidence""",2024,2024-02-04T16:18:01Z,,,
arxiv2024,Are Large Language Models Table-based Fact-Checkers?,No.,1,"""No evidence""",2024,2024-02-04T15:52:59Z,,,
arxiv2024,Conversational Crowdsensing: A Parallel Intelligence Powered Novel Sensing Approach,No.,1,"""No evidence""",2024,2024-02-04T15:10:11Z,,,
arxiv2024,Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation,No.,1,"""No evidence""",2024,2024-02-04T13:21:19Z,,,
arxiv2024,BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback,No.,1,"""No evidence""",2024,2024-02-04T13:16:29Z,,,
arxiv2024,A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer,No.,1,"""No evidence""",2024,2024-02-04T12:29:40Z,,,
arxiv2024,FoldToken: Learning Protein Language via Vector Quantization and Beyond,No.,1,"""No evidence""",2024,2024-02-04T12:18:51Z,,,
arxiv2024,Breaking MLPerf Training: A Case Study on Optimizing BERT,No.,1,"""No evidence""",2024,2024-02-04T11:12:17Z,,,
arxiv2024,LQER: Low-Rank Quantization Error Reconstruction for LLMs,No.,1,"""No evidence""",2024,2024-02-04T10:59:52Z,,,
arxiv2024,Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction,No.,1,"""No evidence""",2024,2024-02-04T09:24:51Z,,,
arxiv2024,Evaluating Large Language Models in Analysing Classroom Dialogue,No.,1,"""No evidence""",2024,2024-02-04T07:39:06Z,,,
arxiv2024,AutoTimes: Autoregressive Time Series Forecasters via Large Language Models,No.,1,"""No evidence""",2024,2024-02-04T06:59:21Z,,,
arxiv2024,Timer: Transformers for Time Series Analysis at Scale,No.,1,"""No evidence""",2024,2024-02-04T06:55:55Z,,,
arxiv2024,Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques,No.,1,"""No evidence""",2024,2024-02-04T05:51:14Z,,,
arxiv2024,Large Language Model Adaptation for Networking,No.,1,"""No evidence""",2024,2024-02-04T04:21:34Z,,,
arxiv2024,Enhance Reasoning for Large Language Models in the Game Werewolf,No.,1,"""No evidence""",2024,2024-02-04T03:47:10Z,,,
arxiv2024,Selecting Large Language Model to Fine-tune via Rectified Scaling Law,No.,1,"""No evidence""",2024,2024-02-04T01:55:00Z,,,
arxiv2024,Jailbreaking Attack against Multimodal Large Language Model,No.,1,"""No evidence""",2024,2024-02-04T01:29:24Z,,,
arxiv2024,Large Language Model for Table Processing: A Survey,No.,1,"""No evidence""",2024,2024-02-04T00:47:53Z,,,
arxiv2024,"Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding",No.,1,"""No evidence""",2024,2024-02-03T19:19:34Z,,,
arxiv2024,GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events,No.,1,"""No evidence""",2024,2024-02-03T16:38:25Z,,,
arxiv2024,Analyzing Sentiment Polarity Reduction in News Presentation through Contextual Perturbation and Large Language Models,No.,1,"""No evidence""",2024,2024-02-03T13:27:32Z,,,
arxiv2024,Rendering Graphs for Graph Reasoning in Multimodal Large Language Models,No.,1,"""No evidence""",2024,2024-02-03T12:19:47Z,,,
arxiv2024,Break the Sequential Dependency of LLM Inference Using Lookahead Decoding,No.,1,"""No evidence""",2024,2024-02-03T06:37:50Z,,,
arxiv2024,Panacea: Pareto Alignment via Preference Adaptation for LLMs,No.,1,"""No evidence""",2024,2024-02-03T05:01:04Z,,,
arxiv2024,"PresAIse, A Prescriptive AI Solution for Enterprises",No.,1,"""No evidence""",2024,2024-02-03T03:23:08Z,,,
arxiv2024,SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks,No.,1,"""No evidence""",2024,2024-02-03T01:33:16Z,,,
arxiv2024,"A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions",No.,1,"""No evidence""",2024,2024-02-03T00:27:22Z,,,
arxiv2024,MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles,No.,1,"""No evidence""",2024,2024-02-03T00:23:36Z,,,
arxiv2024,Large Language Model Agent for Hyper-Parameter Optimization,No.,1,"""No evidence""",2024,2024-02-02T20:12:05Z,,,
arxiv2024,The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models,No.,1,"""No evidence""",2024,2024-02-02T20:01:15Z,,,
arxiv2024,Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision,No.,1,"""No evidence""",2024,2024-02-02T19:45:39Z,,,
arxiv2024,What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement,No.,1,"""No evidence""",2024,2024-02-02T19:43:15Z,,,
arxiv2024,"(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",No.,1,"""No evidence""",2024,2024-02-02T19:35:34Z,,,
arxiv2024,Cross-modality debiasing: using language to mitigate sub-population shifts in imaging,No.,1,"""No evidence""",2024,2024-02-02T18:54:48Z,,,
arxiv2024,Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment,No.,1,"""No evidence""",2024,2024-02-02T18:49:26Z,,,
arxiv2024,Stochastic Two Points Method for Deep Model Zeroth-order Optimization,No.,1,"""No evidence""",2024,2024-02-02T18:39:40Z,,,
arxiv2024,MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models,No.,1,"""No evidence""",2024,2024-02-02T18:35:14Z,,,
arxiv2024,Style Vectors for Steering Generative Large Language Model,No.,1,"""No evidence""",2024,2024-02-02T18:31:15Z,,,
arxiv2024,Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature,No.,1,"""No evidence""",2024,2024-02-02T18:15:51Z,,,
arxiv2024,TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution,No.,1,"""No evidence""",2024,2024-02-02T17:26:23Z,,,
arxiv2024,Building Guardrails for Large Language Models,No.,1,"""No evidence""",2024,2024-02-02T16:35:00Z,,,
arxiv2024,Ecologically rational meta-learned inference explains human category learning,No.,1,"""No evidence""",2024,2024-02-02T16:32:04Z,,,
arxiv2024,An Empirical Analysis of Diversity in Argument Summarization,No.,1,"""No evidence""",2024,2024-02-02T16:26:52Z,,,
arxiv2024,K-Level Reasoning with Large Language Models,No.,1,"""No evidence""",2024,2024-02-02T16:07:05Z,,,
arxiv2024,AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback,No.,1,"""No evidence""",2024,2024-02-02T14:56:48Z,,,
arxiv2024,A Survey on Large Language Model Hallucination via a Creativity Perspective,No.,1,"""No evidence""",2024,2024-02-02T12:21:04Z,,,
arxiv2024,Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion,No.,1,"""No evidence""",2024,2024-02-02T11:57:50Z,,,
arxiv2024,KTO: Model Alignment as Prospect Theoretic Optimization,No.,1,"""No evidence""",2024,2024-02-02T10:53:36Z,,,
arxiv2024,Efficient Causal Graph Discovery Using Large Language Models,No.,1,"""No evidence""",2024,2024-02-02T08:25:32Z,,,
arxiv2024,A Multi-Agent Conversational Recommender System,No.,1,"""No evidence""",2024,2024-02-02T04:20:13Z,,,
arxiv2024,PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models,No.,1,"""No evidence""",2024,2024-02-02T03:22:12Z,,,
arxiv2024,"Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions",No.,1,"""No evidence""",2024,2024-02-02T02:53:11Z,,,
arxiv2024,The Political Preferences of LLMs,No.,1,"""No evidence""",2024,2024-02-02T02:43:10Z,,,
arxiv2024,Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs,No.,1,"""No evidence""",2024,2024-02-02T02:11:31Z,,,
arxiv2024,Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities,No.,1,"""No evidence""",2024,2024-02-02T00:16:45Z,,,
arxiv2024,Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer,No.,1,"""No evidence""",2024,2024-02-01T23:46:05Z,,,
arxiv2024,Plan-Grounded Large Language Models for Dual Goal Conversational Settings,No.,1,"""No evidence""",2024,2024-02-01T22:56:39Z,,,
arxiv2024,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",No.,1,"""No evidence""",2024,2024-02-01T22:54:31Z,,,
arxiv2024,IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition,No.,1,"""No evidence""",2024,2024-02-01T22:37:33Z,,,
arxiv2024,Getting the most out of your tokenizer for pre-training and domain adaptation,No.,1,"""No evidence""",2024,2024-02-01T21:49:34Z,,,
arxiv2024,Executable Code Actions Elicit Better LLM Agents,No.,1,"""No evidence""",2024,2024-02-01T21:38:58Z,,,
arxiv2024,HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent,No.,1,"""No evidence""",2024,2024-02-01T21:10:44Z,,,
arxiv2024,Evaluating Large Language Models for Generalization and Robustness via Data Compression,No.,1,"""No evidence""",2024,2024-02-01T18:56:18Z,,,
arxiv2024,"LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",No.,1,"""No evidence""",2024,2024-02-01T17:28:10Z,,,
arxiv2024,Unlearnable Algorithms for In-context Learning,No.,1,"""No evidence""",2024,2024-02-01T16:43:04Z,,,
arxiv2024,Health-LLM: Personalized Retrieval-Augmented Disease Prediction System,No.,1,"""No evidence""",2024,2024-02-01T16:40:32Z,,,
arxiv2024,Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement,No.,1,"""No evidence""",2024,2024-02-01T16:39:51Z,,,
arxiv2024,Transforming and Combining Rewards for Aligning Large Language Models,No.,1,"""No evidence""",2024,2024-02-01T16:39:28Z,,,
arxiv2024,Intent Assurance using LLMs guided by Intent Drift,No.,1,"""No evidence""",2024,2024-02-01T16:09:19Z,,,
arxiv2024,Ocassionally Secure: A Comparative Analysis of Code Generation Assistants,No.,1,"""No evidence""",2024,2024-02-01T15:49:47Z,,,
arxiv2024,Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning,No.,1,"""No evidence""",2024,2024-02-01T11:57:53Z,,,
arxiv2024,SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models,No.,1,"""No evidence""",2024,2024-02-01T10:26:27Z,,,
arxiv2024,From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models,No.,1,"""No evidence""",2024,2024-02-01T08:37:13Z,,,
arxiv2024,What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection,No.,1,"""No evidence""",2024,2024-02-01T06:21:19Z,,,
arxiv2024,Safety of Multimodal Large Language Models on Images and Text,No.,1,"""No evidence""",2024,2024-02-01T05:57:10Z,,,
arxiv2024,Large Language Models Based Fuzzing Techniques: A Survey,No.,1,"""No evidence""",2024,2024-02-01T05:34:03Z,,,
arxiv2024,Multimodal Embodied Interactive Agent for Cafe Scene,No.,1,"""No evidence""",2024,2024-02-01T02:43:20Z,,,
arxiv2024,PAP-REC: Personalized Automatic Prompt for Recommendation Language Model,No.,1,"""No evidence""",2024,2024-02-01T02:29:16Z,,,
arxiv2024,Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective,No.,1,"""No evidence""",2024,2024-02-01T01:17:46Z,,,
arxiv2024,Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs,No.,1,"""No evidence""",2024,2024-02-01T01:09:00Z,,,
arxiv2024,Exploring the limits of decoder-only models trained on public speech recognition corpora,No.,1,"""No evidence""",2024,2024-01-31T23:29:42Z,,,
arxiv2024,Multimodal Clinical Pseudo-notes for Emergency Department Prediction Tasks using Multiple Embedding Model for EHR (MEME),No.,1,"""No evidence""",2024,2024-01-31T20:31:56Z,,,
arxiv2024,Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research,No.,1,"""No evidence""",2024,2024-01-31T20:29:50Z,,,
arxiv2024,Comparing Template-based and Template-free Language Model Probing,No.,1,"""No evidence""",2024,2024-01-31T19:07:37Z,,,
arxiv2024,KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization,No.,1,"""No evidence""",2024,2024-01-31T18:58:14Z,,,
arxiv2024,Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?,No.,1,"""No evidence""",2024,2024-01-31T18:48:20Z,,,
arxiv2024,LongAlign: A Recipe for Long Context Alignment of Large Language Models,No.,1,"""No evidence""",2024,2024-01-31T18:29:39Z,,,
arxiv2024,SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition,No.,1,"""No evidence""",2024,2024-01-31T18:06:29Z,,,
arxiv2024,EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation,No.,1,"""No evidence""",2024,2024-01-31T17:08:34Z,,,
arxiv2024,Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study,No.,1,"""No evidence""",2024,2024-01-31T16:38:32Z,,,
arxiv2024,Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance,No.,1,"""No evidence""",2024,2024-01-31T15:04:01Z,,,
arxiv2024,LLM Voting: Human Choices and AI Collective Decision Making,No.,1,"""No evidence""",2024,2024-01-31T14:52:02Z,,,
arxiv2024,"I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using AwareBench",No.,1,"""No evidence""",2024,2024-01-31T14:41:23Z,,,
arxiv2024,Probing Language Models' Gesture Understanding for Enhanced Human-AI Interaction,No.,1,"""No evidence""",2024,2024-01-31T14:19:03Z,,,
arxiv2024,ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation,No.,1,"""No evidence""",2024,2024-01-31T12:41:27Z,,,
arxiv2024,SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models,No.,1,"""No evidence""",2024,2024-01-31T11:14:29Z,,,
arxiv2024,Enhancing Large Language Model with Decomposed Reasoning for Emotion Cause Pair Extraction,No.,1,"""No evidence""",2024,2024-01-31T10:20:01Z,,,
arxiv2024,WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts,No.,1,"""No evidence""",2024,2024-01-31T09:49:22Z,,,
arxiv2024,Mitigating the Problem of Strong Priors in LMs with Context Extrapolation,No.,1,"""No evidence""",2024,2024-01-31T09:28:06Z,,,
arxiv2024,Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning,No.,1,"""No evidence""",2024,2024-01-31T09:16:35Z,,,
arxiv2024,Navigating the OverKill in Large Language Models,No.,1,"""No evidence""",2024,2024-01-31T07:26:47Z,,,
arxiv2024,"Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data",No.,1,"""No evidence""",2024,2024-01-31T04:57:12Z,,,
arxiv2024,Towards Visual Syntactical Understanding,No.,1,"""No evidence""",2024,2024-01-30T23:05:43Z,,,
arxiv2024,Detecting mental disorder on social media: a ChatGPT-augmented explainable approach,No.,1,"""No evidence""",2024,2024-01-30T22:22:55Z,,,
arxiv2024,Efficient Tool Use with Chain-of-Abstraction Reasoning,No.,1,"""No evidence""",2024,2024-01-30T21:53:30Z,,,
arxiv2024,A Preliminary Study on Using Large Language Models in Software Pentesting,No.,1,"""No evidence""",2024,2024-01-30T21:42:59Z,,,
arxiv2024,Can Large Language Models Replace Economic Choice Prediction Labs?,No.,1,"""No evidence""",2024,2024-01-30T20:49:47Z,,,
arxiv2024,Customizing Language Model Responses with Contrastive In-Context Learning,No.,1,"""No evidence""",2024,2024-01-30T19:13:12Z,,,
arxiv2024,Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens,No.,1,"""No evidence""",2024,2024-01-30T19:03:49Z,,,
arxiv2024,LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation,No.,1,"""No evidence""",2024,2024-01-30T18:37:45Z,,,
arxiv2024,MouSi: Poly-Visual-Expert Vision-Language Models,No.,1,"""No evidence""",2024,2024-01-30T18:09:11Z,,,
arxiv2024,Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers,No.,1,"""No evidence""",2024,2024-01-30T17:30:44Z,,,
arxiv2024,Transfer Learning for Text Diffusion Models,No.,1,"""No evidence""",2024,2024-01-30T17:11:56Z,,,
arxiv2024,Conditional and Modal Reasoning in Large Language Models,No.,1,"""No evidence""",2024,2024-01-30T16:56:54Z,,,
arxiv2024,"Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios",No.,1,"""No evidence""",2024,2024-01-30T16:52:56Z,,,
arxiv2024,Large Language Model Evaluation via Matrix Entropy,No.,1,"""No evidence""",2024,2024-01-30T16:19:55Z,,,
arxiv2024,"Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations",No.,1,"""No evidence""",2024,2024-01-30T15:44:14Z,,,
arxiv2024,SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity,No.,1,"""No evidence""",2024,2024-01-30T14:52:50Z,,,
arxiv2024,GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries,No.,1,"""No evidence""",2024,2024-01-30T14:47:15Z,,,
arxiv2024,Finetuning Large Language Models for Vulnerability Detection,No.,1,"""No evidence""",2024,2024-01-30T13:46:49Z,,,
arxiv2024,EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain,No.,1,"""No evidence""",2024,2024-01-30T08:57:48Z,,,
arxiv2024,Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia,No.,1,"""No evidence""",2024,2024-01-30T07:55:43Z,,,
arxiv2024,PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models,No.,1,"""No evidence""",2024,2024-01-30T06:55:17Z,,,
arxiv2024,A Cross-Language Investigation into Jailbreak Attacks in Large Language Models,No.,1,"""No evidence""",2024,2024-01-30T06:04:04Z,,,
arxiv2024,SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond the Memory Budget,No.,1,"""No evidence""",2024,2024-01-30T05:29:49Z,,,
arxiv2024,Security and Privacy Challenges of Large Language Models: A Survey,No.,1,"""No evidence""",2024,2024-01-30T04:00:54Z,,,
arxiv2024,T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives,No.,1,"""No evidence""",2024,2024-01-30T01:55:34Z,,,
arxiv2024,Gradient-Based Language Model Red Teaming,No.,1,"""No evidence""",2024,2024-01-30T01:19:25Z,,,
arxiv2024,Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs,No.,1,"""No evidence""",2024,2024-01-30T00:23:29Z,,,
arxiv2024,Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems,No.,1,"""No evidence""",2024,2024-01-30T00:21:41Z,,,
arxiv2024,Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports,No.,1,"""No evidence""",2024,2024-01-29T21:24:43Z,,,
arxiv2024,LLMs as On-demand Customizable Service,No.,1,"""No evidence""",2024,2024-01-29T21:24:10Z,,,
arxiv2024,"Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation",No.,1,"""No evidence""",2024,2024-01-29T20:50:28Z,,,
arxiv2024,InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification,No.,1,"""No evidence""",2024,2024-01-29T19:00:01Z,,,
arxiv2024,Scaling Sparse Fine-Tuning to Large Language Models,No.,1,"""No evidence""",2024,2024-01-29T18:43:49Z,,,
arxiv2024,PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology,No.,1,"""No evidence""",2024,2024-01-29T17:59:19Z,,,
arxiv2024,The Reasoning Under Uncertainty Trap: A Structural AI Risk,No.,1,"""No evidence""",2024,2024-01-29T17:16:57Z,,,
arxiv2024,Towards Optimizing the Costs of LLM Usage,No.,1,"""No evidence""",2024,2024-01-29T16:36:31Z,,,
arxiv2024,Combining Hierachical VAEs with LLMs for clinically meaningful timeline summarisation in social media,No.,1,"""No evidence""",2024,2024-01-29T15:42:57Z,,,
arxiv2024,LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning,No.,1,"""No evidence""",2024,2024-01-29T14:32:27Z,,,
arxiv2024,"""You tell me"": A Dataset of GPT-4-Based Behaviour Change Support Conversations",No.,1,"""No evidence""",2024,2024-01-29T13:54:48Z,,,
arxiv2024,LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs,No.,1,"""No evidence""",2024,2024-01-29T13:48:36Z,,,
arxiv2024,OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models,No.,1,"""No evidence""",2024,2024-01-29T12:05:02Z,,,
arxiv2024,Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization,No.,1,"""No evidence""",2024,2024-01-29T10:23:47Z,,,
arxiv2024,Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues,No.,1,"""No evidence""",2024,2024-01-29T09:07:40Z,,,
arxiv2024,Response Generation for Cognitive Behavioral Therapy with Large Language Models: Comparative Study with Socratic Questioning,No.,1,"""No evidence""",2024,2024-01-29T08:53:41Z,,,
arxiv2024,Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report,No.,1,"""No evidence""",2024,2024-01-29T06:49:53Z,,,
arxiv2024,UnMASKed: Quantifying Gender Biases in Masked Language Models through Linguistically Informed Job Market Prompts,No.,1,"""No evidence""",2024,2024-01-28T23:00:40Z,,,
arxiv2024,ACCESS: Prompt Engineering for Automated Web Accessibility Violation Corrections,No.,1,"""No evidence""",2024,2024-01-28T22:49:33Z,,,
arxiv2024,Fine-Tuned Large Language Models for Symptom Recognition from Spanish Clinical Text,No.,1,"""No evidence""",2024,2024-01-28T22:11:25Z,,,
arxiv2024,LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware Debugging,No.,1,"""No evidence""",2024,2024-01-28T19:45:25Z,,,
arxiv2024,RE-GAINS & EnCHANT: Intelligent Tool Manipulation Systems For Enhanced Query Responses,No.,1,"""No evidence""",2024,2024-01-28T18:26:31Z,,,
arxiv2024,Identifying and Improving Disability Bias in GAI-Based Resume Screening,No.,1,"""No evidence""",2024,2024-01-28T17:04:59Z,,,
arxiv2024,Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation,No.,1,"""No evidence""",2024,2024-01-28T16:18:39Z,,,
arxiv2024,YODA: Teacher-Student Progressive Learning for Language Models,No.,1,"""No evidence""",2024,2024-01-28T14:32:15Z,,,
arxiv2024,LLsM: Generative Linguistic Steganography with Large Language Model,No.,1,"""No evidence""",2024,2024-01-28T13:21:44Z,,,
arxiv2024,PRE: A Peer Review Based Large Language Model Evaluator,No.,1,"""No evidence""",2024,2024-01-28T12:33:14Z,,,
arxiv2024,Evaluating LLM -- Generated Multimodal Diagnosis from Medical Images and Symptom Analysis,No.,1,"""No evidence""",2024,2024-01-28T09:25:12Z,,,
arxiv2024,Contextualization Distillation from Large Language Model for Knowledge Graph Completion,No.,1,"""No evidence""",2024,2024-01-28T08:56:49Z,,,
arxiv2024,Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge,No.,1,"""No evidence""",2024,2024-01-27T22:49:43Z,,,
arxiv2024,Prompting Diverse Ideas: Increasing AI Idea Variance,No.,1,"""No evidence""",2024,2024-01-27T21:02:50Z,,,
arxiv2024,Do We Need Language-Specific Fact-Checking Models? The Case of Chinese,No.,1,"""No evidence""",2024,2024-01-27T20:26:03Z,,,
arxiv2024,Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization,No.,1,"""No evidence""",2024,2024-01-27T20:20:39Z,,,
arxiv2024,DataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure,No.,1,"""No evidence""",2024,2024-01-27T17:06:53Z,,,
arxiv2024,A Survey on Data Augmentation in Large Model Era,No.,1,"""No evidence""",2024,2024-01-27T14:19:33Z,,,
arxiv2024,A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM,No.,1,"""No evidence""",2024,2024-01-27T10:50:11Z,,,
arxiv2024,A Comprehensive Survey of Compression Algorithms for Language Models,No.,1,"""No evidence""",2024,2024-01-27T08:38:56Z,,,
arxiv2024,Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models,No.,1,"""No evidence""",2024,2024-01-27T08:09:33Z,,,
arxiv2024,L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks,No.,1,"""No evidence""",2024,2024-01-27T07:57:20Z,,,
arxiv2024,Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,No.,1,"""No evidence""",2024,2024-01-27T07:08:37Z,,,
arxiv2024,An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios,No.,1,"""No evidence""",2024,2024-01-27T03:37:55Z,,,
arxiv2024,Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models,No.,1,"""No evidence""",2024,2024-01-27T02:29:42Z,,,
arxiv2024,Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately,No.,1,"""No evidence""",2024,2024-01-27T00:18:07Z,,,
arxiv2024,Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks,No.,1,"""No evidence""",2024,2024-01-26T19:25:43Z,,,
arxiv2024,EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty,No.,1,"""No evidence""",2024,2024-01-26T18:59:01Z,,,
arxiv2024,"From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities",No.,1,"""No evidence""",2024,2024-01-26T18:53:03Z,,,
arxiv2024,On the generalization capacity of neural networks during generic multimodal reasoning,No.,1,"""No evidence""",2024,2024-01-26T17:42:59Z,,,
arxiv2024,SliceGPT: Compress Large Language Models by Deleting Rows and Columns,No.,1,"""No evidence""",2024,2024-01-26T17:35:45Z,,,
arxiv2024,Airavata: Introducing Hindi Instruction-tuned LLM,No.,1,"""No evidence""",2024,2024-01-26T17:07:08Z,,,
arxiv2024,The Power of Noise: Redefining Retrieval for RAG Systems,No.,1,"""No evidence""",2024,2024-01-26T14:14:59Z,,,
arxiv2024,ChemDFM: Dialogue Foundation Model for Chemistry,No.,1,"""No evidence""",2024,2024-01-26T12:45:55Z,,,
arxiv2024,MaLLaM -- Malaysia Large Language Model,No.,1,"""No evidence""",2024,2024-01-26T06:56:05Z,,,
arxiv2024,Scientific Large Language Models: A Survey on Biological & Chemical Domains,No.,1,"""No evidence""",2024,2024-01-26T05:33:34Z,,,
arxiv2024,Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs,No.,1,"""No evidence""",2024,2024-01-26T04:11:07Z,,,
arxiv2024,An Empirical Investigation of Domain Adaptation Ability for Chinese Spelling Check Models,No.,1,"""No evidence""",2024,2024-01-26T03:49:55Z,,,
arxiv2024,Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora,No.,1,"""No evidence""",2024,2024-01-26T03:38:23Z,,,
arxiv2024,Language Modelling Approaches to Adaptive Machine Translation,No.,1,"""No evidence""",2024,2024-01-25T23:02:54Z,,,
arxiv2024,Relative Value Biases in Large Language Models,No.,1,"""No evidence""",2024,2024-01-25T21:49:32Z,,,
arxiv2024,Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do,No.,1,"""No evidence""",2024,2024-01-25T21:30:06Z,,,
arxiv2024,The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support,No.,1,"""No evidence""",2024,2024-01-25T18:08:53Z,,,
arxiv2024,ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models,No.,1,"""No evidence""",2024,2024-01-25T17:55:07Z,,,
arxiv2024,"Demystifying Chains, Trees, and Graphs of Thoughts",No.,1,"""No evidence""",2024,2024-01-25T16:34:00Z,,,
arxiv2024,ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT,No.,1,"""No evidence""",2024,2024-01-25T16:10:33Z,,,
arxiv2024,Transformers and Cortical Waves: Encoders for Pulling In Context Across Time,No.,1,"""No evidence""",2024,2024-01-25T16:01:49Z,,,
arxiv2024,Improving Natural Language Capability of Code Large Language Model,No.,1,"""No evidence""",2024,2024-01-25T15:33:20Z,,,
arxiv2024,How Can Large Language Models Understand Spatial-Temporal Data?,No.,1,"""No evidence""",2024,2024-01-25T14:03:15Z,,,
arxiv2024,Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code,No.,1,"""No evidence""",2024,2024-01-25T13:39:54Z,,,
arxiv2024,BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction,No.,1,"""No evidence""",2024,2024-01-25T13:20:47Z,,,
arxiv2024,True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning,No.,1,"""No evidence""",2024,2024-01-25T13:03:20Z,,,
arxiv2024,"When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges",No.,1,"""No evidence""",2024,2024-01-25T12:03:50Z,,,
arxiv2024,Towards Uncertainty-Aware Language Agent,No.,1,"""No evidence""",2024,2024-01-25T08:48:21Z,,,
arxiv2024,BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models,No.,1,"""No evidence""",2024,2024-01-25T06:18:20Z,,,
arxiv2024,WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models,No.,1,"""No evidence""",2024,2024-01-25T03:33:18Z,,,
arxiv2024,A Survey of Deep Learning and Foundation Models for Time Series Forecasting,No.,1,"""No evidence""",2024,2024-01-25T03:14:07Z,,,
arxiv2024,Democratizing Fine-grained Visual Recognition with Large Language Models,No.,1,"""No evidence""",2024,2024-01-24T22:28:26Z,,,
arxiv2024,The Calibration Gap between Model and Human Confidence in Large Language Models,No.,1,"""No evidence""",2024,2024-01-24T22:21:04Z,,,
arxiv2024,Investigating the Efficacy of Large Language Models for Code Clone Detection,No.,1,"""No evidence""",2024,2024-01-24T20:43:36Z,,,
arxiv2024,Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction,No.,1,"""No evidence""",2024,2024-01-24T17:04:28Z,,,
arxiv2024,Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes,No.,1,"""No evidence""",2024,2024-01-24T16:52:37Z,,,
arxiv2024,SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation,No.,1,"""No evidence""",2024,2024-01-24T15:25:01Z,,,
arxiv2024,Can GPT-3.5 Generate and Code Discharge Summaries?,No.,1,"""No evidence""",2024,2024-01-24T15:10:13Z,,,
arxiv2024,Research about the Ability of LLM in the Tamper-Detection Area,No.,1,"""No evidence""",2024,2024-01-24T14:53:06Z,,,
arxiv2024,"How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment",No.,1,"""No evidence""",2024,2024-01-24T14:29:39Z,,,
arxiv2024,APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT,No.,1,"""No evidence""",2024,2024-01-24T10:09:11Z,,,
arxiv2024,ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models,No.,1,"""No evidence""",2024,2024-01-24T09:07:11Z,,,
arxiv2024,UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems,No.,1,"""No evidence""",2024,2024-01-24T06:50:20Z,,,
arxiv2024,From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning,No.,1,"""No evidence""",2024,2024-01-24T04:57:32Z,,,
arxiv2024,ULTRA: Unleash LLMs' Potential for Event Argument Extraction through Hierarchical Modeling and Pair-wise Refinement,No.,1,"""No evidence""",2024,2024-01-24T04:13:28Z,,,
arxiv2024,ARGS: Alignment as Reward-Guided Search,No.,1,"""No evidence""",2024,2024-01-23T23:42:41Z,,,
arxiv2024,XAI for All: Can Large Language Models Simplify Explainable AI?,No.,1,"""No evidence""",2024,2024-01-23T21:47:12Z,,,
arxiv2024,HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments,No.,1,"""No evidence""",2024,2024-01-23T18:59:43Z,,,
arxiv2024,Raidar: geneRative AI Detection viA Rewriting,No.,1,"""No evidence""",2024,2024-01-23T18:57:53Z,,,
arxiv2024,AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,No.,1,"""No evidence""",2024,2024-01-23T18:45:54Z,,,
arxiv2024,Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network,No.,1,"""No evidence""",2024,2024-01-23T18:45:27Z,,,
arxiv2024,How well can large language models explain business processes?,No.,1,"""No evidence""",2024,2024-01-23T15:29:26Z,,,
arxiv2024,Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study,No.,1,"""No evidence""",2024,2024-01-23T14:19:01Z,,,
arxiv2024,Evaluation of large language models for assessing code maintainability,No.,1,"""No evidence""",2024,2024-01-23T12:29:42Z,,,
arxiv2024,Generating Zero-shot Abstractive Explanations for Rumour Verification,No.,1,"""No evidence""",2024,2024-01-23T12:29:37Z,,,
arxiv2024,Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context,No.,1,"""No evidence""",2024,2024-01-23T11:25:34Z,,,
arxiv2024,Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control,No.,1,"""No evidence""",2024,2024-01-23T10:23:13Z,,,
arxiv2024,Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition,No.,1,"""No evidence""",2024,2024-01-23T09:54:36Z,,,
arxiv2024,Automated Fact-Checking of Climate Change Claims with Large Language Models,No.,1,"""No evidence""",2024,2024-01-23T08:49:23Z,,,
arxiv2024,Can Large Language Models Write Parallel Code?,No.,1,"""No evidence""",2024,2024-01-23T08:25:12Z,,,
arxiv2024,BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models,No.,1,"""No evidence""",2024,2024-01-23T06:36:49Z,,,
arxiv2024,Assessing and Understanding Creativity in Large Language Models,No.,1,"""No evidence""",2024,2024-01-23T05:19:47Z,,,
arxiv2024,Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment,No.,1,"""No evidence""",2024,2024-01-23T03:56:22Z,,,
arxiv2024,Zero Shot Open-ended Video Inference,No.,1,"""No evidence""",2024,2024-01-23T03:45:05Z,,,
arxiv2024,Towards Socially and Morally Aware RL agent: Reward Design With LLM,No.,1,"""No evidence""",2024,2024-01-23T03:00:03Z,,,
arxiv2024,Analyzing the Effectiveness of Large Language Models on Text-to-SQL Synthesis,No.,1,"""No evidence""",2024,2024-01-22T22:05:42Z,,,
arxiv2024,"Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection",No.,1,"""No evidence""",2024,2024-01-22T19:39:05Z,,,
arxiv2024,Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models,No.,1,"""No evidence""",2024,2024-01-22T18:59:07Z,,,
arxiv2024,CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation,No.,1,"""No evidence""",2024,2024-01-22T18:51:07Z,,,
arxiv2024,WARM: On the Benefits of Weight Averaged Reward Models,No.,1,"""No evidence""",2024,2024-01-22T18:27:08Z,,,
arxiv2024,The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models,No.,1,"""No evidence""",2024,2024-01-22T16:57:05Z,,,
arxiv2024,Detecting Multimedia Generated by Large AI Models: A Survey,No.,1,"""No evidence""",2024,2024-01-22T15:08:19Z,,,
arxiv2024,CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark,No.,1,"""No evidence""",2024,2024-01-22T13:34:34Z,,,
arxiv2024,Multimodal Deep Learning of Word-of-Mouth Text and Demographics to Predict Customer Rating: Handling Consumer Heterogeneity in Marketing,No.,1,"""No evidence""",2024,2024-01-22T12:28:50Z,,,
arxiv2024,"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",No.,1,"""No evidence""",2024,2024-01-22T12:11:55Z,,,
arxiv2024,Distilling Mathematical Reasoning Capabilities into Small Language Models,No.,1,"""No evidence""",2024,2024-01-22T11:37:18Z,,,
arxiv2024,AI for social science and social science of AI: A Survey,No.,1,"""No evidence""",2024,2024-01-22T10:57:09Z,,,
arxiv2024,"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",No.,1,"""No evidence""",2024,2024-01-22T06:16:29Z,,,
arxiv2024,Revolutionizing Finance with LLMs: An Overview of Applications and Insights,No.,1,"""No evidence""",2024,2024-01-22T01:06:17Z,,,
arxiv2024,Large Language Model based Multi-Agents: A Survey of Progress and Challenges,No.,1,"""No evidence""",2024,2024-01-21T23:36:14Z,,,
arxiv2024,Enhancing Recommendation Diversity by Re-ranking with Large Language Models,No.,1,"""No evidence""",2024,2024-01-21T14:33:52Z,,,
arxiv2024,CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray Report Labeling,No.,1,"""No evidence""",2024,2024-01-21T14:30:20Z,,,
arxiv2024,Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis,No.,1,"""No evidence""",2024,2024-01-21T14:10:27Z,,,
arxiv2024,Training microrobots to swim by a large language model,No.,1,"""No evidence""",2024,2024-01-21T12:18:59Z,,,
arxiv2024,Instructional Fingerprinting of Large Language Models,No.,1,"""No evidence""",2024,2024-01-21T09:51:45Z,,,
arxiv2024,General Flow as Foundation Affordance for Scalable Robot Learning,No.,1,"""No evidence""",2024,2024-01-21T09:39:11Z,,,
arxiv2024,LLMRA: Multi-modal Large Language Model based Restoration Assistant,No.,1,"""No evidence""",2024,2024-01-21T04:50:19Z,,,
arxiv2024,MedLM: Exploring Language Models for Medical Question Answering Systems,No.,1,"""No evidence""",2024,2024-01-21T03:37:47Z,,,
arxiv2024,Using Large Language Model for End-to-End Chinese ASR and NER,No.,1,"""No evidence""",2024,2024-01-21T03:15:05Z,,,
arxiv2024,Confidence Preservation Property in Knowledge Distillation Abstractions,No.,1,"""No evidence""",2024,2024-01-21T01:37:25Z,,,
arxiv2024,ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution,No.,1,"""No evidence""",2024,2024-01-21T00:58:31Z,,,
arxiv2024,Identifying and Analyzing Task-Encoding Tokens in Large Language Models,No.,1,"""No evidence""",2024,2024-01-20T20:55:21Z,,,
arxiv2024,InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance,No.,1,"""No evidence""",2024,2024-01-20T10:41:03Z,,,
arxiv2024,BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models,No.,1,"""No evidence""",2024,2024-01-20T04:53:35Z,,,
arxiv2024,Mining experimental data from Materials Science literature with Large Language Models: an evaluation study,No.,1,"""No evidence""",2024,2024-01-19T23:00:31Z,,,
arxiv2024,FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?,No.,1,"""No evidence""",2024,2024-01-19T21:21:02Z,,,
arxiv2024,Using LLMs to discover emerging coded antisemitic hate-speech in extremist social media,No.,1,"""No evidence""",2024,2024-01-19T17:40:50Z,,,
arxiv2024,Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models,No.,1,"""No evidence""",2024,2024-01-19T15:32:46Z,,,
arxiv2024,"Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models",No.,1,"""No evidence""",2024,2024-01-19T11:48:09Z,,,
arxiv2024,PHOENIX: Open-Source Language Adaption for Direct Preference Optimization,No.,1,"""No evidence""",2024,2024-01-19T09:46:08Z,,,
arxiv2024,Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion,No.,1,"""No evidence""",2024,2024-01-19T07:34:36Z,,,
arxiv2024,Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences,No.,1,"""No evidence""",2024,2024-01-19T07:10:13Z,,,
arxiv2024,A match made in consistency heaven: when large language models meet evolutionary algorithms,No.,1,"""No evidence""",2024,2024-01-19T05:58:30Z,,,
arxiv2024,AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment,No.,1,"""No evidence""",2024,2024-01-19T05:54:35Z,,,
arxiv2024,FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis,No.,1,"""No evidence""",2024,2024-01-19T05:48:07Z,,,
arxiv2024,Knowledge Fusion of Large Language Models,No.,1,"""No evidence""",2024,2024-01-19T05:02:46Z,,,
arxiv2024,DeepEdit: Knowledge Editing as Decoding with Constraints,No.,1,"""No evidence""",2024,2024-01-19T03:48:27Z,,,
arxiv2024,Critical Data Size of Language Models from a Grokking Perspective,No.,1,"""No evidence""",2024,2024-01-19T03:24:36Z,,,
arxiv2024,Large Language Models are Efficient Learners of Noise-Robust Speech Recognition,No.,1,"""No evidence""",2024,2024-01-19T01:29:27Z,,,
arxiv2024,Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?,No.,1,"""No evidence""",2024,2024-01-19T01:14:45Z,,,
arxiv2024,Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?,No.,1,"""No evidence""",2024,2024-01-18T23:00:54Z,,,
arxiv2024,"Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",No.,1,"""No evidence""",2024,2024-01-18T20:14:10Z,,,
arxiv2024,Towards Language-Driven Video Inpainting via Multimodal Large Language Models,No.,1,"""No evidence""",2024,2024-01-18T18:59:13Z,,,
arxiv2024,Spatial-Temporal Large Language Model for Traffic Prediction,No.,1,"""No evidence""",2024,2024-01-18T17:03:59Z,,,
arxiv2024,Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs,No.,1,"""No evidence""",2024,2024-01-18T15:32:24Z,,,
arxiv2024,Self-Rewarding Language Models,No.,1,"""No evidence""",2024,2024-01-18T14:43:47Z,,,
arxiv2024,Gender Bias in Machine Translation and The Era of Large Language Models,No.,1,"""No evidence""",2024,2024-01-18T14:34:49Z,,,
arxiv2024,Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation,No.,1,"""No evidence""",2024,2024-01-18T14:21:56Z,,,
arxiv2024,WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens,No.,1,"""No evidence""",2024,2024-01-18T14:01:20Z,,,
arxiv2024,Veagle: Advancements in Multimodal Representation Learning,No.,1,"""No evidence""",2024,2024-01-18T12:45:25Z,,,
arxiv2024,A Survey on Hardware Accelerators for Large Language Models,No.,1,"""No evidence""",2024,2024-01-18T11:05:03Z,,,
arxiv2024,Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments,No.,1,"""No evidence""",2024,2024-01-18T10:21:15Z,,,
arxiv2024,"A Fast, Performant, Secure Distributed Training Framework For Large Language Model",No.,1,"""No evidence""",2024,2024-01-18T08:33:09Z,,,
arxiv2024,"Leveraging Biases in Large Language Models: ""bias-kNN'' for Effective Few-Shot Learning",No.,1,"""No evidence""",2024,2024-01-18T08:05:45Z,,,
arxiv2024,A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation,No.,1,"""No evidence""",2024,2024-01-18T07:23:51Z,,,
arxiv2024,SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model,No.,1,"""No evidence""",2024,2024-01-18T04:10:20Z,,,
arxiv2024,Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study,No.,1,"""No evidence""",2024,2024-01-17T23:14:52Z,,,
arxiv2024,"Improving Classification Performance With Human Feedback: Label a few, we label the rest",No.,1,"""No evidence""",2024,2024-01-17T19:13:05Z,,,
arxiv2024,Vlogger: Make Your Dream A Vlog,No.,1,"""No evidence""",2024,2024-01-17T18:55:12Z,,,
arxiv2024,Efficient slot labelling,No.,1,"""No evidence""",2024,2024-01-17T17:08:36Z,,,
arxiv2024,Large Language Models Are Neurosymbolic Reasoners,No.,1,"""No evidence""",2024,2024-01-17T16:57:19Z,,,
arxiv2024,Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models,No.,1,"""No evidence""",2024,2024-01-17T09:44:07Z,,,
arxiv2024,GPT in Sheep's Clothing: The Risk of Customized GPTs,No.,1,"""No evidence""",2024,2024-01-17T09:27:13Z,,,
arxiv2024,Code Simulation Challenges for Large Language Models,No.,1,"""No evidence""",2024,2024-01-17T09:23:59Z,,,
arxiv2024,AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models,No.,1,"""No evidence""",2024,2024-01-17T06:42:44Z,,,
arxiv2024,"COCO is ""ALL'' You Need for Visual Instruction Fine-tuning",No.,1,"""No evidence""",2024,2024-01-17T04:43:45Z,,,
arxiv2024,ReFT: Reasoning with Reinforced Fine-Tuning,No.,1,"""No evidence""",2024,2024-01-17T04:43:21Z,,,
arxiv2024,Tuning Language Models by Proxy,No.,1,"""No evidence""",2024,2024-01-16T18:49:55Z,,,
arxiv2024,Scalable Pre-training of Large Autoregressive Image Models,No.,1,"""No evidence""",2024,2024-01-16T18:03:37Z,,,
arxiv2024,EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis,No.,1,"""No evidence""",2024,2024-01-16T17:11:11Z,,,
arxiv2024,Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering,No.,1,"""No evidence""",2024,2024-01-16T17:00:36Z,,,
arxiv2024,The Effect of Group Status on the Variability of Group Representations in LLM-generated Text,No.,1,"""No evidence""",2024,2024-01-16T16:52:00Z,,,
arxiv2024,"Machine Translation with Large Language Models: Prompt Engineering for Persian, English, and Russian Directions",No.,1,"""No evidence""",2024,2024-01-16T15:16:34Z,,,
arxiv2024,"RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",No.,1,"""No evidence""",2024,2024-01-16T14:44:47Z,,,
arxiv2024,Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine,No.,1,"""No evidence""",2024,2024-01-16T14:41:20Z,,,
arxiv2024,Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports,No.,1,"""No evidence""",2024,2024-01-16T14:40:54Z,,,
arxiv2024,Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection,No.,1,"""No evidence""",2024,2024-01-16T13:54:43Z,,,
arxiv2024,Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening,No.,1,"""No evidence""",2024,2024-01-16T12:30:56Z,,,
arxiv2024,AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception,No.,1,"""No evidence""",2024,2024-01-16T10:58:07Z,,,
arxiv2024,Large Language Models are Null-Shot Learners,No.,1,"""No evidence""",2024,2024-01-16T10:53:11Z,,,
arxiv2024,Generative Multi-Modal Knowledge Retrieval with Large Language Models,No.,1,"""No evidence""",2024,2024-01-16T08:44:29Z,,,
arxiv2024,PRewrite: Prompt Rewriting with Reinforcement Learning,No.,1,"""No evidence""",2024,2024-01-16T08:04:50Z,,,
arxiv2024,A Survey of Resource-efficient LLM and Multimodal Foundation Models,No.,1,"""No evidence""",2024,2024-01-16T03:35:26Z,,,
arxiv2024,A Study on Training and Developing Large Language Models for Behavior Tree Generation,No.,1,"""No evidence""",2024,2024-01-16T03:28:29Z,,,
arxiv2024,A Novel Approach for Automatic Program Repair using Round-Trip Translation with Large Language Models,No.,1,"""No evidence""",2024,2024-01-15T22:36:31Z,,,
arxiv2024,Leveraging External Knowledge Resources to Enable Domain-Specific Comprehension,No.,1,"""No evidence""",2024,2024-01-15T21:43:46Z,,,
arxiv2024,The Pitfalls of Defining Hallucination,No.,1,"""No evidence""",2024,2024-01-15T18:53:15Z,,,
arxiv2024,"The Chronicles of RAG: The Retriever, the Chunk and the Generator",No.,1,"""No evidence""",2024,2024-01-15T18:25:18Z,,,
arxiv2024,JumpCoder: Go Beyond Autoregressive Coder via Online Modification,No.,1,"""No evidence""",2024,2024-01-15T18:04:29Z,,,
arxiv2024,Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability,No.,1,"""No evidence""",2024,2024-01-15T18:01:59Z,,,
arxiv2024,Authorship Obfuscation in Multilingual Machine-Generated Text Detection,No.,1,"""No evidence""",2024,2024-01-15T17:57:41Z,,,
arxiv2024,Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding,No.,1,"""No evidence""",2024,2024-01-15T17:26:50Z,,,
arxiv2024,Question Translation Training for Better Multilingual Reasoning,No.,1,"""No evidence""",2024,2024-01-15T16:39:10Z,,,
arxiv2024,Consolidating Strategies for Countering Hate Speech Using Persuasive Dialogues,No.,1,"""No evidence""",2024,2024-01-15T16:31:18Z,,,
arxiv2024,"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",No.,1,"""No evidence""",2024,2024-01-15T15:20:59Z,,,
arxiv2024,"Assistant, Parrot, or Colonizing Loudspeaker? ChatGPT Metaphors for Developing Critical AI Literacies",No.,1,"""No evidence""",2024,2024-01-15T15:15:48Z,,,
arxiv2024,Prompting open-source and commercial language models for grammatical error correction of English learner text,No.,1,"""No evidence""",2024,2024-01-15T14:19:47Z,,,
arxiv2024,MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models,No.,1,"""No evidence""",2024,2024-01-15T11:06:43Z,,,
arxiv2024,TP-Aware Dequantization,No.,1,"""No evidence""",2024,2024-01-15T08:01:40Z,,,
arxiv2024,"Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends",No.,1,"""No evidence""",2024,2024-01-15T07:48:42Z,,,
arxiv2024,Harnessing Large Language Models Over Transformer Models for Detecting Bengali Depressive Social Media Text: A Comprehensive Study,No.,1,"""No evidence""",2024,2024-01-14T15:15:58Z,,,
arxiv2024,CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning,No.,1,"""No evidence""",2024,2024-01-14T13:24:30Z,,,
arxiv2024,Distilling Event Sequence Knowledge From Large Language Models,No.,1,"""No evidence""",2024,2024-01-14T09:34:42Z,,,
arxiv2024,Inroads to a Structured Data Natural Language Bijection and the role of LLM annotation,No.,1,"""No evidence""",2024,2024-01-14T03:16:49Z,,,
arxiv2024,"A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models",No.,1,"""No evidence""",2024,2024-01-14T02:30:19Z,,,
arxiv2024,Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models,No.,1,"""No evidence""",2024,2024-01-13T21:00:21Z,,,
arxiv2024,Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding,No.,1,"""No evidence""",2024,2024-01-13T19:19:04Z,,,
arxiv2024,"Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models",No.,1,"""No evidence""",2024,2024-01-13T16:41:40Z,,,
arxiv2024,PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics Capabilities,No.,1,"""No evidence""",2024,2024-01-13T13:46:14Z,,,
arxiv2024,xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning,No.,1,"""No evidence""",2024,2024-01-13T10:53:53Z,,,
arxiv2024,Knowledge Distillation for Closed-Source Language Models,No.,1,"""No evidence""",2024,2024-01-13T08:43:32Z,,,
arxiv2024,NHANES-GCP: Leveraging the Google Cloud Platform and BigQuery ML for reproducible machine learning with data from the National Health and Nutrition Examination Survey,No.,1,"""No evidence""",2024,2024-01-13T03:41:54Z,,,
arxiv2024,E^2-LLM: Efficient and Extreme Length Extension of Large Language Models,No.,1,"""No evidence""",2024,2024-01-13T02:11:20Z,,,
arxiv2024,Parameter-Efficient Detoxification with Contrastive Decoding,No.,1,"""No evidence""",2024,2024-01-13T01:46:20Z,,,
arxiv2024,Knowledge-Centric Templatic Views of Documents,No.,1,"""No evidence""",2024,2024-01-13T01:22:15Z,,,
arxiv2024,Promptly Predicting Structures: The Return of Inference,No.,1,"""No evidence""",2024,2024-01-12T20:08:39Z,,,
arxiv2024,Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data,No.,1,"""No evidence""",2024,2024-01-12T19:40:11Z,,,
arxiv2024,APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding,No.,1,"""No evidence""",2024,2024-01-12T18:50:36Z,,,
arxiv2024,MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization,No.,1,"""No evidence""",2024,2024-01-12T18:03:54Z,,,
arxiv2024,LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration,No.,1,"""No evidence""",2024,2024-01-12T17:50:35Z,,,
arxiv2024,Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models,No.,1,"""No evidence""",2024,2024-01-12T17:41:38Z,,,
arxiv2024,"Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation",No.,1,"""No evidence""",2024,2024-01-12T16:52:41Z,,,
arxiv2024,Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought,No.,1,"""No evidence""",2024,2024-01-12T16:42:10Z,,,
arxiv2024,Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study,No.,1,"""No evidence""",2024,2024-01-12T14:35:57Z,,,
arxiv2024,Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation,No.,1,"""No evidence""",2024,2024-01-12T13:23:21Z,,,
arxiv2024,A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems,No.,1,"""No evidence""",2024,2024-01-12T12:43:33Z,,,
arxiv2024,ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study,No.,1,"""No evidence""",2024,2024-01-12T11:27:15Z,,,
arxiv2024,An investigation of structures responsible for gender bias in BERT and DistilBERT,No.,1,"""No evidence""",2024,2024-01-12T10:42:20Z,,,
arxiv2024,"A Survey on the Applications of Frontier AI, Foundation Models, and Large Language Models to Intelligent Transportation Systems",No.,1,"""No evidence""",2024,2024-01-12T10:29:48Z,,,
arxiv2024,Cross-Attention Watermarking of Large Language Models,No.,1,"""No evidence""",2024,2024-01-12T09:39:50Z,,,
arxiv2024,"Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning",No.,1,"""No evidence""",2024,2024-01-12T09:31:17Z,,,
arxiv2024,Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers,No.,1,"""No evidence""",2024,2024-01-12T09:15:20Z,,,
arxiv2024,AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters,No.,1,"""No evidence""",2024,2024-01-12T07:10:10Z,,,
arxiv2024,Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model,No.,1,"""No evidence""",2024,2024-01-12T06:49:49Z,,,
arxiv2024,Zero-shot Generative Large Language Models for Systematic Review Screening Automation,No.,1,"""No evidence""",2024,2024-01-12T01:54:08Z,,,
arxiv2024,Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study,No.,1,"""No evidence""",2024,2024-01-12T00:00:32Z,,,
arxiv2024,Extreme Compression of Large Language Models via Additive Quantization,No.,1,"""No evidence""",2024,2024-01-11T18:54:44Z,,,
arxiv2024,Transformers are Multi-State RNNs,No.,1,"""No evidence""",2024,2024-01-11T18:35:26Z,,,
arxiv2024,A Closer Look at AUROC and AUPRC under Class Imbalance,No.,1,"""No evidence""",2024,2024-01-11T18:11:42Z,,,
arxiv2024,Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models,No.,1,"""No evidence""",2024,2024-01-11T18:06:30Z,,,
arxiv2024,Secrets of RLHF in Large Language Models Part II: Reward Modeling,No.,1,"""No evidence""",2024,2024-01-11T17:56:59Z,,,
arxiv2024,Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion,No.,1,"""No evidence""",2024,2024-01-11T17:42:47Z,,,
arxiv2024,GroundingGPT:Language Enhanced Multi-modal Grounding Model,No.,1,"""No evidence""",2024,2024-01-11T17:41:57Z,,,
arxiv2024,When ChatGPT is gone: Creativity reverts and homogeneity persists,No.,1,"""No evidence""",2024,2024-01-11T16:34:09Z,,,
arxiv2024,Combating Adversarial Attacks with Multi-Agent Debate,No.,1,"""No evidence""",2024,2024-01-11T15:57:38Z,,,
arxiv2024,EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction,No.,1,"""No evidence""",2024,2024-01-11T15:45:11Z,,,
arxiv2024,Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs,No.,1,"""No evidence""",2024,2024-01-11T14:27:43Z,,,
arxiv2024,SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully,No.,1,"""No evidence""",2024,2024-01-11T14:09:09Z,,,
arxiv2024,Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models,No.,1,"""No evidence""",2024,2024-01-11T12:11:30Z,,,
arxiv2024,"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",No.,1,"""No evidence""",2024,2024-01-11T09:29:56Z,,,
arxiv2024,Probing Structured Semantics Understanding and Generation of Language Models via Question Answering,No.,1,"""No evidence""",2024,2024-01-11T09:27:50Z,,,
arxiv2024,A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism,No.,1,"""No evidence""",2024,2024-01-11T08:56:13Z,,,
arxiv2024,Zero Resource Cross-Lingual Part Of Speech Tagging,No.,1,"""No evidence""",2024,2024-01-11T08:12:47Z,,,
arxiv2024,CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer,No.,1,"""No evidence""",2024,2024-01-11T07:18:46Z,,,
arxiv2024,Video Anomaly Detection and Explanation via Large Language Models,No.,1,"""No evidence""",2024,2024-01-11T07:09:44Z,,,
arxiv2024,On Detecting Cherry-picking in News Coverage Using Large Language Models,No.,1,"""No evidence""",2024,2024-01-11T04:03:35Z,,,
arxiv2024,The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models,No.,1,"""No evidence""",2024,2024-01-11T01:52:25Z,,,
arxiv2024,REBUS: A Robust Evaluation Benchmark of Understanding Symbols,No.,1,"""No evidence""",2024,2024-01-11T00:30:28Z,,,
arxiv2024,InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks,No.,1,"""No evidence""",2024,2024-01-10T19:04:00Z,,,
arxiv2024,AugSumm: towards generalizable speech summarization using synthetic labels from large language model,No.,1,"""No evidence""",2024,2024-01-10T18:39:46Z,,,
arxiv2024,Leveraging Print Debugging to Improve Code Generation in Large Language Models,No.,1,"""No evidence""",2024,2024-01-10T18:37:59Z,,,
arxiv2024,Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?,No.,1,"""No evidence""",2024,2024-01-10T18:09:36Z,,,
arxiv2024,Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning,No.,1,"""No evidence""",2024,2024-01-10T15:29:21Z,,,
arxiv2024,Pre-trained Large Language Models for Financial Sentiment Analysis,No.,1,"""No evidence""",2024,2024-01-10T15:27:41Z,,,
arxiv2024,Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking,No.,1,"""No evidence""",2024,2024-01-10T14:53:18Z,,,
arxiv2024,DCR: Divide-and-Conquer Reasoning for Multi-choice Question Answering with LLMs,No.,1,"""No evidence""",2024,2024-01-10T14:38:46Z,,,
arxiv2024,"ChatGPT, Let us Chat Sign Language: Experiments, Architectural Elements, Challenges and Research Directions",No.,1,"""No evidence""",2024,2024-01-10T13:39:49Z,,,
arxiv2024,"Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",No.,1,"""No evidence""",2024,2024-01-10T09:25:45Z,,,
arxiv2024,Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems,No.,1,"""No evidence""",2024,2024-01-10T05:32:20Z,,,
arxiv2024,The Impact of Reasoning Step Length on Large Language Models,No.,1,"""No evidence""",2024,2024-01-10T04:37:38Z,,,
arxiv2024,ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain,No.,1,"""No evidence""",2024,2024-01-10T02:59:49Z,,,
arxiv2024,Reinforcement Learning for Optimizing RAG for Domain Chatbots,No.,1,"""No evidence""",2024,2024-01-10T02:57:20Z,,,
arxiv2024,Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations,No.,1,"""No evidence""",2024,2024-01-10T02:22:21Z,,,
arxiv2024,"Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",No.,1,"""No evidence""",2024,2024-01-10T00:05:45Z,,,
arxiv2024,Arabic Text Diacritization In The Age Of Transfer Learning: Token Classification Is All You Need,No.,1,"""No evidence""",2024,2024-01-09T23:32:54Z,,,
arxiv2024,Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers,No.,1,"""No evidence""",2024,2024-01-09T17:44:36Z,,,
arxiv2024,Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models,No.,1,"""No evidence""",2024,2024-01-09T16:27:28Z,,,
arxiv2024,Agent Alignment in Evolving Social Norms,No.,1,"""No evidence""",2024,2024-01-09T15:44:44Z,,,
arxiv2024,Language Detection for Transliterated Content,No.,1,"""No evidence""",2024,2024-01-09T15:40:54Z,,,
arxiv2024,Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values,No.,1,"""No evidence""",2024,2024-01-09T14:57:30Z,,,
arxiv2024,An Assessment on Comprehending Mental Health through Large Language Models,No.,1,"""No evidence""",2024,2024-01-09T14:50:04Z,,,
arxiv2024,Evaluating Language Model Agency through Negotiations,No.,1,"""No evidence""",2024,2024-01-09T13:19:37Z,,,
arxiv2024,MERA: A Comprehensive LLM Evaluation in Russian,No.,1,"""No evidence""",2024,2024-01-09T12:55:21Z,,,
arxiv2024,Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding,No.,1,"""No evidence""",2024,2024-01-09T07:46:26Z,,,
arxiv2024,DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference,No.,1,"""No evidence""",2024,2024-01-09T06:49:40Z,,,
arxiv2024,"Large Language Models for Robotics: Opportunities, Challenges, and Perspectives",No.,1,"""No evidence""",2024,2024-01-09T03:22:16Z,,,
arxiv2024,MARG: Multi-Agent Review Generation for Scientific Papers,No.,1,"""No evidence""",2024,2024-01-08T22:24:17Z,,,
arxiv2024,Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?,No.,1,"""No evidence""",2024,2024-01-08T20:08:04Z,,,
arxiv2024,FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild,No.,1,"""No evidence""",2024,2024-01-08T19:39:36Z,,,
arxiv2024,AI and Generative AI for Research Discovery and Summarization,No.,1,"""No evidence""",2024,2024-01-08T18:42:55Z,,,
arxiv2024,Unveiling Bias in Fairness Evaluations of Large Language Models: A Critical Literature Review of Music and Movie Recommendation Systems,No.,1,"""No evidence""",2024,2024-01-08T17:57:29Z,,,
arxiv2024,FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference,No.,1,"""No evidence""",2024,2024-01-08T17:29:16Z,,,
arxiv2024,Large language models in bioinformatics: applications and perspectives,No.,1,"""No evidence""",2024,2024-01-08T17:26:59Z,,,
arxiv2024,Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series,No.,1,"""No evidence""",2024,2024-01-08T15:21:21Z,,,
arxiv2024,TextMachina: Seamless Generation of Machine-Generated Text Datasets,No.,1,"""No evidence""",2024,2024-01-08T15:05:32Z,,,
arxiv2024,SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems,No.,1,"""No evidence""",2024,2024-01-08T15:01:08Z,,,
arxiv2024,Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning,No.,1,"""No evidence""",2024,2024-01-08T14:26:49Z,,,
arxiv2024,Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex,No.,1,"""No evidence""",2024,2024-01-08T12:30:23Z,,,
arxiv2024,TeleChat Technical Report,No.,1,"""No evidence""",2024,2024-01-08T10:43:19Z,,,
arxiv2024,Enhanced Automated Code Vulnerability Repair using Large Language Models,No.,1,"""No evidence""",2024,2024-01-08T09:01:29Z,,,
arxiv2024,The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance,No.,1,"""No evidence""",2024,2024-01-08T08:28:08Z,,,
arxiv2024,LightHouse: A Survey of AGI Hallucination,No.,1,"""No evidence""",2024,2024-01-08T03:52:40Z,,,
arxiv2024,An Exploratory Study on Automatic Identification of Assumptions in the Development of Deep Learning Frameworks,No.,1,"""No evidence""",2024,2024-01-08T03:50:03Z,,,
arxiv2024,Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions,No.,1,"""No evidence""",2024,2024-01-08T00:27:16Z,,,
arxiv2024,ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback,No.,1,"""No evidence""",2024,2024-01-07T23:17:42Z,,,
arxiv2024,InFoBench: Evaluating Instruction Following Ability in Large Language Models,No.,1,"""No evidence""",2024,2024-01-07T23:01:56Z,,,
arxiv2024,DiarizationLM: Speaker Diarization Post-Processing with Large Language Models,No.,1,"""No evidence""",2024,2024-01-07T14:54:57Z,,,
arxiv2024,Maintaining Journalistic Integrity in the Digital Age: A Comprehensive NLP Framework for Evaluating Online News Content,No.,1,"""No evidence""",2024,2024-01-07T12:27:14Z,,,
arxiv2024,"Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",No.,1,"""No evidence""",2024,2024-01-07T09:08:24Z,,,
arxiv2024,On Leveraging Large Language Models for Enhancing Entity Resolution,No.,1,"""No evidence""",2024,2024-01-07T09:06:58Z,,,
arxiv2024,"The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",No.,1,"""No evidence""",2024,2024-01-07T08:37:29Z,,,
arxiv2024,GRAM: Global Reasoning for Multi-Page VQA,No.,1,"""No evidence""",2024,2024-01-07T08:03:06Z,,,
arxiv2024,LLMs for Robotic Object Disambiguation,No.,1,"""No evidence""",2024,2024-01-07T04:46:23Z,,,
arxiv2024,LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward,No.,1,"""No evidence""",2024,2024-01-07T02:46:39Z,,,
arxiv2024,Malla: Demystifying Real-world Large Language Model Integrated Malicious Services,No.,1,"""No evidence""",2024,2024-01-06T22:25:42Z,,,
arxiv2024,3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding,No.,1,"""No evidence""",2024,2024-01-06T12:20:18Z,,,
arxiv2024,?-CAUSAL: Exploring Defeasibility in Causal Reasoning,No.,1,"""No evidence""",2024,2024-01-06T10:08:33Z,,,
arxiv2024,A Joint-Reasoning based Disease Q&A System,No.,1,"""No evidence""",2024,2024-01-06T09:55:22Z,,,
arxiv2024,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,No.,1,"""No evidence""",2024,2024-01-05T18:59:13Z,,,
arxiv2024,Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks,No.,1,"""No evidence""",2024,2024-01-05T17:58:10Z,,,
arxiv2024,AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models,No.,1,"""No evidence""",2024,2024-01-05T15:52:59Z,,,
arxiv2024,Pheme: Efficient and Conversational Speech Generation,No.,1,"""No evidence""",2024,2024-01-05T14:47:20Z,,,
arxiv2024,From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,No.,1,"""No evidence""",2024,2024-01-05T12:26:46Z,,,
arxiv2024,Detection and Classification of Diabetic Retinopathy using Deep Learning Algorithms for Segmentation to Facilitate Referral Recommendation for Test and Treatment Prediction,No.,1,"""No evidence""",2024,2024-01-05T11:19:24Z,,,
arxiv2024,XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model,No.,1,"""No evidence""",2024,2024-01-05T08:24:30Z,,,
arxiv2024,VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model,No.,1,"""No evidence""",2024,2024-01-05T08:05:07Z,,,
arxiv2024,LMaaS: Exploring Pricing Strategy of Large Model as a Service for Communication,No.,1,"""No evidence""",2024,2024-01-05T07:19:19Z,,,
arxiv2024,CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs,No.,1,"""No evidence""",2024,2024-01-05T00:26:07Z,,,
arxiv2024,"Large Language Models for Social Networks: Applications, Challenges, and Solutions",No.,1,"""No evidence""",2024,2024-01-04T23:37:48Z,,,
arxiv2024,Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging,No.,1,"""No evidence""",2024,2024-01-04T22:49:15Z,,,
arxiv2024,PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model,No.,1,"""No evidence""",2024,2024-01-04T13:27:50Z,,,
arxiv2024,A Dataset and Benchmark for Copyright Protection from Text-to-Image Diffusion Models,No.,1,"""No evidence""",2024,2024-01-04T11:14:01Z,,,
arxiv2024,Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study,No.,1,"""No evidence""",2024,2024-01-04T08:53:08Z,,,
arxiv2024,Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions,No.,1,"""No evidence""",2024,2024-01-04T08:39:13Z,,,
arxiv2024,DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models,No.,1,"""No evidence""",2024,2024-01-04T08:34:16Z,,,
arxiv2024,Using LLM to select the right SQL Query from candidates,No.,1,"""No evidence""",2024,2024-01-04T07:50:24Z,,,
arxiv2024,"Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM",No.,1,"""No evidence""",2024,2024-01-04T07:45:49Z,,,
arxiv2024,Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe,No.,1,"""No evidence""",2024,2024-01-04T06:23:22Z,,,
arxiv2024,An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search,No.,1,"""No evidence""",2024,2024-01-04T04:11:59Z,,,
arxiv2024,MobileAgent: enhancing mobile control via human-machine interaction and SOP integration,No.,1,"""No evidence""",2024,2024-01-04T03:44:42Z,,,
arxiv2024,Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias,No.,1,"""No evidence""",2024,2024-01-03T21:38:40Z,,,
arxiv2024,A Vision Check-up for Language Models,No.,1,"""No evidence""",2024,2024-01-03T18:09:33Z,,,
arxiv2024,Multilingual Instruction Tuning With Just a Pinch of Multilinguality,No.,1,"""No evidence""",2024,2024-01-03T17:48:10Z,,,
arxiv2024,Cross-target Stance Detection by Exploiting Target Analytical Perspectives,No.,1,"""No evidence""",2024,2024-01-03T14:28:55Z,,,
arxiv2024,"GPT-4V(ision) is a Generalist Web Agent, if Grounded",No.,1,"""No evidence""",2024,2024-01-03T08:33:09Z,,,
arxiv2024,Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review,No.,1,"""No evidence""",2024,2024-01-03T03:01:29Z,,,
arxiv2024,Quantifying the Uniqueness of Donald Trump in Presidential Discourse,No.,1,"""No evidence""",2024,2024-01-02T19:00:17Z,,,
arxiv2024,CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation,No.,1,"""No evidence""",2024,2024-01-02T16:20:40Z,,,
arxiv2024,VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM,No.,1,"""No evidence""",2024,2024-01-02T15:56:48Z,,,
arxiv2024,Cheetah: Natural Language Generation for 517 African Languages,No.,1,"""No evidence""",2024,2024-01-02T06:24:13Z,,,
arxiv2024,Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation,No.,1,"""No evidence""",2024,2024-01-02T05:42:14Z,,,
arxiv2024,Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education,No.,1,"""No evidence""",2024,2024-01-02T03:54:50Z,,,
arxiv2024,Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models,No.,1,"""No evidence""",2024,2024-01-02T01:54:22Z,,,
arxiv2024,Detection of Machine-Generated Text: Literature Survey,No.,1,"""No evidence""",2024,2024-01-02T01:44:15Z,,,
arxiv2024,COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training,No.,1,"""No evidence""",2024,2024-01-01T18:58:42Z,,,
arxiv2024,Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education,No.,1,"""No evidence""",2024,2024-01-01T18:11:43Z,,,
arxiv2024,A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models,No.,1,"""No evidence""",2024,2024-01-01T13:53:53Z,,,
arxiv2024,Large Language Models aren't all that you need,No.,1,"""No evidence""",2024,2024-01-01T08:32:50Z,,,
arxiv2024,Digger: Detecting Copyright Content Mis-usage in Large Language Model Training,No.,1,"""No evidence""",2024,2024-01-01T06:04:52Z,,,
arxiv2024,Predicting Anti-microbial Resistance using Large Language Models,No.,1,"""No evidence""",2024,2024-01-01T03:04:14Z,,,
arxiv2024,From Prompt Engineering to Prompt Science With Human in the Loop,No.,1,"""No evidence""",2024,2024-01-01T01:37:36Z,,,
