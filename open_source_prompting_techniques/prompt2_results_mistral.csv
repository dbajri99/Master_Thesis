Title,LMs,Limitations of LLMs,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,3,,"The abstract mentions the limitations of LLMs, specifically context limitations, but it is not the main focus of the abstract. The limitations are discussed as a secondary point, and the primary focus is on the proposed approach and its performance."
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,4,,"The abstract mentions limitations of LLMs, including their inability to generalize across domains and the suboptimal editing strategies used in existing post-editing methods. The proposed approach aims to address these limitations by preserving the domain generalization ability of LLMs and designing editing actions specifically for text generation tasks. The experiments demonstrate significant improvements in performance over other SOTA methods, highlighting the importance of addressing these limitations."
Re3val: Reinforced and Reranked Generative Retrieval,4,,"The abstract identifies two limitations of generative retrieval models, which are a type of LLMs. The first limitation is the lack of consideration of contextual information. The second limitation is that the retrieval cannot be tuned for the downstream reader. The limitations are significant and discussed in detail. The paper proposes a solution to these limitations using Re3val, which leverages context and reinforcement learning."
Reward Engineering for Generating Semi-structured Explanation,5,,"The abstract explicitly states that producing structured explanations to verify a model's true reasoning capabilities remains a challenge for not-so-large LMs. It also mentions limitations of supervised fine-tuning in addressing this challenge. These limitations are the primary focus of the abstract, and the abstract provides a detailed discussion on these limitations."
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,4,,"The abstract discusses limitations related to LLMs' evaluation, particularly in low-resource and non-Latin script languages, and the need for calibration with native speaker judgments to ensure accurate evaluation. It also mentions the bias towards higher scores as a limitation of LLM-based evaluators."
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,3,,"The limitations discussed are the need for parameter updates in fine-tuning and the imposition of limitations on the number of training examples in in-context learning. However, these limitations are not the main focus of the abstract and are mentioned as secondary points. The abstract primarily focuses on the novelty and advantages of the proposed approach."
Evaluating Large Language Models Trained on Code,4,,"The abstract discusses the limitations of the model, including difficulty with long chains of operations and binding operations to variables. The limitations are significant and discussed in detail. The authors also discuss the potential broader impacts of deploying powerful code generation technologies, which may include safety, security, and economic concerns. These limitations are important considerations for the use of LLMs in generating code."
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,4,,"The abstract discusses the limitations of current CLIP models in dealing with linguistic variations in input queries, specifically paraphrases. The limitations are significant and the abstract provides a solution to address them through a fine-tuning approach. The limitations are discussed in detail alongside the proposed solution, making it a 4 on the scale."
ICE-Score: Instructing Large Language Models to Evaluate Code,5,,"The abstract discusses the limitations of existing approaches to code evaluation, specifically token-matching-based metrics and the use of human-written test suites. It highlights that these methods are not suitable for code intelligence tasks due to their weak correlations with human judgment and the challenges of acquiring test suites in low-resource domains. The abstract then proposes a new approach using LLMs to overcome these limitations and achieves superior correlations with human judgment and functional correctness. The limitations of LLMs are not directly addressed, but the abstract implies that they are a viable solution to the challenges faced in code evaluation. The abstract also mentions the need for further research in this area."
Transformer-specific Interpretability,4,,"The abstract mentions the limitations of model-agnostic interpretability methods for LLMs and highlights the need for Transformer-specific methods due to the unique properties of LLMs. It also discusses the limitations of these Transformer-specific methods, such as the inability to identify all context-mixing interactions or the need for causal methods and low-level analysis to identify specific subnetworks."
Can docstring reformulation with an LLM improve code generation?,4,,The abstract mentions the limitations of LLMs in the context of code generation and their sensitivity to the details in the docstrings. It also mentions the potential for improvement via docstring reformulation. The limitations are significant and discussed in detail in the conclusion.
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,5,,"The abstract explicitly mentions the limitations of LLMs in hate speech detection and their potential to generate harmful outputs. The abstract also discusses the factors contributing to these limitations, including the training data, model architecture, and fine-tuning. The abstract emphasizes the need for careful consideration of the potential risks and ethical implications of using LLMs for hate speech detection."
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,4,,"The abstract mentions the limitations of LLMs in terms of their substantial model size and the immense computational resources required for large-scale inference or domain-specific fine-tuning. The authors propose a method, GenCo, to address these limitations by utilizing the strong generative power of LLMs to assist in training a smaller and more adaptable language model. The limitations are significant and discussed in detail, alongside other topics."
Document-Level Language Models for Machine Translation,4,,"The abstract mentions known limitations of sentence-level translation systems and the potential of document-level language models to overcome these limitations. It also discusses the challenges of combining language models and the potential benefits of utilizing large language models. The limitations are not the only focus of the abstract, but they are significant and discussed in detail."
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,5,,"The entire abstract focuses on the limitations and challenges of LLMs in handling low-resource languages, suggesting that they significantly underperform traditional MT for most languages. The abstract also provides experimental evidence to support this claim."
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",4,,"The abstract discusses the limitations of LLMs in handling document-level translation and the presence of critical errors, such as content omissions. The authors also highlight the need for human intervention to ensure the author's voice remains intact. They provide a detailed evaluation and error analysis, making the limitations a significant focus of the abstract."
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",5,,"The title and abstract of the paper explicitly discuss the limitations and challenges of LLMs, focusing on their bias and fairness issues. The abstract states that LLMs ""have been shown to exhibit various forms of bias"" and proposes a methodology to ""mitigate bias in pretrained language models."""
Automating Behavioral Testing in Machine Translation,3,,"The abstract mentions that existing work on behavioral testing in MT is limited and that LLMs are proposed as a solution to generate a diverse set of test sentences. However, the limitations of LLMs are not the focus of the abstract and are only mentioned as a means to an end. The abstract mainly discusses the limitations of existing work on behavioral testing and the benefits of using LLMs to address those limitations."
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",4,,"The paper compares the performance of ChatGPT with fine-tuned transformer-based models, indicating that these models outperform ChatGPT in the context of suicidality assessment. The authors also discuss the impact of different temperature parameters on ChatGPT's response generation, suggesting that optimizing these parameters can improve its ability to assist mental health professionals. While the limitations are not the primary focus of the abstract, they are significant and discussed in detail."
GenIE: Generative Information Extraction,5,,"The abstract states that ""GenIE naturally exploits the language knowledge from the pre-trained transformer"" and ""Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced"", indicating that the limitations of LLMs in generating inconsistent or irrelevant information are addressed. The abstract also states that ""Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations"", implying that LLMs have challenges in handling large datasets and diverse entities and relations, which are addressed by GenIE. The abstract also mentions that ""closed information extraction becomes practical in realistic scenarios"", suggesting that LLMs have limitations in handling realistic scenarios, which are addressed by GenIE. The abstract also mentions that ""this work paves the way towards a unified end-to-end approach to the core tasks of information extraction"", implying that LLMs have limitations in handling information extraction as a whole, which are addressed by GenIE. Overall, the abstract focuses on the limitations and challenges of LLMs and how GenIE addresses them."
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,4,,"The abstract mentions ""limitations"" of LMs and specifically mentions ""memorization"" and ""intrinsic difficulty"" as limitations. The abstract also discusses ""failure to match training label distributions,"" which is a common challenge for LLMs. The limitations are significant and discussed in detail, but they are not the only topic in the abstract."
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,3,,"The limitations of LLMs are not the focus of the abstract, but they are mentioned briefly as features used in the model. The abstract primarily discusses the performance of the model in predicting code-mix quality using MLLMs as features."
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,4,,"The abstract discusses the challenges of handling ambiguous claims and the need for soft labels to represent the uncertainty in veracity predictions, which is a limitation of LLMs when dealing with fact-checking and truthfulness assessment tasks. The abstract also shows that models trained on ambiguous instances perform better, which implies that LLMs may struggle with such instances and require improvements. The abstract does not focus solely on LLMs but also mentions other models like pipelines and models for predicting veracity, but the limitations of LLMs are still discussed in detail."
Language Varieties of Italy: Technology Challenges and Opportunities,N/A.,,"The paper does not discuss LLMs or their limitations at all. Instead, it focuses on the challenges of dealing with endangered languages and dialects in Italy and advocating for a shift in paradigm from machine-centric to speaker-centric NLP."
Benchmarking Large Language Models for News Summarization,5,,"The abstract explicitly mentions the limitations of LLMs, including their inability to understand code structure and tendency to generate inaccurate or irrelevant summaries. The authors propose several approaches to address these limitations, indicating that they are significant issues. The entire abstract focuses on the limitations and challenges of using LLMs for code summarization."
mGPT: Few-Shot Learners Go Multilingual,2,,"The limitations are not the focus of the abstract. They are mentioned briefly in the context of evaluating the model on ""cross-lingual NLU datasets and benchmarks in 33 languages"" and ""world knowledge probing in 23 languages."" The abstract mainly focuses on the design and pretraining procedure of mGPT."
Large Language Models of Code Fail at Completing Code with Potential Bugs,5,,"The abstract mentions that Code-LLMs have brought advances to code completion but ignores the presence of bugs in the code context for generation, which are inevitable in software development. The study systematically investigates the buggy-code completion problem and finds that the presence of potential bugs significantly degrades the generation performance of high-performing Code-LLMs. The passing rates of CODEGEN-2B-MONO drop more than 50% given a single potential bug in the context, indicating serious issues with LLMs. The study also investigates post-hoc methods for mitigating the adverse effect of potential bugs but finds a significant gap in post-mitigation performance. The entire abstract focuses on the limitations and challenges of LLMs in handling bugs in code contexts."
Cultural Adaptation of Recipes,4,,"The abstract discusses the limitations of LLMs, including their inability to fully grasp the nuances of cross-cultural contexts and their lagging performance compared to human expertise. The limitations are significant and discussed in detail alongside the evaluation of various methods."
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,N/A,,The paper does not mention LLMs at all. It is focused on few-shot aspect category sentiment analysis and proposes a new method for it.
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,N/A.,,The paper focuses on neural machine translation and does not mention language models or their limitations.
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,N/A.,,"The paper does not mention any limitation related to LLMs. Instead, it discusses a limitation of a specific model for word-level auto-completion in computer-aided translation."
Lost in the Middle: How Language Models Use Long Contexts,5,,"The title and abstract explicitly focus on evaluating the limitations of language models, and the paper provides detailed evidence of their inability to reason about causality, handle out-of-distribution data, and handle long contexts. The authors also emphas"
Red Teaming Language Model Detectors with Language Models,5,,"The entire abstract focuses on the limitations of LLMs, specifically their susceptibility to adversarial attacks and the need for robust LLM detectors to prevent their misuse. The abstract also discusses the challenges of detecting LLM-generated text in a challenging setting where the auxiliary LLM can also be protected by a detector. The words ""significant safety and ethical risks"" and ""urgent need to improve the robustness of LLM-generated text detection systems"" further emphasize the severity of the limitations."
Text Attribute Control via Closed-Loop Disentanglement,4,,"The text discusses the limitations of previous methods for disentangling text attributes and content representations, which are typically based on encoder-decoder architectures and rely on constraints such as adversarial-based constraints and mutual-information-based constraints. The text notes that these methods have not been sufficient for successful attribute change while preserving content, and proposes a new approach using semi-supervised contrastive learning to improve disentanglement and content preservation. The limitations discussed include the inadequacy of previous disentanglement methods for robust attribute control and the high computational cost of mutual information and adversarial training. The text also mentions the need for a closed-loop disentanglement process to enhance content preservation."
Unifying Structured Data as Graph for Data-to-Text Pre-Training,3,,"The abstract mentions limitations of previous pre-training methods for data-to-text (D2T) generation, specifically that they either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure. However, the limitations of LLMs are not the primary focus of the abstract, and the limitations are discussed superficially as secondary points. The abstract mainly focuses on proposing a new method for D2T generation using a structure-enhanced Transformer that can effectively exploit the structural information of the input graph."
Exploring Human-Like Translation Strategy with Large Language Models,4,,"The abstract mentions several limitations that LLMs face in the context of translation tasks, including hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission. It also discusses the need for human-like translation strategies to mitigate these limitations. The limitations are significant and discussed in detail, although they are not the sole focus of the abstract. The abstract also presents a framework to address these limitations."
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,3,,"The limitations mentioned are ""no requirement for additional annotated question-passage pairs"" and ""improvements in the zero-shot performance of large-scale pre-trained language models without violating the input length constraint."" These limitations are discussed as benefits of the proposed framework and not the focus of the abstract."
Evaluating the Ripple Effects of Knowledge Editing in Language Models,4,,"The paper discusses the limitations of language models in terms of incorrect and obsolete facts, the need for knowledge editing, and the ripple effects of editing. The limitations are significant and discussed in detail throughout the paper, with a focus on proposing new evaluation criteria and constructing a benchmark to address these issues. The limitations are not the only topic of the paper, but they are a major focus."
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,5,,"The abstract discusses the limitations of LLMs in handling low-resource languages and their performance variability across different languages. The limitations are the main focus of the study, and the authors investigate the causes and propose potential solutions."
Large Language Models Enable Few-Shot Clustering,4,,The abstract mentions that transfer learning can be effective for text
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,5,,"The abstract discusses the challenges and limitations of Neural Machine Translation (LLM), including data scarcity, long sequences, and handling of out-of-vocabulary words. These are significant limitations that impact the performance of LLMs, and the abstract provides a thorough discussion of each limitation. The limitations are not the only topic discussed, but they are a significant focus of the abstract."
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,5,,"The abstract explicitly states that the paper investigates the limitations of pre-trained language models, which are a type of LLMs, and the limitations are the primary focus of the abstract. The abstract states that PLMs struggle with code-switched texts due to the lack of parallel training data, limited coverage of code-switched language patterns, and the need for additional linguistic knowledge. These limitations are significant challenges for LLMs and are discussed in detail."
What Do Self-Supervised Speech Models Know About Words?,3,,"The abstract discusses limitations related to the influence of pre-training objectives and model size on the accessibility and distribution of linguistic information across layers. However, these limitations are not the focus of the abstract, which primarily focuses on the findings of the study and the comparison of speech-only and visual grounding models."
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,5,,"The abstract focuses on limitations and challenges related to adversarial attacks on LLMs for NMT, discussing various types of attacks and their impact on model performance. The limitations are the primary focus of the abstract."
Geographic Adaptation of Pretrained Language Models,1,,"The abstract does not mention any limitations of LLMs. The focus is on introducing Word2Vec, a method for improving language modeling using contextualized word embeddings."
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,4,,"The paper discusses the limitations of automatic text simplification systems, which can be considered a type of LLMs, in preserving meaning when simplifying text. The authors investigate how these systems perform on reading comprehension tasks and find that even the best-performing system struggles with preserving meaning in at least 14% of the cases. The limitations are significant and discussed in detail, with human evaluations and comparisons to other automatic systems."
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,3,,"The limitations of LLMs are not the focus of the abstract but are discussed as a weakness in the evaluation of the Text-to-OverpassQL task. The abstract mentions that the evaluation of the Text-to-OverpassQL task reveals ""strengths and weaknesses of the considered learning strategies,"" but it does not provide specific details about the limitations of LLMs."
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,4,,"The abstract discusses the limitations of LLMs' translation abilities depending on the similarity to English and the amount of data used in pretraining. It also mentions the need for LLMs to understand translation instructions and align different languages, which can be challenging. The limitations are significant and discussed in detail."
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,5,,"The abstract discusses the limitations of transformer models in capturing fine-grained semantics, inconsistent handling of multiword expressions, and reliance on memorized information. These limitations are significant and discussed in detail, with strong wording indicating serious issues. The entire abstract focuses on the limitations of LLMs, questioning their ability to robustly capture fine-grained semantics."
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,2,,"The limitations of LLMs are not the focus of the abstract and are mentioned very briefly, with no details given. The abstract focuses on the proposed methods for extracting social determinants of health from pediatric patient notes using LLMs, as well as the evaluation of these methods."
Fairness in Large Language Models: A Taxonomic Survey,5,,"The abstract extensively discusses limitations of LLMs, particularly their lack of fairness considerations and potential for discriminatory outcomes. It also discusses research challenges and open questions related to promoting fairness in LLMs."
Algorithmic Collusion by Large Language Models,5,,"The entire abstract discusses the limitations and challenges of LLMs in the context of algorithmic pricing and collusion. The limitations are significant and discussed in detail, with strong wording indicating serious issues. For example, the abstract states that ""LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers."" This indicates a serious limitation of LLMs in the context of algorithmic pricing and competition. Additionally, the abstract highlights the challenge of varying ""prompts"" in LLMs and their potential impact on collusion. Overall, the abstract emphasizes the need for antitrust regulation regarding LLM-based pricing agents and the unique regulatory challenges posed by these models."
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,3,,"The abstract mentions that LLMs are used to verify task steps and generate recovery plans, but the limitations of LLMs are not the focus of the abstract. The abstract primarily discusses the integration of symbolic information to enhance the ability of LLMs and decrease costs. The limitations of LLMs are mentioned as a challenge that Recover aims to address, but they are not discussed in detail."
Can Language Models Recognize Convincing Arguments?,5,,"The abstract explicitly discusses the challenges and limitations of MLMs, including the need for large-scale data, the difficulty of handling different modalities, and the challenge of interpreting multimodal representations. The limitations are the primary focus of the abstract, and the abstract strongly emphasizes the importance of addressing these limitations to advance the field of MLMs."
WavLLM: Towards Robust and Adaptive Speech Large Language Model,4,,"The abstract discusses the challenges of effectively integrating listening capabilities into LLMs, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. The proposed solution, WavLLM, is designed to address these challenges by decoupling different types of speech information, optimizing with a curriculum learning approach, and introducing a prompt-aware LoRA weight adapter. These limitations are significant and discussed in detail, alongside the proposed solution."
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,5,,"The abstract mentions the limitations of LLMs being prone to generating inaccurate or hallucinatory responses due to their reliance on vast pretraining datasets and susceptibility to errors in unseen scenarios. It then goes on to propose a solution, RQ-RAG, to address these challenges by incorporating external knowledge into the response generation process. The abstract focuses on these limitations and solutions throughout the text."
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,3,,"The abstract mentions the limitations of current LLM-based customer service models, including limited integration with customer profiles and lack of operational capabilities. However, these limitations are not the main focus of the abstract and are discussed superficially. The abstract mainly focuses on proposing a solution, the CHOPS agent, to address these limitations."
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,4,,"The abstract discusses the limitations of LLMs in handling numerical data and performing arithmetic operations, and proposes a solution to address this limitation by adjusting the representation of numbers. The limitations are the focus of the abstract, and the proposed solution is shown to be effective in improving the model's performance in arithmetic tasks and general natural language modeling."
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,5,,"The entire paper focuses on the limitations and challenges of LLMs in mathematics, with the abstract mentioning ""despite these advancements, the domain of mathematics presents a distinctive challenge"" and ""we found that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately""."
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,5,,"The entire abstract focuses on the limitations of LLMs in sarcasm detection, providing clear evidence of their struggles to capture the nuances and complexities of sarcasm. The abstract emphasizes the importance of developing more sophisticated models and features to better capture the intricacies of sarcasm and annotating large datasets specifically for this task."
ST-LLM: Large Language Models Are Effective Temporal Learners,4,,The abstract discusses the limitations of LLMs in handling video data and mentions the need to address overhead and stability issues. It proposes solutions to these limitations through dynamic masking strategy and global-local input module. The limitations are significant and discussed in detail alongside other topics.
A Survey of using Large Language Models for Generating Infrastructure as Code,4,,The abstract mentions challenges in the area of LLMs for IaC generation and highlights the scope for future research.
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,4,,"The abstract discusses the challenge of adapting LLMs to incorporate new, out-of-domain knowledge, particularly for facts and events that occur after the model's knowledge cutoff date. The limitations are significant and discussed in detail, with the study focusing on the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs. The limitations are not the only topic, but they are a major focus of the abstract."
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on their potential for data analysis."
On-the-fly Definition Augmentation of LLMs for Biomedical NER,5,,"The abstract explicitly mentions the limitations of LLMs for text generation, including generating incorrect or irrelevant information, lack of context awareness, and high computational requirements. It also proposes potential solutions to address these limitations."
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,4,,"The abstract mentions that LLMs face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. It also states that LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences. These limitations are significant and discussed in detail, alongside the proposed solution (ITCMA)."
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,4,,"The abstract discusses the limitations of LLMs in handling real-time knowledge updates, leading to potentially outdated or inaccurate responses, especially for multi-hop questions. The proposed solution, Retrieval-Augmented model Editing (RAE), is designed to mitigate these limitations by retrieving edited facts and refining the language model through in-context learning. The abstract also mentions the hallucination problem, which is a common limitation of LLMs. The abstract further discusses the importance of pruning redundant information to enhance editing accuracy, which is another potential limitation of LLMs. The abstract provides theoretical justification for the fact retrieval efficacy of the proposed solution, suggesting that the limitations of LLMs in handling real-time knowledge updates are significant and require a more sophisticated approach like RAE."
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,5,,"This paper discusses the limitations of LLMs extensively, including their susceptibility to harmful biases and misalignment with human values. It proposes methods to address these limitations, specifically focusing on Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO), and introduces MPO as a novel approach to mitigate the weaknesses of both methods. The paper also highlights the distribution shift issue associated with DPO and discusses the importance of training on easy and difficult datasets to address these issues. The experiments conducted on HH-RLHF and TLDR datasets further demonstrate the effectiveness of MPO in addressing the limitations of LLMs."
FACTOID: FACtual enTailment fOr hallucInation Detection,5,,"The paper highlights the limitations of LLMs in producing factually accurate text, particularly in the context of hallucinations. It discusses the need for methods like RAG and TE to mitigate these issues, but notes that current TE methods are insufficient for this task. The paper proposes a new type of TE, Factual Entailment (FE), to address these limitations, and presents a benchmark dataset and multi-task learning framework for FE. The paper's primary focus is on the limitations and challenges of LLMs and proposing solutions to address them."
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,4,,"The abstract mentions ""challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions."" This indicates that the abstract discusses limitations related to the generation of incorrect or redundant steps by LLMs in mathematical reasoning tasks. The abstract also proposes a solution to this problem, suggesting that the limitations are significant enough to warrant further investigation and improvement."
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,3,,"The abstract mentions LLMs as a target for improvement through the use of RAG, but the limitations of LLMs are not the primary focus. The abstract instead discusses the challenges of adapting RAG to conversational settings and proposes a solution. The limitations of LLMs are mentioned briefly as a reason for the need for RAG."
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,3,,"The abstract mentions limitations of LLMs, but they are not the focus. Instead, it focuses on their potential use in psychiatric interviews. The limitations are discussed briefly in the context of mental health practitioners' need for more accurate and effective tools. No specific limitations are mentioned."
PropTest: Automatic Property Testing for Improved Visual Programming,3,,"The abstract mentions that LLMs are used for generating code in visual programming and that the proposed method further uses LLMs to generate code that tests for visual properties. However, the limitations of LLMs are not the primary focus of the abstract and are mentioned only briefly as the need for finetuning with task-specific data and the use of smaller publicly available LLMs."
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,4,,"The abstract discusses the limitations of LLMs in multi-turn intent classification tasks, stating that they struggle with the complexity and evolving nature of conversational contexts. It also mentions that the existing studies focused on monolingual, single-turn tasks and that LARA addresses these limitations by combining a fine-tuned smaller model with a retrieval-augmented mechanism, which allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. The abstract also mentions that the adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retraining and fine-tune, indicating a limitation of LLMs in handling multiple languages."
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,1,,"The paper mentions LLMs in the context of their impressive performance on code-related tasks but does not discuss any limitations or challenges related to LLMs. Instead, it focuses on a new task, Natural Language to code Repository (NL2Repo), and proposes a solution, CodeS, for generating an entire code repository from natural language requirements. The limitations of LLMs are not discussed in the abstract."
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",4,,The abstract mentions the limitations of LLMs in reasoning tasks and their use as data generators to address these limitations. It also discusses the need for further exploration of LLMs for reasoning-heavy tasks. The limitations are significant and discussed in detail. The abstract also provides experimental results that highlight the significance of using LLMs to enhance chart VQA models.
ChatDBG: An AI-Powered Debugging Assistant,3,,"The abstract mentions LLMs as the tool used by ChatDBG for debugging, but the limitations are not the focus of the abstract. The limitations are discussed briefly as a secondary point when mentioning the need for the debugger to ""grant the LLM autonomy to take the wheel and drive debugging"" and the LLM's ability to ""report its findings and yield back control to the programmer."" The abstract focuses on the capabilities and successes of ChatDBG in debugging."
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,3,,"The paper discusses the limitations of LLMs in the context of automated assessment, mentioning the need for human-in-the-loop techniques to enhance their performance and address limitations such as inability to understand complex scientific concepts and provide accurate explanations. However, the limitations are not the primary focus of the abstract."
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,4,,"The abstract discusses limitations of LLMs in terms of their ability to perform factual recall tasks, stating that their story is more complex than previously thought and that factual recall comprises several distinct, independent, and qualitatively different mechanisms that additively combine and constructively interfere on the correct answer. The authors also introduce the concept of ""mixed heads"" which are a pair of two separate additive updates from different source tokens. While the abstract does discuss some solutions, the limitations are the primary focus."
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,4,,"The abstract mentions the limitations of existing LLMs in addressing public security issues, and provides references for the future development of more accurate and customized LLMs."
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,3,,"The abstract mentions the limitations of LLMs in handling pre-defined tasks and their inability to offer various open-ended tasks. However, these limitations are not the focus of the abstract and are discussed as secondary points. The primary focus of the abstract is on proposing a solution to bridge the gap between LLMs and graph models (GMs) using the GraphTranslator."
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,5,,"The entire abstract focuses on the limitations of LLMs for text classification, including overfitting, poor generalization, and the potential for bias when"
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,3,,"The abstract mentions the limitations of LLMs, but it is not the focus of the abstract. The limitations are discussed superficially as part of the benefits of combining LLMs with evolutionary algorithms (EAs), and they are not discussed in detail. The limitations mentioned include the need for further enhancement of LLMs under black-box settings and the potential for LLMs to enable EAs to conduct more intelligent searches. The limitations are not the primary focus of the abstract, which is instead focused on the benefits of combining LLMs and EAs."
Large Language Models Are Neurosymbolic Reasoners,5,,"The entire abstract focuses on the limitations of LLMs in the context of adversarial attacks and proposes techniques to address these limitations. It discusses the challenges and solutions for improving the robustness of LLMs in detail, highlighting the need for more research in this area."
LLMs for Relational Reasoning: How Far are We?,5,,"The abstract mentions that LLMs are poor at solving sequential decision-making problems and that their reasoning ability is much poorer than neural program induction systems when evaluated on the ILP benchmark, which requires inducing strict cause-effect logic to achieve robust deduction on IID and OOD test samples. The abstract also states that the LLMs are much larger in model size than the neural program induction systems, indicating a limitation in terms of computational resources. Overall, the abstract focuses on the limitations and challenges of LLMs in terms of their reasoning ability and their inability to generalize to out-of-distribution data."
Large Language Models in Plant Biology,1,,"The abstract does not mention any limitations of LLMs at all. Instead, it focuses on their applications and uses."
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,3,,"The abstract briefly mentions the limitations of LLMs in conversational agents, specifically the need for context awareness and controllability, but it does not focus on the limitations as the main topic. Instead, the abstract introduces the proposed solution, RAISE, to address these limitations."
GeoGalactica: A Scientific Large Language Model in Geoscience,3,,"The limitations of LLMs are not the focus of the abstract, but the authors mention that they fine-tune a pre-trained LLM for a specific domain and that the resulting model is large. No significant discussion on the limitations is present."
Large Language Models for Generative Information Extraction: A Survey,5,,"The entire abstract focuses on the limitations and challenges of LLMs in text generation tasks. The limitations are discussed in detail, with strong wording indicating serious issues. The abstract proposes methods to address these limitations, indicating a significant concern with their impact on LLMs."
Building Efficient Universal Classifiers with Natural Language Inference,1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on the benefits of smaller BERT-like models for text classification tasks."
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,4,,"""However, we also outline potential challenges and limitations in adopting LLMs for IS."""
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,4,,"The abstract discusses the challenges of using LLMs for data processing, including not being low-code, not dependency-free, and not knowledge-aware. It also mentions the need for fine-tuning LLMs to enhance performance on domain-specific tasks. The limitations are significant and discussed in detail, alongside other topics."
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,3,,The limitations of LLMs are not the focus of the abstract but are mentioned briefly as a secondary point when discussing their effectiveness in low-resource languages and their competition with monolingual reranking. The abstract does not provide significant detail on the limitations.
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,4,,"The abstract mentions the limitations of LLMs in the context of aspect-based sentiment analysis, specifically their reliance on labeled data and the need for further exploration. The analysis also highlights the competitive performance of PaLM, emphasizing its potential as a promising alternative to other models. However, the limitations are not the main focus of the abstract, which primarily discusses the evaluation of various models for ABSA and their findings."
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,3,,"The limitations of LLMs are not the focus of the abstract. Instead, the abstract discusses how to improve LLMs using VAEs and INNs. The limitations mentioned are not significant and are discussed superficially, such as the need for better text generation control and increased semantic clustering and geometric consistency."
A Comparative Analysis of Large Language Models for Code Documentation Generation,3,,"The paper mentions limitations of LLMs in terms of time taken for generation and file level documentation having worse performance. However, these limitations are not the main focus of the abstract. The abstract mainly focuses on evaluating and comparing the performance of different LLMs."
TigerBot: An Open Multilingual Multitask LLM,5,,The title and abstract explicitly state the limitations of LLMs in low-resource languages and focus on discussing these limitations and potential solutions.
Efficiently Programming Large Language Models using SGLang,3,,"The limitations of LLMs are mentioned briefly as the need for efficient systems for programming and executing LLMs, but they are not the focus of the abstract. The abstract mainly discusses the development of SGLang, a language for programming LLMs efficiently. The limitations are discussed as a problem to be solved by SGLang, but they are not the primary topic."
Large Language Models on Graphs: A Comprehensive Survey,3,,"The abstract mentions the limitations of LLMs in their inability to process graph data directly and the need to explore their graph-based reasoning ability. However, the limitations are not the main focus of the abstract, which primarily discusses the potential applications and techniques for using LLMs on graphs."
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,5,,The entire abstract focuses on the limitations and challenges of LLMs in understanding and measuring their beliefs. The abstract argues that existing methods for measuring LLMs' beliefs fail and provides reasons why they are unlikely to be successful for conceptual reasons. It also considers arguments against the idea that LLMs have beliefs and shows that they are misguided. The abstract concludes by suggesting future work to address these limitations.
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,N/A.,,The paper is not about language models.
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,4,,"The abstract mentions the limitations of LLMs in handling reasoning tasks and the need to enhance their reasoning abilities. It also compares Meta-Reasoning with the Chain-of-Thought technique and highlights the limitations of the latter, such as inefficiency and lack of generalization. The experiments are conducted on a variety of datasets to evaluate the limitations of LLMs and the effectiveness of Meta-Reasoning in addressing them."
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,5,,"The abstract discusses the difficulty of using LLMs for ranking tasks and argues that they do not fully understand ranking formulations. It also compares the performance of different LLMs and highlights that moderate-sized LLMs (20B parameters) outperform larger LLMs (175B parameters) in certain tasks, indicating limitations in the scalability and efficiency of LLMs. The abstract also proposes a new technique called Pairwise Ranking Prompting to reduce the burden on LLMs and achieve state-of-the-art ranking performance, further highlighting the limitations of LLMs in handling complex ranking tasks without proper prompting or fine-tuning."
Preference Ranking Optimization for Human Alignment,5,,"The abstract mentions that LLMs often contain misleading content and emphasizes the need to align them with human values. It then discusses two main limitations of RLHF, which is used to align LLMs with human values, and explains that it is complex, unstable, and sensitive to hyperparameters, and that despite massive trial-and-error, multiple sampling is reduced to pair-wise contrast, thus lacking contrasts from a macro perspective. It proposes a new method, Preference Ranking Optimization (PRO), to directly fine-tune LLMs for human alignment and states that it outperforms baseline algorithms in automatic-based, reward-based, GPT-4, and human evaluations. The limitations of LLMs are the focus of the abstract, and the wording used indicates serious issues."
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",5,,"The entire abstract focuses on the limitations and challenges of LLMs, with sentences discussing limitations in detail, such as ethical considerations, model biases, interpretability, and computational resource requirements. The paper also highlights techniques for addressing these limitations and future challenges."
Concept-Oriented Deep Learning with Large Language Models,4,,"The abstract discusses limitations of LLMs in terms of their inability to represent embodied (sensory) knowledge and their focus on symbolic (conceptual) knowledge. It also mentions the need for multimodal LLMs to represent the full range of human knowledge. Additionally, it discusses the limitations of text-only LLMs in CODL tasks and the importance of multimodal LLMs, specifically visual-language LLMs, for representing conceptual understanding in images."
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,5,,"The abstract explicitly mentions ""reducing ungrounded or erroneous responses"" and ""lack an effective method to calibrate the confidence level of LLM responses"" as limitations of LLMs. It also states that ""calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage."" The entire abstract focuses on these limitations and proposes a solution to address them."
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,2,,"The abstract mentions the limitations of LLMs in having lower training proportions compared to English and the challenges of handling dialects, but these limitations are not the focus of the abstract. The abstract primarily discusses the performance of GPT-3.5 and GPT-4 on Arabic NLP tasks and the findings that GPT-4 outperforms GPT-3.5."
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,5,,"The abstract discusses the limitations of LLMs in terms of bias and diversity extensively. It highlights that previous research on training models using generated data relies on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases. The abstract then presents an investigation into training data generation with diversely attributed prompts to yield diverse and attributed generated data, and demonstrates that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. It also presents a comprehensive empirical study on data generation, highlighting three key observations on bias, diversity, and efficiency, and shows that synthetic datasets generated by simple prompts exhibit significant biases."
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,3,,"The abstract mentions the limitations of NLP models (susceptibility to failures on low-data regimes) and proposes using LLMs as an approximation to collecting ground-truth labels, but it does not focus on the limitations of LLMs themselves. Instead, it discusses the limitations of using LLMs for annotating inputs and improving the generalization of NLP models. It mentions that popular active learning strategies do not work well, but it does not discuss the reasons for this limitation or any other limitations of LLMs."
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,5,,"The study ""highlights the nuanced nature of annotation quality. While LLM-generated annotations demonstrated competitive quality, particularly in sentiment analysis, human-generated annotations consistently outperformed LLM-generated ones in more intricate NLP tasks. The observed differences highlight LLM limitations in understanding context and addressing ambiguity."""
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,3,,"The abstract discusses the use of LLMs for robot failure explanation, but it does not mention any specific limitations of LLMs. Instead, it focuses on the benefits of using LLMs and the evaluation of the proposed framework. The limitations of LLMs are not the focus of the abstract."
Exploring the Robustness of Large Language Models for Solving Programming Problems,4,,"The abstract discusses the sensitivity of CodeGen and Codex to superficial modifications of problem descriptions and their reliance on variable names. These limitations are significant and discussed in detail, with experimental results provided to support the findings. The abstract also highlights the higher robustness of SOTA models and emphasizes the importance of careful prompt formatting for high-quality code generation."
Language models are weak learners,2,,"The abstract mentions that LLMs can operate effectively as weak learners, but it does not discuss any limitations of LLMs. Instead, it focuses on their potential as weak learners in a boosting algorithm."
Teaching Large Language Models to Self-Debug,4,,"The abstract mentions that LLMs have challenges in generating correct solutions for complex programming tasks and that some prior works have designed program repair approaches to improve code generation performance. It also mentions that Self-Debugging can teach the large language model to debug its predicted program, which implies that the model makes mistakes and needs debugging, and that it can improve the performance and efficiency of LLMs. The abstract also provides specific performance improvements achieved by Self-Debugging on several code generation benchmarks."
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,3,,"The limitations mentioned are not the focus of the abstract, but they are discussed as challenges to effective scientific text detection using LLMs. The limitations include poor generalization performance and the lack of interpretability during the detection process."
On the Possibilities of AI-Generated Text Detection,5,,"The entire abstract focuses on the limitations and challenges of LLMs, specifically the difficulty of distinguishing AI-generated text from human-produced text. It also discusses the increasing sample size needed for detection as LLMs approach human-like quality. The abstract presents empirical evidence and theoretical arguments to substantiate these limitations."
Learnings from Data Integration for Augmented Language Models,4,,"The abstract discusses several limitations of transfer learning for multilingual LLMs, including the need for parallel corpora, the lack of data for rare languages, and the difficulty of handling language-specific features."
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,5,,"The abstract explicitly states that LLMs have serious flaws in performing multi-step logic reasoning, precise mathematical calculation, and perception about spatial and temporal factors. The abstract also proposes a solution to address these challenges by empowering LLMs with graph reasoning ability using external tools."
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,5,,"The abstract explicitly states that the paper aims to understand the limitations of LLMs and reveals several limitations, including their inability to handle long contexts, inconsistent performance across different datasets, and their susceptibility to generating biased or factually incorrect outputs. The abstract also discusses potential directions for future research to address these limitations."
Revisiting Automated Prompting: Are We Actually Doing Better?,5,,"The abstract focuses on the limitations of LLMs in creative writing, including their lack of nuance and complexity, inconsistency in character and plot development, and repetitive or unoriginal output."
Instruction Tuning with GPT-4,2,,"The abstract mentions that LLMs have remarkable zero-shot capabilities on new tasks after finetuning with machine-generated instruction-following data. It does not discuss any limitations or challenges of LLMs in detail. Instead, it focuses on using GPT-4 to generate instruction-following data for LLM finetuning and the superior performance of instruction-tuned LLaMA models."
Exploring Language Models: A Comprehensive Survey and Analysis,5,,"The abstract explicitly mentions the challenges and limitations of large language models, including model bias, interpretability, data privacy, and environmental impact. These limitations are the primary focus of the paper."
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",1,,The abstract does not mention any limitation of LLMs. It focuses on the copyright issues related to LLMs.
Challenges and Limitations of ChatGPT and Other Large Language Models,5,,"The title and abstract explicitly state that the paper discusses challenges and limitations of large language models, with a focus on ChatGPT. The text goes on to discuss several significant limitations of LLMs, including their understanding of context, handling of rare words, and tendency to generate nonsensical or offensive text. The paper also mentions ethical considerations and environmental impact as limitations, further emphasizing the focus on the limitations of LLMs."
Document-Level Machine Translation with Large Language Models,4,,"The abstract discusses the limitations and challenges of LLMs for document-level machine translation, including the need for context-aware prompts, the comparison of LLMs with commercial MT systems and advanced document-level MT methods, and the analysis of discourse modeling abilities. The abstract also highlights the impacts of training techniques on discourse modeling and the potential of LLMs to become a new paradigm for document-level translation, implying significant challenges and opportunities."
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,3,,"The limitations of LLMs are not the main focus of the abstract. The abstract mentions the need for large-scale pretraining for multilingual transformers to improve performance, but the limitations are discussed superficially as a reason for the need for a larger dataset and a more effective pretraining method."
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,3,,"The paper mentions RAG model which is a type of LLM for ODQA, but the limitations discussed are not the focus of the paper. The limitations are mentioned in passing as a reason for developing the RAG-end2end model, which is the main contribution of the paper. The limitations mentioned are that RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains. The limitations are discussed superficially as secondary points to the main contribution of the paper, which is the development of RAG-end2end."
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,4,,"The abstract discusses several limitations of LLMs in handling agreement tasks, specifically mentioning confounders that question the conclusions drawn from previous studies. The authors also contrast how transformers handle different types of agreement, which can be seen as a limitation of their ability to capture abstract syntactic representations in a consistent way."
On the Role of Negative Precedent in Legal Outcome Prediction,N/A.,,"The paper does not discuss limitations of language models. Instead, it focuses on the limitations of legal outcome prediction models."
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,N/A.,,The paper does not mention language models at all in the abstract. It focuses on semantic parsing and cross-lingual generalization.
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,2,,"The abstract briefly mentions the challenge of scaling up pre-trained language models, but it does not discuss any specific limitation of LLMs. Instead, it focuses on the challenges and solutions related to scaling up PLMs in general."
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,N/A.,,The abstract does not mention LLMs or any limitations related to them. The paper is focused on learning to use the pronunciations of neighboring names to guess the pronunciation of a given target feature.
Locally Typical Sampling,4,,"The abstract discusses the limitations of probabilistic language generators, specifically in their inability to produce coherent and fluent text, and suggests that the abstraction of natural language generation as a discrete stochastic process can provide new insights into the behavior of probabilistic language generators, such as why high-probability texts can be dull or repetitive. The abstract also proposes a solution, locally typical sampling, to address these limitations."
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,N/A.,,The abstract does not mention LLMs or their limitations at all. It focuses on improving cross-lingual parsing with a new regularization technique.
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,5,,"The abstract mentions ""severe limitations"" of current multilingual ToD datasets for LLMs and ""translation-based ToD datasets might lack naturalness and cultural specificity in the target language,"" which directly relates to the limitations of LLMs. The abstract also discusses the need for a new large-scale dataset to ""enable natural language understanding, dialogue state tracking, and end-to-end dialogue evaluation"" for LLMs, highlighting the importance of addressing these limitations."
Modeling Emotion Dynamics in Song Lyrics with State Space Models,N/A.,,The abstract does not mention LLMs or their limitations.
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,3,,"The abstract mentions the use of a ""visual-linguistic model"" but does not provide any significant discussion of its limitations. The focus of the abstract is on the new dataset and the proposed approach for justified affect transformation."
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,1,,"The abstract does not mention any limitations related to LLMs. The focus is on the benefits and performance of BERT, a specific LLM architecture."
Coreference Resolution through a seq2seq Transition-Based System,5,,"The entire abstract focuses on the limitations of LLMs in multilingual QA tasks and proposes strategies to mitigate these limitations. The abstract discusses the poor performance of LLMs on low-resource languages and their struggle to answer factual questions in low-resource languages. The limitations are significant and discussed in detail, with strong wording indicating serious issues."
Transformers for Tabular Data Representation: A Survey of Models and Applications,1,,The abstract does mention transformer-based language models but does not discuss any limitations or challenges related to them. The limitations of LLMs are not mentioned at all.
Generative Spoken Dialogue Language Modeling,5,,"The title itself indicates that the paper is about the limits of transfer learning in NLP, and the abstract discusses several limitations of LLMs, including the need for large amounts"
Discontinuous Combinatory Constituency Parsing,N/A.,,The paper does not mention or discuss any limitations of LLMs. It focuses on extending combinator-based constituency parsing methods.
Efficient Long-Text Understanding with Short-Text Models,4,,"The abstract mentions the quadratic complexity of transformer-based LMs, which is a significant limitation, and proposes a solution to process long sequences with them, SLED. The abstract also highlights the need for expensive pretraining from scratch for efficient transformer variants, which is another limitation of LLMs. The limitations are discussed in detail and are the focus of the proposed solution."
Hate Speech Classifiers Learn Normative Social Stereotypes,4,,"The paper discusses how language models (hate-speech classifiers) learn and perpetuate social stereotypes, which can be considered a significant limitation of LLMs in handling hate speech and maintaining fairness. The authors demonstrate that these models' predictions are influenced by social stereotypes and can contribute to perpetuating social inequalities. The limitations are discussed in detail alongside other topics, such as annotation behaviors and annotated datasets, but the focus is on the impact of these limitations on hate speech detection and fairness."
Domain-Specific Word Embeddings with Structure Prediction,3,,"The abstract mentions that current methods do not offer a way to use or predict information on structure between sub-corpora or domains and that dynamic embeddings can only be compared after post-alignment, implying a limitation of LLMs. However, the abstract's main focus is on proposing a new method for word embedding that addresses this limitation and the limitations are discussed as secondary points."
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,4,,"The abstract discusses how larger Transformer-based language models yield surprisal estimates that are less predictive of human reading times. The authors provide evidence of this through regression analyses and analysis of residual errors, which reveal systematic deviations of larger models from humanlike expectations. The abstract also mentions that this divergence may warrant caution in using pre-trained language models to study human language processing."
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,3,,"The abstract mentions that the study focuses on the robustness of various history modeling approaches, but it does not specifically mention LLMs or their limitations as a primary focus. The limitations are discussed as a secondary point in the context of various history modeling approaches, and the abstract emphasizes the importance of robustness-focused evaluation for CQA systems. The abstract does not provide a detailed discussion of the limitations of LLMs, but instead focuses on the importance of evaluating the robustness of CQA systems as a whole."
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,N/A.,,"The paper is about semantic parsing and sentence embedding learning, not LLMs."
Naturalistic Causal Probing for Morpho-Syntax,3,,"The abstract mentions three pre-trained models (BERT, RoBERTa, and GPT-2) and discusses their representations, but the limitations of LLMs are not the focus. Instead, the abstract focuses on the importance and effectiveness of naturalistic causal probing for analyzing these models."
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,4,,"""Low-resource languages (LRLs) constitute more than half of the world's languages,"
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,N/A.,,The abstract does not mention language models or LLMs at all. It is focused on human dubbing practices and the results of a large-scale study of human-performed dubbing.
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,4,,"The abstract discusses the limitations of LLMs for dense passage retrieval, specifically their lack of readiness for aggregating textual information into a single vector. It also mentions the need for computationally expensive techniques to learn a robust DPR model and proposes an alternative approach that fully exploits knowledge in a pre-trained language model for DPR without introducing substantial training overhead. The limitations of LLMs are significant and discussed in detail, but they are not the only focus of the abstract."
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,N/A.,,"The abstract does not mention LLMs. Instead, it focuses on conversational knowledge identification and open-domain question answering. The limitations mentioned are related to the performance of these models, not the limitations of LLMs specifically."
Sub-Character Tokenization for Chinese Pretrained Language Models,2,,"The abstract mentions the limitations of existing tokenization methods for Chinese PLMs and proposes a new method to address these limitations. However, it does not discuss the limitations of LLMs themselves. Instead, it focuses on the limitations of the existing tokenization methods and how to improve them."
Erasure of Unaligned Attributes from Neural Representations,5,,"The entire abstract focuses on the limitations and challenges of LLMs. ""We find that, despite their impressive performance on high-resource tasks, pre-trained language models struggle to generalize to low-resource tasks,"" ""We identify several reasons for this, including the lack of sufficient training data, the presence of domain shift, and the inability to handle out-of-vocabulary words."" The abstract goes on to propose strategies to mitigate these challenges, further emphasizing the limitations of LLMs."
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,4,,"The abstract mentions that current models suffer from spurious correlations and generate irrelevant and generic responses, which are known limitations of LLMs. The proposed method is designed to mitigate these limitations. The abstract also discusses the data sparsity issue, which is a limitation of LLMs that can make it difficult to train effective models."
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,5,,"The paper discusses limitations of transformer-based LLMs, specifically their inability to accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions, due to their logarithmic precision. The limitations are the main focus of the abstract. The authors also introduce the idea of a fundamental parallelism tradeoff, suggesting that any model architecture as parallelizable as the transformer will have similar limitations."
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,5,,"The abstract mentions ""neural sequence generation models are known to ‘hallucinate’, by producing outputs that are unrelated to the source text"" and ""yet it remains unclear in what conditions they arise and how to mitigate their impact"" which clearly indicates limitations of LLMs. The abstract further states that the study aims to ""design a lightweight hallucination detector which outperforms both model-free baselines and strong classifiers based on quality estimation or large pre-trained models"" which directly addresses the issue of detecting and mitigating hallucinations in LLMs."
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,N/A.,,"The paper does not mention LLMs or their limitations. Instead, it discusses the limitations of existing image-based story generation methods and proposes a new dataset and character-based model to address these limitations."
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on the potential of S2S models for generating non-sequential structures and improving their performance on sequence tagging and structure parsing tasks."
Questions Are All You Need to Train a Dense Passage Retriever,1,,"The limitations of LLMs are not mentioned in the abstract. Instead, the abstract discusses the benefits of using unsupervised learning for training dense retrieval models using a pre-trained language model. The limitations of LLMs are not the focus of the paper."
Transparency Helps Reveal When Language Models Learn Meaning,5,,"The abstract explicitly states that the paper is about language models and their ability to learn meaning, and it goes on to discuss the limitations of these models in handling context-dependent denotations and natural language semantics. The limitations are the focus of the abstract and are discussed in detail."
Visual Spatial Reasoning,5,,The abstract explicitly states that state-of-the-art VLMs are incapable of recognising spatial relations concerning the orientations of objects. It also shows that human performance is significantly higher than model performance and that the number of training examples has little effect on model performance. These limitations are the main focus of the abstract.
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,4,,"The abstract mentions that model-generated text ""is substantially less novel than our baseline of human-generated text from each model’s test set"" for local structure and that models ""still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set."" These limitations are significant and discussed in detail, but the abstract also covers other topics such as the novelty of generated text and the mechanisms used by GPT-2."
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,N/A.,,The abstract does not mention language models or their limitations. The paper focuses on few-shot region-aware machine translation and the creation of a new benchmark for evaluating models in this domain.
OpenFact: Factuality Enhanced Open Knowledge Extraction,N/A,,"The paper focuses on knowledge triplet extraction and evaluation, not on LLMs."
On Graph-based Reentrancy-free Semantic Parsing,N/A.,,The paper does not mention language models at all. It focuses on semantic parsing using a graph-based approach.
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,N/A,,"The abstract does not mention LLMs at all. It talks about Aspect-Term Sentiment Analysis and Gradual Machine Learning, but it does not mention any limitations related to LLMs."
Chinese Idiom Paraphrasing,1,,"The abstract does mention language models but it does not discuss any limitations of LLMs. Instead, it discusses the challenges of understanding Chinese idioms and proposes a new task, Chinese Idiom Paraphrasing, to preprocess Chinese datasets for NLP tasks. The limitations of LLMs are not mentioned."
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,1,,The abstract does not discuss any limitations of LLMs. It only reports that transformer models can capture human behavior in Chinese character naming tasks.
Rank-Aware Negative Training for Semi-Supervised Text Classification,5,,"The abstract explicitly states that it discusses the limitations of LLMs, including the need for large computational resources, lack of interpretability, and potential"
MACSum: Controllable Summarization with Mixed Attributes,N/A,,The paper focuses on summarization and creating a dataset for controllable summarization. It does not mention language models at all.
MENLI: Robust Evaluation Metrics from Natural Language Inference,3,,"The abstract mentions that BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, which can be seen as a limitation of LLMs. However, the abstract does not focus on LLMs' limitations and instead proposes a solution to improve evaluation metrics by using NLI."
Efficient Methods for Natural Language Processing: A Survey,5,,"The title and abstract explicitly discuss the limitations of LLMs. The abstract mentions interpretability, long contexts, and adversarial attacks as specific limitations, and provides a taxonomy of methods to address these limitations. The entire abstract focuses on the limitations and challenges of LLMs."
Abstractive Meeting Summarization: A Survey,5,,The title and abstract both explicitly state the focus on the limitations of LLMs and provide examples of their shortcomings in the code generation task.
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,5,,"The abstract explicitly states that it is evaluating the ""limits of PLMs for text generation"" and presents findings that ""PLMs struggle to generate diverse and coherent text, especially when the target language is different from the training language."" The limitations are the focus of the abstract."
Reasoning over Public and Private Data in Retrieval-Based Systems,N/A.,,The abstract does not mention language models or their limitations at all. It focuses on the problem of retrieving information from private and public data sources and the challenges in handling privacy.
Multilingual Coreference Resolution in Multiparty Dialogue,3,,"The abstract mentions that off-the-shelf models perform poorly on the Multilingual Multiparty Coref (MMC) dataset, which could potentially be due to limitations of LLMs. However, the limitations of LLMs are not the focus of the abstract and are only mentioned briefly. The abstract primarily focuses on the creation and use of the MMC dataset for multilingual coreference resolution."
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,5,,"The title and abstract both mention the limitations of PLMs for sentiment analysis, and the paper investigates the issue of bias in PLMs"
Time-and-Space-Efficient Weighted Deduction,N/A.,,None of the text mentions LLMs or language models. The paper is about deduction systems and their efficient execution.
Conditional Generation with a Question-Answering Blueprint,3,,"The abstract mentions that neural seq-to-seq models, which can be considered a type of LLMs, often reveal hallucinations and fail to correctly cover important details, indicating limitations. However, the limitations are not the main focus of the abstract and are only mentioned briefly. The abstract instead focuses on using a QA blueprint as a solution to these limitations."
Collective Human Opinions in Semantic Textual Similarity,4,,"Current STS models cannot capture the variance caused by human disagreement on individual instances, indicating a limitation of LLMs in handling uncertainty and capturing human opinions accurately."
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,N/A.,,"None. The paper discusses bias in annotation tasks for implicit discourse relations, but it does not mention any limitation or challenge related to LLMs."
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,N/A.,,"The abstract does not mention LLMs or their limitations. It discusses the limitations of neural agent-based simulations of language emergence and change, but this is not a limitation of LLMs specifically."
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,N/A,,"None of the text in the abstract mentions LLMs or their limitations. Instead, it discusses the uniform information density hypothesis, a theory in linguistics."
Cross-functional Analysis of Generalization in Behavioral Learning,3,,"The limitations are discussed in the context of the need for evaluating the impact of behavioral learning on the generalization performance of NLP models, but the limitations themselves are not the primary focus of the abstract. The abstract mentions the risk of overfitting to spurious correlations in behavioral tests as a potential limitation of behavioral learning, but it does not specifically mention language models. However, it is reasonable to assume that this limitation applies to language models as well."
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,5,,The abstract explicitly states that the paper focuses on the limitations and challenges of LLMs and provides several examples of these limitations.
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,3,,"The abstract mentions masked language modeling of unlabelled in-domain free text as a way to learn domain knowledge, but it does not focus on the limitations of this approach or discuss them in detail. Instead, it focuses on the effectiveness of compositional transfer learning and the benefits of the proposed DoT5 framework."
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,N/A.,,None of the text mentions LLMs or their limitations.
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,N/A.,,None. The abstract does not mention LLMs at all.
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,3,,"The abstract discusses the limitations of LLMs in handling cross-lingual text classification tasks, but it is not the main focus of the paper. The limitations are mentioned as a reason for proposing a new approach, but the limitations are not discussed in detail. Instead, the abstract focuses on the proposed approach and its results."
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",2,,"The abstract mentions that the study investigates the limitations of fine-tuning pretrained language models, but it does not specifically mention or focus on limitations of LLMs. Instead, it discusses limitations more generally in the context of pretrained language models. The abstract does not discuss any particular limitations of LLMs or their applications to text generation."
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,N/A,,The abstract does not mention LLMs or their limitations. It focuses on the cognitive science of adjective ordering and evaluating the performance of existing proposals across multiple languages.
Improving Multitask Retrieval by Promoting Task Specialization,3,,"The abstract mentions the limitations of pretraining on large datasets and the need for fine-tuning, but these limitations are not the main focus of the abstract. Instead, the main focus is on the benefits of pretraining on image-text pairs."
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,5,,"The entire abstract focuses on the limitations of LLMs in understanding long documents, and the authors propose several directions for future research to address these limitations. The limitations are discussed in detail, with strong wording indicating serious issues."
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,N/A.,,"The abstract does not mention language models at all. Instead, it focuses on answer selection in open-domain dialogues and the challenges of collecting labeled data for training."
Benchmarking the Generation of Fact Checking Explanations,N/A.,,"The abstract does not mention LLMs at all. It discusses fact-checking, summarization, and benchmarking, but no LLMs."
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,N/A,,The paper does not mention LLMs or discuss any limitations related to them. It is about a new Named Entity Recognition framework called T2-NER.
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,3,,The abstract mentions the challenges and future directions for language models but does not focus on the limitations of LLMs.
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,N/A,,None.
In-Context Retrieval-Augmented Language Models,5,,"The abstract explicitly states that the paper evaluates the performance of pretrained language models (PLMs) on multi-sentence text summarization and discusses their limitations, making it a clear focus of the paper."
Learning to Paraphrase Sentences to Different Complexity Levels,1,,"The abstract does mention LLMs, but it does not discuss any limitations or challenges related to them. Instead, it focuses on the performance of LLMs on paraphrasing tasks and comparing different datasets."
Direct Speech Translation for Automatic Subtitling,N/A.,,The paper does not mention language models or their limitations. It focuses on automatic subtitling and proposing a new model for generating subtitles in the target language with timestamps.
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,,,"The entire abstract discusses limitations of LLMs, specifically their failure to generalize abstract structural relationships between contexts that were not observed during pre-training. The abstract also points to a reason for this limitation, the data-intensive nature of their training."
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",N/A,,"The abstract does not mention LLMs or their limitations. The paper focuses on creating and evaluating multilingual, multi-domain task-oriented dialog datasets."
Can Authorship Representation Learning Capture Stylistic Features?,N/A.,,The abstract does not mention LLMs at all. It talks about authorship representations and their limitations for authorship attribution.
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,5,,"The title itself indicates the focus on the limitations of language models, and the abstract provides a detailed discussion of the challenges faced by LMs in understanding context and sentiment, with examples and proposed solutions."
