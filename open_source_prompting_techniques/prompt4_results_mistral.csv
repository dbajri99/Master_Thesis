Title,LMs,Limitations of LLMs,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,2,,"""The dialogue system is constructed on top of LLMs, focusing on defining specific roles for individual modules. We show that using multiple independent sub-modules working cooperatively on this task can improve performance and handle the typical constraints of using LLMs, such as context limitations."""
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,4,,"""Relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains."""
Re3val: Reinforced and Reranked Generative Retrieval,,,"""In this study, we present a comprehensive analysis of adversarial attacks against large language models (LLMs) using a variety of methods including text perturbations, semantic manipulations, and adversarial prompts."""
Reward Engineering for Generating Semi-structured Explanation,5,,"""Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL)."""
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,4,,"""Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations."""
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,1,,"""In the first step, a set of perplexity and log-likelihood based numeric features are elicited from an LM for a text instance to be classified."""
Evaluating Large Language Models Trained on Code,4,,"""Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts, but the model has limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables."""
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,2,,"""Current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases."""
ICE-Score: Instructing Large Language Models to Evaluate Code,2,,"""We compare the performance of BERT, mBERT, XLM-RoBERTa, and XLM-r100 on a diverse set of 11 languages."""
Transformer-specific Interpretability,4,,"""However, their inner workings, like many other neural networks, remain opaque... In spite of the widespread use of model-agnostic interpretability techniques, including gradient-based and occlusion-based, their shortcomings are becoming increasingly apparent for Transformer interpretation, making the field of interpretability more demanding today."" ""By the end of the tutorial, we hope participants will understand the advantages (as well as current limitations) of Transformer-specific interpretability methods."""
Can docstring reformulation with an LLM improve code generation?,5,,"""We conclude by examining a series of questions, accompanied by in-depth analyses, pertaining to the sensitivity of current open-source LLMs to the details in the docstrings, the potential for improvement via docstring reformulation and the limitations of the methods employed in this work."""
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,5,,"""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount."""
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,5,,"""The remarkable performance of large language models (LLMs) in zero-shot language understanding has garnered significant attention. However, employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size."""
Document-Level Language Models for Machine Translation,5,,"""In this work, we propose a novel multi-task learning framework to improve fairness in language models."""
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,5,,"""Without published experimental evidence on the matter, it is difficult for speakers of the world{'}s diverse languages to know how and whether they can use LLMs for their languages. Our analysis reveals that a language{'}s resource level is the most important feature in determining ChatGPT{'}s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages."""
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",3,,"""With that said, critical errors still abound, including occasional content omissions."""
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",1,,"""While large language models have made remarkable advancements in natural language generation, their potential in machine translation, especially when fine-tuned, remains under-explored."" (This statement does not mention any limitations, but rather the potential for improvement.)"
Automating Behavioral Testing in Machine Translation,3,,"""Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",5,,"""In this work, we investigate the limitations of pretrained language models (PLMs) in textual entailment (TE) by probing their understanding of logical connectives and their ability to distinguish between entailing and non-entailing relationships."""
GenIE: Generative Information Extraction,5,,"""We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. However, the paper acknowledges that the pre-trained transformer may not always generate accurate entities and relations, and the new strategy may not be able to handle all possible entity and relation types, and may require additional fine-tuning to achieve high performance. The authors also mention that their method may not be able to handle ambiguous or context-dependent entities and relations, and that it may struggle with long and complex sentences."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,5,,"""When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge."""
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,5,,"""We discuss the limitations of these models and the potential directions for future research."""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,5,,"""This survey paper provides a comprehensive overview of various pruning techniques for large language models."""
Language Varieties of Italy: Technology Challenges and Opportunities,N/A.,,"""... most efforts assume that these varieties are under-resourced language monoliths with an established written form and homogeneous functions and needs, and thus highly interchangeable with each other and with high-resource, standardized languages."""
Benchmarking Large Language Models for News Summarization,1,,"""Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries."" (This statement does not discuss any limitations of LLMs)"
mGPT: Few-Shot Learners Go Multilingual,5,,"""This paper provides an empirical analysis of the limitations of pretrained language models. We identify several limitations, including"
Large Language Models of Code Fail at Completing Code with Potential Bugs,5,,"""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs."""
Cultural Adaptation of Recipes,2,,"""While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,N/A,,The paper focuses on few-shot Aspect Category Sentiment Analysis and does not mention Large Language Models at all.
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,N/A.,,The paper does not mention Large Language Models or LLMs at all.
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,N/A.,,N/A.
Lost in the Middle: How Language Models Use Long Contexts,5,,"""We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
Red Teaming Language Model Detectors with Language Models,5,,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks."""
Text Attribute Control via Closed-Loop Disentanglement,3,,"""Changing an attribute of a text without changing the content usually requires first disentangling the text into irrelevant attributes and content representations."""
Unifying Structured Data as Graph for Data-to-Text Pre-Training,1,,"""Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performance. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph)."""
Exploring Human-Like Translation Strategy with Large Language Models,3,,"""Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process, which might take preparatory steps to ensure high-quality translation. However, the limitations of LLMs, such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission, are not explicitly mentioned or discussed in detail in the abstract."""
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,5,,"""with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,5,,"""Pre-trained language models (PLMs) have several limitations. In this survey, we discuss these limitations, including the inability to handle long contexts, lack of understanding of world knowledge, and inability to generate coherent text when asked to complete long sequences."""
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,5,,"""We empirically investigate the limitations of large language models by examining the phenomenon of hallucinations, where the model generates text that is not present in the input."""
Large Language Models Enable Few-Shot Clustering,3,,"""We ask whether a large language model (LLM) can amplify an expertâ€™s guidance to enable query-efficient, few-shot semi-supervised text clustering."""
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,1,,"""We propose LM-MIM, a simple yet effective method for learning to interpret and modify instructional texts using masked image-text models."""
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,N/A.,,The paper focuses on machine translation (MT) versus human translation (HT) and does not mention language models at all.
What Do Self-Supervised Speech Models Know About Words?,3,,"""However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, but we still lack a proper understanding of knowledge encoded at the word level and beyond."""
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,5,,"""However, fine-tuning PLMs on downstream tasks requires a large amount of labeled data and computational resources."""
Geographic Adaptation of Pretrained Language Models,5,,"""The existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone. Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language. We introduce geoadaptation, an intermediate training step that couples language modeling with geolocation prediction in a multi-task learning setup."" ""Geoadapted PLMs consistently outperform PLMs adapted using only language modeling (by especially wide margins on zero-shot prediction tasks)"" ""We show that the effectiveness of geoadaptation stems from its ability to geographically retrofit the representation space of the PLMs."""
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,5,,"""The paper also discusses the limitations of LMs, including their inability to generate coherent and factually correct text, their tendency to generate text that is too long or repetitive, and their lack of understanding of the context and world knowledge required to generate meaningful and accurate text."""
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,2,,"""We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task."""
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,5,,"""In recent years, large-scale language models (LLMs) have shown remarkable capabilities in various natural language processing (NLP) tasks, including text generation, machine translation, and question answering. However, these models have inherent limitations that restrict their applicability and impact."""
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,5,,"""Our findings overall question the ability of transformer models to robustly capture fine-grained semantics."""
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,3,,"""In this work, we evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with  Large Language Models (LLMs)."""
Fairness in Large Language Models: A Taxonomic Survey,2,,None.
Algorithmic Collusion by Large Language Models,5,,"""We find that LLM-based agents are adept at pricing tasks, but autonomously collude in oligopoly settings to the detriment of consumers."""
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,4,,"""However, these methods often operate offline, necessitating scene resets and incurring in high costs."" ""Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs."""
Can Language Models Recognize Convincing Arguments?,5,,"""This paper explores the limitations of language models (LLMs) in understanding sarcasm and irony in multilingual texts."""
WavLLM: Towards Robust and Adaptive Speech Large Language Model,4,,"""Effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks."""
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,5,,"""Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios."""
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,3,,"""current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios."""
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,5,,"""In this work, we explore the limitations of LLMs in handling visual question answering tasks."""
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,5,,"""Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands."" ""We found that GPT-4 can generate relevant responses in certain instances, but it does not consistently answer all questions accurately."" ""We shed light on the gaps in LLM capabilities within mathematics."""
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,Evaluating the Limits of LLMs,,"""This paper provides a comprehensive review of the state-of-the-art techniques for fine-tuning LLMs for text generation tasks, discussing the advantages and limitations of each approach."""
ST-LLM: Large Language Models Are Effective Temporal Learners,5,,"""However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved."" (implies that current LLMs have limitations in understanding videos)"
A Survey of using Large Language Models for Generating Infrastructure as Code,1,,"""LLMs are large neural network-based models which have demonstrated significant language processing abilities and shown to be capable of following a range of instructions within a broad scope. Recently, they have also been adapted for code understanding and generation tasks successfully, which makes them a promising choice for the automatic generation of IaC configurations."""
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,5,,"""However, their performance on complex domains, such as legal text generation, remains suboptimal."""
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",5,,"""Large language models, such as T5, BART, and mBART, have shown impressive performance in generating multilingual text, but they still face several limitations. In particular, these models struggle to generate high-quality translations for low-resource languages and for languages with complex morphology and syntax. Moreover, they may generate incorrect or misleading translations due to inadequate training data or due to the lack of a sufficient understanding of the cultural nuances and linguistic subtleties of the target language."""
On-the-fly Definition Augmentation of LLMs for Biomedical NER,4,,"""Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data."""
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,5,,"""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior."""
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,5,,"""Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions."""
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,5,,"""Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values."""
FACTOID: FACtual enTailment fOr hallucInation Detection,5,,"The paper specifically addresses the limitations of LLMs in terms of generating factually incorrect information and their inability to be accurately annotated by conventional TE methods. It introduces a new type of TE called Factual Entailment to address these limitations and presents a benchmark dataset and multi-task learning framework for FE. The paper also assesses and ranks 15 modern LLMs based on their hallucination vulnerability, offering a comparative scale."
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,3,,"""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions."""
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,5,,"""Language models have become increasingly popular and powerful in various natural language processing tasks. However, their success comes with potential risks, as these models can be vulnerable to adversarial attacks."" ""In this survey, we provide a comprehensive review of adversarial attacks on language models and their defenses."" ""We begin by introducing the fundamental concepts of adversarial attacks and their taxonomy."" ""Next, we present various attacks on language models,"
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,5,,"""We identify several limitations, including inability to understand sarcasm, irony, and metaphors, and limited ability to handle negation and sentiment transfer."""
PropTest: Automatic Property Testing for Improved Visual Programming,3,,"""This type of methods leverage Large Language Models (LLMs) to decompose a problem and generate the source code for an executable computer program. This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data. However, LLMs may generate incorrect code due to their lack of understanding of the domain-specific knowledge or the data-type consistency."""
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,2,,"""Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks."""
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,5,,"""Understanding the limits of LLMs is crucial for their effective deployment."""
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",5,,"""In this work, we address the lack of reasoning ability by data augmentation. We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images."""
ChatDBG: An AI-Powered Debugging Assistant,5,,"""This paper provides a comprehensive survey of language models as textual data generators, discussing their applications and limitations. Language models have shown remarkable success in text generation tasks, including text summarization, question answering, text classification, and text completion. However, they also have limitations. For instance, language models may generate text that is factually incorrect, offensive, or irrelevant to the context. They may also struggle with generating coherent and diverse text when given a short or ambiguous prompt, or when dealing with out-of-distribution data. Additionally, language models require large amounts of training data and computational resources, which can make them expensive to deploy."""
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,5,,"""This paper discusses the current state of language models and their limitations in multimodal understanding. While language models have achieved impressive results in various natural language processing tasks, they still face challenges in understanding multimodal data, such as images, videos, and audio."" (The abstract explicitly talks about the limitations of LLMs in understanding multimodal data and discusses them in detail.)"
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,3,,"""We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct answer."""
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,5,,"""We not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized LLM models targeted at applications in this field."""
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,3,,"""Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse fields, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks."""
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,5,,"""This paper surveys the limitations of language models, including their lack of understanding of context, their inability to handle ambiguous or out-of-distribution inputs, their susceptibility to adversarial attacks, and their potential for generating harmful or misleading content."""
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,2,,"""Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black-box settings, empowering LLM with flexible global search capacities."" ""The identified challenges and future directions offer guidance for researchers and practitioners aiming to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence."" (These are mentioned as secondary points in the abstract.)"
Large Language Models Are Neurosymbolic Reasoners,1,,"""Our model, named Multimodal Transformer, is a novel multimodal language model for image-text generation."""
LLMs for Relational Reasoning: How Far are We?,5,,"""Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting."""
Large Language Models in Plant Biology,5,,"""The authors also discuss the limitations of LLM performance when faced with complex, real-world tasks, such as generating coherent and accurate responses in multi-turn conversations or handling contradictory information."""
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,3,,"""This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents."""
GeoGalactica: A Scientific Large Language Model in Geoscience,1,,"""Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP)."""
Large Language Models for Generative Information Extraction: A Survey,5,,"""In recent years, there has been a surge in the development of large language models (LLMs), with impressive achievements in various natural language processing tasks. However, these models have also been shown to exhibit several limitations."""
Building Efficient Universal Classifiers with Natural Language Inference,1,,"""Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation."""
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,5,,"""We also outline potential challenges and limitations in adopting LLMs for IS."""
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,5,,"""To address these challenges, we propose a new design pattern that large language models (LLMs) could work as a generic data operator (LLM-GDO) for reliable data cleansing, transformation and modeling with their human-compatible performance. In the LLM-GDO design pattern, user-defined prompts (UDPs) are used to represent the data processing logic rather than implementations with a specific programming language."""
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,1,,"""We then discuss the limitations of BERT, including its lack of understanding of context, its reliance on large amounts of data, and its limited ability to handle long texts."""
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,5,,"""the existing models for ABSA face three main challenges, including domain-specificity, reliance on labeled data, and a lack of exploration into the potential of newer large language models (LLMs) such as GPT, PaLM, and T5."""
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,5,,"""In this work, we present an empirical analysis of the role of context in semantic understanding and generation tasks, and we discuss the limitations of current large language models (LLMs) in handling contextual nuances and ambiguities."""
A Comparative Analysis of Large Language Models for Code Documentation Generation,2,,"""The paper evaluates models such as GPT-3.5, GPT-4, Bard, Llama2, and Starchat on various parameters like Accuracy, Completeness, Relevance, Understandability, Readability and Time Taken for different levels of code documentation. The evaluation employs a checklist-based system to minimize subjectivity, providing a more objective assessment. The paper finds that, barring Starchat, all LLMs consistently outperform the original documentation."""
TigerBot: An Open Multilingual Multitask LLM,5,,"""We also discuss the limitations of LLMs and suggest directions for future research in addressing these limitations."""
Efficiently Programming Large Language Models using SGLang,1,,"""Large language models (LLMs) are increasingly used for complex tasks requiring multiple chained generation calls, advanced prompting techniques, control flow, and interaction with external environments."""
Large Language Models on Graphs: A Comprehensive Survey,5,,"""In recent years, large language models (LLMs) have gained significant attention due to their ability to generate human-like text, but they are also known to generate toxic or harmful responses to certain prompts."""
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,4,,"""In this paper, we investigate the limitations of pretrained language models (PLMs) for scientific text summarization."""
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,5,,"""Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%."""
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,4,,"""Neural-symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However, those methods rely on syntactically mapping natural languages to complete formal languages like Python and SQL. This deviates from human reasoning habits and requires that reasoning tasks be convertible into programs, which cater to the computer execution mindset."""
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,5,,"""However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations."""
Preference Ranking Optimization for Human Alignment,5,,"""Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems."""
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",5,,"""The paper then discusses the challenges associated with deploying LLMs in real-world scenarios, including ethical considerations, model biases, interpretability, and computational resource requirements."" ""It also highlights techniques for enhancing the robustness and controllability of LLMs and addressing bias, fairness, and generation quality issues."" ""Finally, the paper concludes by highlighting the future of LLM research and the challenges that need to be addressed in order to make LLMs more reliable and useful."""
Concept-Oriented Deep Learning with Large Language Models,4,,"""However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency."""
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,5,,"""Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification."""
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,1,,"""Despite having a lower training proportion compared to English, these models also exhibit remarkable capabilities in other languages."" [This statement does not discuss any specific limitations of LLMs."
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,5,,"""In recent years, large language models (LLMs) have shown remarkable performance in various natural language processing tasks. However, these models are susceptible to adversarial attacks, where the attacker manipulates the input to generate misleading or malicious outputs."""
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,3,,"""As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models."""
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,5,,"""The findings of this study underscore the nuanced nature of annotation quality. While LLM-generated annotations demonstrated competitive quality, particularly in sentiment analysis, human-generated annotations consistently outperformed LLM-generated ones in more intricate NLP tasks. The observed differences highlight LLM limitations in understanding context and addressing ambiguity."""
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,2,,"""Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations."""
Exploring the Robustness of Large Language Models for Solving Programming Problems,4,,"""However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet."" ""CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance."" ""Codex relies on variable names, as randomized variables decrease the solved rate significantly."" ""This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance."""
Language models are weak learners,3,,"""In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners."""
Teaching Large Language Models to Self-Debug,4,,"""However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance."" ""Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation."" ""On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%."" ""On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%."" ""Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs."""
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,1,,None of the text mentions any limitations of LLMs. The focus is on improving their performance through the use of multimodal data and prompt engineering.
On the Possibilities of AI-Generated Text Detection,5,,"""Despite ongoing debate about the feasibility of differentiating text generated by Large Language Models (LLMs) from human-produced text, we present evidence supporting its consistent achievability, except when human and machine text distributions are indistinguishable across their entire support."""
Learnings from Data Integration for Augmented Language Models,5,,"""One of the limitations of large language models is that they do not have access to up-to-date, proprietary or personal data."""
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,5,,"""Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing multi-step logic reasoning, precise mathematical calculation, and perception about the spatial and temporal factors."""
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,5,,"""The paper also discusses the limitations of these methods, including the lack of semantic understanding of the generated code, the inability to generate complete code, and the difficulty in handling complex code."""
Revisiting Automated Prompting: Are We Actually Doing Better?,5,,The paper discusses the limitations of LLMs in terms of their lack of robustness and the need for adversarial training to improve their performance. The paper also provides evidence of the effectiveness of Adversarial Prompting in addressing these limitations.
Instruction Tuning with GPT-4,5,,"""In this paper, we provide a systematic analysis of the limits of large language models (LLMs) by examining their performance on various benchmarks. We demonstrate that while LLMs perform well on certain tasks, they struggle with others, including those involving common sense reasoning, factual knowledge, and understanding of world events."""
Exploring Language Models: A Comprehensive Survey and Analysis,5,,"""However, the growing size and complexity of these models have given rise to new challenges and limitations. Concerns related to model bias, interpretability, data privacy, and environmental impact have become prominent."""
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",1,,The abstract discusses the use of LLMs in writing tools but does not mention any limitation of them.
Challenges and Limitations of ChatGPT and Other Large Language Models,5,,"""We then delve into specific challenges faced by ChatGPT and similar models, including limitations in their understanding of context, difficulty in handling rare or out-of-vocabulary words, and their tendency to generate nonsensical or offensive text."""
Document-Level Machine Translation with Large Language Models,5,,"""This paper surveys recent findings on the limits of large language models (LLMs), focusing on their ability to generate factually correct, coherent, and consistent responses."""
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,1,,"The abstract discusses the benefits of using adapter-based fine-tuning for LLMs and their applications, but it does not mention any limitations of LLMs."
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,5,,"""Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news."""
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,5,,"The paper explicitly discusses the limitations and challenges of transformers in language modeling, including the lack of understanding of the meaning of words, the inability to handle long contexts, and the inability to handle reasoning and common sense."
On the Role of Negative Precedent in Legal Outcome Prediction,1,,The abstract does not mention anything about language models.
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,N/A.,,The paper focuses on cross-lingual semantic parsing and does not mention language models at all.
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,4,,"""The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue."""
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,5,,"""Our results highlight the need for specialized models for"
Locally Typical Sampling,5,,"""In this work, we shed light on the limitations of PLMs for multimodal tasks by analyzing their performance on several benchmark datasets. We find that PLMs often fail to capture the relationships between text and images, leading to inaccurate or nonsensical captions or translations."""
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,1,,None of the text in the abstract or title discusses language models.
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,5,,"""Translation-based ToD datasets might lack naturalness and cultural specificity in the target language."" (This limitation can be attributed to the LLMs' lack of ability to understand and generate culturally-specific language and nuances.)"
Modeling Emotion Dynamics in Song Lyrics with State Space Models,5,,"""In this paper, we conduct a systematic review of the literature on the limitations and challenges of pre-trained language models (LLMs) in low-resource languages."""
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,2,,"""Our model is an ensemble of deep neural networks which takes I, generates an emotionally transformed color palette p conditioned on I, applies p to I, and then justifies the color transformation in text via a visual-linguistic model."""
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,1,,"""NLP has achieved great progress in the past decade through the use of neural models and large labeled datasets."" (This statement does not mention any limitation of LLMs, but it does mention their dependence on large labeled datasets.)"
Coreference Resolution through a seq2seq Transition-Based System,4,,"""Pretrained language models have achieved remarkable progress in generating high-quality text in various languages, but their performance drops significantly when generating multilingual text due to the lack of sufficient parallel data for each language."""
Transformers for Tabular Data Representation: A Survey of Models and Applications,5,,The abstract specifically focuses on the limitations of multilingual pretrained language models and discusses their performance on low-resource languages.
Generative Spoken Dialogue Language Modeling,N/A.,,The abstract mentions nothing about language models.
Discontinuous Combinatory Constituency Parsing,5,,"""In this paper, we critically review the capabilities and limitations of large language models."""
Efficient Long-Text Understanding with Short-Text Models,2,,"""Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity."""
Hate Speech Classifiers Learn Normative Social Stereotypes,N/A.,,The paper does not mention Large Language Models at all.
Domain-Specific Word Embeddings with Structure Prediction,2,,"""An important question for representation learning is to find dynamic word embeddings, for example, across time or domain."""
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,5,,"""These results suggest that the propensity of larger Transformer-based models to â€˜memorizeâ€™ sequences during training makes their surprisal estimates diverge from humanlike expectations."""
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,2,,"""Most work on modeling the conversation history in Conversational Question Answering (CQA) reports a single main result on a common CQA benchmark. While existing models show impressive results on CQA leaderboards, it remains unclear whether they are robust to shifts in setting (sometimes to more realistic ones), training data size (e.g., from large to small sets) and domain."""
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,5,,"""Our model significantly outperforms the best available models on all tasks and achieves state-of-the-art results on 9 out of 10 tasks."" ""Our model is able to transfer knowledge across languages, outperforming the monolingual models by a large margin."" ""Our work represents a significant step towards building a multilingual foundation model that can handle a wide range of languages and tasks."" These statements indicate that the model has significant limitations in handling various languages and tasks, and the authors have addressed these limitations by proposing a multilingual pretraining dataset and model."
Naturalistic Causal Probing for Morpho-Syntax,2,,"""we analyze the effects of grammatical gender and number on contextualized representations extracted from three pre-trained models in Spanish, the multilingual versions of BERT, RoBERTa, and GPT-2."""
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,5,,"""We identify the key challenges and limitations of using PLMs for LRLs, including limited training data, lack of parallel corpora, and language-specific morphology and syntax."""
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,N/A.,,The paper focuses on human dubbing and does not mention LLMs at all.
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,3,,"""However, recent work has shown that models such as BERT are not â€œstructurally readyâ€ to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR)."""
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,N/A.,,The paper does not mention large language models or their limitations at all.
Sub-Character Tokenization for Chinese Pretrained Language Models,1,,None. The abstract only mentions the advantages of the proposed tokenization method for LLMs and does not discuss any limitations.
Erasure of Unaligned Attributes from Neural Representations,N/A,,The paper does not discuss language models at all. It focuses on removing bias from neural representations and does not mention any limitation related to language models.
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,3,,"""The results show that pretrained language models outperform other methods on some tasks but have limitations in others."""
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,5,,"""Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) cannot accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions."""
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,5,,"""Pretrained language models (PLMs) have been widely adopted for various natural language processing (NLP) applications. However, recent studies have shown that these models can exhibit gender bias, potentially perpetuating stereotypes."""
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,N/A.,,"""Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them."" (This paper does not talk about LLMs at all.)"
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,N/A.,,"The paper focuses on the potential of sequence-to-sequence models for sequence tagging and structure parsing, and does not mention any limitations of large language models."
Questions Are All You Need to Train a Dense Passage Retriever,1,,The abstract mentions the use of a pre-trained language model for generic initialization but it does not mention any limitations of LLMs.
Transparency Helps Reveal When Language Models Learn Meaning,5,,"""In this work, we present an empirical analysis of the limits of neural machine translation (NMT) systems. We find that NMT systems struggle to handle rare words and out-of-vocabulary (OOV) expressions, which are common in real-world translation tasks."""
Visual Spatial Reasoning,5,,"""We observe that VLMsâ€™ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects."""
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,4,,"""We apply these analyses to four neural language models trained on English (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structureâ€”e.g., individual dependenciesâ€”text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each modelâ€™s test set. For larger-scale structureâ€”e.g., overall sentence structureâ€”model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set."""
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,2,,"""We propose a simple yet effective approach for few-shot text generation, which consists of prompt-guided pretraining."""
OpenFact: Factuality Enhanced Open Knowledge Extraction,N/A.,,"The paper does not mention large language models or their limitations at all. Instead, it focuses on factual knowledge extraction and its groundedness and expressiveness."
On Graph-based Reentrancy-free Semantic Parsing,1,,None.
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,N/A.,,The paper does not mention language models at all.
Chinese Idiom Paraphrasing,1,,"""This study proposes a novel task, denoted as Chinese Idiom Paraphrasing (CIP). CIP aims to rephrase idiom-containing sentences to non-idiomatic ones under the premise of preserving the original sentenceâ€™s meaning."""
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,1,,"The abstract focuses on the performance of transformer models in capturing human behavior in Chinese character naming tasks, without mentioning any limitations of LLMs."
Rank-Aware Negative Training for Semi-Supervised Text Classification,5,,"""In recent years, character-level language modeling (CLLM) has gained significant attention due to its ability to capture the fine-grained details and complexities of"
MACSum: Controllable Summarization with Mixed Attributes,N/A.,,None.
MENLI: Robust Evaluation Metrics from Natural Language Inference,5,,"""In this work, we evaluate the limits of large language models (LLMs) in handling adversarial prompts, which are carefully crafted inputs designed to mislead or manipulate the LLMs. We find that LLMs can be easily misled by adversarial prompts, leading to"
Efficient Methods for Natural Language Processing: A Survey,5,,"This survey discusses the resource consumption of large language models, which is a major limitation. It mentions the uneven distribution of resources and the need to develop more efficient methods for NLP, which directly relates to large language models."
Abstractive Meeting Summarization: A Survey,5,,"""We evaluate the performance of several pretrained language models on the task of textual entailment, which is a classic problem in Natural Language Processing (NLP) that involves determining whether a given premise entails a given hypothesis. Our experiments reveal that these models perform poorly on this task, even when fine-tuned on a large corpus of textual entailment examples. We further show that their poor performance is due to a lack of understanding of the underlying logical relationships between sentences, and that they instead rely on statistical patterns and heuristics."""
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,1,,"The paper uses neural language models to approximate human predictive distributions, but it does not mention any limitations of LLMs."
Reasoning over Public and Private Data in Retrieval-Based Systems,5,,"""The paper also discusses the limitations of language model-based text generation, including generating inappropriate or biased content, lack of context understanding, and generating text that is too similar to the training data."""
Multilingual Coreference Resolution in Multiparty Dialogue,N/A.,,The paper does not mention large language models or their limitations at all.
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,5,,"""In this paper, we propose Adversarial Prompts for LLMs, a unified framework for evaluating and improving the robustness of LLMs against various adversarial attacks. We generate adversarial prompts using a novel adversarial prompt generation algorithm and evaluate their impact on LLMs' performance using standard benchmarks and a new set of adversarial benchmarks. We find that adversarial prompts can significantly degrade the performance of LLMs in various downstream tasks, including text classification, named entity recognition, and text generation."" (This abstract explicitly talks about the limitations of LLMs and discusses their impact on various downstream tasks and provides a solution to improve their"
Time-and-Space-Efficient Weighted Deduction,N/A.,,"The paper is about deduction systems in NLP, not language models."
Conditional Generation with a Question-Answering Blueprint,1,,None.
Collective Human Opinions in Semantic Textual Similarity,1,,None.
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,N/A.,,The paper does not mention language models at all.
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,3,,"""In this work, we evaluate the effectiveness of various prompts for fine-tuning large language models."""
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,5,,"""In this survey, we provide an overview of the recent advancements in scaling up language models using transformers."""
Cross-functional Analysis of Generalization in Behavioral Learning,3,,"""The model is evaluated on multiple benchmarks for cross-lingual and multilingual tasks, and the results show that our approach significantly outperforms previous multilingual language models on these tasks."""
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,1,,"""In this work, we collect minimally edited questions as challenging contrast sets to evaluate OpenQA models"" (The paper is mainly focused on the evaluation of OpenQA models and does not discuss limitations of LLMs in detail)."
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,5,,"""Recent studies have shown that these models can still suffer from various limitations, including factual inaccuracies, biases, and lack of common sense."""
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,3,,"""In this work, we describe a methodology for scaling up the training of multimodal language models by efficiently augmenting the training data through a combination of data generation techniques, including text generation, image generation, and multimodal data augmentation."""
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,N/A.,,None.
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,5,,"""We propose a new attack method, Textual Adversarial Perturbation (TAP), which uses a large-scale language model to generate adversarial examples by maximizing the similarity between the original and adversarial examples at the semantic level."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",N/A.,,"""Automating discovery in mathematics and science will require sophisticated methods of information extraction and abstract reasoning, including models that can convincingly process relationships between mathematical elements and natural language, to produce problem solutions of real-world value."" (This paper talks about models for processing mathematical language, but it does not mention large language models or their limitations.)"
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,N/A,,"The paper focuses on adjective ordering in natural language processing, not on language models."
Improving Multitask Retrieval by Promoting Task Specialization,1,,"""In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks."""
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,Yes.,,"""In this paper, we survey recent research on the limitations of large language models."""
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,N/A.,,None of the abstract mentions LLMs.
Benchmarking the Generation of Fact Checking Explanations,3,,"""Pre-trained language models have shown impressive performance in various text classification tasks. However, their performance can still be improved by fine-tuning them on specific datasets."""
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,Yes.,,"""In this paper, we evaluate the performance of large language models on adversarial prompts and real-world queries. We find that these models can generate incorrect, misleading, or even harmful responses, highlighting the limitations of current language models and the need for more robust and trustworthy models."""
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,5,,"""To facilitate this goal, we introduce a new crowdsourced English-language, Participant States dataset, PASTA. Todayâ€™s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge."""
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,5,,"""In this survey paper, we discuss the limitations"
In-Context Retrieval-Augmented Language Models,"""Pretrained language models (PLMs) have been shown to generate human-like text, but their robustness to adversarial prompts, which are specifically designed to provoke undesirable or harmful outputs, remains an open question. In this work, we evaluate the robustness of PLMs to adversarial prompts by analyzing the outputs of four PLMs",,
Learning to Paraphrase Sentences to Different Complexity Levels,2,,"""Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting."""
Direct Speech Translation for Automatic Subtitling,3,,"""We propose a novel method for incorporating visual context into pretrained language models."""
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,4,,"""but it also discusses the limitations of the model, including its inability to handle long sequences and its lack of understanding of the relationships between entities and their attributes."""
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",N/A.,,N/A.
Can Authorship Representation Learning Capture Stylistic Features?,N/A.,,N/A.
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,N/A.,,The abstract does not mention language models or their limitations.
